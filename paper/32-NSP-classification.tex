\subsection{Classification Based on One Concatenated Embedding}
~\label{sec:nsp}

Following the finetuning setup of AcroBERT~\cite{chen2023gladis}, %we adapt 
BERT for NSP is adapted to build a supervised sentence pair classifier to link definitions to their page titles in ProofWiki. Every pair of (definition, candidate title with the matching ambiguous term in ProofWiki) is concatenated as an input sequence. The sequence begins with a [CLS] token, followed by a candidate title, a [SEP] token, and then the definition, ending with [SEP]. The input sequence passes through BERT's transformer layers. These layers produce contextual embedding for each token in the sequence. Then, the embedding of [CLS] is fed into a softmax classification layer, which outputs a score to judge how coherent the concatenated sequence is. %We select the
The pair with the highest score is selected as the final predicted output. %We first use the
First the out-of-box BERT for NSP serves as the baseline to see how well the pre-retained natural language inference model can describe the entailment between the titles and definitions. %We then finetune the 
Then the pretrained BERT for NSP is finetuned with the training set using a triplet loss function $\mathcal{L} = \max \left \{ 0, \lambda - d_{\text{neg} } + d_{\text{pos} } \right \}$ that aims to assign higher scores to the correct titles that match the input definition while reducing the scores of irrelevant candidates, where $\lambda = 0.2$ is the margin value, and $d_{\text{pos}}$ and $d_{\text{neg}}$ are the distances for positive and negative pairs, respectively. %We implement this 
This approach is implemented with PyTorch~\cite{paszke2019pytorch} and transfomers~\cite{wolf-etal-2020-transformers}. %We use a 
A batch size of 16 and Adam optimizer with learning rate 1e-5 are used. The learning rate is exponentially decayed at a rate of 0.95 every 1000 steps. %We train the 
The model is trained with the training dataset for 100 epochs and evaluated with the test dataset after each epoch. %All our experiments are run with an NVIDIA Tesla V100S-PCIE 32GB GPU.



