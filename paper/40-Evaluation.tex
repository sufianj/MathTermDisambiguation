\section{Results and Discussion}
\label{sec:experiments}
%We use a
Accuracy and the average of the $F_1$ score for each ambiguous term (macro $F_1$ score) are used to measure how well both approaches can link a definition to the correct title. Table~\ref{tab:all_scores} shows the experimental results of both methods. Overall, our finetuned NSP model performs best, validating AcroBERT's set-up and the helpfulness of BERT for NSP's pretrained weights. Notably, the out-of-the-box SBERT demonstrated excellent performance with much less inference time.
%The performance of prediction based on STS with SBERT models are aligned with the results of ~\cite{reimers2019sentence} and ~\cite{steinfeldt2024evaluation}. 
Given that both BERT for NSP and SBERT are pretrained on NLI tasks~\cite{devlin2019bert,reimers2019sentence}, it may be deduced that i) compared to using the [CLS] representation of concatenated sequence, using separated sentence embeddings captures more information for our task, and/or ii) SBERT's pretraining on (title, abstract) pairs from S2ORC dataset~\cite{lo-wang-2020-s2orc} helps to better understand the entailment between titles and body texts. However, 
%Bert-MLM\_arXiv-MP-class\_zbMath, a
the domain-adapted SBERT model~\footnote{\url{https://huggingface.co/math-similarity/Bert-MLM_arXiv-MP-class_arXiv}}  that the authors of ~\cite{steinfeldt2024evaluation} finetuned with multiple tasks using titles and abstracts of mathematical papers does not yield better results. This might be due to MLM being solely trained on titles and abstracts, diminishing the model's representational capacity for general text. 

\textbf{Limitations:} An interesting finding %of this work
is that SBERT for STS and the finetuned BERT for NSP make some common mistakes, indicating the limits of using only semantic representations. The most common error is when the definition statement includes nested definitions.  Another typical error is that the predicted result is in the correct category but not the definiendum, mainly when the definition contains morphemes in the predicted title or when the definition does not contain some morphemes in the expected title. For example, the definition of ``Consequence Function'' starts with ``Let $\mathbf{G}$ be a game...''~\footnote{\url{https://proofwiki.org/wiki/Definition:Consequence_Function}}
, and the predicted title is ``Definition:Consequence(Game Theory)'~\footnote{\url{https://proofwiki.org/wiki/Definition:Consequence_(Game_Theory)}}
. Thus, enhancing sentence embedding's comprehension of semantic and syntactic knowledge of mathematical definitions is still worth investigating. Other common mistakes reveal the noises in the dataset due to automatic scrapping and \LaTeX conversion of irregular ProofWiki pages.

\squeezeup
\begin{table}
    \centering
    \caption{Accuracy and macro $F_1$ scores. Values are reported as $\rho \cdot 100$.}
    \begin{tabular}{|p{0.4\linewidth}|p{0.15\linewidth}|p{0.1\linewidth}|p{0.1\linewidth}|p{0.1\linewidth}|p{0.1\linewidth}|}
    \hline
        Model & Approach& \multicolumn{2}{l|}{Test} & \multicolumn{2}{l|}{Train}\\ \cline{3-6}
         & & $F_1$ & Acc. & $F_1$ & Acc. \\ \hline
        BERT~\cite{devlin2019bert} & NSP& 80,9 & 84,8 & 79,8 & 83,9 \\ \hline
        Finetuned BERT & NSP& \textbf{92,1} & \textbf{93,8} & - & - \\ \hline
   %     BERT & STS& 26,1 & 35,3 & 30,2 & 40,3 \\ \hline
   %     CC-BERT~\cite{mishraPS21} &STS& 28,2 & 37,9 & 35,6 & 45,4 \\ \hline
        %SBERT-all-MiniLM-L6-v2&STS& 90,1 & 92,4 & 88,1 & 91,0 \\ \hline
        %SBERT-all-MiniLM-L12-v2&STS& 91,2 & 93,3 & \textbf{89,4} & \textbf{92,1} \\ \hline
        SBERT-all-mpnet-base-v2~\cite{reimers2019sentence}
        &STS& \textbf{91,4} & \textbf{93,5} & 89,0 & 91,6 \\ \hline
        %Bert-MLM\_arXiv~\cite{steinfeldt2024evaluation} &STS& 28,3 & 38,3 & 32,8 & 42,7 \\ \hline
        Adapted SBERT~\cite{steinfeldt2024evaluation}%~\footnote{\url{https://huggingface.co/math-similarity/Bert-MLM_arXiv-MP-class_arXiv}} 
        &STS& 43,8 & 52,2 & 54,0 & 61,5 \\ \hline
    \end{tabular}
    \label{tab:all_scores}
\end{table}




