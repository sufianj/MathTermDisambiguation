\usepackage{XZVT-private}
dd_page_title,term,definition,def_page_title,categories
Definition:Above,Above,"In the context of numbers, above means greater than.

Note that this applies to:
:the natural numbers $\N$
:the integers $\Z$
:the rational numbers $\Q$
:the real numbers $\R$

but specifically not the complex numbers $\C$ because the complex numbers do not have a usual ordering.",Definition:Above (Number),['Definitions/Language Definitions']
Definition:Above,Above,"Let $a$ and $b$ be points in $3$-dimensional Euclidean space $\R^3$.

Let $P$ be an arbitrary plane embedded in $S$ be distinguished and defined as horizontal.

Then:
:$a$ is above $b$
 if and only if :
:the height of $a$   $P$ is greater than the height of $b$   $P$.",Definition:Above (Solid Geometry),"['Definitions/Language Definitions', 'Definitions/Solid Geometry']"
Definition:Above,Above,"Let $a$ and $b$ be points in the cartesian plane $\R^2$.

Then:
:$a$ is above $b$
 if and only if :
:the $y$ coordinate of $a$ is greater than the $y$ coordinate of $b$.",Definition:Above (Plane Geometry),"['Definitions/Language Definitions', 'Definitions/Plane Geometry']"
Definition:Absolute,Absolute,"An absolute number is a number in an expression which has a single value.

It is either expressed using actual figures, in an agreed number system, or by a symbol which is understood to represent that specific number.",Definition:Absolute Number,"['Definitions/Algebra', 'Definitions/Numbers']"
Definition:Absolute,Absolute,"A constant is a name for an object (usually a number, but the concept has wider applications) which does not change during the context of a logical or mathematical argument.


A constant can be considered as an operator which takes no operands.

A constant can also be considered as a variable whose domain is a singleton.",Definition:Constant,"['Definitions/Constants', 'Definitions/Algebra']"
Definition:Absolute,Absolute,"=== Definition 1 ===
Let $x \in \R$ be a real number.


The absolute value of $x$ is denoted $\size x$, and is defined using the usual ordering on the real numbers as follows:
:$\size x = \begin{cases} x & : x > 0 \\ 0 & : x = 0 \\ -x & : x < 0 \end{cases}$

=== Definition 2 ===
Let $x \in \R$ be a real number.

The absolute value of $x$ is denoted $\size x$, and is defined as:

:$\size x = +\sqrt {x^2}$

where $+\sqrt {x^2}$ is the positive square root of $x^2$.",Definition:Absolute Value,"['Definitions/Absolute Value Function', 'Definitions/Field Theory', 'Definitions/Algebra', 'Definitions/Real Analysis']"
Definition:Absolute,Absolute,"=== Real Function ===
Let $I \subseteq \R$ be a real interval.


A real function $f: I \to \R$ is said to be absolutely continuous  if and only if  it satisfies the following property:

:For every $\epsilon > 0$ there exists $\delta > 0$ such that the following property holds:

::For every finite set of pairwise disjoint closed real intervals $\closedint {a_1} {b_1}, \dotsc, \closedint {a_n} {b_n} \subseteq I$ such that:
:::$\ds \sum_{i \mathop = 1}^n \size {b_i - a_i} < \delta$
::it holds that:
:::$\ds \sum_{i \mathop = 1}^n \size {\map f {b_i} - \map f {a_i} } < \epsilon$

=== Measure ===
Let $\struct {X, \Sigma}$ be a measurable space.

Let $\mu$ and $\nu$ be measures on $\struct {X, \Sigma}$.


We say that $\nu$ is absolutely continuous with respect to $\mu$ and write: 

:$\nu \ll \mu$

 if and only if :

:$\ds \forall A \in \Sigma : \map \mu A = 0 \implies \map \nu A = 0$

=== Signed Measure ===
Let $\struct {X, \Sigma}$ be a measurable space.

Let $\mu$ be a measure on $\struct {X, \Sigma}$.

Let $\nu$ be a signed measure on $\struct {X, \Sigma}$.

Let $\size \nu$ be the variation of $\nu$.


We say that $\nu$ is absolutely continuous with respect to $\mu$  if and only if :

:$\size \nu$ is absolutely continuous with respect to $\mu$.

We write:

:$\nu \ll \mu$

=== Complex Measure ===
Let $\struct {X, \Sigma}$ be a measurable space.

Let $\mu$ be a measure on $\struct {X, \Sigma}$.

Let $\nu$ be a complex measure on $\struct {X, \Sigma}$.

Let $\size \nu$ be the variation of $\nu$.


We say that $\nu$ is absolutely continuous with respect to $\mu$  if and only if :

:$\size \nu$ is absolutely continuous with respect to $\mu$.

We write:

:$\nu \ll \mu$",Definition:Absolute Continuity,"['Definitions/Absolute Continuity', 'Definitions/Continuity']"
Definition:Absolute,Absolute,"Let $f: \R^n \to \R$ be a real-valued function.

Let $f$ be such that, for all $\mathbf x := \tuple {x_1, x_2, \ldots, x_n} \in \R^n$:
:$\map f {\mathbf x} = \map f {\mathbf y}$
where $\mathbf y$ is a permutation of $\tuple {x_1, x_2, \ldots, x_n}$.

Then $f$ is an absolutely symmetric function.",Definition:Symmetric Function/Absolute,['Definitions/Symmetric Functions']
Definition:Absolute,Absolute,"Let $K$ be a field.


=== Definition 1 ===
Let $K$ be a field.


The absolute Galois group of $K$ is the Galois group $\Gal {K^{\operatorname{sep} } \mid K}$ of its separable closure.

=== Definition 2 ===
Let $K$ be a field.


The absolute Galois group of $K$ is the automorphism group $\Aut {\overline K \mid K}$ of its algebraic closure.",Definition:Absolute Galois Group,"['Definitions/Absolute Galois Groups', 'Definitions/Galois Groups of Field Extensions', 'Definitions/Galois Theory']"
Definition:Absolute,Absolute,"Let $V$ be a Banach space.

Let $\family {v_i}_{i \mathop \in I}$ be an indexed family of elements of $V$.


Then $\ds \sum \set {v_i: i \in I}$ converges absolutely  if and only if  $\ds \sum \set {\norm {v_i}: i \mathop \in I}$ converges.

This nomenclature is appropriate as we have Absolutely Convergent Generalized Sum Converges.",Definition:Generalized Sum/Absolute Net Convergence,"['Definitions/Group Theory', 'Definitions/Generalized Sums']"
Definition:Absolute,Absolute,"Let $T_1 = \struct {S_1, \tau_1}$ be a topological space.

Let $T_2 = \struct {S_2, \tau_2}$ be a topological subspace of $T_1$.


Let $T_2$ be a retract of $T_1$.


$T_2$ is an absolute retract of $T_1$  if and only if :
:for every closed subspace $B$ of a $T_4$ space $T$ such that $B$ is homeomorphic to $A$, then $B$ is a retract of $T$.",Definition:Retract (Topology)/Absolute,['Definitions/Topology']
Definition:Absolute,Absolute,"Let $x_0$ be an approximation to a (true) value $x$.


The absolute error of $x_0$ in $x$ is defined as:

:$\Delta x := x_0 - x$


=== Correction ===
Let $x_0$ be an approximation to a (true) value $x$.

Let $\Delta x$ denote the absolute error of $x_0$.


The correction to $x$ is the quantity that needs to be added to $x_0$ to change it to $x$.

That is, the correction to $x$ is defined as $-\Delta x$.",Definition:Error/Absolute,['Definitions/Errors']
Definition:Absolute,Absolute,"Let $S$ be a sample or a population.

Let $\omega$ be a qualitative variable, or a class interval of a quantitative variable.


The frequency of $\omega$ is the number of individuals in $S$ satisfying $\omega$.",Definition:Frequency (Descriptive Statistics),"['Definitions/Frequency (Descriptive Statistics)', 'Definitions/Class Intervals', 'Definitions/Qualitative Variables', 'Definitions/Descriptive Statistics']"
Definition:Absolute,Absolute,An absolute measure of dispersion is a measure of dispersion that indicates how spread out or scattered a set of observations with respect to the actual values of those observations.,Definition:Absolute Measure of Dispersion,['Definitions/Dispersion (Statistics)']
Definition:Absolute,Absolute,Absolute geometry is the study of Euclidean geometry without the parallel postulate.,Definition:Absolute Geometry,"['Definitions/Absolute Geometry', 'Definitions/Geometry', 'Definitions/Branches of Mathematics']"
Definition:Absolute,Absolute,"A real number $r$ is absolutely normal if it is normal   every number base $b$.

That is,  if and only if  its basis expansion in every number base $b$ is such that:
:no finite sequence of digits of $r$ of length $n$ occurs more frequently than any other such finite sequence of length $n$.


In particular, for every number base $b$, all digits of $r$ have the same natural density in the basis expansion of $r$.",Definition:Absolutely Normal Number,"['Definitions/Absolutely Normal Numbers', 'Definitions/Normal Numbers', 'Definitions/Numbers']"
Definition:Absolute,Absolute,"Absolute temperature is a measure of the amount of heat energy in a body.

It is defined as:
:$T = \dfrac 1 k \paren {\dfrac {\partial U} {\partial \ln g} }$
where:
:$k$ is a constant that relates the mean kinetic energy and absolute temperature of the body $B$
:$U$ is the total energy of $B$
:$g$ is the number of possible states in which $B$ can be.


=== Dimension ===
Temperature is frequently, at elementary levels at least, considered as one of the fundamental dimensions of physics.

In dimensional analysis it is usually assigned the symbol $\Theta$.


Category:Definitions/Dimensions of Measurement
Category:Definitions/Temperature",Definition:Temperature/Absolute,['Definitions/Temperature']
Definition:Absolute,Absolute,"Absolute zero is the lowest temperature which can theoretically be achieved.

It is the temperature where all motion due to thermal effects stops.

Before that temperature can be reached, quantum effects come into play.",Definition:Absolute Zero,['Definitions/Temperature']
Definition:Absolute,Absolute,"Let $\mathbf A$ be an array.


When referring to a specific element of $\mathbf A$ directly by its indices $\tuple {i, j}$, those indices can be referred to as the absolute address of that element.

This terminology is most often seen in the context of computer spreadsheet programs.",Definition:Array/Element/Absolute Address,['Definitions/Arrays']
Definition:Action,Action,"Let $B_1$ and $B_2$ be bodies in space.

Let $B_1$ apply a force $\mathbf F$ on $B_2$.

The force $\mathbf F$ is known as the action that $B_1$ applies to $B_2$.",Definition:Action (Physics),['Definitions/Force']
Definition:Action,Action,"The action for a given segment from point $A$ to point $B$ on the trajectory of a dynamical system is defined by the line integral:

:$\ds \sum_i \int_A^B p_i \rd q_i$

where:
:$q_i$ are the generalized coordinates
:$p_i$ are the generalized momenta.",Definition:Action (Dynamics),['Definitions/Dynamics']
Definition:Action,Action,"The action applied by a system from state $1$ to state $2$ is defined as the definite integral of the Lagrangian over time from state $1$ to state $2$:

:$\ds S_{12} = \int_{t_1}^{t_2} \LL \rd t$
where:
:$S_{12}$ is the action from $1$ to $2$
:$t$ is time
:$\LL$ is the Lagrangian.",Definition:Action Applied by System,"['Definitions/Hamiltonian Mechanics', 'Definitions/Dimensions of Measurement']"
Definition:Action,Action,"Let $X$ be a set.

Let $\struct {G, \circ}$ be a group whose identity is $e$.


=== Left Group Action ===
Let $X$ be a set.

Let $\struct {G, \circ}$ be a group whose identity is $e$.


A (left) group action is an operation $\phi: G \times X \to X$ such that:

:$\forall \tuple {g, x} \in G \times X: g * x := \map \phi {g, x} \in X$

in such a way that the group action axioms are satisfied:
 

=== Right Group Action ===
Let $X$ be a set.

Let $\struct {G, \circ}$ be a group whose identity is $e$.


A right group action is a mapping $\phi: X \times G \to X$ such that:

:$\forall \tuple {x, g} \in X \times G : x * g := \map \phi {x, g} \in X$

in such a way that the right group action axioms are satisfied:
 

The group $G$ thus acts on the set $X$.

The group $G$ can be referred to as the group of transformations, or a transformation group.


=== From Permutation Representation ===
Let $G$ be a group.

Let $X$ be a set.

Let $\struct {\map \Gamma X, \circ}$ be the symmetric group on $X$.

Let $\rho: G \to \struct {\map \Gamma X, \circ}$ be a permutation representation.


The group action of $G$ associated to the permutation representation $\rho$ is the group action $\phi: G \times X \to X$ defined by:
:$\map \phi {g, x} = \map {\rho_g} x$

where $\rho_g : X \to X$ is the permutation representation associated to $\rho$ for $g \in G$ by $\map {\rho_g} x = \map \phi {g, x}$.",Definition:Group Action,['Definitions/Group Actions']
Definition:Action,Action,"Let $R$ be a ring.

Let $M$ be an abelian group.


=== Left Ring Action ===
Let $R$ be a ring.

Let $M$ be an abelian group.

Let $\circ : R \times M \to M$ be a mapping from the cartesian product $R \times M$.


$\circ$ is a left linear ring action of $R$ on $M$  if and only if  $\circ$ satisfies the left ring action axioms:
 

=== Right Ring Action ===
Let $R$ be a ring.

Let $M$ be an abelian group.

Let $\circ : M \times R \to M$ be a mapping from the cartesian product $M \times R$.


$\circ$ is a right linear ring action of $R$ on $M$  if and only if  $\circ$ satisfies the right ring action axioms:
 ",Definition:Linear Ring Action,"['Definitions/Module Theory', 'Definitions/Linear Ring Actions']"
Definition:Acyclic,Acyclic,"Let $\mathbf A$ be an abelian category with enough injectives.

Let $\mathbf B$ be an abelian category.

Let $F : \mathbf A \to \mathbf B$ be a left exact functor.

Let $X$ be an object of $\mathbf A$.


Then $X$ is $F$-acyclic  if and only if  $\mathrm R^i \map F X = 0$ for all positive integers $i \in \Z_{i \mathop \ge 1}$.

In the above $\mathrm R^i F$ denotes the $i$-th right derived functor of $F$.


Category:Definitions/Homological Algebra",Definition:Acyclic Object,['Definitions/Homological Algebra']
Definition:Acyclic,Acyclic,"Let $\mathbf A$ be an abelian category with enough injectives.

Let $\mathbf B$ be an arbitrary abelian category.

Let $F : \mathbf A \to \mathbf B$ be a left-exact functor.

Let $X$ be an object in $\mathbf A$.


An $F$-acyclic resolution of $X$ is a cochain complex $P := \family {d^i : I^i \to I^{i + 1} }_{i \mathop \in \Z}$ in $\mathbf A$, such that:
:$(1): \quad \forall i < 0 : I^i = 0$
:$(2): \quad I^i$ is $F$-acyclic for all $i \ge 0$
together with a morphism $\varepsilon : X \to I^0$, such that the cochain complex:
:$\begin{xy}
\xymatrix{
\dots \ar[r] & 0 \ar[r] & 0 \ar[r] & X \ar[r]^{\varepsilon} & I^0 \ar[r]^{d^0} & I^1 \ar[r]^{d^1} & I^2 \ar[r]^{d^2} & \dots
}
\end{xy}$
is exact.",Definition:Acyclic Resolution,['Definitions/Homological Algebra']
Definition:Acyclic,Acyclic,An acyclic graph is a graph or digraph with no cycles.,Definition:Acyclic Graph,['Definitions/Graph Theory']
Definition:Acyclic,Acyclic,"Let $X$ be a topological space.

Let $\FF$ be an abelian sheaf on $X$.

Let $\map \Gamma {X, -} : \map {\mathbf {Ab} } X \to \mathbf {Ab}$ be the global sections functor on $X$.


Then $\FF$ is acyclic  if and only if  $\FF$ is $\map \Gamma {X, -}$-acyclic in the category of abelian sheaves $\map {\mathbf {Ab} } X$ on $X$.",Definition:Acyclic Sheaf,"['Definitions/Homological Algebra', 'Definitions/Sheaf Theory']"
Definition:Additive Function,Additive Function,"Let $f: S \to S$ be a mapping on an algebraic structure $\struct {S, +}$.


Then $f$ is an additive function  if and only if  it preserves the addition operation:
:$\forall x, y \in S: \map f {x + y} = \map f x + \map f y$",Definition:Additive Function (Conventional),"['Definitions/Analysis', 'Definitions/Abstract Algebra', 'Definitions/Linear Algebra']"
Definition:Additive Function,Additive Function,"Let $R$ be a unique factorization domain.

Let $f : R \to \C$ be a complex-valued function.


Then $f$ is additive  if and only if :
:For all coprime $x, y \in R$: $\map f {x y} = \map f x + \map f y$


=== Arithmetic Function ===
Let $f : \N \to \C$ be an arithmetic function.


Then $f$ is additive  if and only if :
:$m \perp n \implies \map f {m n} = \map f m + \map f n$",Definition:Additive Function on UFD,['Definitions/Ring Theory']
Definition:Additive Function,Additive Function,"Let $\struct {R, +, \times}$ be a ring.

Let $f: R \to R$ be a mapping on $R$.


Then $f$ is described as completely additive  if and only if :

:$\forall m, n \in R: \map f {m \times n} = \map f m + \map f n$


That is, a completely additive function is one where the value of a product of two numbers equals the sum of the value of each one individually.",Definition:Completely Additive Function,"['Definitions/Ring Theory', 'Definitions/Number Theory']"
Definition:Additive Function,Additive Function,"Let $\SS$ be an algebra of sets.

Let $f: \SS \to \overline \R$ be a function, where $\overline \R$ denotes the set of extended real numbers.


Then $f$ is defined to be additive  if and only if :

:$\forall S, T \in \SS: S \cap T = \O \implies \map f {S \cup T} = \map f S + \map f T$

That is, for any two disjoint elements of $\SS$, $f$ of their union equals the sum of $f$ of the individual elements.


Note from Finite Union of Sets in Additive Function that:

:$\ds \map f {\bigcup_{i \mathop = 1}^n S_i} = \sum_{i \mathop = 1}^n \map f {S_i}$

where $S_1, S_2, \ldots, S_n$ is any finite collection of pairwise disjoint elements of $\SS$.",Definition:Additive Function (Measure Theory),['Definitions/Measure Theory']
Definition:Additive Function,Additive Function,"Let $\Sigma$ be a $\sigma$-algebra.

Let $f: \Sigma \to \overline \R$ be a function, where $\overline \R$ denotes the set of extended real numbers.


Then $f$ is defined as countably additive  if and only if :
:$\ds \map f {\bigcup_{n \mathop \in \N} E_n} = \sum_{n \mathop \in \N} \map f {E_n}$

where $\sequence {E_n}$ is any sequence of pairwise disjoint elements of $\Sigma$.


That is, for any countably infinite set of pairwise disjoint elements of $\Sigma$, $f$ of their union equals the sum of $f$ of the individual elements.",Definition:Countably Additive Function,"['Definitions/Set Systems', 'Definitions/Measure Theory']"
Definition:Adjacent,Adjacent,"=== Vertices ===
=== Undirected Graph ===
Let $G = \struct {V, E}$ be an undirected graph.

Two vertices $u, v \in V$ of $G$ are adjacent  if and only if  there exists an edge $e = \set {u, v} \in E$ of $G$ to which they are both incident.


Otherwise they are non-adjacent.

=== Digraph ===
Let $G = \struct {V, E}$ be a digraph.

Two vertices $u, v \in V$ of $G$ are adjacent  if and only if  there exists an arc $e = \tuple {u, v} \in E$ of $G$ to which they are both incident.


Otherwise they are non-adjacent.

=== Non-Adjacent ===
Let $G = \struct {V, E}$ be a graph.

Two vertices $u, v \in V$ of $G$ are non-adjacent  if and only if  they are not adjacent.

=== Edges ===
=== Undirected Graph ===
Let $G = \struct {V, E}$ be an undirected graph.

Two edges $e_1, e_2 \in E$ of $G$ adjacent  if and only if  there exists a vertex $v \in V$ to which they are both incident.


Otherwise they are non-adjacent.

=== Digraph ===
Let $G = \struct {V, E}$ be a digraph.

Two arcs $e_1, e_2 \in E$ of $G$ adjacent  if and only if  there exists a vertex $v \in V$ to which they are both incident.


Otherwise they are non-adjacent.

=== Non-Adjacent ===
Let $G = \struct {V, E}$ be a graph.

Two edges $u, v \in V$ of $G$ are non-adjacent  if and only if  they are not adjacent.

=== Faces ===
:

Let $G = \struct {V, E}$ be a planar graph.

Two faces of $G$ are adjacent  if and only if  they are both incident to the same edge (or edges).

In the above diagram, $BCEF$ and $ABF$ are adjacent, but $BCEF$ and $AFG$ are not adjacent.


Note that faces which are both incident to the same vertex are not considered adjacent unless they are also both incident to the same edge.",Definition:Adjacent (Graph Theory),"['Definitions/Adjacency (Graph Theory)', 'Definitions/Graph Theory']"
Definition:Adjacent,Adjacent,"=== Undirected Graph ===
Let $G = \struct {V, E}$ be an undirected graph.

Two vertices $u, v \in V$ of $G$ are adjacent  if and only if  there exists an edge $e = \set {u, v} \in E$ of $G$ to which they are both incident.


Otherwise they are non-adjacent.

=== Digraph ===
Let $G = \struct {V, E}$ be a digraph.

Two vertices $u, v \in V$ of $G$ are adjacent  if and only if  there exists an arc $e = \tuple {u, v} \in E$ of $G$ to which they are both incident.


Otherwise they are non-adjacent.

=== Non-Adjacent ===
Let $G = \struct {V, E}$ be a graph.

Two vertices $u, v \in V$ of $G$ are non-adjacent  if and only if  they are not adjacent.",Definition:Adjacent (Graph Theory)/Vertices,"['Definitions/Adjacency (Graph Theory)', 'Definitions/Vertices of Graphs']"
Definition:Adjacent,Adjacent,"=== Undirected Graph ===
Let $G = \struct {V, E}$ be an undirected graph.

Two edges $e_1, e_2 \in E$ of $G$ adjacent  if and only if  there exists a vertex $v \in V$ to which they are both incident.


Otherwise they are non-adjacent.

=== Digraph ===
Let $G = \struct {V, E}$ be a digraph.

Two arcs $e_1, e_2 \in E$ of $G$ adjacent  if and only if  there exists a vertex $v \in V$ to which they are both incident.


Otherwise they are non-adjacent.

=== Non-Adjacent ===
Let $G = \struct {V, E}$ be a graph.

Two edges $u, v \in V$ of $G$ are non-adjacent  if and only if  they are not adjacent.",Definition:Adjacent (Graph Theory)/Edges,"['Definitions/Edges of Graphs', 'Definitions/Adjacency (Graph Theory)']"
Definition:Adjacent,Adjacent,":

Let $G = \struct {V, E}$ be a planar graph.

Two faces of $G$ are adjacent  if and only if  they are both incident to the same edge (or edges).

In the above diagram, $BCEF$ and $ABF$ are adjacent, but $BCEF$ and $AFG$ are not adjacent.


Note that faces which are both incident to the same vertex are not considered adjacent unless they are also both incident to the same edge.",Definition:Adjacent (Graph Theory)/Faces,"['Definitions/Adjacency (Graph Theory)', 'Definitions/Faces of Graphs']"
Definition:Adjacent,Adjacent,"Two angles are adjacent if they have an intersecting line in common:

:",Definition:Angle/Adjacent,['Definitions/Angles']
Definition:Adjacent,Adjacent,"=== Adjacent Side to Vertex ===
Each vertex of a polygon is formed by the intersection of two sides.

The two sides that form a particular vertex are referred to as the adjacents of that vertex, or described as adjacent to that vertex.

=== Adjacent Sides ===
Two sides of a polygon that meet at the same vertex are adjacent to each other.

=== Adjacent Vertex to Side ===
Each side of a polygon intersects two other sides, and so is terminated at either endpoint by two vertices.

The two vertices that terminate a particular side are referred to as the adjacents of that side, or described as adjacent to that side.

=== Adjacent Vertices ===
Each side of a polygon intersects two other sides, and so is terminated at either endpoint by two vertices.


Those two vertices are described as adjacent to each other.",Definition:Polygon/Adjacent,"['Definitions/Adjacent (Polygons)', 'Definitions/Polygons']"
Definition:Adjacent,Adjacent,"Each vertex of a polygon is formed by the intersection of two sides.

The two sides that form a particular vertex are referred to as the adjacents of that vertex, or described as adjacent to that vertex.",Definition:Polygon/Adjacent/Side to Vertex,['Definitions/Adjacent (Polygons)']
Definition:Adjacent,Adjacent,"Each side of a polygon intersects two other sides, and so is terminated at either endpoint by two vertices.

The two vertices that terminate a particular side are referred to as the adjacents of that side, or described as adjacent to that side.",Definition:Polygon/Adjacent/Vertex to Side,['Definitions/Adjacent (Polygons)']
Definition:Adjacent,Adjacent,Two sides of a polygon that meet at the same vertex are adjacent to each other.,Definition:Polygon/Adjacent/Sides,['Definitions/Adjacent (Polygons)']
Definition:Adjacent,Adjacent,"Each side of a polygon intersects two other sides, and so is terminated at either endpoint by two vertices.


Those two vertices are described as adjacent to each other.",Definition:Polygon/Adjacent/Vertices,['Definitions/Adjacent (Polygons)']
Definition:Adjacent,Adjacent,"=== Adjacent Face to Vertex ===
Each vertex of a polyhedron is formed by the intersection of a number of faces.

The faces that form a particular vertex are referred to as the adjacents of that vertex, or described as adjacent to that vertex.

=== Adjacent Face to Edge ===
Each edge of a polyhedron is formed by the intersection of two faces.

The faces that form a particular edge are referred to as the adjacents of that edge, or described as adjacent to that edge.

=== Adjacent Faces ===
Two faces of a polyhedron that meet at the same vertex are adjacent to each other.

=== Adjacent Edge to Face ===
The edges that intersect at a particular face are referred to as the adjacents of that face, or described as adjacent to that face.

=== Adjacent Edge to Edge ===
Two edges of a polyhedron that intersect at a particular vertex are referred to as adjacent to each other.

=== Adjacent Vertex to Face ===
The vertices that intersect a particular face are referred to as the adjacents of that face, or described as adjacent to that face.",Definition:Polyhedron/Adjacent,"['Definitions/Adjacent (Polyhedra)', 'Definitions/Polyhedra']"
Definition:Adjacent,Adjacent,"Each vertex of a polyhedron is formed by the intersection of a number of faces.

The faces that form a particular vertex are referred to as the adjacents of that vertex, or described as adjacent to that vertex.",Definition:Polyhedron/Adjacent/Face to Vertex,['Definitions/Adjacent (Polyhedra)']
Definition:Adjacent,Adjacent,"The vertices that intersect a particular face are referred to as the adjacents of that face, or described as adjacent to that face.",Definition:Polyhedron/Adjacent/Vertex to Face,['Definitions/Adjacent (Polyhedra)']
Definition:Adjacent,Adjacent,"Each edge of a polyhedron is formed by the intersection of two faces.

The faces that form a particular edge are referred to as the adjacents of that edge, or described as adjacent to that edge.",Definition:Polyhedron/Adjacent/Face to Edge,['Definitions/Adjacent (Polyhedra)']
Definition:Adjacent,Adjacent,"The edges that intersect at a particular face are referred to as the adjacents of that face, or described as adjacent to that face.",Definition:Polyhedron/Adjacent/Edge to Face,['Definitions/Adjacent (Polyhedra)']
Definition:Adjacent,Adjacent,Two faces of a polyhedron that meet at the same vertex are adjacent to each other.,Definition:Polyhedron/Adjacent/Faces,['Definitions/Adjacent (Polyhedra)']
Definition:Adjacent,Adjacent,"The two sides of a triangle which form a particular vertex are referred to as adjacent to that angle.

Similarly, the two vertices of a triangle to which a particular side contributes are referred to as adjacent to that side.


Category:Definitions/Triangles",Definition:Triangle (Geometry)/Adjacent,['Definitions/Triangles']
Definition:Adjacent,Adjacent,":


In a right-angled triangle, for a given non-right angled vertex, the adjacent side which is not the hypotenuse is referred to as the adjacent.

In the above figure:
: the adjacent to vertex $A$ is side $c$
: the adjacent to vertex $C$ is side $a$.",Definition:Triangle (Geometry)/Right-Angled/Adjacent,['Definitions/Right Triangles']
Definition:Adjacent Faces,Adjacent Faces,":

Let $G = \struct {V, E}$ be a planar graph.

Two faces of $G$ are adjacent  if and only if  they are both incident to the same edge (or edges).

In the above diagram, $BCEF$ and $ABF$ are adjacent, but $BCEF$ and $AFG$ are not adjacent.


Note that faces which are both incident to the same vertex are not considered adjacent unless they are also both incident to the same edge.",Definition:Adjacent (Graph Theory)/Faces,"['Definitions/Adjacency (Graph Theory)', 'Definitions/Faces of Graphs']"
Definition:Adjacent Faces,Adjacent Faces,Two faces of a polyhedron that meet at the same vertex are adjacent to each other.,Definition:Polyhedron/Adjacent/Faces,['Definitions/Adjacent (Polyhedra)']
Definition:Adjoint,Adjoint,"Let $\HH$ and $\KK$ be Hilbert spaces.

Let $\map \BB {\HH, \KK}$ be the set of bounded linear transformations from $\HH$ to $\KK$.

Let $A \in \map \BB {\HH, \KK}$ be a bounded linear transformation.


By Existence and Uniqueness of Adjoint, there exists a unique bounded linear transformation $A^* \in \map \BB {\KK, \HH}$ such that:

:$\forall h \in \HH, k \in \KK: {\innerprod {\map A h} k}_\KK = {\innerprod h {\map {A^*} k} }_\HH$

where $\innerprod \cdot \cdot_\HH$ and $\innerprod \cdot \cdot_\KK$ are inner products on $\HH$ and $\KK$ respectively.


$A^*$ is called the adjoint of $A$.


The operation of assigning $A^*$ to $A$ may be referred to as adjoining.",Definition:Adjoint Linear Transformation,"['Definitions/Adjoints', 'Definitions/Linear Transformations on Hilbert Spaces']"
Definition:Adjoint,Adjoint,"Let $\struct {S, \preceq}$ and $\struct {T, \precsim}$ be ordered sets.

Let $g: S \to T$, $d: T \to S$ be mappings.

Let $\tuple {g, d}$ be a Galois connection.


Then:

:$g$ is called the upper adjoint of the Galois connection.",Definition:Galois Connection/Upper Adjoint,['Definitions/Galois Connections']
Definition:Adjoint,Adjoint,"Let $\struct {S, \preceq}$ and $\struct {T, \precsim}$ be ordered sets.

Let $g: S \to T$, $d: T \to S$ be mappings.

Let $\tuple {g, d}$ be a Galois connection.


Then:

:$d$ is called the lower adjoint of the Galois connection.",Definition:Galois Connection/Lower Adjoint,['Definitions/Galois Connections']
Definition:Adjoint,Adjoint," 
 ",Definition:Adjoint Functor,['Definitions/Category Theory']
Definition:Adjoint,Adjoint,"Let $\mathbf C$, $\mathbf D$ be locally small categories.

Let $F : \mathbf D \to \mathbf C$ and $G : \mathbf C \to \mathbf D$ be functors.

$F$ is a left adjoint functor of $G$  if and only if  there exists an adjunction $\struct {F, G, \alpha}$.",Definition:Left Adjoint Functor,['Definitions/Category Theory']
Definition:Adjoint,Adjoint,"Let $\mathbf C$, $\mathbf D$ be locally small categories.

Let $F : \mathbf D \to \mathbf C$ and $G : \mathbf C \to \mathbf D$ be functors.

$G$ is a right adjoint functor of $F$  if and only if  there exists an adjunction $\struct {F, G, \alpha}$.",Definition:Right Adjoint Functor,['Definitions/Category Theory']
Definition:Affine,Affine,"Affine geometry is the study of the geometry of affine spaces.

Hence it is the study of properties and types of geometric figures which are invariant under an affine transformation.

It provides a modern axiomatic approach to the study of configurations of lines, planes and hypersurfaces.

In particular an affine space can be thought of as a finite dimensional vector space with no distinguished origin, and its affine transformations are those that preserve collinearity.",Definition:Affine Geometry,"['Definitions/Affine Geometry', 'Definitions/Geometry', 'Definitions/Branches of Mathematics']"
Definition:Affine,Affine,"Let $K$ be a field.

Let $A = K \sqbrk {X_1, \ldots, X_n}$ be the ring of polynomial functions in $n$ variables over $K$.


Then a subset $X \subseteq K^n$ is an affine algebraic set  if and only if  it is the zero locus of some set $T \subseteq A$.",Definition:Affine Algebraic Set,"['Definitions/Algebraic Geometry', 'Definitions/Affine Geometry']"
Definition:Affine,Affine,"Let $k$ be a field.

Let $n \ge 1$ be an Integer.

Then a subset $X \subseteq k^n$ is an affine algebraic variety if the following hold:

:$(1): \quad X$ is an affine algebraic set 

:$(2): \quad X$ is irreducible   the Zariski topology.",Definition:Affine Algebraic Variety,['Definitions/Algebraic Geometry']
Definition:Affine,Affine,An affine scheme is a ringed space which is isomorphic to the spectrum of a commutative ring with unity.,Definition:Affine Scheme,['Definitions/Schemes']
Definition:Affine,Affine,"Let $\EE$ be an affine space with difference space $V$.

=== Definition 1 ===
Let $\EE$ be an affine space with difference space $V$.


An affine frame in $\EE$ is an ordered tuple $\tuple {p_0, e_1, \ldots, e_n}$, where:
:$p_0$ is an element of $\EE$ called the origin
:$\tuple {e_1, \ldots, e_n}$ is an ordered basis for $V$.

=== Definition 2 ===
Let $\EE$ be an affine space with difference space $V$.


An affine frame may be given by the set of $n + 1$ points:
:$\tuple {q_0, \ldots, q_n} = \tuple {p_0, p_0 + e_1, \ldots, p_0 + e_n}$

The frame $\tuple {p_0, e_1, \ldots, e_n}$ is then recovered by:
:$\tuple {q_0, q_1 - q_0, \ldots, q_n - q_0}$

Category:Definitions/Affine Geometry",Definition:Affine Frame,['Definitions/Affine Geometry']
Definition:Affine,Affine,"An affine monoid is a monoid that is:
: finitely generated
and:
: isomorphic to a submonoid of a free abelian group $\Z^d$, for some $d \in \Z_{\ge 0}$.",Definition:Affine Monoid,['Definitions/Monoids']
Definition:Affine,Affine,"=== Associativity Axioms ===
Let $K$ be a field.

Let $\struct {V, +_V, \circ}$ be a vector space over $K$.

Let $\EE$ be a set on which two mappings are defined:

:$+ : \EE \times V \to \EE$
:$- : \EE \times \EE \to V$

satisfying the following associativity conditions:

 
 
 
 
 


Then the ordered triple $\struct {\EE, +, -}$ is an affine space.

=== Group Action ===
Let $K$ be a field.

Let $\struct {V, +_V, \circ}$ be a vector space over $K$.

Let $\EE$ be a set.

Let $\phi: \EE \times V \to \EE$ be a free and transitive group action of $\struct {V, +_V}$ on $\EE$.


Then the ordered pair $\struct {\EE, \phi}$ is an affine space.

=== Weyl's Axioms ===
Let $K$ be a field.

Let $\struct{V, +_V, \circ}$ be a vector space over $K$.

Let $\EE$ be a set on which a mapping is defined:

:$- : \EE \times \EE \to V$

satisfying the following associativity conditions:

 
 
 
 


Then the ordered pair $\tuple {\EE, -}$ is an affine space.

=== Addition ===
Let $\tuple {\EE, +, -}$ be an affine space.


Then the mapping $+$ is called affine addition.

=== Subtraction ===
Let $\tuple {\EE, +, -}$ be an affine space.


Then the mapping $-$ is called affine subtraction.

=== Tangent Space ===
Let $\tuple {\EE, +, -}$ be an affine space.

Let $V$ be the vector space that is the codomain of $-$.


Then $V$ is the tangent space of $\EE$.

=== Vector ===
Let $\EE$ be an affine space.

Let $V$ be the tangent space of $\EE$.


An element $v$ of $V$ is called a vector.

=== Point ===
Let $\EE$ be an affine space.


Any element $p$ of $\EE$ is called a point.",Definition:Affine Space,['Definitions/Affine Geometry']
Definition:Affine,Affine,"Let $\EE$ be an affine space with tangent space $E$.

Let $\FF \subseteq \EE$ be a subset of $\EE$.


Then $\FF$ is an affine subspace of $\EE$  if and only if  there exists a point $p \in \EE$ such that:

:$F_p := \set {q - p: q \in \FF}$

is a vector subspace of the vector space $E$.",Definition:Affine Subspace,['Definitions/Affine Geometry']
Definition:Affine,Affine,"Let $\EE$ and $\FF$ be affine spaces with difference spaces $E$ and $F$ respectively.

Let $\LL: \EE \to \FF$ be a mapping.

=== Definition 1 ===
Let $\EE$ and $\FF$ be affine spaces with difference spaces $E$ and $F$ respectively.

Let $\LL: \EE \to \FF$ be a mapping.


$\LL$ is an affine transformation  if and only if  there exists a linear transformation $L: E \to F$ such that for every pair of points $p, q \in \EE$:
:$\map \LL q = \map \LL p + \map L {\vec {p q} }$

=== Definition 2 ===
Let $\EE$ and $\FF$ be affine spaces with difference spaces $E$ and $F$ respectively.

Let $\LL: \EE \to \FF$ be a mapping.


$\LL$ is an affine transformation  if and only if :
:$\forall v_1, v_2 \in \EE: \map \LL {s v_1 + t v_2} = s \map \LL {v_1} + t \map \LL {v_2}$
for some $s, t \in \R$ such that $s + t = 1$.",Definition:Affine Transformation,"['Definitions/Affine Transformations', 'Definitions/Affine Geometry']"
Definition:Age,Age,"The age of a physical object is defined as the period of time over which it has been in existence.


Category:Definitions/Applied Mathematics",Definition:Age (Time),['Definitions/Applied Mathematics']
Definition:Age,Age,"Let $\MM$ be an $\LL$-structure.


An age of $\MM$ is a class $K$ of $\LL$-structures such that:
* if $\AA$ is a finitely generated $\LL$-structure such that there is an $\LL$-embedding $\AA \to \MM$, then $\AA$ is isomorphic to some structure in $K$,
* no two structures in $K$ are isomorphic, and
* $K$ does not contain any structures which are not finitely generated or do not embed into $\MM$.

That is, $K$ is an age of $\MM$  if and only if  it contains exactly one representative from each isomorphism type of the finitely-generated structures that embed into $\MM$.

 

Category:Definitions/Model Theory for Predicate Logic",Definition:Age (Model Theory),['Definitions/Model Theory for Predicate Logic']
Definition:Aggregation,Aggregation,"Parenthesis is a syntactical technique to disambiguate the meaning of a logical formula.

It allows one to specify that a logical formula should (temporarily) be regarded as being a single entity, being on the same level as a statement variable.

Such a formula is referred to as being in parenthesis.

Typically, a formal language, in defining its formal grammar, ensures by means of parenthesis that all of its well-formed words are uniquely readable.


Generally, brackets are used to indicate that certain formulas are in parenthesis.

The brackets that are mostly used are round ones, the left (round) bracket $($ and the right (round) bracket $)$.",Definition:Parenthesis,"['Definitions/Parenthesis', 'Definitions/Symbolic Logic', 'Definitions/Algebra', 'Definitions/Arithmetic']"
Definition:Aggregation,Aggregation,"A set is intuitively defined as any aggregation of objects, called elements, which can be precisely defined in some way or other.

We can think of each set as a single entity in itself, and we can denote it (and usually do) by means of a single symbol.


That is, anything you care to think of can be a set. This concept is known as the  .


However, there are problems with the  . If we allow it to be used without any restrictions at all, paradoxes arise, the most famous example probably being Russell's Paradox.


Hence some sources define a set as a  'well-defined' collection of objects, leaving the concept of what constitutes well-definition to later in the exposition.",Definition:Set,"['Definitions/Set Theory', 'Definitions/Sets']"
Definition:Aggregation,Aggregation,"An aggregation, in the context of physics, is a set of bodies (but usually particles) all of which are under the same or similar conditions, and which are assumed to behave (in certain aspects) as one body.

The concept is deliberately left vague.",Definition:Aggregation (Physics),"['Definitions/Aggregations (Physics)', 'Definitions/Physics', 'Definitions/Applied Mathematics']"
Definition:Algebra,Algebra,Algebra is the branch of mathematics which studies the techniques of manipulation of objects and expressions.,Definition:Algebra (Mathematical Branch),['Definitions/Branches of Mathematics']
Definition:Algebra,Algebra,"Abstract algebra is a branch of mathematics which studies algebraic structures and algebraic systems.

It can be roughly described as the study of sets equipped with operations.",Definition:Abstract Algebra,"['Definitions/Branches of Mathematics', 'Definitions/Algebra', 'Definitions/Abstract Algebra']"
Definition:Algebra,Algebra,"An algebraic structure with $n$ operations is an ordered tuple:
:$\struct {S, \circ_1, \circ_2, \ldots, \circ_n}$
where:
:$S$ is a set
:$\circ_1, \circ_2, \ldots, \circ_n$ are $n$ binary operations which are defined on all the elements of $S \times S$.


=== One Operation ===
An algebraic structure with $1$ operation is an ordered pair:
:$\struct {S, \circ}$
where:
:$S$ is a set
:$\circ$ is a binary operation defined on all the elements of $S \times S$.

=== Two Operations ===
An algebraic structure with $2$ operations is an ordered triple:
:$\struct {S, \circ, *}$
where:
:$S$ is a set
:$\circ$ and $*$ are binary operations defined on all the elements of $S \times S$.",Definition:Algebraic Structure,"['Definitions/Abstract Algebra', 'Definitions/Algebraic Structures']"
Definition:Algebra,Algebra,Linear algebra is the branch of algebra which studies vector spaces and linear transformations between them.,Definition:Linear Algebra (Mathematical Branch),"['Definitions/Linear Algebra', 'Definitions/Algebra', 'Definitions/Linearity', 'Definitions/Branches of Mathematics']"
Definition:Algebra,Algebra,Vector Algebra is the branch of mathematics which studies the algebra of vector spaces.,Definition:Vector Algebra,"['Definitions/Branches of Mathematics', 'Definitions/Linear Algebra', 'Definitions/Vector Algebra']"
Definition:Algebra,Algebra,"An algebra loop $\struct {S, \circ}$ is a quasigroup with an identity element.
:$\exists e \in S: \forall x \in S: x \circ e = x = e \circ x$",Definition:Algebra Loop,['Definitions/Abstract Algebra']
Definition:Algebra,Algebra,"=== Definition 1 ===

Let $S$ be a set.

Let $\powerset S$ be the power set of $S$.

Let $\RR \subseteq \powerset S$ be a set of subsets of $S$.


$\RR$ is an algebra of sets over $S$  if and only if  $\RR$ satisfies the algebra of sets axioms:
 

=== Definition 2 ===
An algebra of sets is a ring of sets with a unit.",Definition:Algebra of Sets,"['Definitions/Set Systems', 'Definitions/Algebras of Sets', 'Definitions/Rings of Sets']"
Definition:Algebra,Algebra,"=== Definition 1 ===
Let $X$ be a set.

Let $\Sigma$ be a system of subsets of $X$.


$\Sigma$ is a $\sigma$-algebra over $X$  if and only if  $\Sigma$ satisfies the sigma-algebra axioms:
 

=== Definition 2 ===
Let $X$ be a set.

Let $\Sigma$ be a system of subsets of $X$.


$\Sigma$ is a $\sigma$-algebra over $X$  if and only if  $\Sigma$ satisfies the sigma-algebra axioms:
 

=== Definition 3 ===
A $\sigma$-algebra $\Sigma$ is a $\sigma$-ring with a unit.

=== Definition 4 ===
Let $X$ be a set.

A $\sigma$-algebra $\Sigma$ over $X$ is an algebra of sets which is closed under countable unions.",Definition:Sigma-Algebra,"['Definitions/Sigma-Algebras', 'Definitions/Set Systems', 'Definitions/Sigma-Rings', 'Definitions/Algebras of Sets', 'Definitions/Measure Theory']"
Definition:Algebra,Algebra,"=== Topological Space ===
Let $\struct {S, \tau}$ be a topological space

The Borel sigma-algebra $\map \BB {S, \tau}$ of $\struct {S, \tau}$ is the $\sigma$-algebra generated by $\tau$.

That is, it is the $\sigma$-algebra generated by the set of open sets in $S$.


=== Borel Set ===
Let $\struct {S, \tau}$ be a topological space.

Let $\map \BB {S, \tau}$ be the Borel $\sigma$-algebra of $\struct {S, \tau}$.


The elements of $\map \BB {S, \tau}$ are called the Borel (measurable) sets of $\struct {S, \tau}$.

=== Metric Space ===
Let $\struct {S, d}$ be a metric space.

The Borel sigma-algebra (or $\sigma$-algebra) on $\struct {S, d}$ is the $\sigma$-algebra generated by the open sets in $\powerset S$.

By the definition of a topology induced by a metric, this definition is a particular instance of the definition of a Borel $\sigma$-algebra on a topological space.


=== Borel Set ===
Let $\struct {S, \tau}$ be a topological space.

Let $\map \BB {S, \tau}$ be the Borel $\sigma$-algebra of $\struct {S, \tau}$.


The elements of $\map \BB {S, \tau}$ are called the Borel (measurable) sets of $\struct {S, \tau}$.

=== Borel Set ===
Let $\struct {S, \tau}$ be a topological space.

Let $\map \BB {S, \tau}$ be the Borel $\sigma$-algebra of $\struct {S, \tau}$.


The elements of $\map \BB {S, \tau}$ are called the Borel (measurable) sets of $\struct {S, \tau}$.",Definition:Borel Sigma-Algebra,"['Definitions/Topology', 'Definitions/Sigma-Algebras', 'Definitions/Measure Theory', 'Definitions/Borel Sigma-Algebras']"
Definition:Algebra,Algebra,"Let $\struct {X, \circ}$ be an algebraic structure.


$\struct {X, \circ}$ is a $B$-algebra  if and only if  $\struct {X, \circ}$ satisfies the $B$-algebra axioms:
 ",Definition:B-Algebra,"['Definitions/Algebraic Structures', 'Definitions/B-Algebras']"
Definition:Algebra,Algebra,"In the context of abstract algebra, in particular ring theory and linear algebra, the following varieties of algebra exist:

* Definition:Boolean Algebra

* Definition:Algebra over Ring: an $R$-module $G_R$ over a commutative ring $R$ with a bilinear mapping $\oplus: G^2 \to G$.

* Definition:Algebra over Field: a vector space $G_F$ over a field $F$ with a bilinear mapping $\oplus: G^2 \to G$.

* Definition:Real Algebra: an algebra over a field where the field in question is the field of real numbers $\R$.

* Definition:Division Algebra: an algebra over a field $\struct {A_F, \oplus}$ such that $\forall a, b \in A_F, b \ne \mathbf 0_A: \exists_1 x \in A_F, y \in A_F: a = b \oplus x, a = y \oplus b$.

* Definition:Associative Algebra: an algebra over a ring in which the bilinear mapping $\oplus$ is associative. 

* Definition:Unitary Algebra, also known as a Unital Algebra: an algebra over a ring $\struct {A_R, \oplus}$ in which there exists an identity element, that is, a unit, usually denoted $1$, for $\oplus$.

* Definition:Unitary Division Algebra: a division algebra $\struct {A_F, \oplus}$ in which there exists an identity element, that is, a unit, usually denoted $1$, for $\oplus$.

* Definition:Graded Algebra: an algebra over a ring where the ring has a gradation, that is, is a graded ring.

* Definition:Filtered Algebra: an algebra over a field which has a sequence of subalgebras which constitute a gradation.

* Definition:Quadratic Algebra: a filtered algebra whose generator consists of degree one elements, with defining relations of degree 2.

* Definition:Lie Algebra",Definition:Algebra (Abstract Algebra),['Definitions/Abstract Algebra']
Definition:Algebraic,Algebraic,Algebraic topology is a branch of topology which uses tools from abstract algebra to study topological spaces.,Definition:Algebraic Topology,"['Definitions/Branches of Mathematics', 'Definitions/Abstract Algebra', 'Definitions/Algebraic Topology', 'Definitions/Topology']"
Definition:Algebraic,Algebraic,"Algebraic geometry is the branch of geometry which studies objects in multi-dimensional space using the techniques of abstract algebra.

In particular, techniques from commutative algebra are mainly used.

It also encompasses the study of algebraic varieties.",Definition:Algebraic Geometry,"['Definitions/Algebraic Geometry', 'Definitions/Geometry', 'Definitions/Branches of Mathematics']"
Definition:Algebraic,Algebraic,"An algebraic variety is the solution set of a system of simultaneous polynomial equations:

 
 
 
 
 
 ",Definition:Algebraic Variety,"['Definitions/Algebraic Varieties', 'Definitions/Algebraic Geometry']"
Definition:Algebraic,Algebraic,"Let $L / K$ be a field extension.

Let $A \subseteq L$ be a subset of $L$.

Let $\map K {\set {X_\alpha: \alpha \in A} }$ be the field of rational functions in the indeterminates $\family {X_\alpha}_{\alpha \mathop \in A}$.


Then $A$ is algebraically independent over $K$  if and only if  there exists a homomorphism:
:$\phi: \map K {\set {X_\alpha: \alpha \in A} } \to L$
such that, for all $\alpha \in A$:
:$\map \phi {X_\alpha} = \alpha$",Definition:Algebraically Independent,['Definitions/Field Extensions']
Definition:Algebraic,Algebraic,"Algebraic number theory is the branch of abstract algebra which studies structures in which the usual number fields are embedded.

As such it can also be considered to be a branch of number theory.",Definition:Algebraic Number Theory,"['Definitions/Branches of Mathematics', 'Definitions/Algebraic Number Theory', 'Definitions/Number Theory']"
Definition:Algebraic,Algebraic,"Let $K / \Q$ be an algebraic number field.

 


Then $\alpha \in K$ is an algebraic integer  if and only if  it satisfies a monic polynomial $f \in \Z \sqbrk X$.

The set of all algebraic integers in $K$ is denoted $\OO_K$.


By Ring of Algebraic Integers it is a ring, hence usually referred to as the ring of algebraic integers of $K$.


=== Quadratic Integer ===
Let $K / \Q$ be an algebraic number field.

Let $K / \Q$ have degree two.


Then an algebraic integer in $K$ is a quadratic integer.",Definition:Algebraic Integer,['Definitions/Algebraic Number Theory']
Definition:Algebraic,Algebraic,"An algebraic number is an algebraic element of the field extension $\C / \Q$.

That is, it is a complex number that is a root of a polynomial with rational coefficients.


The set of algebraic numbers can often be seen denoted as $\mathbb A$.


=== Degree ===
Let $\alpha$ be an algebraic number.

By definition, $\alpha$ is the root of at least one polynomial $P_n$ with rational coefficients.


The degree of $\alpha$ is the degree of the minimal polynomial $P_n$ whose coefficients are all in $\Q$.


=== Algebraic Number over Field ===

Sources which define an algebraic number over a more general field define degree in the following terms:

Let $F$ be a field.

Let $z \in \C$ be algebraic over $F$.


The degree of $\alpha$ is the degree of the minimal polynomial $\map m x$ whose coefficients are all in $F$.",Definition:Algebraic Number,"['Definitions/Numbers', 'Definitions/Algebraic Numbers']"
Definition:Algebraic,Algebraic,"Let $E / F$ be a field extension.

Let $\alpha \in E$.


=== Definition 1 ===
Let $E / F$ be a field extension.

Let $\alpha \in E$.


$\alpha$ is algebraic over $F$  if and only if  it is a root of some nonzero polynomial over $F$:
:$\exists f \in F \sqbrk X \setminus \set 0: \map f \alpha = 0$
where $F \sqbrk X$ denotes the ring of polynomial forms in $X$.

=== Definition 2 ===
Let $E / F$ be a field extension.

Let $\alpha \in E$.


$\alpha$ is algebraic over $F$  if and only if  the evaluation homomorphism $F \sqbrk X \to E$ at $\alpha$ is not injective.",Definition:Algebraic Element of Field Extension,"['Definitions/Field Extensions', 'Definitions/Polynomial Theory']"
Definition:Algebraic,Algebraic,"A field extension $E / F$ is said to be algebraic  if and only if :
: $\forall \alpha \in E: \alpha$ is algebraic over $F$",Definition:Algebraic Field Extension,"['Definitions/Field Extensions', 'Definitions/Polynomial Theory']"
Definition:Algebraic,Algebraic,"Let $K$ be a field.


An algebraic closure of $K$ is an algebraically closed algebraic field extension of $K$.


An algebraic closure of $K$ can be denoted $\overline K$.",Definition:Algebraic Closure,['Definitions/Field Theory']
Definition:Algebraic,Algebraic,"Let $K$ be a field.


Then $K$ is algebraically closed  if and only if :


=== Definition 1 ===
Let $K$ be a field.


$K$ is algebraically closed  if and only if :

The only algebraic field extension of $K$ is $K$ itself.

=== Definition 2 ===
Let $K$ be a field.


$K$ is algebraically closed  if and only if :

Every irreducible polynomial $f$ over $K$ has degree $1$.

=== Definition 3 ===
Let $K$ be a field.


$K$ is algebraically closed  if and only if :

Every polynomial $f$ over $K$ of strictly positive degree has a root in $K$.",Definition:Algebraically Closed Field,['Definitions/Field Extensions']
Definition:Algebraic,Algebraic,"=== Real Algebraic Function ===
Let $y$ be a solution to the polynomial equation:

:$\map {p_0} x + \map {p_1} x y + \dotsb + \map {p_{n - 1} } x y^{n - 1} + \map {p_n} x y^n = 0$

where $\map {p_0} x \ne 0, \map {p_1} x, \dotsc, \map {p_n} x$ are real polynomial functions in $x$.


Then $y = \map f x$ is a (real) algebraic function:

=== Complex Algebraic Function ===
Let $w$ be a solution to the polynomial equation:

:$\map {p_0} z + \map {p_1} z w + \dotsb + \map {p_{n - 1} } z w^{n - 1} + \map {p_n} z w^n = 0$

where $\map {p_0} z \ne 0, \map {p_1} z, \dotsc, \map {p_n} z$ are complex polynomial functions in $z$.


Then $w = \map f z$ is a (complex) algebraic function:",Definition:Algebraic Function,"['Definitions/Analysis', 'Definitions/Algebraic Functions']"
Definition:Algebraic,Algebraic,An algebraic number field is a finite extension of the field of rational numbers $\Q$.,Definition:Algebraic Number Field,"['Definitions/Algebraic Number Theory', 'Definitions/Number Fields']"
Definition:Algebraic,Algebraic,"An algebraic structure with $n$ operations is an ordered tuple:
:$\struct {S, \circ_1, \circ_2, \ldots, \circ_n}$
where:
:$S$ is a set
:$\circ_1, \circ_2, \ldots, \circ_n$ are $n$ binary operations which are defined on all the elements of $S \times S$.


=== One Operation ===
An algebraic structure with $1$ operation is an ordered pair:
:$\struct {S, \circ}$
where:
:$S$ is a set
:$\circ$ is a binary operation defined on all the elements of $S \times S$.

=== Two Operations ===
An algebraic structure with $2$ operations is an ordered triple:
:$\struct {S, \circ, *}$
where:
:$S$ is a set
:$\circ$ and $*$ are binary operations defined on all the elements of $S \times S$.",Definition:Algebraic Structure,"['Definitions/Abstract Algebra', 'Definitions/Algebraic Structures']"
Definition:Algebraic,Algebraic,"An algebraic system is a mathematical system $\SS = \struct {E, O}$ where:

:$E$ is a non-empty set of elements

:$O$ is a set of finitary operations on $E$.",Definition:Algebraic System,['Definitions/Abstract Algebra']
Definition:Algebraic,Algebraic,"Let $\struct {S, \preceq}$ be an ordered set.


Then $\struct {S, \preceq}$ is algebraic  if and only if 
:(for all elements $x$ of $S$: $x^{\mathrm{compact} }$ is directed)
and:
:$\struct {S, \preceq}$ is up-complete and satisfies the axiom of $K$-approximation:
where $x^{\mathrm{compact} }$ denotes the compact closure of $x$.",Definition:Algebraic Ordered Set,['Definitions/Order Theory']
Definition:Alphabet,Alphabet,"Let $\LL$ be a formal language.


The alphabet $\AA$ of $\LL$ is a set of symbols from which collations in $\LL$ may be constructed.

An alphabet consists of the following parts:

:The letters
:The signs.

Depending on the specific nature of any particular formal language, these too may be subcategorized.


 

 


 ",Definition:Formal Language/Alphabet,"['Definitions/Alphabets (Formal Language)', 'Definitions/Formal Languages']"
Definition:Alphabet,Alphabet,"The alphabet of a natural language $\LL$ is the set of symbols, called letters, which are used to represent the sounds of $\LL$.


=== English ===
The English alphabet consists of the following letters:

:$\texttt {A B C D E F G H I J K L M N O P Q R S T U V W X Y Z}$
:$\texttt {a b c d e f g h i j k l m n o p q r s t u v w x y z}$",Definition:Alphabet of Natural Language,"['Definitions/Natural Language', 'Definitions/Language Definitions']"
Definition:Alternant,Alternant,"Let $p \lor q$ be a compound statement whose main connective is the disjunction:
:$p \lor q$  if and only if  $p$ is true or $q$ is true or both are true.


The substatements $p$ and $q$ are known as the disjuncts.",Definition:Disjunction/Disjunct,['Definitions/Disjunction']
Definition:Alternant,Alternant,"An alternant is a determinant of order $n$ such that the element in the $i$th row and $j$th column is defined as:
:$\map {f_i} {r_j}$
where:
:the $f_i$ are $n$ mappings
:the $r_j$ are $n$ elements.",Definition:Alternant (Linear Algebra),"['Definitions/Alternants', 'Definitions/Linear Algebra']"
Definition:Alternative,Alternative,"Let $p \lor q$ be a compound statement whose main connective is the disjunction:
:$p \lor q$  if and only if  $p$ is true or $q$ is true or both are true.


The substatements $p$ and $q$ are known as the disjuncts.",Definition:Disjunction/Disjunct,['Definitions/Disjunction']
Definition:Alternative,Alternative,"Let $\circ$ be a binary operation.


Then $\circ$ is defined as being alternative on $S$  if and only if :

:$\forall T := \set {x, y} \subseteq S: \forall x, y, z \in T: \paren {x \circ y} \circ z = x \circ \paren {y \circ z}$

That is, $\circ$ is associative over any two elements of $S$.


For example, for any $x, y \in S$:
:$\paren {x \circ y} \circ x = x \circ \paren {y \circ x}$
:$\paren {x \circ x} \circ y = x \circ \paren {x \circ y}$
and so on.",Definition:Alternative Operation,['Definitions/Abstract Algebra']
Definition:Altitude,Altitude,An altitude of a polygon is the longest perpendicular from the base to a vertex most distant from the base.,Definition:Altitude of Polygon,"['Definitions/Altitudes (Geometry)', 'Definitions/Polygons']"
Definition:Altitude,Altitude,"Let $\triangle ABC$ be a triangle.

Let a perpendicular be dropped from $\angle A$ to its opposite side $a$ or its production:

:

The line $h_a$ so constructed is called the altitude of $\angle A$.


=== Foot of Altitude ===
Let $\triangle ABC$ be a triangle.

Let $h_a$ be the altitude of $A$:

:


The point at which $h_a$ meets $BC$ (or its production) is the foot of the altitude $h_a$.


Category:Definitions/Triangles",Definition:Altitude of Triangle,"['Definitions/Altitudes (Geometry)', 'Definitions/Triangles']"
Definition:Altitude,Altitude,":

An altitude of a parallelogram is a line drawn perpendicular to its base, through one of its vertices to the side opposite to the base (which is extended if necessary).

In the parallelogram above, line $DE$ is an altitude of the parallelogram $ABCD$.


The term is also used for the length of such a line.",Definition:Quadrilateral/Parallelogram/Altitude,['Definitions/Parallelograms']
Definition:Altitude,Altitude,"The altitude of a geometric figure $\FF$ is the length of a line segment giving the height of $\FF$.


=== Altitude of Polygon ===
An altitude of a polygon is the longest perpendicular from the base to a vertex most distant from the base.

=== Altitude of Polyhedron ===
An altitude of a polyhedron is the longest perpendicular from the base to a vertex most distant from the base.

=== Altitude of Cone ===
:

Let a perpendicular $AE$ be dropped from the apex of a cone to the plane containing its base.

The line segment $AE$ is an altitude of the cone.

=== Altitude of Cylinder ===

:


:

An altitude of a cylinder is a line segment drawn perpendicular to the base and its opposite plane.


In the above diagram, $HJ$ is an altitude of the cylinder $ACBDFE$.

=== Altitude of Prism ===

:
:

An altitude of a prism is a line which is perpendicular to the bases of the prism.

In the above diagram, a line of length $h$ is an altitude of the prism $AJ$.

=== Altitude of Pyramid ===

:
:

An altitude of a pyramid is a straight line perpendicular to the plane of the base to its apex.

In the above diagram, an altitude is a straight line length is $h$.",Definition:Altitude of Geometric Figure,"['Definitions/Altitudes (Geometry)', 'Definitions/Geometric Figures']"
Definition:Altitude,Altitude,An altitude of a polyhedron is the longest perpendicular from the base to a vertex most distant from the base.,Definition:Altitude of Polyhedron,"['Definitions/Altitudes (Geometry)', 'Definitions/Polygons']"
Definition:Altitude,Altitude,":

Let a perpendicular $AE$ be dropped from the apex of a cone to the plane containing its base.

The line segment $AE$ is an altitude of the cone.",Definition:Cone (Geometry)/Altitude,"['Definitions/Altitudes (Geometry)', 'Definitions/Cones']"
Definition:Altitude,Altitude,":

An altitude of a cylinder is a line segment drawn perpendicular to the base and its opposite plane.


In the above diagram, $HJ$ is an altitude of the cylinder $ACBDFE$.",Definition:Altitude of Cylinder,"['Definitions/Altitudes (Geometry)', 'Definitions/Cylinders']"
Definition:Altitude,Altitude,":

An altitude of a prism is a line which is perpendicular to the bases of the prism.

In the above diagram, a line of length $h$ is an altitude of the prism $AJ$.",Definition:Altitude of Prism,"['Definitions/Altitudes (Geometry)', 'Definitions/Prisms']"
Definition:Altitude,Altitude,":

An altitude of a pyramid is a straight line perpendicular to the plane of the base to its apex.

In the above diagram, an altitude is a straight line length is $h$.",Definition:Altitude of Pyramid,"['Definitions/Altitudes (Geometry)', 'Definitions/Pyramids']"
Definition:Altitude,Altitude,"Let $X$ be a point on the celestial sphere.

The (celestial) altitude of $X$ is defined as the angle subtended by the the arc of the vertical circle through $X$ between the celestial horizon and $X$ itself.


=== Symbol ===
",Definition:Celestial Altitude,"['Definitions/Celestial Altitude', 'Definitions/Celestial Sphere']"
Definition:Amplitude,Amplitude,"Let $f: \R \to \R$ be a periodic real function.


The amplitude of $f$ is the maximum absolute difference of the value of $f$ from a reference level.",Definition:Periodic Real Function/Amplitude,['Definitions/Periodic Functions']
Definition:Amplitude,Amplitude,"Let $u = \map F {k, \phi}$ denote the incomplete elliptic integral of the first kind.

The parameter $\phi$ of $u = \map F {k, \phi}$ is called the amplitude of $u$.


=== Symbol ===
",Definition:Incomplete Elliptic Integral of the First Kind/Amplitude,['Definitions/Incomplete Elliptic Integral of the First Kind']
Definition:Amplitude,Amplitude,"Consider a physical system $S$ in a state of simple harmonic motion:
:$x = A \map \sin {\omega t + \phi}$


The parameter $A$ is known as the amplitude of the motion.",Definition:Simple Harmonic Motion/Amplitude,['Definitions/Simple Harmonic Motion']
Definition:Annulus,Annulus,"An annulus is a plane figure whose boundary consists of $2$ concentric circles:

:

In the above diagram, the annulus is the colored area.


=== Center of Annulus ===
:

The center of an annulus is the common center of the $2$ concentric circles that form its boundary.

In the above diagram, the center is the point $O$.


Category:Definitions/Annuli

=== Inner Radius of Annulus ===
:

The inner radius of an annulus is the radius of the smaller of the $2$ concentric circles that form its boundary.

In the above diagram, the inner radius is denoted $r$.

=== Outer Radius of Annulus ===
:

The outer radius of an annulus is the radius of the larger of the $2$ concentric circles that form its boundary.

In the above diagram, the outer radius is denoted $R$.

=== Width of Annulus ===
:

The width of an annulus is the difference between its outer radius and the inner radius

In the above diagram, the width of the annulus is $R - r$.",Definition:Annulus (Geometry),"['Definitions/Annuli', 'Definitions/Concentric Circles', 'Definitions/Circles']"
Definition:Annulus,Annulus,"An annulus in the context of topology is a topological space which is homeomorphic with an annulus in the context of geometry.

:",Definition:Annulus (Topology),"['Definitions/Examples of Topological Spaces', 'Definitions/Annuli']"
Definition:Archimedean,Archimedean,"An Archimedean polyhedron is a convex polyhedron with the following properties:
:$(1): \quad$ Each of its faces is a regular polygon
:$(2): \quad$ It is isogonal
:$(3): \quad$ The faces are not all congruent.
:$(4): \quad$ It is not a regular prism or a regular antiprism.",Definition:Archimedean Polyhedron,"['Definitions/Convex Polyhedra', 'Definitions/Archimedean Polyhedra']"
Definition:Archimedean,Archimedean,"The Archimedean spiral is the locus of the equation expressed in polar coordinates as:
:$r = a \theta$


:


=== Archimedes' Definition ===
 ' definition of his Archimedean spiral is as follows:

:If a straight line of which one extremity remains fixed be made to revolve at a uniform rate in a plane until it returns to the position from which it started, and if, at the same time as the straight line revolves, a point moves at a uniform rate along the straight line, starting from the fixed extremity, the point will describe a spiral in the plane.",Definition:Archimedean Spiral,"['Definitions/Archimedean Spiral', 'Definitions/Spirals']"
Definition:Archimedean,Archimedean,"=== Non-Archimedean Norm (Vector Space) ===
Let $\struct {R, +, \circ}$ be a division ring with norm $\norm {\,\cdot\,}_R$.

Let $X$ be a vector space over $R$, with zero $0_X$.


=== Definition 1 ===
Let $X$ be a vector space.

A norm $\norm {\,\cdot\,} $ on $X$ is non-Archimedean  if and only if  $\norm {\, \cdot \,}$ satisfies the axiom:
 
 
 

=== Definition 2 ===
Let $\struct {R, +, \circ}$ be a division ring with norm $\norm {\,\cdot\,}_R$.

Let $X$ be a vector space over $R$, with zero $0_X$.


A non-Archimedean norm on $X$ is a mapping from $X$ to the non-negative reals:
:$\norm {\, \cdot \,}: X \to \R_{\ge 0}$
satisfying the non-Archimedean norm axioms:
 

The pair $\struct {X, \norm {\, \cdot \, } }$ is a non-Archimedean normed vector space.


=== Archimedean ===
A norm $\norm {\,\cdot\,} $ on a vector space $X$ is Archimedean  if and only if  it is not non-Archimedean.


That is,  if and only if :
:$\exists x, y \in X: \norm {x + y} > \max \set { {\norm x, \norm y} }$

Category:Definitions/Norm Theory

=== Non-Archimedean Norm (Division Ring) ===
Let $\struct {R, +, \circ}$ be a division ring whose zero is denoted $0_R$.


=== Definition 1 ===
Let $\struct {R, +, \circ}$ be a division ring whose zero is denoted $0_R$.


A norm $\norm {\, \cdot \,}$ on $R$ is non-Archimedean  if and only if  $\norm {\, \cdot \,}$ satisfies the axiom:
 
 
 

=== Definition 2 ===
Let $\struct {R, +, \circ}$ be a division ring whose zero is denoted $0_R$.


A non-Archimedean norm on $R$ is a mapping from $R$ to the non-negative reals:
:$\norm {\, \cdot \,}: R \to \R_{\ge 0}$
satisfying the non-Archimedean norm axioms:
 


The pair $\struct {R, \norm {\, \cdot \, } }$ is a non-Archimedean Normed Division Ring.


If $R$ is also a commutative ring, that is, $\struct {R, \norm {\,\cdot\,} }$ is a valued field, then $\struct {R, \norm {\,\cdot\,} }$ is a non-Archimedean Valued Field.


=== Archimedean ===
Let $\norm {\, \cdot \,}$ be a norm on a division ring $R$ satisfying the norm axioms:
: 


A norm $\norm {\, \cdot \,}$ is said to be Archimedean  if and only if  it does not satisfy the  .


That is, $\norm {\, \cdot \,}$ is Archimedean  if and only if :
 
 
 

=== Non-Archimedean Metric ===
A metric $d$ on a metric space $X$ is non-Archimedean  if and only if :
:$\map d {x, y} \le \max \set {\map d {x, z}, \map d {y, z} }$
for all $x, y, z \in X$.


=== Archimedean ===
A metric is Archimedean  if and only if  it is not non-Archimedean.


That is,  if and only if :
:$\exists x, y, z, \in X: \map d {x, y} > \max \set {\map d {x, z}, \map d {y, z} }$

Category:Definitions/Norm Theory

Category:Definitions/Norm Theory",Definition:Non-Archimedean,['Definitions/Norm Theory']
Definition:Archimedean,Archimedean,"Let $\struct  {S, \circ}$ be a closed algebraic structure on which there exists either an ordering or a norm.


Let $\cdot: \Z_{>0} \times S \to S$ be the operation defined as:
:$m \cdot a = \begin{cases}
a & : m = 1 \\
a \circ \paren {\paren {m - 1} \cdot a} & : m > 1 \end {cases}$


=== Archimedean Property on Norm ===
Let $\struct {S, \circ}$ be a closed algebraic structure.

Let $\cdot: \Z_{>0} \times S \to S$ be the operation defined as:
:$m \cdot a = \begin{cases}
a & : m = 1 \\
a \circ \paren {\paren {m - 1} \cdot a} & : m > 1 \end {cases}$


Let $n: S \to \R$ be a norm on $S$.
 
 

Then $n$ satisfies the Archimedean property on $S$  if and only if :
:$\forall a, b \in S: n \paren a < n \paren b \implies \exists m \in \N: n \paren {m \cdot a} > n \paren b$


Using the more common symbology for a norm:
:$\forall a, b \in S: \norm a < \norm b \implies \exists m \in \Z_{>0}: \norm {m \cdot a} > \norm b$


Category:Definitions/Abstract Algebra
Category:Definitions/Norm Theory

=== Archimedean Property on Ordering ===
Let $\struct {S, \circ}$ be a closed algebraic structure.

Let $\cdot: \Z_{>0} \times S \to S$ be the operation defined as:
:$m \cdot a = \begin{cases}
a & : m = 1 \\
a \circ \paren {\paren {m - 1} \cdot a} & : m > 1 \end {cases}$


Let $\preceq$ be an ordering on $S$.


Then $\preceq$ satisfies the Archimedean property on $S$  if and only if :

:$\forall a, b \in S: a \prec b \implies \exists m \in \Z_{>0}: b \prec m \cdot a$


Category:Definitions/Abstract Algebra
Category:Definitions/Order Theory",Definition:Archimedean Property,"['Definitions/Abstract Algebra', 'Definitions/Norm Theory']"
Definition:Argument,Argument,"Let $z = x + i y$ be a complex number.

An argument of $z$, or $\arg z$, is formally defined as a solution to the pair of equations:
:$(1): \quad \dfrac x {\cmod z} = \map \cos {\arg z}$
:$(2): \quad \dfrac y {\cmod z} = \map \sin {\arg z}$
where $\cmod z$ is the modulus of $z$.

From Sine and Cosine are Periodic on Reals, it follows that if $\theta$ is an argument of $z$, then so is $\theta + 2 k \pi$ where $k \in \Z$ is any integer.


Thus, the argument of a complex number $z$ is a continuous multifunction.

 


=== Principal Range ===
It is understood that the argument of a complex number $z$ is unique only up to multiples of $2 k \pi$.

With this understanding, we can limit the choice of what $\theta$ can be for any given $z$ by requiring that $\theta$ lie in some half open interval of length $2 \pi$.

The most usual of these are:
:$\hointr 0 {2 \pi}$
:$\hointl {-\pi} \pi$

but in theory any such interval may be used.

This interval is known as the principal range.

=== Principal Argument ===
Let $R$ be the principal range of the complex numbers $\C$.

The unique value of $\theta$ in $R$ is known as the principal argument, of $z$.

This is denoted $\Arg z$.

Note the capital $A$.

The standard practice is for $R$ to be $\hointl {-\pi} \pi$.

This ensures that the principal argument is continuous on the real axis for positive numbers.

Thus, if $z$ is represented in the complex plane, the principal argument $\Arg z$ is intuitively defined as the angle which $z$ yields with the real ($y = 0$) axis.


 ",Definition:Argument of Complex Number,"['Definitions/Argument of Complex Number', 'Definitions/Complex Numbers', 'Definitions/Complex Analysis', 'Definitions/Polar Form of Complex Number']"
Definition:Argument,Argument,"A logical argument (or just argument) is a process of creating a new statement from one or more existing statements.

An argument proceeds from a set of premises to a conclusion, by means of logical implication, via a procedure called logical inference.


An argument may have more than one premise, but only one conclusion.


While statements may be classified as either true or false, an argument may be classified as either valid or invalid.


Loosely speaking, a valid argument is one that leads unshakeably from true statements to other true statements, whereas an invalid argument is one that can lead you to, for example, a false statement from one which is true.


Thus:
:An argument may be valid, even though its premises are false.
:An argument may be invalid, even though its premises are true.
:An argument may be invalid and its premises false.

It is even possible for the conclusion of an argument to be true, even though the argument is invalid and its premises are false.


To be sure of the truth of a conclusion, it is necessary to make sure both that the premises are true and that the argument is valid.


However, while you may not actually know whether a statement is true or not, you can investigate the consequences of it being either true or false, and see what effect that has on the truth value of the proposition(s) of which it is a part. That, in short, is what the process of logical argument consists of.


An argument may be described symbolically by means of sequents, which specify the flow of an argument.


=== Finitary Argument ===
A finitary argument is a logical argument which starts with a finite number of axioms, and can be translated into a finite number of statements.",Definition:Logical Argument,"['Definitions/Logical Arguments', 'Definitions/Logic']"
Definition:Argument,Argument,"Let $f: S \to T$ be a mapping.

Let $f^{-1} \subseteq T \times S$ be the inverse of $f$, defined as:

:$f^{-1} = \set {\tuple {t, s}: \map f s = t}$


Every $s \in S$ such that $\map f s = t$ is called a preimage of $t$.


The preimage of an element $t \in T$ is defined as:

:$\map {f^{-1} } t := \set {s \in S: \map f s = t}$


This can also be expressed as:
:$\map {f^{-1} } t := \set {s \in \Img {f^{-1} }: \tuple {t, s} \in f^{-1} }$


That is, the preimage of $t$ under $f$ is the image of $t$ under $f^{-1}$.",Definition:Preimage/Mapping/Element,['Definitions/Preimages']
Definition:Argument,Argument,"=== Real Function ===
Let $f: \R \to \R$ be a real function.

Let $\map f x = y$.

Then $x$ is referred to as an independent variable.

=== Complex Function ===
Let $f: \C \to \C$ be a complex function.

Let $\map f z = w$.


Then $z$ is referred to as an independent variable (of $f$).",Definition:Independent Variable,"['Definitions/Independent Variables', 'Definitions/Analysis', 'Definitions/Mapping Theory']"
Definition:Argument,Argument,"Let $P$ be a point in a system of polar coordinates where $O$ is the pole.


The angle measured anticlockwise from the polar axis to $OP$ is called the angular coordinate of $P$, and usually labelled $\theta$.

If the angle is measured clockwise from the polar axis to $OP$, its value is considered negative.",Definition:Polar Coordinates/Angular Coordinate,['Definitions/Polar Coordinates']
Definition:Arrow,Arrow,"Let $\mathbf C$ be a metacategory.


A morphism of $\mathbf C$ is an object $f$, together with:

* A domain $\operatorname {dom} f$, which is an object of $\mathbf C$
* A codomain $\operatorname {cod} f$, also an object of $\mathbf C$


The collection of all morphisms of $\mathbf C$ is denoted $\mathbf C_1$.


If $A$ is the domain of $f$ and $B$ is its codomain, this is mostly represented by writing:

:$f: A \to B$ or $A \stackrel f \longrightarrow B$",Definition:Morphism,"['Definitions/Morphisms', 'Definitions/Category Theory']"
Definition:Arrow,Arrow,"Informally, a functor is a morphism of categories.

It may be described as what one must define in order to define a natural transformation.

This is formalized by defining the category of categories.


=== Covariant Functor ===
Let $\mathbf C$ and $\mathbf D$ be metacategories.


A covariant functor $F: \mathbf C \to \mathbf D$ consists of:

* An object functor $F_0$ that assigns to each object $X$ of $\mathbf C$ an object $FX$ of $\mathbf D$.

* An arrow functor $F_1$ that assigns to each arrow $f: X \to Y$ of $\mathbf C$ an arrow $Ff : FX \to FY$ of $\mathbf D$.


These functors must satisfy, for any morphisms $X \stackrel f \longrightarrow Y \stackrel g \longrightarrow Z$ in $\mathbf C$:

:$\map F {g \circ f} = F g \circ F f$
and:
:$\map F {\operatorname {id}_X} = \operatorname{id}_{F X}$

where $\operatorname {id}_W$ denotes the identity arrow on an object $W$, and $\circ$ is the composition of morphisms.


The behaviour of a covariant functor can be pictured as follows:

::$\begin{xy}
<4em,4em>*{\mathbf C} = ""C"",
<0em,0em>*+{X} = ""a"",
<4em,0em>*+{Y} = ""b"",
<4em,-4em>*+{Z}= ""c"",

""a"";""b"" **@{-} ?>*@{>} ?<>(.5)*!/_1em/{f},
""b"";""c"" **@{-} ?>*@{>} ?<>(.5)*!/_1em/{g},
""a"";""c"" **@{-} ?>*@{>} ?<>(.5)*!/^1em/{g \circ f},

""C""+/r9em/*{\mathbf D},
""C""+/r2em/;""C""+/r6em/ **@{-} ?>*@{>} ?*!/_1em/{F},

""b""+/r2em/+/_2em/;""b""+/r6em/+/_2em/ **@{~} ?>*@2{>} ?<>(.5)*!/_.6em/{F},

""a""+/r13em/*+{FX}=""Fa"", 
""b""+/r13em/*+{FY}=""Fb"", 
""c""+/r13em/*+{FZ}=""Fc"",

""Fa"";""Fb"" **@{-} ?>*@{>} ?<>(.5)*!/_1em/{Ff},
""Fb"";""Fc"" **@{-} ?>*@{>} ?<>(.5)*!/_1em/{Fg},
""Fa"";""Fc"" **@{-} ?>*@{>} ?<>(.7)*!/r3em/{F \left({g \circ f}\right) = \\ Fg \circ Ff},
\end{xy}$

=== Contravariant Functor ===
=== Definition 1 ===
Let $\mathbf C$ and $\mathbf D$ be metacategories.


A contravariant functor $F : \mathbf C \to \mathbf D$ consists of:

* An object functor $F_0$ that assigns to each object $X$ of $\mathbf C$ an object $FX$ of $\mathbf D$.

* An arrow functor $F_1$ that assigns to each arrow $f : X \to Y$ of $\mathbf C$ an arrow $Ff : FY \to FX$ of $\mathbf D$.


These functors must satisfy, for any morphisms $X \stackrel f \longrightarrow Y \stackrel g \longrightarrow Z$ in $\mathbf C$:

:$\map F {g \circ f} = F f \circ F g$
and:
:$\map F {\operatorname {id}_X} = \operatorname {id}_{F X}$

where:
:$\operatorname {id}_W$ denotes the identity arrow on an object $W$
and:
:$\circ$ is the composition of morphisms.

=== Definition 2 ===
Let $\mathbf C$ and $\mathbf D$ be metacategories.

A contravariant functor $F : \mathbf C \to \mathbf D$ is a covariant functor:
:$F: \mathbf C^{\text{op}} \to \mathbf D$
where $\mathbf C^{\text{op}}$ is the dual category of $\mathbf C$.",Definition:Functor,"['Definitions/Category Theory', 'Definitions/Morphisms', 'Definitions/Functors']"
Definition:Arrow,Arrow,"Let $\mathbf C$ be a metacategory.


Its morphism category, denoted $\mathbf C^\to$, is defined as follows:

 


The morphisms of $\mathbf C^\to$ can be made more intuitive by the following diagram:

::$\begin{xy}
<-2em,0em>*+{f} = ""f"",
<2em,0em>*+{f'} = ""f2"",

""f"";""f2"" **@{-} ?>*@{>} ?*!/_1em/{\scriptstyle \tuple {g_1, g_2} },

<3em,0em>*{:},

<7em,2em>*+{A} = ""A"",
<7em,-2em>*+{B} = ""B"",
<11em,2em>*+{A'} = ""A2"",
<11em,-2em>*+{B'} = ""B2"",

""A"";""B"" **@{-} ?>*@{>} ?*!/^1em/{f},
""A"";""A2"" **@{-} ?>*@{>} ?*!/_1em/{g_1},
""A2"";""B2"" **@{-} ?>*@{>} ?*!/_1em/{f'},
""B"";""B2"" **@{-} ?>*@{>} ?*!/^1em/{g_2}
\end{xy}$

The composition likewise benefits from a diagrammatic representation:

::$\begin{xy}
<4em,5em>*{\tuple {h_1, h_2} \circ \tuple {g_1, g_2} },

<0em,2em>*+{A} = ""A"",
<0em,-2em>*+{B} = ""B"",
<4em,2em>*+{A'} = ""A2"",
<4em,-2em>*+{B'} = ""B2"",

""A"";""B"" **@{-} ?>*@{>} ?*!/^1em/{f},
""A"";""A2"" **@{-} ?>*@{>} ?*!/_1em/{g_1},
""A2"";""B2"" **@{-} ?>*@{>} ?*!/_1em/{f'},
""B"";""B2"" **@{-} ?>*@{>} ?*!/^1em/{g_2},

<8em,2em>*+{A} = ""A3"",
<8em,-2em>*+{B} = ""B3"",

""A2"";""A3"" **@{-} ?>*@{>} ?*!/_1em/{h_1},
""B2"";""B3"" **@{-} ?>*@{>} ?*!/^1em/{h_2},
""A3"";""B3"" **@{-} ?>*@{>} ?*!/_1em/{f},

<12em,5em>*{=},
<10em,0em>;<14em,0em> **@{~} ?>*@2{>},

<20em,5em>*+{\tuple {h_1 \circ g_1, h_2 \circ g_2} },

<16em,2em>*+{A} = ""AA"",
<16em,-2em>*+{B} = ""BB"",
<24em,2em>*+{A} = ""AA3"",
<24em,-2em>*+{B} = ""BB3"",

""AA"";""BB"" **@{-} ?>*@{>} ?*!/^1em/{f},
""AA"";""AA3"" **@{-} ?>*@{>} ?*!/_1em/{h_1 \circ g_1},
""AA3"";""BB3"" **@{-} ?>*@{>} ?*!/_1em/{f},
""BB"";""BB3"" **@{-} ?>*@{>} ?*!/^1em/{h_2 \circ g_2},
\end{xy}$",Definition:Morphism Category,"['Definitions/Examples of Categories', 'Definitions/Morphisms']"
Definition:Artinian,Artinian,"Let $A$ be a commutative ring with unity.

Then $A$ is Artinian  if and only if  it is Artinian as an $A$-module.

 ",Definition:Artinian Ring,['Definitions/Ring Theory']
Definition:Artinian,Artinian,"Let $A$ be a commutative ring with unity.

Let $M$ be an $A$-module.


=== Definition 1 ===
Let $A$ be a commutative ring with unity.

Let $M$ be an $A$-module.


$M$ is a Artinian module  if and only if :
:$M$ satisfies the descending chain condition.

=== Definition 2 ===
Let $A$ be a commutative ring with unity.

Let $M$ be an $A$-module


$M$ is a Artinian module  if and only if :
:$M$ satisfies the minimal condition.",Definition:Artinian Module,"['Definitions/Examples of Modules', 'Definitions/Artinian Modules']"
Definition:Associative,Associative,"Let $S$ be a set.

Let $\circ : S \times S \to S$ be a binary operation.


Then $\circ$ is associative  if and only if :

:$\forall x, y, z \in S: \paren {x \circ y} \circ z = x \circ \paren {y \circ z}$",Definition:Associative Operation,"['Definitions/Abstract Algebra', 'Definitions/Operations', 'Definitions/Associativity']"
Definition:Associative,Associative,"Let $\circ$ be a binary operation.

Then $\circ$ is defined as being power-associative on $S$  if and only if :

:$\forall x \in S: \paren {x \circ x} \circ x = x \circ \paren {x \circ x}$",Definition:Power-Associative Operation,"['Definitions/Abstract Algebra', 'Definitions/Operations']"
Definition:Associative,Associative,"Let $\struct {S, \circ}$ be a magma.


Then $\struct {S, \circ}$ is a semigroup  if and only if  $\circ$ is associative on $S$.


That is:

:A semigroup is an algebraic structure which is closed and whose operation is associative.


=== Semigroup Axioms ===

The properties that define a semigroup can be gathered together as follows:

 

=== Multiplicative Semigroup ===
Let $\struct {S, \circ}$ be a semigroup whose operation is multiplication.


Then $\struct {S, \circ}$ is a multiplicative semigroup.


It is implicit in this definition that the elements of $S$ are numbers, or objects derived from numbers upon which the concept of multiplication is applicable.

=== Additive Semigroup ===
Let $\struct {S, \circ}$ be a semigroup whose operation is addition.


Then $\struct {S, \circ}$ is an additive semigroup.


It is implicit in this definition that the elements of $S$ are numbers, or objects derived from numbers upon which the concept of addition is applicable.",Definition:Semigroup,"['Definitions/Semigroups', 'Definitions/Algebraic Structures']"
Definition:Associative,Associative,"Let $R$ be a commutative ring.

Let $\struct {A_R, *}$ be an algebra over $R$.


Then $\struct {A_R, *}$ is an associative algebra  if and only if  $*$ is an associative operation.

That is:

:$\forall a, b, c \in A_R: \paren {a * b} * c = a * \paren {b * c}$",Definition:Associative Algebra,"['Definitions/Associative Algebras', 'Definitions/Algebras', 'Definitions/Associativity']"
Definition:Atom,Atom,"In a particular branch of logic, certain concepts are at such a basic level of simplicity they can not be broken down into anything simpler.

Those concepts are called atoms or described as atomic.


Different branches of logic admit different atoms.


=== Propositional Logic ===
In propositional logic, the atoms are statements.",Definition:Atom (Logic),['Definitions/Logic']
Definition:Atom,Atom,"Let $\struct {X, \Sigma, \mu}$ be a measure space.


An element $x \in X$ is said to be an atom (of $\mu$)  if and only if :

:$(1): \quad \set x \in \Sigma$
:$(2): \quad \map \mu {\set x} > 0$

 ",Definition:Atom of Measure,['Definitions/Measures']
Definition:Atom,Atom,"Let $\struct {X, \Sigma}$ be a measurable space.

Let $E \in \Sigma$ be non-empty.


$E$ is said to be an atom (of $\Sigma$)  if and only if  it satisfies:

:$\forall F \in \Sigma: F \subsetneq E \implies F = \O$


Thus, atoms are the minimal non-empty sets in $\Sigma$ with respect to the subset ordering.",Definition:Atom of Sigma-Algebra,['Definitions/Sigma-Algebras']
Definition:Atom,Atom,"Let $\struct {S, \vee, \wedge, \preceq}$ be a lattice.


An atom of $\struct {S, \vee, \wedge, \preceq}$ is an element $A \in S$ such that:
:$\forall B \in S: B \preceq A, B \ne A \implies B = \bot$
:$A \ne \bot$
where $\bot$ denotes the bottom of $\struct {S, \vee, \wedge, \preceq}$.",Definition:Atom of Lattice,"['Definitions/Atoms of Lattices', 'Definitions/Lattice Theory']"
Definition:Atom,Atom,"An atom (in the context of physics and chemistry) is the smallest piece of matter that can exist of a particular type of substance.


Atoms can be subdivided into smaller particles, but then it ceases to be that substance.


=== Diameter ===
The diameter of an atom (considered approximately spherical) is between about $1$ and $5$ angstroms.",Definition:Atom (Physics),"['Definitions/Atoms', 'Definitions/Atomic Physics', 'Definitions/Chemistry']"
Definition:Automorphism,Automorphism,"An automorphism is an isomorphism from an algebraic structure to itself.

This applies to the term isomorphism as used both in the sense of bijective homomorphism as well as that of an order isomorphism.


Hence an automorphism is a permutation which is either a homomorphism or an order isomorphism, depending on context.


=== Semigroup Automorphism ===
Let $\struct {S, \circ}$ be a semigroup.

Let $\phi: S \to S$ be a (semigroup) isomorphism from $S$ to itself.


Then $\phi$ is a semigroup automorphism.

=== Group Automorphism ===
Let $\struct {G, \circ}$ be a group.

Let $\phi: G \to G$ be a (group) isomorphism from $G$ to itself.


Then $\phi$ is a group automorphism.

=== Ring Automorphism ===
Let $\struct {R, +, \circ}$ be a ring.

Let $\phi: R \to R$ be a (ring) isomorphism.


Then $\phi$ is a ring automorphism.

That is, a ring automorphism is a (ring) isomorphism from a ring to itself.

=== Field Automorphism ===
Let $\struct {F, +, \circ}$ be a field.

Let $\phi: F \to F$ be a (field) isomorphism from $F$ to itself.


Then $\phi$ is a field automorphism.

=== $R$-Algebraic Structure Automorphism ===
Let $\struct {S, \ast_1, \ast_2, \ldots, \ast_n, \circ}_R$ be an $R$-algebraic structure.

Let $\phi: S \to S$ be an $R$-algebraic structure isomorphism from $S$ to itself.


Then $\phi$ is an $R$-algebraic structure automorphism.


This definition continues to apply when $S$ is a module, and also when it is a vector space.


=== Module Automorphism ===
Let $\struct {G, +_G, \circ}_R$ be an $R$-module.

Let $\phi: G \to G$ be a module isomorphism to itself.


Then $\phi$ is a module automorphism.

=== Vector Space Automorphism ===
Let $V$ be a $K$-vector space.

Let $\phi: V \to V$ be a vector space isomorphism to itself.


Then $\phi$ is a vector space automorphism.

=== Ordered Structure Automorphism ===
Let $\struct {S, \circ, \preceq}$ be an ordered structures.

Let $\phi: S \to S$ be an ordered structure isomorphism from $S$ to itself.


Then $\phi$ is an ordered structure automorphism.


=== Ordered Semigroup Automorphism ===
Let $\struct {S, \circ, \preceq}$ be an ordered semigroup.

An ordered semigroup automorphism from $\struct {S, \circ, \preceq}$ to itself is a mapping $\phi: S \to S$ that is both:

:$(1): \quad$ A semigroup automorphism, that is, a semigroup isomorphism from the semigroup $\struct {S, \circ}$ to itself

:$(2): \quad$ An order isomorphism from the ordered set $\struct {S, \preceq}$ to itself.

=== Ordered Group Automorphism ===
Let $\struct {G, \circ, \preceq}$ be an ordered group.

An ordered group automorphism from $\struct {G, \circ, \preceq}$ to itself is a mapping $\phi: G \to G$ that is both:

:$(1): \quad$ A group automorphism, that is, a group isomorphism from the group $\struct {G, \circ}$ to itself

:$(2): \quad$ An order isomorphism from the ordered set $\struct {G, \preceq}$ to itself.

=== Ordered Ring Automorphism ===
Let $\struct {R, +, \circ, \preceq}$ be an ordered ring.


An ordered ring automorphism from $\struct {R, +, \circ, \preceq}$ to itself is a mapping $\phi: R \to R$ that is both:

:$(1): \quad$ An ordered group automorphism from the ordered group $\struct {R, +, \preceq}$ to itself

:$(2): \quad$ A semigroup automorphism from the semigroup $\struct {R, \circ}$ to itself.",Definition:Automorphism (Abstract Algebra),"['Definitions/Automorphisms (Abstract Algebra)', 'Definitions/Isomorphisms (Abstract Algebra)', 'Definitions/Homomorphisms (Abstract Algebra)', 'Definitions/Automorphisms']"
Definition:Automorphism,Automorphism,"Let $\MM$ and $\NN$ be $\LL$-structures with universes $M$ and $N$ respectively.


$j: \MM \to \NN$ is an $\LL$-embedding  if and only if  it is an injective map $M \to N$ which preserves interpretations of all symbols in $\LL$; that is, such that:
:$\map j {\map {f^\MM} {a_1, \dots, a_{n_f} } } = \map {f^\NN} {\map j {a_1}, \ldots, \map j {a_{n_f} } }$ for all function symbols $f$ in $\LL$ and $a_1, \dots, a_{n_f}$ in $M$
:$\tuple {a_1, \ldots, a_{n_R} } \in R^\MM \iff \tuple {\map j {a_1}, \dots, \map j {a_{n_R} } } \in R^\NN$ for all relation symbols $R$ in $\LL$ and $a_1, \dots, a_{n_R}$ in $M$
:$\map j {c^\MM} = c^\NN$ for all constant symbols $c$ in $\LL$.


=== Partial Embedding ===

A common method of constructing isomorphisms and elementary embeddings in proofs is to recursively define them a finite number of elements at a time.  For this  purpose, it is useful to have a definition of embeddings using functions which are only defined on a subset of $M$:


Let $A \subseteq M$ be a subset of $M$.


$j: A \to \NN$ is a partial $\LL$-embedding  if and only if  it is an injective map $A \to N$ which preserves interpretations of all symbols in $\LL$ applied to elements of $A$; that is, such that:
:$\map j {\map {f^\MM} {a_1, \dots, a_{n_f} } } = \map {f^\NN} {\map j {a_1}, \ldots, \map j {a_{n_f} } }$ for  all function symbols $f$ in $\LL$ and $a_1, \dots, a_{n_f}$ in $A$
:$\tuple {a_1, \ldots, a_{n_R} } \in R^\MM \iff \tuple {\map j {a_1}, \dots, \map j {a_{n_R} } } \in R^\NN$ for all relation symbols $R$ in $\LL$ and $a_1, \dots, a_{n_R}$ in $A$
:$\map j {c^\MM} = c^\NN$ for all constant symbols $c$ in $\LL$.


=== Isomorphism ===

$j: \MM \to \NN$ is an $\LL$-isomorphism  if and only if  it is a bijective $\LL$-embedding.


=== Automorphism ===

$j: \MM \to \NN$ is an $\LL$-automorphism  if and only if  it is an $\LL$-isomorphism and $\MM = \NN$.


It is often useful to talk about automorphisms which are constant on subsets of $M$.  So, there is a definition and a notation for doing so:

Let $A \subseteq M$ be a subset of $M$, and let $b \in M$.


An $\LL$-automorphism $j$ is an $A$-automorphism  if and only if  $\map j a = a$ for all $a\in A$.

An $\LL$-automorphism $j$ is an $A, b$-automorphism  if and only if  it is an $\paren {A \cup \set b}$-automorphism; that is: $\map j a = a$ for all $a \in A$ and also $\map j b = b$.",Definition:Embedding (Model Theory),"['Definitions/Model Theory for Predicate Logic', 'Definitions/Automorphisms']"
Definition:Auxiliary,Auxiliary,"Let:
:$(1): \quad y + p y' + q y = 0$
be a constant coefficient homogeneous linear second order ODE.


The auxiliary equation of $(1)$ is the quadratic equation:
:$m^2 + p m + q = 0$",Definition:Auxiliary Equation,"['Definitions/Auxiliary Equations', 'Definitions/Second Order ODEs']"
Definition:Auxiliary,Auxiliary,"Let $L = \struct {S, \vee, \preceq}$ be a bounded below join semilattice.

Let $\RR \subseteq S \times S$ be a relation on $S$.


Then $\RR$ is an auxiliary relation  if and only if  $\RR$ satisfies the auxiliary relation axioms:
 ",Definition:Auxiliary Relation,['Definitions/Order Theory']
Definition:Auxiliary,Auxiliary,"Let $\KK$ be a central conic, that is, an ellipse or a hyperbola.

The auxiliary circle of $\KK$ is the eccentric circle whose diameter coincides with the major axis of $\KK$.


=== Auxiliary Circle of Ellipse ===
Let $E$ be an ellipse.

The auxiliary circle of $E$ is the eccentric circle whose diameter coincides with the major axis of $E$:

:

In the above diagram, the auxiliary circle of $E$ is the circle $C$.

=== Auxiliary Circle of Hyperbola ===
Let $H$ be a hyperbola.

The auxiliary circle of $H$ is the eccentric circle whose diameter coincides with the major axis of $H$:

:

In the above diagram, the auxiliary circle of $H$ is the circle $C$.",Definition:Auxiliary Circle,"['Definitions/Auxiliary Circles', 'Definitions/Central Conics', 'Definitions/Conic Sections']"
Definition:Auxiliary,Auxiliary,"Consider the expression:

:$(1): \quad p \sin x + q \cos x$

where $x \in \R$.

Let $(1)$ be expressed in the form:
:$(2): \quad R \map \cos {x + \alpha}$

or:
:$(3): \quad R \map \sin {x + \alpha}$


The angle $\alpha$ is known as the auxiliary angle of either $(2)$ or $(3)$ as appropriate.",Definition:Auxiliary Angle,"['Definitions/Angles', 'Definitions/Sine Function', 'Definitions/Cosine Function', 'Definitions/Trigonometry']"
Definition:Baire Space,Baire Space,"Let $T = \struct {S, \tau}$ be a topological space.


=== Definition 1 ===
Let $T = \struct {S, \tau}$ be a topological space.


$T$ is a Baire space  if and only if  the union of any countable set of closed sets of $T$ whose interiors are empty also has an empty interior.

=== Definition 2 ===
Let $T = \struct {S, \tau}$ be a topological space.


$T$ is a Baire space  if and only if  the intersection of any countable set of open sets of $T$ which are everywhere dense is everywhere dense.

=== Definition 3 ===
Let $T = \struct {S, \tau}$ be a topological space.


$T$ is a Baire space  if and only if  the interior of the union of any countable set of closed sets of $T$ which are nowhere dense is empty.

=== Definition 4 ===
Let $T = \struct {S, \tau}$ be a topological space.


$T$ is a Baire space  if and only if , whenever the union of any countable set of closed sets of $T$ has an interior point, then one of those closed sets must have an interior point.",Definition:Baire Space (Topology),"['Definitions/Topology', 'Definitions/Baire Spaces']"
Definition:Baire Space,Baire Space,"The Baire space $\mathbf B$ is defined as the set of all infinite sequences of natural numbers.

It can also be defined as the Cartesian product of a countably infinite number of copies of the set of natural numbers.",Definition:Baire Space (Set Theory),['Definitions/Set Theory']
Definition:Bar,Bar,"A bar chart is a form of graph which consists of a finite set of (usually) vertical bars whose length determines the statistic being communicated.


:",Definition:Bar Chart,"['Definitions/Bar Charts', 'Definitions/Graphs (Statistics)']"
Definition:Bar,Bar,"The bar is a CGS unit of pressure.


It is defined as being:

:The amount of pressure equal to exactly $100 \, 000$ pascals.


=== Conversion Factors ===
",Definition:Bar (Unit),"['Definitions/Bar', 'Definitions/Units of Measurement', 'Definitions/CGS', 'Definitions/Pressure']"
Definition:Bar,Bar,A bar is a straight rigid body whose length in one dimension is such that its width in the other two dimensions is negligible.,Definition:Bar (Mechanics),['Definitions/Ideals in Physics']
Definition:Base,Base,"The base of a geometric figure is a specific part of that figure which is distinguished from the remainder of that figure and placed (actually or figuratively) at the bottom of a depiction or visualisation.

In some cases the base is truly qualitiatively different from the rest of the figure.

In other cases the base is selected arbitrarily as one of several parts of the figure which may equally well be so chosen.",Definition:Base of Geometric Figure,['Definitions/Geometric Figures']
Definition:Base,Base,":


For a given triangle, one of the sides can be distinguished as being the base.

It is immaterial which is so chosen.

The usual practice is that the triangle is drawn so that the base is made horizontal, and at the bottom.

In the above diagram, it would be conventional for the side $AC$ to be identified as the base.",Definition:Triangle (Geometry)/Base,['Definitions/Triangles']
Definition:Base,Base,":


The base of an isosceles triangle is specifically defined to be the side which is a different length from the other two.

In the above diagram, $BC$ is the base.",Definition:Triangle (Geometry)/Isosceles/Base,['Definitions/Isosceles Triangles']
Definition:Base,Base,":

In a given parallelogram, one of the sides is distinguished as being the base.

It is immaterial which is so chosen, but usual practice is that it is one of the two longer sides.

In the parallelogram above, line $AB$ is considered to be the base.


Category:Definitions/Parallelograms",Definition:Quadrilateral/Parallelogram/Base,['Definitions/Parallelograms']
Definition:Base,Base,":

The base of a segment of a circle is the straight line forming one of the boundaries of the seqment.

In the above diagram, $AB$ is the base of the highlighted segment.


Category:Definitions/Segments of Circles",Definition:Segment of Circle/Base,['Definitions/Segments of Circles']
Definition:Base,Base,"Consider a cone consisting of the set of all straight lines joining the boundary of a plane figure $PQR$ to a point $A$ not in the same plane of $PQR$:


:


The plane figure $PQR$ is called the base of the cone.",Definition:Cone (Geometry)/Base,['Definitions/Cones']
Definition:Base,Base,"

Let $\triangle AOB$ be a right-angled triangle such that $\angle AOB$ is the right angle.

Let $K$ be the right circular cone formed by the rotation of $\triangle AOB$ around $OB$.

Let $BC$ be the circle described by $B$.

The base of $K$ is the plane surface enclosed by the circle $BC$.


 
: 
:And the base is the circle described by the straight line which is carried round.
 ''
 


Category:Definitions/Right Circular Cones",Definition:Right Circular Cone/Base,['Definitions/Right Circular Cones']
Definition:Base,Base,":


The polygon of a pyramid to whose vertices the apex is joined is called the base of the pyramid.

In the above diagram, $ABCDE$ is the base of the pyramid $ABCDEQ$.",Definition:Pyramid/Base,['Definitions/Pyramids']
Definition:Base,Base,":


The two (equal) vertices adjacent to the base of an isosceles triangle are called the base angles.

In the above diagram, $\angle ABC$ and $\angle ACB$ are the base angles.",Definition:Triangle (Geometry)/Isosceles/Base Angles,['Definitions/Isosceles Triangles']
Definition:Base,Base,"=== Integers ===
Let $n \in \Z$ be an integer.

Let $b \in \Z$ be an integer such that $b > 1$.

By the Basis Representation Theorem, $n$ can be expressed uniquely in the form:

:$\ds n = \sum_{j \mathop = 0}^m r_j b^j$

where:
:$m$ is such that $b^m \le n < b^{m + 1}$
:all the $r_j$ are such that $0 \le r_j < b$.

  
 

The number $b$ is known as the number base to which $n$ is represented.

$n$ is thus described as being (written) in base $b$.


Thus we can write $\ds n = \sum_{j \mathop = 0}^m {r_j b^j}$ as:
:$\sqbrk {r_m r_{m - 1} \ldots r_2 r_1 r_0}_b$
or, if the context is clear:
:${r_m r_{m - 1} \ldots r_2 r_1 r_0}_b$

=== Real Numbers ===
Let $x \in \R$ be a real number such that $x \ge 0$.

Let $b \in \N: b \ge 2$.


See the definition of Basis Expansion for how we can express $x$ in the form:

:$x = \sqbrk {s \cdotp d_1 d_2 d_3 \ldots}_b$

Then we express $m$ as for integers, and arrive at:
:$x = \sqbrk {r_m r_{m - 1} \ldots r_2 r_1 r_0 \cdotp d_1 d_2 d_3 \ldots}_b$

or, if the context is clear:
:$r_m r_{m - 1} \ldots r_2 r_1 r_0 \cdotp d_1 d_2 d_3 \ldots_b$

=== Integer Part ===
Let $x \in \R$ be a real number such that $x \ge 0$.

Let $b \in \N: b \ge 2$.


In the basis expansion:
:$x = \left[{r_m r_{m-1} \ldots r_2 r_1 r_0 . d_1 d_2 d_3 \ldots}\right]_b$
the part $r_m r_{m-1} \ldots r_2 r_1 r_0$ is known as the integer part.

=== Fractional Part ===
Let $x \in \R$ be a real number such that $x \ge 0$.

Let $b \in \N: b \ge 2$.


In the basis expansion:
:$x = \sqbrk {r_m r_{m - 1} \ldots r_2 r_1 r_0 . d_1 d_2 d_3 \ldots}_b$
the part $.d_1 d_2 d_3 \ldots$ is known as the fractional part.

=== Radix Point ===
Let $x \in \R$ be a real number such that $x \ge 0$.

Let $b \in \N: b \ge 2$.


In the basis expansion:
:$x = \sqbrk {r_m r_{m - 1} \ldots r_2 r_1 r_0 \cdotp d_1 d_2 d_3 \ldots}_b$
the dot that separates the integer part from the fractional part is called the radix point.",Definition:Number Base,['Definitions/Number Bases']
Definition:Base,Base,"Let $\log_a$ denote the logarithm function on whatever domain: $\R$ or $\C$.

The constant $a$ is known as the base of the logarithm.",Definition:Logarithm/Base,['Definitions/Logarithms']
Definition:Basis,Basis,"=== Analytic Basis ===
=== Definition 1 ===
Let $\struct {S, \tau}$ be a topological space.


An analytic basis for $\tau$ is a subset $\BB \subseteq \tau$ such that:
:$\ds \forall U \in \tau: \exists \AA \subseteq \BB: U = \bigcup \AA$


That is, such that for all $U \in \tau$, $U$ is a union of sets from $\BB$.

=== Definition 2 ===
Let $\struct {S, \tau}$ be a topological space.

Let $\BB \subseteq \tau$.


Then $\BB$ is an analytic basis for $\tau$  if and only if :
:$\forall U \in \tau: \forall x \in U: \exists V \in \BB: x \in V \subseteq U$

=== Synthetic Basis ===
Let $S$ be a set.


A synthetic basis on $S$ is a subset $\BB \subseteq \powerset S$ of the power set of $S$ such that:

 
 
 
 

That is, the intersection of any pair of elements of $\BB$ is a union of sets of $\BB$.",Definition:Basis (Topology),"['Definitions/Topology', 'Definitions/Topological Bases']"
Definition:Basis,Basis,"Let $K$ be a division ring.

Let $\struct {G, +_G, \circ}_R$ be a vector space over $K$.


=== Definition 1 ===
Let $R$ be a division ring.

Let $\struct {G, +_G, \circ}_R$ be an vector space over $R$.


A basis of $G$ is a linearly independent subset of $G$ which is a generator for $G$.

=== Definition 2 ===
Let $R$ be a division ring.

Let $\struct {G, +_G, \circ}_R$ be an vector space over $R$.


A basis is a maximal linearly independent subset of $G$.",Definition:Basis of Vector Space,"['Definitions/Vector Spaces', 'Definitions/Linear Algebra', 'Definitions/Bases of Vector Spaces']"
Definition:Basis,Basis,"Let $R$ be a ring with unity.

Let $\struct {G, +_G, \circ}_R$ be a unitary $R$-module.


=== Definition 1 ===
Let $R$ be a ring with unity.

Let $\struct {G, +_G, \circ}_R$ be a unitary $R$-module.


A basis of $G$ is a linearly independent subset of $G$ which is a generator for $G$.

=== Definition 2 ===
Let $R$ be a ring with unity.

Let $\struct {G, +_G, \circ}_R$ be a unitary $R$-module.

Let $\BB = \family {b_i}_{i \mathop \in I}$ be a family of elements of $M$.

Let $\Psi: R^{\paren I} \to M$ be the homomorphism given by Universal Property of Free Module on Set.


Then $\BB$ is a basis of $G$   $\Psi$ is an isomorphism.",Definition:Basis of Module,"['Definitions/Linear Algebra', 'Definitions/Module Theory', 'Definitions/Bases of Modules']"
Definition:Basis,Basis,"Let $H$ be a Hilbert space.


A basis for $H$ is a maximal orthonormal subset of $H$.

Thus, $B$ is a basis for $H$  if and only if  for all orthonormal subsets $B'$ of $H$:

:$B \subseteq B' \implies B = B'$",Definition:Basis (Hilbert Space),['Definitions/Hilbert Spaces']
Definition:Below,Below,"In the context of numbers, below means less than.

Note that this applies to:
:the natural numbers $\N$
:the integers $\Z$
:the rational numbers $\Q$
:the real numbers $\R$

but specifically not the complex numbers $\C$ because the complex numbers do not have a usual ordering.",Definition:Below (Number),['Definitions/Language Definitions']
Definition:Below,Below,"Let $a$ and $b$ be points in $3$-dimensional Euclidean space $\R^3$.

Let $P$ be an arbitrary plane embedded in $S$ be distinguished and defined as horizontal.

Then:
:$a$ is below $b$
 if and only if :
:the height of $a$   $P$ is less than the height of $b$   $P$.",Definition:Below (Solid Geometry),"['Definitions/Language Definitions', 'Definitions/Solid Geometry']"
Definition:Below,Below,"Let $a$ and $b$ be points in the cartesian plane $\R^2$.

Then:
:$a$ is below $b$
 if and only if :
:the $y$ coordinate of $a$ is less than the $y$ coordinate of $b$.",Definition:Below (Plane Geometry),"['Definitions/Language Definitions', 'Definitions/Plane Geometry']"
Definition:Bilinear Form,Bilinear Form,"Let $R$ be a ring.

Let $R_R$ denote the $R$-module $R$.

Let $M_R$ be an $R$-module.


A bilinear form on $M_R$ is a bilinear mapping $B : M_R \times M_R \to R_R$.",Definition:Bilinear Form (Linear Algebra),"['Definitions/Bilinear Forms (Linear Algebra)', 'Definitions/Linear Forms (Linear Algebra)', 'Definitions/Linear Algebra', 'Definitions/Module Theory', 'Definitions/Vector Spaces', 'Definitions/Bilinear Forms']"
Definition:Bilinear Form,Bilinear Form,A bilinear form is a linear form of order $2$.,Definition:Bilinear Form (Polynomial Theory),"['Definitions/Bilinear Forms (Polynomial Theory)', 'Definitions/Linear Forms (Polynomial Theory)', 'Definitions/Bilinear Forms']"
Definition:Binomial,Binomial,"Let $a$ and $b$ be two (strictly) positive real numbers such that:
: $(1): \quad \dfrac a b \notin \Q$
: $(2): \quad \left({\dfrac a b}\right)^2 \in \Q$
where $\Q$ denotes the set of rational numbers.


Then $a + b$ is a binomial.


 

=== First Binomial ===
Let $a$ and $b$ be two (strictly) positive real numbers such that $a + b$ is a binomial.


Then $a + b$ is a first binomial  if and only if :
: $(1): \quad a \in \Q$
: $(2): \quad \dfrac {\sqrt {a^2 - b^2} } a \in \Q$
where $\Q$ denotes the set of rational numbers.


 
: 
:Given a rational straight line and a binomial, divided into its terms, such that the square on the greater term is greater than the square on the lesser by the square on a straight line commensurable in length with the greater, then, if the greater term be commensurable in length with the rational straight line set out, let the whole be called a first binomial straight line;
 ''
 

=== Second Binomial ===
Let $a$ and $b$ be two (strictly) positive real numbers such that $a + b$ is a binomial.


Then $a + b$ is a second binomial  if and only if :
: $(1): \quad b \in \Q$
: $(2): \quad \dfrac {\sqrt {a^2 - b^2}} a \in \Q$
where $\Q$ denotes the set of rational numbers.


 
: 
:but if the lesser term be commensurable in length with the rational straight line set out, let the whole be called a second binomial;
 ''
 

=== Third Binomial ===
Let $a$ and $b$ be two (strictly) positive real numbers such that $a + b$ is a binomial.


Then $a + b$ is a third binomial  if and only if :
: $(1): \quad a \notin \Q$
: $(2): \quad b \notin \Q$
: $(3): \quad \dfrac {\sqrt {a^2 - b^2}} a \in \Q$
where $\Q$ denotes the set of rational numbers.


 
: 
:and if neither of the terms be commensurable in length with the rational straight line set out, let the whole be called a third binomial.
 ''
 

=== Fourth Binomial ===
Let $a$ and $b$ be two (strictly) positive real numbers such that $a + b$ is a binomial.


Then $a + b$ is a fourth binomial  if and only if :
: $(1): \quad a \in \Q$
: $(2): \quad \dfrac {\sqrt {a^2 - b^2}} a \notin \Q$
where $\Q$ denotes the set of rational numbers.


 
: 
:Again, if the square on the greater term be greater than the square on the lesser by the square on a straight line incommensurable in length with the greater, then, if the greater term be commensurable in length with the rational straight line set out, let the whole be called a fourth binomial;
 ''
 

=== Fifth Binomial ===
Let $a$ and $b$ be two (strictly) positive real numbers such that $a + b$ is a binomial.


Then $a + b$ is a fifth binomial  if and only if :
: $(1): \quad b \in \Q$
: $(2): \quad \dfrac {\sqrt {a^2 - b^2}} a \notin \Q$

where $\Q$ denotes the set of rational numbers.


 
: 
:if the lesser, a fifth binomial;
 ''
 

=== Sixth Binomial ===
Let $a$ and $b$ be two (strictly) positive real numbers such that $a + b$ is a binomial.


Then $a + b$ is a sixth binomial  if and only if :
: $(1): \quad: a \notin \Q$
: $(2): \quad: b \notin \Q$
: $(3): \quad: \dfrac {\sqrt {a^2 - b^2}} a \notin \Q$
where $\Q$ denotes the set of rational numbers.


 
: 
:and if neither, a sixth binomial.
 ''
 ",Definition:Binomial (Euclidean),['Definitions/Euclidean Number Theory']
Definition:Binomial,Binomial,A binomial is an expression which has $2$ terms.,Definition:Binomial (Algebra),"['Definitions/Binomials (Algebra)', 'Definitions/Algebra']"
Definition:Binomial,Binomial,"=== Definition 1 ===
Let $n \in \Z_{\ge 0}$ and $k \in \Z$.

Then the binomial coefficient $\dbinom n k$ is defined as:

:$\dbinom n k = \begin {cases} \dfrac {n!} {k! \paren {n - k}!} & : 0 \le k \le n \\ & \\ 0 & : \text { otherwise } \end{cases}$

where $n!$ denotes the factorial of $n$.

=== Definition 2 ===
Let $n \in \Z_{\ge 0}$ and $k \in \Z$.

The number of different ways $k$ objects can be chosen (irrespective of order) from a set of $n$ objects is denoted:
:$\dbinom n k$

This number $\dbinom n k$ is known as a binomial coefficient.

=== Definition 3 ===
Let $n \in \Z_{\ge 0}$ and $k \in \Z$.

Then the binomial coefficient $\dbinom n k$ is defined as the coefficient of the term $a^k b^{n - k}$ in the expansion of $\paren {a + b}^n$.",Definition:Binomial Coefficient,['Definitions/Binomial Coefficients']
Definition:Binomial,Binomial,"Let $X$ be a discrete random variable on a probability space $\struct {\Omega, \Sigma, \Pr}$.


Then $X$ has the binomial distribution with parameters $n$ and $p$  if and only if :

:$\Img X = \set {0, 1, \ldots, n}$

:$\map \Pr {X = k} = \dbinom n k p^k \paren {1 - p}^{n - k}$

where $0 \le p \le 1$.


Note that the binomial distribution gives rise to a probability mass function satisfying $\map \Pr \Omega = 1$, because:
:$\ds \sum_{k \mathop \in \Z} \dbinom n k p^k \paren {1 - p}^{n - k} = \paren {p + \paren {1 - p} }^n = 1$

This is apparent from the Binomial Theorem.


It is written:
:$X \sim \Binomial n p$",Definition:Binomial Distribution,"['Definitions/Binomial Distribution', 'Definitions/Examples of Probability Distributions']"
Definition:Bottom,Bottom,"Let $\struct {S, \vee, \wedge, \preceq}$ be a lattice.


=== Definition 1 ===
Let $\struct {S, \vee, \wedge, \preceq}$ be a lattice.

Let $S$ admit a smallest element $\bot$.


Then $\bot$ is called the bottom of $S$.

=== Definition 2 ===
Let $\struct {S, \vee, \wedge, \preceq}$ be a lattice.

Let $\vee$ have an identity element $\bot$.


Then $\bot$ is called the bottom of $S$.",Definition:Bottom of Lattice,"['Definitions/Bottom of Lattice', 'Definitions/Lattice Theory']"
Definition:Bottom,Bottom,"Bottom is a constant of propositional logic interpreted to mean the canonical, undoubted contradiction whose falsehood nobody could possibly ever question.

The symbol used is $\bot$.",Definition:Bottom (Logic),['Definitions/Bottom']
Definition:Boundary,Boundary," 

For example, the endpoints of a line segment are its boundaries.


=== Containment ===
A geometric figure is said to be contained by its boundary or boundaries.

 ",Definition:Boundary (Geometry),['Definitions/Geometry']
Definition:Boundary,Boundary,"Let $T = \struct {S, \tau}$ be a topological space.

Let $H \subseteq S$.


=== Definition from Closure and Interior ===
Let $T = \struct {S, \tau}$ be a topological space.

Let $H \subseteq S$.


The boundary of $H$ consists of all the points in the closure of $H$ which are not in the interior of $H$.

Thus, the boundary of $H$ is defined as:
:$\partial H := H^- \setminus H^\circ$
where $H^-$ denotes the closure and $H^\circ$ the interior of $H$.

=== Definition from Neighborhood ===
Let $T = \struct {S, \tau}$ be a topological space.

Let $H \subseteq S$.


 

$x \in S$ is a boundary point of $H$  if and only if  every neighborhood $N$ of $x$ satisfies:
:$H \cap N \ne \O$
and
:$\overline H \cap N \ne \O$
where $\overline H$ is the complement of $H$ in $S$.

The boundary of $H$ consists of all the boundary point of $H$.

=== Definition from Intersection of Closure with Closure of Complement ===
Let $T = \struct {S, \tau}$ be a topological space.

Let $H \subseteq S$.


The boundary of $H$ is the intersection of the closure of $H$ with the closure of the complement of $H$ in $T$:

:$\partial H = H^- \cap \paren {\overline H}^-$

=== Definition from Closure and Exterior ===
Let $T = \struct {S, \tau}$ be a topological space.

Let $H \subseteq S$.


The boundary of $H$ consists of all the points in $H$ which are not in either the interior or exterior of $H$.

Thus, the boundary of $H$ is defined as:
:$\partial H := H \setminus \paren {H^\circ \cup H^e}$
where:
:$H^\circ$ denotes the interior of $H$
:$H^e$ denotes the exterior of $H$.",Definition:Boundary (Topology),"['Definitions/Set Boundaries', 'Definitions/Topology']"
Definition:Boundary,Boundary,"=== Simple Graph ===
Let $G = \tuple {V, E}$ be a simple graph.

Let $v \in V$ be a vertex of $G$.


Then the boundary of $v$ is the set of all vertices of $G$ which are adjacent to $v$:
:$\map B v = \set {u \in V: \set {u, v} \in E}$",Definition:Boundary (Graph Theory),"['Definitions/Graph Theory', 'Definitions/Boundaries (Graph Theory)']"
Definition:Bounded,Bounded,"Let $\struct {S, \preceq}$ be an ordered set.

Let $T \subseteq S$ be both bounded below and bounded above in $S$.


Then $T$ is bounded in $S$.


=== Subset of Real Numbers ===

The concept is usually encountered where $\struct {S, \preceq}$ is the set of real numbers under the usual ordering $\struct {\R, \le}$:

=== Definition 1 ===
Let $\R$ be the set of real numbers.


Let $T \subseteq \R$ be both bounded below and bounded above in $\R$.


Then $T$ is bounded in $\R$.

=== Definition 2 ===
Let $\R$ be the set of real numbers.


Let $T \subseteq \R$ be a subset of $\R$ such that:
:$\exists K \in \R: \forall x \in T: \size x \le K$
where $\size x$ denotes the absolute value of $x$.


Then $T$ is bounded in $\R$.",Definition:Bounded Set,"['Definitions/Bounded Sets', 'Definitions/Ordered Sets', 'Definitions/Boundedness']"
Definition:Bounded,Bounded,"=== Definition 1 ===
Let $\R$ be the set of real numbers.


Let $T \subseteq \R$ be both bounded below and bounded above in $\R$.


Then $T$ is bounded in $\R$.

=== Definition 2 ===
Let $\R$ be the set of real numbers.


Let $T \subseteq \R$ be a subset of $\R$ such that:
:$\exists K \in \R: \forall x \in T: \size x \le K$
where $\size x$ denotes the absolute value of $x$.


Then $T$ is bounded in $\R$.",Definition:Bounded Set/Real Numbers,"['Definitions/Bounded Sets of Real Numbers', 'Definitions/Bounded Sets', 'Definitions/Real Numbers']"
Definition:Bounded,Bounded,"Let $B$ be a class.


=== Bounded by Set ===
Let $B$ be a class.

Let $x$ be a set.

$B$ is bounded by $x$  if and only if :
:every element of $B$ is a subset of $x$.

=== Bounded Subset of Class ===
Let $B$ be a class.

Let $B$ be a subclass of a class $A$.

Then $B$ is a bounded subset of $A$  if and only if :
:there exists a set $x \in A$ such that $B$ is bounded by $x$ 


That is,  if and only if  every element of $B$ is a subset of $x$.",Definition:Bounded Class,['Definitions/Bounded Classes']
Definition:Bounded,Bounded,"Let $S$ be a set.

Let $\struct {T, \preceq}$ be an ordered set.

Let $f: S \to T$ be a mapping.

Let the image of $f$ be bounded.


Then $f$ is bounded.


That is, $f$ is bounded  if and only if  it is both bounded above and bounded below.


=== Real-Valued Function ===
Let $f: S \to \R$ be a real-valued function.


=== Definition 1 ===
Let $f: S \to \R$ be a real-valued function.


$f$ is bounded on $S$  if and only if :
:$f$ is bounded above on $S$
and also:
:$f$ is bounded below on $S$.

=== Definition 2 ===
Let $f: S \to \R$ be a real-valued function.


$f$ is bounded on $S$  if and only if :
:$\exists K \in \R_{\ge 0}: \forall x \in S: \size {\map f x} \le K$
where $\size {\map f x}$ denotes the absolute value of $\map f x$.

=== Definition 3 ===
Let $f: S \to \R$ be a real-valued function.


$f$ is bounded on $S$  if and only if :
:$\exists a, b \in \R_{\ge 0}: \forall x \in S: \map f x \in \closedint a b$
where $\closedint a b$ denotes the (closed) real interval from $a$ to $b$.

=== Function Attaining its Bounds ===
Let $f: S \to \R$ be a bounded real-valued function.

Let $T$ be a subset of $S$.

Suppose that:
:$\exists a, b \in T: \forall x \in S: \map f a \le \map f x \le \map f b$


Then $f$ attains its bounds on $T$.

=== Complex-Valued Function ===
Let $f: S \to \C$ be a complex-valued function.


Then $f$ is bounded  if and only if  the real-valued function $\cmod f: S \to \R$ is bounded, where $\cmod f$ is the modulus of $f$.


That is, $f$ is bounded if there is a constant $K \ge 0$ such that $\cmod {\map f z} \le K$ for all $z \in S$.


=== Unbounded ===
Let $f: S \to \C$ be a complex-valued function.


Then $f$ is unbounded  if and only if  $f$ is not bounded.


That is, $f$ is unbounded if there does not exist a constant $K \ge 0$ such that $\cmod {f \paren z} \le K$ for all $z \in S$.

=== Normed Division Ring ===
Let $\struct {R, \norm {\,\cdot\,}}$ be a normed division ring.

Let $f: S \to R$ be a mapping from $S$ into $R$.

Then $f$ is bounded  if and only if  the real-valued function $\norm {\,\cdot\,} \circ f: S \to \R$ is bounded, where $\norm {\,\cdot\,} \circ f$ is the composite of $\norm {\,\cdot\,}$ and $f$.


That is, $f$ is bounded if there is a constant $K \in \R_{\ge 0}$ such that $\norm{f \paren {s}} \le K$ for all $s \in S$.

=== Metric Space ===
Let $M$ be a metric space.

Let $f: X \to M$ be a mapping from any set $X$ into $M$.


Then $f$ is a bounded mapping  if and only if  $f \sqbrk X$ is bounded in $M$.

=== Normed Vector Space ===
Let $\struct {R, \norm {\, \cdot \,} }$ be a normed division ring.

Let $\struct {X, \norm {\, \cdot \,} }$ be a normed vector space over $R$. 

Let $S$ be a set.

Let $f : S \to X$ be a mapping.


We say that $f$ is bounded  if and only if  there exists a real number $M > 0$ such that:
:$\norm {\map f x} \le M$ for each $x \in S$.",Definition:Bounded Mapping,"['Definitions/Bounded Mappings', 'Definitions/Mappings', 'Definitions/Boundedness']"
Definition:Bounded,Bounded,"Let $f: S \to \R$ be a real-valued function.


=== Definition 1 ===
Let $f: S \to \R$ be a real-valued function.


$f$ is bounded on $S$  if and only if :
:$f$ is bounded above on $S$
and also:
:$f$ is bounded below on $S$.

=== Definition 2 ===
Let $f: S \to \R$ be a real-valued function.


$f$ is bounded on $S$  if and only if :
:$\exists K \in \R_{\ge 0}: \forall x \in S: \size {\map f x} \le K$
where $\size {\map f x}$ denotes the absolute value of $\map f x$.

=== Definition 3 ===
Let $f: S \to \R$ be a real-valued function.


$f$ is bounded on $S$  if and only if :
:$\exists a, b \in \R_{\ge 0}: \forall x \in S: \map f x \in \closedint a b$
where $\closedint a b$ denotes the (closed) real interval from $a$ to $b$.

=== Function Attaining its Bounds ===
Let $f: S \to \R$ be a bounded real-valued function.

Let $T$ be a subset of $S$.

Suppose that:
:$\exists a, b \in T: \forall x \in S: \map f a \le \map f x \le \map f b$


Then $f$ attains its bounds on $T$.",Definition:Bounded Mapping/Real-Valued,"['Definitions/Bounded Real-Valued Functions', 'Definitions/Real-Valued Functions', 'Definitions/Bounded Mappings']"
Definition:Bounded,Bounded,"A special case of a bounded mapping is a bounded sequence, where the domain of the mapping is $\N$.


Let $\struct {T, \preceq}$ be an ordered set.

Let $\sequence {x_n}$ be a sequence in $T$.


Then $\sequence {x_n}$ is bounded  if and only if  $\exists m, M \in T$ such that $\forall i \in \N$:
:$(1): \quad m \preceq x_i$
:$(2): \quad x_i \preceq M$


That is,  if and only if  it is bounded above and bounded below.


=== Real Sequence ===

The concept is usually encountered where $\struct {T, \preceq}$ is the set of real numbers under the usual ordering: $\struct {\R, \le}$:

Let $\sequence {x_n}$ be a real sequence.


Then $\sequence {x_n}$ is bounded  if and only if  $\exists m, M \in \R$ such that $\forall i \in \N$:
:$m \le x_i$
:$x_i \le M$


That is,  if and only if  it is bounded above and bounded below.


=== Unbounded ===
Let $\sequence {x_n}$ be a real sequence.


$\sequence {x_n}$ is unbounded  if and only if  it is not bounded.

=== Complex Sequence ===
Let $\sequence {z_n}$ be a complex sequence.


Then $\sequence {z_n}$ is bounded  if and only if :
:$\exists M \in \R$ such that $\forall i \in \N: \cmod {z_i} \le M$
where $\cmod {z_i}$ denotes the complex modulus of $z_i$.

=== Normed Division Ring ===
Let $\struct {R, \norm {\, \cdot \,} }$ be a normed division ring.

Let $\sequence {x_n}$ be a sequence in $R$.

Then $\sequence {x_n}$ is bounded  if and only if :
:$\exists K \in \R$ such that $\forall n \in \N: \norm {x_n} \le K$


=== Unbounded ===
Let $\struct {R, \norm {\, \cdot \,} }$ be a normed division ring.

Let $\sequence {x_n}$ be a sequence in $R$.


$\sequence {x_n}$ is unbounded  if and only if  it is not bounded.

=== Normed Vector Space ===
Let $\struct {X, \norm {\, \cdot \,} }$ be a normed vector space.

Let $\sequence {x_n}$ be a sequence in $X$.


Then $\sequence {x_n}$ is bounded  if and only if :
:$\exists K \in \R$ such that $\forall n \in \N: \norm {x_n} \le K$


=== Unbounded ===
Let $\struct {X, \norm {\, \cdot \,} }$ be a normed vector space.

Let $\sequence {x_n}$ be a sequence in $R$.


$\sequence {x_n}$ is unbounded  if and only if  it is not bounded.

=== Metric Space ===
Let $M$ be a metric space.

Let $\sequence {x_n}$ be a sequence in $M$.

Then $\sequence {x_n}$ is a bounded sequence  if and only if  $\sequence {x_n}$ is bounded in $M$.

That is:
:$\exists K \in \R: \forall n, m \in \N: \map d {x_n, x_m} \le K$


=== Unbounded ===
Let $M$ be a metric space.

Let $\sequence {x_n}$ be a sequence in $M$.


$\sequence {x_n}$ is unbounded  if and only if  it is not bounded.

Category:Definitions/Bounded Sequences
Category:Definitions/Metric Spaces",Definition:Bounded Sequence,"['Definitions/Bounded Sequences', 'Definitions/Boundedness', 'Definitions/Sequences']"
Definition:Bounded,Bounded,"Let $\sequence {x_n}$ be a real sequence.


Then $\sequence {x_n}$ is bounded  if and only if  $\exists m, M \in \R$ such that $\forall i \in \N$:
:$m \le x_i$
:$x_i \le M$


That is,  if and only if  it is bounded above and bounded below.


=== Unbounded ===
Let $\sequence {x_n}$ be a real sequence.


$\sequence {x_n}$ is unbounded  if and only if  it is not bounded.",Definition:Bounded Sequence/Real,"['Definitions/Bounded Real Sequences', 'Definitions/Bounded Sequences', 'Definitions/Real Sequences']"
Definition:Bounded,Bounded,"Let $D \subseteq \R^2$ be a subset of the plane.

$D$ is bounded  if and only if  there exists a circle in the plane which completely encloses $D$.",Definition:Bounded Region of Plane,['Definitions/Geometry']
Definition:Bounded,Bounded,"Let $M = \struct {A, d}$ be a metric space.

Let $M' = \struct {B, d_B}$ be a subspace of $M$.


=== Definition 1 ===
Let $M = \struct {A, d}$ be a metric space.

Let $M' = \struct {B, d_B}$ be a subspace of $M$.


$M'$ is bounded (in $M$)  if and only if :
:$\exists a \in A, K \in \R: \forall x \in B: \map {d} {x, a} \le K$

That is, there exists an element of $A$ within a finite distance of all elements of $B$.

=== Definition 2 ===
Let $M = \struct {A, d}$ be a metric space.

Let $M' = \struct {B, d_B}$ be a subspace of $M$.


$M'$ is bounded (in $M$)  if and only if :
:$\exists K \in \R: \forall x, y \in M': \map {d_B} {x, y} \le K$

That is, there exists a finite distance such that all pairs of elements of $B$ are within that distance.

=== Definition 3 ===
Let $M = \struct {A, d}$ be a metric space.

Let $M' = \struct {B, d_B}$ be a subspace of $M$.


$M'$ is bounded (in $M$)  if and only if :
:$\exists x \in A, \epsilon \in \R_{>0}: B \subseteq \map {B_\epsilon} x$

where $\map {B_\epsilon} x$ is the open $\epsilon$-ball of $x$.


That is, $M'$ can be fitted inside an open ball.

=== Definition 4 ===
Let $M = \struct {A, d}$ be a metric space.

Let $M' = \struct {B, d_B}$ be a subspace of $M$.

Let $a' \in A$.


$M'$ is bounded (in $M$)  if and only if :
:$\exists K \in \R: \forall x \in B: \map {d} {x, a'} \le K$",Definition:Bounded Metric Space,"['Definitions/Bounded Metric Spaces', 'Definitions/Metric Spaces']"
Definition:Bounded,Bounded,"Let $D$ be a subset of the complex plane $\C$.


Then $D$ is bounded (in $\C$)  if and only if  there exists $M \in \R$ such that:
: $\forall z \in D: \cmod z \le M$


=== Unbounded ===
Let $D$ be a subset of the complex plane $\C$.


Then $D$ is unbounded (in $\C$)  if and only if :
: $\nexists M \in \R: \forall z \in D: \cmod z \le M$

That is, if $D$ is not bounded in $\C$.",Definition:Bounded Metric Space/Complex,"['Definitions/Bounded Metric Spaces', 'Definitions/Complex Plane']"
Definition:Bounded,Bounded,"Let $\mathbb F \in \set {\R, \C}$.

Let $\struct {V, \tau}$ be a topological vector space over $\mathbb F$.


A subset $B \subseteq V$ is bounded  if and only if :
:for each $U \in \tau$ such that $\mathbf 0_V \in U$ there is an $\epsilon \in \R_{>0}$ such that:
::$\epsilon B \subseteq U$

where:
:$\bf 0_V$ denotes the zero vector of $V$
:$\epsilon B$ denotes the dilation of $B$ by $\epsilon$",Definition:Bounded Subset of Topological Vector Space,['Definitions/Topological Vector Spaces']
Definition:Bounded,Bounded,"Let $M = \struct {X, \norm {\, \cdot \,}}$ be a normed vector space.

Let $M' \subseteq X$. 


=== Definition 1 ===
Let $M = \struct {X, \norm {\, \cdot \,}}$ be a normed vector space.

Let $M' = \struct {Y, \norm {\, \cdot \,}_Y}$ be a subspace of $M$.


$M'$ is bounded (in $M$)  if and only if :
:$\exists x \in X, C \in \R_{> 0}: \forall y \in Y: \norm {x - y} \le C$

=== Definition 2 ===
Let $M = \struct {X, \norm {\, \cdot \,}}$ be a normed vector space.

Let $M' = \struct {Y, \norm {\, \cdot \,}_Y}$ be a subspace of $M$.


$M'$ is bounded (in $M$)  if and only if :
:$\exists \epsilon \in \R_{>0} : \exists x \in X : Y \subseteq \map {B_\epsilon^-} x$

where $\map {B_\epsilon^-} x$ is a closed ball in $M$.",Definition:Bounded Subset of Normed Vector Space,"['Definitions/Bounded Subset of Normed Vector Space', 'Definitions/Bounded Subsets of Normed Vector Spaces', 'Definitions/Normed Vector Spaces', 'Definitions/Bounded Subsets of Normed Vector Spaces']"
Definition:Bounded Above,Bounded Above,"Let $\struct {S, \preceq}$ be an ordered set.


A subset $T \subseteq S$ is bounded above (in $S$)  if and only if  $T$ admits an upper bound (in $S$).


=== Subset of Real Numbers ===

The concept is usually encountered where $\struct {S, \preceq}$ is the set of real numbers under the usual ordering $\struct {\R, \le}$:

Let $\R$ be the set of real numbers.

A subset $T \subseteq \R$ is bounded above (in $\R$)  if and only if  $T$ admits an upper bound (in $\R$).


=== Unbounded Above ===
Let $\R$ be the set of real numbers.

Let $T \subseteq \R$ be a subset of $\R$ .


$T \subseteq \R$ is unbounded above (in $\R$)  if and only if  it is not bounded above.

=== Unbounded Above ===
Let $\struct {S, \preceq}$ be an ordered set.


A subset $T \subseteq S$ is unbounded above (in $S$)  if and only if  it is not bounded above.",Definition:Bounded Above Set,"['Definitions/Bounded Above Sets', 'Definitions/Boundedness']"
Definition:Bounded Above,Bounded Above,"Let $\R$ be the set of real numbers.

A subset $T \subseteq \R$ is bounded above (in $\R$)  if and only if  $T$ admits an upper bound (in $\R$).


=== Unbounded Above ===
Let $\R$ be the set of real numbers.

Let $T \subseteq \R$ be a subset of $\R$ .


$T \subseteq \R$ is unbounded above (in $\R$)  if and only if  it is not bounded above.",Definition:Bounded Above Set/Real Numbers,"['Definitions/Bounded Above Sets of Real Numbers', 'Definitions/Bounded Above Sets', 'Definitions/Real Numbers']"
Definition:Bounded Above,Bounded Above,"Let $f: S \to T$ be a mapping whose codomain is an ordered set $\struct {T, \preceq}$.


Then $f$ is bounded above on $S$ by the upper bound $H$  if and only if :
:$\forall x \in S: \map f x \preceq H$


That is,  if and only if  $f \sqbrk S = \set {\map f x: x \in S}$ is bounded above by $H$.


=== Real-Valued Function ===

The concept is usually encountered where $\struct {T, \preceq}$ is the set of real numbers under the usual ordering $\struct {\R, \le}$:

Let $f: S \to \R$ be a real-valued function.


$f$ is bounded above on $S$ by the upper bound $H$  if and only if :
:$\forall x \in S: \map f x \le H$


That is,  if and only if  the set $\set {\map f x: x \in S}$ is bounded above in $\R$ by $H$.",Definition:Bounded Above Mapping,"['Definitions/Bounded Above Mappings', 'Definitions/Mappings', 'Definitions/Boundedness']"
Definition:Bounded Above,Bounded Above,"Let $f: S \to \R$ be a real-valued function.


$f$ is bounded above on $S$ by the upper bound $H$  if and only if :
:$\forall x \in S: \map f x \le H$


That is,  if and only if  the set $\set {\map f x: x \in S}$ is bounded above in $\R$ by $H$.",Definition:Bounded Above Mapping/Real-Valued,"['Definitions/Bounded Above Real-Valued Functions', 'Definitions/Real-Valued Functions', 'Definitions/Bounded Above Mappings']"
Definition:Bounded Above,Bounded Above,"Let $\struct {T, \preceq}$ be an ordered set.

Let $\sequence {x_n}$ be a sequence in $T$.


Then $\sequence {x_n}$ is bounded above  if and only if :
:$\exists M \in T: \forall i \in \N: x_i \preceq M$


=== Real Sequence ===

The concept is usually encountered where $\struct {T, \preceq}$ is the set of real numbers under the usual ordering $\struct {\R, \le}$:

Let $\sequence {x_n}$ be a real sequence.


Then $\sequence {x_n}$ is bounded above  if and only if :
:$\exists M \in \R: \forall i \in \N: x_i \le M$",Definition:Bounded Above Sequence,"['Definitions/Bounded Above Sequences', 'Definitions/Boundedness', 'Definitions/Sequences']"
Definition:Bounded Above,Bounded Above,"Let $\sequence {x_n}$ be a real sequence.


Then $\sequence {x_n}$ is bounded above  if and only if :
:$\exists M \in \R: \forall i \in \N: x_i \le M$",Definition:Bounded Above Sequence/Real,"['Definitions/Bounded Above Real Sequences', 'Definitions/Bounded Above Sequences', 'Definitions/Real Sequences']"
Definition:Bounded Below,Bounded Below,"Let $\struct {S, \preceq}$ be an ordered set.


A subset $T \subseteq S$ is bounded below (in $S$)  if and only if  $T$ admits a lower bound (in $S$).


=== Subset of Real Numbers ===

The concept is usually encountered where $\struct {S, \preceq}$ is the set of real numbers under the usual ordering $\struct {\R, \le}$:

Let $\R$ be the set of real numbers.

A subset $T \subseteq \R$ is bounded below (in $\R$)  if and only if  $T$ admits a lower bound (in $\R$).


=== Unbounded Below ===
Let $\R$ be the set of real numbers.

Let $T \subseteq \R$ be a subset of $\R$ .


$T \subseteq \R$ is unbounded below (in $\R$)  if and only if  it is not bounded below.",Definition:Bounded Below Set,"['Definitions/Bounded Below Sets', 'Definitions/Boundedness']"
Definition:Bounded Below,Bounded Below,"Let $\R$ be the set of real numbers.

A subset $T \subseteq \R$ is bounded below (in $\R$)  if and only if  $T$ admits a lower bound (in $\R$).


=== Unbounded Below ===
Let $\R$ be the set of real numbers.

Let $T \subseteq \R$ be a subset of $\R$ .


$T \subseteq \R$ is unbounded below (in $\R$)  if and only if  it is not bounded below.",Definition:Bounded Below Set/Real Numbers,"['Definitions/Bounded Below Sets of Real Numbers', 'Definitions/Bounded Below Sets', 'Definitions/Real Numbers']"
Definition:Bounded Below,Bounded Below,"Let $f: S \to T$ be a mapping whose codomain is an ordered set $\struct {T, \preceq}$.


Then $f$ is said to be bounded below (in $T$) by the lower bound $L$  if and only if :
:$\forall x \in S: L \preceq \map f x$


That is, iff $f \sqbrk S = \set {\map f x: x \in S}$ is bounded below by $L$.


=== Real-Valued Function ===

The concept is usually encountered where $\struct {T, \preceq}$ is the set of real numbers under the usual ordering $\struct {\R, \le}$:

Let $f: S \to \R$ be a real-valued function.


Then $f$ is bounded below on $S$ by the lower bound $L$  if and only if :
:$\forall x \in S: L \le \map f x$


That is,  if and only if  the set $\set {\map f x: x \in S}$ is bounded below in $\R$ by $L$.",Definition:Bounded Below Mapping,"['Definitions/Bounded Below Mappings', 'Definitions/Mappings', 'Definitions/Boundedness']"
Definition:Bounded Below,Bounded Below,"Let $f: S \to \R$ be a real-valued function.


Then $f$ is bounded below on $S$ by the lower bound $L$  if and only if :
:$\forall x \in S: L \le \map f x$


That is,  if and only if  the set $\set {\map f x: x \in S}$ is bounded below in $\R$ by $L$.",Definition:Bounded Below Mapping/Real-Valued,"['Definitions/Bounded Below Real-Valued Functions', 'Definitions/Real-Valued Functions', 'Definitions/Bounded Below Mappings']"
Definition:Bounded Below,Bounded Below,"Let $\struct {T, \preceq}$ be an ordered set.

Let $\sequence {x_n}$ be a sequence in $T$.


Then $\sequence {x_n}$ is bounded below  if and only if :
:$\exists m \in T: \forall i \in \N: m \preceq x_i$


=== Real Sequence ===

The concept is usually encountered where $\struct {T, \preceq}$ is the set of real numbers under the usual ordering $\struct {\R, \le}$:

Let $\sequence {x_n}$ be a real sequence.


Then $\sequence {x_n}$ is bounded below  if and only if :
:$\exists m \in \R: \forall i \in \N: m \le x_i$


=== Unbounded Below ===
Let $\sequence {x_n}$ be a real sequence.


$\sequence {x_n}$ is unbounded below  if and only if  there exists no $m$ in $\R$ such that:
:$\forall i \in \N: m \le x_i$


Category:Definitions/Unbounded Below Sequences
Category:Definitions/Real Sequences",Definition:Bounded Below Sequence,"['Definitions/Bounded Below Sequences', 'Definitions/Boundedness', 'Definitions/Sequences']"
Definition:Bounded Below,Bounded Below,"Let $\sequence {x_n}$ be a real sequence.


Then $\sequence {x_n}$ is bounded below  if and only if :
:$\exists m \in \R: \forall i \in \N: m \le x_i$


=== Unbounded Below ===
Let $\sequence {x_n}$ be a real sequence.


$\sequence {x_n}$ is unbounded below  if and only if  there exists no $m$ in $\R$ such that:
:$\forall i \in \N: m \le x_i$


Category:Definitions/Unbounded Below Sequences
Category:Definitions/Real Sequences",Definition:Bounded Below Sequence/Real,"['Definitions/Bounded Below Real Sequences', 'Definitions/Bounded Below Sequences', 'Definitions/Real Sequences']"
Definition:Branch,Branch,"Let $T$ be a rooted tree with root node $r_T$.

A subset $\Gamma$ of $T$ is a branch  if and only if  all the following conditions hold:
:$(1): \quad$ The root node $r_T$ belongs to $\Gamma$
:$(2): \quad$ The parent of each node in $\Gamma \setminus \set {r_T}$ is in $\Gamma$
:$(3): \quad$ Each node in $\Gamma$ either:
::$\text {(a)}: \quad$ is a leaf node of $T$
:or:
::$\text {(b)}: \quad$ has exactly one child node in $\Gamma$.


Informally, a branch of a rooted tree is the path from the root to a leaf.


=== Finite ===
Let $T$ be a rooted tree with root node $r_T$.

Let $\Gamma$ be a branch of $T$.


Then $\Gamma$ is finite  if and only if  it has exactly one leaf node.

=== Infinite ===
Let $T$ be a rooted tree with root node $r_T$.

Let $\Gamma$ be a branch of $T$.


Then $\Gamma$ is infinite  if and only if  it has no leaf node at the end.",Definition:Rooted Tree/Branch,['Definitions/Rooted Trees']
Definition:Branch,Branch,"Let $\struct {T, \preceq}$ be a tree.

A branch of $\struct {T, \preceq}$ is a maximal chain in $\struct {T, \preceq}$.


Category:Definitions/Set Theory",Definition:Tree (Set Theory)/Branch,"['Definitions/Set Theory', 'Definitions/Set Theory']"
Definition:Branch,Branch,"Let $A$ and $B$ be sets.

Let $f: A \to B$ be a multifunction on $A$.

Let $\family {S_i}_{i \mathop \in I}$ be a partitioning of the codomain of $f$ such that:
:$\forall i \in I: f \restriction_{A \times S_i}$ is a mapping.


Then each $f \restriction_{A \times S_i}$ is a branch of $f$.


=== Principal Branch ===
Let $A$ and $B$ be sets.

Let $f: A \to B$ be a multifunction on $A$.

Let $\sequence {S_i}_{i \mathop \in I}$ be a partitioning of the codomain of $f$ into branches.


It is usual to distinguish one such branch of $f$ from the others, and label it the principal branch of $f$.


=== Principal Value ===
Let $A$ and $B$ be sets.

Let $f: A \to B$ be a multifunction on $A$.

Let $x \in A$ be an element of the domain of $f$.

The principal value of $x$ is the element $y$ of the principal branch of $f$ such that $\map f x = y$.

=== Principal Value ===
Let $A$ and $B$ be sets.

Let $f: A \to B$ be a multifunction on $A$.

Let $x \in A$ be an element of the domain of $f$.

The principal value of $x$ is the element $y$ of the principal branch of $f$ such that $\map f x = y$.

=== Branch Point ===
Let $U \subseteq \C$ be an open set.

Let $f : U \to \C$ be a complex multifunction.


A branch point of $f$ is a point $a$ in $U$ such that:

:$f$ has more than one value at one or more points in every neighborhood of $a$
:$f$ has exactly one value at $a$ itself.",Definition:Multifunction/Branch,['Definitions/Multifunctions']
Definition:Branch,Branch,"Let $U \subseteq \C$ be an open set.

Let $f : U \to \C$ be a complex multifunction.


A branch point of $f$ is a point $a$ in $U$ such that:

:$f$ has more than one value at one or more points in every neighborhood of $a$
:$f$ has exactly one value at $a$ itself.",Definition:Branch Point,"['Definitions/Branch Points', 'Definitions/Singular Points', 'Definitions/Complex Analysis']"
Definition:Branch,Branch,A branch of a curve $\CC$ is a part of $\CC$ which is separated from another part of $\CC$ by a discontinuity or a singular point.,Definition:Branch of Curve,"['Definitions/Branches of Curves', 'Definitions/Analytic Geometry']"
Definition:Bridge,Bridge,"Let $G = \struct {V, E}$ be a connected graph.

Let $e \in E$ be an edge of $G$ such that the edge deletion $G - e$ is disconnected.


Then $e$ is known as a bridge of $G$.",Definition:Bridge (Graph Theory),['Definitions/Graph Theory']
Definition:Bridge,Bridge,"Bridge is a game for $4$ players whose mechanism depends on the fact that each of the $4$ players are dealt a hand of $13$ cards from the standard deck of $52$ cards.


For the purposes of   at its current stage of evolution, details of the play of Bridge are not immediately relevant.

As and when a deeper analysis of Bridge becomes appropriate, further details can be incorporated.",Definition:Bridge (Game),"['Definitions/Bridge (Game)', 'Definitions/Examples of Games']"
Definition:Cancellable,Cancellable,"Let $\struct {S, \circ}$ be an algebraic structure.

An element $x \in \struct {S, \circ}$ is cancellable  if and only if :
:$\forall a, b \in S: x \circ a = x \circ b \implies a = b$
:$\forall a, b \in S: a \circ x = b \circ x \implies a = b$


That is,  if and only if  it is both left cancellable and right cancellable.


=== Left Cancellable ===
Let $\struct {S, \circ}$ be an algebraic structure.


An element $x \in \struct {S, \circ}$ is left cancellable  if and only if :

:$\forall a, b \in S: x \circ a = x \circ b \implies a = b$

=== Right Cancellable ===
Let $\struct {S, \circ}$ be an algebraic structure.


An element $x \in \struct {S, \circ}$ is right cancellable  if and only if :

:$\forall a, b \in S: a \circ x = b \circ x \implies a = b$",Definition:Cancellable Element,"['Definitions/Abstract Algebra', 'Definitions/Cancellability']"
Definition:Cancellable,Cancellable,"Let $\struct {S, \circ}$ be an algebraic structure.


The operation $\circ$ in $\struct {S, \circ}$ is cancellable  if and only if :
:$\forall a, b, c \in S: a \circ b = a \circ c \implies b = c$
:$\forall a, b, c \in S: a \circ c = b \circ c \implies a = b$


That is,  if and only if  it is both a left cancellable operation and a right cancellable operation.


=== Left Cancellable Operation ===
Let $\struct {S, \circ}$ be an algebraic structure.


The operation $\circ$ in $\struct {S, \circ}$ is left cancellable  if and only if :
:$\forall a, b, c \in S: a \circ b = a \circ c \implies b = c$

That is,  if and only if  all elements of $\struct {S, \circ}$ are left cancellable.

=== Right Cancellable Operation ===
Let $\struct {S, \circ}$ be an algebraic structure.


The operation $\circ$ in $\struct {S, \circ}$ is right cancellable  if and only if :
:$\forall a, b, c \in S: a \circ c = b \circ c \implies a = b$

That is,  if and only if  all elements of $\struct {S, \circ}$ are right cancellable.",Definition:Cancellable Operation,"['Definitions/Abstract Algebra', 'Definitions/Cancellability']"
Definition:Canonical,Canonical,"Let $r \in \Q$ be a rational number.

The canonical form of $r$ is the expression $\dfrac p q$, where:
:$r = \dfrac p q: p \in \Z, q \in \Z_{>0}, p \perp q$
where $p \perp q$ denotes that $p$ and $q$ have no common divisor except $1$.


That is, in its canonical form, $r$ is expressed as $\dfrac p q$ where:

:$p$ is an integer
:$q$ is a strictly positive integer
:$p$ and $q$ are coprime.",Definition:Rational Number/Canonical Form,"['Definitions/Canonical Form of Rational Number', 'Definitions/Rational Numbers', 'Definitions/Fractions', 'Definitions/Canonical Forms']"
Definition:Canonical,Canonical,"The canonical form of the quadratic equation is:
:$a x^2 + b x + c = 0$
where $a$, $b$ and $c$ are constants.",Definition:Quadratic Equation/Canonical Form,"['Definitions/Quadratic Equations', 'Definitions/Canonical Forms']"
Definition:Canonical,Canonical,"Let $F$ be a field, such as the field of real numbers $\R$.


=== Finite Continued Fraction ===
Let $F$ be a field, such as the field of real numbers $\R$.

Let $n \ge 0$ be a natural number.


Informally, a finite continued fraction of length $n$ in $F$ is an expression of the form:
:$a_0 + \cfrac 1 {a_1 + \cfrac 1 {a_2 + \cfrac 1 {\ddots \cfrac {} {a_{n - 1} + \cfrac 1 {a_n} } } } }$
where $a_0, a_1, a_2, \ldots, a_n \in F$.


Formally, a finite continued fraction of length $n$ in $F$ is a finite sequence, called sequence of partial denominators, whose domain is the integer interval $\closedint 0 n$.


A finite continued fraction should not be confused with its value, when it exists.

=== Infinite Continued Fraction ===
Let $F$ be a field, such as the field of real numbers $\R$.


Informally, an infinite continued fraction in $F$ is an expression of the form:

:$a_0 + \cfrac 1 {a_1 + \cfrac 1 {a_2 + \cfrac 1 {\ddots \cfrac {} {a_{n-1} + \cfrac 1 {a_n + \cfrac 1 {\ddots}}} }}}$
where $a_0, a_1, a_2, \ldots, a_n, \ldots \in F$.


Formally, an infinite continued fraction in $F$ is a sequence, called a sequence of partial denominators, whose domain is $\N_{\ge 0}$.


An infinite continued fraction should not be confused with its value, when it exists.",Definition:Continued Fraction,['Definitions/Continued Fractions']
Definition:Canonical,Canonical,A canonical form of a matrix is a form which all of a certain class of matrix can be reduced by transformations of a standard kind.,Definition:Canonical Form of Matrix,"['Definitions/Canonical Forms', 'Definitions/Matrices']"
Definition:Canonical,Canonical,A canonical form of a mathematical object is a standard way of presenting that object as a mathematical expression.,Definition:Canonical Form,"['Definitions/Canonical Forms', 'Definitions/Mathematics', 'Definitions/Computer Science']"
Definition:Chain,Chain,"Let $\struct {S, \preceq}$ be an ordered set.


A chain in $S$ is a totally ordered subset of $S$.


Thus a totally ordered set is itself a chain in its own right.


=== Chain of Sets ===

An important special case of a chain is where the ordering in question is the subset relation:

Let $S$ be a set.

Let $\powerset S$ be its power set.

Let $N \subseteq \powerset S$ be a subset of $\powerset S$.


Then $N$ is a chain (of sets)  if and only if :

:$\forall X, Y \in N: X \subseteq Y$ or $Y \subseteq X$",Definition:Chain (Order Theory),"['Definitions/Chains (Order Theory)', 'Definitions/Nests', 'Definitions/Order Theory']"
Definition:Chain,Chain,"Let $S$ be a set.

Let $\powerset S$ be its power set.

Let $N \subseteq \powerset S$ be a subset of $\powerset S$.


Then $N$ is a chain (of sets)  if and only if :

:$\forall X, Y \in N: X \subseteq Y$ or $Y \subseteq X$",Definition:Chain (Order Theory)/Subset Relation,"['Definitions/Chains (Order Theory)', 'Definitions/Nests', 'Definitions/Order Theory']"
Definition:Chain,Chain,"A chain is an inelastic thread whose stiffness and width are approximated to zero.

The mass of a chain is usually defined in terms of linear mass density.


Category:Definitions/Ideals in Physics",Definition:Chain (Physics),['Definitions/Ideals in Physics']
Definition:Chain,Chain,"The chain is an imperial unit of length.

 
 
 
 
 
 
 
 
 ",Definition:Imperial/Length/Chain,['Definitions/Chain (Linear Measure)']
Definition:Chain,Chain,"Let $m$ be a positive integer.

Let $\map s m$ be the aliquot sum of $m$.


Define the sequence $\sequence {a_k}$ recursively as:
:$a_{k + 1} = \begin{cases} m & : k = 0 \\ \map s {a_k} & : k > 0 \end{cases}$


A sociable chain is such a sequence $\sequence {a_k}$ where:
:$a_r = a_0$
for some $r > 0$.


=== Order of Sociable Chain ===
Let $m$ be a positive integer.

Let $s \left({m}\right)$ be the aliquot sum of $m$.


Let a sequence $\left\langle{a_k}\right\rangle$ be a sociable chain.

The order of $a_k$ is the smallest $r \in \Z_{>0}$ such that
:$a_r = a_0$


Category:Definitions/Sociable Numbers",Definition:Sociable Chain,"['Definitions/Number Theory', 'Definitions/Recreational Mathematics', 'Definitions/Aliquot Sequences', 'Definitions/Sociable Numbers']"
Definition:Character,Character,"Let $\struct {G, +}$ be a finite abelian group.

Let $\struct {\C_{\ne 0}, \times}$ be the multiplicative group of complex numbers.


A character of $G$ is a group homomorphism:

:$\chi: G \to \C_{\ne 0}$",Definition:Character (Number Theory),['Definitions/Analytic Number Theory']
Definition:Character,Character,"Let $q \in \Z_{>1}$.

Let $\paren {\Z / q \Z}$ denote the ring of integers modulo $q$.

Let $G = \paren {\Z / q \Z}^\times$ be the group of units of $\paren {\Z / q \Z}$.

Let $\C^\times$ be the group of units of $\C$.

A Dirichlet character modulo $q$ is a group homomorphism:
:$\chi: G \to \C^\times$


 

By Reduced Residue System under Multiplication forms Abelian Group, $a + q \Z \in G$  if and only if  $\map \gcd {a, q} = 1$.

It is standard practice to extend $\chi$ to a function on $\Z$ by setting:

:$\map \chi a = \begin{cases}
\map \chi {a + q \Z} & : \map \gcd {a, q} = 1 \\
0 & : \text{otherwise}
\end{cases}$

 


=== Trivial Character ===
 

=== Primitive Character ===
Let $q \in \Z_{>1}$.

Let $\paren {\Z / q \Z}$ denote the ring of integers modulo $q$.

Let $G = \paren {\Z / q \Z}^\times$ be the group of units of $\paren {\Z / q \Z}$.

Let $\C^\times$ be the group of units of $\C$.


Let $\chi_0$ be the trivial (Dirichlet) character modulo $q$.

Let $q^*$ be the least divisor of $q$ such that:
:$\chi = \chi_0 \chi^*$
where $\chi^*$ is some character modulo $q^*$.

If $q = q^*$ then $\chi$ is called primitive, otherwise $\chi$ is imprimitive.

 

Category:Definitions/Dirichlet Characters",Definition:Dirichlet Character,"['Definitions/Analytic Number Theory', 'Definitions/Dirichlet Characters']"
Definition:Character,Character,"Let $p$ be an odd prime.

Let $a \in \Z$ be an integer such that $a \not \equiv 0 \pmod p$.


$a$ is either a quadratic residue or a quadratic non-residue of $p$.

Whether it is or not is known as the quadratic character of $a$ modulo $p$.",Definition:Quadratic Residue/Character,['Definitions/Quadratic Residues']
Definition:Character,Character,"Let $\struct {G, \cdot}$ be a finite group.

Let $V$ be a finite dimensional  $k$-vector space.

Consider a linear representation $\rho: G \to \GL V$ of $G$.

 

Let $\map \tr {\map \rho g}$ denote the trace of $\map \rho g$.


The character associated with $\rho$ is defined as:
:$\chi: G \to k$
where $\map \chi g = \map \tr {\map \rho g}$, the trace of $\map \rho g$; which is a linear automorphism of $V$.

 ",Definition:Character (Representation Theory),['Definitions/Representation Theory']
Definition:Character,Character,"Let $T$ be a topological space.


The character of (the topological space) $T$ is the supremum of the set of characters of the points of $T$:
:$\chi \left({T}\right) := \sup \left\{ {\chi \left({x, T}\right): x \in T}\right\}$",Definition:Character of Topological Space,['Definitions/Topology']
Definition:Character,Character,"Let $T$ be a topological space.

Let $x$ be a point of $T$.

Let $\map {\mathbb B} x$ be the set of all local bases at $x$.


The character of (the point) $x$ in $T$ is the smallest cardinality of the elements of $\map {\mathbb B} x$:
:$\map \chi {x, T} := \min \set {\card \BB: \BB \in \map {\mathbb B} x}$",Definition:Character of Point in Topological Space,['Definitions/Topology']
Definition:Character,Character,"Let $\struct {A, \norm {\, \cdot \,} }$ be a Banach algebra over $\C$.

Let $\phi : A \to \C$ be a non-zero algebra homomorphism on $A$.


We say that $\phi$ is a character on $A$.",Definition:Character (Banach Algebra),['Definitions/Banach Algebras']
Definition:Characteristic,Characteristic,"Let $\struct {R, +, \circ}$ be a ring with unity whose zero is $0_R$ and whose unity is $1_R$.


=== Definition 1 ===
Let $\struct {R, +, \circ}$ be a ring with unity whose zero is $0_R$ and whose unity is $1_R$.

For a natural number $n \in \N$, let $n \cdot x$ be defined as the power of $x$ in the context of the additive group $\struct {R, +}$:

:$n \cdot x = \begin {cases}
0_R & : n = 0 \\
\paren {\paren {n - 1} \cdot x} + x & : n > 0
\end {cases}$


The characteristic $\Char R$ of $R$ is the smallest $n \in \N_{>0}$ such that $n \cdot 1_R = 0_R$.

If there is no such $n$, then $\Char R = 0$.

=== Definition 2 ===
Let $\struct {R, +, \circ}$ be a ring with unity whose zero is $0_R$ and whose unity is $1_R$.

Let $g: \Z \to R$ be the initial homomorphism, with $\map g n = n \cdot 1_R$.

Let $\ideal p$ be the principal ideal of $\struct {\Z, +, \times}$ generated by $p$.


The characteristic $\Char R$ of $R$ is the positive integer $p \in \Z_{\ge 0}$ such that $\ideal p$ is the kernel of $g$.

=== Definition 3 ===
Let $\struct {R, +, \circ}$ be a ring with unity whose zero is $0_R$ and whose unity is $1_R$.


The characteristic of $R$, denoted $\Char R$, is defined as follows.

Let $p$ be the order of $1_R$ in the additive group $\struct {R, +}$ of $\struct {R, +, \circ}$.

If $p \in \Z_{>0}$, then $\Char R := p$.

If $1_R$ is of infinite order, then $\Char R := 0$.",Definition:Characteristic of Ring,"['Definitions/Characteristics of Rings', 'Definitions/Ring Theory']"
Definition:Characteristic,Characteristic,"As a field is a fortiori a ring, the definition of characteristic carries over directly from that of the characteristic of a ring :

Let $\struct {R, +, \circ}$ be a ring with unity whose zero is $0_R$ and whose unity is $1_R$.


=== Definition 1 ===
Let $\struct {R, +, \circ}$ be a ring with unity whose zero is $0_R$ and whose unity is $1_R$.

For a natural number $n \in \N$, let $n \cdot x$ be defined as the power of $x$ in the context of the additive group $\struct {R, +}$:

:$n \cdot x = \begin {cases}
0_R & : n = 0 \\
\paren {\paren {n - 1} \cdot x} + x & : n > 0
\end {cases}$


The characteristic $\Char R$ of $R$ is the smallest $n \in \N_{>0}$ such that $n \cdot 1_R = 0_R$.

If there is no such $n$, then $\Char R = 0$.

=== Definition 2 ===
Let $\struct {R, +, \circ}$ be a ring with unity whose zero is $0_R$ and whose unity is $1_R$.

Let $g: \Z \to R$ be the initial homomorphism, with $\map g n = n \cdot 1_R$.

Let $\ideal p$ be the principal ideal of $\struct {\Z, +, \times}$ generated by $p$.


The characteristic $\Char R$ of $R$ is the positive integer $p \in \Z_{\ge 0}$ such that $\ideal p$ is the kernel of $g$.

=== Definition 3 ===
Let $\struct {R, +, \circ}$ be a ring with unity whose zero is $0_R$ and whose unity is $1_R$.


The characteristic of $R$, denoted $\Char R$, is defined as follows.

Let $p$ be the order of $1_R$ in the additive group $\struct {R, +}$ of $\struct {R, +, \circ}$.

If $p \in \Z_{>0}$, then $\Char R := p$.

If $1_R$ is of infinite order, then $\Char R := 0$.",Definition:Characteristic of Field,"['Definitions/Characteristics of Fields', 'Definitions/Field Theory']"
Definition:Characteristic,Characteristic,"Let $G$ be a group.

Let $H$ be a subgroup such that:
:$\forall \phi \in \Aut G: \phi \sqbrk H = H$
where $\Aut G$ is the automorphism group of $G$.


Then $H$ is  characteristic (in $G$), or a characteristic subgroup of $G$.",Definition:Characteristic Subgroup,['Definitions/Subgroups']
Definition:Characteristic,Characteristic,"Let $R$ be a commutative ring with unity.

Let $\mathbf A$ be a square matrix over $R$ of order $n > 0$.

Let $\mathbf I_n$ be the $n \times n$ identity matrix.

Let $R \sqbrk x$ be the polynomial ring in one variable over $R$.


The characteristic matrix of $\mathbf A$ over $R \sqbrk x$ is the square matrix:
:$\mathbf I_n x - \mathbf A$",Definition:Characteristic Matrix,"['Definitions/Characteristic Matrices', 'Definitions/Matrices']"
Definition:Characteristic,Characteristic,"A characteristic polynomial is a member of the class of polynomials which in some way allows one to sum up a number of characteristics of a particular mathematical object.


 

=== Matrix ===
Let $R$ be a commutative ring with unity.

Let $\mathbf A$ be a square matrix over $R$ of order $n > 0$.

Let $\mathbf I_n$ be the $n \times n$ identity matrix.

Let $R \sqbrk x$ be the polynomial ring in one variable over $R$.


The characteristic polynomial of $\mathbf A$ is the determinant of the characteristic matrix of $\mathbf A$ over $R \sqbrk x$:
:$\map {p_{\mathbf A} } x = \map \det {\mathbf I_n x - \mathbf A}$

=== Linear Operator ===

Let $A$ be a commutative ring with unity.

Let $M$ be a free module over $A$ of finite rank $n > 0$.

Let $\phi : M \to M$ be a linear operator.

Let $A$ be a commutative ring with unity.

Let $M$ be a free module over $A$ of finite rank $n > 0$.

Let $\phi : M \to M$ be a linear operator.


The characteristic polynomial of $\phi$ is the characteristic polynomial of the relative matrix of $\phi$ with respect to a basis of $M$.

=== Field Extension ===
Let $K$ be a field.

Let $L / K$ be a finite field extension of $K$.

Then by Vector Space on Field Extension is Vector Space, $L$ is naturally a vector space over $K$.

Let $\alpha \in L$, and $\theta_\alpha$ be the linear operator:

:$\theta_\alpha: L \to L : \beta \mapsto \alpha \beta$


The characteristic polynomial of $\alpha$ with respect to the extension $L / K$ is:
:$\det \sqbrk {X I_L - \theta_\alpha}$

where:
:$\det$ denotes the determinant of a linear operator
:$X$ is an indeterminate 
:$I_L$ is the identity mapping on $L$.

=== Element of Algebra ===
Let $A$ be a commutative ring with unity.

Let $B$ be an algebra over $A$ such that $B$ is a finite-dimensional free module over $A$.

Let $b \in B$.


The characteristic polynomial of $b$ is the characteristic polynomial of the regular representation $\lambda_b : B \to B$ over $A$.

Category:Definitions/Characteristic Polynomials",Definition:Characteristic Polynomial,['Definitions/Characteristic Polynomials']
Definition:Characteristic,Characteristic,"Let $R$ be a commutative ring with unity.

Let $\mathbf A$ be a square matrix over $R$ of order $n > 0$.

Let $\mathbf I_n$ be the $n \times n$ identity matrix.

Let $R \sqbrk x$ be the polynomial ring in one variable over $R$.


The characteristic equation of $\mathbf A$ is the equation defined as: determinant of the characteristic matrix of $\mathbf A$ over $R \sqbrk x$:
:$\map \det {\mathbf I_n x - \mathbf A} = 0$
where $\map \det {\mathbf I_n x - \mathbf A}$ is the characteristic polynomial of the characteristic matrix of $\mathbf A$ over $R \sqbrk x$.",Definition:Characteristic Equation of Matrix,"['Definitions/Characteristic Equations', 'Definitions/Polynomial Theory', 'Definitions/Matrix Algebra', 'Definitions/Linear Algebra']"
Definition:Characteristic,Characteristic,"Let:
:$(1): \quad y + p y' + q y = 0$
be a constant coefficient homogeneous linear second order ODE.


The auxiliary equation of $(1)$ is the quadratic equation:
:$m^2 + p m + q = 0$",Definition:Auxiliary Equation,"['Definitions/Auxiliary Equations', 'Definitions/Second Order ODEs']"
Definition:Characteristic,Characteristic,"=== Set ===
Let $E \subseteq S$.

The characteristic function of $E$ is the function $\chi_E: S \to \set {0, 1}$ defined as:
:$\map {\chi_E} x = \begin {cases} 1 & : x \in E  \\  0 & : x \notin E \end {cases}$

That is:
:$\map {\chi_E} x = \begin {cases} 1 & : x \in E  \\ 0 & : x \in \relcomp S E \end {cases}$
where $\relcomp S E$ denotes the complement of $E$ relative to $S$.


=== Support ===
Let $S$ be a set

Let $E \subseteq S$ be a subset.

Let $\chi_E: S \to \set {0, 1}$ be the characteristic function of $E$.


The support of $\chi_E$, denoted $\map \supp {\chi_E}$, is the set $E$.

That is:

:$\map \supp {\chi_E} = \set {x \in S: \map {\chi_E} x = 1}$

=== Relation ===
The concept of a characteristic function of a subset carries over directly to relations.


Let $\RR \subseteq S \times T$ be a relation.

The characteristic function of $\RR$ is the mapping $\chi_\RR: S \times T \to \set {0, 1}$ defined as:
:$\map {\chi_\RR} {x, y} = \begin {cases} 1 & : \tuple {x, y} \in \RR \\ 0 & : \tuple {x, y} \notin \RR \end{cases}$


It can be expressed in Iverson bracket notation as:
:$\map {\chi_\RR} {x, y} = \sqbrk {\tuple {x, y} \in \RR}$


More generally, let $\ds \mathbb S = \prod_{i \mathop = 1}^n S_i = S_1 \times S_2 \times \ldots \times S_n$ be the cartesian product of $n$ sets $S_1, S_2, \ldots, S_n$.

Let $\RR \subseteq \mathbb S$ be an $n$-ary relation on $\mathbb S$.

The characteristic function of $\RR$ is the mapping $\chi_\RR: \mathbb S \to \set {0, 1}$ defined as:
:$\map {\chi_\RR} {s_1, s_2, \ldots, s_n} = \begin {cases} 1 & : \tuple {s_1, s_2, \ldots, s_n} \in \RR \\ 0 & : \tuple {s_1, s_2, \ldots, s_n} \notin \RR \end {cases}$


It can be expressed in Iverson bracket notation as:
:$\map {\chi_\RR} {s_1, s_2, \ldots, s_n} = \sqbrk {\tuple {s_1, s_2, \ldots, s_n} \in \RR}$",Definition:Characteristic Function (Set Theory),"['Definitions/Characteristic Functions of Sets', 'Definitions/Characteristic Functions']"
Definition:Characteristic,Characteristic,"Let $n \in \R$ be a positive real number such that $0 < n < 1$.

Let $n$ be presented (possibly approximated) in scientific notation as:
:$a \times 10^d$
where $d \in \Z$ is an integer.


Let $\log_{10} n$ be expressed in the form:
:$\log_{10} n = \begin {cases} c \cdotp m & : d \ge 0 \\ \overline c \cdotp m & : d < 0 \end {cases}$
where:
:$c = \size d$ is the absolute value of $d$
:$m := \log_{10} a$


$c$ is the characteristic of $\log_{10} n$.",Definition:General Logarithm/Common/Characteristic,['Definitions/Logarithms']
Definition:Characteristic Function,Characteristic Function,"Let $E \subseteq S$.

The characteristic function of $E$ is the function $\chi_E: S \to \set {0, 1}$ defined as:
:$\map {\chi_E} x = \begin {cases} 1 & : x \in E  \\  0 & : x \notin E \end {cases}$

That is:
:$\map {\chi_E} x = \begin {cases} 1 & : x \in E  \\ 0 & : x \in \relcomp S E \end {cases}$
where $\relcomp S E$ denotes the complement of $E$ relative to $S$.


=== Support ===
Let $S$ be a set

Let $E \subseteq S$ be a subset.

Let $\chi_E: S \to \set {0, 1}$ be the characteristic function of $E$.


The support of $\chi_E$, denoted $\map \supp {\chi_E}$, is the set $E$.

That is:

:$\map \supp {\chi_E} = \set {x \in S: \map {\chi_E} x = 1}$",Definition:Characteristic Function (Set Theory)/Set,"['Definitions/Characteristic Functions of Sets', 'Definitions/Set Theory']"
Definition:Characteristic Function,Characteristic Function,"The concept of a characteristic function of a subset carries over directly to relations.


Let $\RR \subseteq S \times T$ be a relation.

The characteristic function of $\RR$ is the mapping $\chi_\RR: S \times T \to \set {0, 1}$ defined as:
:$\map {\chi_\RR} {x, y} = \begin {cases} 1 & : \tuple {x, y} \in \RR \\ 0 & : \tuple {x, y} \notin \RR \end{cases}$


It can be expressed in Iverson bracket notation as:
:$\map {\chi_\RR} {x, y} = \sqbrk {\tuple {x, y} \in \RR}$


More generally, let $\ds \mathbb S = \prod_{i \mathop = 1}^n S_i = S_1 \times S_2 \times \ldots \times S_n$ be the cartesian product of $n$ sets $S_1, S_2, \ldots, S_n$.

Let $\RR \subseteq \mathbb S$ be an $n$-ary relation on $\mathbb S$.

The characteristic function of $\RR$ is the mapping $\chi_\RR: \mathbb S \to \set {0, 1}$ defined as:
:$\map {\chi_\RR} {s_1, s_2, \ldots, s_n} = \begin {cases} 1 & : \tuple {s_1, s_2, \ldots, s_n} \in \RR \\ 0 & : \tuple {s_1, s_2, \ldots, s_n} \notin \RR \end {cases}$


It can be expressed in Iverson bracket notation as:
:$\map {\chi_\RR} {s_1, s_2, \ldots, s_n} = \sqbrk {\tuple {s_1, s_2, \ldots, s_n} \in \RR}$",Definition:Characteristic Function (Set Theory)/Relation,"['Definitions/Characteristic Functions of Sets', 'Definitions/Relation Theory']"
Definition:Characteristic Function,Characteristic Function,"Let $\struct {\Omega, \Sigma, \Pr}$ be a probability space.

Let $X$ be a real-valued random variable on $\struct {\Omega, \Sigma, \Pr}$.


The characteristic function of $X$ is the mapping $\phi: \R \to \C$ defined by:

:$\map \phi t = \expect {e^{i t X} }$

where:
:$i$ is the imaginary unit
:$\expect \cdot$ denotes expectation.",Definition:Characteristic Function of Random Variable,"['Definitions/Characteristic Functions of Random Variables', 'Definitions/Random Variables', 'Definitions/Probability Theory', 'Definitions/Characteristic Functions']"
Definition:Cipher,Cipher,"The number zero is defined as being the cardinal of the empty set.


=== Naturally Ordered Semigroup ===
Let $\struct {S, \circ, \preceq}$ be a naturally ordered semigroup.

Then from  , $\struct {S, \circ, \preceq}$ has a smallest element.


This smallest element of $\struct {S, \circ, \preceq}$ is called zero and has the symbol $0$.

That is:
:$\forall n \in S: 0 \preceq n$

=== Natural Numbers ===

=== Integers ===

=== Rational Numbers ===

=== Real Numbers ===

=== Complex Numbers ===
Let $\C$ denote the set of complex numbers.

The zero of $\C$ is the complex number:
:$0 + 0 i$

 ",Definition:Zero (Number),"['Definitions/Abstract Algebra', 'Definitions/Zero']"
Definition:Cipher,Cipher,"Let $n$ be a number expressed in a particular number base, $b$ for example.

Then $n$ can be expressed as:

:$\sqbrk {r_m r_{m - 1} \ldots r_2 r_1 r_0 . r_{-1} r_{-2} \ldots}_b$

where:
:$m$ is such that $b^m \le n < b^{m + 1}$;
:all the $r_i$ are such that $0 \le r_i < b$.

Each of the $r_i$ are known as the digits of $n$ (base $b$).


It is taken for granted that for base $10$ working, the digits are elements of the set of Arabic numerals: $\set {0, 1, 2, 3, 4, 5, 6, 7, 8, 9}$.",Definition:Digit,"['Definitions/Digits', 'Definitions/Numbers']"
Definition:Cipher,Cipher,"The word cipher is an archaic word meaning to calculate using the technique of algorism, that is, using digits as opposed to using an abacus.",Definition:Cipher (Algorism),['Definitions/Algorism']
Definition:Cipher,Cipher,"A cipher, in the context of cryptography, is an algorithm used to transform a message into another string of symbols in order to hide its meaning from a third party.

A cipher operates by replacing each symbol in the original string with a word in some alphabet.

The specific nature of the replacement depends on the specifics of the cipher.


=== Plaintext ===
In the context of cryptography, the plaintext is the message being transformed into an encrypted string.

=== Ciphertext ===
In the context of cryptography, the ciphertext is the ciphered string after the plaintext has been processed by the cipher.

=== Key ===
In the context of cryptography, a key is a piece of information which:
:is used to convert plaintext to ciphertext
:allows the ciphertext to be transformed back into the original plaintext.

These two processes may use different keys.",Definition:Cipher (Cryptography),"['Definitions/Ciphers (Cryptography)', 'Definitions/Cryptography']"
Definition:Cipher,Cipher,"In the context of cryptography, the ciphertext is the ciphered string after the plaintext has been processed by the cipher.",Definition:Cipher (Cryptography)/Ciphertext,"['Definitions/Ciphers (Cryptography)', 'Definitions/Cryptography']"
Definition:Circuit,Circuit,"A circuit is a closed trail with at least one edge.


=== Subgraph ===
The set of vertices and edges which go to make up a circuit form a subgraph.

This subgraph itself is also referred to as a circuit.",Definition:Circuit (Graph Theory),"['Definitions/Circuits (Graph Theory)', 'Definitions/Graph Theory']"
Definition:Circuit,Circuit,"Let $M = \struct {S, \mathscr I}$ be a matroid.


A circuit of $M$ is a dependent subset of $S$ which is a minimal dependent subset with respect to the subset ordering.",Definition:Circuit (Matroid),['Definitions/Matroid Theory']
Definition:Circuit,Circuit,"An electric circuit is a configuration of electrical components whose collective properties can be modelled by means of a network each of whose edges corresponds to a specific component.

Its purpose is to guide and direct energy within a device.",Definition:Electric Circuit,['Definitions/Electronics']
Definition:Circumference,Circumference,"The circumference of a geometric figure is the line that forms its boundary.


=== Circumference of Circle ===
The circumference of a circle is the line that forms its boundary.


=== Concave Circumference ===
The convex circumference of a circle $C$ is the circumference $C$ from a point outside $C$.

=== Concave Circumference ===
The concave circumference of a circle $C$ is the circumference $C$ from a point inside $C$.

=== Circumference of Sphere ===
The circumference of a sphere is the circumference of a great circle of the sphere.",Definition:Circumference of Geometric Figure,['Definitions/Circumference of Geometric Figure']
Definition:Circumference,Circumference,"The circumference of a circle is the line that forms its boundary.


=== Concave Circumference ===
The convex circumference of a circle $C$ is the circumference $C$ from a point outside $C$.

=== Concave Circumference ===
The concave circumference of a circle $C$ is the circumference $C$ from a point inside $C$.",Definition:Circle/Circumference,"['Definitions/Circles', 'Definitions/Circumference of Geometric Figure']"
Definition:Circumference,Circumference,The circumference of a sphere is the circumference of a great circle of the sphere.,Definition:Circumference of Sphere,"['Definitions/Spheres', 'Definitions/Circumference of Geometric Figure']"
Definition:Circumference,Circumference,"Let $G$ be a graph.

The circumference of $G$ is the longest length of any cycle in $G$.


An acyclic graph is defined as having a circumference of infinity.

Category:Definitions/Graph Theory",Definition:Circumference (Graph Theory),['Definitions/Graph Theory']
Definition:Class,Class,Class theory is an extension of set theory which allows the creation of collections that are not sets by classes.,Definition:Class Theory,"['Definitions/Branches of Mathematics', 'Definitions/Class Theory', 'Definitions/Set Theory']"
Definition:Class,Class,"A class is a collection of all sets such that a particular condition holds.


In class-builder notation, this is written as:

:$\set {x: \map p x}$

where $\map p x$ is a statement containing $x$ as a free variable.  

This is read:
:All $x$ such that $\map p x$ holds.",Definition:Class (Class Theory),['Definitions/Class Theory']
Definition:Class,Class,"Let $D$ be a finite set of $n$ observations of a quantitative variable.


=== Integer Data ===
Let $D$ be a finite collection of $n$ data regarding some quantitative variable.

Let the data in $D$ be described by integers.

Let $d_{\min}$ be the value of the smallest datum in $D$.

Let $d_{\max}$ be the value of the largest datum in $D$.
 
Let $P = \set {x_0, x_1, x_2, \ldots, x_{n - 1}, x_n} \subseteq \Z$ be a subdivision of $\closedint a b$, where $a \le x_0 \le x_n \le b$.


The integer interval $\closedint a b$, where $a \le d_{\min} \le d_\max \le b$, is said to be divided into class intervals of integer intervals of the forms $\closedint {x_i} {x_{i + 1} }$ or $\closedint {x_i} {x_i}$  if and only if :

:Every datum is assigned into exactly one class interval

:Every class interval is disjoint from every other class interval

:The union of all class intervals contains the entire integer interval $\closedint {x_0} {x_n}$

By convention, the first and last class intervals are not empty class intervals.

=== Real Data ===
Let $D$ be a finite collection of $n$ data regarding some quantitative variable.

Let the data in $D$ be described by rational numbers or by real numbers.

Let $d_{\min}$ be the value of the smallest datum in $D$.

Let $d_{\max}$ be the value of the largest datum in $D$.
 
Let $P = \set {x_0, x_1, x_2, \ldots, x_{n - 1}, x_n} \subseteq \R$ be a subdivision of $\closedint a b$, where $a \le x_0 \le x_n \le b$.


The closed real interval $\closedint a b$, where $a \le d_{\text {min}} \le d_{\text {max}} \le b$, is said to be divided into class intervals of real intervals with endpoints $x_i$ and $x_{i + 1}$  if and only if :

:Every datum is assigned into exactly one class interval

:Every class interval is disjoint from every other class interval

:The union of all class intervals contains the entire real interval $\closedint {x_0} {x_n}$


The class intervals may be any combination of open, closed, or half-open intervals that fulfill the above criteria, but usually:

:Every class interval except the last is of the form $\closedint {x_i} {x_{i + 1} }$

:The last class interval is of the form $\closedint {x_{n - 1} } {x_n}$ 

By convention, the first and last class intervals are not empty class intervals.

=== Boundary of Class Interval ===
The class boundaries of a class interval are the endpoints of the integer interval or real interval which defines the class interval.

=== Class Mark ===
A class mark is a value within a class interval used to identify that class interval uniquely.

It is usual to use the midpoint.


=== Class Midpoint ===
When a class mark is defined to be the midpoint of the class interval it identifies, it is often called the class midpoint.

=== Empty Class Interval ===
A class interval is empty  if and only if  it is of frequency zero.

=== Relative Sizes of Class Interval ===
",Definition:Class Interval,"['Definitions/Class Intervals', 'Definitions/Descriptive Statistics']"
Definition:Class,Class,"Let $f: \R \to \R$ be a real function.

Then $\map f x$ is of differentiability class $C^k$  if and only if :
:$\dfrac {\d^k} {\d x^k} \map f x \in C$

where $C$ denotes the class of continuous real functions.


That is, $f$ is in differentiability class $k$  if and only if  there exists a $k$th derivative of $f$ which is continuous.


If $\dfrac {\d^k} {\d x^k} \map f x$ is continuous for all $k \in \N$, then $\map f x$ is of differentiability class $C^\infty$.",Definition:Differentiability Class,"['Definitions/Differential Calculus', 'Definitions/Continuity', 'Definitions/Differentiability Classes']"
Definition:Class,Class,"Let $X$ and $Y$ be topological spaces.

Let $K \subseteq X$ be any subset.

Let $f : X \to Y$ be a continuous mapping.

The $K$-homotopy class of $f$ is the equivalence class of $f$ under the equivalence relation defined by homotopy relative to $K$.


=== Homotopy Class of Path ===
Let $T = \struct {S, \tau}$ be a topological space.

Let $f: \closedint 0 1 \to S$ be a path in $T$.


The homotopy class of the path $f$ is the homotopy class of $f$ relative to $\set {0, 1}$.


That is, the equivalence class of $f$ under the equivalence relation defined by path-homotopy.",Definition:Homotopy Class,"['Definitions/Homotopy Classes', 'Definitions/Homotopy Theory', 'Definitions/Algebraic Topology']"
Definition:Class,Class,"Let $S$ be a set.

Let $\RR \subseteq S \times S$ be an equivalence relation on $S$.

Let $x \in S$.


Then the equivalence class of $x$ under $\RR$ is the set:
:$\eqclass x \RR = \set {y \in S: \tuple {x, y} \in \RR}$


If $\RR$ is an equivalence on $S$, then each $t \in S$ that satisfies $\tuple {x, t} \in \RR$ (or $\tuple {t, x} \in \RR$) is called a $\RR$-relative of $x$.


That is, the equivalence class of $x$ under $\RR$ is the set of all $\RR$-relatives of $x$.


=== Representative of Equivalence Class ===
Let $S$ be a set.

Let $\RR \subseteq S \times S$ be an equivalence relation on $S$.

Let $x \in S$.


Let $\eqclass x \RR$ be the equivalence class of $x$ under $\RR$.

Let $y \in \eqclass x \RR$.

Then $y$ is a representative of $\eqclass x \RR$.",Definition:Equivalence Class,"['Definitions/Equivalence Classes', 'Definitions/Equivalence Relations']"
Definition:Class,Class,"The equivalence classes into which the conjugacy relation divides its group into are called conjugacy classes.

The conjugacy class of an element $x \in G$ can be denoted $\conjclass x$.",Definition:Conjugacy Class,['Definitions/Conjugacy']
Definition:Class,Class,"In computability theory, $NP$ is a complexity class in which there exists some nondeterministic Turing machine that will either:
:halt within $\map p {\size x}$ steps
or:
:run forever

where:
:$p$ is a polynomial
:$\size x$ is the length of the input to the Machine.",Definition:NP Complexity Class,"['Definitions/Mathematical Logic', 'Definitions/Computability Theory']"
Definition:Closed,Closed,"Let $P$ be a statement.

$P$ is a closed statement  if and only if  $P$ contains only bound occurrences of any variables that may appear in it.


That is, such that it contains no free occurrences of variables.",Definition:Closed Statement,['Definitions/Predicate Logic']
Definition:Closed,Closed,"=== Topology ===

Let $T = \struct {S, \tau}$ be a topological space.

Let $H \subseteq S$.
Let $T = \left({S, \tau}\right)$ be a topological space.

Let $H \subseteq S$.


=== Definition 1 ===
Let $T = \struct {S, \tau}$ be a topological space.

Let $H \subseteq S$.

$H$ is closed (in $T$)  if and only if  its complement $S \setminus H$ is open in $T$. 

That is, $H$ is closed  if and only if  $\paren {S \setminus H} \in \tau$.

That is,  if and only if  $S \setminus H$ is an element of the topology of $T$.

=== Definition 2 ===
Let $T = \left({S, \tau}\right)$ be a topological space.

Let $H \subseteq S$.

$H$ is closed (in $T$)  if and only if  every limit point of $H$ is also a point of $H$.

That is, by the definition of the derived set:
:$H$ is closed (in $T$)  if and only if  $H' \subseteq H$
where $H'$ denotes the derived set of $H$.

=== Metric Space ===

In the context of metric spaces, the same definition applies:
Let $M = \left({A, d}\right)$ be a metric space.

Let $H \subseteq A$.


=== Definition 1 ===
Let $M = \struct {A, d}$ be a metric space.

Let $H \subseteq A$.


$H$ is closed (in $M$)  if and only if  its complement $A \setminus H$ is open in $M$.

=== Definition 2 ===
Let $M = \left({A, d}\right)$ be a metric space.

Let $H \subseteq A$.


$H$ is closed (in $M$)  if and only if  every limit point of $H$ is also a point of $H$.

=== Normed Vector Space ===
Let $V = \struct {X, \norm {\,\cdot\,} }$ be a normed vector space.

Let $F \subset X$.


==== Definition 1 ====
Let $V = \struct{X, \norm{\,\cdot\,} }$ be a normed vector space.

Let $F \subset X$.


$F$ is closed in $V$  if and only if  its complement $X \setminus F$ is open in $V$.

==== Definition 2 ====
Let $V = \struct{X, \norm{\,\cdot\,} }$ be a normed vector space.

Let $F \subset X$.


$F$ is closed (in $V$)  if and only if  every limit point of $F$ is also a point of $F$.


That is:  if and only if  $F$ contains all its limit points.

=== Complex Analysis ===
Let $S \subseteq \C$ be a subset of the complex plane.

$S$ is closed (in $\C$)  if and only if  every limit point of $S$ is also a point of $S$.


That is:  if and only if  $S$ contains all its limit points.

=== Real Analysis ===
Let $S \subseteq \R$ be a subset of the set of real numbers.


Then $S$ is closed (in $\R$)  if and only if  its complement $\R \setminus S$ is an open set.",Definition:Closed Set,"['Definitions/Topology', 'Definitions/Closed Sets']"
Definition:Closed,Closed,"Let $T = \left({S, \tau}\right)$ be a topological space.

Let $H \subseteq S$.


=== Definition 1 ===
Let $T = \struct {S, \tau}$ be a topological space.

Let $H \subseteq S$.

$H$ is closed (in $T$)  if and only if  its complement $S \setminus H$ is open in $T$. 

That is, $H$ is closed  if and only if  $\paren {S \setminus H} \in \tau$.

That is,  if and only if  $S \setminus H$ is an element of the topology of $T$.

=== Definition 2 ===
Let $T = \left({S, \tau}\right)$ be a topological space.

Let $H \subseteq S$.

$H$ is closed (in $T$)  if and only if  every limit point of $H$ is also a point of $H$.

That is, by the definition of the derived set:
:$H$ is closed (in $T$)  if and only if  $H' \subseteq H$
where $H'$ denotes the derived set of $H$.",Definition:Closed Set/Topology,['Definitions/Closed Sets']
Definition:Closed,Closed,"Let $M = \left({A, d}\right)$ be a metric space.

Let $H \subseteq A$.


=== Definition 1 ===
Let $M = \struct {A, d}$ be a metric space.

Let $H \subseteq A$.


$H$ is closed (in $M$)  if and only if  its complement $A \setminus H$ is open in $M$.

=== Definition 2 ===
Let $M = \left({A, d}\right)$ be a metric space.

Let $H \subseteq A$.


$H$ is closed (in $M$)  if and only if  every limit point of $H$ is also a point of $H$.",Definition:Closed Set/Metric Space,"['Definitions/Closed Sets', 'Definitions/Metric Spaces']"
Definition:Closed,Closed,"Let $T$ be a topological space.

Let $A \subseteq T$.


Then $A$ is regular closed in $T$  if and only if :
:$A = A^{\circ -}$

That is,  if and only if  $A$ equals the closure of its interior.",Definition:Regular Closed Set,['Definitions/Topology']
Definition:Closed,Closed,"Let $X, Y$ be topological spaces.

Let $f: X \to Y$ be a mapping. 


If, for any closed set $V \subseteq X$, the image $\map f V$ is closed in $Y$, then $f$ is referred to as a closed mapping.",Definition:Closed Mapping,['Definitions/Topology']
Definition:Closed,Closed,"Let $T = \struct {S, \tau}$ be a topological space.

Let $p$ be a new element for $S$ such that $S^*_p := S \cup \set p$.


Let $\tau^*_p$ be the set defined as:
:$\tau^*_p := \set {U \cup \set p: U \in \tau} \cup \set \O$

That is, $\tau^*_p$ is the set of all sets formed by adding $p$ to all the open sets of $\tau$ and including the empty set.


Then:
:$\tau^*_p$ is the closed extension topology of $\tau$
and:
:$T^*_p := \struct {S^*_p, \tau^*_p}$ is the closed extension space of $T = \struct {S, \tau}$.",Definition:Closed Extension Topology,['Definitions/Examples of Topologies']
Definition:Closed,Closed,"Let $T = \struct {S, \tau}$ be a topological space.

Let $\gamma: \closedint 0 1 \to S$ be a path in $T$.

Let $\map \gamma 0 = \map \gamma 1$.


Then $\gamma$ is called a loop (in $T$).


=== Simple Loop ===
Let $T = \struct {S, \tau}$ be a topological space.

Let $\gamma: \closedint 0 1 \to S$ be a path in $T$.


$\gamma$ is a simple loop (in $T$)  if and only if :
:$\map \gamma {t_1} \ne \map \gamma {t_2}$ for all $t_1 ,t_2 \in \hointr 0 1$ with $t_1 \ne t_2$
:$\map \gamma 0 = \map \gamma 1$

=== Base Point ===
Let $X$ be a topological space.

Let $\gamma: \closedint 0 1 \to X$ be a loop.


The base point of $\gamma$ is $\map \gamma 0$.

In other words, $\gamma$ is said to be based at $\map \gamma 0$.

=== Set of All Loops ===
Let $T$ be a topological space.


The set of all loops based at $p \in T$ is denoted by $\map \Omega {T, p}$.

=== Constant Loop ===
Let $T$ be a topological space.

Let $p \in T$.

Let $\map \Omega {T, p}$ denote the set of all loops based at $p$.


A constant loop $c_p$ is the loop $c_p \in \map \Omega {T, p}$ such that:

:$\forall t \in \closedint 0 1 : \map {c_p} t = p$

=== Null-Homotopic Loop ===
Let $T = \struct {S, \tau}$ be a topological space.

Let $\gamma$ be a loop in $T$.

Suppose $\gamma$ is path-homotopic to a constant loop.


Then $\gamma$ is said to be null-homotopic.

=== Circle Representative of Loop ===
Let $T = \struct {S, \tau}$ be a topological space.

Let $\gamma: \closedint 0 1 \to S$ be a loop in $T$.

Let $\Bbb S^1 \subseteq \C$ be the unit circle in $\C$:

:$\Bbb S^1 = \set {z \in \C : \size z = 1}$

Suppose $\omega : \closedint 0 1 \to \Bbb S^1$ such that $\map \omega s = \map \exp {2 \pi i s}$.


Then the unique map $\tilde f : \Bbb S^1 \to T$ such that $\tilde f \circ \omega = f$ is called the circle representative of $f$.

=== Loop in Topological Manifold ===
Let $M$ be a topological manifold.

Let $\sigma : \closedint 0 1 \to M$ be a continuous path.

Let $\map \sigma 0 = \map \sigma 1$.


Then $\sigma$ is called a loop.",Definition:Loop (Topology),"['Definitions/Loops (Topology)', 'Definitions/Topology']"
Definition:Closed,Closed,"=== Complex Analysis ===
Let $D \subseteq \C$ be a subset of the set of complex numbers.

$D$ is a closed region  if and only if  $D$ the closure of an open region.

=== Closed Region in the Plane ===
A closed region is a region complete with its boundary.


Category:Definitions/Analytic Geometry",Definition:Closed Region,"['Definitions/Complex Analysis', 'Definitions/Analytic Geometry']"
Definition:Closed,Closed,"Let $f: S \to T$ be a mapping.

Let $S' \subseteq S$.


Then $S'$ is closed under $f$  if and only if :

:$f \sqbrk {S'} \subseteq S'$

where $f \sqbrk {S'}$ is the image of $S'$ under $f$.


That is:
:$x \in S' \implies \map f x \in S'$


=== Arbitrary Product ===
Let $\phi: X^I \to T$ be a mapping or a partial mapping, taking $I$-indexed families as arguments.

Denote with $\Dom \phi$ the domain of $\phi$ (if $\phi$ is a mapping, this is simply $X^I$).


A set $S$ is closed under $\phi$  if and only if :

:$\forall \family {s_i}_{i \mathop \in I} \in S^I \cap \Dom \phi: \map \phi {\family {s_i}_{i \mathop \in I} } \in S$

Phrased in terms of image of a mapping, this translates to:

:$\map \phi {S^I \cap \Dom \phi} \subseteq S$


Thus, in words, $S$ is closed under $\phi$,  if and only if :

:Whenever $\phi$ is defined for an $I$-indexed family from $S$, it maps that indexed family into $S$ again.

=== Class Theoretical Definition ===
Let $A$ and $B$ be classes such that $A$ is a subclass of $B$.

Let $g: B \to B$ be a mapping on $B$.


Then $A$ is closed under $g$  if and only if :

:$\forall x \in A: \map g x \in A$",Definition:Closed under Mapping,"['Definitions/Mapping Theory', 'Definitions/Closedness under Mappings']"
Definition:Closed,Closed,"Let $S$ be a set.

Let $\cl: \powerset S \to \powerset S$ be a closure operator.

Let $T \subseteq S$ be a subset.


=== Definition 1 ===
Let $S$ be a set.

Let $\cl: \powerset S \to \powerset S$ be a closure operator.

Let $T \subseteq S$ be a subset.


$T$ is closed (with respect to $\cl$)  if and only if :
:$\map \cl T = T$

=== Definition 2 ===
Let $S$ be a set.

Let $\cl: \powerset S \to \powerset S$ be a closure operator.

Let $T \subseteq S$ be a subset.


$T$ is closed (with respect to $\cl$)  if and only if  $T$ is in the image of $\cl$:
:$T \in \Img \cl$",Definition:Closed Set/Closure Operator,"['Definitions/Closed Elements', 'Definitions/Closure Operators']"
Definition:Closed,Closed,"Let $\struct {S, \preceq}$ be an ordered set.

Let $\cl$ be a closure operator on $S$.

Let $x \in S$.


=== Definition 1 ===
Let $\struct {S, \preceq}$ be an ordered set.

Let $\cl$ be a closure operator on $S$.

Let $x \in S$.


The element $x$ is a closed element of $S$ (with respect to $\cl$)  if and only if  $x$ is a fixed point of $\cl$:
:$\map \cl x = x$

=== Definition 2 ===
Let $\struct {S, \preceq}$ be an ordered set.

Let $\cl$ be a closure operator on $S$.

Let $x \in S$.


The element $x$ is a closed element of $S$ (with respect to $\cl$)  if and only if  $x$ is in the image of $\cl$:
:$x \in \Img \cl$",Definition:Closed Element,"['Definitions/Closure Operators', 'Definitions/Closed Elements']"
Definition:Closed,Closed,"Let $a, b \in \R$.

The closed (real) interval from $a$ to $b$ is defined as:

:$\closedint a b = \set {x \in \R: a \le x \le b}$",Definition:Real Interval/Closed,['Definitions/Real Intervals']
Definition:Closed,Closed,"A closed walk is a walk whose first vertex is the same as the last.

That is, it is a walk which ends where it starts.",Definition:Walk (Graph Theory)/Closed,['Definitions/Walks']
Definition:Closed,Closed,"A cycle is a circuit in which no vertex except the first (which is also the last) appears more than once.


An $n$-cycle is a cycle with $n$ vertices.


=== Subgraph ===
The set of vertices and edges which go to make up a cycle form a subgraph.

This subgraph itself is also referred to as a cycle.

=== Odd Cycle ===
An odd cycle is a cycle with odd length, that is, with an odd number of edges.

=== Even Cycle ===
An even cycle is a cycle with even length, that is, with an even number of edges.",Definition:Cycle (Graph Theory),"['Definitions/Cycles (Graph Theory)', 'Definitions/Circuits (Graph Theory)', 'Definitions/Paths (Graph Theory)', 'Definitions/Graph Theory']"
Definition:Closed,Closed,"A circuit is a closed trail with at least one edge.


=== Subgraph ===
The set of vertices and edges which go to make up a circuit form a subgraph.

This subgraph itself is also referred to as a circuit.",Definition:Circuit (Graph Theory),"['Definitions/Circuits (Graph Theory)', 'Definitions/Graph Theory']"
Definition:Closed,Closed,"Let $\struct {S, \circ}$ be an algebraic structure.


Then $S$ has the property of closure under $\circ$  if and only if :

:$\forall \tuple {x, y} \in S \times S: x \circ y \in S$


$S$ is said to be closed under $\circ$, or just that $\struct {S, \circ}$ is closed.",Definition:Closure (Abstract Algebra)/Algebraic Structure,['Definitions/Algebraic Closure']
Definition:Closed,Closed,"Let $\struct {S, \circ}_R$ be an $R$-algebraic structure over a ring $R$.

Let $T \subseteq S$ such that $\forall \lambda \in R: \forall x \in T: \lambda \circ x \in T$.


Then $T$ is closed for scalar product.


If $T$ is also closed for operations on $S$, then it is called a closed subset of $S$.",Definition:Closure (Abstract Algebra)/Scalar Product,"['Definitions/Linear Algebra', 'Definitions/Vector Algebra']"
Definition:Closed,Closed,"Let $K$ be a field.


Then $K$ is algebraically closed  if and only if :


=== Definition 1 ===
Let $K$ be a field.


$K$ is algebraically closed  if and only if :

The only algebraic field extension of $K$ is $K$ itself.

=== Definition 2 ===
Let $K$ be a field.


$K$ is algebraically closed  if and only if :

Every irreducible polynomial $f$ over $K$ has degree $1$.

=== Definition 3 ===
Let $K$ be a field.


$K$ is algebraically closed  if and only if :

Every polynomial $f$ over $K$ of strictly positive degree has a root in $K$.",Definition:Algebraically Closed Field,['Definitions/Field Extensions']
Definition:Closed,Closed,"=== Ring Extension ===
Let $\phi : A \hookrightarrow B$ be a ring extension.

Let $C$ be the integral closure of $A$ in $B$.


Then $A$ is integrally closed in $B$  if and only if  $C = \phi(A)$.

=== Integral Domain ===
Let $R$ be an integral domain.


Then $R$ is integrally closed  if and only if  it is integrally closed in its field of fractions.",Definition:Integrally Closed,"['Definitions/Algebraic Number Theory', 'Definitions/Commutative Algebra']"
Definition:Closed,Closed,"Let $\struct {A, +, \circ}$ be a ring with unity $1_A$ and zero $0_A$.

Let $S \subseteq A$ be a subset.


Then $S$ is multiplicatively closed  if and only if :

:$(1): \quad 1_A \in S$
:$(2): \quad x, y \in S \implies x \circ y \in S$",Definition:Multiplicatively Closed Subset of Ring,['Definitions/Localization of Rings']
Definition:Closure,Closure,"=== Ordering ===
=== Definition 1 ===

Let $\struct {S, \preceq}$ be an ordered set.


A closure operator on $S$ is a mapping:
:$\cl: S \to S$
which satisfies the closure axioms as follows for all elements $x, y \in S$:
 


=== Definition 2 ===
Let $\struct {S, \preceq}$ be an ordered set.


A closure operator on $S$ is a mapping:
:$\cl: S \to S$
which satisfies the following condition for all elements $x, y \in S$:
:$x \preceq \map \cl y \iff \map \cl x \preceq \map \cl y$

=== Power Set ===

When the ordering in question is the subset relation on a power set, the definition can be expressed as follows:

Let $S$ be a set.

Let $\powerset S$ denote the power set of $S$.


A closure operator on $S$ is a mapping:
:$\cl: \powerset S \to \powerset S$
which satisfies the closure axioms as follows for all sets $X, Y \subseteq S$:
 ",Definition:Closure Operator,['Definitions/Closure Operators']
Definition:Closure,Closure,"Let $S$ be a set.

Let $\cl$ be a closure operator on $S$.

Let $T \subseteq S$ be a subset of $S$.


The closure of $T$ is its image $\map \cl T$.",Definition:Closure of Set under Closure Operator,['Definitions/Closure Operators']
Definition:Closure,Closure,"=== Upper Closure of an Element ===
Let $\struct {S, \preccurlyeq}$ be an ordered set.

Let $a \in S$.


The upper closure of $a$ (in $S$) is defined as:

:$a^\succcurlyeq := \set {b \in S: a \preccurlyeq b}$


That is, $a^\succcurlyeq$ is the set of all elements of $S$ that succeed $a$.

=== Upper Closure of a Set ===
Let $\struct {S, \preceq}$ be an ordered set or preordered set.

Let $T \subseteq S$.


The upper closure of $T$ (in $S$) is defined as:

:$T^\succeq := \bigcup \set {t^\succeq: t \in T}$
where $t^\succeq$ denotes the upper closure of $t$ in $S$.

That is:
:$T^\succeq := \set {u \in S: \exists t \in T: t \preceq u}$",Definition:Upper Closure,"['Definitions/Order Theory', 'Definitions/Upper Closures']"
Definition:Closure,Closure,"=== Lower Closure of Element ===
Let $\struct {S, \preccurlyeq}$ be an ordered set.

Let $a \in S$.


The lower closure of $a$ (in $S$) is defined as:

:$a^\preccurlyeq := \set {b \in S: b \preccurlyeq a}$


That is, $a^\preccurlyeq$ is the set of all elements of $S$ that precede $a$.


=== Class Theory ===
 
Let $A$ be a class under an ordering $\preccurlyeq$.

Let $a \in A$.


The lower closure of $a$ (in $A$) is defined as:

:$a^\preccurlyeq := \set {b \in A: b \preccurlyeq a}$

=== Lower Closure of Subset ===
Let $\struct {S, \preccurlyeq}$ be an ordered set or preordered set.

Let $T \subseteq S$.


The lower closure of $T$ (in $S$) is defined as:

:$T^\preccurlyeq := \bigcup \set {t^\preccurlyeq: t \in T}$
where $t^\preccurlyeq$ is the lower closure of $t$.

That is:
:$T^\preccurlyeq := \set {l \in S: \exists t \in T: l \preccurlyeq t}$",Definition:Lower Closure,"['Definitions/Order Theory', 'Definitions/Lower Closures']"
Definition:Closure,Closure,"Let $\struct {S, \circ}$ be an algebraic structure.


Then $S$ has the property of closure under $\circ$  if and only if :

:$\forall \tuple {x, y} \in S \times S: x \circ y \in S$


$S$ is said to be closed under $\circ$, or just that $\struct {S, \circ}$ is closed.",Definition:Closure (Abstract Algebra)/Algebraic Structure,['Definitions/Algebraic Closure']
Definition:Closure,Closure,"Let $A$ be an extension of a commutative ring with unity $R$.


Let $C$ be the set of all elements of $A$ that are integral over $R$.

Then $C$ is called the integral closure of $R$ in $A$.",Definition:Integral Closure,"['Definitions/Algebraic Number Theory', 'Definitions/Commutative Algebra']"
Definition:Closure,Closure,"Let $G$ be a group.

Let $S \subseteq G$ be a subset.


=== Definition 1 ===
Let $G$ be a group. 

Let $S \subseteq G$ be a subset.


The normal subgroup generated by $S$, denoted $\gen {S^G}$,  is the intersection of all normal subgroups of $G$ containing $S$.

=== Definition 2 ===
 

Let $G$ be a group. 

Let $S \subseteq G$ be a subset.


The normal subgroup generated by $S$, denoted $\gen {S^G}$,  is the smallest normal subgroup of $G$ containing $S$:
:$\gen {S^G} = \gen {x S x^{-1}: x \in G}$",Definition:Generated Normal Subgroup,"['Definitions/Generated Normal Subgroups', 'Definitions/Normal Subgroups', 'Definitions/Normality in Groups', 'Definitions/Generated Subgroups', 'Definitions/Generators of Groups', 'Definitions/Subgroups']"
Definition:Closure,Closure,"Let $T = \struct {S, \tau}$ be a topological space.

Let $H \subseteq S$.


=== Definition 1 ===
Let $T = \struct {S, \tau}$ be a topological space.

Let $H \subseteq S$.


The closure of $H$ (in $T$) is defined as:
:$H^- := H \cup H'$
where $H'$ is the derived set of $H$.


That is, $H^-$ is the union of $H$ and its limit points.

=== Definition 2 ===
Let $T = \struct {S, \tau}$ be a topological space.

Let $H \subseteq S$.


The closure of $H$ (in $T$) is defined as:
:$\ds H^- := \bigcap \leftset {K \supseteq H: K}$ is closed in $\rightset T$


That is, $H^-$ is the intersection of all closed sets in $T$ which contain $H$.

=== Definition 3 ===
Let $T = \struct {S, \tau}$ be a topological space.

Let $H \subseteq S$.


The closure of $H$ (in $T$), denoted $H^-$, is defined as the smallest closed set of $T$ that contains $H$.

=== Definition 4 ===
Let $T = \struct {S, \tau}$ be a topological space.

Let $H \subseteq S$.


The closure of $H$ (in $T$) is defined as the union of $H$ and its boundary in $T$:
:$H^- := H \cup \partial H$

=== Definition 5 ===
Let $T = \struct {S, \tau}$ be a topological space.

Let $H \subseteq S$.


The closure of $H$ (in $T$) is the union of the set of all isolated points of $H$ and the set of all limit points of $H$:
:$H^- := H^i \cup H'$

=== Definition 6 ===
Let $T = \struct {S, \tau}$ be a topological space.

Let $H \subseteq S$.


The closure of $H$ (in $T$), denoted $H^-$, is the set of all adherent points of $H$.


=== Adherent Point ===
Let $A \subseteq S$.

Let $T = \struct{S, \tau}$ be a topological space.

Let $H \subseteq S$.


A point $x \in S$ is an adherent point of $H$  if and only if  every neighborhood $N$ of $x$ satisfies:
:$H \cap N \ne \O$",Definition:Closure (Topology),"['Definitions/Topology', 'Definitions/Set Closures']"
Definition:Closure,Closure,"Let $M = \struct {A, d}$ be a metric space.

Let $H \subseteq A$.

Let $H'$ be the set of limit points of $H$.

Let $H^i$ be the set of isolated points of $H$.


The closure of $H$ (in $M$) is the union of all isolated points of $H$ and all limit points of $H$:
:$H^- := H' \cup H^i$",Definition:Closure (Topology)/Metric Space,"['Definitions/Metric Spaces', 'Definitions/Set Closures']"
Definition:Closure,Closure,"Let $M = \struct {X, \norm {\, \cdot \,} }$ be a normed vector space.

Let $S \subseteq X$.


The closure of $S$ (in $M$) is the union of $S$ and $S'$, the set of all limit points of $S$:
:$S^- := S \cup S'$",Definition:Closure/Normed Vector Space,"['Definitions/Set Closures', 'Definitions/Normed Vector Spaces']"
Definition:Closure,Closure,"=== Definition 1 ===
Let $x$ be a set.

Then the transitive closure of $x$ is the smallest transitive superset of $x$.

The following is not equivalent to the above, but they are almost the same.

=== Definition 2 ===
Let $x$ be a set.

For each natural number $n \in \N_{\ge 0}$ let:

: $\bigcup^n x = \underbrace{\bigcup \bigcup \cdots \bigcup}_n x$


Then the transitive closure of $x$ is the union of the sets:
:$\left\{ {x}\right\}, x, \bigcup x, \bigcup^2 x, \dots, \bigcup^n x, \dots$


More precisely:

Let $F$ be the mapping on the universal class defined by letting:
:$F \left({a}\right) = \bigcup a$
for each set $a$.

Let $G$ be the mapping on the natural numbers defined recursively by letting:

: $G \left({0}\right) = \left\{ {x}\right\}$
: $G \left({n^+}\right) = F \left({G \left({n}\right)}\right)$

for each natural number $n$.

Then the transitive closure of $x$ is defined as the union of the image of $G$.",Definition:Transitive Closure (Set Theory),['Definitions/Relational Closures']
Definition:Closure,Closure,"=== Definition 1 ===
Let $\RR$ be a relation on a set $S$.


The reflexive closure of $\RR$ is denoted $\RR^=$, and is defined as:

:$\RR^= := \RR \cup \set {\tuple {x, x}: x \in S}$

That is:

:$\RR^= := \RR \cup \Delta_S$
where $\Delta_S$ is the diagonal relation on $S$.

=== Definition 2 ===
Let $\RR$ be a relation on a set $S$.


The reflexive closure of $\RR$ is defined as the smallest reflexive relation on $S$ that contains $\RR$ as a subset.


The reflexive closure of $\RR$ is denoted $\RR^=$.

=== Definition 3 ===
Let $\RR$ be a relation on a set $S$.

Let $\QQ$ be the set of all reflexive relations on $S$ that contain $\RR$.

The reflexive closure of $\RR$ is denoted $\RR^=$, and is defined as:

:$\RR^= := \bigcap \QQ$

That is:

:$\RR^=$ is the intersection of all reflexive relations on $S$ containing $\RR$.",Definition:Reflexive Closure,"['Definitions/Relation Theory', 'Definitions/Reflexive Relations', 'Definitions/Reflexive Closures']"
Definition:Closure,Closure,"Let $\RR$ be a relation on a set $S$.


=== Definition 1 ===
Let $\RR$ be a relation on a set $S$.


The symmetric closure of $\RR$ is denoted $\RR^\leftrightarrow$, and is defined as the union of $\RR$ with its inverse:

:$\RR^\leftrightarrow = \RR \cup \RR^{-1}$

=== Definition 2 ===
Let $\RR$ be a relation on a set $S$.


The symmetric closure of $\RR$ is denoted $\RR^\leftrightarrow$, and is defined as the smallest symmetric relation on $S$ which contains $\RR$.",Definition:Symmetric Closure,"['Definitions/Relation Theory', 'Definitions/Symmetric Closures', 'Definitions/Closure Operators']"
Definition:Closure,Closure,"=== Smallest Transitive Superset ===
Let $\RR$ be a relation on a set $S$.


The transitive closure of $\RR$ is defined as the smallest transitive relation on $S$ which contains $\RR$ as a subset.


The transitive closure of $\RR$ is denoted $\RR^+$.

=== Intersection of Transitive Supersets ===
Let $\RR$ be a relation on a set $S$.


The transitive closure of $\RR$ is defined as the intersection of all transitive relations on $S$ which contain $\RR$.


The transitive closure of $\RR$ is denoted $\RR^+$.

=== Finite Chain ===
Let $\RR$ be a relation on a set or class $S$.


The transitive closure of $\RR$ is the relation $\RR^+$ defined as follows:

For $x, y \in S$, $x \mathrel {\RR^+} y$  if and only if  for some $n \in \N_{>0}$ there exist $s_0, s_1, \dots, s_n \in S$ such that $s_0 = x$, $s_n = y$, and:

 
 
 
 
 
 


That is:

:$\forall k \in \N_n: s_k \mathrel \RR s_{k + 1}$

=== Union of Compositions ===
Let $\RR$ be a relation on a set $S$.

Let:

:$\RR^n := \begin{cases}
\RR & : n = 1 \\
\RR^{n-1} \circ \RR & : n > 1
\end{cases}$

where $\circ$ denotes composition of relations.

Finally, let:

:$\ds \RR^+ = \bigcup_{i \mathop = 1}^\infty \RR^i$


Then $\RR^+$ is called the transitive closure of $\RR$.",Definition:Transitive Closure (Relation Theory),"['Definitions/Transitive Closures', 'Definitions/Transitive Relations', 'Definitions/Examples of Closure Operators']"
Definition:Codomain,Codomain,"=== Relation ===
The codomain of a relation $\RR \subseteq S \times T$ is $T$.

It can be denoted $\Cdm \RR$.

=== Mapping ===

The term codomain is usually seen when the relation in question is actually a mapping:
Let $S$ and $T$ be sets.

Let $f: S \to T$ be a mapping.

The codomain of $f$ is $T$.

It is denoted on   by $\Cdm f$.",Definition:Codomain (Relation Theory),"['Definitions/Codomains (Relation Theory)', 'Definitions/Relation Theory']"
Definition:Codomain,Codomain,"Let $f: X \to Y$ be a morphism.

Then the codomain of $f$ is defined to be the object $Y$.

This is usually denoted $Y = \Cdm f$.

Category:Definitions/Morphisms",Definition:Codomain (Category Theory),['Definitions/Morphisms']
Definition:Column,Column,"A row of a truth table is one of the vertical lines headed by a statement form presenting all the truth values that the statement form takes.

Each entry in the column corresponds to one specific combination of truth values taken by the propositional variables that the statement form comprises.",Definition:Truth Table/Column,['Definitions/Truth Tables']
Definition:Column,Column,"Let $\mathbf A$ be an $m \times n$ matrix.

For each $j \in \closedint 1 n$, the columns of $\mathbf A$ are the ordered $m$-tuples:
: $c_j = \tuple {a_{1 j}, a_{2 j}, \ldots, a_{m j} }$

where $c_j$ is called the $j$th column of $\mathbf A$.


A column of an $m \times n$ matrix can also be treated as a $m \times 1$ column matrix in its own right:
:$c_j = \begin {bmatrix} a_{1 j} \\ a_{2 j} \\ \vdots \\ a_{m j} \end {bmatrix}$
for $j = 1, 2, \ldots, n$.",Definition:Matrix/Column,['Definitions/Matrices']
Definition:Column,Column,"Let $\mathbf L$ be a Latin square.

The columns of $\mathbf L$ are the lines of elements reading down the page.",Definition:Latin Square/Column,['Definitions/Latin Squares']
Definition:Common,Common,"Logarithms base $10$ are often referred to as common logarithms.


=== Notation for Negative Logarithm ===
Let $n \in \R$ be a real number such that $0 < n < 1$.

Let $n$ be presented (possibly approximated) in scientific notation as:
:$a \times 10^{-d}$
where $d \in \Z_{>0}$ is a (strictly) positive integer.

Let $\log_{10} n$ denote the common logarithm of $n$.

Then it is the standard convention to express $\log_{10} n$ in the form:
:$\log_{10} n = \overline d \cdotp m$

where $m := \log_{10} a$ is the mantissa of $\log_{10} n$.


The overline notation is commonly read as bar, that is:
:$\overline 2$ is read as bar two.",Definition:General Logarithm/Common,['Definitions/Logarithms']
Definition:Common,Common,"Let $x_1, x_2, \ldots, x_n \in \R$ be real numbers.

The arithmetic mean of $x_1, x_2, \ldots, x_n$ is defined as:

:$\ds A_n := \dfrac 1 n \sum_{k \mathop = 1}^n x_k$

That is, to find out the arithmetic mean of a set of numbers, add them all up and divide by how many there are.",Definition:Arithmetic Mean,"['Definitions/Arithmetic Mean', 'Definitions/Pythagorean Means', 'Definitions/Algebra', 'Definitions/Measures of Central Tendency']"
Definition:Common,Common,"Consider the expression:
:$\dfrac a b + \dfrac c d$

where $a$, $b$, $c$ and $d$ are any expressions whatsoever which evaluate to a number such that neither $c$ nor $d$ evaluate to zero.

In order to be able to perform the required addition, it is necessary to put the expressions $\dfrac a b$ and $\dfrac c d$ over a common denominator.


Hence the operation is:
:to multiply both the numerator (top) and denominator (bottom) of $\dfrac a b$ by $d$
and in the same operation:
:to multiply both the numerator (top) and denominator (bottom) of $\dfrac c d$ by $b$

in order to obtain the expression:
:$\dfrac {a d} {b d} + \dfrac {b c} {b d}$


Hence one may perform the operation as:
:$\dfrac {a d + b c} {b d}$

and either evaluate or simplify appropriately.


=== Lowest Common Denominator ===
Let $\dfrac a b$ and $\dfrac c d$ be fractions.

The lowest common denominator of $\dfrac a b$ and $\dfrac c d$ is the lowest common multiple of the denominators of $\dfrac a b$ and $\dfrac c d$:
:$\lcm \set {b, d}$",Definition:Common Denominator,"['Definitions/Common Denominators', 'Definitions/Proof Techniques', 'Definitions/Arithmetic', 'Definitions/Algebra']"
Definition:Common,Common,"Let $\dfrac a b$ and $\dfrac c d$ be fractions.

The lowest common denominator of $\dfrac a b$ and $\dfrac c d$ is the lowest common multiple of the denominators of $\dfrac a b$ and $\dfrac c d$:
:$\lcm \set {b, d}$",Definition:Common Denominator/Lowest,"['Definitions/Lowest Common Denominator', 'Definitions/Common Denominators']"
Definition:Common,Common,"=== Integral Domain ===
Let $\struct {D, +, \times}$ be an integral domain.

Let $S \subseteq D$ be a finite subset of $D$.


Let $c \in D$ such that $c$ divides all the elements of $S$, that is:

:$\forall x \in S: c \divides x$


Then $c$ is a common divisor of all the elements in $S$.

=== Integers ===

The definition is usually applied when the integral domain in question is the set of integers $\Z$, thus:

Let $S$ be a finite set of integers, that is:

:$S = \set {x_1, x_2, \ldots, x_n: \forall k \in \N^*_n: x_k \in \Z}$


Let $c \in \Z$ such that $c$ divides all the elements of $S$, that is:

:$\forall x \in S: c \divides x$


Then $c$ is a common divisor of all the elements in $S$.

=== Real Numbers ===

The definition can also be applied when the integral domain in question is the real numbers $\R$, thus:

Let $S$ be a finite set of real numbers, that is:

:$S = \set {x_1, x_2, \ldots, x_n: \forall k \in \N^*_n: x_k \in \R}$


Let $c \in \R$ such that $c$ divides all the elements of $S$, that is:

:$\forall x \in S: c \divides x$


Then $c$ is a common divisor of all the elements in $S$.",Definition:Common Divisor,"['Definitions/Common Divisors', 'Definitions/Number Theory', 'Definitions/Integral Domains']"
Definition:Common,Common,"Let $S$ be a finite set of real numbers, that is:

:$S = \set {x_1, x_2, \ldots, x_n: \forall k \in \N^*_n: x_k \in \R}$


Let $c \in \R$ such that $c$ divides all the elements of $S$, that is:

:$\forall x \in S: c \divides x$


Then $c$ is a common divisor of all the elements in $S$.",Definition:Common Divisor/Real Numbers,"['Definitions/Common Divisors', 'Definitions/Real Analysis']"
Definition:Common,Common,"=== Integral Domain ===
Let $\struct {D, +, \times}$ be an integral domain whose zero is $0$.

Let $a, b \in D: a \ne 0 \lor b \ne 0$.

Let $d \divides a$ denote that $d$ is a divisor of $a$.


Let $d \in D$ have the following properties:

:$(1): \quad d \divides a \land d \divides b$
:$(2): \quad c \divides a \land c \divides b \implies c \divides d$

Then $d$ is called a greatest common divisor of $a$ and $b$ (abbreviated GCD or gcd) and denoted $\gcd \set {a, b}$.


That is, in the integral domain $D$, $d$ is the GCD of $a$ and $b$  if and only if :
:$d$ is a common divisor of $a$ and $b$
:Any other common divisor of $a$ and $b$ also divides $d$.


When $a = b = 0$, $\gcd \set {a, b}$ is undefined.


We see that, trivially:
:$\gcd \set {a, b} = \gcd \set {b, a}$
so the set notation is justified.

=== Integers ===

When the integral domain in question is the integers $\Z$, the GCD is often defined differently, as follows:
Let $a, b \in \Z: a \ne 0 \lor b \ne 0$.


The greatest common divisor of $a$ and $b$ is defined as:

:the largest $d \in \Z_{>0}$ such that $d \divides a$ and $d \divides b$

where $\divides$ denotes divisibility.

This is denoted $\gcd \set {a, b}$.


When $a = b = 0$, $\gcd \set {a, b}$ is undefined.


=== General Definition ===

This definition can be extended to any (finite) number of integers.
Let $S = \set {a_1, a_2, \ldots, a_n} \subseteq \Z$ such that $\exists x \in S: x \ne 0$ (that is, at least one element of $S$ is non-zero).

=== Definition 1 ===
Let $S = \set {a_1, a_2, \ldots, a_n} \subseteq \Z$ such that $\exists x \in S: x \ne 0$ (that is, at least one element of $S$ is non-zero).


The greatest common divisor of $S$:
:$\gcd \paren S = \gcd \set {a_1, a_2, \ldots, a_n}$

is defined as the largest $d \in \Z_{>0}$ such that:
:$\forall x \in S: d \divides x$
where $\divides$ denotes divisibility.


By convention:
:$\map \gcd \O = 1$

=== Definition 2 ===
Let $S = \set {a_1, a_2, \ldots, a_n} \subseteq \Z$ such that $\exists x \in S: x \ne 0$ (that is, at least one element of $S$ is non-zero).


The greatest common divisor of $S$:
:$\gcd \paren S = \gcd \set {a_1, a_2, \ldots, a_n}$

is defined as the (strictly) positive integer $d \in \Z_{>0}$ such that:

 
 
 
 

where $\divides$ denotes divisibility.


By convention:
:$\map \gcd \O = 1$


By convention:
:$\map \gcd \O = 1$

=== Polynomial Ring over Field ===
Let $F$ be a field.

Let $P, Q, R \in F \sqbrk X$ be polynomials.


Then $R$ is the greatest common divisor of $P$ and $Q$  if and only if  it is a monic greatest common divisor.

This is denoted $\gcd \set {P, Q} = R$.

=== Real Numbers ===

The concept can be extended to the set of real numbers:
Let $a, b \in \R$ be commensurable.

Then there exists a greatest element $d \in \R_{>0}$ such that:
:$d \divides a$
:$d \divides b$
where $d \divides a$ denotes that $d$ is a divisor of $a$.


This is called the greatest common divisor of $a$ and $b$ and denoted $\gcd \set {a, b}$.",Definition:Greatest Common Divisor,"['Definitions/Greatest Common Divisor', 'Definitions/Number Theory', 'Definitions/Integral Domains', 'Definitions/Polynomial Theory']"
Definition:Common,Common,"Let $a, b \in \R$ be commensurable.

Then there exists a greatest element $d \in \R_{>0}$ such that:
:$d \divides a$
:$d \divides b$
where $d \divides a$ denotes that $d$ is a divisor of $a$.


This is called the greatest common divisor of $a$ and $b$ and denoted $\gcd \set {a, b}$.",Definition:Greatest Common Divisor/Real Numbers,"['Definitions/Euclidean Number Theory', 'Definitions/Real Analysis', 'Definitions/Greatest Common Divisor']"
Definition:Common,Common,"Let $S$ be a finite set of non-zero integers, that is:

:$S = \set {x_1, x_2, \ldots, x_n: \forall k \in \N^*_n: x_k \in \Z, x_k \ne 0}$


Let $m \in \Z$ such that all the elements of $S$ divide $m$, that is:

:$\forall x \in S: x \divides m$


Then $m$ is a common multiple of all the elements in $S$.

 ",Definition:Common Multiple,['Definitions/Number Theory']
Definition:Common,Common,"=== Integral Domain ===
Let $D$ be an integral domain and let $a, b \in D$ be nonzero.

$l$ is the lowest common multiple of $a$ and $b$  if and only if :
:$(1): \quad$ both $a$ and $b$ divide $l$
:$(2): \quad$ if $m$ is another element such that $a$ and $b$ divide $m$, then $l$ divides $m$.

=== Integers ===
=== Definition 1 ===
For all $a, b \in \Z: a b \ne 0$, there exists a smallest $m \in \Z: m > 0$ such that $a \divides m$ and $b \divides m$.

This $m$ is called the lowest common multiple of $a$ and $b$, and denoted $\lcm \set {a, b}$.

=== Definition 2 ===
Let $a, b \in \Z$ be integers such that $a b \ne 0$.

Then the lowest common multiple of $a$ and $b$ is the (strictly) positive integer $m$ which satisfies:
:$(1): \quad a \divides m$ and $b \divides m$
:$(2): \quad $If there exists $c \in \Z_{>0}$ such that $a \divides c$ and $b \divides c$, then $m \le c$
where $\divides$ denotes divisibility.

=== General Definition ===

This definition can be extended to any (finite) number of integers.
Let $S = \set {a_1, a_2, \ldots, a_n} \subseteq \Z$ such that $\ds \prod_{a \mathop \in S} a = 0$ (that is, all elements of $S$ are non-zero).

Then the lowest common multiple of $S$:
:$\map \lcm S = \lcm \set {a_1, a_2, \ldots, a_n}$

is defined as the smallest $m \in \Z_{>0}$ such that:
:$\forall x \in S: x \divides m$

where $\divides$ denotes divisibility.",Definition:Lowest Common Multiple,"['Definitions/Lowest Common Multiple', 'Definitions/Number Theory', 'Definitions/Integral Domains']"
Definition:Common,Common,"Let $\sequence {a_k}$ be the arithmetic sequence:

:$a_k = a_0 + k d$ for $k = 0, 1, 2, \ldots, n - 1$


The term $d$ is the common difference of $\sequence {a_k}$.",Definition:Arithmetic Sequence/Common Difference,['Definitions/Arithmetic Sequences']
Definition:Common,Common,"Let $\sequence {x_n}$ be a geometric sequence in $\R$ defined as:
:$x_n = a r^n$ for $n = 0, 1, 2, 3, \ldots$


The parameter:
:$r \in \R: r \ne 0$
is called the common ratio of $\sequence {x_n}$.",Definition:Geometric Sequence/Common Ratio,['Definitions/Geometric Sequences']
Definition:Common,Common,A vulgar fraction is a fraction representing a rational number whose numerator and denominator are both integers.,Definition:Fraction/Vulgar,"['Definitions/Vulgar Fractions', 'Definitions/Fractions']"
Definition:Common,Common,"Let $A$ and $B$ be planes.

The common section of $A$ and $B$ is the intersection of $A$ and $B$.",Definition:Common Section,['Definitions/Euclidean Geometry']
Definition:Commutative Algebra,Commutative Algebra,"Let $R$ be a commutative ring.

Let $\struct {A_R, \oplus}$ be an algebra over $R$.


Then $\struct {A_R, \oplus}$ is a commutative algebra  if and only if  $\oplus$ is a commutative operation.

That is:

:$\forall a, b \in A_R: a \oplus b = b \oplus a$",Definition:Commutative Algebra (Abstract Algebra),"['Definitions/Commutative Algebras', 'Definitions/Algebras', 'Definitions/Commutativity']"
Definition:Commutative Algebra,Commutative Algebra,Commutative algebra is the branch of abstract algebra concerned with commutative and unitary rings.,Definition:Commutative Algebra (Mathematical Branch),"['Definitions/Branches of Mathematics', 'Definitions/Commutative Algebra', 'Definitions/Abstract Algebra', 'Definitions/Ring Theory', 'Definitions/Module Theory', 'Definitions/Algebraic Geometry', 'Definitions/Algebraic Number Theory', 'Definitions/Commutativity']"
Definition:Compact,Compact,"=== Euclidean Space ===
Let $\R^n$ denote Euclidean $n$-space.

Let $H \subseteq \R^n$.


Then $H$ is compact in $\R^n$  if and only if  $H$ is closed and bounded.


=== Real Analysis ===

The same definition applies when $n = 1$, that is, for the real number line:

Let $\R$ be the real number line considered as a topological space under the Euclidean topology.

Let $H \subseteq \R$.

Let $\R$ be the real number line considered as a topological space under the Euclidean topology.

Let $H \subseteq \R$.


=== Definition 1 ===
Let $\R$ be the real number line considered as a topological space under the Euclidean topology.

Let $H \subseteq \R$.


$H$ is compact in $\R$  if and only if  $H$ is closed and bounded.

=== Definition 2 ===
Let $\R$ be the real number line considered as a topological space under the Euclidean topology.

Let $H \subseteq \R$.


$H$ is compact in $\R$  if and only if :
:when $H$ is the union of a set of neighborhoods which are open in $H$,
:then $H$ is also the union of a finite number of these neighborhoods.

=== Complex Analysis ===
Let $D$ be a subset of the complex plane $\C$.


Then $D$ is compact (in $\C$)  if and only if :
:$D$ is closed in $\C$
and
:$D$ is bounded in $\C$.

=== Topology ===
=== Definition 1 ===
A topological space $T = \struct {S, \tau}$ is compact  if and only if  every open cover for $S$ has a finite subcover.

=== Definition 2 ===
A topological space $T = \struct {S, \tau}$ is compact  if and only if  it satisfies the Finite Intersection Axiom.

=== Definition 3 ===
A topological space $T = \struct {S, \tau}$ is compact  if and only if  $\tau$ has a sub-basis $\BB$ such that:
:from every cover of $S$ by elements of $\BB$, a finite subcover of $S$ can be selected.

=== Definition 4 ===
A topological space $T = \struct {S, \tau}$ is compact  if and only if  every filter on $S$ has a limit point in $S$.

=== Definition 5 ===
A topological space $T = \struct {S, \tau}$ is compact  if and only if  every ultrafilter on $S$ converges.

=== Metric Space ===
Let $M = \struct {A, d}$ be a metric space.

Let $\tau$ denote the topology on $A$ induced by $d$.


Then $M$ is compact  if and only if  $\struct {A, \tau}$ is a compact topological space.


=== Complex Plane ===
Let $D$ be a subset of the complex plane $\C$.


Then $D$ is compact (in $\C$)  if and only if :
:$D$ is closed in $\C$
and
:$D$ is bounded in $\C$.

=== Normed Vector Space ===
Let $\struct {X, \norm {\,\cdot\,} }$ be a normed vector space. 

Let $K \subseteq X$.


Then $K$ is compact  if and only if  every sequence in $K$ has a convergent subsequence with limit $L \in K$.

That is, if:

:$\sequence {x_n}_{n \mathop \in \N} :\forall n \in \N : x_n \in K \implies \exists \sequence {x_{n_k} }_{k \mathop \in \N} : \exists L \in K: \ds  \lim_{k \mathop \to \infty} x_{n_k} = L$


=== Compact Subspace ===
Let $M = \struct{X, \norm {\,\cdot\,}}$ be a normed vector space.

Let $K \subseteq X$ be a subset of $X$.


The normed vector subspace $M_K = \struct {K, \norm {\,\cdot\,}_K}$ is compact in $M$  if and only if  $M_K$ is itself a compact normed vector space.",Definition:Compact Space,['Definitions/Compact Spaces']
Definition:Compact,Compact,"Let $D$ be a subset of the complex plane $\C$.


Then $D$ is compact (in $\C$)  if and only if :
:$D$ is closed in $\C$
and
:$D$ is bounded in $\C$.",Definition:Compact Space/Metric Space/Complex,"['Definitions/Compact Spaces', 'Definitions/Complex Analysis']"
Definition:Compact,Compact,"Let $\R^n$ denote Euclidean $n$-space.

Let $H \subseteq \R^n$.


Then $H$ is compact in $\R^n$  if and only if  $H$ is closed and bounded.


=== Real Analysis ===

The same definition applies when $n = 1$, that is, for the real number line:

Let $\R$ be the real number line considered as a topological space under the Euclidean topology.

Let $H \subseteq \R$.

Let $\R$ be the real number line considered as a topological space under the Euclidean topology.

Let $H \subseteq \R$.


=== Definition 1 ===
Let $\R$ be the real number line considered as a topological space under the Euclidean topology.

Let $H \subseteq \R$.


$H$ is compact in $\R$  if and only if  $H$ is closed and bounded.

=== Definition 2 ===
Let $\R$ be the real number line considered as a topological space under the Euclidean topology.

Let $H \subseteq \R$.


$H$ is compact in $\R$  if and only if :
:when $H$ is the union of a set of neighborhoods which are open in $H$,
:then $H$ is also the union of a finite number of these neighborhoods.

=== Complex Analysis ===
Let $D$ be a subset of the complex plane $\C$.


Then $D$ is compact (in $\C$)  if and only if :
:$D$ is closed in $\C$
and
:$D$ is bounded in $\C$.",Definition:Compact Space/Euclidean Space,"['Definitions/Euclidean Space', 'Definitions/Compact Spaces']"
Definition:Compact,Compact,"Let $\R$ be the real number line considered as a topological space under the Euclidean topology.

Let $H \subseteq \R$.


=== Definition 1 ===
Let $\R$ be the real number line considered as a topological space under the Euclidean topology.

Let $H \subseteq \R$.


$H$ is compact in $\R$  if and only if  $H$ is closed and bounded.

=== Definition 2 ===
Let $\R$ be the real number line considered as a topological space under the Euclidean topology.

Let $H \subseteq \R$.


$H$ is compact in $\R$  if and only if :
:when $H$ is the union of a set of neighborhoods which are open in $H$,
:then $H$ is also the union of a finite number of these neighborhoods.",Definition:Compact Space/Real Analysis,"['Definitions/Real Analysis', 'Definitions/Compact Spaces', 'Definitions/Compact Space (Real Analysis)']"
Definition:Compact,Compact,"=== Definition 1 ===
A topological space $T = \struct {S, \tau}$ is compact  if and only if  every open cover for $S$ has a finite subcover.

=== Definition 2 ===
A topological space $T = \struct {S, \tau}$ is compact  if and only if  it satisfies the Finite Intersection Axiom.

=== Definition 3 ===
A topological space $T = \struct {S, \tau}$ is compact  if and only if  $\tau$ has a sub-basis $\BB$ such that:
:from every cover of $S$ by elements of $\BB$, a finite subcover of $S$ can be selected.

=== Definition 4 ===
A topological space $T = \struct {S, \tau}$ is compact  if and only if  every filter on $S$ has a limit point in $S$.

=== Definition 5 ===
A topological space $T = \struct {S, \tau}$ is compact  if and only if  every ultrafilter on $S$ converges.",Definition:Compact Space/Topology,"['Definitions/Compact Spaces', 'Definitions/Topology']"
Definition:Compact,Compact,"Let $T = \struct {S, \tau}$ be a topological space.

Let $H \subseteq S$ be a subset of $S$.


=== Definition 1 ===
Let $T = \struct {S, \tau}$ be a topological space.

Let $H \subseteq S$ be a subset of $S$.


The topological subspace $T_H = \struct {H, \tau_H}$ is compact in $T$  if and only if  $T_H$ is itself a compact topological space.

=== Definition 2 ===

Let $T = \struct {S, \tau}$ be a topological space.

Let $H \subseteq S$ be a subset of $S$.


$H$ is compact in $T$  if and only if  every open cover $\CC \subseteq \tau$ for $H$ has a finite subcover.",Definition:Compact Space/Topology/Subspace,['Definitions/Compact Spaces']
Definition:Compact,Compact,"Let $M = \struct {A, d}$ be a metric space.

Let $\tau$ denote the topology on $A$ induced by $d$.


Then $M$ is compact  if and only if  $\struct {A, \tau}$ is a compact topological space.


=== Complex Plane ===
Let $D$ be a subset of the complex plane $\C$.


Then $D$ is compact (in $\C$)  if and only if :
:$D$ is closed in $\C$
and
:$D$ is bounded in $\C$.",Definition:Compact Space/Metric Space,['Definitions/Compact Spaces']
Definition:Compact,Compact,"Let $\struct {X, \norm {\,\cdot\,} }$ be a normed vector space. 

Let $K \subseteq X$.


Then $K$ is compact  if and only if  every sequence in $K$ has a convergent subsequence with limit $L \in K$.

That is, if:

:$\sequence {x_n}_{n \mathop \in \N} :\forall n \in \N : x_n \in K \implies \exists \sequence {x_{n_k} }_{k \mathop \in \N} : \exists L \in K: \ds  \lim_{k \mathop \to \infty} x_{n_k} = L$


=== Compact Subspace ===
Let $M = \struct{X, \norm {\,\cdot\,}}$ be a normed vector space.

Let $K \subseteq X$ be a subset of $X$.


The normed vector subspace $M_K = \struct {K, \norm {\,\cdot\,}_K}$ is compact in $M$  if and only if  $M_K$ is itself a compact normed vector space.",Definition:Compact Space/Normed Vector Space,['Definitions/Normed Vector Spaces']
Definition:Compact,Compact,"Let $\struct {S, \preceq}$ be an ordered set.

Let $x \in S$.


Then $x$ is compact (element)  if and only if  $x \ll x$

where $\ll$ denotes the way below relation.",Definition:Compact Element,['Definitions/Order Theory']
Definition:Compact,Compact,"Let $X$ be a topological space.

Let $M$ be a metric space.

Let $\left\langle{f_n}\right\rangle$ be a sequence of mappings $f_n : X \to M$.

Let $f: X \to M$ be a mapping.


Then $f_n$ converges compactly to $f$  if and only if  $f_n$ converges uniformly to $f$ on every compact subset of $X$.",Definition:Compact Convergence,['Definitions/Convergence']
Definition:Compact,Compact,"Let $\struct {X, \tau_1}$ be a topological space.

Let $\struct {Y, \tau_2}$ be a compact space.

Let $f: X \to Y$ be a topological embedding.

Let $\Img f$ be everywhere dense in $Y$.


Then either $f$ or $\struct {Y, \tau_2}$ may be called a compactification of $\struct {X, \tau_1}$.",Definition:Compactification,"['Definitions/Topology', 'Definitions/Compact Spaces']"
Definition:Compatible,Compatible,"Let $\struct {S, \circ}$ be a closed algebraic structure.

Let $\RR$ be a relation on $S$.


Then $\RR$ is compatible with $\circ$  if and only if :

:$\forall x, y, z \in S: x \mathrel \RR y \implies \paren {x \circ z} \mathrel \RR \paren {y \circ z}$

:$\forall x, y, z \in S: x \mathrel \RR y \implies \paren {z \circ x} \mathrel \RR \paren {z \circ y}$",Definition:Relation Compatible with Operation,['Definitions/Compatible Relations']
Definition:Compatible,Compatible,"Let $\struct {S, \circ}$ be a closed algebraic structure.

Let $\RR$ be a relation in $S$.


Then $\RR$ is strongly compatible with $\circ$  if and only if :

:$\forall x, y, z \in S: x \mathrel \RR y \iff \paren {x \circ z} \mathrel \RR \paren {y \circ z}$

:$\forall x, y, z \in S: x \mathrel \RR y \iff \paren {z \circ x} \mathrel \RR \paren {z \circ y}$.


That is,  if and only if  $\RR$ is compatible with $\circ$ and conversely compatible with $\circ$.",Definition:Relation Strongly Compatible with Operation,['Definitions/Compatible Relations']
Definition:Compatible,Compatible,"Let $F$ be a (unary) operation which can be applied to sets.


Then $F$ is compatible with set equivalence  if and only if :

:$F \sqbrk A = F \sqbrk B \iff A \sim B$

where:
:$A$ and $B$ are arbitrary sets
:$F \sqbrk A$ denotes the image of $A$ under $F$
:$\sim$ denotes set equivalence.",Definition:Operation Compatible with Set Equivalence,['Definitions/Set Theory']
Definition:Compatible,Compatible,"Let $\struct {R, +, \circ}$ be a ring whose zero is $0_R$.


An ordering $\preccurlyeq$ on $R$ is compatible with the ring structure of $R$  if and only if  $\preccurlyeq$ satisies the ring compatible ordering axioms:
 ",Definition:Ordering Compatible with Ring Structure,"['Definitions/Ordered Rings', 'Definitions/Ordered Integral Domains']"
Definition:Compatible,Compatible,"Let $\struct {M, \cdot}$ be a semigroup.

Let $\struct {R, +, \circ}$ be a ring.

Let $\sequence {R_n}_{n \mathop \in M}$ be a gradation of type $M$ on the additive group of $R$.


The gradation is compatible with the ring structure  if and only if 
:$\forall m, n \in M : \forall x \in S_m, y \in S_n: x \circ y \in S_{m \cdot n}$

and so:

:$S_m S_n \subseteq S_ {m \cdot n}$",Definition:Gradation Compatible with Ring Structure,['Definitions/Ring Theory']
Definition:Compatible,Compatible,"Let $\struct {S, \circ}$ be a closed algebraic structure.

Let $\RR$ be a relation in $S$.


Then $\RR$ is conversely compatible with $\circ$  if and only if :

:$\forall x, y, z \in S: \paren {x \circ z} \mathrel \RR \paren {y \circ z} \implies x \mathrel \RR y$

:$\forall x, y, z \in S: \paren {z \circ x} \mathrel \RR \paren {z \circ y} \implies x \mathrel \RR y$",Definition:Relation Conversely Compatible with Operation,['Definitions/Compatible Relations']
Definition:Compatible,Compatible,"A relation $\RR$ is universally compatible on a set $S$  if and only if  it is compatible with every closed operation that can be defined on $S$.

Category:Definitions/Abstract Algebra",Definition:Universally Compatible Relation,['Definitions/Abstract Algebra']
Definition:Compatible,Compatible,"Let $\struct {S, \circ}$ be an algebraic structure.

Let $\RR$ be an equivalence relation on $S$.


Then $\RR$ is a congruence relation for $\circ$  if and only if :

:$\forall x_1, x_2, y_1, y_2 \in S: \paren {x_1 \mathrel \RR x_2} \land \paren {y_1 \mathrel \RR y_2} \implies \paren {x_1 \circ y_1} \mathrel \RR \paren {x_2 \circ y_2}$",Definition:Congruence Relation,"['Definitions/Abstract Algebra', 'Definitions/Equivalence Relations']"
Definition:Compatible,Compatible,"Let $\UU_1$ and $\UU_2$ be quasiuniformities on a set $S$.

Let $\struct {\struct {S, \UU_1}, \tau_1}$ and $\struct {\struct {S, \UU_2}, \tau_2}$ be the quasiuniform spaces generated by $\UU_1$ and $\UU_2$.


Then $\UU_1$ and $\UU_2$ are compatible (with each other)  if and only if  their topologies are equal.

That is,  if and only if  $\tau_1 = \tau_2$.",Definition:Compatible Quasiuniformities,['Definitions/Uniformities']
Definition:Compatible,Compatible,"Let $M$ be a topological space.

Let $\mathscr F, \mathscr G$ be $d$-dimensional atlases of class $C^k$ on $M$.


=== Definition 1 ===
Let $M$ be a topological space.

Let $\mathscr F, \mathscr G$ be $d$-dimensional atlases of class $C^k$ on $M$.


$\mathscr F, \mathscr G$ are $C^k$-compatible  if and only if  their union $\mathscr F \cup \mathscr G$ is an atlas of class $C^k$.

=== Definition 2 ===
Let $M$ be a topological space.

Let $\mathscr F, \mathscr G$ be $d$-dimensional atlases of class $C^k$ on $M$.


$\mathscr F$ and $\mathscr G$ are $C^k$-compatible  if and only if  every pair of charts $\struct {U, \phi} \in \mathscr F$ and $\struct {V, \psi} \in \mathscr G$ are $C^k$-compatible.",Definition:Compatible Atlases,"['Definitions/Compatible Atlases', 'Definitions/Manifolds']"
Definition:Compatible,Compatible,"Let $M$ be a topological space.

Let $d$ be a natural number.

Let $\struct {U, \phi}$ and $\struct {V, \psi}$ be $d$-dimensional charts of $M$.


Then $\struct {U, \phi}$ and $\struct {V, \psi}$ are $C^k$-compatible  if and only if  their transition mapping:
:$\psi \circ \phi^{-1}: \map \phi {U \cap V} \to \map \psi {U \cap V}$
is of class $C^k$.


=== Smoothly Compatible Charts ===
Let $M$ be a topological space.

Let $d$ be a natural number.

Let $\struct {U, \phi}$ and $\struct {V, \psi}$ be $d$-dimensional charts of $M$.


$\struct {U, \phi}$ and $\struct {V, \psi}$ are smoothly compatible  if and only if  their transition mapping:
:$\psi \circ \phi^{-1} : \map \phi {U \cap V} \to \map \psi {U \cap V}$
is of class $C^\infty$.


Category:Definitions/Manifolds",Definition:Compatible Charts,['Definitions/Manifolds']
Definition:Compatible,Compatible,"Let $M$ be a topological space.

Let $d$ be a natural number.

Let $\struct {U, \phi}$ and $\struct {V, \psi}$ be $d$-dimensional charts of $M$.


$\struct {U, \phi}$ and $\struct {V, \psi}$ are smoothly compatible  if and only if  their transition mapping:
:$\psi \circ \phi^{-1} : \map \phi {U \cap V} \to \map \psi {U \cap V}$
is of class $C^\infty$.


Category:Definitions/Manifolds",Definition:Compatible Charts/Smooth,['Definitions/Manifolds']
Definition:Compatible,Compatible,"Let $M$ be a topological space.

Let $A$ be a $d$-dimensional $C^k$-atlas on $M$.

Let $\struct {U, \phi}$ be a $d$-dimensional chart of $M$.


Then $\struct {U, \phi}$ is $C^k$-compatible with $A$  if and only if  $\struct {U, \phi}$ is $C^k$-compatible with every chart of $A$.

Category:Definitions/Manifolds",Definition:Chart Compatible with Atlas,['Definitions/Manifolds']
Definition:Compatible,Compatible,"Let $A$ and $B$ be rings.

Let $\struct {M, +}$ be an abelian group.

Let $* : A \times M \to M$ and $\circledast: B \times M \to M$ be left or right linear ring actions so that:
:$(1): \quad \struct {M, +, *}$ is a left or right module over $A$
:$(2): \quad \struct {M, +, \circledast}$ is a left or right module over $B$


=== Definition 1 ===
Let $A$ and $B$ be rings.

Let $\struct {M, +}$ be an abelian group.

Let $* : A \times M \to M$ and $\circledast: B \times M \to M$ be left or right linear ring actions so that:
:$(1): \quad \struct {M, +, *}$ is a left or right module over $A$
:$(2): \quad \struct {M, +, \circledast}$ is a left or right module over $B$


The module structures are compatible  if and only if  for all $a \in A$, $b \in B$, the homotheties $h_a$ and $h_b$ commute.

That is, for all $m \in M$, $a \in A$, $b \in B$:
:$a * \paren {b \circledast m} = b \circledast \paren {a * m}$

=== Definition 2 ===
Let $A$ and $B$ be rings.

Let $\struct {M, +}$ be an abelian group.

Let $* : A \times M \to M$ and $\circledast: B \times M \to M$ be left or right linear ring actions so that:
:$(1): \quad \struct {M, +, *}$ is a left or right module over $A$
:$(2): \quad \struct {M, +, \circledast}$ is a left or right module over $B$


The module structures are compatible  if and only if  for all $a \in A$, the homothety $h_a : M \to M$ is an endomorphism of the $B$-module $M$.

That is,  if and only if  the image of the ring representation $A \to \map {\operatorname {End} } M$ is contained in the endomorphism ring $\map {\operatorname {End}_B } M$.

=== Definition 3 ===
Let $A$ and $B$ be rings.

Let $\struct {M, +}$ be an abelian group.

Let $* : A \times M \to M$ and $\circledast: B \times M \to M$ be left or right linear ring actions so that:
:$(1): \quad \struct {M, +, *}$ is a left or right module over $A$
:$(2): \quad \struct {M, +, \circledast}$ is a left or right module over $B$


The module structures are compatible  if and only if  for all $b \in A$, the homothety $h_b : M \to M$ is an endomorphism of the $A$-module $M$.

That is,  if and only if  the image of the ring representation $B \to \map {\operatorname {End} } M$ is contained in the endomorphism ring $\map {\operatorname {End}_A} M$.",Definition:Compatible Module Structures,"['Definitions/Module Theory', 'Definitions/Compatible Module Structures']"
Definition:Complement,Complement,":

Let $\angle BAC$ be a right angle.

Let $\angle BAD + \angle DAC = \angle BAC$.

That is:
:$\angle DAC = \angle BAC - \angle BAD$


Then $\angle DAC$ is the complement of $\angle BAD$.


Hence, for any angle $\alpha$ (whether less than a right angle or not), the complement of $\alpha$ is $\dfrac \pi 2 - \alpha$.

Measured in degrees, the complement of $\alpha$ is $90^\circ - \alpha$.


If $\alpha$ is the complement of $\beta$, then it follows that $\beta$ is the complement of $\alpha$.

Hence we can say that $\alpha$ and $\beta$ are complementary.


It can be seen from this that the complement of an angle greater than a right angle is negative.


Thus complementary angles are two angles whose measures add up to the measure of a right angle.

That is, their measurements add up to $90$ degrees or $\dfrac \pi 2$ radians.",Definition:Complementary Angles,['Definitions/Angles']
Definition:Complement,Complement,"Let $ABDC$ and $EFHG$ be two parallelograms with the same angles, which share a diagonal, such that $ABDC \cap EFHG \ne \O$.

:

Then the two parallelograms $CIGK$ and $BJHL$ are known as the complements of the parallelograms $ABDC$ and $EFHG$.",Definition:Complements of Parallelograms,['Definitions/Parallelograms']
Definition:Complement,Complement,"Let $\RR \subseteq S \times T$ be a relation.


The complement of $\RR$ is the relative complement of $\RR$ with respect to $S \times T$:
:$\relcomp {S \times T} \RR := \set {\tuple {s, t} \in S \times T: \tuple {s, t} \notin \RR}$


If the sets $S$ and $T$ are implicit, then $\map \complement \RR$ can be used.",Definition:Complement of Relation,['Definitions/Relation Theory']
Definition:Complement,Complement,"The set complement (or, when the context is established, just complement) of a set $S$ in a universe $\mathbb U$ is defined as:

:$\map \complement S = \relcomp {\mathbb U} S = \mathbb U \setminus S$

See the definition of Relative Complement for the definition of $\relcomp {\mathbb U} S$.


Thus the complement of a set $S$ is the relative complement of $S$ in the universe, or the complement of $S$ relative to the universe.

A common alternative to the symbology $\map \complement S$, which we will sometimes use, is $\overline S$.",Definition:Set Complement,"['Definitions/Set Complement', 'Definitions/Set Theory']"
Definition:Complement,Complement,"Let $S$ be a set, and let $T \subseteq S$, that is: let $T$ be a subset of $S$.

Then the set difference $S \setminus T$ can be written $\relcomp S T$, and is called the relative complement of $T$ in $S$, or the complement of $T$ relative to $S$.

Thus:
:$\relcomp S T = \set {x \in S : x \notin T}$",Definition:Relative Complement,"['Definitions/Set Theory', 'Definitions/Relative Complement']"
Definition:Complement,Complement,"The (logical) complement of a propositional formula $\mathbf A$ is the negation of $\mathbf A$, that is, $\neg \mathbf A$.

Conversely, the complement of $\neg \mathbf A$ is defined to be $\mathbf A$.


=== Complementary Pair ===
For any propositional formula $\mathbf A$, the set $\left\{{\mathbf A, \neg \mathbf A}\right\}$ is called a complementary pair of formulas.",Definition:Logical Complement,['Definitions/Propositional Logic']
Definition:Complement,Complement,"Let $\struct {S, \vee, \wedge, \preceq}$ be a bounded lattice.

Denote by $\bot$ and $\top$ the bottom and top of $S$, respectively.

Let $a \in S$.


Then $b \in S$ is called a complement of $a$  if and only if :

:$b \vee a = \top$
:$b \wedge a = \bot$


If $a$ has a unique complement, it is denoted by $\neg a$.


=== Complemented Lattice ===
Let $\struct {S, \vee, \wedge, \preceq}$ be a bounded lattice.

Suppose that every $a \in S$ admits a complement.


Then $\struct {S, \vee, \wedge, \preceq}$ is called a complemented lattice.",Definition:Complement (Lattice Theory),['Definitions/Lattice Theory']
Definition:Complement,Complement,"=== Simple Graph ===
Let $G = \struct {V, E}$ be a simple graph.

The complement of $G$ is the simple graph $\overline G = \struct {V, \overline E}$ which consists of:
:The same vertex set $V$ of $G$
:The set $\overline E$ defined such that $\set {u, v} \in \overline E \iff \set {u, v} \notin E$, where $u$ and $v$ are distinct.

=== Loop-Graph ===
Let $G = \struct {V, E}$ be a loop-graph.

The complement of $G$ is the loop-graph $\overline G = \struct {V, \overline E}$ which consists of:
:The same vertex set $V$ of $G$;
:The set $\overline E$ defined such that:
::$\set {u, v} \in \overline E \iff \set {u, v} \notin E$
::$\set {v, v} \in \overline E \iff \set {v, v} \notin E$


That is, the complement $\overline G$ of a loop-graph $G$ has loops on all vertices where there are no loops in $G$.",Definition:Complement (Graph Theory),"['Definitions/Complements of Graphs', 'Definitions/Graph Theory']"
Definition:Complete,Complete,"=== Definition 1 ===
A metric space $M = \struct {A, d}$ is complete  if and only if  every Cauchy sequence is convergent.

=== Definition 2 ===
A metric space $M = \struct {A, d}$ is complete  if and only if  the intersection of every nested sequence of closed balls whose radii tend to zero is non-empty.",Definition:Complete Metric Space,"['Definitions/Complete Metric Spaces', 'Definitions/Metric Spaces']"
Definition:Complete,Complete,"Let $T = \struct {S, \tau}$ be a topological space.

Let $M = \struct {S, d}$ be a complete metric space such that $\struct {S, \tau}$ is the topological space induced by $d$.


If there exists such a complete metric space, then $T$ is described as topologically complete.",Definition:Topologically Complete Space,"['Definitions/Complete Metric Spaces', 'Definitions/Topology']"
Definition:Complete,Complete,"=== Definition 1 ===
Let $\struct {S, \preceq}$ be a lattice.


Then $\struct {S, \preceq}$ is a complete lattice  if and only if :

:$\forall T \subseteq S: T$ admits both a supremum and an infimum.

=== Definition 2 ===
Let $\struct {S, \preceq}$ be an ordered set.


Then $\struct {S, \preceq}$ is a complete lattice  if and only if :

:$\forall S' \subseteq S: \inf S', \sup S' \in S$

That is,  if and only if  all subsets of $S$ have both a supremum and an infimum.",Definition:Complete Lattice,"['Definitions/Lattice Theory', 'Definitions/Complete Lattices']"
Definition:Complete,Complete,"Let $\struct {S, \preceq}$ be an ordered set.

Then $\struct {S, \preceq}$ has the Dedekind completeness property  if and only if  every non-empty subset of $S$ that is bounded above admits a supremum (in $S$).",Definition:Dedekind Completeness Property,"['Definitions/Dedekind Completeness Property', 'Definitions/Order Theory']"
Definition:Complete,Complete,An inductive ordered set is an ordered set in which every chain has an upper bound.,Definition:Inductive Ordered Set,"['Definitions/Set Theory', 'Definitions/Order Theory']"
Definition:Complete,Complete,"Let $S$ be a set of truth functions.


Then $S$ is functionally complete  if and only if  all possible truth functions are definable from $S$.",Definition:Functionally Complete,['Definitions/Truth Functions']
Definition:Complete,Complete,"Let $\LL$ be a logical language.

Let $\mathscr P$ be a proof system for $\LL$, and let $\mathscr M$ be a formal semantics for $\LL$.


Then $\mathscr P$ is said to be complete for $\mathscr M$  if and only if :

:Every $\mathscr M$-tautology is a $\mathscr P$-theorem.

Symbolically, this can be expressed as the statement that, for every logical formula $\phi$ of $\LL$:

:$\models_{\mathscr M} \phi$ implies $\vdash_{\mathscr P} \phi$


=== Strongly Complete Proof System ===
Let $\LL$ be a logical language.

Let $\mathscr P$ be a proof system for $\LL$, and let $\mathscr M$ be a formal semantics for $\LL$.


$\mathscr P$ is strongly complete for $\mathscr M$  if and only if :

:Every $\mathscr M$-semantic consequence is a $\mathscr P$-provable consequence.

Symbolically, this can be expressed as the statement that, for every collection $\FF$ of logical formulas, and every logical formula $\phi$ of $\LL$:

:$\FF \models_{\mathscr M} \phi$ implies $\FF \vdash_{\mathscr P} \phi$",Definition:Complete Proof System,"['Definitions/Complete Proof Systems', 'Definitions/Proof Systems']"
Definition:Complete,Complete,"Let $\LL$ be a language.

Let $\mathscr M$ be a formal semantics for $\LL$.

Let $T$ be an $\LL$-theory.


$T$ is complete (with respect to $\LL$ and $\mathscr M$)  if and only if :
:$T$ is satisfiable for $\mathscr M$
:for every $\LL$-sentence $\phi$, either $T \models_{\mathscr M} \phi$ or $T \models_{\mathscr M} \neg \phi$
where $T \models_{\mathscr M} \phi$ denotes semantic entailment.",Definition:Complete Theory,"['Definitions/Model Theory', 'Definitions/Formal Semantics']"
Definition:Complete,Complete,"Let $G = \struct {V, E}$ be a simple graph such that every vertex is adjacent to every other vertex.

Then $G$ is called complete.


The complete graph of order $p$ is denoted $K_p$.",Definition:Complete Graph,['Definitions/Complete Graphs']
Definition:Complete,Complete,"A complete bipartite graph is a bipartite graph $G = \struct {A \mid B, E}$ in which every vertex in $A$ is adjacent to every vertex in $B$.

The complete bipartite graph where $A$ has $m$ vertices and $B$ has $n$ vertices is denoted $K_{m, n}$.",Definition:Complete Bipartite Graph,"['Definitions/Complete Bipartite Graphs', 'Definitions/Bipartite Graphs', 'Definitions/Graph Theory']"
Definition:Completion,Completion,"Let $\struct {S, \preceq_S}$ be an ordered set.


An ordered set $\struct {T, \preceq_T}$ is an order completion of $S$  if and only if :

:$(1):\quad S \subseteq T$

:$(2):\quad {\preceq_T \restriction_S} = {\preceq_S}$, where $\restriction$ denotes restriction

:$(3):\quad \struct {T, \preceq_T}$ is a complete ordered set

:$(4):\quad$ For all ordered sets $\struct {T', \preceq_{T'} }$ satisfying $(1), (2)$ and $(3)$, there is a unique increasing injection $\phi: T' \to T$",Definition:Order Completion,['Definitions/Order Theory']
Definition:Completion,Completion,"Let $\struct {X, \Sigma, \mu}, \struct {\tilde X, \Sigma^*, \bar \mu}$ be measure spaces.

Then:
:$\struct {\tilde X, \Sigma^*, \bar \mu}$ is a completion of $\struct {X, \Sigma, \mu}$
or:
:$\struct {\tilde X, \Sigma^*, \bar \mu}$ completes $\struct {X, \Sigma, \mu}$

 if and only if  the following conditions hold:

:$(1): \quad \struct {\tilde X, \Sigma^*, \bar \mu}$ is a complete measure space
:$(2): \quad \tilde X = X$
:$(3): \quad \Sigma$ is a sub-$\sigma$-algebra of $\Sigma^*$
:$(4): \quad \forall E \in \Sigma: \map {\bar \mu} E = \map \mu E$, that is: $\bar \mu \restriction_\Sigma = \mu$",Definition:Completion (Measure Space),['Definitions/Measure Theory']
Definition:Completion,Completion,"Let $M_1 = \struct {A, d}$ and $M_2 = \struct {\tilde A, \tilde d}$ be metric spaces.

Then $M_2$ is a completion of $M_1$, or $M_2$ completes $M_1$,  if and only if :
:$(1): \quad M_2$ is a complete metric space
:$(2): \quad A \subseteq \tilde A$
:$(3): \quad A$ is dense in $M_2$
:$(4): \quad \forall x, y \in A : \map {\tilde d} {x, y} = \map d {x, y}$. In terms of restriction of functions, this says that $\map {\tilde d {\restriction_A} } = d$.


It is immediate from this definition that a completion of a metric space $M_1$ consists of:
:A complete metric space $M_2$
:An isometry $\phi : A \to \tilde A$
such that $\map \phi A = \set {\map \phi x: x \in A}$ is dense in $M_2$.

An isometry is often required to be bijective, so here one should consider $\phi$ as a mapping from $A$ to the image of $\phi$.

Therefore to insist that $\phi$ be an isometry, in this context, is to say that $\phi$ must be an injection that preserves the metric of $M_1$.",Definition:Completion (Metric Space),['Definitions/Complete Metric Spaces']
Definition:Completion,Completion,"Let $\struct {R_1, \norm {\, \cdot \,}_1}$ and $\struct {R_2, \norm {\, \cdot \,}_2}$ be normed division rings.

Let $M_1 = \struct {R_1, d_1}$ and $M_2 = \struct {R_2, d_2}$ be the metric spaces where $d_1: R_1 \times R_1 \to \R_{\ge 0}$ and $d_2: R_2 \times R_2 \to \R_{\ge 0}$ are the metrics induced by $\norm {\, \cdot \,}_1$ and $\norm {\, \cdot \,}_2$ respectively.


Then $\struct {R_2, \norm {\, \cdot \,}_2}$ is a completion of $\struct {R_1, \norm {\, \cdot \,}_1}$  if and only if :
:$(1): \quad$ there exists a distance-preserving ring monomorphism $\phi: R_1 \to R_2$
:$(2): \quad M_2$ is a metric completion of $\map \phi {M_1}$.


That is, $\struct {R_2, \norm{\,\cdot\,}_2}$ is a completion of $\struct {R_1,\norm{\,\cdot\,}_1}$  if and only if :
:$(a): \quad M_2$ is a complete metric space
:$(b): \quad$ there exists a distance-preserving ring monomorphism $\phi: R_1 \to R_2$
:$(c): \quad \map \phi {R_1}$ is a dense subspace in $M_2$.",Definition:Completion (Normed Division Ring),['Definitions/Normed Division Rings']
Definition:Complex,Complex,Complex analysis is a branch of mathematics that studies complex functions.,Definition:Analysis/Complex,"['Definitions/Branches of Mathematics', 'Definitions/Complex Analysis', 'Definitions/Analysis']"
Definition:Complex,Complex,"
=== Informal Definition ===
A complex number is a number in the form $a + b i$ or $a + i b$ where:
:$a$ and $b$ are real numbers
:$i$ is a square root of $-1$, that is, $i = \sqrt {-1}$.

=== Formal Definition ===
A complex number is an ordered pair $\tuple {x, y}$ where $x, y \in \R$ are real numbers, on which the operations of addition and multiplication are defined as follows:


=== Complex Addition ===
Let $\tuple {x_1, y_1}$ and $\tuple {x_2, y_2}$ be complex numbers.

Then $\tuple {x_1, y_1} + \tuple {x_2, y_2}$ is defined as:

:$\tuple {x_1, y_1} + \tuple {x_2, y_2}:= \tuple {x_1 + x_2, y_1 + y_2}$

=== Complex Multiplication ===
Let $\tuple {x_1, y_1}$ and $\tuple {x_2, y_2}$ be complex numbers.


Then $\tuple {x_1, y_1} \tuple {x_2, y_2}$ is defined as:

:$\tuple {x_1, y_1} \tuple {x_2, y_2} := \tuple {x_1 x_2 - y_1 y_2, x_1 y_2 + y_1 x_2}$

=== Scalar Product ===
Let $\tuple {x, y}$ be a complex number.

Let $m \in \R$ be a real number.


Then $m \tuple {x, y}$ is defined as:

:$m \tuple {x, y} := \tuple {m x, m y}$

=== Construction from Cayley-Dickson Construction ===
The complex numbers can be defined by the Cayley-Dickson construction from the set of real numbers $\R$.

From Real Numbers form Algebra, $\R$ forms a nicely normed $*$-algebra.

Let $a, b \in \R$.

Then $\tuple {a, b} \in \C$, where:

:$\tuple {a, b} \tuple {c, d} = \tuple {a c - d \overline b, \overline a d + c b}$
:$\overline {\tuple {a, b} } = \tuple {\overline a, -b}$
where:
:$\overline a$ is the conjugate of $a$
and 
:$\overline {\tuple {a, b} }$ is the conjugation operation on $\C$.

From Real Numbers form Algebra, $\overline a = a$ and so the above translate into:

:$\tuple {a, b} \tuple {c, d} = \tuple {a c - d b, a d + c b}$
:$\overline {\tuple {a, b} } = \tuple {a, -b}$


It is clear by direct comparison with the formal definition that this construction genuinely does generate the complex numbers.",Definition:Complex Number,"['Definitions/Complex Numbers', 'Definitions/Standard Number Fields', 'Definitions/Numbers', 'Definitions/Complex Analysis', 'Definitions/Analysis']"
Definition:Complex,Complex,"A complex function is a function whose domain and codomain are subsets of the set of complex numbers $\C$.


=== Independent Variable ===
Let $f: \C \to \C$ be a complex function.

Let $\map f z = w$.


Then $z$ is referred to as an independent variable (of $f$).

=== Dependent Variable ===
Let $f: \C \to \C$ be a complex function.

Let $\map f z = w$.


Then $w$ is referred to as the dependent variable (of $f$).",Definition:Complex Function,"['Definitions/Complex Analysis', 'Definitions/Complex Functions']"
Definition:Complex,Complex,"Let $z = a + i b$ be a complex number.


Then the (complex) conjugate of $z$ is denoted $\overline z$ and is defined as:

:$\overline z := a - i b$


That is, you get the complex conjugate of a complex number by negating its imaginary part.


=== Complex Conjugation ===
The operation of complex conjugation is the mapping:
: $\overline \cdot: \C \to \C: z \mapsto \overline z$.
where $\overline z$ is the complex conjugate of $z$.


That is, it maps a complex number to its complex conjugate.


Category:Definitions/Complex Conjugates",Definition:Complex Conjugate,"['Definitions/Complex Conjugates', 'Definitions/Complex Numbers', 'Definitions/Complex Analysis']"
Definition:Complex,Complex,"Let $\struct {S, \circ}$ be an algebraic structure.


We can define an operation on the power set $\powerset S$ as follows:

:$\forall A, B \in \powerset S: A \circ_\PP B = \set {a \circ b: a \in A, b \in B}$


This is called the operation induced on $\powerset S$ by $\circ$, and $A \circ_\PP B$ is called the subset product of $A$ and $B$.


It is usual to write $A \circ B$ for $A \circ_\PP B$.


=== Subset Product with Singleton ===

When one of the subsets in a subset product is a singleton, we can (and often do) dispose of the set braces. Thus:
Let $\struct {S, \circ}$ be an algebraic structure.


Let $A \subseteq S$ be a subset of $S$.

Then:
:$(1): \quad a \circ S := \set a \circ S$
:$(2): \quad S \circ a := S \circ \set a$

where $\set a \circ S$ and $S \circ \set a$ denote the subset product of $\set a$ with $S$.


That is:
:$a \circ S = \set {a \circ s: s \in S}$
:$S \circ a = \set {s \circ a: s \in S}$",Definition:Subset Product,"['Definitions/Abstract Algebra', 'Definitions/Group Theory', 'Definitions/Subset Products']"
Definition:Complex,Complex,"Let $G$ be a group.

Let $K \subseteq G$ be a subset of $G$.


Then $K$ is referred to by some sources as a complex of elements of $G$.",Definition:Complex (Group Theory),"['Definitions/Group Theory', 'Definitions/Subsets', 'Definitions/Complexes of Groups']"
Definition:Complex,Complex,"Let $R$ be a commutative ring with unity.

Let $\ds M = \bigoplus_{n \mathop \in \Z} M^n$ be a $\Z$-graded $R$-module that is also a differential module with differential $\d$.


Then $M$ is a differential complex if $\d$ satisfies:

:$\map \d {M^n} \subseteq M^{n + 1}$

for all $n \in \Z$.


The notation $\d_n := \d \restriction_{M_n}$ is often seen.",Definition:Differential Complex,['Definitions/Homological Algebra']
Definition:Complex,Complex,A complex fraction is a fraction such that the numerator or denominator or both are themselves fractions.,Definition:Fraction/Complex,"['Definitions/Complex Fractions', 'Definitions/Fractions']"
Definition:Component,Component,A substatement of a compound statement is one of the statements that comprise it.,Definition:Compound Statement/Substatement,['Definitions/Compound Statements']
Definition:Component,Component,"Let $S$ be a set.

Let $\mathbb S = \set {S_1 \mid S_2 \mid \cdots}$ be a partition of $S$.


The elements $S_1, S_2, \ldots \in \mathbb S$ are known as the components of the partition.


Category:Definitions/Set Partitions",Definition:Set Partition/Component,['Definitions/Set Partitions']
Definition:Component,Component,"Let $T = \left({S, \tau}\right)$ be a topological space.

Let the relation $\sim $ be defined on $T$ as follows:

:$x \sim y$  if and only if  $x$ and $y$ are connected in $T$.

That is,  if and only if  there exists a connected set of $T$ that contains both $x$ and $y$.


=== Definition 1 ===
Let $T = \struct {S, \tau}$ be a topological space.

Let the relation $\sim $ be defined on $T$ as follows:

:$x \sim y$  if and only if  $x$ and $y$ are connected in $T$.

That is,  if and only if  there exists a connected set of $T$ that contains both $x$ and $y$.


From Connectedness of Points is Equivalence Relation, $\sim$ is an equivalence relation.

From the Fundamental Theorem on Equivalence Relations, the points in $T$ can be partitioned into equivalence classes.

These equivalence classes are called the (connected) components of $T$.


If $x \in S$, then the component of $T$ containing $x$ (that is, the set of points $y \in S$ with $x \sim y$) is denoted by $\map {\operatorname {Comp}_x} T$.

=== Definition 2 ===
Let $T = \struct{S, \tau}$ be a topological space.

Let the relation $\sim $ be defined on $T$ as follows:

:$x \sim y$  if and only if  $x$ and $y$ are connected in $T$.

That is,  if and only if  there exists a connected set of $T$ that contains both $x$ and $y$.


The component of $T$ containing $x$ is defined as:

:$\ds \map {\operatorname{Comp}_x} T = \bigcup \leftset{A \subseteq S: x \in A \land A}$ is connected $\rightset{}$

=== Definition 3 ===
Let $T = \struct {S, \tau}$ be a topological space.

Let the relation $\sim $ be defined on $T$ as follows:

:$x \sim y$  if and only if  $x$ and $y$ are connected in $T$.

That is,  if and only if  there exists a connected set of $T$ that contains both $x$ and $y$.


The component of $T$ containing $x$ is defined as:
: the maximal connected set of $T$ that contains $x$.",Definition:Component (Topology),['Definitions/Connected Sets']
Definition:Component,Component,"Let $T$ be a topological space.


=== Equivalence Class ===
Let $T$ be a topological space.


Let $\sim$ be the equivalence relation on $T$ defined as:

:$x \sim y \iff x$ and $y$ are path-connected.

The equivalence classes of $\sim$ are called the path components of $T$.

If $x \in T$, then the path component of $T$ containing $x$ (that is, the set of points $y \in T$ with $x \sim y$) can be denoted by $\map {\operatorname{PC}_x} T$.


From Path-Connectedness is Equivalence Relation, $\sim $ is an equivalence relation. 

From the Fundamental Theorem on Equivalence Relations, the points in $T$ can be partitioned into equivalence classes.

=== Union of Path-Connected Sets ===
Let $T$ be a topological space.


The path component of $T$ containing $x$ is defined as:

:$\ds \map {\operatorname{PC}_x} T = \bigcup \leftset {A \subseteq S: x \in A \land A}$ is path-connected $\rightset {}$

Category:Definitions/Path-Connected Spaces

=== Maximal Path-Connected Set ===
Let $T$ be a topological space.


The path component of $T$ containing $x$ is defined as:
:the maximal path-connected set of $T$ that contains $x$.",Definition:Path Component,['Definitions/Path-Connected Spaces']
Definition:Component,Component,"Let $T$ be a topological space.

Let us define the relation $\sim$ on $T$ as follows:

:$x \sim y \iff x$ and $y$ are arc-connected.


We have that $\sim $ is an equivalence relation, so from the Fundamental Theorem on Equivalence Relations, the points in $T$ can be partitioned into equivalence classes.

These equivalence classes are called the arc components of $T$.


If $x \in T$, then the arc component of $T$ containing $x$ (that is, the set of points $y \in T$ with $x \sim y$) can be denoted by $\map {\operatorname {AC}_x} T$.",Definition:Arc Component,['Definitions/Arc-Connected Spaces']
Definition:Component,Component,"Let $T = \struct {S, \tau}$ be a topological space.


A subset $Y \subseteq S$ is an irreducible component of $T$  if and only if :
:$Y$ is irreducible
:$Y$ is not a proper subset of an irreducible subset of $S$.

That is,  if and only if :
:$Y$ is maximal in the ordered set of irreducible subsets of $S$, ordered by the subset relation.",Definition:Irreducible Component,['Definitions/Irreducible Spaces']
Definition:Component,Component,"Let $G$ be a graph.

Let $H$ be a subgraph of $G$ such that:

:$H$ is connected

:$H$ is not contained in any connected subgraph of $G$ which has more vertices or edges than $H$ has.


Then $H$ is a component of $G$.",Definition:Component of Graph,['Definitions/Graph Theory']
Definition:Component,Component,"Let $\mathbf a$ be a vector quantity embedded in an $n$-dimensional Cartesian coordinate system $C_n$.


Let $\mathbf a$ be represented with its initial point at the origin of $C_n$.

Let $\mathbf e_1, \mathbf e_2, \ldots, \mathbf e_n$ be the unit vectors in the positive direction of the coordinate axes of $C_n$.

Then:
:$\mathbf a = a_1 \mathbf e_1 + a_2 \mathbf e_2 + \cdots + a_3 \mathbf e_n$

where:
:$a_1 \mathbf e_1, a_2 \mathbf e_2, \ldots, a_3 \mathbf e_n$ are the component vectors of $\mathbf a$ in the directions of $\mathbf e_1, \mathbf e_2, \ldots, \mathbf e_n$ 
:$a_1, a_2, \ldots, a_3$ are the components of $\mathbf a$ in the directions of $\mathbf e_1, \mathbf e_2, \ldots, \mathbf e_n$.


The number of components in $\mathbf a$ is determined by the number of dimensions in the Cartesian coordinate system of its frame of reference.


A vector quantity with $n$ components can be referred to as an $n$-vector.


It is usually more convenient to write $\mathbf a$ as the ordered tuple $\tuple {a_1, a_2, \ldots, a_n}$ instead of $\mathbf a = a_1 \mathbf e_1 + a_2 \mathbf e_2 + \cdots + a_3 \mathbf e_n$.


There are two special cases:


=== Cartesian Plane ===
Let $\mathbf a$ be a vector quantity embedded in a Cartesian plane $P$.


Let $\mathbf a$ be represented with its initial point at the origin of $P$.

Let $\mathbf i$ and $\mathbf j$ be the unit vectors in the positive direction of the $x$-axis and $y$-axis.

Then:
:$\mathbf a = x \mathbf i + y \mathbf j$

where:
:$x \mathbf i$ and $y \mathbf j$ are the component vectors of $\mathbf a$ in the $\mathbf i$ and $\mathbf j$ directions
:$x$ and $y$ are the components of $\mathbf a$ in the $\mathbf i$ and $\mathbf j$ directions.


It is usually more convenient to write $\mathbf a$ as the ordered pair $\tuple {x, y}$ instead of $\mathbf a = x \mathbf i + y \mathbf j$.

=== Cartesian $3$-Space ===
Let $\mathbf a$ be a vector quantity embedded in Cartesian $3$-space $S$.

Let $\mathbf i$, $\mathbf j$ and $\mathbf k$ be the unit vectors in the positive directions of the $x$-axis, $y$-axis and $z$-axis respectively.

Then:
:$\mathbf a = x \mathbf i + y \mathbf j + z \mathbf k$

where:
:$x \mathbf i$, $y \mathbf j$ and $z \mathbf k$ are the component vectors of $\mathbf a$ in the $\mathbf i, \mathbf j, \mathbf k$ directions
:$x$, $y$ and $z$ are the components of $\mathbf a$ in the $\mathbf i$, $\mathbf j$ and $\mathbf k$ directions.

It is usual to arrange that the coordinate axes form a right-handed Cartesian $3$-space.


It is usually more convenient to write $\mathbf a$ as the ordered tuple $\tuple {x, y, z}$ instead of $\mathbf a = x \mathbf i + y \mathbf j + z \mathbf k$.


=== $x$ Component ===
Let:
:$\mathbf a = x \mathbf i + y \mathbf j + z \mathbf k$ be a vector quantity embedded in Cartesian $3$-space $S$

where $\mathbf i$, $\mathbf j$ and $\mathbf k$ be the unit vectors in the positive directions of the $x$-axis, $y$-axis and $z$-axis respectively.


The value $x$ is known as the $x$ component of $\mathbf a$.

=== $y$ Component ===
Let:
:$\mathbf a = x \mathbf i + y \mathbf j + z \mathbf k$ be a vector quantity embedded in Cartesian $3$-space $S$

where $\mathbf i$, $\mathbf j$ and $\mathbf k$ be the unit vectors in the positive directions of the $x$-axis, $y$-axis and $z$-axis respectively.


The value $y$ is known as the $y$ component of $\mathbf a$.

=== $z$ Component ===
Let:
:$\mathbf a = x \mathbf i + y \mathbf j + z \mathbf k$ be a vector quantity embedded in Cartesian $3$-space $S$

where $\mathbf i$, $\mathbf j$ and $\mathbf k$ be the unit vectors in the positive directions of the $x$-axis, $y$-axis and $z$-axis respectively.


The value $z$ is known as the $z$ component of $\mathbf a$.",Definition:Vector Quantity/Component,['Definitions/Vectors']
Definition:Component,Component,An electrical component is a device whose purpose is to control electricity in a specific manner.,Definition:Electrical Component,"['Definitions/Electrical Components', 'Definitions/Electronics']"
Definition:Composite,Composite,"=== Indexed Iteration ===
Let $\struct {G, *}$ be a magma.

Let $a, b \in \Z$ be integers.

Let $\closedint a b$ be the integer interval between $a$ and $b$.

Let $f: \closedint a b \to G$ be a mapping.


The indexed iteration of $*$ of $f$ from $a$ to $b$ is recursively defined and denoted:

:$\ds \prod_{k \mathop = a}^b \map f k = \begin {cases} \map f a & : b = a \\ \paren {\ds \prod_{k \mathop = a}^{b - 1} \map f k} * \map f b & : b > a \end {cases}$


For each ordered $n$-tuple $\tuple {a_1, a_2, \ldots, a_n} \in S^n$, the composite of $\tuple {a_1, a_2, \ldots, a_n}$ for $\oplus$ is the value at $\tuple {a_1, a_2, \ldots, a_n}$ of the $n$-ary operation defined by $\oplus$.


This composite is recursively defined and denoted:

 
 
 
 
 


=== Degenerate case ===

 

Let $\struct {G, *}$ be a unitary magma with identity $e$.

Let $a, b \in \Z$ be integers such that $a < b$.


Then:
:$\ds \prod_{i \mathop = a}^b \map f i = e$

=== Iteration over Finite Set ===
Let $\struct {G, *}$ be a commutative semigroup.

Let $S$ be a finite non-empty set.

Let $f: S \to G$ be a mapping.

Let $n \in \N$ be the cardinality of $S$.

Let $g: \N_{

=== Commutative Monoid ===

Let $G$ be a commutative monoid.

Let $S$ be a non-empty set.

Let $f: S \to G$ be a mapping

 

=== Iteration over Set with Finite Support ===
Let $\struct {G, *}$ be a commutative monoid.

Let $S$ be a set.

Let $f: S \to G$ be a mapping.

Let the support $\map \supp f$ be finite.


The iteration of $*$ of $f$ over $S$, denoted $\ds \prod_{s \mathop \in S} \map f s$, is the iteration over the finite set $\map \supp f$ of $f$:
:$\ds \prod_{s \mathop \in S} \map f s = \prod_{s \mathop \in \map \supp f} \map f s$",Definition:Iterated Binary Operation,['Definitions/Binary Operations']
Definition:Composite,Composite,"A composite number $c$ is a positive integer that has strictly more than two positive divisors.

That is, an integer greater than $1$ which is not prime is defined as composite.


 
: 
:A composite number is that which is measured by some number.
 ''
 

=== Sequence of Composite Numbers ===
 ",Definition:Composite Number,"['Definitions/Composite Numbers', 'Definitions/Number Theory']"
Definition:Composition,Composition,"Let $f_1: S_1 \to S_2$ and $f_2: S_2 \to S_3$ be mappings such that the domain of $f_2$ is the same set as the codomain of $f_1$.


=== Definition 1 ===
Let $S_1$, $S_2$ and $S_3$ be sets.

Let $f_1: S_1 \to S_2$ and $f_2: S_2 \to S_3$ be mappings such that the domain of $f_2$ is the same set as the codomain of $f_1$.


The composite mapping $f_2 \circ f_1$ is defined as:

:$\forall x \in S_1: \map {\paren {f_2 \circ f_1} } x := \map {f_2} {\map {f_1} x}$


:


=== Commutative Diagram ===
Let $S_1$, $S_2$ and $S_3$ be sets.

Let $f_1: S_1 \to S_2$ and $f_2: S_2 \to S_3$ be mappings such that the domain of $f_2$ is the same set as the codomain of $f_1$.


The concept of composition of mappings can be illustrated by means of a commutative diagram.

This diagram illustrates the specific example of $f_2 \circ f_1$:

::$\begin{xy}\xymatrix@+1em{
 S_1
  \ar[r]^*+{f_1}
  \ar@{-->}[rd]_*[l]+{f_2 \mathop \circ f_1}
&
 S_2
  \ar[d]^*+{f_2}

\\
&
 S_3
}\end{xy}$

=== Definition 2 ===
Let $S_1$, $S_2$ and $S_3$ be sets.

Let $f_1: S_1 \to S_2$ and $f_2: S_2 \to S_3$ be mappings such that the domain of $f_2$ is the same set as the codomain of $f_1$.


The composite of $f_1$ and $f_2$ is defined and denoted as:

:$f_2 \circ f_1 := \set {\tuple {x, z} \in S_1 \times S_3: \tuple {\map {f_1} x, z} \in f_2}$


:


=== Commutative Diagram ===
Let $S_1$, $S_2$ and $S_3$ be sets.

Let $f_1: S_1 \to S_2$ and $f_2: S_2 \to S_3$ be mappings such that the domain of $f_2$ is the same set as the codomain of $f_1$.


The concept of composition of mappings can be illustrated by means of a commutative diagram.

This diagram illustrates the specific example of $f_2 \circ f_1$:

::$\begin{xy}\xymatrix@+1em{
 S_1
  \ar[r]^*+{f_1}
  \ar@{-->}[rd]_*[l]+{f_2 \mathop \circ f_1}
&
 S_2
  \ar[d]^*+{f_2}

\\
&
 S_3
}\end{xy}$

=== Definition 3 ===
Let $S_1$, $S_2$ and $S_3$ be sets.

Let $f_1: S_1 \to S_2$ and $f_2: S_2 \to S_3$ be mappings such that the domain of $f_2$ is the same set as the codomain of $f_1$.


The composite of $f_1$ and $f_2$ is defined and denoted as:

:$f_2 \circ f_1 := \set {\tuple {x, z} \in S_1 \times S_3: \exists y \in S_2: \map {f_1} x = y \land \map {f_2} y = z}$


That is:
:$f_2 \circ f_1 := \set {\tuple {x, z} \in S_1 \times S_3: \exists y \in S_2: \tuple {x, y} \in f_1 \land \tuple {y, z} \in f_2}$


:


=== Commutative Diagram ===
Let $S_1$, $S_2$ and $S_3$ be sets.

Let $f_1: S_1 \to S_2$ and $f_2: S_2 \to S_3$ be mappings such that the domain of $f_2$ is the same set as the codomain of $f_1$.


The concept of composition of mappings can be illustrated by means of a commutative diagram.

This diagram illustrates the specific example of $f_2 \circ f_1$:

::$\begin{xy}\xymatrix@+1em{
 S_1
  \ar[r]^*+{f_1}
  \ar@{-->}[rd]_*[l]+{f_2 \mathop \circ f_1}
&
 S_2
  \ar[d]^*+{f_2}

\\
&
 S_3
}\end{xy}$

:

=== General Definition ===
Let $f_1: S_1 \to S_2, f_2: S_2 \to S_3, \ldots, f_n: S_n \to S_{n + 1}$ be mappings such that the domain of $f_k$ is the same set as the codomain of $f_{k - 1}$.


Then the composite of $f_1, f_2, \ldots, f_n$ is defined and denoted as:

 
 
 
 

=== Commutative Diagram ===
Let $S_1$, $S_2$ and $S_3$ be sets.

Let $f_1: S_1 \to S_2$ and $f_2: S_2 \to S_3$ be mappings such that the domain of $f_2$ is the same set as the codomain of $f_1$.


The concept of composition of mappings can be illustrated by means of a commutative diagram.

This diagram illustrates the specific example of $f_2 \circ f_1$:

::$\begin{xy}\xymatrix@+1em{
 S_1
  \ar[r]^*+{f_1}
  \ar@{-->}[rd]_*[l]+{f_2 \mathop \circ f_1}
&
 S_2
  \ar[d]^*+{f_2}

\\
&
 S_3
}\end{xy}$

=== Composition as a Binary Operation ===
Let $\sqbrk {S \to S}$ be the set of all mappings from a set $S$ to itself.

Then the concept of composite mapping defines a binary operation on $\sqbrk {S \to S}$:

:$\forall f, g \in \sqbrk {S \to S}: g \circ f = \set {\tuple {s, t}: s \in S, \tuple {f \paren s, t} \in g} \in \sqbrk {S \to S}$


Thus, for every pair $\tuple {f, g}$ of mappings in $\sqbrk {S \to S}$, the composition $g \circ f$ is another element of $\sqbrk {S \to S}$.",Definition:Composition of Mappings,"['Definitions/Mapping Theory', 'Definitions/Composite Mappings']"
Definition:Composition,Composition,"Let $\RR_1 \subseteq S_1 \times T_1$ and $\RR_2 \subseteq S_2 \times T_2$ be relations.


Then the composite of $\RR_1$ and $\RR_2$ is defined and denoted as:

:$\RR_2 \circ \RR_1 := \set {\tuple {x, z} \in S_1 \times T_2: \exists y \in S_2 \cap T_1: \tuple {x, y} \in \RR_1 \land \tuple {y, z} \in \RR_2}$


 

 

It is clear that the composite relation $\RR_2 \circ \RR_1$ can also be defined as:

:$\map {\RR_2 \circ \RR_1} {S_1} = \map {\RR_2} {\map {\RR_1} {S_1} }$


Note that:
:$(1): \quad \RR_2 \circ \RR_1 \subseteq S_1 \times T_2$
:$(2): \quad$ The domain of $\RR_2 \circ \RR_1$ equals the domain of $\RR_1$, that is, $S_1$
:$(3): \quad$ The codomain of $\RR_2 \circ \RR_1$ equals the codomain of $\RR_2$, that is, $T_2$.",Definition:Composition of Relations,['Definitions/Relation Theory']
Definition:Composition,Composition,"Let $G$ be a finite group.

=== Definition 1 ===
Let $G$ be a finite group.


A composition series for $G$ is a normal series for $G$ which has no proper refinement.


=== Composition Length ===
 

=== Composition Factor ===
 

=== Definition 2 ===
Let $G$ be a finite group.


A composition series for $G$ is a sequence of normal subgroups of $G$:
:$\set e = G_0 \lhd G_1 \lhd \cdots \lhd G_n = G$
where:
:$G_{i - 1} \lhd G_i$ denotes that $G_{i - 1}$ is a proper normal subgroup of $G_i$
such that:
:for all $i \in \set {1, 2, \ldots, n}$, $G_{i - 1}$ is a proper maximal normal subgroup of $G_i$.


=== Composition Length ===
 

=== Composition Factor ===
 

=== Composition Length ===
 

=== Composition Factor ===
 ",Definition:Composition Series,"['Definitions/Composition Series', 'Definitions/Normal Series', 'Definitions/Finite Groups']"
Definition:Composition,Composition,"Let $\mathbf C$ be a metacategory.

Let $\left({g, f}\right)$ be a pair of composable morphisms.


Then the composition of $f$ and $g$ is a morphism $g \circ f$ of $\mathbf C$ subject to:

:$\operatorname{dom} \left({g \circ f}\right) = \operatorname{dom} f$
:$\operatorname{cod} \left({g \circ f}\right) = \operatorname{cod} g$


This composition of morphisms can be thought of as an abstraction of both composition of mappings and transitive relations.",Definition:Composition of Morphisms,"['Definitions/Category Theory', 'Definitions/Morphisms']"
Definition:Composition,Composition,"Let $\mathbf C, \mathbf D$ and $\mathbf E$ be metacategories.

Let $F: \mathbf C \to \mathbf D$ and $G: \mathbf D \to \mathbf E$ be (covariant) functors.


The composition of $G$ with $F$ is the functor $GF: \mathbf C \to \mathbf E$ defined by:

:For all objects $C$ of $\mathbf C$: $\hskip{2.9cm} GF \left({C}\right) := G \left({FC}\right)$
:For all morphisms $f: C_1 \to C_2$ of $\mathbf C$: $\quad GF \left({f}\right) := G \left({Ff}\right)$

$GF$ is said to be a composite functor.",Definition:Composition of Functors,['Definitions/Category Theory']
Definition:Composition,Composition,"Let $\mathbf C$ be a metacategory.

Let $C$ and $D$ be objects of $\mathbf C$.

Let $\mathbf C / C$ and $\mathbf C / D$ be the associated slice categories.


Let $g: C \to D$ be a morphism of $\mathbf C$.

Then $g$ defines a composition functor $g_* : \mathbf C / C \to \mathbf C / D$:

 
 
 
 


That it is in fact a functor is shown on Composition Functor on Slice Categories is Functor.


The effect of $g_*$ is captured in the following commutative diagram:

::$\begin{xy}
<-3em,0em>*+{X} = ""X"",
<3em,0em>*+{X'} = ""X2"",
<0em,-4em>*+{C} = ""C"",
<0em,-8em>*+{D} = ""D"",

""X"";""X2"" **@{-} ?>*@{>} ?*!/_1em/{a},
""X"";""C"" **@{-} ?>*@{>} ?<>(.4)*!/^.6em/{f},
""X2"";""C"" **@{-} ?>*@{>} ?<>(.4)*!/_.6em/{f'},
""C"";""D"" **@{-} ?>*@{>} ?<>(.4)*!/^.6em/{g},

""X"";""D"" **\crv{<-5em,-4em>} ?>*@{>} ?*!/^1.6em/{g_* f = \\ g \circ f},
""X2"";""D"" **\crv{<5em,-4em>} ?>*@{>} ?*!/_1.6em/{g_* f' = \\ g \circ f'},
\end{xy}$",Definition:Composition Functor on Slice Categories,"['Definitions/Slice Categories', 'Definitions/Category Theory']"
Definition:Composition,Composition,"Let $\mathbf C$ be a metacategory.

Let $C$ and $D$ be objects of $\mathbf C$.

Let $\mathbf{Sub}_{\mathbf C} \left({C}\right)$ and $\mathbf{Sub}_{\mathbf C} \left({D}\right)$ be the associated categories of subobjects.


Let $g: C \to D$ be a monomorphism of $\mathbf C$.

Then $g$ defines a composition functor $g_* : \mathbf{Sub}_{\mathbf C} \left({C}\right) \to \mathbf{Sub}_{\mathbf C} \left({D}\right)$:

 
 
 
 


That it is in fact a functor is shown on Composition Functor on Categories of Subobjects is Functor.


The effect of $g_*$ is captured in the following commutative diagram:

::$\begin{xy}
<-3em,0em>*+{X} = ""X"",
<3em,0em>*+{X'} = ""X2"",
<0em,-4em>*+{C} = ""C"",
<0em,-8em>*+{D} = ""D"",

""X"";""X2"" **@{-} ?>*@{>} ?*!/_1em/{a},
""X"";""C"" **@{-} ?>*@{>} ?<>(.4)*!/^.6em/{f},
""X2"";""C"" **@{-} ?>*@{>} ?<>(.4)*!/_.6em/{f'},
""C"";""D"" **@{-} ?>*@{>} ?<>(.4)*!/^.6em/{g},

""X"";""D"" **\crv{<-5em,-4em>} ?>*@{>} ?*!/^1.6em/{g_* f = \\ g \circ f},
""X2"";""D"" **\crv{<5em,-4em>} ?>*@{>} ?*!/_1.6em/{g_* f' = \\ g \circ f'},
\end{xy}$",Definition:Composition Functor on Categories of Subobjects,['Definitions/Categories of Subobjects']
Definition:Composition,Composition,"A $k$-composition of a (strictly) positive integer $n \in \Z_{> 0}$ is an ordered $k$-tuple:
:$c = \tuple {c_1, c_2, \ldots, c_k}$
such that:
:$(1): \quad c_1 + c_2 + \cdots + c_k = n$
:$(2): \quad \forall i \in \closedint 1 k: c_i \in \Z_{>0}$, that is, all the $c_i$ are strictly positive integers.

Category:Definitions/Combinatorics",Definition:Composition (Combinatorics),['Definitions/Combinatorics']
Definition:Composition,Composition,"Let $R = a : b$ be a ratio.

Then the composition of $R$ is the ratio $a + b : b$.


 
: 
:Composition of a ratio means taking the antecedent together with the consequent as one in relation to the consequent by itself.
 
 

Category:Definitions/Euclidean Algebra",Definition:Composition of Ratio,['Definitions/Euclidean Algebra']
Definition:Concave,Concave,"Let $P$ be a polygon.

$P$ is a concave polygon  if and only if :
:at least one internal angle of $P$ is greater than $180 \degrees$.",Definition:Concave Polygon,['Definitions/Polygons']
Definition:Concave,Concave,"Let $P$ be a polyhedron.

=== Definition 1 ===
Let $P$ be a polyhedron.

$P$ is a concave polyhedron  if and only if :
:at least one face of $P$ lies in a plane which intersects at least one other face.

=== Definition 2 ===
Let $P$ be a polyhedron.

$P$ is a concave polyhedron  if and only if :
:at least one face of $P$ lies in a plane such that $P$ does not lie completely on one side of that plane.",Definition:Concave Polyhedron,"['Definitions/Concave Polyhedra', 'Definitions/Polyhedra']"
Definition:Concave,Concave,"Let $f$ be a real function which is defined on a real interval $I$.

=== Definition 1 ===
Let $f$ be a real function which is defined on a real interval $I$.


$f$ is concave on $I$  if and only if :

:$\forall x, y \in I: \forall \alpha, \beta \in \R_{>0}, \alpha + \beta = 1: \map f {\alpha x + \beta y} \ge \alpha \map f x + \beta \map f y$


:

=== Strictly Concave ===
Let $f$ be a real function which is defined on a real interval $I$.


$f$ is strictly concave on $I$  if and only if :

:$\forall x, y \in I, x \ne y: \forall \alpha, \beta \in \R_{>0}, \alpha + \beta = 1: f \left({\alpha x + \beta y}\right) > \alpha f \left({x}\right) + \beta f \left({y}\right)$


:


The geometric interpretation is that any point on the chord drawn on the graph of any strictly concave function always lies below the graph.

=== Definition 2 ===
Let $f$ be a real function which is defined on a real interval $I$.


$f$ is concave on $I$  if and only if :

:$\ds \forall x_1, x_2, x_3 \in I: x_1 < x_2< x_3: \frac {\map f {x_2} - \map f {x_1} } {x_2 - x_1} \ge \frac {\map f {x_3} - \map f {x_2} } {x_3 - x_2}$


Hence a geometrical interpretation: the slope of $P_1 P_2$ is greater than that of $P_2 P_3$:


:


=== Strictly Concave ===
Let $f$ be a real function which is defined on a real interval $I$.


$f$ is strictly concave on $I$  if and only if :

:$\forall x_1, x_2, x_3 \in I: x_1 < x_2 < x_3: \dfrac {f \left({x_2}\right) - f \left({x_1}\right)} {x_2 - x_1} > \dfrac {f \left({x_3}\right) - f \left({x_2}\right)} {x_3 - x_2}$


Hence a geometrical interpretation: the slope of $P_1 P_2$ is greater than that of $P_2 P_3$:


:

=== Definition 3 ===
Let $f$ be a real function which is defined on a real interval $I$.


$f$ is concave on $I$  if and only if :

:$\forall x_1, x_2, x_3 \in I: x_1 < x_2< x_3: \dfrac {\map f {x_2} - \map f {x_1} } {x_2 - x_1} \ge \dfrac {\map f {x_3} - \map f {x_1} } {x_3 - x_1}$


Hence a geometrical interpretation: the slope of $P_1 P_2$ is greater than that of $P_1 P_3$:


:


=== Strictly Concave ===
Let $f$ be a real function which is defined on a real interval $I$.


$f$ is strictly concave on $I$  if and only if :

:$\forall x_1, x_2, x_3 \in I: x_1 < x_2 < x_3: \dfrac {f \left({x_2}\right) - f \left({x_1}\right)} {x_2 - x_1} > \dfrac {f \left({x_3}\right) - f \left({x_1}\right)} {x_3 - x_1}$


Hence a geometrical interpretation: the slope of $P_1 P_2$ is greater than that of $P_1 P_3$:


:",Definition:Concave Real Function,"['Definitions/Concave Real Functions', 'Definitions/Real Functions', 'Definitions/Real Analysis']"
Definition:Cone,Cone,"A cone is a three-dimensional geometric figure which consists of the set of all straight lines joining the boundary of a plane figure $PQR$ to a point $A$ not in the same plane of $PQR$:


:",Definition:Cone (Geometry),['Definitions/Cones']
Definition:Cone,Cone,"Let $\mathbf C$ be a metacategory.

Let $D: \mathbf J \to \mathbf C$ be a $\mathbf J$-diagram in $\mathbf C$.


A cone to $D$ comprises an object $C$ of $\mathbf C$, and a morphism:

:$c_j: C \to D_j$

for each object of $\mathbf J$, such that for each morphism $\alpha: i \to j$ of $\mathbf J$:

::$\begin{xy}\xymatrix@+0.5em@L+2px{
 C
  \ar[d]_*+{c_i}
  \ar[dr]^*+{c_j}

\\
 D_i
  \ar[r]_*+{D_\alpha}
&
 D_j
}\end{xy}$

is a commutative diagram.

 ",Definition:Cone (Category Theory),['Definitions/Category Theory']
Definition:Congruence,Congruence,"In the field of Euclidean geometry, two geometric figures are congruent  if and only if :

:they are, informally speaking, both ""the same size and shape""

:they differ only in position in space

:one figure can be overlaid on the other figure with a series of rotations, translations, and reflections.


Specifically:
:all corresponding angles of the congruent figures must have the same measurement
:all corresponding sides of the congruent figures must be be the same length.",Definition:Congruence (Geometry),"['Definitions/Congruence (Geometry)', 'Definitions/Geometry']"
Definition:Congruence,Congruence,"Let $\struct {X, d}$ be a metric space.

Two subsets $A, B \subseteq X$ of $X$ are said to be congruent  if and only if  there exists an isometry $f: X \to X$ such that $\map {f^\to} A = B$.

Such an isometry is called a congruence.",Definition:Congruence (Metric Spaces),['Definitions/Metric Spaces']
Definition:Congruence,Congruence,"Let $\struct {R, +, \circ}$ be a ring, and let $J$ be an ideal of $R$.


The notation:

:$a \equiv b \pmod J$

is used to mean:

:$a + \paren {-b} \in J$",Definition:Congruence Modulo an Ideal,['Definitions/Ideal Theory']
Definition:Congruence,Congruence,"Let $\struct {S, \circ}$ be an algebraic structure.

Let $\RR$ be an equivalence relation on $S$.


Then $\RR$ is a congruence relation for $\circ$  if and only if :

:$\forall x_1, x_2, y_1, y_2 \in S: \paren {x_1 \mathrel \RR x_2} \land \paren {y_1 \mathrel \RR y_2} \implies \paren {x_1 \circ y_1} \mathrel \RR \paren {x_2 \circ y_2}$",Definition:Congruence Relation,"['Definitions/Abstract Algebra', 'Definitions/Equivalence Relations']"
Definition:Congruence,Congruence,"Let $z \in \R$.


=== Definition by Remainder after Division ===
Let $z \in \R$.


We define a relation $\RR_z$ on the set of all $x, y \in \R$:
:$\RR_z := \set {\tuple {x, y} \in \R \times \R: \exists k \in \Z: x = y + k z}$


This relation is called congruence modulo $z$, and the real number $z$ is called the modulus.


When $\tuple {x, y} \in \RR_z$, we write:
:$x \equiv y \pmod z$
and say:
:$x$ is congruent to $y$ modulo $z$.


Similarly, when $\tuple {x, y} \notin \RR_z$, we write:
:$x \not \equiv y \pmod z$
and say:
:$x$ is not congruent (or incongruent) to $y$ modulo $z$.

=== Definition by Modulo Operation ===
Let $z \in \R$.

Let $\bmod$ be defined as the modulo operation:

:$x \bmod y := \begin {cases} x - y \floor {\dfrac x y} & : y \ne 0 \\ x & : y = 0 \end {cases}$


Then congruence modulo $z$ is the relation on $\R$ defined as:
:$\forall x, y \in \R: x \equiv y \pmod z \iff x \bmod z = y \bmod z$ 


The real number $z$ is called the modulus.

=== Definition by Integer Multiple ===
Let $z \in \R$.

Let $x, y \in \R$.


Then $x$ is congruent to $y$ modulo $z$  if and only if  their difference is an integer multiple of $z$:
:$x \equiv y \pmod z \iff \exists k \in \Z: x - y = k z$",Definition:Congruence (Number Theory),"['Definitions/Congruence (Number Theory)', 'Definitions/Modulo Arithmetic', 'Definitions/Number Theory']"
Definition:Congruence,Congruence,"Let $R$ be a commutative ring with unity.

Let $n$ be a positive integer.

Let $\mathbf A$ and $\mathbf B$ be square matrices of order $n$ over $R$.


Then:
:$\mathbf A$ and $\mathbf B$ are congruent
 if and only if :
:there exists an invertible matrix $\mathbf P \in R^{n \times n}$ such that $\mathbf B = \mathbf P^\intercal \mathbf A \mathbf P$
where $\mathbf P^\intercal$ denotes the transpose of $\mathbf P$.",Definition:Matrix Congruence,"['Definitions/Matrix Congruence', 'Definitions/Matrix Equivalence', 'Definitions/Matrix Algebra', 'Definitions/Linear Algebra']"
Definition:Conjugate,Conjugate,"The conjugate of an angle $\theta$ is the angle $\phi$ such that:
:$\theta + \phi = 2 \pi$
where $\theta$ and $\pi$ are expressed in radians.

That is, it is the angle that makes the given angle equal to a full angle.


Equivalently, the conjugate of an angle $\theta$ is the angle $\phi$ such that:
:$\theta + \phi = 360 \degrees$
where $\theta$ and $\pi$ are expressed in degrees.


Thus, conjugate angles are two angles whose measures add up to the measure of $4$ right angles.

That is, their measurements add up to $360$ degrees or $2 \pi$ radians.",Definition:Conjugate Angles,"['Definitions/Conjugate Angles', 'Definitions/Angles']"
Definition:Conjugate,Conjugate,"

:


Consider a hyperbola $K$ whose foci are $F_1$ and $F_2$.


Let $PQ$ and $RS$ be line segments constructed through the vertices of $K$ parallel to the minor axis of $K$ and intersecting the asymptotes of $K$ at $P$, $Q$, $R$ and $S$ as above.

Construct the line segments $PR$ and $QS$.

Let $C_1$ and $C_2$ be the points of intersection of $PR$ and $QS$ with the minor axis of $K$.


The conjugate axis of $K$ is the line segment $C_1 C_2$.",Definition:Hyperbola/Conjugate Axis,['Definitions/Hyperbolas']
Definition:Conjugate,Conjugate,"Let $\KK$ be a conic section.

Let $P$ and $Q$ be points in the plane of $\KK$.

Let:
:$P$ lie on the polar of $Q$
:$Q$ lie on the polar of $P$.


$P$ and $Q$ are known as conjugate points with respect to $\KK$.


=== Conjugate Points with respect to Circle ===
Let $\CC$ be a circle.

Let $P$ and $Q$ be points in the plane of $\CC$.

Let:
:$P$ lie on the polar of $Q$
:$Q$ lie on the polar of $P$.


$P$ and $Q$ are known as conjugate points with respect to $\CC$.",Definition:Conjugate Points (Geometry),"['Definitions/Conjugate Points', 'Definitions/Polars of Points']"
Definition:Conjugate,Conjugate,"Let $\CC$ be a circle.

Let $\PP$ and $\QQ$ be the straight lines in the plane of $\CC$.


Let $P$ and $Q$ be the poles of $\PP$ and $\QQ$ with respect to $\CC$ respectively.

Let $P$ and $Q$ be such that $P$ lies on $\QQ$ and $Q$ lies on $\PP$.

Then $\PP$ and $\QQ$ are known as conjugate lines with respect to $\CC$.",Definition:Conjugate Lines,"['Definitions/Conjugate Lines', 'Definitions/Polars of Points', 'Definitions/Conic Sections']"
Definition:Conjugate,Conjugate,"Let $K$ be a conic section.

Let $D_1$ and $D_2$ be diameters of $K$ such that:
:$D_1$ belongs to the system of parallel chords whose midpoints define $D_2$
and:
:$D_2$ belongs to the system of parallel chords whose midpoints define $D_1$.

Then $D_1$ and $D_2$ are known as conjugate diameters.",Definition:Conjugate Diameters,"['Definitions/Conjugate Diameters', 'Definitions/Conic Sections']"
Definition:Conjugate,Conjugate,"Let $A = \struct {A_F, \oplus}$ be an algebra over a field $F$.

Let $C: A_F \to A_F$ be a conjugation on $A$.

Let $a \in A$.


Then $\map C a$ is called the conjugate of $a$.",Definition:Conjugation on Algebra/Conjugate,['Definitions/Conjugations on Algebras']
Definition:Conjugate,Conjugate,"Let $\left({G, \circ}\right)$ be a group.

=== Conjugate of an Element ===
Let $\struct {G, \circ}$ be a group.


=== Definition 1 ===
Let $\struct {G, \circ}$ be a group.


The  conjugacy relation $\sim$ is defined on $G$ as:
:$\forall \tuple {x, y} \in G \times G: x \sim y \iff \exists a \in G: a \circ x = y \circ a$


This can be voiced as:
:$x$ is the conjugate of $y$ (by $a$ in $G$)
or:
:$x$ is conjugate to $y$ (by $a$ in $G$)

=== Definition 2 ===
Let $\struct {G, \circ}$ be a group.


The  conjugacy relation $\sim$ is defined on $G$ as:
:$\forall \tuple {x, y} \in G \times G: x \sim y \iff \exists a \in G: a \circ x \circ a^{-1} = y$


This can be voiced as:
:$x$ is the conjugate of $y$ (by $a$ in $G$)
or:
:$x$ is conjugate to $y$ (by $a$ in $G$)

This can be voiced as:
:$x$ is the conjugate of $y$ (by $a$ in $G$)
or:
:$x$ is conjugate to $y$ (by $a$ in $G$)

=== Conjugate of a Set ===
Let $\struct {G, \circ}$ be a group.

Let $S \subseteq G, a \in G$.

Then the $G$-conjugate of $S$ by $a$ is:

:$S^a := \set {y \in G: \exists x \in S: y = a \circ x \circ a^{-1} } = a \circ S \circ a^{-1}$


That is, $S^a$ is the set of all elements of $G$ that are the conjugates of elements of $S$ by $a$.


When $G$ is the only group under consideration, we usually just refer to the conjugate of $S$ by $a$.

Category:Definitions/Conjugacy
Category:Definitions/Group Theory",Definition:Conjugate (Group Theory),"['Definitions/Conjugacy', 'Definitions/Group Theory']"
Definition:Conjugate,Conjugate,"Let $G$ be a group.

Let $S \subseteq G$ be a subset.


=== Definition 1 ===
Let $G$ be a group. 

Let $S \subseteq G$ be a subset.


The normal subgroup generated by $S$, denoted $\gen {S^G}$,  is the intersection of all normal subgroups of $G$ containing $S$.

=== Definition 2 ===
 

Let $G$ be a group. 

Let $S \subseteq G$ be a subset.


The normal subgroup generated by $S$, denoted $\gen {S^G}$,  is the smallest normal subgroup of $G$ containing $S$:
:$\gen {S^G} = \gen {x S x^{-1}: x \in G}$",Definition:Generated Normal Subgroup,"['Definitions/Generated Normal Subgroups', 'Definitions/Normal Subgroups', 'Definitions/Normality in Groups', 'Definitions/Generated Subgroups', 'Definitions/Generators of Groups', 'Definitions/Subgroups']"
Definition:Conjugate,Conjugate,"Let $\alpha = r + s \sqrt n$ be a quadratic irrational.


Then its conjugate is defined as:
:$\tilde \alpha = r - s \sqrt n$


Thus $\alpha$ and $\tilde \alpha$ are known as conjugate quadratic irrationals.


Notation may vary.",Definition:Conjugate of Quadratic Irrational,['Definitions/Quadratic Irrationals']
Definition:Conjugate,Conjugate,"Let $z = a + i b$ be a complex number.


Then the (complex) conjugate of $z$ is denoted $\overline z$ and is defined as:

:$\overline z := a - i b$


That is, you get the complex conjugate of a complex number by negating its imaginary part.


=== Complex Conjugation ===
The operation of complex conjugation is the mapping:
: $\overline \cdot: \C \to \C: z \mapsto \overline z$.
where $\overline z$ is the complex conjugate of $z$.


That is, it maps a complex number to its complex conjugate.


Category:Definitions/Complex Conjugates",Definition:Complex Conjugate,"['Definitions/Complex Conjugates', 'Definitions/Complex Numbers', 'Definitions/Complex Analysis']"
Definition:Conjugate,Conjugate,"Let $\mathbf x = a \mathbf 1 + b \mathbf i + c \mathbf j + d \mathbf k$ be a quaternion.


The conjugate quaternion of $\mathbf x$ is defined as:
:$\overline {\mathbf x} = a \mathbf 1 - b \mathbf i - c \mathbf j - d \mathbf k$.


=== Matrix Form ===
Let $\mathbf x$ be a quaternion defined in matrix form as:
:$\mathbf x = \begin{bmatrix} a + bi & c + di \\ -c + di & a - bi \end{bmatrix}$


The conjugate quaternion of $\mathbf x$ is defined as:
:$\overline {\mathbf x} = \begin{bmatrix} a - bi & -c - di \\ c - di & a + bi \end{bmatrix}$


That is, if:
:$\mathbf x = \begin{bmatrix} p & q \\ -\overline q & \overline p \end{bmatrix}$

then:
:$\overline {\mathbf x} = \begin{bmatrix} \overline p & -q \\ \overline q & p \end{bmatrix}$

Category:Definitions/Quaternions

=== Ordered Pair of Complex Numbers ===
Let $\mathbf x$ be a quaternion defined as an ordered pair $\left({a, b}\right)$  of complex numbers.


The conjugate quaternion of $\mathbf x$ is defined as:
:$\overline {\mathbf x} = \overline {\left({a, b}\right)} = \left({\overline a, -b}\right)$",Definition:Conjugate Quaternion,['Definitions/Quaternions']
Definition:Conjugate,Conjugate,"=== Definition 1 ===
Let:
:$-\map {\dfrac \d {\d x} } {P h'} + Q h = 0$

with boundary conditions:
:$\map h a = 0, \quad \map h c = 0, \quad a < c \le b$

Suppose:
:$\map h x = 0 \quad \neg \forall x \in \closedint a b$

Suppose:
:$\map h a = 0, \quad \map h {\tilde a} = 0, \quad a \ne \tilde a$


Then the point $\tilde a$ is called conjugate to the point $a$   solution to the aforementioned differential equation.

 

=== Definition 2 ===
Let $y = \map y x$ and $y^* = \map {y^*} x$ be extremal functions.

Let:

:$M = \tuple {a, \map y a}$

:$\tilde M = \tuple {\tilde a, \map y {\tilde a} }$

Let $y$ and $y^*$ both pass through the point $M$.

Let:

:$\map {y^*} {x - \tilde a} - \map y {x - \tilde a} = \epsilon \size {\map {y^*} {x - \tilde a} - \map y {x - \tilde a} }_1$

where:

:$\size {\map {y^*} {x - \tilde a} - \map y {x - \tilde a} }_1 \to 0 \implies \epsilon \to 0$


Then $\tilde M$ is conjugate to $M$.

 

=== Definition 3 ===
Let $y = \map y x$ and $y = \map {\tilde y} x$ be extremal functions.

Let:

:$M = \paren {a, \map y a}$

:$\tilde M = \paren {\tilde a, \map y {\tilde a} }$

Let both $y = \map y x$ and $y = \map {\tilde y} x$ pass through the point $M$.

Let 

:$\ds \lim_{\norm {\map y x - \map {\tilde y} x}_{1, \infty} \to 0} \sqbrk {\paren {x, \map y x}: \map y x - \map {\tilde y} x = 0} = \tilde M$

In other words, let $\tilde M$ be the limit points of intersection of $y = \map y x$ and $y = \map {\tilde y} x$ as $\norm {\map y x - \map {\tilde y} x}_{1, \infty} \to 0$.

 


Then $\tilde M$ is conjugate to $M$.

=== Dependent on $N$ Functions ===
Let $K$ be a functional such that:

:$\ds K \sqbrk h = \int_a^b \paren {\mathbf h'\mathbf P \mathbf h' + \mathbf h \mathbf Q \mathbf h} \rd x$

Consider Euler's equation related to the functional $K$:

:$-\map {\dfrac \d {\d x} } {\mathbf P \mathbf h'} + \mathbf Q \mathbf h = 0$

where $\mathbf P$ and $\mathbf Q$ are symmetric matrices.

Let the general solution to this equation be:

:$\set {\mathbf h^{\paren i} = \paren {\sequence {h_{ij} } }: i,j \in \N_{\le N} }$

Let:

:$\exists j: \forall k \ne j: \paren {\map {\mathbf h^{\paren j} } a = 0} \land \paren {\map {h_{j j}'} a = 1, h'_{j k} = 0}$

Let the determinant, built from $h_{ij}$, be such that:

:$\size {h_{i j} } \paren {\tilde a} = 0$

Here $i$ denotes rows, and $j$ denotes columns.


Then $\tilde a$ is said to be conjugate to point $a$   the functional $K$.

=== With Respect to Original Functional ===
Let:

:$\ds \int_a^b \map F {x, y, y'}$

be the original functional.

Let $\tilde a$ be conjugate to $a$.

Let:
:$\ds \int_a^b \paren {P h'^2 + Q h^2} \rd x$

be the second variation of $\ds \int_a^b \map F {x, y, y'}$.


Then $\tilde a$ is conjugate to $a$   to the original functional $\ds \int_a^b \map F {x, y, y'}$.

 ",Definition:Conjugate Point (Calculus of Variations),['Definitions/Calculus of Variations']
Definition:Conjugate Pair,Conjugate Pair,"Let $z \in \C$ be a complex number.

Let $\overline z$ be the complex conjugate of $z$.


Then $z$ and $\overline z$ are a conjugate pair.",Definition:Complex Conjugate/Conjugate Pair,['Definitions/Complex Conjugates']
Definition:Conjugate Pair,Conjugate Pair,"=== Harmonic Range ===
Let $AB$ and $PQ$ be line segments on a straight line such that $\tuple {AB, PQ}$ is a harmonic range.

Then $P$ and $Q$ are said to be harmonic conjugates with respect to $A$ and $B$.

=== Harmonic Pencil ===
Let $AB$ and $PQ$ be line segments on a straight line such that $\tuple {AB, PQ}$ is a harmonic range.

Let $O$ be a point which is not on the straight line $AB$.

Let $\map O {AB, PQ}$ be the harmonic pencil formed from $O$ and $\tuple {AB, PQ}$.


:


The rays $OP$ and $OQ$ are said to be harmonic conjugates with respect to $OA$ and $OB$.",Definition:Harmonic Conjugates,"['Definitions/Harmonic Conjugates', 'Definitions/Harmonic Ranges']"
Definition:Connected,Connected,"Let $\RR \subseteq S \times S$ be a relation on a set $S$.


Then $\RR$ is connected  if and only if :
:$\forall a, b \in S: a \ne b \implies \tuple {a, b} \in \RR \lor \tuple {b, a} \in \RR$


That is,  if and only if  every pair of distinct elements is comparable.",Definition:Connected Relation,"['Definitions/Connected Relations', 'Definitions/Relation Theory']"
Definition:Connected,Connected,"=== Topological Space ===
Let $T = \struct {S, \tau}$ be a non-empty topological space.


=== Definition 1 ===
Let $T = \struct {S, \tau}$ be a topological space.


$T$ is connected  if and only if  it admits no separation.


That is, $T$ is connected  if and only if  there exist no open sets $A, B \in \tau$ such that $A, B \ne \O$, $A \cup B = S$ and $A \cap B = \O$.

=== Definition 2 ===
Let $T = \struct {S, \tau}$ be a topological space.


$T$ is connected  if and only if  it has no two disjoint nonempty closed sets whose union is $S$.

=== Definition 3 ===
Let $T = \struct {S, \tau}$ be a topological space.


$T$ is connected  if and only if  its only subsets whose boundary is empty are $S$ and $\O$.

=== Definition 4 ===
Let $T = \struct {S, \tau}$ be a topological space.


$T$ is connected  if and only if  its only clopen sets are $S$ and $\O$.

=== Definition 5 ===
Let $T = \struct {S, \tau}$ be a non-empty topological space.


$T$ is connected  if and only if  there are no two non-empty separated sets whose union is $S$.

=== Definition 6 ===

Let $T = \struct {S, \tau}$ be a topological space.


$T$ is connected  if and only if  there exists no continuous surjection from $T$ onto a discrete two-point space.

=== Definition 7 ===
Let $T = \struct {S, \tau}$ be a topological space.


$T$ is connected  if and only if :
:there do not exist disjoint, non-empty open sets $X$ and $Y$ of $T$ such that $X \cup Y = S$.

=== Set of Topological Space ===
Let $T = \struct {S, \tau}$ be a topological space.

Let $H \subseteq S$ be a non-empty subset of $S$.


=== Definition 1 ===

Let $T = \struct {S, \tau}$ be a topological space.

Let $H \subseteq S$ be a non-empty subset of $S$.


$H$ is a connected set of $T$  if and only if  it is not the union of any two non-empty separated sets of $T$.


=== Definition 2 ===
Let $T = \struct {S, \tau}$ be a topological space.

Let $H \subseteq S$ be a non-empty subset of $S$.


$H$ is a connected set of $T$  if and only if  it is not disconnected in $T$.

=== Definition 3 ===
Let $T = \struct {S, \tau}$ be a topological space.

Let $H \subseteq S$ be a non-empty subset of $S$.


$H$ is a connected set of $T$  if and only if :
:the topological subspace $\struct {H, \tau_H}$ of $T$ is a connected topological space.

=== Definition 4 ===
Let $T = \struct {S, \tau}$ be a topological space.

Let $H \subseteq S$ be a non-empty subset of $S$.


$H$ is a connected set of $T$  if and only if :
:there do not exist disjoint, non-empty subsets $X$ and $Y$ of $H$ such that $X \cup Y = H$ such that:
::no limit point of $X$ is an element of $Y$
::no limit point of $Y$ is an element of $X$.

=== Definition 5 ===
Let $T = \struct {S, \tau}$ be a topological space.

Let $H \subseteq S$ be a non-empty subset of $S$.


$H$ is a connected set of $T$  if and only if :
:$H$ cannot be partitioned into $2$ non-empty subsets so that each subset has no element in common with the closure of the other.

=== Points in Topological Space ===
Let $T = \struct {S, \tau}$ be a topological space.

Let $a, b \in S$.


Then $a$ and $b$ are connected (in $T$)  if and only if  there exists a connected set in $T$ containing both $a$ and $b$.",Definition:Connected (Topology),['Definitions/Connected Spaces']
Definition:Connected,Connected,"Let $T = \struct {S, \tau}$ be a non-empty topological space.


=== Definition 1 ===
Let $T = \struct {S, \tau}$ be a topological space.


$T$ is connected  if and only if  it admits no separation.


That is, $T$ is connected  if and only if  there exist no open sets $A, B \in \tau$ such that $A, B \ne \O$, $A \cup B = S$ and $A \cap B = \O$.

=== Definition 2 ===
Let $T = \struct {S, \tau}$ be a topological space.


$T$ is connected  if and only if  it has no two disjoint nonempty closed sets whose union is $S$.

=== Definition 3 ===
Let $T = \struct {S, \tau}$ be a topological space.


$T$ is connected  if and only if  its only subsets whose boundary is empty are $S$ and $\O$.

=== Definition 4 ===
Let $T = \struct {S, \tau}$ be a topological space.


$T$ is connected  if and only if  its only clopen sets are $S$ and $\O$.

=== Definition 5 ===
Let $T = \struct {S, \tau}$ be a non-empty topological space.


$T$ is connected  if and only if  there are no two non-empty separated sets whose union is $S$.

=== Definition 6 ===

Let $T = \struct {S, \tau}$ be a topological space.


$T$ is connected  if and only if  there exists no continuous surjection from $T$ onto a discrete two-point space.

=== Definition 7 ===
Let $T = \struct {S, \tau}$ be a topological space.


$T$ is connected  if and only if :
:there do not exist disjoint, non-empty open sets $X$ and $Y$ of $T$ such that $X \cup Y = S$.",Definition:Connected (Topology)/Topological Space,['Definitions/Connected Spaces']
Definition:Connected,Connected,"Let $T = \struct {S, \tau}$ be a topological space.

Let $H \subseteq S$ be a non-empty subset of $S$.


=== Definition 1 ===

Let $T = \struct {S, \tau}$ be a topological space.

Let $H \subseteq S$ be a non-empty subset of $S$.


$H$ is a connected set of $T$  if and only if  it is not the union of any two non-empty separated sets of $T$.


=== Definition 2 ===
Let $T = \struct {S, \tau}$ be a topological space.

Let $H \subseteq S$ be a non-empty subset of $S$.


$H$ is a connected set of $T$  if and only if  it is not disconnected in $T$.

=== Definition 3 ===
Let $T = \struct {S, \tau}$ be a topological space.

Let $H \subseteq S$ be a non-empty subset of $S$.


$H$ is a connected set of $T$  if and only if :
:the topological subspace $\struct {H, \tau_H}$ of $T$ is a connected topological space.

=== Definition 4 ===
Let $T = \struct {S, \tau}$ be a topological space.

Let $H \subseteq S$ be a non-empty subset of $S$.


$H$ is a connected set of $T$  if and only if :
:there do not exist disjoint, non-empty subsets $X$ and $Y$ of $H$ such that $X \cup Y = H$ such that:
::no limit point of $X$ is an element of $Y$
::no limit point of $Y$ is an element of $X$.

=== Definition 5 ===
Let $T = \struct {S, \tau}$ be a topological space.

Let $H \subseteq S$ be a non-empty subset of $S$.


$H$ is a connected set of $T$  if and only if :
:$H$ cannot be partitioned into $2$ non-empty subsets so that each subset has no element in common with the closure of the other.",Definition:Connected (Topology)/Set,"['Definitions/Connected Sets', 'Definitions/Topology']"
Definition:Connected,Connected,"Let $T = \struct {S, \tau}$ be a topological space.

Let $a, b \in S$.


Then $a$ and $b$ are connected (in $T$)  if and only if  there exists a connected set in $T$ containing both $a$ and $b$.",Definition:Connected (Topology)/Points,['Definitions/Connected Spaces']
Definition:Connected,Connected,"The connected sum of two manifolds $A^n, B^n$ of dimension $n$ is defined as follows:

Let $\Bbb D^n$ be a closed n-disk.

Let $\alpha: \Bbb D^n \to A^n$ be a continuous (or, in the case of smooth manifolds, a smooth) injection.

Let $\beta: \Bbb D^n \to B^n$ be a similar function.  


Define the set:
:$S = \paren {A^n \setminus \map \alpha {\paren {\Bbb D^n}^\circ} } \cup \paren {B^n \setminus \map \beta {\paren {\Bbb D^n}^\circ} }$
where:
:$\setminus$ denotes set difference
:$\paren {\Bbb D^n}^\circ$ denotes the interior of $\Bbb  D^n$.


Define an equivalence relation $\sim$ on $S$ as:
:$x \sim y \iff \paren {\paren {x = y} \lor \paren {\map {\alpha^{-1} } x = \map {\beta^{-1} } y} }$


Since the interiors of the disks were removed from the manifolds, it necessarily follows that:
:$\map {\alpha^{-1} } x, \map {\beta^{-1} } y \in \partial \Bbb D^n$


The connected sum $A^n \# B^n$ is defined as the quotient space of $S$ under $\sim$.

Category:Definitions/Topology",Definition:Connected Sum,['Definitions/Topology']
Definition:Connected,Connected,"Let $D \subseteq \C$ be a subset of the set of complex numbers.


=== Definition 1 ===
Let $D \subseteq \C$ be a subset of the set of complex numbers.


$D$ is connected  if and only if  every pair of points in $D$ can be joined by a staircase contour.

=== Definition 2 ===
Let $D \subseteq \C$ be a subset of the set of complex numbers.


$D$ is connected  if and only if  every pair of points in $D$ can be joined by a polygonal path all points of which are in $D$.",Definition:Connected Set (Complex Analysis),['Definitions/Complex Analysis']
Definition:Connected,Connected,"=== Vertices ===
Let $G$ be a graph.

Two vertices $u, v \in G$ are connected  if and only if  either:

:$(1): \quad u = v$
:$(2): \quad u \ne v$, and there exists a walk between them.

=== Graph ===
Let $G$ be a graph.

Then $G$ is connected  if and only if  every pair of vertices in $G$ is connected.


=== Disconnected ===
Let $G = \struct {V, E}$ be a graph.

Then $G$ is disconnected  if and only if  it is not connected.

That is,  if and only if  there exists (at least) two vertices $u, v \in V$ such that $u$ and $v$ are not connected.",Definition:Connected (Graph Theory),"['Definitions/Connectedness (Graph Theory)', 'Definitions/Graph Theory']"
Definition:Connected,Connected,"Let $G$ be a graph.

Then $G$ is connected  if and only if  every pair of vertices in $G$ is connected.


=== Disconnected ===
Let $G = \struct {V, E}$ be a graph.

Then $G$ is disconnected  if and only if  it is not connected.

That is,  if and only if  there exists (at least) two vertices $u, v \in V$ such that $u$ and $v$ are not connected.",Definition:Connected (Graph Theory)/Graph,['Definitions/Connectedness (Graph Theory)']
Definition:Connected,Connected,"Let $G$ be a graph.

Two vertices $u, v \in G$ are connected  if and only if  either:

:$(1): \quad u = v$
:$(2): \quad u \ne v$, and there exists a walk between them.",Definition:Connected (Graph Theory)/Vertices,"['Definitions/Connectedness (Graph Theory)', 'Definitions/Vertices of Graphs']"
Definition:Consequence,Consequence,"Let $\mathscr P$ be a proof system for a formal language $\LL$.

Let $\FF$ be a collection of WFFs of $\LL$.


Denote with $\map {\mathscr P} \FF$ the proof system obtained from $\mathscr P$ by adding all the WFFs from $\FF$ as axioms.

Let $\phi$ be a theorem of $\map {\mathscr P} \FF$.


Then $\phi$ is called a provable consequence of $\FF$, and this is denoted as:

:$\FF \vdash_{\mathscr P} \phi$


Note in particular that for $\FF = \O$, this notation agrees with the notation for a $\mathscr P$-theorem:

:$\vdash_{\mathscr P} \phi$",Definition:Provable Consequence,"['Definitions/Provable Consequences', 'Definitions/Logical Implication', 'Definitions/Proof Systems']"
Definition:Consequence,Consequence,"Let $\mathscr M$ be a formal semantics for a formal language $\LL$.

Let $\FF$ be a collection of WFFs of $\LL$.


Let $\map {\mathscr M} \FF$ be the formal semantics obtained from $\mathscr M$ by retaining only the structures of $\mathscr M$ that are models of $\FF$.

Let $\phi$ be a tautology for $\map {\mathscr M} \FF$.


Then $\phi$ is called a semantic consequence of $\FF$, and this is denoted as:

:$\FF \models_{\mathscr M} \phi$


That is to say, $\phi$ is a semantic consequence of $\FF$  if and only if , for each $\mathscr M$-structure $\MM$:

:$\MM \models_{\mathscr M} \FF$ implies $\MM \models_{\mathscr M} \phi$

where $\models_{\mathscr M}$ is the models relation.


Note in particular that for $\FF = \O$, the notation agrees with the notation for a $\mathscr M$-tautology:

:$\models_{\mathscr M} \phi$


The concept naturally generalises to sets of formulas $\GG$ on the  :

:$\FF \models_{\mathscr M} \GG$

 if and only if  $\FF \models_{\mathscr M} \phi$ for every $\phi \in \GG$.


 

 ",Definition:Semantic Consequence,"['Definitions/Semantic Consequences', 'Definitions/Formal Semantics', 'Definitions/Logical Implication']"
Definition:Consequence,Consequence,A consequence is a state in a game which results from a move made by a player in that game made according to the rules.,Definition:Consequence (Game Theory),['Definitions/Game Theory']
Definition:Consequence,Consequence,"Let $G$ be a game.

Let $P$ be a player of $G$.

Let $A$ be the set of moves available to $P$.

Let $C$ be the set of consequences of those moves.


A consequence function for $P$ is a mapping from the set $A$ to the set $C$:
:$g: A \to C$",Definition:Consequence Function,['Definitions/Game Theory']
Definition:Consistent,Consistent,"Let $\LL$ be a logical language.

Let $\mathscr P$ be a proof system for $\LL$.


=== Proof System ===
Let $\LL$ be a logical language.

Let $\mathscr P$ be a proof system for $\LL$.


Then $\mathscr P$ is consistent  if and only if :

:There exists a logical formula $\phi$ such that $\not \vdash_{\mathscr P} \phi$

That is, some logical formula $\phi$ is not a theorem of $\mathscr P$.


=== Propositional Logic ===
Let $\LL_0$ be the language of propositional logic.

Let $\mathscr P$ be a proof system for $\LL_0$.


=== Definition 1 ===
Let $\LL_0$ be the language of propositional logic.

Let $\mathscr P$ be a proof system for $\LL_0$.


Then $\mathscr P$ is consistent  if and only if :

:There exists a logical formula $\phi$ such that $\not \vdash_{\mathscr P} \phi$

That is, some logical formula $\phi$ is not a theorem of $\mathscr P$.


Category:Definitions/Proof Systems
Category:Definitions/Propositional Logic

=== Definition 2 ===
Let $\LL$ be the language of propositional logic.

Let $\mathscr P$ be a proof system for $\LL_0$.

Suppose that in $\mathscr P$, the Rule of Explosion (Variant 3) holds.


Then $\mathscr P$ is consistent  if and only if :

:For every logical formula $\phi$, not both of $\phi$ and $\neg \phi$ are theorems of $\mathscr P$

=== Set of Formulas ===
Let $\LL$ be a logical language.

Let $\mathscr P$ be a proof system for $\LL$.

Let $\FF$ be a collection of logical formulas.


Then $\FF$ is consistent for $\mathscr P$  if and only if :

:There exists a logical formula $\phi$ such that $\FF \nvdash_{\mathscr P} \phi$.

That is, some logical formula $\phi$ is not a provable consequence of $\FF$.


=== Propositional Logic ===
 
Let $\LL_0$ be the language of propositional logic.

Let $\mathscr P$ be a proof system for $\LL_0$.

Let $\FF$ be a collection of logical formulas.


=== Definition 1 ===
Let $\LL_0$ be the language of propositional logic.

Let $\mathscr P$ be a proof system for $\LL_0$.

Let $\FF$ be a collection of logical formulas.


Then $\FF$ is consistent for $\mathscr P$  if and only if :

:There exists a logical formula $\phi$ such that $\FF \not \vdash_{\mathscr P} \phi$

That is, some logical formula $\phi$ is not a $\mathscr P$-provable consequence of $\FF$.


Category:Definitions/Proof Systems
Category:Definitions/Propositional Logic

=== Definition 2 ===
Let $\LL$ be the language of propositional logic.

Let $\mathscr P$ be a proof system for $\LL_0$.

Let $\FF$ be a collection of logical formulas.

Suppose that in $\mathscr P$, the Rule of Explosion (Variant 3) holds.


Then $\FF$ is consistent for $\mathscr P$  if and only if :

:For every logical formula $\phi$, not both of $\phi$ and $\neg \phi$ are $\mathscr P$-provable consequences of $\FF$",Definition:Consistent (Logic),"['Definitions/Proof Systems', 'Definitions/Logical Consistency']"
Definition:Consistent,Consistent,"A system of simultaneous equations is referred to as consistent  if and only if  it has at least one solution.

That is,  if and only if  there exists a set of values for its variables such that all the equations are satisfied.


=== Inconsistent ===
A set of equations is described as inconsistent  if and only if  they are not consistent

That is, there exists no set of values for its variables such that all the equations are satisfied.",Definition:Consistent Simultaneous Equations,"['Definitions/Consistent Simultaneous Equations', 'Definitions/Simultaneous Equations']"
Definition:Consistent,Consistent,"Let $X_1, X_2, \ldots, X_n$ be random variables.

Let the joint distribution of $X_1, X_2, \ldots, X_n$ be indexed by a population parameter $\theta$.

Let $\hat \theta$ be an estimator of $\theta$.

Then $\hat \theta$ is consistent  if and only if :
:$\ds \lim_{n \mathop \to \infty} \map \Pr {\size {\hat \theta - \theta} \ge \epsilon} = 0$
for all $\epsilon > 0$.",Definition:Consistent Estimator,"['Definitions/Consistent Estimators', 'Definitions/Estimators']"
Definition:Content,Content,"=== Integer Polynomial ===
Let $f \in \Z \sqbrk X$ be a polynomial with integer coefficients.

Then the content of $f$, denoted $\cont f$, is the greatest common divisor of the coefficients of $f$.

=== Rational Polynomial ===
Let $f \in \Q \sqbrk X$ be a polynomial with rational coefficients.


The content of $f$ is defined as:
:$\cont f := \dfrac {\cont {n f} } n$
where $n \in \N$ is such that $n f \in \Z \sqbrk X$.

=== Polynomial in GCD Domain ===
Let $D$ be a GCD domain.

Let $K$ be the field of quotients of $D$.

Let $f \in K \sqbrk X$ be a polynomial.

Let $a \in D$ be such that $a f \in D \sqbrk X$.

Let $d$ be the greatest common divisor of the coefficients of $a f$.

Then we define the content of $f$ to be:
:$\cont f := \dfrac d a$",Definition:Content of Polynomial,"['Definitions/Polynomial Theory', 'Definitions/Content of Polynomial']"
Definition:Content,Content,"Let $f \in \Z \sqbrk X$ be a polynomial with integer coefficients.

Then the content of $f$, denoted $\cont f$, is the greatest common divisor of the coefficients of $f$.",Definition:Content of Polynomial/Integer,"['Definitions/Polynomial Theory', 'Definitions/Content of Polynomial']"
Definition:Content,Content,"Let $f \in \Q \sqbrk X$ be a polynomial with rational coefficients.


The content of $f$ is defined as:
:$\cont f := \dfrac {\cont {n f} } n$
where $n \in \N$ is such that $n f \in \Z \sqbrk X$.",Definition:Content of Polynomial/Rational,['Definitions/Content of Polynomial']
Definition:Continuous,Continuous,"The concept of continuity makes precise the intuitive notion that a function has no ""jumps"" at a given point.

Loosely speaking, in the case of a real function, continuity at a point is defined as the property that the graph of the function does not have a ""break"" at the point.

Thus, a small change in the independent variable causes a similar small change in the dependent variable

This concept appears throughout mathematics and correspondingly has many variations and generalizations.",Definition:Continuous Mapping,"['Definitions/Continuous Mappings', 'Definitions/Mappings', 'Definitions/Continuity']"
Definition:Continuous,Continuous,Continuous geometry is a branch of the projective geometry which investigates spaces whose dimension can range over the continuous interval $\closedint 0 1$.,Definition:Continuous Geometry,"['Definitions/Continuous Geometry', 'Definitions/Projective Geometry', 'Definitions/Branches of Mathematics']"
Definition:Continuous,Continuous,"Let $L = \left({X, \preceq}\right)$ be an ordered set.

Let $S = \left({Y, \preceq'}\right)$ be an ordered subset of $L$.


Then $S$ is continuous lattice subframe of $L$  if and only if 
:$S$ inherits infima and directed suprema.",Definition:Continuous Lattice Subframe,['Definitions/Order Theory']
Definition:Continuous,Continuous,Data which can be described with a continuous variable obtained by the process of measurement are known as continuous data.,Definition:Sample Statistic/Continuous,['Definitions/Sample Statistics']
Definition:Continuous,Continuous,"=== Informal Definition ===
The concept of continuity makes precise the intuitive notion that a function has no ""jumps"" or ""holes"" at a given point.

Loosely speaking, a real function $f$ is continuous at a point $p$  if and only if  the graph of $f$ does not have a ""break"" at $p$.

=== Continuity at a Point ===
Let $A \subseteq \R$ be a subset of the real numbers.

Let $f: A \to \R$ be a real function.

Let $x \in A$ be a point of $A$.


$f$ is continuous at $x$  if and only if  the limit $\ds \lim_{y \mathop \to x} \map f y$ exists and:
:$\ds \lim_{y \mathop \to x} \map f y = \map f x$

=== Continuous Everywhere ===
Let $f: \R \to \R$ be a real function.


Then $f$ is everywhere continuous  if and only if  $f$ is continuous at every point in $\R$.

=== Continuity on a Subset of Domain ===
Let $A \subseteq \R$ be any subset of the real numbers.

Let $f: A \to \R$ be a real function.


Then $f$ is continuous on $A$  if and only if  $f$ is continuous at every point of $A$.",Definition:Continuous Real Function,"['Definitions/Continuous Real Functions', 'Definitions/Continuous Functions', 'Definitions/Continuous Mappings', 'Definitions/Real Functions']"
Definition:Continuous,Continuous,"Let $A \subseteq \R$ be an open subset of the real numbers $\R$.

Let $f: A \to \R$ be a real function.


Let $x_0 \in A$. 

Then $f$ is said to be left-continuous at $x_0$  if and only if  the limit from the left of $\map f x$ as $x \to x_0$ exists and:

:$\ds \lim_{\substack {x \mathop \to x_0^- \\ x_0 \mathop \in A} } \map f x = \map f {x_0}$

where $\ds \lim_{x \mathop \to x_0^-}$ is a limit from the left.


Furthermore, $f$ is said to be left-continuous  if and only if :

:$\forall x_0 \in A$, $f$ is left-continuous at $x_0$",Definition:Continuous Real Function/Left-Continuous,['Definitions/Continuous Real Functions']
Definition:Continuous,Continuous,"Let $S \subseteq \R$ be an open subset of the real numbers $\R$.

Let $f: S \to \R$ be a real function.


Let $x_0 \in S$. 

Then $f$ is said to be right-continuous at $x_0$  if and only if  the limit from the right of $\map f x$ as $x \to x_0$ exists and:

:$\ds \lim_{\substack {x \mathop \to x_0^+ \\ x_0 \mathop \in A}} \map f x = \map f {x_0}$

where $\ds \lim_{x \mathop \to x_0^+}$ is a limit from the right.


Furthermore, $f$ is said to be right-continuous  if and only if :

:$\forall x_0 \in S$, $f$ is right-continuous at $x_0$",Definition:Continuous Real Function/Right-Continuous,['Definitions/Continuous Real Functions']
Definition:Continuous,Continuous,"Let $\R^n$ be the cartesian $n$-space.

Let $f: \R^n \to \R$ be a real-valued function on $\R^n$.


Then $f$ is continuous on $\R^n$  if and only if :
:$\forall a \in \R^n: \forall \epsilon \in \R_{>0}: \exists \delta \in \R_{>0}: \forall x \in \R^n: \map d {x, a} < \delta \implies \size {\map f x - \map f a} < \epsilon$
where $\map d {x, a}$ is the distance function on $\R^n$:

:$\ds d: \R^n \to \R: \map d {x, y} := \sqrt {\sum_{i \mathop = 1}^n \paren {x_i - y_i}^2}$

where $x = \tuple {x_1, x_2, \ldots, x_n}, y = \tuple {y_1, y_2, \ldots, y_n}$ are general elements of $\R^n$.",Definition:Continuous Real-Valued Vector Function,"['Definitions/Continuity', 'Definitions/Real-Valued Functions']"
Definition:Continuous,Continuous,"As the complex plane is a metric space, the same definition of continuity applies to complex functions as to metric spaces.



Let $A_1, A_2 \subseteq \C$ be subsets of the complex plane.

Let $f: A_1 \to A_2$ be a complex function from $A_1$ to $A_2$.

Let $a \in A_1$.


=== Definition using Limit ===
Let $A_1, A_2 \subseteq \C$ be subsets of the complex plane.

Let $f: A_1 \to A_2$ be a complex function from $A_1$ to $A_2$.

Let $a \in A_1$.


$f$ is continuous at (the point) $a$  if and only if :
:The limit of $\map f z$ as $z \to a$ exists, and
:$\ds \lim_{z \mathop \to a} \map f z = \map f a$

=== Epsilon-Delta Definition ===
Let $A_1, A_2 \subseteq \C$ be subsets of the complex plane.

Let $f: A_1 \to A_2$ be a complex function from $A_1$ to $A_2$.

Let $a \in A_1$.


$f$ is continuous at (the point) $a$  if and only if :

:$\forall \epsilon > 0: \exists \delta > 0: \forall z \in A_1: \cmod {z - a} < \delta \implies \cmod {\map f z - \map f a} < \epsilon$

=== Epsilon-Neighborhood Definition ===
Let $A_1, A_2 \subseteq \C$ be subsets of the complex plane.

Let $f: A_1 \to A_2$ be a complex function from $A_1$ to $A_2$.

Let $a \in A_1$.


Let $A_1$ be open in $\C$.


$f$ is continuous at (the point) $a$  if and only if :
:$\forall \map {\NN_\epsilon} {\map f a}: \exists \map {\NN_\delta} a: f \sqbrk {\map {\NN_\delta} a} \subseteq \map {\NN_\epsilon} {\map f a}$
where $\map {\NN_\epsilon} a$ is the $\epsilon$-neighborhood of $a$ in $A_1$.


That is, for every $\epsilon$-neighborhood of $\map f a$ in $\C$, there exists a $\delta$-neighborhood of $a$ in $\C$ whose image is a subset of that $\epsilon$-neighborhood.

=== Open Sets Definition ===
Let $A_1, A_2 \subseteq \C$ be subsets of the complex plane.

Let $f: A_1 \to A_2$ be a complex function from $A_1$ to $A_2$.


Let $A_1$ be open in $\C$.


$f$ is continuous  if and only if :
:for every set $U \subseteq \C$ which is open in $\C$, $f^{-1} \sqbrk U$ is open in $\C$.",Definition:Continuous Complex Function,"['Definitions/Continuous Complex Functions', 'Definitions/Continuous Functions', 'Definitions/Continuous Mappings', 'Definitions/Complex Functions']"
Definition:Continuous,Continuous,"Let $S$ be a set of mappings.

Let $y \in S$ be a mapping.

Let $J \sqbrk y: S \to \R$ be a functional.

Suppose:
  
:$\forall \epsilon \in \R_{>0}: \exists \delta \in \R_{>0}: \size {y - y_0} < \delta \implies \size {J \sqbrk y - J \sqbrk {y_0} } < \epsilon$


Then $J \sqbrk y$ is said to be a continuous functional and is continuous at the point $y_0 \in S$.",Definition:Continuity/Functional,['Definitions/Calculus of Variations']
Definition:Continuous,Continuous,"Let $M_1 = \struct {A_1, d_1}$ and $M_2 = \struct {A_2, d_2}$ be metric spaces.

Let $f: A_1 \to A_2$ be a mapping from $A_1$ to $A_2$.

Let $a \in A_1$ be a point in $A_1$.


=== Continuous at a Point ===
Let $M_1 = \struct {A_1, d_1}$ and $M_2 = \struct {A_2, d_2}$ be metric spaces.

Let $f: A_1 \to A_2$ be a mapping from $A_1$ to $A_2$.

Let $a \in A_1$ be a point in $A_1$.


=== $\epsilon$-$\delta$ Definition ===

Let $M_1 = \struct {A_1, d_1}$ and $M_2 = \struct {A_2, d_2}$ be metric spaces.

Let $f: A_1 \to A_2$ be a mapping from $A_1$ to $A_2$.

Let $a \in A_1$ be a point in $A_1$.


$f$ is continuous at (the point) $a$ (with respect to the metrics $d_1$ and $d_2$)  if and only if :
:$\forall \epsilon \in \R_{>0}: \exists \delta \in \R_{>0}: \forall x \in A_1: \map {d_1} {x, a} < \delta \implies \map {d_2} {\map f x, \map f a} < \epsilon$
where $\R_{>0}$ denotes the set of all strictly positive real numbers.

=== Definition by Limits ===
Let $M_1 = \struct {A_1, d_1}$ and $M_2 = \struct {A_2, d_2}$ be metric spaces.

Let $f: A_1 \to A_2$ be a mapping from $A_1$ to $A_2$.

Let $a \in A_1$ be a point in $A_1$.


$f$ is continuous at (the point) $a$ (with respect to the metrics $d_1$ and $d_2$)  if and only if :
:$(1): \quad$ The limit of $\map f x$ as $x \to a$ exists
:$(2): \quad \ds \lim_{x \mathop \to a} \map f x = \map f a$.

=== $\epsilon$-Ball Definition ===
Let $M_1 = \struct {A_1, d_1}$ and $M_2 = \struct {A_2, d_2}$ be metric spaces.

Let $f: A_1 \to A_2$ be a mapping from $A_1$ to $A_2$.

Let $a \in A_1$ be a point in $A_1$.


$f$ is continuous at (the point) $a$ (with respect to the metrics $d_1$ and $d_2$)  if and only if :
:$\forall \epsilon \in \R_{>0}: \exists \delta \in \R_{>0}: f \sqbrk {\map {B_\delta} {a; d_1} } \subseteq \map {B_\epsilon} {\map f a; d_2}$
where $\map {B_\epsilon} {\map f a; d_2}$ denotes the open $\epsilon$-ball of $\map f a$ with respect to the metric $d_2$, and similarly for $\map {B_\delta} {a; d_1}$.

=== Definition by Neighborhoods ===
Let $M_1 = \struct {A_1, d_1}$ and $M_2 = \struct {A_2, d_2}$ be metric spaces.

Let $f: A_1 \to A_2$ be a mapping from $A_1$ to $A_2$.

Let $a \in A_1$ be a point in $A_1$.


$f$ is continuous at (the point) $a$ (with respect to the metrics $d_1$ and $d_2$)  if and only if :
:for each neighborhood $N'$ of $\map f a$ in $M_2$ there exists a corresponding neighborhood $N$ of $a$ in $M_1$ such that $f \sqbrk N \subseteq N'$.

=== Continuous on a Space ===
Let $M_1 = \struct {A_1, d_1}$ and $M_2 = \struct {A_2, d_2}$ be metric spaces.

Let $f: A_1 \to A_2$ be a mapping from $A_1$ to $A_2$.


=== Definition 1 ===

Let $M_1 = \struct {A_1, d_1}$ and $M_2 = \struct {A_2, d_2}$ be metric spaces.

Let $f: A_1 \to A_2$ be a mapping from $A_1$ to $A_2$.


$f$ is continuous from $\struct {A_1, d_1}$ to $\struct {A_2, d_2}$  if and only if  it is continuous at every point $x \in A_1$.

=== Definition 2 ===
Let $M_1 = \struct {A_1, d_1}$ and $M_2 = \struct {A_2, d_2}$ be metric spaces.

Let $f: A_1 \to A_2$ be a mapping from $A_1$ to $A_2$.


$f$ is continuous from $\struct {A_1, d_1}$ to $\struct {A_2, d_2}$  if and only if :
:for every $U \subseteq A_2$ which is open in $M_2$, $f^{-1} \sqbrk U$ is open in $M_1$.


By definition, this is equivalent to the continuity of $f$ with respect to the induced topologies on $A_1$ and $A_2$.",Definition:Continuous Mapping (Metric Space),"['Definitions/Continuous Mappings on Metric Spaces', 'Definitions/Continuous Mappings', 'Definitions/Metric Spaces']"
Definition:Continuous,Continuous,"Let $T_1 = \left({S_1, \tau_1}\right)$ and $T_2 = \left({S_2, \tau_2}\right)$ be topological spaces.

Let $A, B \subseteq S_1$ be subsets of $S_1$ such that $A \subseteq B$.

Let $f: A \to S_2$ and $g: B \to S_2$ be continuous mappings.


Then $g$ is a continuous extension of $f$  if and only if :
:$\forall s \in A: f \left({s}\right) = g \left({s}\right)$


That is, a continuous extension of $f$ is a continuous mapping on a superset which agrees with $f$ on the domain of $f$.


Simply, it is a continuous mapping which is an extension.


=== Real Function ===
Let $A$, $B \subseteq \R$ be subsets of the real numbers such that $A \subseteq B$.

Let $f: A \to \R$ and $g: B \to \R$ be continuous real functions.


Then $g$ is a continuous extension of $f$  if and only if :
:$\forall x \in A : \map f x = \map g x$


Category:Definitions/Continuous Real Functions

Category:Definitions/Continuity",Definition:Continuous Extension,['Definitions/Continuity']
Definition:Continuous,Continuous,"Let $T_1 = \struct {S_1, \tau_1}$ and $T_2 = \struct {S_2, \tau_2}$ be topological spaces.

Let $f: S_1 \to S_2$ be a mapping from $S_1$ to $S_2$.",Definition:Continuous Mapping (Topology),"['Definitions/Continuous Mappings (Topology)', 'Definitions/Continuous Mappings', 'Definitions/Continuity', 'Definitions/Topology']"
Definition:Continuous,Continuous,"Let $\mathbf C$, $\mathbf D$ be metacategories.

Let $F: \mathbf C \to \mathbf D$ be a functor.


Then $F$ is continuous  if and only if  for all diagrams $D: \mathbf J \to \mathbf C$ with limit ${\varprojlim \,}_j \, D_j$:

:$\map F {{\varprojlim \,}_j \, D_j} \cong {\varprojlim \,}_j \, F D_j$

where $F D: \mathbf J \to \mathbf D$ is the diagram obtained by composition of $F$ with $D$, and $\mathbf J$ is an arbitrary metacategory.",Definition:Continuous Functor,"['Definitions/Category Theory', 'Definitions/Limits and Colimits']"
Definition:Convex,Convex,"A geometric figure is convex  if and only if  a line segment joining two points on its boundary lies entirely inside it.


=== Convex Polygon ===
Let $P$ be a polygon.

$P$ is a convex polygon  if and only if :
:For all points $A$ and $B$ located inside $P$, the line $AB$ is also inside $P$.

=== Convex Polyhedron ===
Let $P$ be a polyhedron.


$P$ is a convex polyhedron  if and only if :
:For all points $A$ and $B$ located inside $P$, the line $AB$ is also inside $P$.",Definition:Convex Geometric Figure,['Definitions/Geometric Figures']
Definition:Convex,Convex,"=== Definition 1 ===
Let $P$ be a polygon.

$P$ is a convex polygon  if and only if :
:For all points $A$ and $B$ located inside $P$, the line $AB$ is also inside $P$.

=== Definition 2 ===
Let $P$ be a polygon.

$P$ is a convex polygon  if and only if :
:every internal angle of $P$ is not greater than $180 \degrees$.

=== Definition 3 ===
Let $P$ be a polygon.

$P$ is a convex polygon  if and only if :
:the region enclosed by $P$ lies entirely on the same side of each side of $P$


 

=== Definition 4 ===
Let $P$ be a polygon.

$P$ is a convex polygon  if and only if :
:the region enclosed by $P$ is the intersection of a finite number of half-planes.


Note that an intersection of a finite number of half-planes is not necessarily a polygon.

 

=== Definition 5 ===
Let $P$ be a polygon.

$P$ is a convex polygon  if and only if :
:the region enclosed by $P$ is the intersection of all half-planes that contain $P$ and that are created by all the lines that are tangent to $P$.



By tangent we mean any line $l$ that contain one or more point of $P$ and has $P$ entirely in one of the half-planes created by $l$. 

In this sense any line, that is spanned by a side of $P$, is tangent to $P$.",Definition:Convex Polygon,"['Definitions/Convex Polygons', 'Definitions/Polygons']"
Definition:Convex,Convex,"Let $P$ be a polyhedron.


=== Definition 1 ===
Let $P$ be a polyhedron.


$P$ is a convex polyhedron  if and only if :
:For all points $A$ and $B$ located inside $P$, the line $AB$ is also inside $P$.

=== Definition 2 ===
Let $P$ be a polyhedron.


$P$ is a convex polyhedron  if and only if :
:For every face of $P$, the plane in which it is embedded does not intersect the interior of $P$.

=== Definition 3 ===
Let $P$ be a polyhedron.


$P$ is a convex polyhedron  if and only if :
:For each face of $P$, the whole of $P$ lies on one side of the plane of that face.",Definition:Convex Polyhedron,"['Definitions/Convex Polyhedra', 'Definitions/Polyhedra']"
Definition:Convex,Convex,"=== Definition 1 ===
A subset $A$ of an ordered set $\struct {S, \preceq}$ is convex (in $S$)  if and only if :
:$\forall x, y \in A: \forall z \in S: x \preceq z \preceq y \implies z \in A$

=== Definition 2 ===
A subset $A$ of an ordered set $\struct {S, \preceq}$ is convex (in $S$)  if and only if :
:$\forall x, y \in A: \forall z \in S: x \prec z \prec y \implies z \in A$",Definition:Convex Set (Order Theory),"['Definitions/Order Theory', 'Definitions/Convex Sets (Order Theory)']"
Definition:Convex,Convex,"Let $\Bbb F \in \set {\R, \C}$.

Let $V$ be a vector space over $\Bbb F$.

Let $C \subseteq V$.

=== Definition 1 ===
Let $\Bbb F \in \set {\R, \C}$.

Let $V$ be a vector space over $\Bbb F$.

Let $C \subseteq V$.


We say that $C$ is convex  if and only if :

:$t x + \paren {1 - t} y \in C$

for each $x, y \in C$ and $t \in \closedint 0 1$.

=== Definition 2 ===
Let $\Bbb F \in \set {\R, \C}$.

Let $V$ be a vector space over $\Bbb F$.

Let $C \subseteq V$.


We say that $C$ is convex  if and only if :

:$t C + \paren {1 - t} C \subseteq C$

for each $t \in \closedint 0 1$, where $t C + \paren {1 - t} C$ denotes a linear combination of subsets.


=== Line Segment ===
Let $V$ be a vector space over $\R$ or $\C$.

Let $x, y \in V$.


The set:

:$\set {t x + \paren {1 - t} y: t \in \closedint 0 1}$ 

is called the (straight) line segment joining $x$ and $y$.


A convex set can thus be described as a set containing all straight line segments between its elements.",Definition:Convex Set (Vector Space),"['Definitions/Vector Spaces', 'Definitions/Convex Sets (Vector Spaces)', 'Definitions/Convex Analysis']"
Definition:Convex,Convex,"Let $f$ be a real function which is defined on a real interval $I$.

=== Definition 1 ===
Let $f$ be a real function which is defined on a real interval $I$.


$f$ is convex on $I$  if and only if :

:$\forall x, y \in I: \forall \alpha, \beta \in \R_{>0}, \alpha + \beta = 1: \map f {\alpha x + \beta y} \le \alpha \map f x + \beta \map f y$


:


The geometric interpretation is that any point on the chord drawn on the graph of any convex function always lies on or above the graph.


=== Strictly Convex ===
Let $f$ be a real function which is defined on a real interval $I$.


$f$ is strictly convex on $I$  if and only if :

:$\forall x, y \in I, x \ne y: \forall \alpha, \beta \in \R_{>0}, \alpha + \beta = 1: \map f {\alpha x + \beta y} < \alpha \map f x + \beta \map f y$


:


The geometric interpretation is that any point on the chord drawn on the graph of any convex function always lies above the graph.

=== Definition 2 ===
Let $f$ be a real function which is defined on a real interval $I$.


$f$ is convex on $I$  if and only if :

:$\forall x_1, x_2, x_3 \in I: x_1 < x_2 < x_3: \dfrac {\map f {x_2} - \map f {x_1} } {x_2 - x_1} \le \dfrac {\map f {x_3} - \map f {x_2} } {x_3 - x_2}$


Hence a geometrical interpretation: the slope of $P_1 P_2$ is less than or equal to that of $P_2 P_3$:


:


=== Strictly Convex ===
Let $f$ be a real function which is defined on a real interval $I$.


$f$ is strictly convex on $I$  if and only if :

:$\forall x_1, x_2, x_3 \in I: x_1 < x_2 < x_3: \dfrac {f \left({x_2}\right) - f \left({x_1}\right)} {x_2 - x_1} < \dfrac {f \left({x_3}\right) - f \left({x_2}\right)} {x_3 - x_2}$


Hence a geometrical interpretation: the slope of $P_1 P_2$ is less than that of $P_2 P_3$:


:

=== Definition 3 ===
Let $f$ be a real function which is defined on a real interval $I$.


$f$ is convex on $I$  if and only if :

:$\forall x_1, x_2, x_3 \in I: x_1 < x_2 < x_3: \dfrac {\map f {x_2} - \map f {x_1} } {x_2 - x_1} \le \dfrac {\map f {x_3} - \map f {x_1} } {x_3 - x_1}$


Hence a geometrical interpretation: the slope of $P_1 P_2$ is less than or equal to that of $P_1 P_3$:


:


=== Strictly Convex ===
Let $f$ be a real function which is defined on a real interval $I$.


$f$ is strictly convex on $I$  if and only if :

:$\forall x_1, x_2, x_3 \in I: x_1 < x_2 < x_3: \dfrac {f \left({x_2}\right) - f \left({x_1}\right)} {x_2 - x_1} < \dfrac {f \left({x_3}\right) - f \left({x_1}\right)} {x_3 - x_1}$


Hence a geometrical interpretation: the slope of $P_1 P_2$ is less than that of $P_1 P_3$:


:",Definition:Convex Real Function,"['Definitions/Convex Real Functions', 'Definitions/Real Functions', 'Definitions/Real Analysis']"
Definition:Convex Set,Convex Set,"=== Definition 1 ===
A subset $A$ of an ordered set $\struct {S, \preceq}$ is convex (in $S$)  if and only if :
:$\forall x, y \in A: \forall z \in S: x \preceq z \preceq y \implies z \in A$

=== Definition 2 ===
A subset $A$ of an ordered set $\struct {S, \preceq}$ is convex (in $S$)  if and only if :
:$\forall x, y \in A: \forall z \in S: x \prec z \prec y \implies z \in A$",Definition:Convex Set (Order Theory),"['Definitions/Order Theory', 'Definitions/Convex Sets (Order Theory)']"
Definition:Convex Set,Convex Set,"Let $\Bbb F \in \set {\R, \C}$.

Let $V$ be a vector space over $\Bbb F$.

Let $C \subseteq V$.

=== Definition 1 ===
Let $\Bbb F \in \set {\R, \C}$.

Let $V$ be a vector space over $\Bbb F$.

Let $C \subseteq V$.


We say that $C$ is convex  if and only if :

:$t x + \paren {1 - t} y \in C$

for each $x, y \in C$ and $t \in \closedint 0 1$.

=== Definition 2 ===
Let $\Bbb F \in \set {\R, \C}$.

Let $V$ be a vector space over $\Bbb F$.

Let $C \subseteq V$.


We say that $C$ is convex  if and only if :

:$t C + \paren {1 - t} C \subseteq C$

for each $t \in \closedint 0 1$, where $t C + \paren {1 - t} C$ denotes a linear combination of subsets.


=== Line Segment ===
Let $V$ be a vector space over $\R$ or $\C$.

Let $x, y \in V$.


The set:

:$\set {t x + \paren {1 - t} y: t \in \closedint 0 1}$ 

is called the (straight) line segment joining $x$ and $y$.


A convex set can thus be described as a set containing all straight line segments between its elements.",Definition:Convex Set (Vector Space),"['Definitions/Vector Spaces', 'Definitions/Convex Sets (Vector Spaces)', 'Definitions/Convex Analysis']"
Definition:Convolution,Convolution,"Let $f$ and $g$ be real functions which are integrable.

The convolution integral of $f$ and $g$ is defined as:
:$\ds \map f t * \map g t := \int_{-\infty}^\infty \map f u \map g {t - u} \rd u$


=== Positive Real Domain ===
Let $f$ and $g$ be functions which are integrable.

Let $f$ and $g$ be supported on the positive real numbers $\R_{\ge 0}$ only.

The convolution integral of $f$ and $g$ may be defined as:
:$\ds \map f t * \map g t := \int_0^t \map f u \map g {t - u} \rd u$

=== Cross-Correlation ===
Let $f$ and $g$ be real functions which are integrable.


The cross-correlation of $f$ and $g$ is defined as:
:$\ds \map f t \star \map g t := \int_{-\infty}^\infty \map f u \map g {t + u} \rd u$",Definition:Convolution Integral,"['Definitions/Convolution Integrals', 'Definitions/Integral Calculus']"
Definition:Convolution,Convolution,"Let $\BB^n$ be the Borel $\sigma$-algebra on $\R^n$, and let $\lambda^n$ be Lebesgue measure on $\R^n$.


=== Convolution of Measurable Functions ===
Let $\BB^n$ be the Borel $\sigma$-algebra on $\R^n$, and let $\lambda^n$ be Lebesgue measure on $\R^n$.

Let $f, g: \R^n \to \R$ be $\BB^n$-measurable functions such that for all $x \in \R^n$:

:$\ds \int_{\R^n} \map f {x - y} \map g y \rd \map {\lambda^n} y$

is finite.


The convolution of $f$ and $g$, denoted $f * g$, is the mapping defined by:

:$\ds f * g: \R^n \to \R, \map {f * g} x := \int_{\R^n} \map f {x - y} \map g y \rd \map {\lambda^n} y$

=== Convolution of Measurable Function and Measure ===
Let $\mu$ be a measure on the Borel $\sigma$-algebra $\BB^n$ on $\R^n$.

Let $f: \R^n \to \R$ be a $\BB^n$-measurable function such that for all $x \in \R^n$:

:$\ds \int_{\R^n} \map f {x - y} \rd \map \mu y$

is finite.


The convolution of $f$ and $\mu$ is the mapping $f * \mu: \R^n \to \R$ defined as:

:$\ds \forall x \in \R^n: \map {f * \mu} x := \int_{\R^n} \map f {x - y} \rd \map \mu y$

=== Convolution of Measures ===
Let $\mu$ and $\nu$ be measures on the Borel $\sigma$-algebra $\BB^n$ on $\R^n$.


The convolution of $\mu$ and $\nu$, denoted $\mu * \nu$, is the measure defined by:

:$\ds \mu * \nu: \BB^n \to \overline \R, \map {\mu * \nu} B := \int \map {\chi_B} {x + y} \map {\rd \mu} x \map {\rd \nu} y$
where $\chi_B$ is the characteristic function of $B$.",Definition:Convolution (Measure Theory),['Definitions/Measure Theory']
Definition:Convolution,Convolution,"Let $\BB^n$ be the Borel $\sigma$-algebra on $\R^n$, and let $\lambda^n$ be Lebesgue measure on $\R^n$.

Let $f, g: \R^n \to \R$ be $\BB^n$-measurable functions such that for all $x \in \R^n$:

:$\ds \int_{\R^n} \map f {x - y} \map g y \rd \map {\lambda^n} y$

is finite.


The convolution of $f$ and $g$, denoted $f * g$, is the mapping defined by:

:$\ds f * g: \R^n \to \R, \map {f * g} x := \int_{\R^n} \map f {x - y} \map g y \rd \map {\lambda^n} y$",Definition:Convolution of Measurable Functions,['Definitions/Measure Theory']
Definition:Convolution,Convolution,"Let $\mu$ be a measure on the Borel $\sigma$-algebra $\BB^n$ on $\R^n$.

Let $f: \R^n \to \R$ be a $\BB^n$-measurable function such that for all $x \in \R^n$:

:$\ds \int_{\R^n} \map f {x - y} \rd \map \mu y$

is finite.


The convolution of $f$ and $\mu$ is the mapping $f * \mu: \R^n \to \R$ defined as:

:$\ds \forall x \in \R^n: \map {f * \mu} x := \int_{\R^n} \map f {x - y} \rd \map \mu y$",Definition:Convolution of Measurable Function and Measure,['Definitions/Measure Theory']
Definition:Convolution,Convolution,"Let $\mu$ and $\nu$ be measures on the Borel $\sigma$-algebra $\BB^n$ on $\R^n$.


The convolution of $\mu$ and $\nu$, denoted $\mu * \nu$, is the measure defined by:

:$\ds \mu * \nu: \BB^n \to \overline \R, \map {\mu * \nu} B := \int \map {\chi_B} {x + y} \map {\rd \mu} x \map {\rd \nu} y$
where $\chi_B$ is the characteristic function of $B$.",Definition:Convolution of Measures,['Definitions/Measure Theory']
Definition:Convolution,Convolution,"Let $f, g$ be arithmetic functions.


=== Definition 1 ===
Let $f, g$ be arithmetic functions.


The Dirichlet convolution of $f$ and $g$ is the arithmetic function:
:$\ds \map {\paren {f * g} } n = \sum_{d \mathop \divides n} \map f d \map g {\frac n d}$
where the summation runs over the set of positive divisors $d$ of $n$.

=== Definition 2 ===
Let $f, g$ be arithmetic functions.


The Dirichlet convolution of $f$ and $g$ is the arithmetic function:
:$\ds \map {\paren {f * g} } n = \sum_{a b \mathop = n} \map f a \map g b$
where the summation runs over all pairs of positive integers $\tuple {a, b}$ with $a b = n$.",Definition:Dirichlet Convolution,"['Definitions/Analytic Number Theory', 'Definitions/Dirichlet Convolution', 'Definitions/Number Theory']"
Definition:Convolution,Convolution,"Let $\struct {M, \cdot}$ be a divisor-finite monoid.

Let $\struct {R, +, \times}$ be a non-associative ring.

Let $f, g : M \to R$ be mappings.


The convolution of $f$ and $g$ is the mapping $f * g: M \to R$ defined as:
:$\forall m \in M: \map {\paren {f * g} } m := \ds \sum_{x y \mathop = m} \map f x \times \map g y$
where the summation is over the finite set $\set {\tuple {x, y} \in M^2: x y = m}$.",Definition:Convolution of Mappings on Divisor-Finite Monoid,['Definitions/Monoids']
Definition:Coordinate,Coordinate,"Let $\sequence {a_n}$ be a coordinate system of a unitary $R$-module $G$.

Let $\ds x \in G: x = \sum_{k \mathop = 1}^n \lambda_k a_k$.

The scalars $\lambda_1, \lambda_2, \ldots, \lambda_n$ can be referred to as the coordinates of $x$ relative to $\sequence {a_n}$.


=== Elements of Ordered Pair ===
Let $\tuple {a, b}$ be an ordered pair.

The following terminology is used:
:$a$ is called the first coordinate
:$b$ is called the second coordinate.

This definition is compatible with the equivalent definition in the context of Cartesian coordinate systems.",Definition:Coordinate System/Coordinate,['Definitions/Coordinate Systems']
Definition:Coordinate,Coordinate,"Let $\ds \prod_{i \mathop \in I} S_i$ be a cartesian product.

Let $j \in I$, and let $s = \sequence {s_i}_{i \mathop \in I} \in \ds \prod_{i \mathop \in I} S_i$.


Then $s_j$ is called the $j$th coordinate of $s$.


If the indexing set $I$ consists of ordinary numbers $1, 2, \ldots, n$, one speaks about, for example, the first, second, or $n$th coordinate.

For an element $\tuple {s, t} \in S \times T$ of a binary cartesian product, $s$ is the first coordinate, and $t$ is the second coordinate.",Definition:Cartesian Product/Coordinate,['Definitions/Cartesian Product']
Definition:Coterminal,Coterminal,"Coterminal angles are angles which are rotations between the same $2$ lines.

That is, they are angles with the same arms.",Definition:Coterminal Angles,"['Definitions/Coterminal Angles', 'Definitions/Angles']"
Definition:Coterminal,Coterminal,Two sides of a polygon that meet at the same vertex are adjacent to each other.,Definition:Polygon/Adjacent/Sides,['Definitions/Adjacent (Polygons)']
Definition:Coterminal,Coterminal,Two edges of a polyhedron that intersect at a particular vertex are referred to as adjacent to each other.,Definition:Polyhedron/Adjacent/Edge to Edge,['Definitions/Adjacent (Polyhedra)']
Definition:Coterminal,Coterminal,"=== Undirected Graph ===
Let $G = \struct {V, E}$ be an undirected graph.

Two edges $e_1, e_2 \in E$ of $G$ adjacent  if and only if  there exists a vertex $v \in V$ to which they are both incident.


Otherwise they are non-adjacent.

=== Digraph ===
Let $G = \struct {V, E}$ be a digraph.

Two arcs $e_1, e_2 \in E$ of $G$ adjacent  if and only if  there exists a vertex $v \in V$ to which they are both incident.


Otherwise they are non-adjacent.

=== Non-Adjacent ===
Let $G = \struct {V, E}$ be a graph.

Two edges $u, v \in V$ of $G$ are non-adjacent  if and only if  they are not adjacent.",Definition:Adjacent (Graph Theory)/Edges,"['Definitions/Edges of Graphs', 'Definitions/Adjacency (Graph Theory)']"
Definition:Couple,Couple,"A couple, in the context of mechanics, is a system of $2$ forces that are:
:equal in magnitude
:exactly opposite in direction
:with different lines of action.",Definition:Couple (Mechanics),"['Definitions/Couples (Mechanics)', 'Definitions/Mechanics']"
Definition:Couple,Couple,"A Tusi couple is a hypocycloid with $2$ cusps.


:",Definition:Tusi Couple,['Definitions/Hypocycloids']
Definition:Critical Point,Critical Point,"Let $f: X \to Y$ be a smooth map of manifolds.


A point $x \in X$ is called a critical point of $f$  if and only if  $\d f_x: T_x \sqbrk X \to T_y \sqbrk Y$ is not surjective at $x$.

 

Category:Definitions/Topology",Definition:Critical Point (Topology),['Definitions/Topology']
Definition:Critical Point,Critical Point,"Let $M$ be a smooth manifold.

Let $f \in \map {\CC^\infty} M : M \to \R$ be a smooth real-valued function.

Let $p \in M$ be a base point in $M$.

Let $\rd f_p$ be the differential of $f$ at $p$.

Suppose $\rd f_p = 0$.


Then $p$ is called a critical point of $f$.
 ",Definition:Critical Point/Smooth Manifold,['Definitions/Smooth Manifolds']
Definition:Critical Point,Critical Point,"Let $\struct {M, g}$ be a Riemannian manifold.

Let $I = \closedint a b$ be a closed real interval.

Let $J \subseteq \R$ be an open real interval.

Let $\gamma : I \to M$ be an admissible curve.

Let $L_g$ be the Riemannian length of some admissible curve.

Let $\Gamma : J \times I \to M$ be the proper variation of $\gamma$ such that:

:$\forall s \in J, \forall t \in I : \tuple {s, t} \stackrel {\Gamma}{\mapsto} \map {\Gamma_s} t$

Suppose:

:$\forall \Gamma : \ds \dfrac d {d s} \map {L_g} {\Gamma_s} = 0$


Then $\gamma$ is called the critical point of $L_g$.",Definition:Critical Point of Riemannian Length,"['Definitions/Riemannian Manifolds', 'Definitions/Curves']"
Definition:Critical Point,Critical Point,"A critical point is a point on a graph where the curve has a vertical tangent.

That is, where limit of the derivative tends to infinity.",Definition:Critical Point (Analysis),"['Definitions/Critical Points (Analysis)', 'Definitions/Real Analysis']"
Definition:Cubic,Cubic,Cubic is an adjective which means in the shape of a cube.,Definition:Cubic (Geometry),['Definitions/Cubes']
Definition:Cubic,Cubic,"A cubic equation is a polynomial equation of the form:
: $a x^3 + b x^2 + c x + d = 0$


=== Discriminant ===
 

=== Resolvent Equation ===
Let $P$ be the cubic equation:
:$a x^3 + b x^2 + c x + d = 0$ with $a \ne 0$


Let:
:$y = x + \dfrac b {3 a}$
:$Q = \dfrac {3 a c - b^2} {9 a^2}$
:$R = \dfrac {9 a b c - 27 a^2 d - 2 b^3} {54 a^3}$

Let $y = u + v$ where $u v = -Q$.


The resolvent equation of the cubic is given by:
:$u^6 - 2 R u^3 - Q^3$",Definition:Cubic Equation,"['Definitions/Cubic Equations', 'Definitions/Polynomial Equations']"
Definition:Cubic,Cubic,A cubic polynomial is a polynomial of degree $3$.,Definition:Cubic Polynomial,['Definitions/Polynomial Theory']
Definition:Cubic,Cubic,"A cubic graph is a $3$-regular graph, that is, a graph whose vertices all have degree $3$.",Definition:Cubic Graph,"['Definitions/Cubic Graphs', 'Definitions/Regular Graphs', 'Definitions/Graph Theory']"
Definition:Cut,Cut,"Let $\alpha \subset \Q$ be a subset of the set of rational numbers $\Q$ which has the following properties:

:$(1): \quad \alpha \ne \O$ and $\alpha \ne \Q$, that is: $\alpha$ contains at least one rational number but not all rational numbers

:$(2): \quad$ If $p \in \alpha$ and $q \in \Q$ such that $q < p$, then $q \in \alpha$

:$(3): \quad \alpha$ does not contain a greatest element.


Then $\alpha$ is called a cut.


=== Lower Number ===
Let $\alpha$ be a cut.

Let $p \in \alpha$.


Then $p$ is referred to as a lower number of $\alpha$.

=== Upper Number ===
Let $\alpha$ be a cut.

Let $q \in \Q$ such that $q \notin \alpha$.


Then $p$ is referred to as an upper number of $\alpha$.

=== Rational Cut ===
Let $r \in \Q$ be a rational number.

Let $\alpha$ be the cut consisting of all rational numbers $p$ such that $p < r$.


Then $\alpha$ is referred to as a rational cut.


To express the fact that $\alpha$ is a rational cut, the notation $\alpha = r^*$ can be used.",Definition:Cut (Analysis),"['Definitions/Cuts', 'Definitions/Real Analysis', 'Definitions/Order Theory']"
Definition:Cut,Cut,"Let $\struct {S, \preceq}$ be a totally ordered set.


=== Definition 1 ===
Let $\struct {S, \preceq}$ be a totally ordered set.


A Dedekind cut of $\struct {S, \preceq}$ is a non-empty proper subset $L \subsetneq S$ such that:
:$(1): \quad \forall x \in L: \forall y \in S: y \prec x \implies y \in L$ ($L$ is a lower section in $S$)
:$(2): \quad \forall x \in L: \exists y \in L: x \prec y$

=== Definition 2 ===
Let $\struct {S, \preceq}$ be a totally ordered set.


A Dedekind cut of $\struct {S, \preceq}$ is an ordered pair $\tuple {L, R}$ such that:
:$(1): \quad \set {L, R}$ is a partition of $S$.
:$(2): \quad L$ does not have a greatest element.
:$(3): \quad \forall x \in L: \forall y \in R: x \prec y$.",Definition:Dedekind Cut,"['Definitions/Order Theory', 'Definitions/Real Analysis', 'Definitions/Dedekind Cuts']"
Definition:Cut,Cut,"Let $G = \struct {V, E}$ be a connected graph.

Let $v$ be a vertex of $G$.


Then $v$ is a cut-vertex of $G$  if and only if  the vertex deletion $G - v$ is a vertex cut of $G$.

That is, such that $G - v$ is disconnected.


Thus, a cut-vertex of $G$ is a singleton vertex cut of $G$.",Definition:Cut-Vertex,['Definitions/Vertices of Graphs']
Definition:Cut,Cut,"Let $T = \struct {S, \tau}$ be a topological space.

Let $H \subseteq S$ be a connected set in $T$ and let $p \in H$.

Let $p \in H$ such that $H \setminus \set p$ is disconnected, where $\setminus$ denotes set difference.


Then $p$ is a cut point of $H$.",Definition:Cut Point,"['Definitions/Connected Spaces', 'Definitions/Disconnected Sets']"
Definition:Cut,Cut,"Let $G = \struct {V, E}$ be a graph.


A vertex cut of $G$ is a set of vertices $W \subseteq \map V G$ such that the vertex deletion $G \setminus W$ is disconnected.",Definition:Vertex Cut,['Definitions/Vertices of Graphs']
Definition:Cut,Cut,"Let $G$ be a graph.


An edge cut of $G$ is a set of edges $W \subseteq \map E G$ such that the edge deletion $G \setminus W$ is disconnected.",Definition:Edge Cut,['Definitions/Edges of Graphs']
Definition:Cycle,Cycle,"Let $\N_k$ be used to denote the initial segment of natural numbers:
:$\N_k = \closedint 1 k = \set {1, 2, 3, \ldots, k}$

Let $\rho: \N_n \to \N_n$ be a permutation of $n$ letters.


The $k$-cycle $\rho$ is denoted:
:$\begin {pmatrix} i & \map \rho i & \ldots & \map {\rho^{k - 1} } i \end{pmatrix}$


From Existence and Uniqueness of Cycle Decomposition, all permutations can be defined as the product of disjoint cycles.

As Disjoint Permutations Commute, the order in which they are performed does not matter.


So, for a given permutation $\rho$, the cycle notation for $\rho$ consists of all the disjoint cycles into which $\rho$ can be decomposed, concatenated as a product.

It is conventional to omit $1$-cycles from the expression, and to write those cycles with lowest starting number first.


=== Canonical Representation ===
The permutation:

:$\begin{pmatrix}
  1 & 2 & 3 & 4 & 5 \\
  2 & 1 & 4 & 3 & 5 
\end{pmatrix}$

can be expressed in cycle notation as:

:$\begin{pmatrix} 1 & 2 \end{pmatrix} \begin{pmatrix} 3 & 4 \end{pmatrix}$

or as:

:$\begin{pmatrix} 3 & 4 \end{pmatrix} \begin{pmatrix} 5 \end{pmatrix} \begin{pmatrix} 1 & 2 \end{pmatrix}$

or as:

:$\begin{pmatrix} 4 & 3 \end{pmatrix} \begin{pmatrix} 2 & 1 \end{pmatrix}$

etc.

However, only the first is conventional. This is known as the canonical representation.",Definition:Permutation on n Letters/Cycle Notation,['Definitions/Permutation Theory']
Definition:Cycle,Cycle,"A cycle is a circuit in which no vertex except the first (which is also the last) appears more than once.


An $n$-cycle is a cycle with $n$ vertices.


=== Subgraph ===
The set of vertices and edges which go to make up a cycle form a subgraph.

This subgraph itself is also referred to as a cycle.

=== Odd Cycle ===
An odd cycle is a cycle with odd length, that is, with an odd number of edges.

=== Even Cycle ===
An even cycle is a cycle with even length, that is, with an even number of edges.",Definition:Cycle (Graph Theory),"['Definitions/Cycles (Graph Theory)', 'Definitions/Circuits (Graph Theory)', 'Definitions/Paths (Graph Theory)', 'Definitions/Graph Theory']"
Definition:Cycle,Cycle,"A cycle, or periodic solution, is a solution of a differential equation which is a periodic function.

Category:Definitions/Differential Equations",Definition:Cycle (Periodic Solution),['Definitions/Differential Equations']
Definition:Cycle,Cycle,"The repeating block in a periodic (or purely periodic) continued fraction $F$ is called the cycle of $F$.


=== Cycle Length ===
The number of partial denominators in a cycle of a periodic continued fraction is called the cycle length.",Definition:Periodic Continued Fraction/Cycle,['Definitions/Continued Fractions']
Definition:Cyclic,Cyclic,"=== Definition 1 ===
The group $G$ is cyclic  if and only if  every element of $G$ can be expressed as the power of one element of $G$:
:$\exists g \in G: \forall h \in G: h = g^n$
for some $n \in \Z$.

=== Definition 2 ===
The group $G$ is cyclic  if and only if  it is generated by one element $g \in G$:
:$G = \gen g$",Definition:Cyclic Group,"['Definitions/Cyclic Groups', 'Definitions/Abelian Groups', 'Definitions/Group Theory']"
Definition:Cyclic,Cyclic,A cyclic polygon is a polygon $P$ which can be inscribed in a circle.,Definition:Cyclic Polygon,['Definitions/Polygons']
Definition:Cyclic,Cyclic,"A cyclic quadrilateral is a quadrilateral which can be circumscribed:

:


This is an example of a quadrilateral which cannot be circumscribed, and so therefore is not cyclic:

:",Definition:Cyclic Quadrilateral,"['Definitions/Quadrilaterals', 'Definitions/Circumscribe']"
Definition:Decomposition,Decomposition,"A set $S \subseteq \R^n$ is decomposable in $m$ sets $A_1, \ldots, A_m \subset \R^n$  if and only if  there exist isometries $\phi_1, \ldots, \phi_m: \R^n \to \R^n$ such that:

:$(1):\quad \ds S = \bigcup_{k \mathop = 1}^m \map {\phi_k} {A_k}$ 
:$(2):\quad \forall i \ne j: \map {\phi_i} {A_i} \cap \map {\phi_j} {A_j} = \O$

Such a union is known as a decomposition.

 ",Definition:Decomposable Set,['Definitions/Topology']
Definition:Decomposition,Decomposition,"Let $\struct {S_1, \circ {\restriction_{S_1} } }, \struct {S_2, \circ {\restriction_{S_2} } }, \ldots, \struct {S_n, \circ {\restriction_{S_n} } }$ be closed algebraic substructures of an algebraic structure $\struct {S, \circ}$

where $\circ {\restriction_{S_1} }, \circ {\restriction_{S_2} }, \ldots, \circ {\restriction_{S_n} }$ are the operations induced by the restrictions of $\circ$ to $S_1, S_2, \ldots, S_n$ respectively.

Let $\struct {S, \circ}$ be the internal direct product of $S_1$, $S_2, \ldots, S_n$.


The set of algebraic substructures $\struct {S_1, \circ {\restriction_{S_1} } }, \struct {S_2, \circ {\restriction_{S_2} } }, \ldots, \struct {S_n, \circ {\restriction_{S_n} } }$ whose (external) direct product is isomorphic with $\struct {S, \circ}$ is called a decomposition of $S$.",Definition:Internal Direct Product/Decomposition,['Definitions/Internal Direct Products']
Definition:Decomposition,Decomposition,"Let $\struct {H_1, \circ {\restriction_{H_1} } }, \struct {H_2, \circ {\restriction_{H_2} } }, \ldots, \struct {H_n, \circ {\restriction_{H_n} } }$ be subgroups of a group $\struct {G, \circ}$

where $\circ {\restriction_{H_1} }, \circ {\restriction_{H_2} }, \ldots, \circ {\restriction_{H_n} }$ are the operations induced by the restrictions of $\circ$ to $H_1, H_2, \ldots, H_n$ respectively.

Let $\struct {G, \circ}$ be the internal group direct product of $H_1$, $H_2, \ldots, H_n$.


The set of subgroups $\struct {H_1, \circ {\restriction_{H_1} } }, \struct {H_2, \circ {\restriction_{H_2} } }, \ldots, \struct {H_n, \circ {\restriction_{H_n} } }$ whose group direct product is isomorphic with $\struct {G, \circ}$ is called a decomposition of $G$.",Definition:Internal Group Direct Product/Decomposition,['Definitions/Internal Group Direct Products']
Definition:Decomposition,Decomposition,"Let $n > 1 \in \Z$.


From the Fundamental Theorem of Arithmetic, $n$ has a unique factorization of the form:

 
 
 
 

where:
:$p_1 < p_2 < \cdots < p_r$ are distinct primes
:$k_1, k_2, \ldots, k_r$ are (strictly) positive integers.


This unique expression is known as the prime decomposition of $n$.


=== Multiplicity ===
Let $n > 1 \in \Z$.

Let:
:$n = p_1^{k_1} p_2^{k_2} \cdots p_r^{k_r}$
be the prime decomposition of $n$, where:
:$p_1 < p_2 < \cdots < p_r$ are distinct primes
:$k_1, k_2, \ldots, k_r$ are (strictly) positive integers.


For each $p_j \in \set {p_1, p_2, \ldots, p_r}$, its power $k_j$ is known as the multiplicity of $p_j$.",Definition:Prime Decomposition,"['Definitions/Prime Decompositions', 'Definitions/Prime Numbers', 'Definitions/Factorization', 'Definitions/Number Theory']"
Definition:Decomposition,Decomposition,"Let $\map R x = \dfrac {\map P x} {\map Q x}$ be a rational function, where $\map P x$ and $\map Q x$ are expressible as polynomial functions.

Let $\map Q x$ be expressible as:
:$\map Q x = \ds \prod_{k \mathop = 1}^n \map {q_k} x$
where the $\map {q_k} x$ are themselves polynomial functions of degree at least $1$.


Let $\map R x$ be expressible as:
:$\map R x = \map r x \ds \sum_{k \mathop = 0}^n \dfrac {\map {p_k} x} {\map {q_k} x}$
where:
:$\map r x$ is a polynomial function which may or may not be the null polynomial, or be of degree $0$ (that is, a constant)
:each of the $\map {p_k} x$ are polynomial functions
:the degree of $\map {p_k} x$ is strictly less than the degree of $\map {q_k} x$ for all $k$.


Then $\map r x \ds \sum_{k \mathop = 0}^n \dfrac {\map {p_k} x} {\map {q_k} x}$ is a partial fractions expansion of $\map R x$.",Definition:Partial Fractions Expansion,"['Definitions/Partial Fractions Expansions', 'Definitions/Algebra', 'Definitions/Real Analysis', 'Definitions/Analysis']"
Definition:Definite,Definite,"Let $\C$ be the field of complex numbers.

Let $\F$ be a subfield of $\C$.

Let $V$ be a vector space over $\F$

Let $\innerprod \cdot \cdot: V \times V \to \mathbb F$ be a mapping.


Then $\innerprod \cdot \cdot: V \times V \to \mathbb F$ is non-negative definite  if and only if :

:$\forall x \in V: \innerprod x x \in \R_{\ge 0}$


That is, the image of $\innerprod x x$ is always a non-negative real number.",Definition:Non-Negative Definite Mapping,['Definitions/Hilbert Spaces']
Definition:Definite,Definite,"Let $\closedint a b$ be a closed real interval.

Let $f: \closedint a b \to \R$ be a real function.


=== Riemann Integral ===
Let $\closedint a b$ be a closed real interval.

Let $f: \closedint a b \to \R$ be a real function.

Let $\Delta$ be a finite subdivision of $\closedint a b$, $\Delta = \set {x_0, \ldots, x_n}$, $x_0 = a$ and $x_n = b$.

Let there for $\Delta$ be a corresponding sequence $C$ of sample points $c_i$, $C = \tuple {c_1, \ldots, c_n}$, where $c_i \in \closedint {x_{i - 1} } {x_i}$ for every $i \in \set {1, \ldots, n}$.

Let $\map S {f; \Delta, C}$ denote the Riemann sum of $f$ for the subdivision $\Delta$ and the sample point sequence $C$.


Then $f$ is said to be (properly) Riemann integrable on $\closedint a b$  if and only if :
:$\exists L \in \R: \forall \epsilon \in \R_{>0}: \exists \delta \in \R_{>0}: \forall$ finite subdivisions $\Delta$ of $\closedint a b: \forall$ sample point sequences $C$ of $\Delta: \norm \Delta < \delta \implies \size {\map S {f; \Delta, C} - L} < \epsilon$
where $\norm \Delta$ denotes the norm of $\Delta$.


The real number $L$ is called the Riemann integral of $f$ over $\closedint a b$ and is denoted:
:$\ds \int_a^b \map f x \rd x$


More usually (and informally), we say:
:$f$ is (Riemann) integrable over $\closedint a b$.


=== Riemann Integral as Integral Operator ===
Let $C \closedint a b$ be the space of continuous functions.

Let $x \in C \closedint a b$ be a Riemann integrable function.

Let $\R$ be the set of real numbers.


The Riemann integral operator, denoted by $I$, is the mapping $I : C \closedint a b \to \R$ such that:

:$\ds \map I x := \int_a^b \map x t \rd t$

where $\ds \int_a^b \map x t \rd t$ is the Riemann integral.

=== Darboux Integral ===
Let $\closedint a b$ be a closed real interval.

Let $f: \closedint a b \to \R$ be a real function.

Let $f$ be bounded on $\closedint a b$.


Suppose that:
:$\ds \underline {\int_a^b} \map f x \rd x = \overline {\int_a^b} \map f x \rd x$
where $\ds \underline {\int_a^b}$ and $\ds \overline {\int_a^b}$ denote the lower Darboux integral and upper Darboux integral, respectively.


Then the definite (Darboux) integral of $f$ over $\closedint a b$ is defined as:
:$\ds \int_a^b \map f x \rd x = \underline {\int_a^b} \map f x \rd x = \overline {\int_a^b} \map f x \rd x$


$f$ is formally defined as (properly) integrable over $\closedint a b$ in the sense of Darboux, or (properly) Darboux integrable over $\closedint a b$.


More usually (and informally), we say:
:$f$ is (Darboux) integrable over $\closedint a b$.",Definition:Definite Integral,"['Definitions/Definite Integrals', 'Definitions/Integral Calculus']"
Definition:Degenerate,Degenerate,"A degenerate case is a specific manifestation of a particular type of object being included in another, usually simpler, type of object.",Definition:Degenerate Case,"['Definitions/Language Definitions', 'Definitions/Degenerate Cases']"
Definition:Degenerate,Degenerate,"A point-circle is the locus in the Cartesian plane of an equation of the form:

:$(1): \quad \paren {x - a}^2 + \paren {y - b}^2 = 0$

where $a$ and $b$ are real constants.


There is only one point in the Cartesian plane which satisfies $(1)$, and that is the point $\tuple {a, b}$.

It can be considered to be a circle whose radius is equal to zero.",Definition:Point-Circle,"['Definitions/Point-Circles', 'Definitions/Degenerate Conics', 'Definitions/Circles']"
Definition:Degenerate,Degenerate,":

Let $C$ be a double napped right circular cone whose base is $B$.

Let $\theta$ be half the opening angle of $C$.

That is, let $\theta$ be the angle between the axis of $C$ and a generatrix of $C$.

Let a plane $D$ intersect $C$.

Let $\phi$ be the inclination of $D$ to the axis of $C$.


Let $K$ be the set of points which forms the intersection of $C$ with $D$.

Then $K$ is a conic section, whose nature depends on $\phi$.


=== Circle ===
:

Let $C$ be a double napped right circular cone whose base is $B$.

Let $\theta$ be half the opening angle of $C$.

That is, let $\theta$ be the angle between the axis of $C$ and a generatrix of $C$.

Let a plane $D$ intersect $C$.

Let $\phi$ be the inclination of $D$ to the axis of $C$.


Let $K$ be the set of points which forms the intersection of $C$ with $D$.

Then $K$ is a conic section, whose nature depends on $\phi$.


=== Circle ===



:


Let $\phi = \dfrac \pi 2$, thereby making $D$ perpendicular to the axis of $C$.

Then $D$ and $B$ are parallel, and so $K$ is a circle.


=== Transverse Section ===


=== Parabola ===



:


Let $\phi = \theta$.

Then $K$ is a parabola.

=== Ellipse ===



:


Let $\theta < \phi < \dfrac \pi 2$.

That is, the angle between $D$ and the axis of $C$ is between that for which $K$ is a circle and that which $K$ is a parabola.

Then $K$ is an ellipse.

=== Hyperbola ===



:


Let $\phi < \theta$.

Then $K$ is a hyperbola.

Note that in this case $D$ intersects $C$ in two places: one for each nappe of $C$.

=== Degenerate Hyperbola ===


:


Let $\phi < \theta$, that is: so as to make $K$ a hyperbola.

However, let $D$ pass through the apex of $C$.

Then $K$ degenerates into a pair of intersecting straight lines.


:


Let $\phi = \dfrac \pi 2$, thereby making $D$ perpendicular to the axis of $C$.

Then $D$ and $B$ are parallel, and so $K$ is a circle.


=== Transverse Section ===
:

Let $C$ be a double napped right circular cone whose base is $B$.

Let $\theta$ be half the opening angle of $C$.

That is, let $\theta$ be the angle between the axis of $C$ and a generatrix of $C$.

Let a plane $D$ intersect $C$.

Let $\phi$ be the inclination of $D$ to the axis of $C$.


Let $K$ be the set of points which forms the intersection of $C$ with $D$.

Then $K$ is a conic section, whose nature depends on $\phi$.


=== Circle ===


=== Parabola ===


=== Ellipse ===


=== Hyperbola ===


=== Degenerate Hyperbola ===



:


Let $\phi = \dfrac \pi 2$, thereby making $D$ perpendicular to the axis of $C$.

The plane $D$ which is parallel to $B$, whose intersection with the cone is a circle, is known as a transverse section of the cone.

=== Parabola ===
:

Let $C$ be a double napped right circular cone whose base is $B$.

Let $\theta$ be half the opening angle of $C$.

That is, let $\theta$ be the angle between the axis of $C$ and a generatrix of $C$.

Let a plane $D$ intersect $C$.

Let $\phi$ be the inclination of $D$ to the axis of $C$.


Let $K$ be the set of points which forms the intersection of $C$ with $D$.

Then $K$ is a conic section, whose nature depends on $\phi$.


=== Circle ===



:


Let $\phi = \dfrac \pi 2$, thereby making $D$ perpendicular to the axis of $C$.

Then $D$ and $B$ are parallel, and so $K$ is a circle.


=== Transverse Section ===


=== Parabola ===



:


Let $\phi = \theta$.

Then $K$ is a parabola.

=== Ellipse ===



:


Let $\theta < \phi < \dfrac \pi 2$.

That is, the angle between $D$ and the axis of $C$ is between that for which $K$ is a circle and that which $K$ is a parabola.

Then $K$ is an ellipse.

=== Hyperbola ===



:


Let $\phi < \theta$.

Then $K$ is a hyperbola.

Note that in this case $D$ intersects $C$ in two places: one for each nappe of $C$.

=== Degenerate Hyperbola ===


:


Let $\phi < \theta$, that is: so as to make $K$ a hyperbola.

However, let $D$ pass through the apex of $C$.

Then $K$ degenerates into a pair of intersecting straight lines.


:


Let $\phi = \theta$.

Then $K$ is a parabola.

=== Ellipse ===
:

Let $C$ be a double napped right circular cone whose base is $B$.

Let $\theta$ be half the opening angle of $C$.

That is, let $\theta$ be the angle between the axis of $C$ and a generatrix of $C$.

Let a plane $D$ intersect $C$.

Let $\phi$ be the inclination of $D$ to the axis of $C$.


Let $K$ be the set of points which forms the intersection of $C$ with $D$.

Then $K$ is a conic section, whose nature depends on $\phi$.


=== Circle ===



:


Let $\phi = \dfrac \pi 2$, thereby making $D$ perpendicular to the axis of $C$.

Then $D$ and $B$ are parallel, and so $K$ is a circle.


=== Transverse Section ===


=== Parabola ===



:


Let $\phi = \theta$.

Then $K$ is a parabola.

=== Ellipse ===



:


Let $\theta < \phi < \dfrac \pi 2$.

That is, the angle between $D$ and the axis of $C$ is between that for which $K$ is a circle and that which $K$ is a parabola.

Then $K$ is an ellipse.

=== Hyperbola ===



:


Let $\phi < \theta$.

Then $K$ is a hyperbola.

Note that in this case $D$ intersects $C$ in two places: one for each nappe of $C$.

=== Degenerate Hyperbola ===


:


Let $\phi < \theta$, that is: so as to make $K$ a hyperbola.

However, let $D$ pass through the apex of $C$.

Then $K$ degenerates into a pair of intersecting straight lines.


:


Let $\theta < \phi < \dfrac \pi 2$.

That is, the angle between $D$ and the axis of $C$ is between that for which $K$ is a circle and that which $K$ is a parabola.

Then $K$ is an ellipse.

=== Hyperbola ===
:

Let $C$ be a double napped right circular cone whose base is $B$.

Let $\theta$ be half the opening angle of $C$.

That is, let $\theta$ be the angle between the axis of $C$ and a generatrix of $C$.

Let a plane $D$ intersect $C$.

Let $\phi$ be the inclination of $D$ to the axis of $C$.


Let $K$ be the set of points which forms the intersection of $C$ with $D$.

Then $K$ is a conic section, whose nature depends on $\phi$.


=== Circle ===



:


Let $\phi = \dfrac \pi 2$, thereby making $D$ perpendicular to the axis of $C$.

Then $D$ and $B$ are parallel, and so $K$ is a circle.


=== Transverse Section ===


=== Parabola ===



:


Let $\phi = \theta$.

Then $K$ is a parabola.

=== Ellipse ===



:


Let $\theta < \phi < \dfrac \pi 2$.

That is, the angle between $D$ and the axis of $C$ is between that for which $K$ is a circle and that which $K$ is a parabola.

Then $K$ is an ellipse.

=== Hyperbola ===



:


Let $\phi < \theta$.

Then $K$ is a hyperbola.

Note that in this case $D$ intersects $C$ in two places: one for each nappe of $C$.

=== Degenerate Hyperbola ===


:


Let $\phi < \theta$, that is: so as to make $K$ a hyperbola.

However, let $D$ pass through the apex of $C$.

Then $K$ degenerates into a pair of intersecting straight lines.


:


Let $\phi < \theta$.

Then $K$ is a hyperbola.

Note that in this case $D$ intersects $C$ in two places: one for each nappe of $C$.

=== Degenerate Hyperbola ===
:

Let $C$ be a double napped right circular cone whose base is $B$.

Let $\theta$ be half the opening angle of $C$.

That is, let $\theta$ be the angle between the axis of $C$ and a generatrix of $C$.

Let a plane $D$ intersect $C$.

Let $\phi$ be the inclination of $D$ to the axis of $C$.


Let $K$ be the set of points which forms the intersection of $C$ with $D$.

Then $K$ is a conic section, whose nature depends on $\phi$.


=== Circle ===



:


Let $\phi = \dfrac \pi 2$, thereby making $D$ perpendicular to the axis of $C$.

Then $D$ and $B$ are parallel, and so $K$ is a circle.


=== Transverse Section ===


=== Parabola ===



:


Let $\phi = \theta$.

Then $K$ is a parabola.

=== Ellipse ===



:


Let $\theta < \phi < \dfrac \pi 2$.

That is, the angle between $D$ and the axis of $C$ is between that for which $K$ is a circle and that which $K$ is a parabola.

Then $K$ is an ellipse.

=== Hyperbola ===



:


Let $\phi < \theta$.

Then $K$ is a hyperbola.

Note that in this case $D$ intersects $C$ in two places: one for each nappe of $C$.

=== Degenerate Hyperbola ===


:


Let $\phi < \theta$, that is: so as to make $K$ a hyperbola.

However, let $D$ pass through the apex of $C$.

Then $K$ degenerates into a pair of intersecting straight lines.

:


Let $\phi < \theta$, that is: so as to make $K$ a hyperbola.

However, let $D$ pass through the apex of $C$.

Then $K$ degenerates into a pair of intersecting straight lines.

:


Let $\phi < \theta$, that is: so as to make $K$ a hyperbola.

However, let $D$ pass through the apex of $C$.

Then $K$ degenerates into a pair of intersecting straight lines.",Definition:Conic Section/Intersection with Cone/Degenerate Hyperbola,"['Definitions/Degenerate Conics', 'Definitions/Hyperbolas', 'Definitions/Examples of Degenerate Cases']"
Definition:Degenerate,Degenerate,"A degenerate parabola is the conic section whose slicing plane passes through the apex of the cone and is thus tangent to the cone

Hence it consists of a single straight line.",Definition:Degenerate Parabola,"['Definitions/Degenerate Conics', 'Definitions/Parabolas', 'Definitions/Examples of Degenerate Cases']"
Definition:Degenerate,Degenerate,"Let $\mathbb K$ be a field.

Let $V$ be a vector space over $\mathbb K$.

Let $b : V \times V \to \mathbb K$ be a bilinear form on $V$.


Then $b$ is degenerate   there exists $v \in V\setminus \set 0$ such that $\map b {v, u} = 0$ for all $u \in V$.


=== Nondegenerate Bilinear Form ===
Let $\mathbb K$ be a field.

Let $V$ be a vector space over $\mathbb K$.


A bilinear form on $V$ which is not degenerate is nondegenerate.",Definition:Degenerate Bilinear Form,"['Definitions/Bilinear Forms (Linear Algebra)', 'Definitions/Examples of Degenerate Cases']"
Definition:Degenerate,Degenerate,"Let $\struct {S, \vee, \wedge, \neg}$ be a Boolean algebra.


Then $\struct {S, \vee, \wedge, \neg}$ is said to be degenerate  if and only if  $S$ is a singleton.",Definition:Degenerate Boolean Algebra,"['Definitions/Boolean Algebras', 'Definitions/Examples of Degenerate Cases']"
Definition:Degenerate,Degenerate,"Let $T = \struct {S, \tau}$ be a topological space.

Let $H \subseteq S$ be a subset of $T$.


$H$ is a degenerate connected set of $T$  if and only if  it is a connected set of $T$ containing exactly one element.


=== Non-Degenerate Connected Set ===
Let $T = \left({S, \tau}\right)$ be a topological space.


A non-degenerate connected set of $T$ is a connected set of $T$ containing more than one element.


Category:Definitions/Connected Sets

=== Degenerate Connected Space ===

When $H = S$ itself, the entire space can be referred to in this way:

Let $T = \struct {S, \tau}$ be a topological space.


$T$ is a degenerate connected space  if and only if  it contains exactly one element.",Definition:Degenerate Connected Set,"['Definitions/Connected Sets', 'Definitions/Examples of Degenerate Cases']"
Definition:Degenerate,Degenerate,"Let $T = \struct {S, \tau}$ be a topological space.

A degenerate continuum of $T$ is a continuum in $T$ containing exactly one element.


=== Non-Degenerate Continuum ===
Let $T = \struct {S, \tau}$ be a topological space.


A non-degenerate continuum of $T$ is a continuum in $T$ containing more than one element.

Category:Definitions/Continua (Topology)",Definition:Degenerate Continuum,['Definitions/Continua (Topology)']
Definition:Degenerate,Degenerate,"Let $X$ be a discrete random variable on a probability space.


Then $X$ has a degenerate distribution with parameter $r$  if and only if :

:$\Omega_X = \set r$

:$\map \Pr {X = k} = \begin {cases}
1 & : k = r \\
0 & : k \ne r
\end {cases}$

That is, there is only value that $X$ can take, namely $r$, which it takes with certainty.


 

It trivially gives rise to a probability mass function satisfying $\map \Pr \Omega = 1$.

Equally trivially, it has an expectation of $r$ and a variance of $0$.",Definition:Degenerate Distribution,"['Definitions/Probability Theory', 'Definitions/Examples of Degenerate Cases']"
Definition:Degree,Degree,"The degree of a monomial is defined as:
:$\ds \sum_{j \mathop \in J} k_j$
that is, the modulus of the corresponding multiindex.


Category:Definitions/Monomials",Definition:Monomial of Free Commutative Monoid/Degree,['Definitions/Monomials']
Definition:Degree,Degree,"Let $\alpha$ be an algebraic number.

By definition, $\alpha$ is the root of at least one polynomial $P_n$ with rational coefficients.


The degree of $\alpha$ is the degree of the minimal polynomial $P_n$ whose coefficients are all in $\Q$.


=== Algebraic Number over Field ===

Sources which define an algebraic number over a more general field define degree in the following terms:

Let $F$ be a field.

Let $z \in \C$ be algebraic over $F$.


The degree of $\alpha$ is the degree of the minimal polynomial $\map m x$ whose coefficients are all in $F$.",Definition:Algebraic Number/Degree,['Definitions/Algebraic Numbers']
Definition:Degree,Degree,"Let $F$ be a field.

Let $z \in \C$ be algebraic over $F$.


The degree of $\alpha$ is the degree of the minimal polynomial $\map m x$ whose coefficients are all in $F$.",Definition:Algebraic Number over Field/Degree,"['Definitions/Algebraic Numbers', 'Definitions/Field Extensions']"
Definition:Degree,Degree,"Let $V$ and $W$ be two vector spaces over a field $F$.

Let $f: V \to W$ be a homogeneous function of degree $n$ from $V$ to $W$:
:$f \left({\alpha \mathbf v}\right) = \alpha^n f \left({\mathbf v}\right)$
for all nonzero $\mathbf v \in V$ and $\alpha \in F$.


The element $n \in \N$ is the degree of $f$.


Category:Definitions/Homogeneous Functions",Definition:Homogeneous Function/Degree,['Definitions/Homogeneous Functions']
Definition:Degree,Degree,"Let $f: \R^2 \to \R$ be a homogeneous function of two variables:

:$\exists n \in \Z: \forall t \in \R: \map f {t x, t y} = t^n \map f {x, y}$


The integer $n$ is known as the degree of $f$.",Definition:Homogeneous Function/Real Space/Degree,['Definitions/Homogeneous Functions']
Definition:Degree,Degree,"Let $G$ be an abelian group.

Let $\Delta$ be a set.


A gradation of type $\Delta$ on $G$ is a family of subgroups $\family {G_\lambda}_{\lambda \mathop \in \Delta}$ of which $G$ is the internal direct sum.",Definition:Gradation on Abelian Group,['Definitions/Group Theory']
Definition:Degree,Degree,"Let $E / F$ be a field extension.


The degree of $E / F$, denoted $\index E F$, is the dimension of $E / F$ when $E$ is viewed as a vector space over $F$.


=== Finite ===
Let $E / F$ be a field extension.


$E / F$ is a finite field extension  if and only if  its degree $\index E F$ is finite.

=== Infinite ===
Let $E / F$ be a field extension.


$E / F$ is an infinite field extension  if and only if  its degree $\index E F$ is not finite.


Category:Definitions/Field Extensions",Definition:Field Extension/Degree,['Definitions/Field Extensions']
Definition:Degree,Degree,"Let $E / F$ be a field extension.

Let $\alpha \in E$ be algebraic over $F$.


The degree of $\alpha$ is the degree of the minimal polynomial $\map {\mu_F} \alpha$ whose coefficients are all in $F$.",Definition:Degree of Algebraic Element,['Definitions/Field Extensions']
Definition:Degree,Degree,"Let $K$ be a field, and let $L/K$ be a field extension of $K$.

The transcendence degree of $L/K$ is the largest cardinality of an algebraically independent subset $A \subseteq L$.

Category:Definitions/Field Extensions",Definition:Transcendence Degree,['Definitions/Field Extensions']
Definition:Degree,Degree,"Let $G = \struct {V, E}$ be an undirected graph.

Let $v \in V$ be a vertex of $G$.


The degree of $v$ in $G$ is the number of edges to which it is incident.

It is denoted $\map {\deg_G} v$, or just $\map \deg v$ if it is clear from the context which graph is being referred to.


That is:
:$\map {\deg_G} v = \card {\set {u \in V : \set {u, v} \in E} }$


=== Even Vertex ===
Let $G = \struct {V, E}$ be an undirected graph.

Let $v \in V$ be a vertex of $G$.


If the degree of $v$ is even, then $v$ is called an even vertex.

=== Odd Vertex ===
Let $G = \struct {V, E}$ be an undirected graph.

Let $v \in V$ be a vertex of $G$.


If the degree of $v$ is odd, then $v$ is an odd vertex.

=== Isolated Vertex ===
Let $G = \struct {V, E}$ be an undirected graph.

Let $v \in V$ be a vertex of $G$.


If the degree of $v$ is zero, then $v$ is an isolated vertex.",Definition:Degree of Vertex,"['Definitions/Degrees of Vertices', 'Definitions/Vertices of Graphs']"
Definition:Degree,Degree,"The degree (of arc) is a unit of measurement of the length of an arc of a circle.

It is defined as the length of the arc which subtends $1$ degree (of angle) at the center of the circle.",Definition:Degree of Arc,"['Definitions/Degrees of Arc', 'Definitions/Arc Length', 'Definitions/Units of Measurement']"
Definition:Degree,Degree,The degree of an algebraic curve is defined as the highest degree of the polynomial equations defining it.,Definition:Algebraic Curve/Degree,"['Definitions/Degrees of Algebraic Curves', 'Definitions/Algebraic Curves']"
Definition:Degree,Degree,"Celsius is a temperature scale.

Its two reference points are:
:$0 \cels$, which is set at the melting point of water.
:$100 \cels$, which is set at the boiling point of water, as defined at sea level and standard atmospheric pressure.


A temperature measured in Celsius is often referred to as so many degrees Celsius.


=== Conversion Factors ===


=== Symbol ===
The symbol for the degree Celsius is $\cels$.",Definition:Celsius,"['Definitions/Celsius', 'Definitions/Degrees of Temperature', 'Definitions/Temperature', 'Definitions/Units of Measurement']"
Definition:Degree,Degree,"Fahrenheit is a temperature scale.

Its two reference points are:
:$32 \fahr$, which is set at the melting point of water.
:$212 \fahr$, which is set at the boiling point of water, as defined at sea level and standard atmospheric pressure.


A temperature measured in Fahrenheit is often referred to as so many degrees Fahrenheit.


=== Conversion Factors ===


=== Symbol ===
 ",Definition:Fahrenheit,"['Definitions/Fahrenheit', 'Definitions/Temperature', 'Definitions/Units of Measurement']"
Definition:Degree,Degree,"Let $\closedint a b$ be a closed real interval.

Let $T := \set {a = t_0, t_1, t_2, \ldots, t_{n - 1}, t_n = b}$ form a subdivision of $\closedint a b$.

Let $S: \closedint a b \to \R$ be a spline function on $\closedint a b$ on $T$.


The degree of $S$ is the maximum degree of the polynomials $P_k$ fitted between $t_k$ and $t_{k + 1}$.


=== Order ===
Let $\closedint a b$ be a closed real interval.

Let $T := \set {a = t_0, t_1, t_2, \ldots, t_{n - 1}, t_n = b}$ form a subdivision of $\closedint a b$.

Let $S: \closedint a b \to \R$ be a spline function on $\closedint a b$ on $T$.


Some sources, instead of referring to the degree of a spline, use the order.

Let the maximum degree of the polynomials $P_k$ fitted between $t_k$ and $t_{k + 1}$ be $n$.

The order of $S$ is then $n + 1$.


Category:Definitions/Splines

Category:Definitions/Splines",Definition:Spline Function/Degree,['Definitions/Splines']
Definition:Degrees of Freedom,Degrees of Freedom,"The number of degrees of freedom is essentially the number of independent units of information in a sample relevant to the estimation of a parameter or calculation of a statistic.

One approach is to regard the $n$ observations as the initial data, one of which is used to determine the total or mean.

As the mean must be known before we can determine deviations from it, there are $n - 1$ degrees of freedom left to estimate the variance.

Hence, in the sense that the total is fixed, only $n - 1$ values can be assigned arbitrarily, as the remaining one is then fixed to ensure the correct total.",Definition:Degrees of Freedom (Statistics),"['Definitions/Degrees of Freedom (Statistics)', 'Definitions/Statistics']"
Definition:Degrees of Freedom,Degrees of Freedom,"Consider an oscillating system $S$ disturbed from equilibrium.

=== Definition 1 ===
Consider an oscillating system $S$ disturbed from equilibrium.


The number of degrees of freedom of $S$ is the number of normal modes of oscillation of $S$.

=== Definition 2 ===
Consider an oscillating system $S$ disturbed from equilibrium.


The number of degrees of freedom of $S$ is the number of independent variables needed to specify the configuration of $S$ at any time.",Definition:Degrees of Freedom (Physics),"['Definitions/Degrees of Freedom (Physics)', 'Definitions/Physics']"
Definition:Deltoid,Deltoid,"A deltoid is a dart such that:
:the $2$ sides which are adjacent to the reflex angle are equal to each other
:the other $2$ sides are also equal to each other.


:

In the above diagram, the figure on the right is a deltoid.",Definition:Quadrilateral/Dart/Deltoid,['Definitions/Quadrilaterals']
Definition:Deltoid,Deltoid,"A deltoid is a hypocycloid with $3$ cusps.


:",Definition:Deltoid (Hypocycloid),['Definitions/Hypocycloids']
Definition:Dense,Dense,"Let $T = \struct {S, \tau}$ be a topological space.

Let $H \subseteq S$ be a subset.


=== Definition 1 ===
Let $T = \struct {S, \tau}$ be a topological space.

Let $H \subseteq S$ be a subset.


The subset $H$ is (everywhere) dense in $T$  if and only if :
:$H^- = S$
where $H^-$ is the closure of $H$.


That is,  if and only if  every point in $S$ is a point or a limit point of $H$.

=== Definition 2 ===
Let $T = \struct {S, \tau}$ be a topological space.

Let $H \subseteq S$ be a subset.


The subset $H$ is (everywhere) dense in $T$  if and only if  the intersection of $H$ with every non-empty open set of $T$ is non-empty:
:$\forall U \in \tau \setminus \set \O: H \cap U \ne \O$

=== Definition 3 ===
Let $T = \struct {S, \tau}$ be a topological space.

Let $H \subseteq S$ be a subset.


The subset $H$ is (everywhere) dense in $T$  if and only if  every neighborhood of every point of $S$ contains at least one point of $H$.",Definition:Everywhere Dense,"['Definitions/Everywhere Dense', 'Definitions/Denseness', 'Definitions/Topology']"
Definition:Dense,Dense,"Let $T = \struct {S, \tau}$ be a topological space.

Let $H \subseteq S$.


Then $H$ is dense-in-itself  if and only if  it contains no isolated points.",Definition:Dense-in-itself,"['Definitions/Topology', 'Definitions/Denseness']"
Definition:Dense,Dense,"Let $T = \struct {S, \tau}$ be a topological space.

Let $H \subseteq S$.


=== Definition 1 ===
Let $T = \struct {S, \tau}$ be a topological space.

Let $H \subseteq S$.


$H$ is nowhere dense in $T$  if and only if :
:$\paren {H^-}^\circ = \O$
where $H^-$ denotes the closure of $H$ and $H^\circ$ denotes its interior.


That is, $H$ is nowhere dense in $T$  if and only if  the interior of its closure is empty.

Another way of putting it is that $H$ is nowhere dense in $T$  if and only if  it consists entirely of boundary.

=== Definition 2 ===
Let $T = \struct {S, \tau}$ be a topological space.

Let $H \subseteq S$.


$H$ is nowhere dense in $T$  if and only if :
:$H^-$ contains no open set of $T$ which is non-empty
where $H^-$ denotes the closure of $H$.",Definition:Nowhere Dense,"['Definitions/Nowhere Dense', 'Definitions/Denseness', 'Definitions/Topology']"
Definition:Dense,Dense,"Let $\struct {S, \preceq}$ be an ordered set.


Then $\struct {S, \preceq}$ is defined as densely ordered  if and only if  strictly between every two elements of $S$ there exists another element of $S$:

:$\forall a, b \in S: a \prec b \implies \exists c \in S: a \prec c \prec b$
where $a \prec b$ denotes that $a \preceq b$ but $a \ne b$.


=== Densely Ordered Subset ===
Let $\struct {S, \preceq}$ be an ordered set.


A subset $T \subseteq S$ is said to be densely ordered in $\struct {S, \preceq}$  if and only if :
:$\forall a, b \in S: a \prec b \implies \exists c \in T: a \prec c \prec b$",Definition:Densely Ordered,"['Definitions/Order Theory', 'Definitions/Densely Ordered']"
Definition:Dense,Dense,"Let $L = \struct {S, \wedge, \preceq}$ be a bounded below meet semilattice.

=== Dense Element ===
Let $L = \struct {S, \wedge, \preceq}$ be a bounded below meet semilattice.

Let $x \in S$.


Then $x$ is dense  if and only if 
:$\forall y \in S: y \ne \bot \implies x \wedge y \ne \bot$

where $\bot$ denotes the smallest element in $L$.

=== Dense Subset ===
Let $L = \struct {S, \wedge, \preceq}$ be a bounded below meet semilattice.

Let $A$ be a subset of $S$.


Then $A$ is dense  if and only if  it includes only dense elements.

That means that  if and only if  $\forall x \in A: x$ is a dense element.

Category:Definitions/Lattice Theory",Definition:Dense (Lattice Theory),['Definitions/Lattice Theory']
Definition:Density,Density,"Density is a physical quantity that is variously specified according to context.

Its general meaning is that of some physical quantity per unit volume, or area, or length.


=== Mass Density ===
Mass density is a physical quantity.

The mass density of a body is its mass per unit volume.


For a homogeneous body it is found by finding its total mass and dividing it by its total volume:
:$\rho = \dfrac m V$
where:
:$m$ is the body's mass
:$V$ is the body's volume

However, if the substance of the body varies throughout, then its mass density may be a function of position within the body.


=== Symbol ===


=== Dimension ===
The dimension of (mass) density is $\mathsf {M L}^{-3}$: mass per unit volume.


Category:Definitions/Mass Density
Category:Definitions/Dimensions of Measurement

=== Units ===
* The SI units of mass density are $\mathrm {kg} \, \mathrm m^{-3}$ (kilograms per cubic metre).

* The CGS units of mass density are $\mathrm g \, \mathrm{cm}^{-3}$ or, less formally: $\mathrm g / \mathrm {cc}$ (grams per cubic centimetre).

* The FPS units of mass density are $\mathrm {lb} \, \mathrm {ft}^{-3}$ (pounds per cubic foot).


=== Conversion Factors ===


=== Area Mass Density ===
The area mass density of a two-dimensional body is its mass per unit area.


=== Symbol ===
The usual symbol used to denote area mass density is $\rho_A$ (Greek letter rho).

However, some sources simply use $\rho$ if the context makes it clear that it refers to area mass density rather than volume mass density.

Occasionally, $\sigma$ (Greek letter sigma) is also used, but this is more commonly used for surface charge density.


Category:Definitions/Area Mass Density

=== Dimension ===
The dimension of area mass density is $\mathsf M \mathsf L^{-2}$: mass per unit area.


Category:Definitions/Area Mass Density
Category:Definitions/Dimensions of Measurement

=== Units ===
* The SI units of area mass density are $\mathrm {kg} \ \mathrm m^{-2}$ (kilograms per square metre).

* The CGS units of area mass density are $\mathrm g \ \mathrm{cm}^{-2}$ (grams per square centimetre).

Thus:
:$1 \ \mathrm g \ \mathrm{cm}^{-2} = 10 \ \mathrm{kg} \ \mathrm m^{-2}$


Category:Definitions/Area Mass Density
Category:Definitions/Units of Measurement

=== Linear Mass Density ===
The linear mass density of a one-dimensional body is its mass per unit length.


=== Symbol ===
The usual symbol used to denote linear mass density is $\mu$ (Greek letter mu).

Sometimes $\lambda$ (Greek letter lambda) is also used.


Category:Definitions/Linear Mass Density

=== Dimension ===
The dimension of linear mass density is $\mathsf M \mathsf L^{-1}$: mass per unit length.


Category:Definitions/Linear Mass Density
Category:Definitions/Dimensions of Measurement

=== Units ===
* The SI units of linear mass density are $\mathrm {kg} \ \mathrm m^{-1}$ (kilograms per metre).

* The CGS units of linear mass density are $\mathrm g \ \mathrm{cm}^{-1}$ (grams per centimetre).

Thus:
:$1 \ \mathrm{kg} \ \mathrm m^{-1} = 10 \ \mathrm g \ \mathrm{cm}^{-1}$


Category:Definitions/Linear Mass Density
Category:Definitions/Units of Measurement

=== Electric Charge Density ===
Let $A$ be a point in space in which an electric field acts.

Let $\delta V$ be a volume element containing $A$.


The (electric) charge density $\map \rho {\mathbf r}$ at $A$ is defined as:
 
 
 
 

where:
:$Q$ denotes the electric charge within $\delta V$
:$\mathbf r$ denotes the position vector of $A$.


Thus the electric charge density is the quantity of electric charge per unit volume, at any given point in that volume:


=== Symbol ===


=== Dimension ===
The dimension of electric charge density is $\mathsf {I T L}^{-3}$: electric charge per unit volume.

=== Units ===
The SI units of electric charge density are $\mathrm C \, \mathrm m^{-3}$ (coulombs per cubic metre).


Category:Definitions/Electric Charge Density
Category:Definitions/Units of Measurement",Definition:Density (Physics),"['Definitions/Density (Physics)', 'Definitions/Physics', 'Definitions/Physical Quantities', 'Definitions/Examples of Scalar Quantities']"
Definition:Density,Density,"Mass density is a physical quantity.

The mass density of a body is its mass per unit volume.


For a homogeneous body it is found by finding its total mass and dividing it by its total volume:
:$\rho = \dfrac m V$
where:
:$m$ is the body's mass
:$V$ is the body's volume

However, if the substance of the body varies throughout, then its mass density may be a function of position within the body.


=== Symbol ===


=== Dimension ===
The dimension of (mass) density is $\mathsf {M L}^{-3}$: mass per unit volume.


Category:Definitions/Mass Density
Category:Definitions/Dimensions of Measurement

=== Units ===
* The SI units of mass density are $\mathrm {kg} \, \mathrm m^{-3}$ (kilograms per cubic metre).

* The CGS units of mass density are $\mathrm g \, \mathrm{cm}^{-3}$ or, less formally: $\mathrm g / \mathrm {cc}$ (grams per cubic centimetre).

* The FPS units of mass density are $\mathrm {lb} \, \mathrm {ft}^{-3}$ (pounds per cubic foot).


=== Conversion Factors ===
",Definition:Mass Density,"['Definitions/Mass Density', 'Definitions/Density (Physics)', 'Definitions/Physics', 'Definitions/Physical Quantities', 'Definitions/Examples of Scalar Quantities']"
Definition:Density,Density,"The area mass density of a two-dimensional body is its mass per unit area.


=== Symbol ===
The usual symbol used to denote area mass density is $\rho_A$ (Greek letter rho).

However, some sources simply use $\rho$ if the context makes it clear that it refers to area mass density rather than volume mass density.

Occasionally, $\sigma$ (Greek letter sigma) is also used, but this is more commonly used for surface charge density.


Category:Definitions/Area Mass Density

=== Dimension ===
The dimension of area mass density is $\mathsf M \mathsf L^{-2}$: mass per unit area.


Category:Definitions/Area Mass Density
Category:Definitions/Dimensions of Measurement

=== Units ===
* The SI units of area mass density are $\mathrm {kg} \ \mathrm m^{-2}$ (kilograms per square metre).

* The CGS units of area mass density are $\mathrm g \ \mathrm{cm}^{-2}$ (grams per square centimetre).

Thus:
:$1 \ \mathrm g \ \mathrm{cm}^{-2} = 10 \ \mathrm{kg} \ \mathrm m^{-2}$


Category:Definitions/Area Mass Density
Category:Definitions/Units of Measurement",Definition:Mass Density/Area,"['Definitions/Area Mass Density', 'Definitions/Mass Density', 'Definitions/Physical Quantities']"
Definition:Density,Density,"The linear mass density of a one-dimensional body is its mass per unit length.


=== Symbol ===
The usual symbol used to denote linear mass density is $\mu$ (Greek letter mu).

Sometimes $\lambda$ (Greek letter lambda) is also used.


Category:Definitions/Linear Mass Density

=== Dimension ===
The dimension of linear mass density is $\mathsf M \mathsf L^{-1}$: mass per unit length.


Category:Definitions/Linear Mass Density
Category:Definitions/Dimensions of Measurement

=== Units ===
* The SI units of linear mass density are $\mathrm {kg} \ \mathrm m^{-1}$ (kilograms per metre).

* The CGS units of linear mass density are $\mathrm g \ \mathrm{cm}^{-1}$ (grams per centimetre).

Thus:
:$1 \ \mathrm{kg} \ \mathrm m^{-1} = 10 \ \mathrm g \ \mathrm{cm}^{-1}$


Category:Definitions/Linear Mass Density
Category:Definitions/Units of Measurement",Definition:Mass Density/Linear,"['Definitions/Linear Mass Density', 'Definitions/Mass Density', 'Definitions/Physical Quantities']"
Definition:Density,Density,"Let $A$ be a point in space in which an electric field acts.

Let $\delta V$ be a volume element containing $A$.


The (electric) charge density $\map \rho {\mathbf r}$ at $A$ is defined as:
 
 
 
 

where:
:$Q$ denotes the electric charge within $\delta V$
:$\mathbf r$ denotes the position vector of $A$.


Thus the electric charge density is the quantity of electric charge per unit volume, at any given point in that volume:


=== Symbol ===


=== Dimension ===
The dimension of electric charge density is $\mathsf {I T L}^{-3}$: electric charge per unit volume.

=== Units ===
The SI units of electric charge density are $\mathrm C \, \mathrm m^{-3}$ (coulombs per cubic metre).


Category:Definitions/Electric Charge Density
Category:Definitions/Units of Measurement",Definition:Electric Charge Density,"['Definitions/Electric Charge Density', 'Definitions/Electric Charge', 'Definitions/Electrostatics', 'Definitions/Examples of Scalar Quantities']"
Definition:Density,Density,"Let $\PP$ be a set of prime numbers.

For $s \in \C$, let $\ds \map f s = \sum_{p \mathop \in \PP}: p^{-s}$.


$S$ has Dirichlet density $\alpha$  if and only if :

:$\ds \lim_{s \mathop \to 1^+} \set {\frac {\map f s} {\map \ln {s - 1} } } = -\alpha$

where $1^+$ indicates a limit from above along the real line.

 ",Definition:Dirichlet Density,['Definitions/Analytic Number Theory']
Definition:Diagonal,Diagonal,A diagonal of a polyhedron $P$ is a straight line connecting $2$ vertices of $P$ which are not adjacent to the same face.,Definition:Diagonal of Polyhedron,"['Definitions/Diagonals of Polyhedra', 'Definitions/Polyhedra']"
Definition:Diagonal,Diagonal,"Let $ABCD$ be a parallelogram:

:

The diameters of $ABCD$ are the lines $AC$ and $BD$ joining their opposite vertices.",Definition:Diameter of Parallelogram,['Definitions/Parallelograms']
Definition:Diagonal,Diagonal,"Let $S$ be a set.

The diagonal relation on $S$ is the relation $\Delta_S$ on $S$ defined as:

:$\Delta_S = \set {\tuple {x, x}: x \in S} \subseteq S \times S$

Alternatively:

:$\Delta_S = \set {\tuple {x, y}: x, y \in S: x = y}$


=== Class Theory ===
 
Let $V$ be a basic universe.

The diagonal relation on $V$ is the relation $\Delta_V$ on $V$ defined as:

:$\Delta_V = \set {\tuple {x, x}: x \in V}$

Alternatively:

:$\Delta_V = \set {\tuple {x, y}: x, y \in V: x = y}$",Definition:Diagonal Relation,"['Definitions/Diagonal Relation', 'Definitions/Examples of Equivalence Relations', 'Definitions/Examples of Relations']"
Definition:Diagonal,Diagonal,"Let $S$ be a set.

Let $S \times S$ be the Cartesian product of $S$ with itself.


Then the diagonal mapping on $S$ is defined as $\Delta: S \to S \times S$:
:$\forall x \in S: \Delta \left({x}\right) = \left({x, x}\right)$


Clearly $\Delta$ is an injection, and is not a surjection unless $S$ is a singleton.

",Definition:Diagonal Mapping,['Definitions/Mapping Theory']
Definition:Diagonal,Diagonal,The elements of the main diagonal of a matrix or a determinant are called the diagonal elements.,Definition:Main Diagonal/Diagonal Elements,"['Definitions/Diagonal Elements', 'Definitions/Main Diagonal', 'Definitions/Matrices']"
Definition:Diagonal,Diagonal,"Let $\mathbf A = \begin{bmatrix}
a_{11} & a_{12} & \cdots & a_{1n} \\
a_{21} & a_{22} & \cdots & a_{2n} \\
\vdots & \vdots & \ddots & \vdots \\
a_{n1} & a_{n2} & \cdots & a_{nn} \\
\end{bmatrix}$ be a square matrix of order $n$.

Then $\mathbf A$ is a diagonal matrix  if and only if  all elements of $\mathbf A$ are zero except for possibly its diagonal elements.


Thus $\mathbf A = \begin{bmatrix}
a_{11} & 0 & \cdots & 0 \\
0 & a_{22} & \cdots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \cdots & a_{nn} \\
\end{bmatrix}$.


It follows by the definition of triangular matrix that a diagonal matrix is both an upper triangular matrix and a lower triangular matrix.",Definition:Diagonal Matrix,"['Definitions/Diagonal Matrices', 'Definitions/Square Matrices', 'Definitions/Matrices']"
Definition:Diagonal,Diagonal,"A diagonalizable matrix $\mathbf A$ is a square matrix which is similar to a diagonal matrix.

That is, $\mathbf A$ is diagonalizable  if and only if  there exists an invertible matrix $\mathbf X$ such that $\mathbf X^-1 \mathbf A \mathbf X$ is a diagonal matrix.",Definition:Diagonalizable Matrix,"['Definitions/Diagonalizable Matrices', 'Definitions/Matrices']"
Definition:Diagram,Diagram,"A diagram is a graphical technique for illustrating a concept in picture form.

It is generally considered that its use should be limited to that of an aid to understanding, and should not be used in order to prove something.",Definition:Diagram (Graphical Technique),"['Definitions/Diagrams', 'Definitions/Proof Techniques']"
Definition:Diagram,Diagram,"Let $\mathbf J$ and $\mathbf C$ be metacategories.


A diagram of type $\mathbf J$ in $\mathbf C$ is a functor $D: \mathbf J \to \mathbf C$.


=== Index Category ===

In this context, $\mathbf J$ is referred to as the index category.

Its objects are typically denoted by lowercase letters, $i, j$ etc.


Furthermore, one writes $D_i$ in place of the formally more correct $D \left({i}\right)$.

Similarly, for $\alpha: i \to j$ a morphism one writes $D_\alpha$ in place of $D \left({\alpha}\right)$.",Definition:Diagram (Category Theory),['Definitions/Category Theory']
Definition:Diameter,Diameter,"Let $\KK$ be a conic section.

A diameter of $\KK$ is the locus of the midpoints of a system of parallel chords of $\KK$.


=== Ellipse ===
Let $\KK$ be an ellipse.

A diameter of $\KK$ is the locus of the midpoints of a system of parallel chords of $\KK$.


:

=== Hyperbola ===
Let $\KK$ be a hyperbola.

A diameter of $\KK$ is the locus of the midpoints of a system of parallel chords of $\KK$.


:

=== Parabola ===
Let $\KK$ be a parabola.

A diameter of $\KK$ is the locus of the midpoints of a system of parallel chords of $\KK$.


:",Definition:Diameter of Conic Section,"['Definitions/Conic Sections', 'Definitions/Diameters of Conic Sections']"
Definition:Diameter,Diameter,":

 
: 
:A diameter of the circle is any straight line drawn through the center and terminated in both directions by the circumference of the circle, and such a straight line also bisects the center.
 ''
 

In the above diagram, the line $CD$ is a diameter.",Definition:Circle/Diameter,"['Definitions/Circles', 'Definitions/Diameters of Conic Sections']"
Definition:Diameter,Diameter,"Let $\KK$ be an ellipse.

A diameter of $\KK$ is the locus of the midpoints of a system of parallel chords of $\KK$.


:",Definition:Diameter of Ellipse,"['Definitions/Ellipses', 'Definitions/Diameters of Conic Sections']"
Definition:Diameter,Diameter,"Let $\KK$ be a hyperbola.

A diameter of $\KK$ is the locus of the midpoints of a system of parallel chords of $\KK$.


:",Definition:Diameter of Hyperbola,"['Definitions/Hyperbolas', 'Definitions/Diameters of Conic Sections']"
Definition:Diameter,Diameter,"Let $\KK$ be a parabola.

A diameter of $\KK$ is the locus of the midpoints of a system of parallel chords of $\KK$.


:",Definition:Diameter of Parabola,"['Definitions/Diameters of Conic Sections', 'Definitions/Parabolas']"
Definition:Diameter,Diameter,"By the definition of a sphere, there is one point inside it such that the distance between that point and any given point on the surface of the sphere are equal, and that point is called the center of the sphere.


The diameter of a sphere is the length of any straight line drawn from a point on the surface to another point on the surface through the center.


 
: 
:A diameter of the sphere is any straight line drawn through the centre and terminated in both directions by the surface of the sphere.
 ''
 ",Definition:Sphere/Geometry/Diameter,['Definitions/Spheres']
Definition:Diameter,Diameter,"Let $ABCD$ be a parallelogram:

:

The diameters of $ABCD$ are the lines $AC$ and $BD$ joining their opposite vertices.",Definition:Diameter of Parallelogram,['Definitions/Parallelograms']
Definition:Diameter,Diameter,The diameter of a geometric figure is the greatest length that can be formed between two opposite parallel straight lines that can be drawn tangent to its boundary.,Definition:Geometric Figure/Diameter,['Definitions/Geometric Figures']
Definition:Diameter,Diameter,"Let $M = \struct {A, d}$ be a metric space.

Let $S \subseteq A$ be subset of $A$.


Then the diameter of $S$ is the extended real number defined by:

:$\map \diam S := \begin {cases} \sup \set {\map d {x, y}: x, y \in S} & : \text {if this quantity is finite} \\ + \infty & : \text {otherwise} \end {cases}$


Thus, by the definition of the supremum, the diameter is the smallest real number $D$ such that any two points of $S$ are at most a distance $D$ apart.

If $d: S^2 \to \R$ does not admit a supremum, then $\map \diam S$ is infinite.",Definition:Diameter of Subset of Metric Space,['Definitions/Metric Spaces']
Definition:Difference,Difference,"The (set) difference between two sets $S$ and $T$ is written $S \setminus T$, and means the set that consists of the elements of $S$ which are not elements of $T$:
:$x \in S \setminus T \iff x \in S \land x \notin T$


It can also be defined as:
:$S \setminus T = \set {x \in S: x \notin T}$
:$S \setminus T = \set {x: x \in S \land x \notin T}$",Definition:Set Difference,"['Definitions/Set Theory', 'Definitions/Set Difference']"
Definition:Difference,Difference,"Let $a$ and $b$ be real numbers.

The absolute difference between $a$ and $b$ is defined and denoted as:
:$\size {a - b}$
where $\size {\, \cdot \,}$ is the absolute value function.",Definition:Absolute Difference,['Definitions/Subtraction']
Definition:Differentiable,Differentiable,"Let $M$ be a second-countable locally Euclidean space of dimension $d$. 

Let $\mathscr F$ be a $d$-dimensional differentiable structure on $M$ of class $\CC^k$, where $k \ge 1$.


Then $\struct {M, \mathscr F}$ is a differentiable manifold of class $\CC^k$ and dimension $d$.",Definition:Topological Manifold/Differentiable Manifold,"['Definitions/Topological Manifolds', 'Definitions/Differentiable Manifolds']"
Definition:Differentiable,Differentiable,"=== Real Function ===
=== At a Point ===

Let $f$ be a real function defined on an open interval $\openint a b$.

Let $\xi$ be a point in $\openint a b$.


==== Definition 1 ====
Let $f$ be a real function defined on an open interval $\openint a b$.

Let $\xi$ be a point in $\openint a b$.


$f$ is differentiable at the point $\xi$  if and only if  the limit:
:$\ds \lim_{x \mathop \to \xi} \frac {\map f x - \map f \xi} {x - \xi}$
exists.

==== Definition 2 ====
Let $f$ be a real function defined on an open interval $\openint a b$.

Let $\xi$ be a point in $\openint a b$.


$f$ is differentiable at the point $\xi$  if and only if  the limit:
:$\ds \lim_{h \mathop \to 0} \frac {\map f {\xi + h} - \map f \xi} h$
exists.


Category:Definitions/Differentiable Real Functions

These limits, if they exist, are called the derivative of $f$ at $\xi$.

=== On an Open Interval ===
Let $f$ be a real function defined on an open interval $\openint a b$.


Then $f$ is differentiable on $\openint a b$  if and only if  $f$ is differentiable at each point of $\openint a b$.


=== On a Closed Interval ===
Let $f$ be a real function defined on a closed interval $\closedint a b$.

Let $f$ be differentiable on the open interval $\openint a b$.


If the following limit from the right exists:

:$\ds \lim_{x \mathop \to a^+} \frac {\map f x - \map f a} {x - a}$

as well as this limit from the left:
:$\ds \lim_{x \mathop \to b^-} \frac {\map f x - \map f b} {x - b}$

then $f$ is differentiable on the closed interval $\closedint a b$.


Similar definitions for differentiability on a half-open interval can be expressed for a real function which has either a limit from the right at $a$ or a limit from the left at $b$, but not both.

=== On the Real Number Line ===
Let $f$ be a real function defined on $\R$.

By definition, $\R$ is an (unbounded) open interval.


Let $f$ be differentiable on the open interval $\R$.

That is, let $f$ be differentiable at every point of $\R$.


Then $f$ is differentiable everywhere (on $\R$).


Category:Definitions/Differentiable Real Functions

=== Complex Function ===
=== At a Point ===

Let $U \subset \C$ be an open set.

Let $f : U \to \C$ be a complex function.

Let $z_0 \in U$ be a point in $U$.


Then $f$ is complex-differentiable at $z_0$  if and only if  the limit:

:$\ds \lim_{h \mathop \to 0} \frac {\map f {z_0+h} - \map f {z_0}} h$

exists as a finite number.


This limit, if it exists, is called the derivative of $f$ at $z_0$.

=== In an Open Set ===
Let $U \subseteq \C$ be an open set.

Let $f : U \to \C$ be a complex function.


Then $f$ is holomorphic in $U$  if and only if  $f$ is differentiable at each point of $U$.

We also say that $f$ is complex-differentiable in $U$.


 

Category:Definitions/Complex Differential Calculus

=== Real-Valued Function ===
=== At a Point ===

Let $U$ be an open subset of $\R^n$. 

Let $\norm \cdot $ denote the Euclidean norm on $\R^n$.

Let $f: U \to \R$ be a real-valued function.

Let $x \in U$.

==== Definition 1 ====
Let $U$ be an open subset of $\R^n$.

Let $\norm \cdot $ denote the Euclidean norm on $\R^n$.

Let $f: U \to \R$ be a real-valued function.

Let $x \in U$.



$f$ is differentiable at $x$  if and only if  there exist $\alpha_1, \ldots, \alpha_n \in \R$ and a real-valued function $r: U \setminus \set x \to \R$ such that:

:$(1):\quad \map f {x + h} = \map f x + \alpha_1 h_1 + \cdots + \alpha_n h_n + \map r h \norm h$

:$(2):\quad \ds \lim_{h \mathop \to 0} \map r h = 0$


Category:Definitions/Differentiable Real-Valued Functions

==== Definition 2 ====
Let $U$ be an open subset of $\R^n$. 

Let $f: U \to \R$ be a real-valued function.

Let $\norm \cdot $ denote the Euclidean norm on $\R^n$.

Let $x \in U$.



$f$ is differentiable at $x$  if and only if  there exists a linear transformation $T: \R^n \to \R$ and a real-valued function $r: U \setminus \set x \to \R$ such that:

:$(1): \quad \map f {x + h} = \map f x + \map T h + \map r h \norm h$
:$(2): \quad \ds \lim_{h \mathop \to 0} \map r h = 0$


Category:Definitions/Differentiable Real-Valued Functions

=== In an Open Set ===
Let $\mathbb X$ be an open subset of $\R^n$. 

Let $f: \mathbb X \to \R$ be a real-valued function.


Then $f$ is differentiable in the open set $\mathbb X$  if and only if  $f$ is differentiable at each point of $\mathbb X$.

=== Vector-Valued Function ===
Let $m, n \ge 1$ be natural numbers.

=== At a Point ===

Let $\mathbb X$ be an open subset of $\R^n$. 

Let $f = \tuple {f_1, f_2, \ldots, f_m}^\intercal: \mathbb X \to \R^m$ be a vector valued function.


$f$ is differentiable at $x \in \R^n$  if and only if  there exists a linear transformation $T: \R^n \to \R^m$ and a mapping $r : U \to \R^m$ such that:

:$(1): \quad \map f {x + h} = \map f x + \map T h + \map r h \cdot \norm h$

:$(2): \quad \ds \lim_{h \mathop \to 0} \map r h = 0$


Category:Definitions/Differentiable Vector-Valued Functions

=== In an Open Set ===
Let $\mathbb X$ be an open subset of $\R^n$. 

Let $f = \tuple {f_1, f_2, \ldots, f_m}^\intercal: \mathbb X \to \R^m$ be a vector valued function.

Let $S \subseteq \mathbb X$.


Then $f$ is differentiable in the open set $S$  if and only if  $f$ is differentiable at each $x$ in $S$.


This can be denoted $f \in \map {\CC^1} {S, \R^m}$.

=== Function With Values in Normed Space ===
Let $U \subset \R$ be an open set.

Let $\struct {X, \norm {\, \cdot \,}_X}$ be a normed vector space.

A function $f : U \to X$ is (strongly) differentiable at $x \in U$  if and only if  there exists $\map {f'} x \in X$ such that:

:$\ds \lim_{h \mathop \to 0} \norm {\frac {\map f {x + h} - \map f x} h - \map {f'} x}_X = 0$

Moreover, $f$ is called (strongly) differentiable if it is differentiable at every point of $U$.

=== Between Differentiable Manifolds ===
Let $M$ and $N$ be differentiable manifolds.

Let $f : M \to N$ be continuous.

=== Definition 1 ===

$f$  is differentiable  if and only if  for every pair of charts $(U, \phi)$ and $(V,\psi)$ of $M$ and $N$:
:$\psi\circ f\circ \phi^{-1} : \phi ( U \cap f^{-1}(V)) \to \psi(V)$
is differentiable.


=== Definition 2 ===


$f$  is differentiable  if and only if  $f$ is  differentiable at every point of $M$.


=== At a Point ===
Let $M$ and $N$ be differentiable manifolds.

Let $f: M \to N$ be continuous.

Let $p \in M$.


==== Definition 1 ====
$f$ is differentiable at $p$  if and only if  for every pair of charts $\struct {U, \phi}$ and $\struct {V, \psi}$ of $M$ and $N$ with $p \in U$ and $\map f p \in V$:
:$\psi \circ f \circ \phi^{-1}: \map \phi {U \cap \map {f^{-1} } V} \to \map \psi V$
is differentiable at $\map \phi p$.


Category:Definitions/Differentiable Manifolds

==== Definition 2 ====
$f$ is differentiable at $p$  if and only if  there exists a pair of charts $\struct {U, \phi}$ and $\struct {V, \psi}$ of $M$ and $N$ with $p \in U$ and $\map f p \in V$ such that:
:$\psi \circ f \circ \phi^{-1}: \map \phi {U \cap \map {f^{-1} } V} \to \map \psi V$
is differentiable at $\map \phi p$.


Category:Definitions/Differentiable Manifolds",Definition:Differentiable Mapping,['Definitions/Differential Calculus']
Definition:Differentiable,Differentiable,"Let $S$ be a normed linear space of mappings.

Let $y, h \in S: \R \to \R$ be real functions.

Let $J \sqbrk y$, $\phi \sqbrk {y; h}$ be functionals.

Let $\Delta J \sqbrk {y; h}$ be an increment of the functional $J$ such that:

:$\Delta J \sqbrk {y; h} = \phi \sqbrk {y;h} + \epsilon \norm h$

where $\epsilon = \epsilon \sqbrk {y; h}$ is a functional, and $\norm h$ is the norm of $S$.

Suppose $\phi \sqbrk {y; h}$ is a linear   $h$ and:

:$\ds \lim_{\norm h \mathop \to 0} \epsilon = 0$


Then the functional $J \sqbrk y $ is said to be differentiable.",Definition:Differentiable Functional,['Definitions/Calculus of Variations']
Definition:Differentiable,Differentiable,"A differentiable function $f$ is continuously differentiable  if and only if  $f$ is of differentiability class $C^1$.

That is, if the first order derivative of $f$ (and possibly higher) is continuous.


=== Real Function ===
Let $I\subset\R$ be an open interval.


Then $f$ is continuously differentiable on $I$  if and only if  $f$ is differentiable on $I$ and its derivative is continuous on $I$.


=== Real-Valued Function ===
Let $U$ be an open subset of $\R^n$. 

Let $f: U \to \R$ be a real-valued function.


Then $f$ is continuously differentiable in the open set $U$  if and only if :
:$(1): \quad f$ is differentiable in $U$.
:$(2): \quad$ the partial derivatives of $f$ are continuous in $U$.


This can be denoted:
:$f \in \map {\CC^1} {\mathbb X, \R}$


=== Vector-Valued Function ===
Let $U \subset \R^n$ be an open set.

Let $f: U \to \R^m$ be a vector-valued function.


Then $f$ is continuously differentiable in $U$  if and only if  $f$ is differentiable in $U$ and its partial derivatives are continuous in $U$.


Category:Definitions/Differentiable Vector-Valued Functions",Definition:Continuously Differentiable,"['Definitions/Differential Calculus', 'Definitions/Differentiability Classes']"
Definition:Dihedral,Dihedral,"A dihedral is a configuration in solid geometry formed by two half-planes meeting at a common straight line.


=== Dihedral Angle ===
A dihedral angle is the angle contained by two straight lines drawn perpendicular to the common section at the same point, one in each of the two half-planes forming a dihedral.",Definition:Dihedral (Geometry),"['Definitions/Dihedrals (Geometry)', 'Definitions/Solid Geometry']"
Definition:Dihedral,Dihedral,"A dihedral angle is the angle contained by two straight lines drawn perpendicular to the common section at the same point, one in each of the two half-planes forming a dihedral.",Definition:Dihedral (Geometry)/Angle,"['Definitions/Dihedral Angles', 'Definitions/Dihedrals (Geometry)']"
Definition:Dihedral,Dihedral,"The dihedral group $D_n$ of order $2 n$ is the group of symmetries of the regular $n$-gon.


=== Even Polygon ===
 

:

=== Odd Polygon ===
 

:",Definition:Dihedral Group,"['Definitions/Dihedral Groups', 'Definitions/Examples of Groups', 'Definitions/Examples of Symmetry Groups']"
Definition:Dimension,Dimension,The dimension of a (geometrical) space is the minimum number of coordinates needed to specify a point in it.,Definition:Dimension (Geometry),"['Definitions/Dimensions (Geometry)', 'Definitions/Geometry']"
Definition:Dimension,Dimension,"=== Module ===
Let $R$ be a ring with unity.

Let $G$ be a unitary $R$-module which has a basis of $n$ elements.

Then $G$ is said to have a dimension of $n$ or to be $n$-dimensional.


=== Symbol ===


=== Finite Dimensional Module ===
Let $G$ be a (unitary) module which is $n$-dimensional for some $n \in \N_{>0}$.

Then $G$ is finite dimensional.

=== Vector Space ===

Let $K$ be a division ring.

Let $V$ be a vector space over $K$.

Let $K$ be a division ring.

Let $V$ be a vector space over $K$.


The dimension of $V$ is the number of vectors in a basis for $V$.

 ",Definition:Dimension (Linear Algebra),"['Definitions/Dimension (Linear Algebra)', 'Definitions/Module Theory', 'Definitions/Linear Algebra']"
Definition:Dimension,Dimension,"Let $\sqbrk a_{m n}$ be an $m \times n$ matrix.

Then the parameters $m$ and $n$ are known as the order of the matrix.


=== Square Matrix ===
Let $\mathbf A$ be an $n \times n$ square matrix.

That is, let $\mathbf A$ have $n$ rows (and by definition $n$ columns).


Then the order of $\mathbf A$ is defined as being $n$.

=== Column Matrix ===
Let $\mathbf A$ be an $n \times 1$ column matrix.

Then the order of $\mathbf A$ is defined as being $n$.


Category:Definitions/Orders of Matrices
Category:Definitions/Column Matrices

=== Row Matrix ===
Let $\mathbf A$ be a $1 \times n$ row matrix.

Then the order of $\mathbf A$ is defined as being $n$.


Category:Definitions/Orders of Matrices
Category:Definitions/Row Matrices",Definition:Matrix/Order,"['Definitions/Orders of Matrices', 'Definitions/Matrices']"
Definition:Dimension,Dimension,"Let $H$ be a Hilbert space, and let $E$ be a basis of $H$.


Then the dimension $\dim H$ of $H$ is defined as $\card E$, the cardinality of $E$.",Definition:Dimension (Hilbert Space),['Definitions/Hilbert Spaces']
Definition:Dimension,Dimension,"=== Locally Euclidean Space ===
Let $M$ be a locally Euclidean space. 

Let $\struct {U, \kappa}$ be a coordinate chart such that: 
:$\kappa: U \to \map \kappa U \subseteq \R^n$
for some $n \in \N$.


Then the natural number $n$ is called the dimension of $M$.

=== Hausdorff Dimension ===
 

=== Lebesgue Covering Dimension ===
 

=== Inductive Dimension ===
 

Category:Definitions/Topology",Definition:Dimension (Topology),['Definitions/Topology']
Definition:Dimension,Dimension,"Let $M$ be a locally Euclidean space. 

Let $\struct {U, \kappa}$ be a coordinate chart such that: 
:$\kappa: U \to \map \kappa U \subseteq \R^n$
for some $n \in \N$.


Then the natural number $n$ is called the dimension of $M$.",Definition:Dimension (Topology)/Locally Euclidean Space,['Definitions/Topology']
Definition:Dimension,Dimension,"Let $\R^n$ be the $n$-dimensional Euclidean space.


Let $F \subseteq \R^n$.

The Hausdorff-Besicovitch dimension of $F$ is defined as:

 
 
 
 
where $\map {\HH^s} \cdot$ denotes the $s$-dimensional Hausdorff measure on $\R^n$.",Definition:Hausdorff-Besicovitch Dimension,"['Definitions/Hausdorff-Besicovitch Dimension', 'Definitions/Fractals']"
Definition:Dimension,Dimension,"Let $\struct {k, +, \circ}$ be a field.

Let $V$ be a vector space over $k$ of finite dimension.

Let $\GL V$ be the general linear group of $V$.

Let $\struct {G, \cdot}$ be a finite group.

Let $\rho: G \to \GL V$ be a linear representation of $G$ on $V$.


The dimension or degree of $\rho$, written $\map \deg \rho$ is the dimension of the vector space $V$.

Category:Definitions/Representation Theory",Definition:Dimension (Representation Theory),['Definitions/Representation Theory']
Definition:Dimension,Dimension,"=== Krull Dimension of a Ring ===
Let $\struct {R, +, \circ}$ be a commutative ring with unity.


The Krull dimension of $R$ is the supremum of lengths of chains of prime ideals, ordered by the subset relation:
 
 
 
 
where:
:$\map {\mathrm {ht} } {\mathfrak p}$ is the height of $\mathfrak p$
:$\Spec R$ is the prime spectrum of $R$



In particular, the Krull dimension is $\infty$ if there exist arbitrarily long chains.

=== Krull Dimension of a Topological Space ===
Let $T$ be a topological space.


Its Krull dimension $\map {\dim_{\mathrm {Krull} } } T$ is the supremum of lengths of chains of closed irreducible sets of $T$, ordered by the subset relation.

Thus, the Krull dimension is $\infty$ if there exist arbitrarily long chains.

 

Category:Definitions/Algebraic Geometry",Definition:Krull Dimension,['Definitions/Algebraic Geometry']
Definition:Dimension,Dimension,"Let $\struct {R, +, \circ}$ be a commutative ring with unity.


The Krull dimension of $R$ is the supremum of lengths of chains of prime ideals, ordered by the subset relation:
 
 
 
 
where:
:$\map {\mathrm {ht} } {\mathfrak p}$ is the height of $\mathfrak p$
:$\Spec R$ is the prime spectrum of $R$



In particular, the Krull dimension is $\infty$ if there exist arbitrarily long chains.",Definition:Krull Dimension of Ring,"['Definitions/Ring Theory', 'Definitions/Ideal Theory', 'Definitions/Commutative Algebra']"
Definition:Dimension,Dimension,"Let $T$ be a topological space.


Its Krull dimension $\map {\dim_{\mathrm {Krull} } } T$ is the supremum of lengths of chains of closed irreducible sets of $T$, ordered by the subset relation.

Thus, the Krull dimension is $\infty$ if there exist arbitrarily long chains.",Definition:Krull Dimension of Topological Space,['Definitions/Irreducible Spaces']
Definition:Dimension,Dimension,The order of a differential equation is defined as being the order of the highest order derivative that is present in the equation.,Definition:Differential Equation/Order,"['Definitions/Order of Differential Equation', 'Definitions/Differential Equations']"
Definition:Dimension,Dimension,The dimension of a configuration space $S$ is the number of degrees of freedom of the system defined by $S$.,Definition:Configuration Space/Dimension,['Definitions/Configuration Spaces']
Definition:Dimension,Dimension,"Every physical quantity has a dimension associated with it.

No attempt is made here to provide an abstract definition of this term. Instead, it will be defined by example.


=== Fundamental Dimensions ===
The SI-recommended fundamental dimensions are:

 
 
 
 
 
 
 
 
 

=== Units ===
Compare with units of measurement.

This concept of dimension is more abstract than that of units, which are standard quantities of the particular dimension in question.


=== Examples ===
 

Category:Definitions/Units of Measurement
Category:Definitions/Dimensions of Measurement",Definition:Dimension (Measurement),"['Definitions/Dimensions of Measurement', 'Definitions/Dimensional Analysis', 'Definitions/Measurement', 'Definitions/Physical Quantities', 'Definitions/Physics', 'Definitions/Applied Mathematics']"
Definition:Direct Image,Direct Image,"Let $f: S \to T$ be a mapping.


=== Definition 1 ===
The image of a mapping $f: S \to T$ is the set:

:$\Img f = \set {t \in T: \exists s \in S: \map f s = t}$

That is, it is the set of values taken by $f$.

=== Definition 2 ===
The image of a mapping $f: S \to T$ is the set:

:$\Img f = f \sqbrk S$

where $f \sqbrk S$ is the image of $S$ under $f$.


=== Class Theory ===

 
Let $V$ be a basic universe.

Let $A \subseteq V$ and $B \subseteq V$ be classes.

Let $f: A \to B$ be a class mapping.

The image of $\RR$ is defined and denoted as:
:$\Img \RR := \set {y \in V: \exists x \in V: \tuple {x, y} \in \RR}$

That is, it is the class of all $y$ such that $\tuple {x, y} \in \RR$ for at least one $x$.",Definition:Image (Relation Theory)/Mapping/Mapping,['Definitions/Images']
Definition:Direct Image,Direct Image,"Let $\RR \subseteq S \times T$ be a relation.


The image of $\RR$ is defined as:

:$\Img \RR := \RR \sqbrk S = \set {t \in T: \exists s \in S: \tuple {s, t} \in \RR}$


=== General Definition ===
Let $\ds \prod_{i \mathop = 1}^n S_i$ be the cartesian product of sets $S_1$ to $S_n$.

Let $\ds \RR \subseteq \prod_{i \mathop = 1}^n S_i$ be an $n$-ary relation on $\ds \prod_{i \mathop = 1}^n S_i$.

The image of $\RR$ is the set defined as:
:$\Img \RR := \set {s_n \in S_n: \exists \tuple {s_1, s_2, \ldots, s_{n - 1} } \in \ds \prod_{i \mathop = 1}^{n - 1} S_i: \tuple {s_1, s_2, \ldots, s_n} \in \RR}$


The concept is usually encountered when $\RR$ is an endorelation on $S$:
:$\Img \RR := \set {s_n \in S: \exists \tuple {s_1, s_2, \ldots, s_{n - 1} } \in S^{n - 1}: \tuple {s_1, s_2, \ldots, s_n} \in \RR}$

=== Class Theory ===

 
Let $V$ be a basic universe.

Let $\RR \subseteq V \times V$ be a relation in $V$.

The image of $\RR$ is defined and denoted as:
:$\Img \RR := \set {y \in V: \exists x \in V: \tuple {x, y} \in \RR}$

That is, it is the class of all $y$ such that $\tuple {x, y} \in \RR$ for at least one $x$.",Definition:Image (Relation Theory)/Relation/Relation,"['Definitions/Images', 'Definitions/Relations']"
Definition:Direct Image,Direct Image,"Let $S$ and $T$ be sets.

Let $\powerset S$ and $\powerset T$ be their power sets.


=== Relation ===
Let $S$ and $T$ be sets.

Let $\powerset S$ and $\powerset T$ be their power sets.

Let $\RR \subseteq S \times T$ be a relation on $S \times T$.


The direct image mapping of $\RR$ is the mapping $\RR^\to: \powerset S \to \powerset T$ that sends a subset $X \subseteq T$ to its image under $\RR$:

:$\forall X \in \powerset S: \map {\RR^\to} X = \begin {cases} \set {t \in T: \exists s \in X: \tuple {s, t} \in \RR} & : X \ne \O \\ \O & : X = \O \end {cases}$

=== Mapping ===
Let $S$ and $T$ be sets.

Let $\powerset S$ and $\powerset T$ be their power sets.

Let $f \subseteq S \times T$ be a mapping from $S$ to $T$.


The direct image mapping of $f$ is the mapping $f^\to: \powerset S \to \powerset T$ that sends a subset $X \subseteq S$ to its image under $f$:
:$\forall X \in \powerset S: \map {f^\to} X = \begin {cases} \set {t \in T: \exists s \in X: \map f s = t} & : X \ne \O \\ \O & : X = \O \end {cases}$


=== Direct Image Mapping as Set of Images of Subsets ===

 

The direct image mapping of $f$ can be seen to be the set of images of all the subsets of the domain of $f$:

:$\forall X \subseteq S: f \sqbrk X = \map {f^\to} X$


Both approaches to this concept are used in  .",Definition:Direct Image Mapping,"['Definitions/Induced Mappings', 'Definitions/Images', 'Definitions/Direct Image Mappings']"
Definition:Direct Image,Direct Image,"Let $S$ and $T$ be sets.

Let $\powerset S$ and $\powerset T$ be their power sets.

Let $f \subseteq S \times T$ be a mapping from $S$ to $T$.


The direct image mapping of $f$ is the mapping $f^\to: \powerset S \to \powerset T$ that sends a subset $X \subseteq S$ to its image under $f$:
:$\forall X \in \powerset S: \map {f^\to} X = \begin {cases} \set {t \in T: \exists s \in X: \map f s = t} & : X \ne \O \\ \O & : X = \O \end {cases}$


=== Direct Image Mapping as Set of Images of Subsets ===

 

The direct image mapping of $f$ can be seen to be the set of images of all the subsets of the domain of $f$:

:$\forall X \subseteq S: f \sqbrk X = \map {f^\to} X$


Both approaches to this concept are used in  .",Definition:Direct Image Mapping/Mapping,['Definitions/Direct Image Mappings']
Definition:Direct Image,Direct Image,"Let $S$ and $T$ be sets.

Let $\powerset S$ and $\powerset T$ be their power sets.

Let $\RR \subseteq S \times T$ be a relation on $S \times T$.


The direct image mapping of $\RR$ is the mapping $\RR^\to: \powerset S \to \powerset T$ that sends a subset $X \subseteq T$ to its image under $\RR$:

:$\forall X \in \powerset S: \map {\RR^\to} X = \begin {cases} \set {t \in T: \exists s \in X: \tuple {s, t} \in \RR} & : X \ne \O \\ \O & : X = \O \end {cases}$",Definition:Direct Image Mapping/Relation,['Definitions/Direct Image Mappings']
Definition:Direct Image,Direct Image,"Let $X$ be a topological space.

Let $\mathbf C$ be a category.

Let $\FF$ be a $\mathbf C$-valued sheaf on $X$.


The direct image of $\FF$ is the direct image of the presheaf $\FF$.",Definition:Direct Image of Sheaf,['Definitions/Sheaf Theory']
Definition:Directed,Directed,"A digraph is a graph each of whose edges has a direction:



In the above graph, the vertices are $v_1, v_2, v_3$ and $v_4$.


=== Arc ===
Let $G = \struct {V, E}$ be a digraph.

The arcs are the elements of $E$.

Informally, an arc is a line that joins one vertex to another.


If $e \in E$ is an arc joining the vertex $u$ to the vertex $v$, it is denoted $u v$.


=== Endvertex ===
Let $D = \struct {V, E}$ be a digraph.

Let $e = u v$ be an arc of $D$, that is, $e \in E$.


The endvertices of $e$ are the vertices $u$ and $v$.


=== Initial Vertex ===
Let $D = \struct {V, E}$ be a digraph.

Let $e = u v$ be an arc of $D$, that is, $e \in E$.


The initial vertex of $e$ is the endvertex $u$ which $e$ is incident from.

=== Final Vertex ===
Let $D = \struct {V, E}$ be a digraph.

Let $e = u v$ be an arc of $D$, that is, $e \in E$.


The final vertex of $e$ is the endvertex $v$ which $e$ is incident to.

In the above graph, the arcs are $v_1 v_2, v_2 v_4, v_4 v_3, v_4 v_1$ and $v_1 v_4$.


As can be seen, in this general definition it is allowable for an arc to go in both directions between a given pair of vertices.


=== Formal Definition ===



A directed graph or digraph $D$ is a non-empty set $V$ together with an antireflexive relation $E$ on $V$.

The elements of $E$ are the arcs.


Thus the above digraph can be defined as:

:$D = \struct {V, E}:$
::$V = \set {v_1, v_2, v_3, v_4}$
::$E = \set {\tuple {v_1, v_2}, \tuple {v_2, v_4}, \tuple {v_4, v_3}, \tuple {v_4, v_1}, \tuple {v_1, v_4} }$

=== Category-Theoretic Definition ===
Let $\mathbf {Set}$ be the category of sets.

A digraph is an arrangement of the following form in $\mathbf{Set}$:

::$\begin{xy}
<0em,0em>*{E} = ""E"",
<5em,0em>*{V} = ""V"",

""E""+/^.3em/+/r1em/;""V""+/^.3em/+/l1em/ **@{-} ?>*@{>} ?*!/_.6em/{s},
""E""+/_.3em/+/r1em/;""V""+/_.3em/+/l1em/ **@{-} ?>*@{>} ?*!/^.6em/{t},
\end{xy}$

=== Symmetric Digraph ===
Let $D = \struct {V, E}$ be a digraph such that the relation $E$ in $D$ is symmetric.

Then $D$ is called a symmetric digraph.


It follows from the definition of a (simple) graph that a symmetric digraph is in fact the same thing as an undirected graph.

=== Simple Digraph ===
Let $D = \struct {V, E}$ be a digraph.

If the relation $E$ in $D$ is also specifically asymmetric, then $D$ is called a simple digraph.

That is, in a simple digraph there are no pairs of arcs (like there are between $v_1$ and $v_4$ in the diagram above) which go in both directions between two vertices.",Definition:Digraph,"['Definitions/Digraphs', 'Definitions/Graph Theory']"
Definition:Directed,Directed,"Let $\struct {S, \precsim}$ be a preordered set.


Then $\struct {S, \precsim}$ is a directed preordering  if and only if  every pair of elements of $S$ has an upper bound in $S$:
:$\forall x, y \in S: \exists z \in S: x \precsim z$ and $y \precsim z$",Definition:Directed Preordering,['Definitions/Preorder Theory']
Definition:Directed,Directed,"Let $\struct {S, \precsim}$ be a preordered set.

Let $H$ be a non-empty subset of $S$.

Then $H$ is a directed subset of $S$  if and only if :

:$\forall x, y \in H: \exists z \in H: x \precsim z$ and $y \precsim z$",Definition:Directed Subset,['Definitions/Preorder Theory']
Definition:Directed,Directed,"Let $\R^n$ be a real cartesian space of $n$ dimensions.

Let $\rho: \left[{a \,.\,.\, b}\right] \to \R^n$ be a smooth path in $\R^n$.


The directed smooth curve with parameterization $\rho$ is defined as an equivalence class of smooth paths as follows:

A smooth path $\sigma: \left[{a \,.\,.\, b}\right] \to \R^n$ belongs to the equivalence class of $\rho$  if and only if :
: there exists a bijective differentiable strictly increasing real function:
:: $\phi: \left[{c \,.\,.\, d}\right] \to \left[{a \,.\,.\, b}\right]$
: such that $\sigma = \rho \circ \phi$.


It follows from Directed Smooth Curve Relation is Equivalence and Fundamental Theorem on Equivalence Relations that this does in fact define an equivalence class.


If a directed smooth curve is only defined by a smooth path $\rho$, then it is often denoted with the same symbol $\rho$.


=== Parameterization ===
Let $\R^n$ be a real cartesian space of $n$ dimensions.

Let $C$ be a directed smooth curve in $\R^n$.

Let $\rho: \closedint a b \to \C$ be a smooth path in $\R^n$.


Then $\rho$ is a parameterization of $C$  if and only if  $\rho$ is an element of the equivalence class that constitutes $C$.


=== Complex Plane ===

The definition carries over to the complex plane, in which context it is usually applied:

Let $C$ be a directed smooth curve in the complex plane $\C$.

Let $\gamma: \closedint a b \to \C$ be a smooth path in $\C$.


Then $\gamma$ is a parameterization of $C$  if and only if  $\gamma$ is a representative of the equivalence class that constitutes $C$.


=== Reparameterization ===
Let $\gamma : \closedint a b \to \C$ be a smooth path in $\C$.

Let $C$ be a directed smooth curve in the complex plane $\C$ parameterized by $\gamma$.

Let $\phi: \closedint c d \to \closedint a b$ be a bijective differentiable strictly increasing real function.

Let $\sigma : \closedint c d \to \C$ be defined by:

:$\sigma = \gamma \circ \phi$ 


Then $\sigma$ is called a reparameterization of $C$.

=== Endpoints ===
Let $\R^n$ be a real cartesian space of $n$ dimensions.

Let $C$ be a directed smooth curve in $\R^n$.


Let $C$ be parameterized by a smooth path $\rho: \left[{a \,.\,.\, b}\right] \to \C$.


Then:
: $\rho \left({a}\right)$ is the start point of $C$

: $\rho \left({b}\right)$ is the end point of $C$.


Collectively, $\rho \left({a}\right)$ and $\rho \left({b}\right)$ are known as the endpoints of $\rho$.


=== Complex Plane ===

The definition carries over to the complex plane, in which context it is usually applied:

Let $C$ be a directed smooth curve in the complex plane $\C$.

Let $C$ be parameterized by a smooth path $\gamma: \left[{a \,.\,.\, b}\right] \to \C$.


Then:
: $\gamma \left({a}\right)$ is the start point of $C$

: $\gamma \left({b}\right)$ is the end point of $C$.


Collectively, $\gamma \left({a}\right)$ and $\gamma \left({b}\right)$ are known as the endpoints of $\rho$.",Definition:Directed Smooth Curve,['Definitions/Vector Analysis']
Definition:Directed,Directed,"A directed line segment is a line segment endowed with the additional property of direction.

It is often used in the context of applied mathematics to represent a vector quantity.


 

 ",Definition:Directed Line Segment,"['Definitions/Directed Line Segments', 'Definitions/Straight Lines', 'Definitions/Analytic Geometry', 'Definitions/Vectors', 'Definitions/Applied Mathematics']"
Definition:Directed,Directed,"There are two versions of the Directed Hamilton Cycle Problem.


=== Function Version ===
:Given a digraph $G$ with $n$ vertices, to find a Hamilton cycle in $G$.

=== Decision Version ===
:Given a digraph $G$ with $n$ vertices, to determine whether $G$ has a Hamilton cycle.",Definition:Directed Hamilton Cycle Problem,['Definitions/Digraphs']
Definition:Directed,Directed,"Let $G = \struct {V, A}$ be a digraph.


A directed walk in $G$ is a finite or infinite sequence $\sequence {x_k}$ such that:

:$\forall k \in \N: k + 1 \in \Dom {\sequence {x_k} }: \tuple {x_k, x_{k + 1} } \in A$",Definition:Directed Walk,"['Definitions/Digraphs', 'Definitions/Walks']"
Definition:Directed,Directed,"A directed network is a network whose underlying graph is a digraph:


:",Definition:Network/Directed,['Definitions/Network Theory']
Definition:Directed,Directed,"Let $\mathbf C$ be a category.

Let $I$ be a directed ordered set.

Let $\mathbf I$ be the order category of $I$.

Let $D : \mathbf I \to \mathbf C$ be a diagram.


The direct limit of $D$ is the colimit of the diagram $D$.",Definition:Direct Limit,['Definitions/Examples of Limits and Colimits']
Definition:Directrix,Directrix,"A directrix is a curve which defines the generators of a ruled surface.


=== Directrix of Cone ===
Let $K$ be a cone.

Let $B$ be the base of $K$.


The boundary of $B$ is known as the directrix of $K$.

=== Directrix of Cylindrical Surface ===
The directrix of a cylindrical surface $S$ is the curve $C$ through which pass all the parallel straight lines forming $S$.",Definition:Directrix of Ruled Surface,"['Definitions/Directrices of Ruled Surfaces', 'Definitions/Directrices', 'Definitions/Analytic Geometry', 'Definitions/Geometry']"
Definition:Directrix,Directrix,"Let $K$ be a conic section specified in terms of:
:a given straight line $D$
:a given point $F$
:a given constant $e$

where $K$ is the locus of points $P$ such that the distance $p$ from $P$ to $D$ and the distance $q$ from $P$ to $F$ are related by the condition:
:$q = e p$


The line $D$ is known as the directrix of the conic section.",Definition:Conic Section/Directrix,"['Definitions/Directrices', 'Definitions/Conic Sections']"
Definition:Directrix,Directrix,"Let $\KK$ be a conchoid.

Let $\CC$ be the curve   which the conchoid is constructed.


$\CC$ is known as the directrix of $\KK$.


=== Also known as ===
",Definition:Conchoid/Directrix,"['Definitions/Conchoids', 'Definitions/Directrices']"
Definition:Disconnected,Disconnected,"=== Topological Space ===
Let $T = \struct {S, \tau}$ be a topological space.


=== Definition $1$ ===
Let $T = \struct {S, \tau}$ be a topological space.


$T$ is disconnected  if and only if  $T$ is not connected.

=== Definition $2$ ===
Let $T = \struct {S, \tau}$ be a topological space.


$T$ is disconnected  if and only if  there exist non-empty open sets $U, V \in \tau$ such that:
:$S = U \cup V$
:$U \cap V = \O$

That is, if there exists a partition of $S$ into open sets of $T$.

=== Subset of Topological Space ===

Let $T = \struct {S, \tau}$ be a topological space.

Let $H \subseteq S$ be a non-empty subset of $S$.

Let $T = \struct {S, \tau}$ be a topological space.

Let $H \subseteq S$ be a non-empty subset of $S$.


$H$ is a disconnected set of $T$  if and only if  it is not a connected set of $T$.

=== Points in Topological Space ===
Let $T = \left({S, \tau}\right)$ be a topological space.

Let $a, b \in S$.


Then $a$ and $b$ are disconnected (in $T$)  if and only if  they are not connected (in $T$).


Category:Definitions/Disconnected Spaces",Definition:Disconnected (Topology),['Definitions/Disconnected Spaces']
Definition:Disconnected,Disconnected,"Let $G = \struct {V, E}$ be a graph.

Then $G$ is disconnected  if and only if  it is not connected.

That is,  if and only if  there exists (at least) two vertices $u, v \in V$ such that $u$ and $v$ are not connected.",Definition:Connected (Graph Theory)/Graph/Disconnected,"['Definitions/Disconnected Graphs', 'Definitions/Connectedness (Graph Theory)']"
Definition:Discontinuous,Discontinuous,"=== Discontinuous Real Function ===
Let $f$ be a real function.

Then $f$ is discontinuous  if and only if  there exists at least one $a \in \R$ at which $f$ is discontinuous.


=== At a Point ===
Let $A \subseteq \R$ be a subset of the real numbers.

Let $f : A \to \R$ be a real function.

Let $a\in A$.


Then $f$ is discontinuous at $a$  if and only if  $f$ is not continuous at $a$.

=== Discontinuous Topological Space ===
Let $T_1 = \struct {A_1, \tau_1}$ and $T_2 = \struct {A_2, \tau_2}$ be topological spaces.

Let $f: A_1 \to A_2$ $x \in T_1$ be a mapping from $A_1$ to $A_2$.


Then by definition $f$ is continuous at $x$ if for every neighborhood $N$ of $\map f x$ there exists a neighborhood $M$ of $x$ such that $f \sqbrk M \subseteq N$.

Therefore, $f$ is discontinuous at $x$ if for some neighborhood $N$ of $\map f x$ and every neighborhood $M$ of $x$:
:$f \sqbrk M \nsubseteq N$

The point $x$ is called a discontinuity of $f$.",Definition:Discontinuous Mapping,"['Definitions/Discontinuous Mappings', 'Definitions/Continuity']"
Definition:Discontinuous,Discontinuous,"Let $f$ be a real function.

Then $f$ is discontinuous  if and only if  there exists at least one $a \in \R$ at which $f$ is discontinuous.


=== At a Point ===
Let $A \subseteq \R$ be a subset of the real numbers.

Let $f : A \to \R$ be a real function.

Let $a\in A$.


Then $f$ is discontinuous at $a$  if and only if  $f$ is not continuous at $a$.",Definition:Discontinuous Mapping/Real Function,['Definitions/Discontinuous Mappings']
Definition:Discontinuous,Discontinuous,"Let $A \subseteq \R$ be a subset of the real numbers.

Let $f : A \to \R$ be a real function.

Let $a\in A$.


Then $f$ is discontinuous at $a$  if and only if  $f$ is not continuous at $a$.",Definition:Discontinuous Mapping/Real Function/Point,"['Definitions/Discontinuous Mappings', 'Definitions/Discontinuities (Real Analysis)']"
Definition:Discontinuous,Discontinuous,"Let $T_1 = \struct {A_1, \tau_1}$ and $T_2 = \struct {A_2, \tau_2}$ be topological spaces.

Let $f: A_1 \to A_2$ $x \in T_1$ be a mapping from $A_1$ to $A_2$.


Then by definition $f$ is continuous at $x$ if for every neighborhood $N$ of $\map f x$ there exists a neighborhood $M$ of $x$ such that $f \sqbrk M \subseteq N$.

Therefore, $f$ is discontinuous at $x$ if for some neighborhood $N$ of $\map f x$ and every neighborhood $M$ of $x$:
:$f \sqbrk M \nsubseteq N$

The point $x$ is called a discontinuity of $f$.",Definition:Discontinuous Mapping/Topological Space/Point,['Definitions/Discontinuous Mappings']
Definition:Discrete,Discrete,Discrete mathematics is the branch of mathematics which studies mathematical structures that are discrete rather than continuous.,Definition:Discrete Mathematics,"['Definitions/Branches of Mathematics', 'Definitions/Discrete Mathematics']"
Definition:Discrete,Discrete,"Let $\CC$ be a metacategory.


Then $\CC$ is said to be discrete  if and only if  it comprises only identity morphisms.

If the collection $\CC$ constitutes the objects of $\mathbf C$, then $\mathbf C$ may also be denoted $\map {\mathbf {Dis} } \CC$.",Definition:Discrete Category,['Definitions/Examples of Categories']
Definition:Discrete,Discrete,"Let $S \ne \O$ be a set.

Let $\tau = \powerset S$ be the power set of $S$.

That is, let $\tau$ be the set of all subsets of $S$:
:$\tau := \set {H: H \subseteq S}$


Then $\tau$ is called the discrete topology on $S$ and $\struct {S, \tau} = \struct {S, \powerset S}$ the discrete space on $S$, or just a discrete space.


=== Finite Discrete Topology ===
Let $S \ne \O$ be a set.

Let $\tau = \powerset S$ be the power set of $S$.

That is, let $\tau$ be the set of all subsets of $S$:
:$\tau := \set {H: H \subseteq S}$


Let $S$ be a finite set.

Then $\tau = \powerset S$ is a finite discrete topology, and $\struct {S, \tau} = \struct {S, \powerset S}$ is a finite discrete space.

=== Infinite Discrete Topology ===
Let $S \ne \O$ be a set.

Let $\tau = \powerset S$ be the power set of $S$.

That is, let $\tau$ be the set of all subsets of $S$:
:$\tau := \set {H: H \subseteq S}$


Let $S$ be an infinite set.

Then $\tau = \powerset S$ is an infinite discrete topology, and $\struct {S, \tau} = \struct {S, \powerset S}$ is an infinite discrete space.


=== Countable Discrete Topology ===
Let $S \ne \O$ be an infinite set.

Let $\tau = \powerset S$ be the power set of $S$.

That is, let $\tau$ be the set of all subsets of $S$:
:$\tau := \set {H: H \subseteq S}$


Let $S$ be a countably infinite set.

Then $\tau = \powerset S$ is a countable discrete topology, and $\struct {S, \tau} = \struct {S, \powerset S}$ is a countable discrete space.

=== Uncountable Discrete Topology ===
Let $S \ne \O$ be a set.

Let $\tau = \powerset S$ be the power set of $S$.

That is, let $\tau$ be the set of all subsets of $S$:
:$\tau := \set {H: H \subseteq S}$


Let $S$ be an uncountably infinite set.

Then $\tau = \powerset S$ is an uncountable discrete topology, and $\struct {S, \tau} = \struct {S, \powerset S}$ is an uncountable discrete space.",Definition:Discrete Topology,"['Definitions/Examples of Topologies', 'Definitions/Discrete Topology']"
Definition:Discrete,Discrete,"A discrete group is a topological group whose topology is discrete.

Category:Definitions/Topological Groups",Definition:Discrete Group,['Definitions/Topological Groups']
Definition:Discrete,Discrete,"Let $G$ be a subgroup of the additive group of real numbers.


Then $G$ is discrete  if and only if :
:$\forall g \in G: \exists \epsilon \in \R_{>0}: \openint {g - \epsilon} {g + \epsilon} \cap G = \set g$

That is, there exists a neighborhood of $g$ which contains no other elements of $G$.


Category:Definitions/Subgroups
Category:Definitions/Topological Groups
Category:Definitions/Real Numbers",Definition:Discrete Subgroup/Real Numbers,"['Definitions/Subgroups', 'Definitions/Topological Groups', 'Definitions/Real Numbers']"
Definition:Discrete,Discrete,"The standard discrete metric on a set $S$ is the metric satisfying:

:$\map d {x, y} = \begin {cases} 0 & : x = y \\ 1 & : x \ne y \end {cases}$


This can be expressed using the Kronecker delta notation as:
:$\map d {x, y} = 1 - \delta_{x y}$


The resulting metric space $M = \struct {S, d}$ is the standard discrete metric space on $S$.",Definition:Standard Discrete Metric,"['Definitions/Standard Discrete Metric', 'Definitions/Examples of Metric Spaces']"
Definition:Discrete,Discrete,"Let $T = \struct {S, \tau}$ be a topological space.

Let $\FF$ be a set of subsets of $S$.


Then $\FF$ is discrete  if and only if  each element of $S$ has a neighborhood which intersects at most one of the sets in $\FF$.",Definition:Discrete Set of Subsets,['Definitions/Topology']
Definition:Discrete,Discrete,Discrete geometry is a branch of geometry that studies constructive methods of discrete geometric objects.,Definition:Discrete Geometry,"['Definitions/Branches of Mathematics', 'Definitions/Discrete Geometry', 'Definitions/Geometry']"
Definition:Discrete,Discrete,"Let $\struct {X, \Sigma, \mu}$ be a measure space.


Then $\mu$ is said to be a discrete measure  if and only if  it is a series of Dirac measures.

That is,  if and only if  there exist:
:a sequence $\sequence {x_n}_{n \mathop \in \N}$ in $X$
and:
:a sequence $\sequence {\lambda_n}_{n \mathop \in \N}$ in $\R$

such that:

:$(1):\quad \forall E \in \Sigma: \map \mu E = \ds \sum_{n \mathop \in \N} \lambda_n \, \map {\delta_{x_n} } E$

where $\delta_{x_n}$ denotes the Dirac measure at $x_n$.


By Series of Measures is Measure, defining $\mu$ by $(1)$ yields a measure.",Definition:Discrete Measure,"['Definitions/Measure Theory', 'Definitions/Measures', 'Definitions/Measures']"
Definition:Discrete,Discrete,A discrete probability distribution is a probability distribution of a discrete random variable.,Definition:Discrete Probability Distribution,"['Definitions/Discrete Probability Distributions', 'Definitions/Probability Distributions', 'Definitions/Probability Theory']"
Definition:Discrete,Discrete,"Let $\EE$ be an experiment.

Let $\Omega$ denote the sample space of $\EE$.


If $\Omega$ is a countable set, whether finite or infinite, then it is known as a discrete sample space.",Definition:Sample Space/Discrete,['Definitions/Probability Theory']
Definition:Discrete,Discrete,A discrete variable is a variable which is not continuous.,Definition:Variable/Discrete,"['Definitions/Descriptive Statistics', 'Definitions/Variables']"
Definition:Discrete,Discrete,Data which can be described with a discrete variable are known as discrete data.,Definition:Sample Statistic/Discrete,['Definitions/Sample Statistics']
Definition:Discriminant,Discriminant," 


Let $k$ be a field.

Let $\map f X \in k \sqbrk X$ be a polynomial of degree $n$.

Let $\overline k$ be an algebraic closure of $k$.

Let the roots of $f$ in $\overline k$ be $\alpha_1, \alpha_2, \ldots, \alpha_n$.


Then the discriminant $\map \Delta f$ of $f$ is defined as:

:$\ds \map \Delta f := \prod_{1 \mathop \le i \mathop < j \mathop \le n} \paren {\alpha_i - \alpha_j}^2$


=== Quadratic Equation ===

The concept is usually encountered in the context of a quadratic equation $a x^2 + b x + c = 0$:
Consider the quadratic equation:
:$a x^2 + b x + c = 0$

The expression $b^2 - 4 a c$ is called the discriminant of the equation.

=== Cubic Equation ===

In the context of a cubic equation $a x^3 + b x^2 + c x + d = 0$:
 ",Definition:Discriminant of Polynomial,"['Definitions/Discriminants of Polynomials', 'Definitions/Discriminants', 'Definitions/Polynomial Theory']"
Definition:Discriminant,Discriminant,"Let $\mathbb K$ be a field.

Let $V$ be a vector space over $\mathbb K$ of finite dimension $n>0$.

Let $b : V\times V \to \mathbb K$ be a bilinear form on $V$.

Let $A$ be the matrix of $b$ relative to an ordered basis of $V$.


If $b$ is nondegenerate, its discriminant is the equivalence class of the determinant $\det A$ in the quotient group $\dfrac {\mathbb K^\times} {\paren {\mathbb K^\times}^2}$.

If $b$ is degenerate, its discriminant is $0$.",Definition:Discriminant of Bilinear Form,"['Definitions/Bilinear Forms (Linear Algebra)', 'Definitions/Discriminants']"
Definition:Discriminant,Discriminant,"Let $K$ be a conic section embedded in a Cartesian plane with the general equation:
:$a x^2 + 2 h x y + b y^2 + 2 g x + 2 f y + c = 0$
where $a, b, c, f, g, h \in \R$.


The discriminant of $K$ is defined as the determinant calculated as:
:$\Delta = \begin {vmatrix} a & h & g \\ h & b & f \\ g & f & c \end {vmatrix}$",Definition:Discriminant of Conic Section,"['Definitions/Discriminants of Conic Sections', 'Definitions/Discriminants', 'Definitions/Conic Sections']"
Definition:Discriminant,Discriminant,"A discriminant function is a function which assigns a given individual to one of a number of populations according to the data appertaining to that individual.

It is based on measurements on individuals for whom the population to which each one belongs is known.

It is chosen to minimize the probabilities or costs of misclassification.",Definition:Discriminant Function,"['Definitions/Discriminant Functions', 'Definitions/Descriptive Statistics', 'Definitions/Discriminants']"
Definition:Dispersion,Dispersion,"Let $S$ be a sample of a population in the context of statistics.

The dispersion of $S$ is a general term meaning how much the data describing the sample are spread out.


The word can also be applied to a random variable.


Measures of dispersion include the following:


=== Range ===
Let $S$ be a set of observations of a quantitative variable.


The range of $S$ is defined as:
:$\map R S := \map \max S - \map \min S$

where $\map \max S$ and $\map \min S$ are the greatest value of $S$ and the least value of $S$ respectively.

=== Interquartile Range ===
The interquartile range is a measure of dispersion in statistics.


Let $Q_1$ and $Q_3$ be first quartile and third quartile respectively.

The interquartile range is defined and denoted as:

:$\operatorname {IQR} := Q_3 - Q_1$

=== Mean Absolute Deviation ===
Let $S = \set {x_1, x_2, \ldots, x_n}$ be a set of observations.

Let $\bar x$ denote a measure of central tendency of $S$.


The mean absolute deviation   $\bar x$ of $S$ is defined as the arithmetic mean of the absolute values of the deviation of the elements of $S$ from $\bar x$ :

:$\ds \sum_{i \mathop = 1}^n \dfrac 1 n \size {x_i - \bar x}$


=== Discrete Random Variable ===
Let $X$ be a discrete random variable.

Let $\bar x$ denote a measure of central tendency of $X$.


The mean absolute deviation of $X$ is the first absolute moment of $X$ about $\bar x$.

=== Continuous Random Variable ===
Let $X$ be a continuous random variable.

Let $m$ denote the median of $X$.

Let the frequency function of $X$ be $f$.


The mean absolute deviation of $X$ is defined as:
:$\ds \int_{-\infty}^{+\infty} \size {x - m} \map f x \rd x$

=== Variance ===
=== Discrete Random Variable ===
Let $X$ be a discrete random variable.

Then the variance of $X$, written $\var X$, is a measure of how much the values of $X$ varies from the expectation $\expect X$, and is defined as:

=== Definition 1 ===
Let $X$ be a discrete random variable.

Then the variance of $X$, written $\var X$, is a measure of how much the values of $X$ varies from the expectation $\expect X$, and is defined as:

:$\var X := \expect {\paren {X - \expect X}^2}$

That is: it is the expectation of the squares of the deviations from the expectation.

=== Definition 2 ===

Using $\mu = \expect X$, we can consider $\paren {X - \mu}^2$ as a function of $X$ and apply Expectation of Function of Discrete Random Variable, to obtain:
Let $X$ be a discrete random variable.

Then the variance of $X$, written $\var X$, is defined as:

:$\ds \var X := \sum_{x \mathop \in \Omega_X} \paren {x - \mu^2} \map \Pr {X = x}$
where:
:$\mu := \expect X$ is the expectation of $X$
:$\Omega_X$ is the image of $X$
:$\map \Pr {X = x}$ is the probability mass function of $X$.

=== Definition 3 ===

Far easier to work with than the above definition is the result:
Let $X$ be a discrete random variable.

Let $\expect X$ be the expectation of $X$.

Then the variance of $X$, written $\var X$, is defined as:

:$\var X := \expect {X^2} - \paren {\expect X}^2$

=== Continuous Random Variable ===
Let $X$ be a continuous random variable. 

Then the variance of $X$, written $\var X$, is a measure of how much the values of $X$ varies from the expectation $\expect X$, and is defined as:

:$\var X := \expect {\paren {X - \expect X}^2}$

That is, the expectation of the squares of the deviations from the expectation.


Letting $\mu = \expect X$, this is often given as: 

:$\var X = \expect {\paren {X - \mu}^2}$

=== Standard Deviation ===
Let $X$ be a random variable.

Then the standard deviation of $X$, written $\sigma_X$ or $\sigma$, is defined as the principal square root of the variance of $X$:

:$\sigma_X := \sqrt {\var X}$",Definition:Dispersion (Statistics),"['Definitions/Dispersion (Statistics)', 'Definitions/Statistics']"
Definition:Dispersion,Dispersion,"Let $T = \struct {S, \tau}$ be a topological space.

Let $H \subseteq S$ be a connected set in $T$ and let $p \in H$.

Let $p \in H$ such that $H \setminus \set p$ is totally disconnected, where $\setminus$ denotes set difference.


Then $p$ is a dispersion point of $H$.",Definition:Dispersion Point,['Definitions/Totally Disconnected Spaces']
Definition:Distance,Distance,"Let $x, y \in \C$ be complex numbers.

Let $\cmod {x - y}$ be the complex modulus of $x - y$.


Then the function $d: \C^2 \to \R$:
:$\map d {x, y} = \cmod {x - y}$
is called the distance between $x$ and $y$.",Definition:Distance/Points/Complex Numbers,['Definitions/Complex Analysis']
Definition:Distance,Distance,"Let $\struct {A, d}$ be a metric space.

The mapping $d: A \times A \to \R$ is referred to as a distance function on $A$.


Here, $d: A \times A \to \R$ is a real-valued function satisfying the metric space axioms:
 ",Definition:Metric Space/Distance Function,"['Definitions/Distance Functions', 'Definitions/Metric Spaces']"
Definition:Distance,Distance,"Let $M = \struct {A, d}$ be a metric space.

Let $x \in A$.

Let $S, T$ be subsets of $A$.


The distance between $x$ and $S$ is defined and annotated $\ds \map d {x, S} = \inf_{y \mathop \in S} \paren {\map d {x, y} }$.

The distance between $S$ and $T$ is defined and annotated $\ds \map d {S, T} = \inf_{\substack {x \mathop \in S \\ y \mathop \in T} } \paren {\map d {x, y} }$.",Definition:Distance/Sets/Metric Spaces,"['Definitions/Metric Spaces', 'Definitions/Distance Function']"
Definition:Distance,Distance,The distance between two points $A$ and $B$ in space is defined as the length of a straight line that would be drawn from $A$ to $B$.,Definition:Linear Measure/Distance,['Definitions/Length']
Definition:Distance,Distance,"The angular distance between two points $A$ and $B$   a reference point $P$ is the angle between the straight lines $AP$ and $BP$, that is:
:$\angle APB$",Definition:Angular Distance,['Definitions/Angles']
Definition:Distance,Distance,"Let $u$ and $v$ be two codewords of a linear code.

The Hamming distance between $u$ and $v$ is the number of corresponding terms at which $u$ and $v$ are different.",Definition:Hamming Distance,"['Definitions/Hamming Distance', 'Definitions/Linear Codes']"
Definition:Distance,Distance,"=== Undirected Graph ===
Let $G = \tuple {V, E}$ be an undirected graph.

Let $u, v \in V$ be vertices of $V$.


The distance between $u$ and $v$ is the length of the shortest path from $u$ to $v$.

=== Digraph ===
Let $G = \tuple {V, E}$ be a digraph.

Let $u, v \in V$ be vertices of $V$.


The distance from $u$ to $v$ is the length of the shortest path from $u$ to $v$.",Definition:Distance (Graph Theory),"['Definitions/Distance (Graph Theory)', 'Definitions/Graph Theory']"
Definition:Distributive,Distributive,"Let $S$ be a set on which is defined two binary operations, defined on all the elements of $S \times S$, which we will denote as $\circ$ and $*$.


The operation $\circ$ is distributive over $*$, or distributes over $*$,  if and only if :
:$\circ$ is right distributive over $*$
and:
:$\circ$ is left distributive over $*$.


=== Left Distributive ===
Let $S$ be a set on which is defined two binary operations, defined on all the elements of $S \times S$, denoted here as $\circ$ and $*$.

The operation $\circ$ is left distributive over the operation $*$  if and only if :

:$\forall a, b, c \in S: a \circ \paren {b * c} = \paren {a \circ b} * \paren {a \circ c}$

=== Right Distributive ===
Let $S$ be a set on which is defined two binary operations, defined on all the elements of $S \times S$, denoted here as $\circ$ and $*$.

The operation $\circ$ is right distributive over the operation $*$  if and only if :

:$\forall a, b, c \in S: \paren {a * b} \circ c = \paren {a \circ c} * \paren {b \circ c}$

So as to streamline what may turn into cumbersome language, some further definitions:

=== Distributand ===
Let $S$ be a set on which is defined two binary operations, defined on all the elements of $S \times S$, which we will denote as $\circ$ and $*$.

Let $\circ$ be distributive over $*$.

Then $*$ is a distributand of $\circ$.


=== Linguistic Note ===


Category:Definitions/Distributive Operations

=== Distributor ===
Let $S$ be a set on which is defined two binary operations, defined on all the elements of $S \times S$, which we will denote as $\circ$ and $*$.

Let $\circ$ be distributive over $*$.

Then $\circ$ is a distributor of $*$.


Category:Definitions/Distributive Operations",Definition:Distributive Operation,"['Definitions/Distributive Operations', 'Definitions/Abstract Algebra']"
Definition:Distributive,Distributive,"Let $\struct {S, \vee, \wedge, \preceq}$ be a lattice.


=== Definition 1 ===
Let $\struct {S, \vee, \wedge, \preceq}$ be a lattice.


Then $\struct {S, \vee, \wedge, \preceq}$ is distributive  if and only if  $\struct {S, \vee, \wedge, \preceq}$ satisfies one of the distributive lattice axioms:
 

=== Definition 2 ===
Let $\struct {S, \vee, \wedge, \preceq}$ be a lattice.


Then $\struct {S, \vee, \wedge, \preceq}$ is distributive  if and only if  $\struct {S, \vee, \wedge, \preceq}$ satisfies all of the distributive lattice axioms:
 ",Definition:Distributive Lattice,"['Definitions/Distributive Lattices', 'Definitions/Order Theory', 'Definitions/Lattice Theory']"
Definition:Divergence,Divergence,"Let $\struct {\mathbb K, \norm {\,\cdot\,} }$ be a valued field.

Let $\sequence {a_n}$ be a sequence of elements of $\mathbb K$.


If either:

:there exist infinitely many $n \in \N$ with $a_n = 0$

:there exists $n_0 \in \N$ with $a_n \ne 0$ for all $n > n_0$ and the sequence of partial products of $\ds \prod_{n \mathop = n_0 + 1}^\infty a_n$ converges to $0$

the product diverges to $0$, and we assign the value:
:$\ds \prod_{n \mathop = 1}^\infty a_n = 0$


Category:Definitions/Divergent Products
Category:Definitions/Infinite Products",Definition:Divergent Product/Divergence to Zero,"['Definitions/Divergent Products', 'Definitions/Infinite Products']"
Definition:Divergence,Divergence,"=== Physical Interpretation ===
Let $\mathbf V$ be a vector field acting over a region of space $R$.

The divergence of $\mathbf V$ at a point $P$ is the total flux away from $P$ per unit volume.

It is a scalar field.

=== Geometrical Representation ===
Let $R$ be a region of space embedded in a Cartesian coordinate frame.

Let $\mathbf V$ be a vector field acting over $R$.


The divergence of $\mathbf V$ at a point $A$ in $R$ is defined as:

 
 
 
 

where:
:$\nabla$ denotes the Del operator
:$\cdot$ denotes the dot product
:$V_x$, $V_y$ and $V_z$ denote the magnitudes of the components of $\mathbf V$ at $A$ in the directions of the coordinate axes $x$, $y$ and $z$ respectively.

=== Real Cartesian Space ===
Let $\map {\R^n} {x_1, x_2, \ldots, x_n}$ denote the real Cartesian space of $n$ dimensions.

Let $\tuple {\mathbf e_1, \mathbf e_2, \ldots, \mathbf e_n}$ be the standard ordered basis on $\R^n$.

Let $\mathbf f = \tuple {\map {f_1} {\mathbf x}, \map {f_2} {\mathbf x}, \ldots, \map {f_n} {\mathbf x} }: \R^n \to \R^n$ be a vector-valued function on $\R^n$.


Let the partial derivative of $\mathbf f$ with respect to $x_k$ exist for all $f_k$.


The divergence of $\mathbf f$ is defined as:

 
 
 
 
 


=== Cartesian $3$-Space ===

In $3$ dimensions with the standard ordered basis $\tuple {\mathbf i, \mathbf j, \mathbf k}$, this is usually rendered:

Let $R$ be a region of Cartesian $3$-space $\R^3$.

Let $\map {\mathbf V} {x, y, z}$ be a vector field acting over $R$.


The divergence of $\mathbf V$ is defined as:

 
 
 
 

=== Riemannian Manifold ===
Let $\struct {M, g}$ be a Riemannian manifold equiped with a metric $g$.

Let $\mathbf X : \map {\CC^\infty} M \to \map {\CC^\infty} M$ be a smooth vector field.


The divergence of $\mathbf X$ is defined as:

 
 
 
 

where:
:$\star_g$ is the Hodge star operator of $\struct {M, g}$
:$\d_{\d R}$ is de Rham differential.

=== Integral Form ===
Let $R$ be a region of space embedded in a Cartesian coordinate frame.

Let $\mathbf V$ be a vector field acting over $R$.


The divergence of $\mathbf V$ at a point $A$ in $R$ is defined as:

:$\ds \operatorname {div} \mathbf V := \lim_{\delta \tau \mathop \to 0} \dfrac {\int_S \mathbf V \cdot \d S} {\delta \tau}$

where:
:$S$ is the surface of a volume element $\delta \tau$ containing $A$
:$\cdot$ denotes the dot product
:$\ds \int_S$ denotes the surface integral over $S$.",Definition:Divergence Operator,"['Definitions/Divergence Operator', 'Definitions/Vector Analysis', 'Definitions/Vector Calculus']"
Definition:Divergent,Divergent,"A sequence which is not convergent is divergent.

 

=== Divergent Real Sequence ===
A real sequence which is not convergent is divergent.",Definition:Divergent Sequence,"['Definitions/Divergent Sequences', 'Definitions/Divergence', 'Definitions/Sequences']"
Definition:Divergent,Divergent,A series which is not convergent is divergent.,Definition:Divergent Series,"['Definitions/Divergent Series', 'Definitions/Divergence', 'Definitions/Series']"
Definition:Divergent,Divergent,A function which is not convergent is divergent.,Definition:Divergent Function,['Definitions/Real Analysis']
Definition:Divergent,Divergent,"An improper integral of a real function $f$ is said to diverge if any of the following hold:

:$(1): \quad f$ is continuous on $\hointr a \to$ and the limit $\ds \lim_{b \mathop \to +\infty} \int_a^b \map f x \rd x$ does not exist

:$(2): \quad f$ is continuous on $\hointl \gets b$ and the limit $\ds \lim_{a \mathop \to -\infty} \int_a^b \map f x \rd x$ does not exist

:$(3): \quad f$ is continuous on $\hointr a b$, has an infinite discontinuity at $b$, and the limit $\ds \lim_{c \mathop \to b^-} \int_a^c \map f x \rd x$ does not exist

:$(4): \quad f$ is continuous on $\hointl a b$, has an infinite discontinuity at $a$, and the limit $\ds \lim_{c \mathop \to a^+} \int_c^b \map f x \rd x$ does not exist.",Definition:Divergent Improper Integral,"['Definitions/Divergent Improper Integrals', 'Definitions/Improper Integrals', 'Definitions/Integral Calculus']"
Definition:Divergent,Divergent,"An infinite product which is not convergent is divergent.

 


=== Divergence to zero ===
Let $\struct {\mathbb K, \norm {\,\cdot\,} }$ be a valued field.

Let $\sequence {a_n}$ be a sequence of elements of $\mathbb K$.


If either:

:there exist infinitely many $n \in \N$ with $a_n = 0$

:there exists $n_0 \in \N$ with $a_n \ne 0$ for all $n > n_0$ and the sequence of partial products of $\ds \prod_{n \mathop = n_0 + 1}^\infty a_n$ converges to $0$

the product diverges to $0$, and we assign the value:
:$\ds \prod_{n \mathop = 1}^\infty a_n = 0$


Category:Definitions/Divergent Products
Category:Definitions/Infinite Products",Definition:Divergent Product,"['Definitions/Divergent Products', 'Definitions/Infinite Products', 'Definitions/Divergence']"
Definition:Divisor,Divisor,"Let $\struct {\Z, +, \times}$ be the ring of integers.

Let $x, y \in \Z$.


Then $x$ divides $y$ is defined as:
:$x \divides y \iff \exists t \in \Z: y = t \times x$


=== Aliquot Part ===
An aliquot part of an integer $n$ is a divisor of $n$ which is strictly less than $n$.

=== Aliquant Part ===
An aliquant part of an integer $n$ is a positive integer which is less than $n$ but is not a divisor of $n$.",Definition:Divisor (Algebra)/Integer,"['Definitions/Divisors', 'Definitions/Number Theory']"
Definition:Divisor,Divisor,"Let $n \in \Z$ be an integer.

Then $p$ is a prime factor of $n$  if and only if :
: $(1): \quad p$ is a prime number
: $(2): \quad p$ is a divisor (that is, factor) of $n$.",Definition:Prime Factor,['Definitions/Prime Numbers']
Definition:Divisor,Divisor,"Let $\struct {R, +, \circ}$ be an ring with unity whose zero is $0_R$ and whose unity is $1_R$.

Let $x, y \in D$.

We define the term $x$ divides $y$ in $R$ as follows:
:$x \mathrel {\divides_R} y \iff \exists t \in R: y = t \circ x$


When no ambiguity results, the subscript is usually dropped, and $x$ divides $y$ in $R$ is just written $x \divides y$.",Definition:Divisor (Algebra)/Ring with Unity,"['Definitions/Divisibility', 'Definitions/Factorization']"
Definition:Divisor,Divisor,"Let $D$ be an integral domain.

Let $D \sqbrk x$ be the polynomial ring in one variable over $D$.

Let $f, g \in D \sqbrk x$ be polynomials.


Then:
:$f$ divides $g$
:$f$ is a divisor of $g$
:$g$ is divisible by $f$
 if and only if :
:$\exists h \in D \sqbrk x : g = f h$


This is denoted:
:$f \divides g$


=== Notation ===
The conventional notation for $x$ is a divisor of $y$ is ""$x \mid y$"", but there is a growing trend to follow the notation ""$x \divides y$"", as espoused by   etc.

From  :
:The notation '$m \mid n$' is actually much more common than '$m \divides n$' in current mathematics literature. But vertical lines are overused -- for absolute values, set delimiters, conditional probabilities, etc. -- and backward slashes are underused. Moreover, '$m \divides n$' gives an impression that $m$ is the denominator of an implied ratio. So we shall boldly let our divisibility symbol lean leftward.


An unfortunate unwelcome side-effect of this notational convention is that to indicate non-divisibility, the conventional technique of implementing $/$ through the notation looks awkward with $\divides$, so $\not \! \backslash$ is eschewed in favour of $\nmid$.


Some sources use $\ \vert \mkern -10mu {\raise 3pt -} \ $ or similar to denote non-divisibility.


Category:Definitions/Divisors",Definition:Divisor of Polynomial,"['Definitions/Divisors of Polynomials', 'Definitions/Divisors', 'Definitions/Polynomial Theory']"
Definition:Divisor,Divisor,"Let $c = a / b$ denote the division operation on two elements $a$ and $b$ of a field or a Euclidean domain.

The element $b$ is the divisor of $a$.",Definition:Division/Divisor,['Definitions/Division']
Definition:Domain,Domain,"=== Relation ===
Let $\RR \subseteq S \times T$ be a relation.

The domain of $\RR$ is defined and denoted as:
:$\Dom \RR := \set {s \in S: \exists t \in T: \tuple {s, t} \in \RR}$


That is, it is the same as what is defined here as the preimage of $\RR$.


=== General Definition ===
Let $\ds \prod_{i \mathop = 1}^n S_i$ be the cartesian product of sets $S_1$ to $S_n$.

Let $\ds \RR \subseteq \prod_{i \mathop = 1}^n S_i$ be an $n$-ary relation on $\ds \prod_{i \mathop = 1}^n S_i$.

The domain of $\RR$ is the set defined as:
:$\ds \Dom \RR := \set {\tuple {s_1, s_2, \ldots, s_{n - 1} } \in \prod_{i \mathop = 1}^{n - 1} S_i: \exists s_n \in S_n: \tuple {s_1, s_2, \ldots, s_n} \in \RR}$


The concept is usually encountered when $\RR$ is an endorelation on $S$:
:$\ds \Dom \RR := \set {\tuple {s_1, s_2, \ldots, s_{n - 1} } \in S^{n - 1}: \exists s_n \in S_n: \tuple {s_1, s_2, \ldots, s_n} \in \RR}$

=== Class Theory ===
 
Let $V$ be a basic universe.

Let $\RR \subseteq V \times V$ be a relation in $V$.

The domain of $\RR$ is defined and denoted as:
:$\Dom \RR := \set {x \in V: \exists y \in V: \tuple {x, y} \in \RR}$

That is, it is the class of all $x$ such that $\tuple {x, y} \in \RR$ for at least one $y$.

=== Mapping ===

The term domain is usually seen when the relation in question is actually a mapping.

Let $f: S \to T$ be a mapping.

The domain of $f$ is $S$, and can be denoted $\Dom f$.

=== Binary Operation ===
Let $\circ: S \times S \to T$ be a binary operation.

The domain of $\circ$ is the set $S$ and can be denoted $\Dom \circ$.


This definition can be considered as the same as that for the domain of a mapping, where the domain would be defined as $S \times S$.

Category:Definitions/Abstract Algebra",Definition:Domain (Relation Theory),['Definitions/Relation Theory']
Definition:Domain,Domain,"The collection of all possible objects that a variable may refer to has to be specified.

This collection is the domain of the variable.",Definition:Variable/Domain,"['Definitions/Algebra', 'Definitions/Variables']"
Definition:Domain,Domain,"Let $f: X \to Y$ be a morphism.

Then the domain of $f$ is defined to be the object $X$.

This is usually denoted $X = \Dom f$ or $X = \map D f$.",Definition:Domain (Category Theory),['Definitions/Morphisms']
Definition:Domain,Domain,"=== Definition 1 ===
An integral domain $\struct {D, +, \circ}$ is:

:a commutative ring which is non-null
:with a unity
:in which there are no (proper) zero divisors, that is:
::: $\forall x, y \in D: x \circ y = 0_D \implies x = 0_D \text{ or } y = 0_D$

that is, in which all non-zero elements are cancellable.

=== Definition 2 ===
An integral domain $\struct {D, +, \circ}$ is a commutative ring such that $\struct {D^*, \circ}$ is a monoid, all of whose elements are cancellable.

In this context, $D^*$ denotes the ring $D$ without zero: $D \setminus \set {0_D}$.

=== Integral Domain Axioms ===
 ",Definition:Integral Domain,"['Definitions/Integral Domains', 'Definitions/Ring Theory']"
Definition:Dominate,Dominate,"Let $S$ and $T$ be sets.


=== Definition 1 ===
Let $S$ and $T$ be sets.


Then $S$ is dominated by $T$  if and only if  there exists an injection from $S$ to $T$.


The notation $S \preccurlyeq T$ is used to indicate that $S$ dominates $T$.

=== Definition 2 ===
Let $S$ and $T$ be sets.


Then $S$ is dominated by $T$  if and only if  $S$ is equivalent to some subset of $T$.

That is,  if and only if  there exists a bijection $f: S \to T'$ for some $T' \subseteq T$.

The notation $S \preccurlyeq T$ is used to indicate that $S$ dominates $T$.

=== Strictly Dominated ===
Let $S, T$ be sets.


$S$ is strictly dominated by set $T$  if and only if  $S \preccurlyeq T$ but $\neg T \preccurlyeq S$.

This can be written $S \prec T$ or $S < T$.


Category:Definitions/Set Theory",Definition:Dominate (Set Theory),['Definitions/Set Theory']
Definition:Dominate,Dominate,"Let $\sequence {a_n}$ be a sequence in $\R$.

Let $\sequence {z_n}$ be a sequence in $\C$.


Then $\sequence {a_n}$ dominates $\sequence {z_n}$  if and only if :
:$\forall n \in \N: \cmod {z_n} \le a_n$

Category:Definitions/Analysis
Category:Definitions/Complex Analysis",Definition:Dominate (Analysis),"['Definitions/Analysis', 'Definitions/Complex Analysis']"
Definition:Dram,Dram,"The dram is an avoirdupois unit of mass.

=== Conversion Factors ===
",Definition:Avoirdupois/Dram,['Definitions/Dram (Avoirdupois)']
Definition:Dram,Dram,"The drachm is an apothecaries' unit of mass.

=== Conversion Factors ===
",Definition:Apothecaries' Weights and Measures/Mass/Drachm,['Definitions/Drachm']
Definition:Dual,Dual,"=== Inverse of Complement ===
Let $\RR \subseteq S \times T$ be a binary relation.


Then the dual of $\RR$ is denoted $\RR^d$ and is defined as:

:$\RR^d := \paren {\overline \RR}^{-1}$

where:
:$\overline \RR$ denotes the complement of $\RR$
:$\paren {\overline \RR}^{-1}$ denotes the inverse of the complement of $\RR$.

=== Complement of Inverse ===
Let $\RR \subseteq S \times T$ be a binary relation.


Then the dual of $\RR$ is denoted $\RR^d$ and is defined as:

:$\RR^d := \overline {\paren {\RR^{-1} } }$

where:
:$\RR^{-1}$ denotes the inverse of $\RR$
:$\overline {\paren {\RR^{-1} } }$ denotes the complement of the inverse of $\RR$.


Category:Definitions/Relation Theory",Definition:Dual Relation,['Definitions/Relation Theory']
Definition:Dual,Dual,"Let $\struct {S, \preceq}$ be an ordered set.

Let $\succeq$ be the inverse relation to $\preceq$.

That is, for all $a, b \in S$:

:$a \succeq b$  if and only if  $b \preceq a$


Then $\succeq$ is called the dual ordering of $\preceq$.


=== Dual Ordered Set ===
Let $\struct {S, \preceq}$ be an ordered set.

Let $\succeq$ be the dual ordering of $\preceq$.


The ordered set $\struct {S, \succeq}$ is called the dual ordered set (or just dual) of $\struct {S, \preceq}$.


That it indeed is an ordered set is a consequence of Dual Ordering is Ordering.

=== Notation for Dual Ordering ===
To denote the dual of an ordering, the conventional technique is to reverse the symbol.

Thus:
:$\succeq$ denotes $\preceq^{-1}$
:$\succcurlyeq$ denotes $\preccurlyeq^{-1}$
:$\curlyeqsucc$ denotes $\curlyeqprec^{-1}$

and so:
:$a \preceq b \iff b \succeq a$
:$a \preccurlyeq b \iff b \succcurlyeq a$
:$a \curlyeqprec b \iff b \curlyeqsucc a$


Similarly for the standard symbols used to denote an ordering on numbers:
:$\ge$ denotes $\le^{-1}$
:$\geqslant$ denotes $\leqslant^{-1}$
:$\eqslantgtr$ denotes $\eqslantless^{-1}$

and so on.

=== Notation for Dual Strict Ordering ===
To denote the dual of an strict ordering, the conventional technique is to reverse the symbol.

Thus:
:$\succ$ denotes $\prec^{-1}$

and so:
:$a \prec b \iff b \succ a$


Similarly for the standard symbol used to denote a strict ordering on numbers:
:$>$ denotes $<^{-1}$

and so on.",Definition:Dual Ordering,"['Definitions/Dual Orderings', 'Definitions/Order Theory']"
Definition:Dual,Dual,"Let $\struct {S, \preceq}$ be an ordered set.

Let $\succeq$ be the dual ordering to $\preceq$.

Let $\Sigma$ be any statement pertaining to $\struct {S, \preceq}$ (be it in natural language or a formal language).


The dual statement of $\Sigma$, denoted $\Sigma^*$, is the statement obtained from replacing every reference to $\preceq$ in $\Sigma$ with a reference to its dual $\succeq$.

This dual statement may then be turned into a statement about $\preceq$ again by applying the equivalences on Dual Pairs (Order Theory).",Definition:Dual Statement (Order Theory),['Definitions/Order Theory']
Definition:Dual,Dual,"Let $\struct {S, \preceq_S}$ and $\struct {T, \preceq_T}$ be ordered sets.

Let $\phi: S \to T$ be a bijection.


Then $\phi$ is a dual isomorphism between $\struct {S, \preceq_S}$ and $\struct {T, \preceq_T}$  if and only if  $\phi$ and $\phi^{-1}$ are decreasing mappings.


If there is a dual isomorphism between $\struct {S, \preceq_S}$ and $\struct {T, \preceq_T}$, then $\struct {S, \preceq_S}$ is dual to $\struct {T, \preceq_T}$.

Equivalently, $\struct {S, \preceq_S}$ is dual to $\struct {T, \preceq_T}$  if and only if  $S$ with the dual ordering is isomorphic to $T$.",Definition:Dual Isomorphism (Order Theory),['Definitions/Order Theory']
Definition:Dual,Dual,"Let $\struct {S, \preceq_1}$ and $\struct {T, \preceq_2}$ be ordered sets.


A dual order embedding is a mapping $\phi: S \to T$ such that:

:$\forall x, y \in S: x \preceq_1 y \iff \map \phi y \preceq_2 \map \phi x$


That is:
:if $\phi$ is an order embedding of $\struct {S, \preceq_1}$ into $\struct {T, \succeq_2}$
where $\succeq_2$ is the dual of $\preceq_2$.

Category:Definitions/Order Embeddings",Definition:Dual Order Embedding,['Definitions/Order Embeddings']
Definition:Dual,Dual,"Let $\left({S, \preceq}\right)$ be an ordered set.

If $S$ is dual to $S$, then $S$ is self-dual.",Definition:Self-Dual (Order Theory),['Definitions/Order Theory']
Definition:Dual,Dual,"Let $\struct {R, +, \times}$ be a commutative ring.

Let $\struct {G, +_G, \circ}_R$ be an $R$-module.

Let $\struct {R, +_R, \circ}_R$ denote the $R$-module $R$.


The $R$-module $\map {\LL_R} {G, R}$ of all linear forms on $G$ is usually denoted $G^*$ and is called the algebraic dual of $G$.


=== Double Dual ===
Let $R$ be a commutative ring.

Let $G$ be an $R$-module.


The double dual $G^{**}$ of $G$ is the dual of its dual $G^*$.


Category:Definitions/Algebraic Duals",Definition:Algebraic Dual,"['Definitions/Linear Forms (Linear Algebra)', 'Definitions/Algebraic Duals']"
Definition:Dual,Dual,"Let $R$ be a commutative ring.

Let $G$ be an $R$-module.


The double dual $G^{**}$ of $G$ is the dual of its dual $G^*$.


Category:Definitions/Algebraic Duals",Definition:Algebraic Dual/Double Dual,['Definitions/Algebraic Duals']
Definition:Dual,Dual,"Let $R$ be a commutative ring with unity.

Let $\struct {G, +_G, \circ}_R$ be an $n$-dimensional module over $R$.

Let $\sequence {a_n}$ be an ordered basis of $G$.

Let $G^*$ be the algebraic dual of $G$.


Then there is an ordered basis $\sequence {a'_n}$ of $G^*$ satisfying $\forall i, j \in \closedint 1 n: \map {a'_i} {a_j} = \delta_{i j}$.


This ordered basis $\sequence {a'_n}$ of $G^*$ is called the ordered basis of $G^*$ dual to $\sequence {a_n}$, or the ordered dual basis of $G^*$.",Definition:Ordered Dual Basis,['Definitions/Linear Algebra']
Definition:Dual,Dual,"Let $V$ be a vector space.

Let $\phi: V \to \R$ be a linear mapping.


The set of all $\phi$ is called a dual space (of $V$) and is denoted by $V^*$.",Definition:Dual Vector Space,"['Definitions/Vector Spaces', 'Definitions/Algebraic Duals']"
Definition:Dual,Dual,"Let $\struct {X, \norm \cdot_X}$ be a normed vector space.

Let $X^\ast$ be the vector space of bounded linear functionals on $X$. 

Let $\norm \cdot_{X^\ast}$ be the norm on bounded linear functionals.


We say that $\struct {X^\ast, \norm \cdot_{X^\ast} }$ is the normed dual space of $X$.",Definition:Normed Dual Space,"['Definitions/Normed Vector Spaces', 'Definitions/Functional Analysis', 'Definitions/Normed Dual Spaces']"
Definition:Dual,Dual,"Let $\struct {X, \norm \cdot_X}$ be a normed vector space.

Let $\struct {X^\ast, \norm \cdot_{X^\ast} }$ be the normed dual of $\struct {X, \norm \cdot_X}$.


We define the second normed dual, written $\struct {X^{\ast \ast}, \norm \cdot_{X^{\ast \ast} } }$ as the normed dual of $\struct {X^\ast, \norm \cdot_{X^\ast} }$.",Definition:Second Normed Dual,"['Definitions/Normed Dual Spaces', 'Definitions/Second Normed Duals']"
Definition:Dual,Dual,"Let $\mathbf C$ be a metacategory.


Its dual category, denoted $\mathbf C^{\text{op} }$, is defined as follows:

 

It can be seen that this comes down to the metacategory obtained by reversing the direction of all morphisms of $\mathbf C$.",Definition:Dual Category,"['Definitions/Category Theory', 'Definitions/Examples of Categories']"
Definition:Dual,Dual,"=== Morphisms-Only Category Theory ===

Let $\Sigma$ be a statement in the language of category theory.

The dual statement $\Sigma^*$ of $\Sigma$ is the statement obtained from substituting:

 
 
 
 
 


=== Object Category Theory ===

In the more convenient description of metacategories by using objects, the dual statement $\Sigma^*$ of $\Sigma$ then becomes the statement obtained from substituting:

 
 
 
 
 
 


=== Example ===

For example, if $\Sigma$ is the statement:

:$\exists g: g \circ f = \operatorname{id}_{\Dom f}$

describing that $f$ is a split mono, then $\Sigma^*$ becomes:

:$\exists g: f \circ g = \operatorname{id}_{\Cdm f}$

which precisely expresses $f$ to be a split epi.


For a set $\EE$ of statements, write:

:$\EE^* := \set {\Sigma^*: \Sigma \in \EE}$ 

for the set comprising of the dual statement of those in $\EE$.",Definition:Dual Statement (Category Theory),['Definitions/Category Theory']
Definition:Dual,Dual,"Let $P$ be a polyhedron.

The dual polyhedron $D$ of $P$ is the polyhedron which can be constructed as follows:

:$(1): \quad$ The vertices of $D$ are the centroids of the faces of $P$.

:$(2): \quad$ For each edge of $P$ which is adjacent to two faces $F_1$ and $F_2$ of $P$, an edge of $D$ is constructed which is adjacent to the vertices of $D$ forming the centroids of $F_1$ and $F_2$.",Definition:Dual Polyhedron,['Definitions/Polyhedra']
Definition:Eccentricity,Eccentricity,"Let $K$ be a conic section specified in terms of:
:a given straight line $D$
:a given point $F$
:a given constant $e$

where $K$ is the locus of points $P$ such that the distance $p$ from $P$ to $D$ and the distance $q$ from $P$ to $F$ are related by the condition:
:$q = e p$


The constant $e$ is known as the eccentricity of the conic section.",Definition:Conic Section/Eccentricity,"['Definitions/Eccentricity of Conic Section', 'Definitions/Conic Sections']"
Definition:Eccentricity,Eccentricity,"Let $G = \struct {V, E}$ be a graph.

Let $v \in V$ be a vertex of $G$.


The eccentricity of $v$ is the maximum distance from $v$ to another vertex of $G$:


That is:
:$\map E v = \ds \max_{u \mathop \in V} \map D {v, u}$

where $\map D {v, u}$ denotes the distance from $v$ to $u$.",Definition:Eccentricity of Vertex,['Definitions/Graph Theory']
Definition:Edge,Edge," 


Let $G = \struct {V, E}$ be a graph.

The edges are the elements of $E$.


In the above, the edges are $AB, AE, BE, CD, CE, CF, DE, DF, FG$.


=== Join ===
Let $G = \struct {V, E}$ be a graph.

Let $u$ and $v$ be vertices of $G$.

Let $e = u v$ be an edge of $G$.


Then $e$ joins the vertices $u$ and $v$.",Definition:Graph (Graph Theory)/Edge,"['Definitions/Edges of Graphs', 'Definitions/Edges']"
Definition:Edge,Edge,The edges of a polyhedron are the sides of the polygons which constitute its faces.,Definition:Polyhedron/Edge,"['Definitions/Edges of Polyhedra', 'Definitions/Polyhedra', 'Definitions/Edges']"
Definition:Edge,Edge,Each of the half-lines that form a polyhedral angle is known as an edge of the polyhedral angle.,Definition:Polyhedral Angle/Edge,"['Definitions/Polyhedral Angles', 'Definitions/Edges']"
Definition:Edge,Edge,"Let $\PP$ denote the plane.

Let $\LL$ denote an infinite straight line in $\PP$.

Let $\HH$ denote one of the half-planes into which $\LL$ divides $\PP$.

Then $\LL$ is called the edge of $\HH$.",Definition:Half-Plane/Edge,"['Definitions/Half-Planes', 'Definitions/Edges']"
Definition:Edge,Edge,"Let $\mathbf C$ be a metacategory.


A morphism of $\mathbf C$ is an object $f$, together with:

* A domain $\operatorname {dom} f$, which is an object of $\mathbf C$
* A codomain $\operatorname {cod} f$, also an object of $\mathbf C$


The collection of all morphisms of $\mathbf C$ is denoted $\mathbf C_1$.


If $A$ is the domain of $f$ and $B$ is its codomain, this is mostly represented by writing:

:$f: A \to B$ or $A \stackrel f \longrightarrow B$",Definition:Morphism,"['Definitions/Morphisms', 'Definitions/Category Theory']"
Definition:Efficiency,Efficiency,"Let $T_0$ and $T_1$ both be statistics used as estimators.

Efficiency is a comparison of the variances of $T_0$ and $T_1$.


Thus $T_0$ is of higher efficiency than $T_1$  if and only if  $T_0$ has a smaller variance than $T_1$.",Definition:Efficiency (Statistics),"['Definitions/Efficiency (Statistics)', 'Definitions/Statistics', 'Definitions/Efficiency']"
Definition:Efficiency,Efficiency,One design is more efficient than another if the same precision can be achieved with the same resources.,Definition:Efficiency (Experimental Design),"['Definitions/Efficiency (Experimental Design)', 'Definitions/Experimental Designs', 'Definitions/Efficiency']"
Definition:Efficiency,Efficiency,"Let $S$ be a system.

=== Definition 1 ===
Let $S$ be a system.


The efficiency of $S$ is defined as:
:$\EE = \dfrac {E_O} {E_I} \times 100 \%$
where:
:$E_I$ denotes the energy input to $S$ over a particular time interval $T$
:$E_O$ denotes the energy output from $S$ over that same time interval $T$.

=== Definition 2 ===
Let $S$ be a system.


The efficiency of $S$ is defined as:
:$\EE = \dfrac {W_L} {W_E} \times 100 \%$
where:
:$W_L$ denotes the work done by the load of $S$
:$W_E$ denotes the work done by the effort of $S$.",Definition:Efficiency (Physics),"['Definitions/Efficiency (Physics)', 'Definitions/Physics', 'Definitions/Efficiency']"
Definition:Elevation,Elevation,The elevation of a point $P$ is the height of $P$ above some reference horizontal baseline or plane.,Definition:Elevation of Point,"['Definitions/Elevation of Point', 'Definitions/Elevation']"
Definition:Elevation,Elevation,"Let $A$ and $B$ be points in space such that $A$ is higher than $B$.

The angle of elevation of $A$ from $B$ is the angle between the line $AB$ and the horizontal.",Definition:Angle of Elevation,"['Definitions/Angles of Elevation', 'Definitions/Angles', 'Definitions/Elevation']"
Definition:Embedding,Embedding,"Let $A, B$ be topological spaces.

Let $f: A \to B$ be a mapping.

Let the image of $f$ be given the subspace topology.

Let the restriction $f {\restriction_{A \times f\sqbrk A }}$ of $f$ to its image be a homeomorphism.


Then $f$ is an embedding (of $A$ into $B$).",Definition:Embedding (Topology),"['Definitions/Embeddings (Topology)', 'Definitions/Homeomorphisms (Topological Spaces)', 'Definitions/Topology']"
Definition:Embedding,Embedding,"Let $\MM$ and $\NN$ be $\LL$-structures with universes $M$ and $N$ respectively.


$j: \MM \to \NN$ is an $\LL$-embedding  if and only if  it is an injective map $M \to N$ which preserves interpretations of all symbols in $\LL$; that is, such that:
:$\map j {\map {f^\MM} {a_1, \dots, a_{n_f} } } = \map {f^\NN} {\map j {a_1}, \ldots, \map j {a_{n_f} } }$ for all function symbols $f$ in $\LL$ and $a_1, \dots, a_{n_f}$ in $M$
:$\tuple {a_1, \ldots, a_{n_R} } \in R^\MM \iff \tuple {\map j {a_1}, \dots, \map j {a_{n_R} } } \in R^\NN$ for all relation symbols $R$ in $\LL$ and $a_1, \dots, a_{n_R}$ in $M$
:$\map j {c^\MM} = c^\NN$ for all constant symbols $c$ in $\LL$.


=== Partial Embedding ===

A common method of constructing isomorphisms and elementary embeddings in proofs is to recursively define them a finite number of elements at a time.  For this  purpose, it is useful to have a definition of embeddings using functions which are only defined on a subset of $M$:


Let $A \subseteq M$ be a subset of $M$.


$j: A \to \NN$ is a partial $\LL$-embedding  if and only if  it is an injective map $A \to N$ which preserves interpretations of all symbols in $\LL$ applied to elements of $A$; that is, such that:
:$\map j {\map {f^\MM} {a_1, \dots, a_{n_f} } } = \map {f^\NN} {\map j {a_1}, \ldots, \map j {a_{n_f} } }$ for  all function symbols $f$ in $\LL$ and $a_1, \dots, a_{n_f}$ in $A$
:$\tuple {a_1, \ldots, a_{n_R} } \in R^\MM \iff \tuple {\map j {a_1}, \dots, \map j {a_{n_R} } } \in R^\NN$ for all relation symbols $R$ in $\LL$ and $a_1, \dots, a_{n_R}$ in $A$
:$\map j {c^\MM} = c^\NN$ for all constant symbols $c$ in $\LL$.


=== Isomorphism ===

$j: \MM \to \NN$ is an $\LL$-isomorphism  if and only if  it is a bijective $\LL$-embedding.


=== Automorphism ===

$j: \MM \to \NN$ is an $\LL$-automorphism  if and only if  it is an $\LL$-isomorphism and $\MM = \NN$.


It is often useful to talk about automorphisms which are constant on subsets of $M$.  So, there is a definition and a notation for doing so:

Let $A \subseteq M$ be a subset of $M$, and let $b \in M$.


An $\LL$-automorphism $j$ is an $A$-automorphism  if and only if  $\map j a = a$ for all $a\in A$.

An $\LL$-automorphism $j$ is an $A, b$-automorphism  if and only if  it is an $\paren {A \cup \set b}$-automorphism; that is: $\map j a = a$ for all $a \in A$ and also $\map j b = b$.",Definition:Embedding (Model Theory),"['Definitions/Model Theory for Predicate Logic', 'Definitions/Automorphisms']"
Definition:Embedding,Embedding,"Let $K$ and $L$ be fields.

A (field) monomorphism $\phi: K \to L$ is called an embedding of $K$ in $L$.",Definition:Embedding (Galois Theory),"['Definitions/Galois Theory', 'Definitions/Field Theory']"
Definition:Embedding,Embedding,"Let $\struct {R, +, \circ}$ and $\struct {S, \oplus, *}$ be rings.

Let $\phi: R \to S$ be a (ring) homomorphism.


Then $\phi$ is a ring monomorphism  if and only if  $\phi$ is an injection.",Definition:Ring Monomorphism,"['Definitions/Monomorphisms (Abstract Algebra)', 'Definitions/Ring Homomorphisms']"
Definition:Embedding,Embedding,"Let $C$ and $D$ be categories.

Let $F : C \to D$ be a functor.


=== Definition 1 ===

The functor $F$ is an embedding  if and only if  it is:
* injective on objects
* faithful


=== Definition 2 ===

The functor $F$ is an embedding  if and only if  it is injective on morphisms.


=== Definition 3 ===

The functor $F$ is an embedding  if and only if  it is a monomorphisms in the category of categories.",Definition:Embedding of Categories,['Definitions/Category Theory']
Definition:Empty,Empty,"The empty set is a set which has no elements.

That is, $x \in \O$ is false, whatever $x$ is.


It is usually denoted by some variant of a zero with a line through it, for example $\O$ or $\emptyset$, and can always be represented as $\set {}$.",Definition:Empty Set,"['Definitions/Empty Set', 'Definitions/Set Theory']"
Definition:Empty,Empty,"A class is defined as being empty  if and only if  it has no elements.

That is:
:$\forall x: x \notin A$
or:
:$\neg \exists x: x \in A$


The empty class is usually denoted $\O$ or $\emptyset$.

On   the preferred symbol is $\O$.",Definition:Empty Class (Class Theory),['Definitions/Class Theory']
Definition:Empty,Empty,"Let $T$ be a set.


Then the mapping $e: \O \to T$ whose domain is the empty set and whose codomain is $T$ is called the empty mapping:
:$e \subseteq \O \times T = \O$",Definition:Empty Mapping,['Definitions/Mapping Theory']
Definition:Empty,Empty,"The null relation is a relation $\RR$ in $S$ to $T$ such that $\RR$ is the empty set:
:$\RR \subseteq S \times T: \RR = \O$


That is, no element of $S$ relates to any element in $T$:
:$\RR: S \times T: \forall \tuple {s, t} \in S \times T: \neg s \mathrel \RR t$",Definition:Null Relation,"['Definitions/Null Relation', 'Definitions/Empty Set', 'Definitions/Examples of Relations']"
Definition:Empty,Empty,"An edgeless graph is a graph with no edges.

That is, an edgeless graph is a graph of size zero.

Equivalently, an edgeless graph is a graph whose vertices are all isolated.


The edgeless graph of order $n$ is denoted $N_n$ and can be referred to as the $n$-edgeless graph.",Definition:Edgeless Graph,"['Definitions/Edgeless Graphs', 'Definitions/Graph Theory']"
Definition:Empty,Empty,"Take the summation:
:$\ds \sum_{\map \Phi j} a_j$
where $\map \Phi j$ is a propositional function of $j$.

Suppose that there are no values of $j$ for which $\map \Phi j$ is true.

Then $\ds \sum_{\map \Phi j} a_j$ is defined as being $0$.

This summation is called a vacuous summation.


This is because:
:$\forall a: a + 0 = a$
where $a$ is a number.

Hence for all $j$ for which $\map \Phi j$ is false, the sum is unaffected.


This is most frequently seen in the form:
:$\ds \sum_{j \mathop = m}^n a_j = 0$
where $m > n$.

In this case, $j$ can not at the same time be both greater than or equal to $m$ and less than or equal to $n$.


Some sources consider such a treatment as abuse of notation.",Definition:Summation/Vacuous Summation,['Definitions/Summations']
Definition:Empty,Empty,A class interval is empty  if and only if  it is of frequency zero.,Definition:Class Interval/Empty,['Definitions/Class Intervals']
Definition:Empty,Empty,"The category $\mathbf 0$, zero, is the empty category:


:$\qquad$


with:
:no objects
and consequently:
:no morphisms.",Definition:Zero (Category),['Definitions/Examples of Categories']
Definition:Empty Class,Empty Class,"A class is defined as being empty  if and only if  it has no elements.

That is:
:$\forall x: x \notin A$
or:
:$\neg \exists x: x \in A$


The empty class is usually denoted $\O$ or $\emptyset$.

On   the preferred symbol is $\O$.",Definition:Empty Class (Class Theory),['Definitions/Class Theory']
Definition:Empty Class,Empty Class,A class interval is empty  if and only if  it is of frequency zero.,Definition:Class Interval/Empty,['Definitions/Class Intervals']
Definition:Entropy,Entropy,"Let $X$ be a discrete random variable.

Let $X$ take a finite number of values with probabilities $p_1, p_2, \dotsc, p_n$.


The uncertainty of $X$ is defined as:

:$\map H X = \ds -\sum_k p_k \lg p_k$

where:
:$\lg$ denotes logarithm base $2$
:the summation is over those $k$ where $p_k > 0$.


=== Units ===
The unit of measurement used to quantify uncertainty is the bit.


Category:Definitions/Uncertainty",Definition:Uncertainty,"['Definitions/Uncertainty', 'Definitions/Information Theory', 'Definitions/Probability Theory']"
Definition:Entropy,Entropy,"Differential entropy extends the concept of entropy to continuous random variables.

Let $X$ be a continuous random variable.

Let $X$ have probability density function $f_X$. 

Then the differential entropy of $X$, $\map h X$ measured in nats, is given by: 

:$\ds \map h X = -\int_{-\infty}^\infty \map {f_X} x \ln \map {f_X} x \rd x$


Where $\map {f_X} x = 0$, we take $\map {f_X} x \ln \map {f_X} x = 0$ by convention.",Definition:Differential Entropy,['Definitions/Probability Theory']
Definition:Entropy,Entropy,"Entropy is a property of a thermodynamic system.

It quantifies the number $\Omega$ of microstates that are consistent with the macroscopic quantities that characterize the system.


The entropy of a system is equal to the expectation of the value:
:$k \ln P$
where:
:$k$ is a constant which relates the mean kinetic energy and absolute temperature of the system
:$P$ is the coefficient of probability of the system.


 ",Definition:Entropy (Physics),"['Definitions/Physics', 'Definitions/Physical Quantities', 'Definitions/Examples of Scalar Quantities']"
Definition:Epicycle,Epicycle,"An epicycle is the orbit described by a body moving in a uniform circular motion around a point which is itself moving in a uniform circular motion around another point.

:

That point may itself also be moving in a uniform circular motion around yet another point.",Definition:Epicycle (Ptolemaic Astronomy),"['Definitions/Geometry', 'Definitions/Circles', 'Definitions/Epicycles']"
Definition:Epicycle,Epicycle,"Let an epicycloid be generated by rolling a circle $C_1$ around the outside of another circle $C_2$.


:


The circle $C_1$ can be referred to as the epicycle of the epicycloid.",Definition:Epicycloid/Generator/Epicycle,['Definitions/Epicycloids']
Definition:Epicycle,Epicycle,"Let a hypocycloid be generated by rolling circle $C_1$ around the inside of another (larger) circle $C_2$.


:


The circle $C_1$ can be referred to as the epicycle of the hypocycloid.",Definition:Hypocycloid/Generator/Epicycle,"['Definitions/Hypocycloids', 'Definitions/Epicycles']"
Definition:Epimorphism,Epimorphism,"A homomorphism which is a surjection is an epimorphism.


=== Semigroup Epimorphism ===
Let $\struct {S, \circ}$ and $\struct {T, *}$ be semigroups.

Let $\phi: S \to T$ be a (semigroup) homomorphism.


Then $\phi$ is a semigroup epimorphism  if and only if  $\phi$ is a surjection.

=== Monoid Epimorphism ===
Let $\struct {S, \circ}$ and $\struct {T, *}$ be monoids.

Let $\phi: S \to T$ be a (monoid) homomorphism.


Then $\phi$ is a monoid epimorphism  if and only if  $\phi$ is a surjection.

=== Group Epimorphism ===
Let $\struct {G, \circ}$ and $\struct {H, *}$ be groups.

Let $\phi: G \to H$ be a (group) homomorphism.


Then $\phi$ is a group epimorphism  if and only if  $\phi$ is a surjection.

=== Ring Epimorphism ===
Let $\struct {R, +, \circ}$ and $\struct {S, \oplus, *}$ be rings.

Let $\phi: R \to S$ be a (ring) homomorphism.


Then $\phi$ is a ring epimorphism  if and only if  $\phi$ is a surjection.

=== Field Epimorphism ===
Let $\struct {F, +, \circ}$ and $\struct {K, \oplus, *}$ be fields.

Let $\phi: R \to S$ be a (field) homomorphism.


Then $\phi$ is a field epimorphism  if and only if  $\phi$ is a surjection.

=== $R$-Algebraic Structure Epimorphism ===
Let $\struct {S, \ast_1, \ast_2, \ldots, \ast_n, \circ}_R$ and $\struct {T, \odot_1, \odot_2, \ldots, \odot_n, \otimes}_R$ be $R$-algebraic structures.


Then $\phi: S \to T$ is an $R$-algebraic structure epimorphism  if and only if :

:$(1): \quad \phi$ is a surjection
:$(2): \quad \forall k: k \in \closedint 1 n: \forall x, y \in S: \map \phi {x \ast_k y} = \map \phi x \odot_k \map \phi y$
:$(3): \quad \forall x \in S: \forall \lambda \in R: \map \phi {\lambda \circ x} = \lambda \otimes \map \phi x$


This definition also applies to modules, and also to vector spaces.",Definition:Epimorphism (Abstract Algebra),"['Definitions/Epimorphisms (Abstract Algebra)', 'Definitions/Homomorphisms (Abstract Algebra)', 'Definitions/Surjections', 'Definitions/Epimorphisms']"
Definition:Epimorphism,Epimorphism,"Let $\mathbf C$ be a metacategory.

An epimorphism is a morphism $f \in \mathbf C_1$ such that:

: $g \circ f = h \circ f \implies g = h$

for all morphisms $g, h \in \mathbf C_1$ for which these compositions are defined.


That is, an epimorphism is a morphism which is right cancellable. 



One writes $f: C \twoheadrightarrow D$ to denote that $f$ is an epimorphism.",Definition:Epimorphism (Category Theory),['Definitions/Category Theory']
Definition:Equator,Equator,"The (geographical) equator is the great circle described on the surface of Earth whose plane is perpendicular to Earth's axis of rotation.


:",Definition:Geographical Equator,"['Definitions/Geographical Equator', 'Definitions/Geographical Coordinates', 'Definitions/Equator']"
Definition:Equator,Equator,"Consider the celestial sphere with observer $O$.

Let $P$ be the north celestial pole.


The great circle whose plane is perpendicular to $OP$ is known as the celestial equator.


:",Definition:Celestial Equator,"['Definitions/Celestial Equator', 'Definitions/Celestial Sphere', 'Definitions/Equator']"
Definition:Equator,Equator,"The galactic equator is the great circle that is the intersection of the plane of the Milky Way with the celestial sphere.


=== Galactic Poles ===
The galactic poles are the two poles of the great circle that is the galactic equator.

=== Galactic Axis ===
The galactic axis is the axis of the great circle that is the galactic equator.",Definition:Galactic Equator,"['Definitions/Galactic Equator', 'Definitions/Galactic Coordinate System', 'Definitions/Spherical Astronomy', 'Definitions/Equator']"
Definition:Equiangular,Equiangular,An equiangular polygon is a polygon in which all the vertices have the same angle.,Definition:Polygon/Equiangular,"['Definitions/Equiangular Polygons', 'Definitions/Equiangular', 'Definitions/Polygons']"
Definition:Equiangular,Equiangular,Two geometric figures are equiangular (with each other) when the angles of each pair of their corresponding vertices are equal.,Definition:Equiangular Geometric Figures,"['Definitions/Geometric Figures', 'Definitions/Equiangular']"
Definition:Equiangular,Equiangular,"The logarithmic spiral is the locus of the equation expressed in Polar coordinates as:
:$r = a e^{b \theta}$


:",Definition:Logarithmic Spiral,"['Definitions/Logarithmic Spiral', 'Definitions/Spirals']"
Definition:Equiangular,Equiangular,"A rectangular hyperbola is a hyperbola whose transverse axis is equal to its conjugate axis.


=== Standard Form ===
Let $K$ be a Rectangular hyperbola embedded in a cartesian plane.

$K$ is in standard form  if and only if :
:$(1)$ its major axis is aligned with the straight line $y = x$
:$(2)$ its minor axis is aligned with the straight line $y = -x$.


:",Definition:Rectangular Hyperbola,"['Definitions/Rectangular Hyperbolas', 'Definitions/Hyperbolas']"
Definition:Equiangular,Equiangular,"Let $T$ be a transformation of the plane.

Let $T$ have the property that:
:for all pairs of curves $\CC_1$ and $\CC_2$ which intersect at angle $\theta$, the images of $\CC_1$ and $\CC_2$ under $T$ also intersect at angle $\theta$.

Then $T$ is defined as being a conformal transformation.",Definition:Conformal Transformation,"['Definitions/Conformal Transformations', 'Definitions/Analytic Geometry', 'Definitions/Mapping Theory']"
Definition:Equilibrium,Equilibrium,"Let $S$ be a stochastic process.

Suppose that the observations of the time series to which $S$ gives rise have a constant mean level.

Then $S$ is said to be in (statistical) equilibrium.",Definition:Statistical Equilibrium,['Definitions/Stochastic Processes']
Definition:Equilibrium,Equilibrium,"Let $B$ be a particle, a system of particles, or a body.

Let $B$ be such that:
:it is subject to neither acceleration nor angular acceleration
:the resultant of the external forces acting on $S$ is zero
:the sum of all the moments of all the external forces acting on $S$ is also zero.

Then $B$ is said to be in equilibrium.


=== Stable Equilibrium ===
Let $B$ be a particle, a system of particles, or a body which is in equilibrium.

Let $B$ be such that if a sufficiently small displacement is applied, then it returns to its original position.


$B$ is then said to be in stable equilibrium.

=== Unstable Equilibrium ===
Let $B$ be a particle, a system of particles, or a body which is in equilibrium.

Let $B$ be such that if a displacement is applied, however small, $B$ moves to a position different from its original position.


$B$ is then said to be in unstable equilibrium.

=== Neutral Equilibrium ===
Let $B$ be a particle, a system of particles, or a body which is in equilibrium.

Let $B$ be such that if a small displacement is applied, $B$ remains in its new position.


$B$ is then said to be in neutral equilibrium.",Definition:Equilibrium (Mechanics),"['Definitions/Equilibrium (Mechanics)', 'Definitions/Mechanics', 'Definitions/Physics', 'Definitions/Equilibrium']"
Definition:Equilibrium,Equilibrium,"Let $B$ be a particle, a system of particles, or a body which is in equilibrium.

Let $B$ be such that if a sufficiently small displacement is applied, then it returns to its original position.


$B$ is then said to be in stable equilibrium.",Definition:Equilibrium (Mechanics)/Stable,"['Definitions/Stable Equilibrium', 'Definitions/Equilibrium (Mechanics)']"
Definition:Equilibrium,Equilibrium,"Let $B$ be a particle, a system of particles, or a body which is in equilibrium.

Let $B$ be such that if a displacement is applied, however small, $B$ moves to a position different from its original position.


$B$ is then said to be in unstable equilibrium.",Definition:Equilibrium (Mechanics)/Unstable,"['Definitions/Unstable Equilibrium', 'Definitions/Equilibrium (Mechanics)']"
Definition:Equilibrium,Equilibrium,"Let $B$ be a particle, a system of particles, or a body which is in equilibrium.

Let $B$ be such that if a small displacement is applied, $B$ remains in its new position.


$B$ is then said to be in neutral equilibrium.",Definition:Equilibrium (Mechanics)/Neutral,"['Definitions/Neutral Equilibrium', 'Definitions/Equilibrium (Mechanics)']"
Definition:Equilibrium,Equilibrium,"The equilibrium position of a body $B$ attached to a spring $S$ is the position it occupies when $S$ is exerting no force upon $B$.

For an ideal spring obeying Hooke's Law $\mathbf F = -k \mathbf x$, the equilibrium position is set to be the point $\mathbf x = \bszero$.",Definition:Spring/Equilibrium Position,['Definitions/Mechanics']
Definition:Equilibrium,Equilibrium,An equilibrium point is a stable outcome of a game associated with a particular set of strategies.,Definition:Equilibrium Point,['Definitions/Game Theory']
Definition:Equilibrium,Equilibrium,"A system of strategies, one for each player, is in equilibrium  if and only if  they result in an equilibrium point.

Such strategies are known as equilibrium strategies.


In the singular, an equilibrium strategy is one that contributes to an equilibrium point.",Definition:Equilibrium Strategy,"['Definitions/Equilibrium Strategies', 'Definitions/Strategies']"
Definition:Equilibrium,Equilibrium,"Let a strategic game $G$ be modelled by:
:$G = \stratgame N {A_i} {\succsim_i}$


A Nash equilibrium of $G$ is a profile $a^* \in A$ of moves which has the property that:
:$\forall i \in N: \forall a_i \in A_i: \tuple {a^*_{-i}, a^*_i} \succsim_i \tuple {a^*_{-i}, a_i}$


Thus, for $a^*$ to be a Nash equilibrium, no player $i$ has a move yielding a preferable outcome to that when $a^*_i$ is chosen, given that every other player $j$ has chosen his own equilibrium move.

That is, no player can profitably deviate, if no other player also deviates.

 ",Definition:Nash Equilibrium,['Definitions/Game Theory']
Definition:Equivalence,Equivalence,"If two statements $p$ and $q$ are such that:

:$p \vdash q$, that is: $p$ therefore $q$
:$q \vdash p$, that is: $q$ therefore $p$

then $p$ and $q$ are said to be (logically) equivalent.


That is:
:$p \dashv \vdash q$
means:
:$p \vdash q$ and $q \vdash p$


Note that because the conclusion of an argument is a single statement, there can be only one statement on either side of the $\dashv \vdash$ sign.


In symbolic logic, the notion of logical equivalence occurs in the form of provable equivalence and semantic equivalence.


=== Provable Equivalence ===
Let $\mathscr P$ be a proof system for a formal language $\LL$.

Let $\phi, \psi$ be $\LL$-WFFs.


Then $\phi$ and $\psi$ are $\mathscr P$-provably equivalent  if and only if :

:$\phi \vdash_{\mathscr P} \psi$ and $\psi \vdash_{\mathscr P} \phi$

that is,  if and only if  they are $\mathscr P$-provable consequences of one another.


The provable equivalence of $\phi$ and $\psi$ can be denoted by:

:$\phi \dashv \vdash_{\mathscr P} \psi$

=== Semantic Equivalence ===
Let $\mathscr M$ be a formal semantics for a formal language $\LL$.

Let $\phi, \psi$ be $\LL$-WFFs.


Then $\phi$ and $\psi$ are $\mathscr M$-semantically equivalent  if and only if :

:$\phi \models_{\mathscr M} \psi$ and $\psi \models_{\mathscr M} \phi$

that is,  if and only if  they are $\mathscr M$-semantic consequences of one another.


Equivalently, $\phi$ and $\psi$ are $\mathscr M$-semantically equivalent  if and only if , for each $\mathscr M$-structure $\MM$:

:$\MM \models_{\mathscr M} \phi$  if and only if  $\MM \models_{\mathscr M} \psi$


 

 ",Definition:Logical Equivalence,"['Definitions/Logical Equivalence', 'Definitions/Logic']"
Definition:Equivalence,Equivalence,"Let $\mathscr M$ be a formal semantics for a formal language $\LL$.

Let $\phi, \psi$ be $\LL$-WFFs.


Then $\phi$ and $\psi$ are $\mathscr M$-semantically equivalent  if and only if :

:$\phi \models_{\mathscr M} \psi$ and $\psi \models_{\mathscr M} \phi$

that is,  if and only if  they are $\mathscr M$-semantic consequences of one another.


Equivalently, $\phi$ and $\psi$ are $\mathscr M$-semantically equivalent  if and only if , for each $\mathscr M$-structure $\MM$:

:$\MM \models_{\mathscr M} \phi$  if and only if  $\MM \models_{\mathscr M} \psi$


 

 ",Definition:Semantic Equivalence,['Definitions/Formal Semantics']
Definition:Equivalence,Equivalence,"Let $\mathscr P$ be a proof system for a formal language $\LL$.

Let $\phi, \psi$ be $\LL$-WFFs.


Then $\phi$ and $\psi$ are $\mathscr P$-provably equivalent  if and only if :

:$\phi \vdash_{\mathscr P} \psi$ and $\psi \vdash_{\mathscr P} \phi$

that is,  if and only if  they are $\mathscr P$-provable consequences of one another.


The provable equivalence of $\phi$ and $\psi$ can be denoted by:

:$\phi \dashv \vdash_{\mathscr P} \psi$",Definition:Provable Equivalence,['Definitions/Proof Systems']
Definition:Equivalence,Equivalence,"Let $S$ and $T$ be sets.

Then $S$ and $T$ are equivalent  if and only if :
:there exists a bijection $f: S \to T$ between the elements of $S$ and those of $T$.

That is,  if and only if  they have the same cardinality.


This can be written $S \sim T$.


If $S$ and $T$ are not equivalent we write $S \nsim T$.",Definition:Set Equivalence,"['Definitions/Set Equivalence', 'Definitions/Set Theory']"
Definition:Equivalence,Equivalence,"Two matrices $\mathbf A = \sqbrk a_{m n}, \mathbf B = \sqbrk b_{m n}$ are row equivalent if one can be obtained from the other by a finite sequence of elementary row operations.

This relationship can be denoted $\mathbf A \sim \mathbf B$.",Definition:Row Equivalence,"['Definitions/Matrix Theory', 'Definitions/Elementary Row Operations']"
Definition:Equivalence,Equivalence,"Let $\RR$ be a relation on a set $S$.


=== Definition 1 ===
Let $\RR$ be a relation on a set $S$.


Let $\RR$ be:

:$(1): \quad$ reflexive
:$(2): \quad$ symmetric
:$(3): \quad$ transitive

Then $\RR$ is an equivalence relation on $S$.

=== Definition 2 ===
Let $\RR \subseteq S \times S$ be a relation on a set $S$.


$\RR$ is an equivalence relation  if and only if :

:$\Delta_S \cup \RR^{-1} \cup \RR \circ \RR \subseteq \RR$

where:
:$\Delta_S$ denotes the diagonal relation on $S$
:$\RR^{-1}$ denotes the inverse relation
:$\circ$ denotes composition of relations",Definition:Equivalence Relation,['Definitions/Equivalence Relations']
Definition:Euclidean,Euclidean,"Euclidean geometry is the branch of geometry in which the parallel postulate applies.

An assumption which is currently under question is whether or not ordinary space is itself Euclidean.


Euclidean geometry adheres to Euclid's postulates.",Definition:Euclidean Geometry,"['Definitions/Euclidean Geometry', 'Definitions/Geometry', 'Definitions/Pure Mathematics', 'Definitions/Branches of Mathematics']"
Definition:Euclidean,Euclidean,"Let $S$ be one of the standard number fields $\Q$, $\R$, $\C$.

Let $S^n$ be a cartesian space for $n \in \N_{\ge 1}$.

Let $d: S^n \times S^n \to \R$ be the usual (Euclidean) metric on $S^n$.

Then $\tuple {S^n, d}$ is a Euclidean space.",Definition:Euclidean Space,"['Definitions/Euclidean Space', 'Definitions/Linear Algebra', 'Definitions/Examples of Topologies', 'Definitions/Geometry']"
Definition:Euclidean,Euclidean,"Let $S$ be one of the standard number fields $\Q$, $\R$, $\C$.

Let $S^n$ be a cartesian space for $n \in \N_{\ge 1}$.

Let $M = \struct {S^n, d}$ be a Euclidean space.


The topology $\tau_d$ induced by the Euclidean metric $d$ is called the Euclidean topology.",Definition:Euclidean Space/Euclidean Topology,"['Definitions/Euclidean Space', 'Definitions/Linear Algebra', 'Definitions/Geometry']"
Definition:Euclidean,Euclidean,"Let $M_{1'} = \struct {A_{1'}, d_{1'} }$ and $M_{2'} = \struct {A_{2'}, d_{2'} }$ be metric spaces.

Let $A_{1'} \times A_{2'}$ be the cartesian product of $A_{1'}$ and $A_{2'}$.


The Euclidean metric on $A_{1'} \times A_{2'}$ is defined as:

:$\map {d_2} {x, y} := \paren {\paren {\map {d_{1'} } {x_1, y_1} }^2 + \paren {\map {d_{2'} } {x_2, y_2} }^2}^{1/2}$

where $x = \tuple {x_1, x_2}, y = \tuple {y_1, y_2} \in A_{1'} \times A_{2'}$.


=== General Definition ===
Let $M_{1'} = \struct {A_{1'}, d_{1'} }, M_{2'} = \struct {A_{2'}, d_{2'} }, \ldots, M_{n'} = \struct {A_{n'}, d_{n'} }$ be metric spaces.

Let $\ds \AA = \prod_{i \mathop = 1}^n A_{i'}$ be the cartesian product of $A_{1'}, A_{2'}, \ldots, A_{n'}$.


The Euclidean metric on $\ds \AA = \prod_{i \mathop = 1}^n A_{i'}$ is defined as:

:$\ds \map {d_2} {x, y} := \paren {\sum_{i \mathop = 1}^n \paren {\map {d_{i'} } {x_i, y_i} }^2}^{\frac 1 2}$

where $x = \tuple {x_1, x_2, \ldots, x_n}, y = \tuple {y_1, y_2, \ldots, y_n} \in \AA$.

=== Riemannian Manifold ===
Let $x \in \R^n$ be a point.

Let $\tuple {x_1, \ldots, x_n}$ be the standard coordinates.

Let $T_x \R^n$ be the tangent space of $\R^n$ at $x$.

Let $T_x \R^n$ be identified with $\R^n$:

:$T_x \R^n \cong \R^n$
 

Let $v, w \in T_x \R^n$ be vectors such that:

:$\ds v = \sum_{i \mathop = 1}^n v^i \valueat {\partial_i} x$

:$\ds w = \sum_{i \mathop = 1}^n w^i \valueat {\partial_i} x$

Let $g$ be a Riemannian metric such that:

:$\ds g_x = \innerprod v w_x = \sum_{i \mathop = 1}^n v^i w^i$


Then $g$ is called the Euclidean metric.

 ",Definition:Euclidean Metric,"['Definitions/Euclidean Metric', 'Definitions/Examples of Metric Spaces']"
Definition:Euclidean,Euclidean,"Let $\mathbf v = \tuple {v_1, v_2, \ldots, v_n}$ be a vector in the real Euclidean $n$-space $\R^n$.


The Euclidean norm of $\mathbf v$ is defined as:
:$\ds \norm {\mathbf v} = \paren {\sum_{k \mathop = 1}^n v_k^2}^{1/2}$",Definition:Euclidean Norm,"['Definitions/Euclidean Norms', 'Definitions/Normed Spaces', 'Definitions/Euclidean Space']"
Definition:Euclidean,Euclidean,"Let $\struct {D, +, \circ}$ be an integral domain.

Let there exist a Euclidean valuation on $D$.

Then $D$ is called a Euclidean domain.


=== Euclidean Valuation ===
Let $\struct {D, +, \circ}$ be an integral domain with zero $0_D$.

Let there exist a mapping $\nu: D \setminus \set {0_D} \to \N$ such that for all $a \in D, b \in D_{\ne 0_D}$:

 
 
 
 

Then $\nu$ is a Euclidean valuation on $D$.",Definition:Euclidean Domain,"['Definitions/Euclidean Domains', 'Definitions/Integral Domains']"
Definition:Euclidean,Euclidean,"Let $\RR \subseteq S \times S$ be a relation in $S$.


=== Left-Euclidean ===
Let $\RR \subseteq S \times S$ be a relation in $S$.


$\RR$ is left-Euclidean  if and only if :

:$\tuple {x, z} \in \RR \land \tuple {y, z} \in \RR \implies \tuple {x, y} \in \RR$

=== Right-Euclidean ===
Let $\RR \subseteq S \times S$ be a relation in $S$.


$\RR$ is right-Euclidean  if and only if :

:$\tuple {x, y} \in \RR \land \tuple {x, z} \in \RR \implies \tuple {y, z} \in \RR$

=== Euclidean ===

$\RR$ is Euclidean  if and only if  it is both left-Euclidean and right-Euclidean.


",Definition:Euclidean Relation,"['Definitions/Euclidean Relations', 'Definitions/Relation Theory']"
Definition:Euler Characteristic,Euler Characteristic,"Let $G = \struct {V, E}$ be a finite graph.

Let $G$ be embedded in a surface.


The Euler characteristic of $G$ is written $\map \chi G$ and is defined as:
:$\map \chi G = v - e + f$
where:
:$v = \size V$ is the number of vertices
:$e = \size E$ is the number of edges
:$f$ is the number of faces.",Definition:Euler Characteristic of Finite Graph,"['Definitions/Euler Characteristic of Finite Graph', 'Definitions/Euler Characteristic', 'Definitions/Graph Theory']"
Definition:Euler Characteristic,Euler Characteristic,"Let $S$ be a surface.

Let $T$ be a triangulation of $S$.

The Euler characteristic of $S$ is written $\map \chi S$ and is defined as:
:$\map \chi S = v - e + f$
where:
:$v = \size V$ is the number of vertices of $T$
:$e = \size E$ is the number of edges of $T$
:$f$ is the number of faces of $T$.",Definition:Euler Characteristic of Surface,"['Definitions/Euler Characteristic of Surface', 'Definitions/Euler Characteristic', 'Definitions/Graph Theory']"
Definition:Expansion,Expansion,"Let $S$ be a set.

Let $\tau_1$ and $\tau_2$ be topologies on $S$ such that $\tau_1 \subseteq \tau_2$.


Then $\tau_2$ is an expansion of $\tau_1$.",Definition:Expansion of Topology,['Definitions/Topology']
Definition:Expansion,Expansion,"Let $x \in \R$ be a real number.

The decimal expansion of $x$ is the expansion of $x$ in base $10$.


$x = \floor x + \ds \sum_{j \mathop \ge 1} \frac {d_j} {10^j}$:
:$\sqbrk {s \cdotp d_1 d_2 d_3 \ldots}_{10}$
where:
:$s = \floor x$, the floor of $x$
:it is not the case that there exists $m \in \N$ such that $d_M = 9$ for all $M \ge m$.
(That is, the sequence of digits does not end with an infinite sequence of $9$s.)


=== Decimal Point ===
Let $x \in \R$ have a decimal expansion:
:$n. d_1 d_2 d_3 \ldots$


The dot that separates the integer part from the fractional part of $x$ is called the decimal point.

That is, it is the radix point when used specifically for a base $10$ representation.

=== Size Less than 1 ===
A number $x$ such that $\size x < 0$ has a units digit which is zero.

Such a number may be expressed either with or without the zero, for example:
:$0 \cdotp 568$
or:
:$\cdotp 568$

While both are commonplace, the form with the zero is less prone to the mistake where decimal point is missed when reading it.

=== Decimal Place ===
Let $x \in \R$ be a real number.

Let the decimal expansion of $x$ be:

:$x = \sqbrk {s \cdotp d_1 d_2 d_3 \ldots}_{10}$

Then $d_k$ is defined as being the digit in the $k$th decimal place.",Definition:Decimal Expansion,"['Definitions/Decimal Expansions', 'Definitions/Decimal Notation', 'Definitions/Basis Expansions', 'Definitions/Decimal']"
Definition:Expansion,Expansion,"=== Positive Real Numbers ===
Let $x \in \R$ be a real number such that $x \ge 0$.

Let $b \in \N: b \ge 2$.


Let us define the recursive sequence:
:$\forall n \in \N: n \ge 1: \sequence {f_n} = \begin {cases}
b \paren {x - \floor x} & : n = 1 \\
b \paren {f_{n - 1} - \floor {f_{n - 1} } } & : n > 1
\end{cases}$

Then we define:
:$\forall n \in \N: n \ge 1: \sequence {d_n} = \floor {f_n}$


It follows from the method of construction and the definition of the floor function that:

:$\forall n: 0 \le f_n < b$ and hence $\forall n: 0 \le d_n \le b - 1$
:$\forall n: f_n = 0 \implies f_{n + 1} = 0$ and hence $d_{n + 1} = 0$.


Hence we can express $x = \floor x + \displaystyle \sum_{j \mathop \ge 1} \frac {d_j} {b^j}$ as:
:$\sqbrk {s \cdotp d_1 d_2 d_3 \ldots}_b$
where:
:$s = \floor x$
:it is not the case that there exists $m \in \N$ such that $d_M = b - 1$ for all $M \ge m$.
(That is, the sequence of digits does not end with an infinite sequence of $b - 1$.)


This is called the expansion of $x$ in base $b$.

The generic term for such an expansion is a basis expansion.


It follows from the Division Theorem that for a given $b$ and $x$ this expansion is unique.


=== Termination ===
Let $b \in \N: b \ge 2$.

Let the basis expansion of $x$ in base $b$ be:

:$\sqbrk {s \cdotp d_1 d_2 d_3 \ldots}_b$


Let it be the case that:
:$\exists m \in \N: \forall k \ge m: d_k = 0$

That is, every digit of $x$ in base $b$ after a certain point is zero.

Then $x$ is said to terminate.

=== Recurrence ===
Let $b \in \N: b \ge 2$.

Let $x$ be a real number.

Let the basis expansion of $x$ in base $b$ be:

:$\sqbrk {s \cdotp d_1 d_2 d_3 \ldots}_b$

Let there be a finite sequence of $p$ digits of $x$:
:$\tuple {d_{r + 1} d_{r + 1} \ldots d_{r + p} }$
such that for all $k \in \Z_{\ge 0}$ and for all $j \in \set {1, 2, \ldots, p}$:
:$d_{r + j + k p} = d_{r + j}$

where $p$ is the smallest $p$ to have this property.

That is, let $x$ be of the form:

:$\sqbrk {s \cdotp d_1 d_2 d_3 \ldots d_r d_{r + 1} d_{r + 2} \ldots d_{r + p} d_{r + 1} d_{r + 2} \ldots d_{r + p} d_{r + 1} d_{r + 2} \ldots d_{r + p} d_{r + 1} \ldots}_b$


That is, $\tuple {d_{r + 1} d_{r + 2} \ldots d_{r + p} }$ repeats from then on, or recurs.

Then $x$ is said to recur.


=== Non-Recurring Part ===
Let $b \in \N: b \ge 2$.

Let $x$ be a real number.

Let the basis expansion of $x$ in base $b$ be recurring:

:$\sqbrk {s \cdotp d_1 d_2 d_3 \ldots d_r d_{r + 1} d_{r + 2} \ldots d_{r + p} d_{r + 1} d_{r + 2} \ldots d_{r + p } d_{r + 1} d_{r + 2} \ldots d_{r + p} d_{r + 1} \ldots}_b$


The non-recurring part of $x$  is:
:$\sqbrk {s \cdotp d_1 d_2 d_3 \ldots d_r}$

=== Recurring Part ===
Let $b \in \N: b \ge 2$.

Let $x$ be a real number.

Let the basis expansion of $x$ in base $b$ be recurring:

:$\sqbrk {s \cdotp d_1 d_2 d_3 \ldots d_r d_{r + 1} d_{r + 2} \ldots d_{r + p} d_{r + 1} d_{r + 2} \ldots d_{r + p } d_{r + 1} d_{r + 2} \ldots d_{r + p} d_{r + 1} \ldots}_b$


The recurring part of $x$  is:
:$\sqbrk {d_{r + 1} d_{r + 2} \ldots d_{r + p}}$


=== Period ===
Let $b \in \N: b \ge 2$.

Let $x$ be a rational number.

Let the basis expansion of $x$ in base $b$ be:

:$\sqbrk {s \cdotp d_1 d_2 d_3 \ldots}_b$

From Basis Expansion of Rational Number, $x$ either terminates or recurs.


The period of recurrence is the number of digits in the recurring part after which it repeats itself.


Category:Definitions/Basis Expansions

=== Period ===
Let $b \in \N: b \ge 2$.

Let $x$ be a rational number.

Let the basis expansion of $x$ in base $b$ be:

:$\sqbrk {s \cdotp d_1 d_2 d_3 \ldots}_b$

From Basis Expansion of Rational Number, $x$ either terminates or recurs.


The period of recurrence is the number of digits in the recurring part after which it repeats itself.


Category:Definitions/Basis Expansions

=== Negative Real Numbers ===
Let $x \in \R: x < 0$.

We take the absolute value $y$ of $x$, that is:
:$y = \size x$

Then we take the expansion of $y$ in base $b$:
:$\size {s . d_1 d_2 d_3 \ldots}_b$
where $s = \floor y$.

Finally, the expansion of $x$ in base $b$ is defined as:
:$-\sqbrk {s . d_1 d_2 d_3 \ldots}_b$


=== Termination ===
Let $b \in \N: b \ge 2$.

Let the basis expansion of $x$ in base $b$ be:

:$\sqbrk {s \cdotp d_1 d_2 d_3 \ldots}_b$


Let it be the case that:
:$\exists m \in \N: \forall k \ge m: d_k = 0$

That is, every digit of $x$ in base $b$ after a certain point is zero.

Then $x$ is said to terminate.

=== Recurrence ===
Let $b \in \N: b \ge 2$.

Let $x$ be a real number.

Let the basis expansion of $x$ in base $b$ be:

:$\sqbrk {s \cdotp d_1 d_2 d_3 \ldots}_b$

Let there be a finite sequence of $p$ digits of $x$:
:$\tuple {d_{r + 1} d_{r + 1} \ldots d_{r + p} }$
such that for all $k \in \Z_{\ge 0}$ and for all $j \in \set {1, 2, \ldots, p}$:
:$d_{r + j + k p} = d_{r + j}$

where $p$ is the smallest $p$ to have this property.

That is, let $x$ be of the form:

:$\sqbrk {s \cdotp d_1 d_2 d_3 \ldots d_r d_{r + 1} d_{r + 2} \ldots d_{r + p} d_{r + 1} d_{r + 2} \ldots d_{r + p} d_{r + 1} d_{r + 2} \ldots d_{r + p} d_{r + 1} \ldots}_b$


That is, $\tuple {d_{r + 1} d_{r + 2} \ldots d_{r + p} }$ repeats from then on, or recurs.

Then $x$ is said to recur.


=== Non-Recurring Part ===
Let $b \in \N: b \ge 2$.

Let $x$ be a real number.

Let the basis expansion of $x$ in base $b$ be recurring:

:$\sqbrk {s \cdotp d_1 d_2 d_3 \ldots d_r d_{r + 1} d_{r + 2} \ldots d_{r + p} d_{r + 1} d_{r + 2} \ldots d_{r + p } d_{r + 1} d_{r + 2} \ldots d_{r + p} d_{r + 1} \ldots}_b$


The non-recurring part of $x$  is:
:$\sqbrk {s \cdotp d_1 d_2 d_3 \ldots d_r}$

=== Recurring Part ===
Let $b \in \N: b \ge 2$.

Let $x$ be a real number.

Let the basis expansion of $x$ in base $b$ be recurring:

:$\sqbrk {s \cdotp d_1 d_2 d_3 \ldots d_r d_{r + 1} d_{r + 2} \ldots d_{r + p} d_{r + 1} d_{r + 2} \ldots d_{r + p } d_{r + 1} d_{r + 2} \ldots d_{r + p} d_{r + 1} \ldots}_b$


The recurring part of $x$  is:
:$\sqbrk {d_{r + 1} d_{r + 2} \ldots d_{r + p}}$


=== Period ===
Let $b \in \N: b \ge 2$.

Let $x$ be a rational number.

Let the basis expansion of $x$ in base $b$ be:

:$\sqbrk {s \cdotp d_1 d_2 d_3 \ldots}_b$

From Basis Expansion of Rational Number, $x$ either terminates or recurs.


The period of recurrence is the number of digits in the recurring part after which it repeats itself.


Category:Definitions/Basis Expansions

=== Period ===
Let $b \in \N: b \ge 2$.

Let $x$ be a rational number.

Let the basis expansion of $x$ in base $b$ be:

:$\sqbrk {s \cdotp d_1 d_2 d_3 \ldots}_b$

From Basis Expansion of Rational Number, $x$ either terminates or recurs.


The period of recurrence is the number of digits in the recurring part after which it repeats itself.


Category:Definitions/Basis Expansions",Definition:Basis Expansion,"['Definitions/Number Bases', 'Definitions/Real Numbers', 'Definitions/Basis Expansions']"
Definition:Expansion,Expansion,"An (algebraic) expansion is a algebraic expression written as the sum of terms.

The word is also used for the actual process of converting the power of a multinomial, in order to obtain such an expression.",Definition:Algebraic Expansion,"['Definitions/Algebraic Expansions', 'Definitions/Algebra']"
Definition:Expansion,Expansion,"Let $S$ be a set.

Let $\Bbb S = \set {S_1, S_2, \ldots, S_n}$ form a partition of $S$.


Then the representation by such a partition $\ds \bigcup_{k \mathop = 1}^n S_k = S$ is also called a finite expansion of $S$.


The notations:
:$S = S_1 \mid S_2 \mid \cdots \mid S_n$
or:
:$\Bbb S = \set {S_1 \mid S_2 \mid \cdots \mid S_n}$
are sometimes seen.


Category:Definitions/Set Partitions",Definition:Set Partition/Finite Expansion,['Definitions/Set Partitions']
Definition:Expansion,Expansion,"Suppose our universe of discourse consists of the objects $\mathbf X_1, \mathbf X_2, \mathbf X_3, \ldots$ and so on.


(There may be an infinite number of objects in this universe.)


=== Universal Quantifier ===
Suppose our universe of discourse consists of the objects $\mathbf X_1, \mathbf X_2, \mathbf X_3, \ldots$ and so on.

Let $\forall$ be the universal quantifier.

What $\forall x: \map P x$ means is:

:$\mathbf X_1$ has property $P$, and $\mathbf X_2$ has property $P$, and $\mathbf X_3$ has property $P$, and ...

This translates into propositional logic as:

:$\map P {\mathbf X_1} \land \map P {\mathbf X_2} \land \map P {\mathbf X_3} \land \ldots$


This expression of $\forall x$ as a conjunction is known as the propositional expansion of $\forall x$. 


The propositional expansion for the universal quantifier can exist in actuality only when the number of objects in the universe is finite.

If the universe is infinite, then the propositional expansion can exist only conceptually, and the universal quantifier cannot be eliminated.


Category:Definitions/Propositional Expansions
Category:Definitions/Universal Quantifier

=== Existential Quantifier ===
Suppose our universe of discourse consists of the objects $\mathbf X_1, \mathbf X_2, \mathbf X_3, \ldots$ and so on.

Let $\exists$ be the existential quantifier.

What $\exists x: \map P x$ means is:

:At least one of $\mathbf X_1, \mathbf X_2, \mathbf X_3, \ldots$ has property $P$.

This means:

:Either $\mathbf X_1$ has property $P$, or $\mathbf X_2$ has property $P$, or $\mathbf X_3$ has property $P$, or ...

This translates into propositional logic as:

:$\map P {\mathbf X_1} \lor \map P {\mathbf X_2} \lor \map P {\mathbf X_3} \lor \ldots$


This expression of $\exists x$ as a disjunction is known as the propositional expansion of $\exists x$.


The propositional expansion for the existential quantifier can exist in actuality only when the number of objects in the universe is finite.

If the universe is infinite, then the propositional expansion can exist only conceptually, and the existential quantifier cannot be eliminated.


Category:Definitions/Propositional Expansions
Category:Definitions/Existential Quantifier",Definition:Propositional Expansion,"['Definitions/Propositional Expansions', 'Definitions/Quantifiers']"
Definition:Extension,Extension,"Let:

:$\RR_1 \subseteq X \times Y$ be a relation on $X \times Y$
:$\RR_2 \subseteq S \times T$ be a relation on $S \times T$
:$X \subseteq S$
:$Y \subseteq T$
:$\RR_2 \restriction_{X \times Y}$ be the restriction of $\RR_2$ to $X \times Y$.


Let $\RR_2 \restriction_{X \times Y} = \RR_1$.


Then $\RR_2$ extends or is an extension of $\RR_1$.",Definition:Extension of Relation,['Definitions/Relation Theory']
Definition:Extension,Extension,"As a mapping is, by definition, also a relation, the definition of an extension of a mapping is the same as that for an extension of a relation:

Let:

:$f_1 \subseteq X \times Y$ be a mapping on $X \times Y$
:$f_2 \subseteq S \times T$ be a mapping on $S \times T$
:$X \subseteq S$
:$Y \subseteq T$
:$f_2 \restriction_{X \times Y}$ be the restriction of $f_2$ to $X \times Y$.


Let $f_2 \restriction_{X \times Y} = f_1$.

That is, let $f_1$ be a subset of $f_2$.


Then $f_2$ extends or is an extension of $f_1$.",Definition:Extension of Mapping,"['Definitions/Mapping Theory', 'Definitions/Restrictions']"
Definition:Extension,Extension,"Let $\left({S, \circ}\right)$ be a magma.

Let $\left({T, \circ \restriction_T}\right)$ be a submagma of $\left({S, \circ}\right)$, where $\circ \restriction_T$ denotes the restriction of $\circ$ to $T$.


Then:
: $\left({S, \circ}\right)$ is an extension of $\left({T, \circ \restriction_T}\right)$
or
: $\left({S, \circ}\right)$ extends $\left({T, \circ \restriction_T}\right)$


We can use the term directly to the operation itself and say:
: $\circ$ is an extension of $\circ \restriction_T$
or:
: $\circ$ extends $\circ \restriction_T$",Definition:Extension of Operation,['Definitions/Abstract Algebra']
Definition:Extension,Extension,"As a sequence is, by definition, also a mapping, the definition of an extension of a sequence is the same as that for an extension of a mapping:

Let:

: $\left \langle {a_k} \right \rangle_{k \mathop \in A}$ be a sequence on $A$, where $A \subseteq \N$.
: $\left \langle {b_k} \right \rangle_{k \mathop \in B}$ be a sequence on $B$, where $B \subseteq \N$.
: $A \subseteq B$
: $\forall k \in A: b_k = a_k$.

Then $\left \langle {b_k} \right \rangle_{k \mathop \in B}$ extends or is an extension of $\left \langle {a_k} \right \rangle_{k \mathop \in A}$.",Definition:Extension of Sequence,['Definitions/Sequences']
Definition:Extension,Extension,"Let $A$ and $B$ be classes.

$B$ is an extension of $A$  if and only if :
:$A \subseteq B$


=== Immediate Extension ===
Let $A$ and $B$ be classes.

Let $B$ be an extension of $A$.

$B$ is an immediate extension of $A$  if and only if  $B$ contains exactly one more element than $A$.",Definition:Extension of Class,"['Definitions/Class Theory', 'Definitions/Subclasses', 'Definitions/Class Extensions']"
Definition:Extension,Extension,"Let $A$ and $B$ be classes.

Let $B$ be an extension of $A$.

$B$ is an immediate extension of $A$  if and only if  $B$ contains exactly one more element than $A$.",Definition:Extension of Class/Immediate,['Definitions/Class Extensions']
Definition:Extension,Extension,"Let $F$ be a field.


A field extension over $F$ is a field $E$ where $F \subseteq E$.

That is, such that $F$ is a subfield of $E$.


This can be expressed:
:$E$ is a field extension over a field $F$
or:
:$E$ over $F$ is a field extension 
as:
:$E / F$ is a field extension.  


$E / F$ can be voiced as $E$ over $F$.",Definition:Field Extension,['Definitions/Field Extensions']
Definition:Extension,Extension,"Let $R$ and $S$ be commutative rings with unity.

Let $\phi : R \to S$ be a ring monomorphism.

Then $\phi : R \to S$ is a ring extension of $R$.


Alternatively, we can define $S$ to be a ring extension of $R$ if $R$ is a subring of $S$ (provided we insist that a subring inherits the multiplicative identity from its parent ring).


These definitions are equivalent up to isomorphism, for if $R \subseteq S$ is a subring, then the identity $\operatorname{id} : R \to S$ is a monomorphism.

Conversely if $\phi : R \to S$ is a monomorphism, then $\operatorname{im}\phi \subseteq S$ is a subring of $S$.

Moreover by Surgery for Rings, we can find a ring $T$, isomorphic to $S$, that contains $R$ as a subring.

 ",Definition:Ring Extension,['Definitions/Ring Theory']
Definition:Extension,Extension,"Let $A$ and $B$ be commutative ring with unity.

Let $f : A \to B$ be a ring homomorphism.

Let $\mathfrak a$ be an ideal of $A$.


The extension of $\mathfrak a$ by $f$ is the ideal generated by its image under $f$:
:$\mathfrak a^e = \left\langle f \sqbrk {\mathfrak a} \right\rangle$",Definition:Extension of Ideal,['Definitions/Ideal Theory']
Definition:Extension,Extension,"Let $\AA, \BB$ be structures for a signature $\LL$.


Then $\BB$ is an extension of $\AA$  if and only if  $\AA$ is a substructure of $\BB$.",Definition:Extension of Structure,['Definitions/Model Theory for Predicate Logic']
Definition:Extension,Extension,"Let $\left({T, \mathbf H, \Phi}\right)$ be a propositional tableau.


=== Definition 1 ===
Let $T$ be a propositional tableau.


A tableau $T'$ is an extension of $T$ if $T'$ can be obtained from $T$ by repeatedly adding nodes to the leaf nodes of $T$ (by means of the tableau extension rules).

=== Definition 2 ===
Let $\left({T, \mathbf H, \Phi}\right)$ be a propositional tableau.


An extension of $T$ is a propositional tableau $\left({S, \mathbf H', \Phi'}\right)$ such that:

:$S$ is an extension of $T$;
:$\mathbf H = \mathbf H'$;
:$\Phi'$ is an extension of $\Phi$.",Definition:Extension of Propositional Tableau,['Definitions/Propositional Tableaus']
Definition:Exterior,Exterior,"Let $T$ be a topological space.

Let $H \subseteq T$.


=== Definition 1 ===
Let $T$ be a topological space.

Let $H \subseteq T$.


The exterior of $H$ is the complement of the closure of $H$ in $T$.

=== Definition 2 ===
Let $T$ be a topological space.

Let $H \subseteq T$.


The exterior of $H$ is the interior of the complement of $H$ in $T$.",Definition:Exterior (Topology),"['Definitions/Set Exteriors', 'Definitions/Topology']"
Definition:Exterior,Exterior,"Let an exact $n$-form $\omega$ be given on an $m$-manifold, with local coordinates $x_1, x_2, \dots, x_m$.

Let a local coordinate expression for $\omega$ be given:

:$\omega = \map f {x_1, \ldots, x_m} \rd x_{\map \phi 1} \wedge \d x_{\map \phi 2} \wedge \cdots \wedge \d x_{\map \phi n}$

where:
:$\phi: \set {1, \ldots, n} \to \set {1, \ldots, m}$ is an injection which determines which coordinate vectors $\omega$ acts on.
:$\wedge$ denotes the wedge product.


The exterior derivative $\d \omega$ is the $\paren {n + 1}$-form defined as:

:$\ds \d \omega = \paren {\sum_{k \mathop = 1}^m \frac {\partial f} {\partial x_k} \rd x_k} \wedge \d x_{\map \phi 1} \wedge \d x_{\map \phi 2} \wedge \dots \wedge \d x_{\map \phi n}$


For inexact forms:
:$\map \d {a + b} = \d a + \d b$",Definition:Exterior Derivative,['Definitions/Differential Forms']
Definition:Exterior,Exterior,"Contrary to intuition, the external angle of a vertex of a polygon is not the size of the angle between the sides forming that vertex, as measured outside the polygon.

An external angle is in fact an angle formed by one side of a polygon and a line produced from an adjacent side.

:

While $\angle AFE$ is the internal angle of vertex $F$, the external angle of this vertex is $\angle EFG$.",Definition:Polygon/External Angle,"['Definitions/External Angles', 'Definitions/Polygons']"
Definition:Exterior,Exterior,":


An exterior angle of a transversal is an angle which is not between the two lines cut by a transversal.

In the above figure, the exterior angles with respect to the transversal $EF$ are:
:$\angle AHE$
:$\angle CJF$
:$\angle BHE$
:$\angle DJF$",Definition:Transversal (Geometry)/Exterior Angle,['Definitions/Transversals (Geometry)']
Definition:Exterior,Exterior,"Let $S \subseteq \C$ be a subset of the complex plane.

Let $z_0 \in \C$.


=== Definition 1 ===
Let $S \subseteq \C$ be a subset of the complex plane.

Let $z_0 \in \C$.


$z_0$ is an exterior point of $S$  if and only if  $z_0$ has an $\epsilon$-neighborhood which is disjoint from $S$.

=== Definition 2 ===
Let $S \subseteq \C$ be a subset of the complex plane.

Let $z_0 \in \C$.


$z_0$ is an exterior point of $S$  if and only if :
:$z_0$ is not an interior point of $S$
and:
:$z_0$ is not a boundary point of $S$.",Definition:Exterior Point (Complex Analysis),['Definitions/Complex Analysis']
Definition:Exterior,Exterior,"Let $f: \closedint 0 1 \to \R^2$ be a Jordan curve.


It follows from the Jordan Curve Theorem that $\R^2 \setminus \Img f$ is a union of two disjoint connected components, one of which is unbounded.

This unbounded component is called the exterior of $f$, and is denoted as $\Ext f$.",Definition:Jordan Curve/Exterior,['Definitions/Jordan Curves']
Definition:Exterior Angle,Exterior Angle,"Contrary to intuition, the external angle of a vertex of a polygon is not the size of the angle between the sides forming that vertex, as measured outside the polygon.

An external angle is in fact an angle formed by one side of a polygon and a line produced from an adjacent side.

:

While $\angle AFE$ is the internal angle of vertex $F$, the external angle of this vertex is $\angle EFG$.",Definition:Polygon/External Angle,"['Definitions/External Angles', 'Definitions/Polygons']"
Definition:Exterior Angle,Exterior Angle,":


An exterior angle of a transversal is an angle which is not between the two lines cut by a transversal.

In the above figure, the exterior angles with respect to the transversal $EF$ are:
:$\angle AHE$
:$\angle CJF$
:$\angle BHE$
:$\angle DJF$",Definition:Transversal (Geometry)/Exterior Angle,['Definitions/Transversals (Geometry)']
Definition:Extraction of Root,Extraction of Root,The process of evaluating roots of a given real number is referred to as extraction.,Definition:Root of Number/Extraction,"['Definitions/Extraction of Roots', 'Definitions/Roots of Numbers']"
Definition:Extraction of Root,Extraction of Root,The process of finding roots of a given equation is referred to as extraction.,Definition:Root of Equation/Extraction,"['Definitions/Extraction of Roots', 'Definitions/Roots of Equations']"
Definition:Face,Face,":

The faces of a planar graph are the areas which are surrounded by edges.

In the above, the faces are $BCEF$, $ABF$, $CFG$, $AFG$ and $ABCDCEG$.


=== Incident ===
:


Let $G = \struct {V, E}$ be a planar graph:

Then a face of $G$ is incident to an edge $e$ of $G$ if $e$ is one of those which surrounds the face.

Similarly, a face of $G$ is incident to a vertex $v$ of $G$ if $v$ is at the end of one of those incident edges.


In the above graph, for example, the face $BCEF$ is incident to:
:the edges $BC, CE, EF, FB$
:the vertices $B, C, E, F$.

=== Adjacent ===
:

Let $G = \struct {V, E}$ be a planar graph.

Two faces of $G$ are adjacent  if and only if  they are both incident to the same edge (or edges).

In the above diagram, $BCEF$ and $ABF$ are adjacent, but $BCEF$ and $AFG$ are not adjacent.


Note that faces which are both incident to the same vertex are not considered adjacent unless they are also both incident to the same edge.",Definition:Planar Graph/Face,"['Definitions/Faces of Graphs', 'Definitions/Planar Graphs', 'Definitions/Faces']"
Definition:Face,Face,"The faces of a polyhedron are the polygons which contain it.


 ",Definition:Polyhedron/Face,"['Definitions/Faces of Polyhedra', 'Definitions/Polyhedra', 'Definitions/Faces']"
Definition:Face,Face,"The faces of a three-dimensional figure are the surfaces which form its extremities.


 ",Definition:Geometric Figure/Three-Dimensional Figure/Face,"['Definitions/Faces of Solid Figures', 'Definitions/Solid Geometry', 'Definitions/Faces']"
Definition:Face,Face," 
and:
: 
:An extremity of a solid is a surface.
 ''
 

=== Plane Surface ===
 


=== Side ===
From the definition of surface, it follows that a plane locally separates space into two sides.

Thus the sides of a plane are the parts of that space into which the plane separates it.


Category:Definitions/Surfaces

=== The Plane ===
The plane is the term used for the general plane surface which is infinite in all directions.

=== Regular Surface ===
A subset $S \subseteq \R^3$ is a regular surface  if and only if  for each $p \in S$ there exist:
:a neighborhood $V \subseteq \R^3$ of $p$
:an open set $U \subseteq \R^2$
:a surjective mapping $\mathbf x : U \to V \cap S$, written as:
::$\map {\mathbf x} {u, v} := \struct {\map x {u, v}, \map y {u, v}, \map z {u, v} }$

such that:
:$(1): \quad \map x {u, v}, \map y {u, v}, \map z {u, v}$ are smooth
:$(2): \quad \mathbf x: U \to V \cap S$ is a homeomorphism
:$(3): \quad$ For each $q \in U$, the differential $\d_q \mathbf x: \R^2 \to \R^3$ of $\mathbf x$ at $q$ is one-to-one",Definition:Surface (Geometry),"['Definitions/Geometry', 'Definitions/Solid Geometry', 'Definitions/Surfaces']"
Definition:Face,Face,Each of the planes bounded by the defining half-lines is known as a face of the polyhedral angle.,Definition:Polyhedral Angle/Face,"['Definitions/Faces of Polyhedral Angles', 'Definitions/Polyhedral Angles', 'Definitions/Faces']"
Definition:Factor,Factor,"Let $\struct {\Z, +, \times}$ be the ring of integers.

Let $x, y \in \Z$.


Then $x$ divides $y$ is defined as:
:$x \divides y \iff \exists t \in \Z: y = t \times x$


=== Aliquot Part ===
An aliquot part of an integer $n$ is a divisor of $n$ which is strictly less than $n$.

=== Aliquant Part ===
An aliquant part of an integer $n$ is a positive integer which is less than $n$ but is not a divisor of $n$.",Definition:Divisor (Algebra)/Integer,"['Definitions/Divisors', 'Definitions/Number Theory']"
Definition:Factor,Factor,"Let $S$ and $T$ be sets.

Let $S \times T$ be the cartesian product of $S$ and $T$.


Then the sets $S$ and $T$ are called the factors of $S \times T$.",Definition:Cartesian Product/Factors,['Definitions/Cartesian Product']
Definition:Factor,Factor,"Let $\family {\struct {X_i, \tau_i} }_{i \mathop \in I}$ be an indexed family of topological spaces where $I$ is an arbitrary index set.

Let $\struct {\XX, \tau}$ be the product space of $\family {\struct {x_i, \tau_i} }_{i \mathop \in I}$.


Each of the topological spaces $\struct {X_i, \tau_i}$ are called the factors of $\struct {\XX, \tau}$, and can be referred to as factor spaces.",Definition:Product Topology/Factor Space,['Definitions/Product Topology']
Definition:Family,Family,"Let $I$ and $S$ be sets.

Let $x: I \to S$ be an indexing function for $S$.

Let $x_i$ denote the image of an element $i \in I$ of the domain $I$ of $x$.

Let $\family {x_i}_{i \mathop \in I}$ denote the set of the images of all the elements $i \in I$ under $x$.


The image $\Img x$, consisting of the terms $\family {x_i}_{i \mathop \in I}$, along with the indexing function $x$ itself, is called a family of elements of $S$ indexed by $I$.",Definition:Indexing Set/Family,['Definitions/Indexed Families']
Definition:Family,Family,"Let $\SS$ be a set of sets.

Let $I$ be an indexing set.

Let $\family {S_i}_{i \mathop \in I}$ be a family of elements of $\SS$ indexed by $I$.


Then $\family {S_i}_{i \mathop \in I}$ is referred to as an indexed family of sets.",Definition:Indexing Set/Family of Sets,['Definitions/Indexed Families']
Definition:Family,Family,"Let $S$ be a set.

Let $I$ be an indexing set.

For each $i \in I$, let $S_i$ be a corresponding subset of $S$.

Let $\family {S_i}_{i \mathop \in I}$ be a family of subsets of $S$ indexed by $I$.


Then $\family {S_i}_{i \mathop \in I}$ is referred to as an indexed family of subsets (of $S$ by $I$).",Definition:Indexing Set/Family of Subsets,['Definitions/Indexed Families']
Definition:Field,Field,"A field is a non-trivial division ring whose ring product is commutative.


Thus, let $\struct {F, +, \times}$ be an algebraic structure.


Then $\struct {F, +, \times}$ is a field  if and only if :
:$(1): \quad$ the algebraic structure $\struct {F, +}$ is an abelian group
:$(2): \quad$ the algebraic structure $\struct {F^*, \times}$ is an abelian group where $F^* = F \setminus \set {0_F}$
:$(3): \quad$ the operation $\times$ distributes over $+$.


This definition gives rise to the field axioms, as follows:

=== Field Axioms ===
 

=== Addition ===
The distributand $+$ of a field $\struct {F, +, \times}$ is referred to as field addition, or just addition.


=== Additive Group ===
Let $\struct {F, +, \times}$ be a field.


The group $\struct {F, +}$ is known as the additive group of $F$.

=== Additive Inverse ===
Let $\struct {F, +, \times}$ be a field whose addition operation is $+$.

Let $a \in R$ be any arbitrary element of $F$.


The additive inverse of $a$ is its inverse under addition, denoted $-a$:

:$a + \paren {-a} = 0_F$

where $0_F$ is the zero of $R$.


=== Additive Inverse of Number ===

The concept is often encountered in the context of numbers:
Let $\Bbb F$ be one of the standard number systems: $\N$, $\Z$, $\Q$, $\R$, $\C$.

Let $a \in \Bbb F$ be any arbitrary number.


The additive inverse of $a$ is its inverse under addition, denoted $-a$:

:$a + \paren {-a} = 0$

Category:Definitions/Field Theory
Category:Definitions/Addition

=== Product ===
Let $\struct {F, +, \times}$ be a field.


The distributive operation $\times$ in $\struct {F, +, \times}$ is known as the (field) product.",Definition:Field (Abstract Algebra),['Definitions/Field Theory']
Definition:Field,Field,"Let $S$ and $T$ be sets.

Let $\RR \subseteq S \times T$ be a relation.


The field of $\RR$ is defined as:
:$\Field \RR := \set {x \in S: \exists t \in T: \tuple {x, t} \in \RR} \cup \set {x \in T: \exists s \in S: \tuple {s, x} \in \RR}$

That is, it is the union of the domain of $\RR$ with its image.


=== Class Theory ===
 
Let $V$ be a basic universe.

Let $\RR \subseteq V \times V$ be a relation in $V$.


The field of $\RR$ is defined as:
:$\Field \RR := \set {x \in V: \exists y \in V: \tuple {x, y} \in \RR} \cup \set {y \in V: \exists x \in V: \tuple {x, y} \in \RR}$

That is, it is the union of the domain of $\RR$ with its image.",Definition:Field of Relation,['Definitions/Relation Theory']
Definition:Field,Field,"A field, in the context of physics, is a region of space which is acted on by a point-function.

That is, in which a physical quantity is associated with every point of spacetime.


The physical quantity in question may be either in vector or scalar form.


=== Scalar Field ===
Let $F$ be a field which acts on a region of space $S$.

Let the point-function giving rise to $F$ be a scalar quantity.


Then $F$ is a scalar field.

=== Vector Field ===
Let $F$ be a field which acts on a region of space $S$.

Let the point-function giving rise to $F$ be a vector quantity.


Then $F$ is a vector field.

=== Vector Field on Smooth Manifold ===
Let $M$ be a smooth manifold.

Let $TM$ be the tangent bundle of $M$.

Let $T_p M$ be the tangent space at $p \in M$.


Then by the vector field on $M$ we mean the continuous map $X : M \to TM$ such that:

:$\forall p \in M : X_p \in T_p M$",Definition:Field (Physics),"['Definitions/Fields (Physics)', 'Definitions/Physics']"
Definition:Field,Field,"Let $F$ be a field which acts on a region of space $S$.

Let the point-function giving rise to $F$ be a vector quantity.


Then $F$ is a vector field.

=== Vector Field on Smooth Manifold ===
Let $M$ be a smooth manifold.

Let $TM$ be the tangent bundle of $M$.

Let $T_p M$ be the tangent space at $p \in M$.


Then by the vector field on $M$ we mean the continuous map $X : M \to TM$ such that:

:$\forall p \in M : X_p \in T_p M$",Definition:Vector Field,"['Definitions/Vector Fields', 'Definitions/Vectors', 'Definitions/Vector Spaces', 'Definitions/Fields (Physics)']"
Definition:Finitary,Finitary,"A finitary operation is an operation which takes a finite number of operands.


Category:Definitions/Operations",Definition:Operation/Arity/Finitary,['Definitions/Operations']
Definition:Finitary,Finitary,"A finitary argument is a logical argument which starts with a finite number of axioms, and can be translated into a finite number of statements.",Definition:Logical Argument/Finitary,"['Definitions/Finitary Arguments', 'Definitions/Logical Arguments']"
Definition:Finite Type,Finite Type,"Let $R$ be a ring.

Let $M$ be a module over $R$.


Then $M$ is finitely generated  if and only if  there is a generator for $M$ which is finite.",Definition:Finitely Generated Module,['Definitions/Generators of Modules']
Definition:Finite Type,Finite Type,"Let $\struct {X, \OO_X}$ and $\struct {Y, \OO_Y}$ be schemes.

Let $f : \struct {X, \OO_X} \to \struct {Y, \OO_Y}$ be a morphism of schemes.


$f$ is of finite type  if and only if  $f$ is locally of finite type and quasi-compact.",Definition:Morphism of Schemes of Finite Type,"['Definitions/Algebraic Geometry', 'Definitions/Schemes']"
Definition:Finitely Generated,Finitely Generated,"Let $\struct {A, \circ}$ be an algebraic structure.

Let $\struct {A, \circ}$ have a generator which is finite.


Then $\struct {A, \circ}$ is finitely generated.",Definition:Finitely Generated Algebraic Structure,['Definitions/Abstract Algebra']
Definition:Finitely Generated,Finitely Generated,"Let $\struct {M, \circ}$ be a monoid.

Let $S \subseteq M$.

Let $H$ be the smallest submonoid of $M$ such that $S \subseteq H$.


Then:
:$S$ is a generator of $\struct {H, \circ}$
:$S$ generates $\struct {H, \circ}$
:$\struct {H, \circ}$ is the submonoid of $\struct {M, \circ}$ generated by $S$.


This is written $H = \gen S$.


If $S$ is a singleton, for example $S = \set x$, then we can (and usually do) write $H = \gen x$ for $H = \gen {\set x}$.",Definition:Generator of Monoid,['Definitions/Monoids']
Definition:Finitely Generated,Finitely Generated,"Let $G$ be a group.


$G$ is finitely generated  if and only if  $G$ has a generator which is finite.",Definition:Finitely Generated Group,"['Definitions/Generators of Groups', 'Definitions/Group Theory']"
Definition:Finitely Generated,Finitely Generated,"Let $R$ be a ring.

Let $M$ be a module over $R$.


Then $M$ is finitely generated  if and only if  there is a generator for $M$ which is finite.",Definition:Finitely Generated Module,['Definitions/Generators of Modules']
Definition:Finitely Generated,Finitely Generated,"Let $E / F$ be a field extension.


Then $E$ is said to be finitely generated over $F$  if and only if , for some $\alpha_1, \ldots, \alpha_n \in E$:

:$E = F \left({\alpha_1, \ldots, \alpha_n}\right)$ 

where $F \left({\alpha_1, \ldots, \alpha_n}\right)$ is the field in $E$ generated by $F \cup \left\{{\alpha_1, \ldots, \alpha_n}\right\}$.",Definition:Finitely Generated Field Extension,['Definitions/Field Extensions']
Definition:Finitely Generated,Finitely Generated,"Let $A$ be a commutative ring.

Let $B$ be an $A$-algebra.


Then $B$ is finitely generated  if and only if  $B$ has a generator which is finite.",Definition:Finitely Generated Algebra,['Definitions/Algebras']
Definition:Focus,Focus,"Let $\KK$ be a conic section specified in terms of:
:a given straight line $D$
:a given point $F$
:a given constant $\epsilon$

where $K$ is the locus of points $P$ such that the distance $p$ from $P$ to $D$ and the distance $q$ from $P$ to $F$ are related by the condition:
:$q = \epsilon \, p$


The point $F$ is known as the focus of $\KK$.",Definition:Conic Section/Focus,"['Definitions/Foci of Conic Sections', 'Definitions/Conic Sections', 'Definitions/Foci']"
Definition:Focus,Focus,"Let $K$ be an ellipse specified in terms of:
:a given straight line $D$
:a given point $F$
:a given constant $\epsilon$ such that $0 < \epsilon < 1$

where $K$ is the locus of points $P$ such that the distance $p$ from $P$ to $D$ and the distance $q$ from $P$ to $F$ are related by the condition:
:$q = \epsilon \, p$


The point $F$ is known as the focus of the ellipse.",Definition:Ellipse/Focus,"['Definitions/Foci of Ellipses', 'Definitions/Foci of Conic Sections', 'Definitions/Ellipses']"
Definition:Focus,Focus,":


Let $K$ be a parabola specified in terms of:
:a given straight line $D$
:a given point $F$

where $K$ is the locus of points $P$ such that the distance $p$ from $P$ to $D$ equals the distance $q$ from $P$ to $F$:
:$p = q$


The point $F$ is known as the focus of the parabola.",Definition:Parabola/Focus,"['Definitions/Parabolas', 'Definitions/Foci of Conic Sections']"
Definition:Focus,Focus,":


Let $K$ be a hyperbola specified in terms of:
:a given straight line $D$
:a given point $F$
:a given constant $\epsilon$ such that $\epsilon > 1$

where $K$ is the locus of points $P$ such that the distance $p$ from $P$ to $D$ and the distance $q$ from $P$ to $F$ are related by the condition:
:$q = \epsilon \, p$


The point $F_1$ is known as a focus of the hyperbola.

The symmetrically-positioned point $F_2$ is also a focus of the hyperbola.",Definition:Hyperbola/Focus,"['Definitions/Hyperbolas', 'Definitions/Foci of Conic Sections']"
Definition:Foot,Foot,"The foot is the FPS base unit and an imperial unit of length.


=== Conversion Factors ===


=== Symbol ===
 ",Definition:FPS/Length/Foot,['Definitions/Foot (Linear Measure)']
Definition:Foot,Foot,"Let $\triangle ABC$ be a triangle.

Let $h_a$ be the altitude of $A$:

:


The point at which $h_a$ meets $BC$ (or its production) is the foot of the altitude $h_a$.


Category:Definitions/Triangles",Definition:Altitude of Triangle/Foot,['Definitions/Triangles']
Definition:Foot,Foot,":


The foot of a perpendicular is the point where it intersects the line to which it is at right angles.

In the above diagram, the point $C$ is the foot of the perpendicular $CD$.",Definition:Right Angle/Perpendicular/Foot,['Definitions/Perpendiculars']
Definition:Formula,Formula,"Let $\FF$ be a formal language whose alphabet is $\AA$.

A well-formed formula is a collation in $\AA$ which can be built by using the rules of formation of the formal grammar of $\FF$.


That is, a collation in $\AA$ is a well-formed formula in $\FF$  if and only if  it has a parsing sequence in $\FF$.",Definition:Well-Formed Formula,['Definitions/Formal Languages']
Definition:Formula,Formula,"Let $\LL$ be a formal language used in the field of symbolic logic.


Then the well-formed formulas of $\LL$ are often referred to as logical formulas.

They are symbolic representations of statements, and often of compound statements in particular.",Definition:Logical Formula,['Definitions/Symbolic Logic']
Definition:Free,Free,"Let $S$ be a set.


A free monoid over $S$ is a monoid $M$ together with a mapping $i: S \to M$, subject to:

:For all monoids $N$, for all mappings $f: S \to N$, there is a unique monoid homomorphism $\bar f: M \to N$, such that:

::$\bar f \circ i = f$

This condition is called the universal (mapping) property or UMP of the free monoid over $S$.


=== Category-Theoretic Formulation ===

Let $\mathbf{Mon}$ be the category of monoids, and let $\mathbf{Set}$ be the category of sets.

Let $\left\vert{\cdot}\right\vert$ be the underlying set functor on $\mathbf{Mon}$.


Let $M \in \mathbf{Mon}_0$ be a monoid, and let $i: S \to \left\vert{M}\right\vert$ be a mapping.

Then $\left({M, i}\right)$ is said to be a free monoid over $S$  if and only if :

:For all $N \in \mathbf{Mon}_0$ and $f: S \to \left\vert{N}\right\vert \in \mathbf{Set}_1$, a unique $\bar f \in \mathbf{Mon}_1$ makes the following diagram commute:

::$\begin{xy}
<0em,4em>*{\mathbf{Mon} :},
<0em,1em>*{\mathbf{Set} :},

<4em,4em>*+{M} = ""M"",
<8em,4em>*+{N} = ""N"",
""M"";""N"" **@{.} ?>*@{>} ?*!/_1em/{\bar f},

<4em,1em>*+{\left\vert{M}\right\vert} = ""MM"",
<8em,1em>*+{\left\vert{N}\right\vert} = ""NN"",
<4em,-3em>*+{S} = ""S"",

""MM"";""NN"" **@{-} ?>*@{>} ?*!/_1em/{\left\vert{\bar f}\right\vert},
""S"";""MM"" **@{-} ?>*@{>} ?*!/_1em/{i},
""S"";""NN"" **@{-} ?>*@{>} ?*!/^1em/{f}
\end{xy}$

This condition is called the universal (mapping) property or UMP of the free monoid over $S$.",Definition:Free Monoid,"['Definitions/Monoids', 'Definitions/Category of Monoids', 'Definitions/Category of Sets']"
Definition:Free,Free,"The free commutative monoid on an indexed set $X = \family {X_j: j \in J}$ is the set $M$ of all monomials under the standard multiplication.

 
That is, it is the set $M$ of all finite sequences of $X$.",Definition:Free Commutative Monoid,"['Definitions/Monoids', 'Definitions/Examples of Monoids', 'Definitions/Polynomial Theory']"
Definition:Free,Free,"=== Definition 1 ===
A group $G$ is a free group  if and only if  it is isomorphic to the free group on some set.

=== Definition 2 ===
A group $G$ is a free group  if and only if  it has a presentation of the form $\gen S$, where $S$ is a set.

That is, it has a presentation without relators.",Definition:Free Group,"['Definitions/Group Theory', 'Definitions/Free Groups']"
Definition:Free,Free,"Let $X$ be a set.


A free group on $X$ is a certain $X$-pointed group, that is, a pair $\struct {F, \iota}$ where:
:$F$ is a group
:$\iota : X \to F$ is a mapping
that can be defined as follows:


=== Definition 1 ===
Let $X$ be a set.


A free group on $X$ is an $X$-pointed group $\struct {F, \iota}$ that satisfies the following universal property:
:For every $X$-pointed group $\struct {G, \kappa}$ there exists a unique group homomorphism $\phi : F \to G$ such that:
::$\phi \circ \iota = \kappa$
:that is, a morphism of pointed groups $F \to G$.

=== Definition 2 ===
Let $X$ be a set.


The free group on $X$ is the pair $\struct {F, \iota}$ such that:
:$F$ is the group of reduced group words on $X$
:$\iota : X \to F$ is the canonical injection.",Definition:Free Group on Set,"['Definitions/Free Groups', 'Definitions/Group Theory']"
Definition:Free,Free,"Let $G$ be an abelian group.


$G$ is a free abelian group  if and only if  the $\Z$-module associated with $G$ is a free $\Z$-module.

That is, $G$ is a free abelian group  if and only if  it has a basis over $\Z$.",Definition:Free Abelian Group,['Definitions/Abelian Groups']
Definition:Free,Free,"Let $\Z$ be the additive group of integers.

Let $S$ be a set.


The free abelian group on $S$ is the pair $\struct {\Z^{\paren S}, \iota}$ where:
* $\Z^{\paren S}$ is the direct sum of $S$ copies of $\Z$. That is, of the indexed family $S \to \set {\Z}$
* $\iota : S \to \Z^{\paren S}$ is the canonical mapping, which sends $s$ to the mapping $\delta_{st} \in \Z^{\paren S}$, where $\delta$ denotes Kronecker delta.",Definition:Free Abelian Group on Set,['Definitions/Abelian Groups']
Definition:Free,Free,"Let $R$ be a ring with unity.

Let $G$ be a unitary $R$-module.


Then $G$ is described as free  if and only if  there exists a basis of $G$.",Definition:Free Module,['Definitions/Module Theory']
Definition:Free,Free,"Let $R$ be a ring.

 

Let $I$ be an indexing set.


The free $R$-module on $I$ is the direct sum of $R$ as a module over itself:
:$\ds R^{\paren I} := \bigoplus_{i \mathop \in I} R$
of the family $I \to \set R$ to the singleton $\set R$.",Definition:Free Module on Set,"['Definitions/Module Theory', 'Definitions/Abstract Algebra']"
Definition:Free,Free,"Let $G$ be a group with identity $e$ acting on a set $X$.


The group action is free  if and only if :
:$\forall g \in G: \forall x \in X : g * x = x \implies g = e$",Definition:Free Group Action,['Definitions/Group Actions']
Definition:Frequency,Frequency,"Let $\map f t$ be a periodic function of time $t$.


The frequency of $f$ is the number of periods of $f$ that occur during a unit time interval.


=== Dimension ===
The dimension of measurement of frequency is:
:$\mathsf T^{-1}$


Category:Definitions/Dimensions of Measurement
Category:Definitions/Frequency

=== Units ===
The SI unit of measurement of frequency  is the hertz $\mathrm {Hz}$:
:$1 \ \mathrm {Hz} = 1 \ \mathrm{s}^{-1}$

that is, $1$ per second of time.",Definition:Frequency (Physics),"['Definitions/Physics', 'Definitions/Mechanics', 'Definitions/Applied Mathematics', 'Definitions/Frequency']"
Definition:Frequency,Frequency,"Let $f: \R \to \R$ be a periodic real function.

The frequency $\nu$ of $f$ is the reciprocal of the period $L$ of $f$:
:$\nu = \dfrac 1 L$

where:
:$\forall x \in X: \map f x = \map f {x + L}$",Definition:Periodic Real Function/Frequency,"['Definitions/Frequency of Periodic Real Function', 'Definitions/Periodic Functions']"
Definition:Frequency,Frequency,"Let $S$ be a sample or a population.

Let $\omega$ be a qualitative variable, or a class interval of a quantitative variable.


The frequency of $\omega$ is the number of individuals in $S$ satisfying $\omega$.",Definition:Frequency (Descriptive Statistics),"['Definitions/Frequency (Descriptive Statistics)', 'Definitions/Class Intervals', 'Definitions/Qualitative Variables', 'Definitions/Descriptive Statistics']"
Definition:Frequency,Frequency,"Let $S$ be a sample or a finite population.

Let $\omega$ be a qualitative variable, or a class interval of a quantitative variable.


The relative frequency of $\omega$ is defined as:

:$\map {\operatorname {RF} } \omega := \dfrac {f_\omega} n$

where:

:$f_\omega$ is the (absolute) frequency of $\omega$

:$n$ is the number of individuals in $S$.",Definition:Relative Frequency,"['Definitions/Relative Frequency', 'Definitions/Frequency (Descriptive Statistics)']"
Definition:Frequency,Frequency,"Let $\struct {\Omega, \Sigma, \Pr}$ be a probability space.

Let $X$ be a discrete random variable on $\struct {\Omega, \Sigma, \Pr}$.


=== Absolute ===
Let $\struct {\Omega, \Sigma, \Pr}$ be a probability space.

Let $X$ be a discrete random variable on $\struct {\Omega, \Sigma, \Pr}$.


The absolute cumulative frequency of $X$ is defined as:
:$\forall x \in \Dom X: \map {\text {acf} } x = \ds \sum_{y \mathop \le x} \map \Omega y$

 

=== Relative ===
Let $\struct {\Omega, \Sigma, \Pr}$ be a probability space.

Let $X$ be a discrete random variable on $\struct {\Omega, \Sigma, \Pr}$.


The relative cumulative frequency of $X$ is defined as:
:$\forall x \in \Dom X: \map {\text {acf} } x = \dfrac {\ds \sum_{y \mathop \le x} \map \Omega y} {\size {\Dom X} }$

 ",Definition:Cumulative Frequency,"['Definitions/Cumulative Frequency', 'Definitions/Descriptive Statistics']"
Definition:Frequency Curve,Frequency Curve,A frequency curve is a smooth curve approximating a frequency polygon for a large data set.,Definition:Frequency Curve (Statistics),"['Definitions/Frequency Curves (Statistics)', 'Definitions/Frequency Polygons', 'Definitions/Frequency Curves']"
Definition:Frequency Curve,Frequency Curve,A frequency curve is a curve representing a frequency function.,Definition:Frequency Curve (Probability Theory),"['Definitions/Frequency Curves (Probability Theory)', 'Definitions/Frequency Functions', 'Definitions/Frequency Curves']"
Definition:Frchet Space,Frchet Space,"Let $T = \struct {S, \tau}$ be a topological space.


=== Definition 1 ===

Let $T = \struct {S, \tau}$ be a topological space.


$\struct {S, \tau}$ is a Frchet space or $T_1$ space  if and only if :

:$\forall x, y \in S$ such that $x \ne y$, both:
::$\exists U \in \tau: x \in U, y \notin U$
:and:
::$\exists V \in \tau: y \in V, x \notin V$

That is:
:for any two distinct elements $x, y \in S$ there exist open sets $U, V \in \tau$ such that $x$ is in $U$ but not in $V$, and $y$ is in $V$ but not in $U$.


That is:
:$\struct {S, \tau}$ is $T_1$  if and only if  every two elements of $S$ are separated.

=== Definition 2 ===
Let $T = \struct {S, \tau}$ be a topological space.


$\struct {S, \tau}$ is a Frchet space or $T_1$ space  if and only if  all points of $S$ are closed in $T$.",Definition:Frchet Space (Topology),"['Definitions/T1 Spaces', 'Definitions/Separation Axioms']"
Definition:Frchet Space,Frchet Space,"Let $\R^\omega$ denote the countable-dimensional real Cartesian space.

Let:
:$x := \family {x_i}_{i \mathop \in \N} = \tuple {x_0, x_1, x_2, \ldots}$
and:
:$y := \family {y_i}_{i \mathop \in \N} = \tuple {y_0, y_1, y_2, \ldots}$
denote arbitrary elements of $\R^\omega$.


Let the distance function $d: \R^\omega \times \R^\omega \to \R$ be applied to $\R^\omega$ as:
:$\forall x, y \in \R^\omega: \map d {x, y} = \ds \sum_{i \mathop \in \N} \dfrac {2^{-i} \size {x_i - y_i} } {1 + \size {x_i - y_i} }$


The distance function $d$ is referred to as the Frchet (product) metric.


The resulting metric space $\struct {\R^\omega, d}$ is then referred to as the Frchet (metric) space.",Definition:Frchet Space (Functional Analysis),"['Definitions/Frchet Product Metric', 'Definitions/Examples of Metric Spaces']"
Definition:Galois Group,Galois Group,"Let $L / K$ be a field extension.


The Galois group of $L / K$ is the subgroup of the automorphism group of $L$ consisting of field automorphisms that fix $K$ point-wise:
:$\Gal {L / K} = \set {\sigma \in \Aut L: \forall k \in K: \map \sigma k = k}$


=== Topological Group ===
The notation $\Gal {L / K}$ is also a shorthand for the topological group:
:$\struct {\Gal {L / K}, \tau}$
where $\tau$ is the Krull topology.",Definition:Galois Group of Field Extension,"['Definitions/Galois Groups of Field Extensions', 'Definitions/Field Extensions', 'Definitions/Galois Theory', 'Definitions/Examples of Groups', 'Definitions/Galois Groups']"
Definition:Galois Group,Galois Group,"Let $P$ be a polynomial.

The Galois group of $P$ is the symmetry group of the roots of $P$.",Definition:Galois Group of Polynomial,"['Definitions/Galois Groups of Polynomials', 'Definitions/Galois Groups', 'Definitions/Polynomial Theory']"
Definition:Generated,Generated,"Let $\struct {A, \circ}$ be an algebraic structure.

Let $G \subseteq A$ be any subset of $A$.


The algebraic substructure generated by $G$ is the smallest substructure of $\struct {A, \circ}$ which contains $G$.


It is written $\gen G$.",Definition:Generated Algebraic Substructure,"['Definitions/Abstract Algebra', 'Definitions/Algebraic Structures']"
Definition:Generated,Generated,"Let $\struct {M, \circ}$ be a monoid whose identity is $e_M$.

Let $S \subseteq M$

Let $H$ be the smallest (with respect to set inclusion) submonoid of $M$ such that $\paren {S \cup \set {e_M} } \subseteq H$.


Then $\struct {H, \circ}$ is the submonoid of $\struct {M, \circ}$ generated by $S$.


This is written $H = \gen S$.


If $S$ is a singleton, for example $S = \set x$, then we can (and usually do) write $H = \gen x$ for $H = \gen {\set x}$.


=== Generator ===
Let $\struct {M, \circ}$ be a monoid

Let $S \subseteq M$.

Let $\struct {H, \circ}$ be the submonoid of $\struct {M, \circ}$ generated by $S$.

Then $S$ is known as a generator of $\struct {H, \circ}$.",Definition:Generated Submonoid,['Definitions/Monoids']
Definition:Generated,Generated,"Let $G$ be a group.

Let $S \subset G$ be a subset.


=== Definition 1 ===
Let $G$ be a group.

Let $S \subset G$ be a subset.


The subgroup generated by $S$ is the smallest subgroup containing $S$.

=== Definition 2 ===
Let $G$ be a group.

Let $S \subset G$ be a subset.


The subgroup generated by $S$ is the intersection of all subgroups of $G$ containing $S$.

=== Definition 3 ===
Let $G$ be a group.

Let $S \subset G$ be a subset.

Let $S^{-1}$ be the set of inverses of $S$.


The subgroup generated by $S$ is the set of words on the union $S \cup S^{-1}$.",Definition:Generated Subgroup,"['Definitions/Generated Subgroups', 'Definitions/Subgroups', 'Definitions/Generators of Groups']"
Definition:Generated,Generated,"Let $G$ be a group.

Let $S \subseteq G$ be a subset.


=== Definition 1 ===
Let $G$ be a group. 

Let $S \subseteq G$ be a subset.


The normal subgroup generated by $S$, denoted $\gen {S^G}$,  is the intersection of all normal subgroups of $G$ containing $S$.

=== Definition 2 ===
 

Let $G$ be a group. 

Let $S \subseteq G$ be a subset.


The normal subgroup generated by $S$, denoted $\gen {S^G}$,  is the smallest normal subgroup of $G$ containing $S$:
:$\gen {S^G} = \gen {x S x^{-1}: x \in G}$",Definition:Generated Normal Subgroup,"['Definitions/Generated Normal Subgroups', 'Definitions/Normal Subgroups', 'Definitions/Normality in Groups', 'Definitions/Generated Subgroups', 'Definitions/Generators of Groups', 'Definitions/Subgroups']"
Definition:Generated,Generated,"Let $R$ be a ring.

Let $M$ be an $R$-module.

Let $S\subset M$ be a subset of $M$.


=== $R$-module ===
Let $R$ be a ring.

Let $M = \struct {G, +, \circ}_R$ be an $R$-module.

Let $S \subset M$ be a subset of $M$.


The submodule generated by $S$ is the intersection of all submodules of $M$ containing $S$.

=== Unitary $R$-Module ===
Let $R$ be a ring with unity.

Let $M$ be a unitary $R$-module.

Let $S\subset M$ be a subset.


The submodule generated by $S$ is the set of all linear combinations of elements of $S$.",Definition:Generated Submodule,['Definitions/Generators of Modules']
Definition:Generated,Generated,"Let $\struct {R, +, \circ}$ be a ring with unity.

Let $S \subseteq R$ be a subset of $R$.


The ideal generated by $S$ is the smallest ideal of $R$ containing $S$, that is, the intersection of all ideals containing $S$.


=== Unitary Rings ===
Let $\struct {R, +, \circ}$ be a ring with unity.

Let $S \subseteq R$ be a subset of $R$.


The ideal generated by $S$ is the set of all two-sided linear combinations of elements of $S$.

=== Commutative and Unitary Rings ===
Let $\struct {R, +, \circ}$ be a commutative ring with unity.

Let $S \subseteq R$ be a subset of $R$.


The ideal generated by $S$ is the set of all linear combinations of elements of $S$.",Definition:Generated Ideal of Ring,"['Definitions/Generators of Ideals', 'Definitions/Ideal Theory']"
Definition:Generated,Generated,"Let $\struct {R, +, \circ}$ be a ring.

Let $S \subseteq R$ be a subset.


The subring generated by $S$ is the smallest subring of $R$ containing $S$; that is, it is the intersection of all subrings of $R$ containing $S$.",Definition:Generated Subring,['Definitions/Ring Theory']
Definition:Generated,Generated,"Let $S$ be a commutative rings with unity.

Let $R$ be a subring of $S$ with unity such that the unity of $R$ is the unity of $S$.

That is, $S$ is a ring extension of $R$.


Let $T \subseteq S$ be a subset of $S$.


=== Definition 1 ===
Let $S$ be a commutative rings with unity.

Let $R$ be a subring of $S$ with unity such that the unity of $R$ is the unity of $S$.

That is, $S$ is a ring extension of $R$.


Let $T \subseteq S$ be a subset of $S$.


=== Definition 1 ===
Let $S$ be a commutative rings with unity.

Let $R$ be a subring of $S$ with unity such that the unity of $R$ is the unity of $S$.

That is, $S$ is a ring extension of $R$.


Let $T \subseteq S$ be a subset of $S$.


=== Definition 1 ===


=== Definition 2 ===


The ring extension $R \sqbrk T$ generated by $T$ is the smallest subring of $S$ containing $T$ and $R$, that is, the intersection of all subrings of $S$ containing $T$ and $R$.

Thus $T$ is a generator of $R \sqbrk T$  if and only if  $R \sqbrk T$ has no proper subring containing $T$ and $R$.

=== Definition 2 ===
Let $S$ be a commutative rings with unity.

Let $R$ be a subring of $S$ with unity such that the unity of $R$ is the unity of $S$.

That is, $S$ is a ring extension of $R$.


Let $T \subseteq S$ be a subset of $S$.


=== Definition 1 ===


=== Definition 2 ===


Let $R \sqbrk {\set {X_t} }$ be the polynomial ring in $T$ variables $X_t$.

Let $\operatorname {ev} : R \sqbrk {\set {X_t} } \to S$ be the evaluation homomorphism associated with the inclusion $T \hookrightarrow S$.


The ring extension $R \sqbrk T$ generated by $T$ is $\Img {\operatorname {ev} }$, the image of $\operatorname {ev}$.

$T$ is said to be a generator of $R \sqbrk T$.

The ring extension $R \sqbrk T$ generated by $T$ is the smallest subring of $S$ containing $T$ and $R$, that is, the intersection of all subrings of $S$ containing $T$ and $R$.

Thus $T$ is a generator of $R \sqbrk T$  if and only if  $R \sqbrk T$ has no proper subring containing $T$ and $R$.

=== Definition 2 ===
Let $S$ be a commutative rings with unity.

Let $R$ be a subring of $S$ with unity such that the unity of $R$ is the unity of $S$.

That is, $S$ is a ring extension of $R$.


Let $T \subseteq S$ be a subset of $S$.


=== Definition 1 ===
Let $S$ be a commutative rings with unity.

Let $R$ be a subring of $S$ with unity such that the unity of $R$ is the unity of $S$.

That is, $S$ is a ring extension of $R$.


Let $T \subseteq S$ be a subset of $S$.


=== Definition 1 ===


=== Definition 2 ===


The ring extension $R \sqbrk T$ generated by $T$ is the smallest subring of $S$ containing $T$ and $R$, that is, the intersection of all subrings of $S$ containing $T$ and $R$.

Thus $T$ is a generator of $R \sqbrk T$  if and only if  $R \sqbrk T$ has no proper subring containing $T$ and $R$.

=== Definition 2 ===
Let $S$ be a commutative rings with unity.

Let $R$ be a subring of $S$ with unity such that the unity of $R$ is the unity of $S$.

That is, $S$ is a ring extension of $R$.


Let $T \subseteq S$ be a subset of $S$.


=== Definition 1 ===


=== Definition 2 ===


Let $R \sqbrk {\set {X_t} }$ be the polynomial ring in $T$ variables $X_t$.

Let $\operatorname {ev} : R \sqbrk {\set {X_t} } \to S$ be the evaluation homomorphism associated with the inclusion $T \hookrightarrow S$.


The ring extension $R \sqbrk T$ generated by $T$ is $\Img {\operatorname {ev} }$, the image of $\operatorname {ev}$.

$T$ is said to be a generator of $R \sqbrk T$.

Let $R \sqbrk {\set {X_t} }$ be the polynomial ring in $T$ variables $X_t$.

Let $\operatorname {ev} : R \sqbrk {\set {X_t} } \to S$ be the evaluation homomorphism associated with the inclusion $T \hookrightarrow S$.


The ring extension $R \sqbrk T$ generated by $T$ is $\Img {\operatorname {ev} }$, the image of $\operatorname {ev}$.

$T$ is said to be a generator of $R \sqbrk T$.",Definition:Generated Ring Extension,['Definitions/Ring Extensions']
Definition:Generated,Generated,"Let $E / F$ be a field extension.

Let $S \subset E$ be a subset of $E$.


=== Definition 1 ===
Let $E / F$ be a field extension.

Let $S \subset E$ be a subset of $E$.


The field extension $F \sqbrk S$ generated by $S$ is the smallest subfield extension of $E$ containing $S$, that is, the intersection of all subfields of $E$ containing $S$ and $F$.

Thus $S$ is a generator of $F \sqbrk S$  if and only if  $F \sqbrk S$ has no proper subfield extension containing $S$.

=== Definition 2 ===
Let $E / F$ be a field extension.

Let $S \subset E$ be a subset of $E$.


Let $F \sqbrk {\set {X_s} }$ be the polynomial ring in $S$ variables $X_s$.

Let $\operatorname {ev} : F \sqbrk {\set {X_s} } \to E$ be the evaluation homomorphism associated to the inclusion $S \hookrightarrow E$.


The field extension $F \sqbrk S$ generated by $S$ is the set of all elements of $E$ of the form $\map {\operatorname {ev} } f / \map {\operatorname {ev} } g$, where $\map {\operatorname {ev} } g \ne 0$.

$S$ is said to be a generator of $F \sqbrk S$.",Definition:Generated Field Extension,['Definitions/Field Extensions']
Definition:Generated,Generated,"Let $S$ be a set.

Let $\BB$ be a synthetic basis of $S$.


=== Definition 1 ===
Let $S$ be a set.

Let $\BB$ be a synthetic basis of $S$.


The topology on $S$ generated by $\BB$ is defined as:
:$\tau = \set{\bigcup \AA: \AA \subseteq \BB}$

That is, the set of all unions of sets from $\BB$.


Category:Definitions/Topology

=== Definition 2 ===
Let $S$ be a set.

Let $\BB$ be a synthetic basis of $S$.


The topology on $S$ generated by $\BB$ is defined as:
:$\tau = \set {U \subseteq S: U = \bigcup \set {B \in \BB: B \subseteq U}}$


Category:Definitions/Topology

=== Definition 3 ===
Let $S$ be a set.

Let $\BB$ be a synthetic basis of $S$.


The topology on $S$ generated by $\BB$ is defined as:
:$\tau = \set {U \subseteq S: \forall x \in U: \exists B \in \BB: x \in B \subseteq U}$


Category:Definitions/Topology",Definition:Topology Generated by Synthetic Basis,['Definitions/Topology']
Definition:Generated,Generated,"Let $X$ be a set.

Let $\SS \subseteq \powerset X$ be a synthetic sub-basis on $X$.


=== Definition 1 ===
Let $S$ be a set.

Let $\SS \subseteq \powerset S$ be a synthetic sub-basis on $S$.


Define:
:$\ds \BB = \set {\bigcap \FF: \FF \subseteq \SS, \FF \text{ is finite} }$

That is, $\BB$ is the set of all finite intersections of sets in $\SS$.

Note that $\FF$ is allowed to be empty in the above definition.


The topology generated by $\SS$, denoted $\map \tau \SS$, is defined as:
:$\ds \map \tau \SS = \set {\bigcup \AA: \AA \subseteq \BB}$


It follows directly from Synthetic Basis formed from Synthetic Sub-Basis and Union from Synthetic Basis is Topology that $\map \tau \SS$ is a topology on $S$.

=== Definition 2 ===
Let $X$ be a set.

Let $\SS \subseteq \powerset X$ be a synthetic sub-basis on $X$.


The topology generated by $\SS$, denoted $\map \tau \SS$, is defined as the unique topology on $X$ that satisfies the following axioms:
:$(1): \quad \SS \subseteq \map \tau \SS$
:$(2): \quad$ For any topology $\TT$ on $X$, the implication $\SS \subseteq \TT \implies \map \tau \SS \subseteq \TT$ holds.

That is, $\map \tau \SS$ is the coarsest topology on $X$ for which every element of $\SS$ is open.",Definition:Topology Generated by Synthetic Sub-Basis,"['Definitions/Topology', 'Definitions/Sub-Bases', 'Definitions/Topology Generated by Synthetic Sub-Basis']"
Definition:Generated,Generated,"Let $X$ be a set.

Let $\GG \subseteq \powerset X$ be a collection of subsets of $X$.


Then the Dynkin system generated by $\GG$, denoted $\map \delta \GG$, is the smallest Dynkin system on $X$ that contains $\GG$.

That is, $\map \delta \GG$ is subject to:

:$(1):\quad \GG \subseteq \map \delta \GG$
:$(2):\quad \GG \subseteq \DD \implies \map \delta \GG \subseteq \DD$ for any Dynkin system $\DD$ on $X$


In fact, $\map \delta \GG$ always exists, and is unique, as proved on Existence and Uniqueness of Dynkin System Generated by Collection of Subsets.


=== Generator ===

One says that $\GG$ is a generator for $\map \delta \GG$.",Definition:Dynkin System Generated by Collection of Subsets,['Definitions/Dynkin Systems']
Definition:Generated,Generated,"Let $X$ be a set.

Let $\GG \subseteq \powerset X$ be a collection of subsets of $X$.


=== Definition 1 ===
Let $X$ be a set.

Let $\GG \subseteq \powerset X$ be a collection of subsets of $X$.


The $\sigma$-algebra generated by $\GG$, denoted $\map \sigma \GG$, is the smallest $\sigma$-algebra on $X$ that contains $\GG$.

That is, $\map \sigma \GG$ is subject to:

:$(1): \quad \GG \subseteq \map \sigma \GG$
:$(2): \quad$ for all $\sigma$-algebras $\Sigma$ on $X$: $\GG \subseteq \Sigma \implies \map \sigma \GG \subseteq \Sigma$


=== Generator ===
Let $X$ be a set.

Let $\GG \subseteq \powerset X$ be a collection of subsets of $X$.

Let $\map \sigma {\GG}$ be the $\sigma$-algebra generated by $\GG$.


One says that $\GG$ is a generator for $\map \sigma {\GG}$.

Also, elements $G$ of $\GG$ may be called generators.

=== Definition 2 ===
Let $X$ be a set.

Let $\GG \subseteq \powerset X$ be a collection of subsets of $X$.


The $\sigma$-algebra generated by $\GG$, $\map \sigma \GG$, is the intersection of all $\sigma$-algebras on $X$ that contain $\GG$.


=== Generator ===
Let $X$ be a set.

Let $\GG \subseteq \powerset X$ be a collection of subsets of $X$.

Let $\map \sigma {\GG}$ be the $\sigma$-algebra generated by $\GG$.


One says that $\GG$ is a generator for $\map \sigma {\GG}$.

Also, elements $G$ of $\GG$ may be called generators.

=== Generator ===
Let $X$ be a set.

Let $\GG \subseteq \powerset X$ be a collection of subsets of $X$.

Let $\map \sigma {\GG}$ be the $\sigma$-algebra generated by $\GG$.


One says that $\GG$ is a generator for $\map \sigma {\GG}$.

Also, elements $G$ of $\GG$ may be called generators.",Definition:Sigma-Algebra Generated by Collection of Subsets,['Definitions/Sigma-Algebras']
Definition:Generated,Generated,"Let $I$ be an indexing set.

Let $\family {\struct {X_i, \Sigma_i} }_{i \mathop \in I}$ be a family of measurable spaces.

Let $X$ be a set.

Let $\family {f_i: X \to X_i}_{i \mathop \in I}$ be a family of mappings.


Then the $\sigma$-algebra generated by $\family {f_i}_{i \mathop \in I}$, $\map \sigma {f_i: i \in I}$, is the smallest $\sigma$-algebra on $X$ such that every $f_i$ is $\map \sigma {f_i: i \in I} \, / \, \Sigma_i$-measurable.

That is, $\map \sigma {f_i: i \in I}$ is subject to:

:$(1):\quad \forall i \in I: \forall E_i \in \Sigma_i: \map {f_i^{-1} } {E_i} \in \map \sigma {f_i: i \in I}$
:$(2):\quad \map \sigma {f_i: i \in I} \subseteq \Sigma$ for all $\sigma$-algebras $\Sigma$ on $X$ satisfying $(1)$


In fact, $\map \sigma {f_i: i \in I}$ always exists, and is unique, as proved on Existence and Uniqueness of Sigma-Algebra Generated by Collection of Mappings.",Definition:Sigma-Algebra Generated by Collection of Mappings,['Definitions/Sigma-Algebras']
Definition:Generated,Generated,"Let $X$ be a set.

Let $\GG \subseteq \powerset X$ be a collection of subsets of $X$.


Then the monotone class generated by $\GG$, $\map {\mathfrak m} \GG$, is the smallest monotone class on $X$ that contains $\GG$.

That is, $\map {\mathfrak m} \GG$ is subject to:

:$(1): \quad \GG \subseteq \map {\mathfrak m} \GG$
:$(2): \quad \GG \subseteq \MM \implies \map {\mathfrak m} \GG \subseteq \MM$ for any monotone class $\MM$ on $X$


=== Generator ===

One says that $\GG$ is a generator for $\map {\mathfrak m} \GG$.",Definition:Monotone Class Generated by Collection of Subsets,['Definitions/Set Systems']
Definition:Generated,Generated,"Let $S$ be a set.

Let $\powerset S$ be the power set of $S$.

Let $\BB \subset \powerset S$ be a filter basis of a filter $\FF$ on $S$.


$\FF$ is said to be generated by $\BB$.",Definition:Filter Basis/Generated Filter,['Definitions/Filter Bases']
Definition:Generator,Generator,"Let $\struct {A, \circ}$ be an algebraic structure.

Let $G \subset A$ be a subset.


=== Definition 1 ===

The subset $G$ is a generator of $A$  if and only if  $A$ is the algebraic substructure generated by $G$.


=== Definition 2 ===

The subset $G$ is a generator of $A$  if and only if :

:$\forall x, y \in G: x \circ y \in A$;
:$\forall z \in A: \exists x, y \in \map W G: z = x \circ y$
where $\map W G$ is the set of words of $G$.

That is, every element in $A$ can be formed as the product of a finite number of elements of $G$.


If $G$ is such a set, then we can write $A = \gen G$.",Definition:Generator of Algebraic Structure,['Definitions/Algebraic Structures']
Definition:Generator,Generator,"Let $\struct {S, \circ}$ be a semigroup.

Let $\O \subset X \subseteq S$.

Let $\struct {T, \circ}$ be the smallest subsemigroup of $\struct {S, \circ}$ such that $X \subseteq T$.


Then:
:$X$ is a generator  of $\struct {T, \circ}$
:$X$ generates $\struct {T, \circ}$
:$\struct {T, \circ}$ is the subsemigroup of $\struct {S, \circ}$ generated by $X$.


This is written:
:$T = \gen X$",Definition:Generator of Subsemigroup,"['Definitions/Semigroups', 'Definitions/Subsemigroups']"
Definition:Generator,Generator,"Let $\struct {M, \circ}$ be a monoid.

Let $S \subseteq M$.

Let $H$ be the smallest submonoid of $M$ such that $S \subseteq H$.


Then:
:$S$ is a generator of $\struct {H, \circ}$
:$S$ generates $\struct {H, \circ}$
:$\struct {H, \circ}$ is the submonoid of $\struct {M, \circ}$ generated by $S$.


This is written $H = \gen S$.


If $S$ is a singleton, for example $S = \set x$, then we can (and usually do) write $H = \gen x$ for $H = \gen {\set x}$.",Definition:Generator of Monoid,['Definitions/Monoids']
Definition:Generator,Generator,"Let $\struct {G, \circ}$ be a group.

Let $S \subseteq G$.


Then $S$ is a generator of $G$, denoted $G = \gen S$,  if and only if  $G$ is the subgroup generated by $S$.",Definition:Generator of Group,"['Definitions/Generators of Groups', 'Definitions/Group Theory']"
Definition:Generator,Generator,"Let $\struct {M, \circ}$ be a monoid

Let $S \subseteq M$.

Let $\struct {H, \circ}$ be the submonoid of $\struct {M, \circ}$ generated by $S$.

Then $S$ is known as a generator of $\struct {H, \circ}$.",Definition:Generated Submonoid/Generator,['Definitions/Monoids']
Definition:Generator,Generator,"Let $\struct {G, \circ}$ be a group.

Let $S \subseteq G$.

Let $H$ be the subgroup generated by $S$.


Then $S$ is a generator of $H$, denoted $H = \gen S$,  if and only if  $H$ is the subgroup generated by $S$.


=== Definition by Predicate ===
A generator of a subgroup can be defined by a predicate.

For example:

:$\gen {x \in G: x^2 = e}$

defines the subgroup of $G$ generated by the elements of $G$ of order $2$.",Definition:Generator of Subgroup,"['Definitions/Group Theory', 'Definitions/Generators of Groups']"
Definition:Generator,Generator,"Let $G$ be a cyclic group generated by the element $g$.

Let $a \in G$ be an element of $G$ such that $\gen a = G$.

Then $a$ is a generator of $G$.",Definition:Cyclic Group/Generator,"['Definitions/Cyclic Groups', 'Definitions/Generators of Groups']"
Definition:Generator,Generator,"Let $R$ be a commutative ring. 

Let $I \subset R$ be an ideal.

Let $S \subset I$ be a subset.


Then:
:$S$ is a generator of $I$
 if and only if :
:$I$ is the ideal generated by $S$.",Definition:Generator of Ideal of Ring,"['Definitions/Generators of Ideals', 'Definitions/Ideal Theory', 'Definitions/Ring Theory']"
Definition:Generator,Generator,"Let $R$ be a ring.

Let $M$ be an $R$-module.

Let $S \subseteq M$ be a subset.


=== Definition 1 ===
Let $R$ be a ring.

Let $M$ be an $R$-module.

Let $S \subseteq M$ be a subset.


$S$ is a generator of $M$  if and only if  $M$ is the submodule generated by $S$.

=== Definition 2 ===
Let $R$ be a ring.

Let $M$ be an $R$-module.

Let $S \subseteq M$ be a subset.


$S$ is a generator of $M$  if and only if  $M$ has no proper submodule containing $S$.",Definition:Generator of Module,"['Definitions/Generators of Modules', 'Definitions/Module Theory', 'Definitions/Linear Algebra']"
Definition:Generator,Generator,"Let $R$ be a ring.

Let $S \subset R$ be a subset.


Then $S$ is a generator of $R$  if and only if  $R$ is the subring generated by $S$.",Definition:Generator of Ring,['Definitions/Ring Theory']
Definition:Generator,Generator,"Let $\struct {D, +, \circ}$ be a division ring.

Let $S \subseteq D$.


The division subring generated by $S$ is the smallest division subring of $D$ containing $S$.",Definition:Generated Division Subring,['Definitions/Division Rings']
Definition:Generator,Generator,"Let $F$ be a field.

Let $S \subseteq F$ be a subset and $K \le F$ a subfield.


The field generated by $S$ is the smallest subfield of $F$ containing $S$.

The subring of $F$ generated by $K \cup S$, written $K \sqbrk S$, is the smallest subring of $F$ containing $K \cup S$.

The subfield of $F$ generated by $K \cup S$, written $\map K S$, is the smallest subfield of $F$ containing $K \cup S$.",Definition:Generator of Field,['Definitions/Field Theory']
Definition:Generator,Generator,"Let $E / F$ be a field extension.

Let $S \subset E$ be a subset of $E$.


=== Definition 1 ===
Let $E / F$ be a field extension.

Let $S \subset E$ be a subset of $E$.


The field extension $F \sqbrk S$ generated by $S$ is the smallest subfield extension of $E$ containing $S$, that is, the intersection of all subfields of $E$ containing $S$ and $F$.

Thus $S$ is a generator of $F \sqbrk S$  if and only if  $F \sqbrk S$ has no proper subfield extension containing $S$.

=== Definition 2 ===
Let $E / F$ be a field extension.

Let $S \subset E$ be a subset of $E$.


Let $F \sqbrk {\set {X_s} }$ be the polynomial ring in $S$ variables $X_s$.

Let $\operatorname {ev} : F \sqbrk {\set {X_s} } \to E$ be the evaluation homomorphism associated to the inclusion $S \hookrightarrow E$.


The field extension $F \sqbrk S$ generated by $S$ is the set of all elements of $E$ of the form $\map {\operatorname {ev} } f / \map {\operatorname {ev} } g$, where $\map {\operatorname {ev} } g \ne 0$.

$S$ is said to be a generator of $F \sqbrk S$.",Definition:Generated Field Extension,['Definitions/Field Extensions']
Definition:Generator,Generator,"Let $\struct {A_R, \oplus}$ be an algebra over a ring $R$.

Let $S \subseteq A_R$ be a subset of $A_R$.


The subalgebra generated by $S$ is the smallest subalgebra $B_R$ of $A_R$ which contains $S$.",Definition:Generator of Algebra,['Definitions/Algebras']
Definition:Generator,Generator,"Let $K$ be a division ring.

Let $\mathbf V$ be a vector space over $K$.

Let $S \subseteq \mathbf V$ be a subset of $\mathbf V$.


$S$ is a generator of $\mathbf V$  if and only if  every element of $\mathbf V$ is a linear combination of elements of $S$.",Definition:Generator of Vector Space,"['Definitions/Generators of Vector Spaces', 'Definitions/Vector Spaces', 'Definitions/Linear Algebra']"
Definition:Generator,Generator,"A generatrix is an element of a set of straight lines which constitute a given surface.


=== Generatrix of Right Circular Cone ===


Let $K$ be a right circular cone.

Let $A$ be the apex of $K$.

Let $B$ be the base of $K$.


Then a line joining the apex of $K$ to its directrix is a generatrix of $K$.

=== Generatrix of Cylinder ===
Each of the parallel straight lines forming a cylindrical surface $S$ which pass through the directrix of $S$ is called a generatrix of $S$.",Definition:Generatrix,"['Definitions/Solid Geometry', 'Definitions/Generatrices']"
Definition:Genus,Genus,"Let $S$ be a surface.

Let $G = \struct {V, E}$ be a graph which is embedded in $S$.

Let $G$ be such that each of its faces is a simple closed curve.

Let $\map \chi G = v - e + f = 2 - 2 p$ be the Euler characteristic of $G$ where:
:$v = \size V$ is the number of vertices
:$e = \size E$ is the number of edges
:$f$ is the number of faces.

Then $p$ is known as the genus of $S$.",Definition:Genus of Surface,"['Definitions/Genera of Surfaces', 'Definitions/Graph Theory', 'Definitions/Genera']"
Definition:Genus,Genus,The genus of a compact topological manifold is the number of handles it has.,Definition:Genus of Manifold,"['Definitions/Genera of Manifolds', 'Definitions/Topological Manifolds', 'Definitions/Genera']"
Definition:Genus,Genus,The genus of a Riemann surface $R$ is the number of linearly independent holomorphic $1$-forms that are defined on $R$.,Definition:Genus of Riemann Surface,"['Definitions/Genera of Riemann Surfaces', 'Definitions/Riemann Surfaces', 'Definitions/Genera']"
Definition:Genus,Genus,"Let $\CC$ be a plane algebraic curve with no singular points.

The genus of $\CC$ is defined as:
:$\dbinom {d - 1} 2$
where $d$ denotes the degree of $\CC$.


=== Singular Points ===
Let $\CC$ be a plane algebraic curve which has $1$ or more singular points.

The genus of $\CC$ is defined as:
:$\dbinom {d - 2} 2 - \sum \delta$
where:
:$d$ denotes the degree of $\CC$
:each term of the summation corresponds to one of the singular points of $\CC$
:$\delta$ is $1$ for a double point, but for more complicated singular points is larger.",Definition:Genus of Plane Algebraic Curve,"['Definitions/Genera of Plane Algebraic Curves', 'Definitions/Algebraic Curves', 'Definitions/Genera']"
Definition:Grade,Grade,"The grad is a measurement of plane angle.

It is defined as $\dfrac 1 {100}$ of a right angle.

 
 
 
 ",Definition:Grad,"['Definitions/Grads', 'Definitions/Angles', 'Definitions/Units of Measurement']"
Definition:Grade,Grade,"=== Straight Line ===
Let $\LL$ be a straight line embedded in a Cartesian plane.

The slope of $\LL$ is defined as the tangent of the angle that $\LL$ makes with the $x$-axis.


=== General Form ===
Let $\LL$ be a straight line embedded in a Cartesian plane.

Let $\LL$ be given by the equation:

:$l x + m y + n = 0$

The slope of $\LL$ is defined by means of the ordered pair $\tuple {-l, m}$, where:
:for $m \ne 0$, $\psi = \map \arctan {-\dfrac l m}$
:for $m = 0$, $\psi = \dfrac \pi 2$
where $\psi$ is the angle that $\LL$ makes with the $x$-axis.


Thus:
:when $m = 0$, the slope of $\LL$ is $\tuple {l, 0}$ and $\LL$ is parallel to the $y$-axis
:when $l = 0$, the slope of $\LL$ is $\tuple {0, m}$ and $\LL$ is parallel to the $x$-axis.

=== Curve ===
Let $P$ be a point on a curve $\CC$ embedded in a Cartesian plane.

The slope of $\CC$ at $P$ is defined as the slope of the tangent to $\CC$ at $P$.",Definition:Slope,"['Definitions/Slope', 'Definitions/Geometry', 'Definitions/Analytic Geometry']"
Definition:Graph,Graph,"Graph theory is the branch of mathematics concerned with the structure and properties of graphs.

As a (graph-theoretical) graph has the same conceptual definition as a relation, it follows that there is considerable overlap between the fields of graph theory and relation theory.",Definition:Graph Theory,"['Definitions/Graph Theory', 'Definitions/Topology', 'Definitions/Relation Theory', 'Definitions/Branches of Mathematics']"
Definition:Graph,Graph,"A graph is intuitively defined as a pair consisting of a set of vertices and a set of edges each with a vertex at each end.


=== Vertex ===
 

Let $G = \struct {V, E}$ be a graph.

The vertices (singular: vertex) are the elements of $V$.

Informally, the vertices are the points that are connected by the edges.


In the above, the vertices are the points $A, B, C, D, E, F, G$ which are marked as dots.

=== Edge ===
 


Let $G = \struct {V, E}$ be a graph.

The edges are the elements of $E$.


In the above, the edges are $AB, AE, BE, CD, CE, CF, DE, DF, FG$.


=== Join ===
Let $G = \struct {V, E}$ be a graph.

Let $u$ and $v$ be vertices of $G$.

Let $e = u v$ be an edge of $G$.


Then $e$ joins the vertices $u$ and $v$.",Definition:Graph (Graph Theory),"['Definitions/Graphs (Graph Theory)', 'Definitions/Graph Theory']"
Definition:Graph,Graph,"A graph is an interpretation of a metagraph within set theory.


Let $\mathfrak U$ be a class of sets.

A metagraph $\GG$ is a graph  if and only if :

:$(1): \quad$ The objects form a subset $\operatorname{vert} \GG \subseteq \mathfrak U$

:$(2): \quad$ The morphisms form a subset $\operatorname{edge} \GG \subseteq \mathfrak U$

 

If the class $\mathfrak U$ is a set, then morphisms are functions, and the domain and codomain in the definition of a morphism are those familiar from set theory.

If $\mathfrak U$ is a proper class this is not the case, for example the morphisms of $\CC$ need not be functions.",Definition:Graph (Category Theory),['Definitions/Category Theory']
Definition:Graph,Graph,"Let $S$ and $T$ be sets.

Let $f: S \to T$ be a mapping.


The graph of $f$ is the relation $\RR \subseteq S \times T$ defined as $\RR = \set {\tuple {x, \map f x}: x \in S}$


Alternatively, this can be expressed:
:$G_f = \set {\tuple {s, t} \in S \times T: \map f s = t}$
where $G_f$ is the graph of $f$.


The word is usually used in the context of a diagram:


:

=== Graph of Real Function ===
Let $U \subseteq \R^n$ be an open subset of $n$-dimensional Euclidean space.

Let $f : U \to \R^k$ be a real function.


The graph $\map \Gamma f$ of the function $f$ is the subset of $\R^n \times \R^k$ such that:

:$\map \Gamma f = \set {\tuple {x, y} \in \R^n \times \R^k: x \in U \subseteq \R^n : \map f x = y}$

where $\times$ denotes the Cartesian product.

=== Graph of Relation ===

The concept can still be applied when $f$ is a relation, but in this case a vertical line through a point in the graph is not guaranteed to intersect the graph at one and only one point.

Let $S \times T$ be the cartesian product of two sets $S$ and $T$.

Let $\RR$ be a relation on $S \times T$.


The graph of $\RR$ is the set of all ordered pairs $\tuple {s, t}$ of $S \times T$ such that $s \mathrel \RR t$:
:$\map \TT \RR = \set {\tuple {s, t}: s \mathrel \RR t}$",Definition:Graph of Mapping,"['Definitions/Graphs of Mappings', 'Definitions/Graphs of Relations', 'Definitions/Real Functions', 'Definitions/Mapping Theory']"
Definition:Graph,Graph,"Let $U \subseteq \R^n$ be an open subset of $n$-dimensional Euclidean space.

Let $f : U \to \R^k$ be a real function.


The graph $\map \Gamma f$ of the function $f$ is the subset of $\R^n \times \R^k$ such that:

:$\map \Gamma f = \set {\tuple {x, y} \in \R^n \times \R^k: x \in U \subseteq \R^n : \map f x = y}$

where $\times$ denotes the Cartesian product.",Definition:Graph of Real Function,"['Definitions/Real Functions', 'Definitions/Mapping Theory']"
Definition:Graph,Graph,"Let $S \times T$ be the cartesian product of two sets $S$ and $T$.

Let $\RR$ be a relation on $S \times T$.


The graph of $\RR$ is the set of all ordered pairs $\tuple {s, t}$ of $S \times T$ such that $s \mathrel \RR t$:
:$\map \TT \RR = \set {\tuple {s, t}: s \mathrel \RR t}$",Definition:Relation/Graph,"['Definitions/Graphs of Relations', 'Definitions/Relations']"
Definition:Graph,Graph,"In the context of statistics, a graph is a word that loosely means a pictorial or diagrammatic presentation of a set of data.


=== Bar Chart ===
A bar chart is a form of graph which consists of a finite set of (usually) vertical bars whose length determines the statistic being communicated.


:

=== Pie Chart ===
A pie chart is a form of graph which consists of a circle divided into sectors whose areas represent the proportion of the corresponding statistic relative to the whole.


:

=== Pictograph ===
A pictograph is a variant of a bar chart whose  bars consist of a number of icons or other pictorial symbols arranged in a (usually) line.


:$\begin {array} {r|l} \text {Happy} &  \\ \text {Sad} &  \\ \text {Unimpressed} &  \\ \text {Disgusted} &  \\ \end {array}$",Definition:Graph (Statistics),"['Definitions/Statistics', 'Definitions/Diagrams', 'Definitions/Graphs (Statistics)']"
Definition:Harmonic,Harmonic,"Let $x_1, x_2, \ldots, x_n \in \R$ be real numbers which are all strictly positive.

The harmonic mean $H_n$ of $x_1, x_2, \ldots, x_n$ is defined as:

:$\ds \dfrac 1 {H_n} := \frac 1 n \paren {\sum_{k \mathop = 1}^n \frac 1 {x_k} }$

That is, to find the harmonic mean of a set of $n$ numbers, take the reciprocal of the arithmetic mean of their reciprocals.",Definition:Harmonic Mean,"['Definitions/Harmonic Mean', 'Definitions/Pythagorean Means', 'Definitions/Measures of Central Tendency', 'Definitions/Algebra', 'Definitions/Number Theory', 'Definitions/Analysis']"
Definition:Harmonic,Harmonic,"The harmonic numbers are denoted $H_n$ and are defined for positive integers $n$:
:$\ds \forall n \in \Z, n \ge 0: H_n = \sum_{k \mathop = 1}^n \frac 1 k$

From the definition of vacuous summation it is clear that $H_0 = 0$.

=== General Harmonic Numbers ===
Let $r \in \R_{>0}$.

For $n \in \N_{> 0}$ the harmonic numbers order $r$ are defined as follows:
:$\ds \map {H^{\paren r} } n = \sum_{k \mathop = 1}^n \frac 1 {k^r}$


=== Complex Extension ===
Let $r \in \R_{>0}$.

For $z \in \C \setminus \Z_{< 0}$ the harmonic numbers order $r$ can be extended to the complex plane as:
:$\ds \harm r z = \sum_{k \mathop = 1}^{\infty} \paren {\frac 1 {k^r} - \frac 1 {\paren {k + z}^r} }$

 ",Definition:Harmonic Numbers,"['Definitions/Harmonic Numbers', 'Definitions/Discrete Mathematics', 'Definitions/Number Theory', 'Definitions/Real Analysis']"
Definition:Harmonic,Harmonic,"Let $r \in \R_{>0}$.

For $n \in \N_{> 0}$ the harmonic numbers order $r$ are defined as follows:
:$\ds \map {H^{\paren r} } n = \sum_{k \mathop = 1}^n \frac 1 {k^r}$


=== Complex Extension ===
Let $r \in \R_{>0}$.

For $z \in \C \setminus \Z_{< 0}$ the harmonic numbers order $r$ can be extended to the complex plane as:
:$\ds \harm r z = \sum_{k \mathop = 1}^{\infty} \paren {\frac 1 {k^r} - \frac 1 {\paren {k + z}^r} }$

 ",Definition:Harmonic Numbers/General Definition,"['Definitions/General Harmonic Numbers', 'Definitions/Harmonic Numbers', 'Definitions/P-Series', 'Definitions/Riemann Zeta Function']"
Definition:Harmonic,Harmonic,"Let $n \in \Z_{>0}$ be a positive integer.

$n$ is an Ore number  if and only if  the harmonic mean of its divisors is an integer.",Definition:Ore Number,"['Definitions/Ore Numbers', 'Definitions/Number Theory']"
Definition:Harmonic,Harmonic,"The Leibniz Harmonic Triangle is a triangular array where:
:the zeroth element in the $n$th row, counting from $0$, is $\dfrac 1 {n + 1}$
:subsequent elements in the $n$th row are the zeroth element divided by the corresponding element of Pascal's triangle:


:$\begin{array}{r|rrrrrr}
n & 0 & 1 & 2 & 3 & 4 & 5 \\
\hline
0  & \frac 1 1 \\
1  & \frac 1 2 & \frac 1 2 \\
2  & \frac 1 3 & \frac 1 6 &  \frac 1 3 \\
3  & \frac 1 4 & \frac 1 {12} & \frac 1 {12} & \frac 1 4 \\
4  & \frac 1 5 & \frac 1 {20} & \frac 1 {30} & \frac 1 {20} & \frac 1 5 \\
5  & \frac 1 6 & \frac 1 {30} & \frac 1 {60} & \frac 1 {60} & \frac 1 {30} & \frac 1 6 \\
\end{array}$


=== Row ===
Consider the Leibniz harmonic triangle:
The Leibniz Harmonic Triangle is a triangular array where:
:the zeroth element in the $n$th row, counting from $0$, is $\dfrac 1 {n + 1}$
:subsequent elements in the $n$th row are the zeroth element divided by the corresponding element of Pascal's triangle:


:$\begin{array}{r|rrrrrr}
n & 0 & 1 & 2 & 3 & 4 & 5 \\
\hline
0  & \frac 1 1 \\
1  & \frac 1 2 & \frac 1 2 \\
2  & \frac 1 3 & \frac 1 6 &  \frac 1 3 \\
3  & \frac 1 4 & \frac 1 {12} & \frac 1 {12} & \frac 1 4 \\
4  & \frac 1 5 & \frac 1 {20} & \frac 1 {30} & \frac 1 {20} & \frac 1 5 \\
5  & \frac 1 6 & \frac 1 {30} & \frac 1 {60} & \frac 1 {60} & \frac 1 {30} & \frac 1 6 \\
\end{array}$


=== Row ===
Consider the Leibniz harmonic triangle:
The Leibniz Harmonic Triangle is a triangular array where:
:the zeroth element in the $n$th row, counting from $0$, is $\dfrac 1 {n + 1}$
:subsequent elements in the $n$th row are the zeroth element divided by the corresponding element of Pascal's triangle:


:$\begin{array}{r|rrrrrr}
n & 0 & 1 & 2 & 3 & 4 & 5 \\
\hline
0  & \frac 1 1 \\
1  & \frac 1 2 & \frac 1 2 \\
2  & \frac 1 3 & \frac 1 6 &  \frac 1 3 \\
3  & \frac 1 4 & \frac 1 {12} & \frac 1 {12} & \frac 1 4 \\
4  & \frac 1 5 & \frac 1 {20} & \frac 1 {30} & \frac 1 {20} & \frac 1 5 \\
5  & \frac 1 6 & \frac 1 {30} & \frac 1 {60} & \frac 1 {60} & \frac 1 {30} & \frac 1 6 \\
\end{array}$


=== Row ===


=== Column ===


=== Diagonal ===


Each of the horizontal lines of numbers corresponding to a given $n$ is known as the $n$th row of Leibniz harmonic triangle.

Hence the top row, containing a single $1$, is identified as the zeroth row, or row $0$.

=== Column ===
Consider the Leibniz harmonic triangle:
The Leibniz Harmonic Triangle is a triangular array where:
:the zeroth element in the $n$th row, counting from $0$, is $\dfrac 1 {n + 1}$
:subsequent elements in the $n$th row are the zeroth element divided by the corresponding element of Pascal's triangle:


:$\begin{array}{r|rrrrrr}
n & 0 & 1 & 2 & 3 & 4 & 5 \\
\hline
0  & \frac 1 1 \\
1  & \frac 1 2 & \frac 1 2 \\
2  & \frac 1 3 & \frac 1 6 &  \frac 1 3 \\
3  & \frac 1 4 & \frac 1 {12} & \frac 1 {12} & \frac 1 4 \\
4  & \frac 1 5 & \frac 1 {20} & \frac 1 {30} & \frac 1 {20} & \frac 1 5 \\
5  & \frac 1 6 & \frac 1 {30} & \frac 1 {60} & \frac 1 {60} & \frac 1 {30} & \frac 1 6 \\
\end{array}$


=== Row ===


=== Column ===


=== Diagonal ===


Each of the vertical lines of numbers is known as the $m$th column of Leibniz harmonic triangle.

The leftmost column, containing the reciprocals of the non-negative integers, is identified as the zeroth column, or column $0$.

=== Diagonal ===
Consider the Leibniz harmonic triangle:
The Leibniz Harmonic Triangle is a triangular array where:
:the zeroth element in the $n$th row, counting from $0$, is $\dfrac 1 {n + 1}$
:subsequent elements in the $n$th row are the zeroth element divided by the corresponding element of Pascal's triangle:


:$\begin{array}{r|rrrrrr}
n & 0 & 1 & 2 & 3 & 4 & 5 \\
\hline
0  & \frac 1 1 \\
1  & \frac 1 2 & \frac 1 2 \\
2  & \frac 1 3 & \frac 1 6 &  \frac 1 3 \\
3  & \frac 1 4 & \frac 1 {12} & \frac 1 {12} & \frac 1 4 \\
4  & \frac 1 5 & \frac 1 {20} & \frac 1 {30} & \frac 1 {20} & \frac 1 5 \\
5  & \frac 1 6 & \frac 1 {30} & \frac 1 {60} & \frac 1 {60} & \frac 1 {30} & \frac 1 6 \\
\end{array}$


=== Row ===


=== Column ===


=== Diagonal ===


The $n$th diagonal of Leibniz harmonic triangle consists of the entries in row $n + m$ and column $m$ for $m \ge 0$:
:$\left({n, 0}\right), \left({n + 1, 1}\right), \left({n + 2, 2}\right), \ldots$

Hence the diagonal leading down and to the right from $\left({0, 0}\right)$, containing  the reciprocals of the non-negative integers, is identified as the zeroth diagonal, or diagonal $0$.

Each of the horizontal lines of numbers corresponding to a given $n$ is known as the $n$th row of Leibniz harmonic triangle.

Hence the top row, containing a single $1$, is identified as the zeroth row, or row $0$.

=== Column ===
Consider the Leibniz harmonic triangle:
The Leibniz Harmonic Triangle is a triangular array where:
:the zeroth element in the $n$th row, counting from $0$, is $\dfrac 1 {n + 1}$
:subsequent elements in the $n$th row are the zeroth element divided by the corresponding element of Pascal's triangle:


:$\begin{array}{r|rrrrrr}
n & 0 & 1 & 2 & 3 & 4 & 5 \\
\hline
0  & \frac 1 1 \\
1  & \frac 1 2 & \frac 1 2 \\
2  & \frac 1 3 & \frac 1 6 &  \frac 1 3 \\
3  & \frac 1 4 & \frac 1 {12} & \frac 1 {12} & \frac 1 4 \\
4  & \frac 1 5 & \frac 1 {20} & \frac 1 {30} & \frac 1 {20} & \frac 1 5 \\
5  & \frac 1 6 & \frac 1 {30} & \frac 1 {60} & \frac 1 {60} & \frac 1 {30} & \frac 1 6 \\
\end{array}$


=== Row ===
Consider the Leibniz harmonic triangle:
The Leibniz Harmonic Triangle is a triangular array where:
:the zeroth element in the $n$th row, counting from $0$, is $\dfrac 1 {n + 1}$
:subsequent elements in the $n$th row are the zeroth element divided by the corresponding element of Pascal's triangle:


:$\begin{array}{r|rrrrrr}
n & 0 & 1 & 2 & 3 & 4 & 5 \\
\hline
0  & \frac 1 1 \\
1  & \frac 1 2 & \frac 1 2 \\
2  & \frac 1 3 & \frac 1 6 &  \frac 1 3 \\
3  & \frac 1 4 & \frac 1 {12} & \frac 1 {12} & \frac 1 4 \\
4  & \frac 1 5 & \frac 1 {20} & \frac 1 {30} & \frac 1 {20} & \frac 1 5 \\
5  & \frac 1 6 & \frac 1 {30} & \frac 1 {60} & \frac 1 {60} & \frac 1 {30} & \frac 1 6 \\
\end{array}$


=== Row ===


=== Column ===


=== Diagonal ===


Each of the horizontal lines of numbers corresponding to a given $n$ is known as the $n$th row of Leibniz harmonic triangle.

Hence the top row, containing a single $1$, is identified as the zeroth row, or row $0$.

=== Column ===
Consider the Leibniz harmonic triangle:
The Leibniz Harmonic Triangle is a triangular array where:
:the zeroth element in the $n$th row, counting from $0$, is $\dfrac 1 {n + 1}$
:subsequent elements in the $n$th row are the zeroth element divided by the corresponding element of Pascal's triangle:


:$\begin{array}{r|rrrrrr}
n & 0 & 1 & 2 & 3 & 4 & 5 \\
\hline
0  & \frac 1 1 \\
1  & \frac 1 2 & \frac 1 2 \\
2  & \frac 1 3 & \frac 1 6 &  \frac 1 3 \\
3  & \frac 1 4 & \frac 1 {12} & \frac 1 {12} & \frac 1 4 \\
4  & \frac 1 5 & \frac 1 {20} & \frac 1 {30} & \frac 1 {20} & \frac 1 5 \\
5  & \frac 1 6 & \frac 1 {30} & \frac 1 {60} & \frac 1 {60} & \frac 1 {30} & \frac 1 6 \\
\end{array}$


=== Row ===


=== Column ===


=== Diagonal ===


Each of the vertical lines of numbers is known as the $m$th column of Leibniz harmonic triangle.

The leftmost column, containing the reciprocals of the non-negative integers, is identified as the zeroth column, or column $0$.

=== Diagonal ===
Consider the Leibniz harmonic triangle:
The Leibniz Harmonic Triangle is a triangular array where:
:the zeroth element in the $n$th row, counting from $0$, is $\dfrac 1 {n + 1}$
:subsequent elements in the $n$th row are the zeroth element divided by the corresponding element of Pascal's triangle:


:$\begin{array}{r|rrrrrr}
n & 0 & 1 & 2 & 3 & 4 & 5 \\
\hline
0  & \frac 1 1 \\
1  & \frac 1 2 & \frac 1 2 \\
2  & \frac 1 3 & \frac 1 6 &  \frac 1 3 \\
3  & \frac 1 4 & \frac 1 {12} & \frac 1 {12} & \frac 1 4 \\
4  & \frac 1 5 & \frac 1 {20} & \frac 1 {30} & \frac 1 {20} & \frac 1 5 \\
5  & \frac 1 6 & \frac 1 {30} & \frac 1 {60} & \frac 1 {60} & \frac 1 {30} & \frac 1 6 \\
\end{array}$


=== Row ===


=== Column ===


=== Diagonal ===


The $n$th diagonal of Leibniz harmonic triangle consists of the entries in row $n + m$ and column $m$ for $m \ge 0$:
:$\left({n, 0}\right), \left({n + 1, 1}\right), \left({n + 2, 2}\right), \ldots$

Hence the diagonal leading down and to the right from $\left({0, 0}\right)$, containing  the reciprocals of the non-negative integers, is identified as the zeroth diagonal, or diagonal $0$.

Each of the vertical lines of numbers is known as the $m$th column of Leibniz harmonic triangle.

The leftmost column, containing the reciprocals of the non-negative integers, is identified as the zeroth column, or column $0$.

=== Diagonal ===
Consider the Leibniz harmonic triangle:
The Leibniz Harmonic Triangle is a triangular array where:
:the zeroth element in the $n$th row, counting from $0$, is $\dfrac 1 {n + 1}$
:subsequent elements in the $n$th row are the zeroth element divided by the corresponding element of Pascal's triangle:


:$\begin{array}{r|rrrrrr}
n & 0 & 1 & 2 & 3 & 4 & 5 \\
\hline
0  & \frac 1 1 \\
1  & \frac 1 2 & \frac 1 2 \\
2  & \frac 1 3 & \frac 1 6 &  \frac 1 3 \\
3  & \frac 1 4 & \frac 1 {12} & \frac 1 {12} & \frac 1 4 \\
4  & \frac 1 5 & \frac 1 {20} & \frac 1 {30} & \frac 1 {20} & \frac 1 5 \\
5  & \frac 1 6 & \frac 1 {30} & \frac 1 {60} & \frac 1 {60} & \frac 1 {30} & \frac 1 6 \\
\end{array}$


=== Row ===
Consider the Leibniz harmonic triangle:
The Leibniz Harmonic Triangle is a triangular array where:
:the zeroth element in the $n$th row, counting from $0$, is $\dfrac 1 {n + 1}$
:subsequent elements in the $n$th row are the zeroth element divided by the corresponding element of Pascal's triangle:


:$\begin{array}{r|rrrrrr}
n & 0 & 1 & 2 & 3 & 4 & 5 \\
\hline
0  & \frac 1 1 \\
1  & \frac 1 2 & \frac 1 2 \\
2  & \frac 1 3 & \frac 1 6 &  \frac 1 3 \\
3  & \frac 1 4 & \frac 1 {12} & \frac 1 {12} & \frac 1 4 \\
4  & \frac 1 5 & \frac 1 {20} & \frac 1 {30} & \frac 1 {20} & \frac 1 5 \\
5  & \frac 1 6 & \frac 1 {30} & \frac 1 {60} & \frac 1 {60} & \frac 1 {30} & \frac 1 6 \\
\end{array}$


=== Row ===


=== Column ===


=== Diagonal ===


Each of the horizontal lines of numbers corresponding to a given $n$ is known as the $n$th row of Leibniz harmonic triangle.

Hence the top row, containing a single $1$, is identified as the zeroth row, or row $0$.

=== Column ===
Consider the Leibniz harmonic triangle:
The Leibniz Harmonic Triangle is a triangular array where:
:the zeroth element in the $n$th row, counting from $0$, is $\dfrac 1 {n + 1}$
:subsequent elements in the $n$th row are the zeroth element divided by the corresponding element of Pascal's triangle:


:$\begin{array}{r|rrrrrr}
n & 0 & 1 & 2 & 3 & 4 & 5 \\
\hline
0  & \frac 1 1 \\
1  & \frac 1 2 & \frac 1 2 \\
2  & \frac 1 3 & \frac 1 6 &  \frac 1 3 \\
3  & \frac 1 4 & \frac 1 {12} & \frac 1 {12} & \frac 1 4 \\
4  & \frac 1 5 & \frac 1 {20} & \frac 1 {30} & \frac 1 {20} & \frac 1 5 \\
5  & \frac 1 6 & \frac 1 {30} & \frac 1 {60} & \frac 1 {60} & \frac 1 {30} & \frac 1 6 \\
\end{array}$


=== Row ===


=== Column ===


=== Diagonal ===


Each of the vertical lines of numbers is known as the $m$th column of Leibniz harmonic triangle.

The leftmost column, containing the reciprocals of the non-negative integers, is identified as the zeroth column, or column $0$.

=== Diagonal ===
Consider the Leibniz harmonic triangle:
The Leibniz Harmonic Triangle is a triangular array where:
:the zeroth element in the $n$th row, counting from $0$, is $\dfrac 1 {n + 1}$
:subsequent elements in the $n$th row are the zeroth element divided by the corresponding element of Pascal's triangle:


:$\begin{array}{r|rrrrrr}
n & 0 & 1 & 2 & 3 & 4 & 5 \\
\hline
0  & \frac 1 1 \\
1  & \frac 1 2 & \frac 1 2 \\
2  & \frac 1 3 & \frac 1 6 &  \frac 1 3 \\
3  & \frac 1 4 & \frac 1 {12} & \frac 1 {12} & \frac 1 4 \\
4  & \frac 1 5 & \frac 1 {20} & \frac 1 {30} & \frac 1 {20} & \frac 1 5 \\
5  & \frac 1 6 & \frac 1 {30} & \frac 1 {60} & \frac 1 {60} & \frac 1 {30} & \frac 1 6 \\
\end{array}$


=== Row ===


=== Column ===


=== Diagonal ===


The $n$th diagonal of Leibniz harmonic triangle consists of the entries in row $n + m$ and column $m$ for $m \ge 0$:
:$\left({n, 0}\right), \left({n + 1, 1}\right), \left({n + 2, 2}\right), \ldots$

Hence the diagonal leading down and to the right from $\left({0, 0}\right)$, containing  the reciprocals of the non-negative integers, is identified as the zeroth diagonal, or diagonal $0$.

The $n$th diagonal of Leibniz harmonic triangle consists of the entries in row $n + m$ and column $m$ for $m \ge 0$:
:$\left({n, 0}\right), \left({n + 1, 1}\right), \left({n + 2, 2}\right), \ldots$

Hence the diagonal leading down and to the right from $\left({0, 0}\right)$, containing  the reciprocals of the non-negative integers, is identified as the zeroth diagonal, or diagonal $0$.",Definition:Leibniz Harmonic Triangle,"['Definitions/Binomial Coefficients', 'Definitions/Discrete Mathematics', 'Definitions/Leibniz Harmonic Triangle']"
Definition:Harmonic,Harmonic,"A harmonic function is a is a twice continuously differentiable function $f: U \to \R$ (where $U$ is an open set of $\R^n$) which satisfies Laplace's equation:

:$\dfrac {\partial^2 f} {\partial {x_1}^2} + \dfrac {\partial^2 f} {\partial {x_2}^2} + \cdots + \dfrac {\partial^2 f} {\partial {x_n}^2} = 0$

everywhere on $U$.


This is usually written using the $\nabla^2$ symbol to denote the Laplacian, as:

:$\nabla^2 f = 0$


=== Riemannian Manifold ===
Let $\struct {M, g}$ be a compact Riemannian manifold with or without boundary.

Let $\map {C^\infty} M$ be the smooth function space.

Let $u \in \map {C^\infty} M$ be a smooth real function on $M$.

Let $\nabla^2$ be the Laplace-Beltrami operator.


Then $u$ is said to be harmonic  if and only if :
:$\nabla^2 u = 0$",Definition:Harmonic Function,"['Definitions/Harmonic Functions', 'Definitions/Potential Theory']"
Definition:Harmonic,Harmonic,Harmonic analysis is the study of functions by expressing them as the sum of series of a family of functions such as sines and cosines.,Definition:Harmonic Analysis,"['Definitions/Harmonic Analysis', 'Definitions/Analysis']"
Definition:Harmonic,Harmonic,"A harmonic sequence is a sequence $\sequence {a_k}$ in $\R$ defined as:
:$h_k = \dfrac 1 {a + k d}$
where:
:$k \in \set {0, 1, 2, \ldots}$
:$-\dfrac a d \notin \set {0, 1, 2, \ldots}$


Thus its general form is:
:$\dfrac 1 a, \dfrac 1 {a + d}, \dfrac 1 {a + 2 d}, \dfrac 1 {a + 3 d}, \ldots$


=== Initial Term ===
Let $\sequence {h_k}$ be the harmonic sequence:

:$a_k = \dfrac 1 {a + k d}$ for $k = 0, 1, 2, \ldots$


The term $a$ is the initial term of $\sequence {a_k}$.


Category:Definitions/Harmonic Sequences

=== Common Difference ===
Let $\sequence {h_k}$ be the harmonic sequence:

:$a_k = \dfrac 1 {a + k d}$ for $k = 0, 1, 2, \ldots$


The term $d$ is the common difference of $\sequence {a_k}$.


Category:Definitions/Harmonic Sequences",Definition:Harmonic Sequence,"['Definitions/Number Theory', 'Definitions/Analytic Number Theory', 'Definitions/Arithmetic', 'Definitions/Algebra', 'Definitions/Harmonic Sequences']"
Definition:Harmonic,Harmonic,"The series defined as:
:$\ds \sum_{n \mathop = 1}^\infty \frac 1 n = 1 + \frac 1 2 + \frac 1 3 + \frac 1 4 + \cdots$

is known as the harmonic series.


=== General Harmonic Series ===
Let $\sequence {x_n}$ be a sequence of numbers such that $\sequence {\size {x_n} }$ is a harmonic sequence.


Then the series defined as:
:$\ds \sum_{n \mathop = 1}^\infty x_n$

is a harmonic series.",Definition:Harmonic Series,"['Definitions/Harmonic Series', 'Definitions/Series', 'Definitions/Harmonic Numbers', 'Definitions/Real Analysis']"
Definition:Harmonic,Harmonic,"Mercator's constant is the real number:

 
 
 
 
 

 ",Definition:Mercator's Constant,"[""Definitions/Mercator's Constant"", 'Definitions/Harmonic Series', 'Definitions/Specific Numbers']"
Definition:Harmonic,Harmonic,"Let $\sequence {x_n}$ be a sequence of numbers such that $\sequence {\size {x_n} }$ is a harmonic sequence.


Then the series defined as:
:$\ds \sum_{n \mathop = 1}^\infty x_n$

is a harmonic series.",Definition:Harmonic Series/General,"['Definitions/Harmonic Series', 'Definitions/Series']"
Definition:Harmonic,Harmonic,"The term harmonic progression is used to mean one of the following:


=== Harmonic Sequence ===
A harmonic sequence is a sequence $\sequence {a_k}$ in $\R$ defined as:
:$h_k = \dfrac 1 {a + k d}$
where:
:$k \in \set {0, 1, 2, \ldots}$
:$-\dfrac a d \notin \set {0, 1, 2, \ldots}$


Thus its general form is:
:$\dfrac 1 a, \dfrac 1 {a + d}, \dfrac 1 {a + 2 d}, \dfrac 1 {a + 3 d}, \ldots$


=== Initial Term ===
Let $\sequence {h_k}$ be the harmonic sequence:

:$a_k = \dfrac 1 {a + k d}$ for $k = 0, 1, 2, \ldots$


The term $a$ is the initial term of $\sequence {a_k}$.


Category:Definitions/Harmonic Sequences

=== Common Difference ===
Let $\sequence {h_k}$ be the harmonic sequence:

:$a_k = \dfrac 1 {a + k d}$ for $k = 0, 1, 2, \ldots$


The term $d$ is the common difference of $\sequence {a_k}$.


Category:Definitions/Harmonic Sequences

=== Harmonic Series ===
The series defined as:
:$\ds \sum_{n \mathop = 1}^\infty \frac 1 n = 1 + \frac 1 2 + \frac 1 3 + \frac 1 4 + \cdots$

is known as the harmonic series.


=== General Harmonic Series ===
Let $\sequence {x_n}$ be a sequence of numbers such that $\sequence {\size {x_n} }$ is a harmonic sequence.


Then the series defined as:
:$\ds \sum_{n \mathop = 1}^\infty x_n$

is a harmonic series.",Definition:Harmonic Progression,['Definitions/Harmonic Sequences']
Definition:Harmonic,Harmonic,"A harmonic is a solution $\phi$ to Laplace's equation in $2$ dimensions:
:$\nabla^2 \phi = 0$
that is:
:$\dfrac {\partial^2 \phi} {\partial x^2} + \dfrac {\partial^2 \phi} {\partial y^2} = 0$",Definition:Harmonic (Analysis),"['Definitions/Harmonics (Analysis)', ""Definitions/Laplace's Equation""]"
Definition:Harmonic,Harmonic,A spherical harmonic is a solution $\phi$ to Laplace's equation in $3$ dimensions when expressed in spherical coordinates.,Definition:Spherical Harmonic,"['Definitions/Spherical Harmonics', ""Definitions/Laplace's Equation""]"
Definition:Harmonic,Harmonic,"A surface harmonic is a spherical harmonic:
:$r^n \paren {a_n \map {P_n} {\cos \theta} + \ds \sum_{m \mathop = 1}^n \paren { {a_n}^m \cos m \phi + {b_n}^m \sin m \phi} \map { {P_n}^m} {\cos \theta} }$

such that $r = 1$.

That is:
:$a_n \map {P_n} {\cos \theta} + \ds \sum_{m \mathop = 1}^n \paren { {a_n}^m \cos m \phi + {b_n}^m \sin m \phi} \map { {P_n}^m} {\cos \theta}$",Definition:Surface Harmonic,"['Definitions/Surface Harmonics', 'Definitions/Spherical Harmonics']"
Definition:Harmonic,Harmonic,"A tesseral harmonic is a surface harmonic in the form:
:$\cos m \phi \, \map { {P_n}^m} {\cos \theta}$
or:
:$\sin m \phi \, \map { {P_n}^m} {\cos \theta}$

such that $m < n$.",Definition:Tesseral Harmonic,"['Definitions/Tesseral Harmonics', 'Definitions/Surface Harmonics', 'Definitions/Spherical Harmonics']"
Definition:Harmonic,Harmonic,"A sectoral harmonic is a surface harmonic in the form:
:$\cos m \phi \, \map { {P_n}^m} {\cos \theta}$
or:
:$\sin m \phi \, \map { {P_n}^m} {\cos \theta}$

such that $m = n$.",Definition:Sectoral Harmonic,"['Definitions/Sectoral Harmonics', 'Definitions/Surface Harmonics', 'Definitions/Spherical Harmonics']"
Definition:Harmonic,Harmonic,"Let $H$ be a spherical harmonic in the form:
:$r^n \paren {a_n \map {P_n} {\cos \theta} + \ds \sum_{m \mathop = 1}^n \paren { {a_n}^m \cos m \phi + {b_n}^m \sin m \phi} \map { {P_n}^m} {\cos \theta} }$

The function $\map {P_n} {\cos \theta}$ is known as a zonal harmonic.",Definition:Zonal Harmonic,"['Definitions/Zonal Harmonics', 'Definitions/Spherical Harmonics']"
Definition:Harmonic,Harmonic,"Let $P$ be a physical particle.

Let its position $\map x t$ be a real function, where $t$ is time.

Let $k > 0$.


Then the potential energy of the form:

:$\map U x = \dfrac 1 2 k x^2$

is called the harmonic potential energy.",Definition:Harmonic Potential Energy,"['Definitions/Lagrangian Mechanics', 'Definitions/Physics']"
Definition:Harmonic,Harmonic,"Let $P$ be a physical particle.

Let the potential energy of $P$ be that of the harmonic potential.


Then $P$ is called the harmonic oscillator.",Definition:Harmonic Oscillator,['Definitions/Physics']
Definition:Harmonic,Harmonic,"Consider a physical system $S$ whose motion can be expressed in the form of the following equation:
:$x = A \map \sin {\omega t + \phi}$
where $A$ and $\phi$ are constants.


Then $S$ is in a state of simple harmonic motion.


=== Amplitude ===
Consider a physical system $S$ in a state of simple harmonic motion:
:$x = A \map \sin {\omega t + \phi}$


The parameter $A$ is known as the amplitude of the motion.

=== Phase ===
Consider a physical system $S$ in a state of simple harmonic motion:
:$x = A \map \sin {\omega t + \phi}$


The expression $\omega t + \phi$ is known as the phase of the motion.


=== Initial Phase ===
Consider a physical system $S$ in a state of simple harmonic motion:
:$x = A \map \sin {\omega t + \phi}$


The parameter $\phi$ is known as the initial phase of the motion.

Let $S_1$ and $S_2$ be physical systems in a state of simple harmonic motion described respectively by the equations:

 
 
 
 

=== Out of Phase ===
Let $S_1$ and $S_2$ be physical systems in a state of simple harmonic motion described respectively by the equations:

 
 
 
 

$S_1$ and $S_2$ are out of phase  if and only if  $\alpha_1 \ne \alpha_2$.


=== Phase Difference ===
Let $S_1$ and $S_2$ be physical systems in a state of simple harmonic motion described respectively by the equations:

 
 
 
 

such that $S_1$ and $S_2$ are out of phase.

The phase difference of $S_1$ and $S_2$ is defined as $\size {\alpha_1 - \alpha_2}$.

=== In Phase ===
Let $S_1$ and $S_2$ be physical systems in a state of simple harmonic motion described respectively by the equations:

 
 
 
 

$S_1$ and $S_2$ are in phase  if and only if  $\alpha_1 = \alpha_2$.

=== Period ===
Consider a physical system $S$ in a state of simple harmonic motion:
:$x = A \map \sin {\omega t + \phi}$


The period $T$ of the motion of $S$ is the time required for one complete cycle:
:$T = \dfrac {2 \pi} \omega$

=== Frequency ===
Consider a physical system $S$ in a state of simple harmonic motion:
:$x = A \map \sin {\omega t + \phi}$


The frequency $\nu$ of the motion of $S$ is the number of complete cycles per unit time:
:$\nu = \dfrac 1 T = \dfrac \omega {2 \pi}$",Definition:Simple Harmonic Motion,"['Definitions/Simple Harmonic Motion', 'Definitions/Harmonic Motion', 'Definitions/Mechanics']"
Definition:Harmonic,Harmonic,"Let $A$, $B$, $C$ and $D$ be points on a straight line.

Let the cross-ratio $\set {A, B; C, D}$ of $A$, $B$, $C$ and $D$ be equal to $-1$:
:$\dfrac {AC / CB} {AD / DB} = -1$
that is:
:$\dfrac {AC \cdot DB} {AD \cdot CB} = -1$


Then $\set {A, B; C, D}$ is known as a harmonic ratio.",Definition:Harmonic Ratio,"['Definitions/Harmonic Ratios', 'Definitions/Cross-Ratios']"
Definition:Harmonic,Harmonic,"Let $A$ and $B$ be points on a straight line.

Let $P$ and $Q$ lie on $AB$ such that $P$ is on the line segment $AB$ while $Q$ is outside the line segment $AB$.


:


Let $P$ and $Q$ be positioned such that the cross-ratio $\set {A, B; P, Q}$ forms a harmonic ratio:
:$\dfrac {AP} {PB} = -\dfrac {AQ} {QB}$


Then $\tuple {AB, PQ}$ are said to be a harmonic range.",Definition:Harmonic Range,"['Definitions/Harmonic Ranges', 'Definitions/Straight Lines', 'Definitions/Analytic Geometry']"
Definition:Harmonic,Harmonic,"=== Harmonic Range ===
Let $AB$ and $PQ$ be line segments on a straight line such that $\tuple {AB, PQ}$ is a harmonic range.

Then $P$ and $Q$ are said to be harmonic conjugates with respect to $A$ and $B$.

=== Harmonic Pencil ===
Let $AB$ and $PQ$ be line segments on a straight line such that $\tuple {AB, PQ}$ is a harmonic range.

Let $O$ be a point which is not on the straight line $AB$.

Let $\map O {AB, PQ}$ be the harmonic pencil formed from $O$ and $\tuple {AB, PQ}$.


:


The rays $OP$ and $OQ$ are said to be harmonic conjugates with respect to $OA$ and $OB$.",Definition:Harmonic Conjugates,"['Definitions/Harmonic Conjugates', 'Definitions/Harmonic Ranges']"
Definition:Harmonic,Harmonic,"Let $A$ and $B$ be points on a straight line.

Let $P$ and $Q$ lie on $AB$ such that $\tuple {AB, PQ}$ is a harmonic range.


Let $O$ be a point which is not on the straight line $AB$.


:


Then the pencil $\map O {AB, PQ}$ formed by joining $O$ to the four points $A$, $B$, $P$ and $Q$ is said to be a harmonic pencil.",Definition:Harmonic Pencil,"['Definitions/Harmonic Pencils', 'Definitions/Harmonic Ranges']"
Definition:Height,Height,"Height, like depth, is used as a term for linear measure in a dimension perpendicular to both length and breadth.


However, whereas depth has connotations of down, height is used for distances up from the plane.


=== Euclidean Definition ===

When discussing the size and shape of a general polygon, the words height and width are often seen.

The height of a polygon is the linear measure going up the page.


 
: 
:The height of any figure is the perpendicular drawn from the vertex to the base.
 ''
 

In contrast, the width is the linear measure going across the page.",Definition:Linear Measure/Height,['Definitions/Length']
Definition:Height,Height,"The height of a triangle is the length of a perpendicular from the apex to whichever side has been chosen as its base.


That is, the length of the altitude so defined.


:

Thus the length of the altitude $h_a$ so constructed is called the height of $\triangle ABC$.",Definition:Triangle (Geometry)/Height,['Definitions/Triangles']
Definition:Height,Height,"The height of a polygon is the length of a perpendicular from the base to the vertex most distant from the base.


 
: 
:The height of any figure is the perpendicular drawn from the vertex to the base.
 ''
 ",Definition:Polygon/Height,['Definitions/Polygons']
Definition:Height,Height,":

Let a perpendicular $AE$ be dropped from the apex of a cone to the plane containing its base.

The length $h$ of the line $AE$ is the height of the cone.",Definition:Cone (Geometry)/Height,['Definitions/Cones']
Definition:Height,Height,"Let $A$ be a commutative ring with unity.

Let $\mathfrak p$ be a prime ideal in $A$.


The height of $\mathfrak p$ is the supremum over all $n$ such that there exists a chain of prime ideals:

:$\mathfrak p_0 \subsetneqq \mathfrak p_1 \subsetneqq \cdots \subsetneqq \mathfrak p_n = \mathfrak p$


It is denoted by:
:$\map {\operatorname {ht} } {\mathfrak p}$

 ",Definition:Height of Prime Ideal,"['Definitions/Ring Theory', 'Definitions/Commutative Algebra']"
Definition:Height,Height,"Let $A$ be a commutative ring with unity.

Let $I$ be a proper ideal in $A$.


The height of $I$ is defined as:
:$\map {\operatorname {ht} } I := \inf \set {\map {\operatorname {ht} } {\mathfrak p} : \mathfrak p \in \Spec A \text{ s.t. } I \subseteq \mathfrak p }$
where:
:$\map {\operatorname {ht} } {\mathfrak p}$ is the height of $\mathfrak p$
:$\Spec A$ is the prime spectrum of $A$",Definition:Height of Proper Ideal,"['Definitions/Ring Theory', 'Definitions/Commutative Algebra']"
Definition:Homogeneous,Homogeneous,"A homogeneous expression is an algebraic expression in which the variables can be replaced throughout by the product of that variable with a given non-zero constant, and the constant can be extracted as a factor of the resulting expression.",Definition:Homogeneous Expression,"['Definitions/Homogeneous Expressions', 'Definitions/Expressions', 'Definitions/Algebra', 'Definitions/Homogeneity']"
Definition:Homogeneous,Homogeneous,A homogeneous equation is formed when a homogeneous expression is equated to zero.,Definition:Homogeneous Equation,"['Definitions/Homogeneous Equations', 'Definitions/Homogeneous Expressions', 'Definitions/Homogeneity']"
Definition:Homogeneous,Homogeneous,"A homogeneous quadratic equation is a quadratic equation in two variables such that each term is of degree $2$:

:$a x^2 + h x y + b y^2 = 0$",Definition:Homogeneous Quadratic Equation,['Definitions/Quadratic Equations']
Definition:Homogeneous,Homogeneous,A straight line or plane is homogeneous  if and only if  it contains the origin.,Definition:Homogeneous (Analytic Geometry),['Definitions/Analytic Geometry']
Definition:Homogeneous,Homogeneous,"Let $\CC$ denote the Cartesian plane.

Let $P = \tuple {x, y}$ be an arbitrary point in $\CC$.


Let $x$ and $y$ be expressed in the forms:

 
 
 
 

where $Z$ is an arbitrary real number.


$P$ is then determined by the ordered triple $\tuple {X, Y, Z}$, the terms of which are called its homogeneous Cartesian coordinates.",Definition:Homogeneous Cartesian Coordinates,"['Definitions/Homogeneous Cartesian Coordinates', 'Definitions/Cartesian Coordinate Systems', 'Definitions/Projective Geometry', 'Definitions/Homogeneity']"
Definition:Homogeneous,Homogeneous,A homogeneous polynomial is a polynomial whose monomials with nonzero coefficients all have the same total degree.,Definition:Homogeneous Polynomial,"['Definitions/Homogeneous Polynomials', 'Definitions/Homogeneous Expressions', 'Definitions/Polynomial Theory', 'Definitions/Homogeneity']"
Definition:Homogeneous,Homogeneous,"A system of homogeneous linear equations is a set of simultaneous linear equations:

:$\ds \forall i \in \closedint 1 m: \sum_{j \mathop = 1}^n \alpha_{i j} x_j = \beta_i$

such that all the $\beta_i$ are equal to zero:

:$\ds \forall i \in \closedint 1 m : \sum_{j \mathop = 1}^n \alpha_{i j} x_j = 0$

That is:

 
 
 
 
 
 


=== Matrix Representation ===
A system of homogeneous linear equations is often expressed as:

:$\mathbf A \mathbf x = \mathbf 0$

where:

:$\mathbf A = \begin {bmatrix}
a_{11} & a_{12} & \cdots & a_{1n} \\
a_{21} & a_{22} & \cdots & a_{2n} \\
\vdots & \vdots & \ddots & \vdots \\
a_{m1} & a_{m2} & \cdots & a_{mn} \\
\end {bmatrix}$,  $\mathbf x = \begin{bmatrix} x_1 \\ x_2 \\ \vdots \\ x_n \end{bmatrix}$, $\mathbf 0 = \begin {bmatrix} 0 \\ 0 \\ \vdots \\ 0 \end {bmatrix}$

are matrices.",Definition:Homogeneous Linear Equations,"['Definitions/Algebra', 'Definitions/Linear Algebra']"
Definition:Homogeneous,Homogeneous,"Let $V$ and $W$ be two vector spaces over a field $\GF$.

Let $f: V \to W$ be a function from $V$ to $W$.

Then $f$ is homogeneous of degree $n$  if and only if :
:$\map f {\alpha \mathbf v} = \alpha^n \map f {\mathbf v}$
for all nonzero $\mathbf v \in V$ and $\alpha \in \GF$.


=== Degree ===
Let $V$ and $W$ be two vector spaces over a field $F$.

Let $f: V \to W$ be a homogeneous function of degree $n$ from $V$ to $W$:
:$f \left({\alpha \mathbf v}\right) = \alpha^n f \left({\mathbf v}\right)$
for all nonzero $\mathbf v \in V$ and $\alpha \in F$.


The element $n \in \N$ is the degree of $f$.


Category:Definitions/Homogeneous Functions

=== Zero Degree ===

A special case is when $n = 0$:

Let $V$ and $W$ be two vector spaces over a field $F$.

Let $f: V \to W$ be a function from $V$ to $W$.


$f$ is a homogeneous function of degree zero  if and only if :
:$\map f {\alpha \mathbf v} = \alpha^0 \map f {\mathbf v} = \map f {\mathbf v}$


Category:Definitions/Homogeneous Functions",Definition:Homogeneous Function,"['Definitions/Homogeneous Functions', 'Definitions/Algebra', 'Definitions/Analysis']"
Definition:Homogeneous,Homogeneous,"Let $f: \R^2 \to \R$ be a real-valued function of two variables.

$\map f {x, y}$ is a homogeneous function  if and only if :
:$\exists n \in \Z: \forall t \in \R: \map f {t x, t y} = t^n \map f {x, y}$


Thus, loosely speaking, a homogeneous function of $x$ and $y$ is one where $x$ and $y$ are both of the same ""power"".


=== Degree ===
Let $f: \R^2 \to \R$ be a homogeneous function of two variables:

:$\exists n \in \Z: \forall t \in \R: \map f {t x, t y} = t^n \map f {x, y}$


The integer $n$ is known as the degree of $f$.

=== Zero Degree ===

A special case is when $n = 0$:

Let $f: \R^2 \to \R$ be a real-valued function of two variables.


$\map f {x, y}$ is a homogeneous function of degree zero or of zero degree  if and only if :
:$\forall t \in \R: \map f {t x, t y} = t^0 \map f {x, y} = \map f {x, y}$


Category:Definitions/Homogeneous Functions",Definition:Homogeneous Function/Real Space,"['Definitions/Homogeneous Functions', 'Definitions/Real Analysis']"
Definition:Homogeneous,Homogeneous,"A homogeneous differential equation is a first order ordinary differential equation of the form:
:$\map M {x, y} + \map N {x, y} \dfrac {\d y} {\d x} = 0$
where both $M$ and $N$ are homogeneous functions of the same degree.",Definition:Homogeneous Differential Equation,"['Definitions/Homogeneous Differential Equations', 'Definitions/Homogeneous Functions', 'Definitions/First Order ODEs']"
Definition:Homogeneous,Homogeneous,"An integral equation of the second kind
:$\map g x = \map f x + \lambda \ds \int_{\map a x}^{\map b x} \map K {x, y} \map g y \rd x$
is described as homogeneous  if and only if  $\map f x \equiv 0$.


That is, if it is of the form:
:$\map g x = \lambda \ds \int_{\map a x}^{\map b x} \map K {x, y} \map g y \rd x$
where:
:$\map K {x, y}$ is a known function
:$\map a x$ and $\map b x$ are known functions of $x$, or constant
:$\map g x$ is an unknown function.


=== Kernel ===
Consider the integral equation:

:of the first kind:
::$\map f x = \lambda \ds \int_{\map a x}^{\map b x} \map K {x, y} \map g y \rd x$

:of the second kind:
::$\map g x = \map f x + \lambda \ds \int_{\map a x}^{\map b x} \map K {x, y} \map g y \rd x$

:of the third kind:
::$\map u x \map g x = \map f x + \lambda \ds \int_{\map a x}^{\map b x} \map K {x, y} \map g y \rd x$


The function $\map K {x, y}$ is known as the kernel of the integral equation.

=== Parameter ===
Consider the integral equation:

:of the first kind:
::$\map f x = \lambda \ds \int_{\map a x}^{\map b x} \map K {x, y} \map g y \rd x$

:of the second kind:
::$\map g x = \map f x + \lambda \ds \int_{\map a x}^{\map b x} \map K {x, y} \map g y \rd x$

:of the third kind:
::$\map u x \map g x = \map f x + \lambda \ds \int_{\map a x}^{\map b x} \map K {x, y} \map g y \rd x$


The number $\lambda$ is known as the parameter of the integral equation.",Definition:Integral Equation of the Second Kind/Homogeneous,"['Definitions/Integral Equations of the Second Kind', 'Definitions/Integral Equations', 'Definitions/Second Kind']"
Definition:Homogeneous,Homogeneous,"Let $T$ be an $\LL$-theory.

Let $\kappa$ be an infinite cardinal.


A model $\MM$ of $T$ is $\kappa$-homogeneous  if and only if  for every subset $A$ and element $b$ in the universe of $\MM$ with the cardinality of $A$ strictly less than $\kappa$, if $f: A \to \MM$ is partial elementary, then $f$ extends to an elementary map $f^*: A \cup \set b \to \MM$.

That is, $\MM$ is $\kappa$-homogeneous  if and only if  for all $A \subseteq \MM$ with  $\card A < \kappa$ and all $b \in \MM$, every elementary $f: A \to \MM$ extends to an elementary $f^*: A \cup \set b \to \MM$.


We say $\MM$ is homogeneous  if and only if  it is $\kappa$-homogeneous where $\kappa$ is the cardinality of the universe of $\MM$.",Definition:Homogeneous (Model Theory),['Definitions/Model Theory for Predicate Logic']
Definition:Homogeneous,Homogeneous,"A body is said to be homogeneous  if and only if  the substance of any part of it is indistinguishable from any other part.


=== Warning ===
Just to specify that a body is made of the same substance throughout is not an adequate definition of homogeneous.

For example, a column of air in the atmosphere is denser at the bottom than at the top.

The fact that it is ""all made of air"" is one thing, but the air at the bottom can be distinguished from that higher up because the densities are different.

Thus a column of air is not homogeneous.


Category:Definitions/Homogeneity",Definition:Homogeneous (Physics),"['Definitions/Physics', 'Definitions/Homogeneity']"
Definition:Homomorphism,Homomorphism,"Let $\struct {S, \circ}$ and $\struct {T, *}$ be algebraic structures.

Let $\phi: \struct {S, \circ} \to \struct {T, *}$ be a mapping from $\struct {S, \circ}$ to $\struct {T, *}$.

Let $\circ$ have the morphism property under $\phi$, that is:

:$\forall x, y \in S: \map \phi {x \circ y} = \map \phi x * \map \phi y$


Then $\phi$ is a homomorphism.


This can be generalised to algebraic structures with more than one operation:

Let:
:$\struct {S_1, \circ_1, \circ_2, \ldots, \circ_n}$
:$\struct {T, *_1, *_2, \ldots, *_n}$
be algebraic structures.


Let $\phi: \struct {S_1, \circ_1, \circ_2, \ldots, \circ_n} \to \struct {T, *_1, *_2, \ldots, *_n}$ be a mapping from $\struct {S_1, \circ_1, \circ_2, \ldots, \circ_n}$ to $\struct {T, *_1, *_2, \ldots, *_n}$.

Let, $\forall k \in \closedint 1 n$, $\circ_k$ have the morphism property under $\phi$, that is:

:$\forall x, y \in S: \map \phi {x \circ_k y} = \map \phi x *_k \map \phi y$


Then $\phi$ is a homomorphism.


=== Semigroup Homomorphism ===
Let $\struct {S, \circ}$ and $\struct {T, *}$ be semigroups.

Let $\phi: S \to T$ be a mapping such that $\circ$ has the morphism property under $\phi$.


That is, $\forall a, b \in S$:
:$\map \phi {a \circ b} = \map \phi a * \map \phi b$


Then $\phi: \struct {S, \circ} \to \struct {T, *}$ is a semigroup homomorphism.

=== Monoid Homomorphism ===
Let $\struct {S, \circ}$ and $\struct {T, *}$ be monoids.

Let $\phi: S \to T$ be a mapping such that $\circ$ has the morphism property under $\phi$.

That is, $\forall a, b \in S$:
:$\map \phi {a \circ b} = \map \phi a * \map \phi b$

Suppose further that $\phi$ preserves identities, that is:

:$\map \phi {e_S} = e_T$


Then $\phi: \struct {S, \circ} \to \struct {T, *}$ is a monoid homomorphism.

=== Group Homomorphism ===
Let $\struct {G, \circ}$ and $\struct {H, *}$ be groups.

Let $\phi: G \to H$ be a mapping such that $\circ$ has the morphism property under $\phi$.


That is, $\forall a, b \in G$:
:$\map \phi {a \circ b} = \map \phi a * \map \phi b$


Then $\phi: \struct {G, \circ} \to \struct {H, *}$ is a group homomorphism.

=== Ring Homomorphism ===
Let $\struct {R, +, \circ}$ and $\struct {S, \oplus, *}$ be rings.

Let $\phi: R \to S$ be a mapping such that both $+$ and $\circ$ have the morphism property under $\phi$.


That is, $\forall a, b \in R$:

 
 
 
 


Then $\phi: \struct {R, +, \circ} \to \struct {S, \oplus, *}$ is a ring homomorphism.

=== Field Homomorphism ===
Let $\struct {F, +, \times}$ and $\struct {K, \oplus, \otimes}$ be fields.

Let $\phi: F \to K$ be a mapping such that both $+$ and $\times$ have the morphism property under $\phi$.


That is, $\forall a, b \in F$:

 
 
 
 


Then $\phi: \struct {F, +, \times} \to \struct {K, \oplus, \otimes}$ is a field homomorphism.

=== $F$-Homomorphism ===
Let $R, S$ be rings with unity.

Let $F$ be a subfield of both $R$ and $S$.


Then a ring homomorphism $\varphi: R \to S$ is called an $F$-homomorphism if:
:$\forall a \in F: \map \phi a = a$


That is, $\phi \restriction_F = I_F$ where:
:$\phi \restriction_F$ is the restriction of $\phi$ to $F$
:$I_F$ is the identity mapping on $F$.

=== $R$-Algebraic Structure Homomorphism ===
Let $R$ be a ring.

Let $\struct {S, \ast_1, \ast_2, \ldots, \ast_n, \circ}_R$ and $\struct {T, \odot_1, \odot_2, \ldots, \odot_n, \otimes}_R$ be $R$-algebraic structures.

Let $\phi: S \to T$ be a mapping.


Then $\phi$ is an $R$-algebraic structure homomorphism  if and only if :

:$(1): \quad \forall k \in \closedint 1 n: \forall x, y \in S: \map \phi {x \ast_k y} = \map \phi x \odot_k \map \phi y$
:$(2): \quad \forall x \in S: \forall \lambda \in R: \map \phi {\lambda \circ x} = \lambda \otimes \map \phi x$
where $\closedint 1 n = \set {1, 2, \ldots, n}$ denotes an integer interval.


Note that this definition also applies to modules and vector spaces.

=== $G$-Module Homomorphism ===
Let $\struct {G, \cdot}$ be a group.

Let $\struct {V, \phi}$ and $\struct {W, \mu}$ be $G$-modules.


Then a linear transformation $f: V \to W$ is called a $G$-module homomorphism  if and only if :
:$\forall g \in G: \forall v \in V: \map f {\map \phi {g, v} } = \map \mu {g, \map f v}$

=== Homomorphism of Complexes ===
Let $\struct {R, +, \cdot}$ be a ring.

Let:
:$M: \quad \cdots \longrightarrow M_i \stackrel {d_i} {\longrightarrow} M_{i + 1} \stackrel {d_{i + 1} } {\longrightarrow} M_{i + 2} \stackrel {d_{i + 2} } {\longrightarrow} \cdots$
and
:$N: \quad \cdots \longrightarrow N_i \stackrel {d'_i} {\longrightarrow} N_{i + 1} \stackrel {d'_{i + 1} } {\longrightarrow} N_{i + 2} \stackrel {d'_{i + 2} } {\longrightarrow} \cdots$
be two differential complexes of $R$-modules.

Let $\phi = \set {\phi_i: i \in \Z}$ be a family of module homomorphisms $\phi_i: M_i \to N_i$.


Then $\phi$ is a homomorphism of complexes  if and only if  for each $i \in \Z$:
:$\phi_{i + 1} \circ d_i = \phi_i \circ d'_i$


That is, for each $i \in \Z$ we have a commutative diagram:

::$\begin{xy}\xymatrix@L+2mu@+1em {
 M_i \ar[r]^*{d_i}
     \ar[d]^*{\phi_i} &
M_{i+1} \ar[d]^*{\phi_{i+1}} \\
 N_i \ar[r]^*{d'_i} &
 N_{i+1} } \end{xy}$",Definition:Homomorphism (Abstract Algebra),"['Definitions/Homomorphisms (Abstract Algebra)', 'Definitions/Abstract Algebra', 'Definitions/Mapping Theory', 'Definitions/Homomorphisms']"
Definition:Homomorphism,Homomorphism,"Let $\struct {G, \circ}$ and $\struct {H, *}$ be groups.

Let $\phi: G \to H$ be a mapping such that $\circ$ has the morphism property under $\phi$.


That is, $\forall a, b \in G$:
:$\map \phi {a \circ b} = \map \phi a * \map \phi b$


Then $\phi: \struct {G, \circ} \to \struct {H, *}$ is a group homomorphism.",Definition:Group Homomorphism,"['Definitions/Group Homomorphisms', 'Definitions/Homomorphisms (Abstract Algebra)', 'Definitions/Group Theory']"
Definition:Homomorphism,Homomorphism,"Let $\struct {R, +, \circ}$ and $\struct {S, \oplus, *}$ be rings.

Let $\phi: R \to S$ be a mapping such that both $+$ and $\circ$ have the morphism property under $\phi$.


That is, $\forall a, b \in R$:

 
 
 
 


Then $\phi: \struct {R, +, \circ} \to \struct {S, \oplus, *}$ is a ring homomorphism.",Definition:Ring Homomorphism,"['Definitions/Ring Homomorphisms', 'Definitions/Homomorphisms (Abstract Algebra)', 'Definitions/Ring Theory']"
Definition:Homomorphism,Homomorphism,"Let $\struct {F, +, \times}$ and $\struct {K, \oplus, \otimes}$ be fields.

Let $\phi: F \to K$ be a mapping such that both $+$ and $\times$ have the morphism property under $\phi$.


That is, $\forall a, b \in F$:

 
 
 
 


Then $\phi: \struct {F, +, \times} \to \struct {K, \oplus, \otimes}$ is a field homomorphism.",Definition:Field Homomorphism,"['Definitions/Field Homomorphisms', 'Definitions/Ring Homomorphisms', 'Definitions/Homomorphisms (Abstract Algebra)', 'Definitions/Field Theory']"
Definition:Homomorphism,Homomorphism,"Let $R, S$ be rings with unity.

Let $F$ be a subfield of both $R$ and $S$.


Then a ring homomorphism $\varphi: R \to S$ is called an $F$-homomorphism if:
:$\forall a \in F: \map \phi a = a$


That is, $\phi \restriction_F = I_F$ where:
:$\phi \restriction_F$ is the restriction of $\phi$ to $F$
:$I_F$ is the identity mapping on $F$.",Definition:F-Homomorphism,"['Definitions/Ring Homomorphisms', 'Definitions/Field Theory']"
Definition:Homomorphism,Homomorphism,"Let $R$ be a ring.

Let $\struct {S, \ast_1, \ast_2, \ldots, \ast_n, \circ}_R$ and $\struct {T, \odot_1, \odot_2, \ldots, \odot_n, \otimes}_R$ be $R$-algebraic structures.

Let $\phi: S \to T$ be a mapping.


Then $\phi$ is an $R$-algebraic structure homomorphism  if and only if :

:$(1): \quad \forall k \in \closedint 1 n: \forall x, y \in S: \map \phi {x \ast_k y} = \map \phi x \odot_k \map \phi y$
:$(2): \quad \forall x \in S: \forall \lambda \in R: \map \phi {\lambda \circ x} = \lambda \otimes \map \phi x$
where $\closedint 1 n = \set {1, 2, \ldots, n}$ denotes an integer interval.


Note that this definition also applies to modules and vector spaces.",Definition:R-Algebraic Structure Homomorphism,"['Definitions/R-Algebraic Structure Homomorphisms', 'Definitions/Homomorphisms (Abstract Algebra)', 'Definitions/Linear Algebra']"
Definition:Homomorphism,Homomorphism,"Let $G = \struct {\map V G, \map E G}$ and $H = \struct {\map V H, \map E H}$ be graphs.


Let there exist a mapping $F: \map V G \to \map V H$ such that:
:for each edge $\set {u, v} \in \map E G$
:there exists an edge $\set {\map F u, \map F v} \in \map E H$.


Then $G$ and $H$ are homomorphic.

The mapping $F$ is called a homomorphism from $G$ to $H$.",Definition:Homomorphism (Graph Theory),"['Definitions/Graph Theory', 'Definitions/Homomorphisms']"
Definition:Ideal,Ideal,"Let $\struct {R, +, \circ}$ be a ring.

Let $\struct {J, +}$ be a subgroup of $\struct {R, +}$.


Then $J$ is an ideal of $R$  if and only if :
:$\forall j \in J: \forall r \in R: j \circ r \in J \land r \circ j \in J$

that is,  if and only if :
:$\forall r \in R: J \circ r \subseteq J \land r \circ J \subseteq J$


The letter $J$ is frequently used to denote an ideal.


=== Left Ideal ===
Let $\struct {R, +, \circ}$ be a ring.

Let $\struct {J, +}$ be a subgroup of $\struct {R, +}$.


$J$ is a left ideal of $R$  if and only if :
:$\forall j \in J: \forall r \in R: r \circ j \in J$

that is,  if and only if :
:$\forall r \in R: r \circ J \subseteq J$

=== Right Ideal ===
Let $\struct {R, +, \circ}$ be a ring.

Let $\struct {J, +}$ be a subgroup of $\struct {R, +}$.


$J$ is a right ideal of $R$  if and only if :
:$\forall j \in J: \forall r \in R: j \circ r \in J$

that is,  if and only if :
:$\forall r \in R: J \circ r \subseteq J$

It follows that in a commutative ring, a left ideal, a right ideal and an ideal are the same thing.


=== Proper Ideal ===
Let $\struct {R, +, \circ}$ be a ring.


A proper ideal $J$ of $\struct {R, +, \circ}$ is an ideal of $R$ such that $J$ is a proper subset of $R$.

That is, such that $J \subseteq R$ and $J \ne R$.",Definition:Ideal of Ring,"['Definitions/Ideals of Rings', 'Definitions/Ring Theory', 'Definitions/Ideal Theory']"
Definition:Ideal,Ideal,"Let $\struct {R, +, \circ}$ be a ring.

Let $\struct {J, +}$ be a subgroup of $\struct {R, +}$.


$J$ is a left ideal of $R$  if and only if :
:$\forall j \in J: \forall r \in R: r \circ j \in J$

that is,  if and only if :
:$\forall r \in R: r \circ J \subseteq J$",Definition:Ideal of Ring/Left Ideal,['Definitions/Ideal Theory']
Definition:Ideal,Ideal,"Let $\struct {R, +, \circ}$ be a ring.

Let $\struct {J, +}$ be a subgroup of $\struct {R, +}$.


$J$ is a right ideal of $R$  if and only if :
:$\forall j \in J: \forall r \in R: j \circ r \in J$

that is,  if and only if :
:$\forall r \in R: J \circ r \subseteq J$",Definition:Ideal of Ring/Right Ideal,['Definitions/Ideal Theory']
Definition:Ideal,Ideal,"Let $R$ be a ring.

Let $\struct {A, \ast}$ be an $R$-algebra.

Let $\struct {J, \ast}$ be a subalgebra of $\struct {A, \ast}$. 


We say that $J$ is an ideal of $A$  if and only if :
:for each $a \in A$ and $x \in J$ we have $a \ast x \in J$ and $x \ast a \in J$. 


=== Left Ideal ===
Let $R$ be a ring.

Let $\struct {A, \ast}$ be an $R$-algebra.

Let $\struct {J, \ast}$ be a subalgebra of $\struct {A, \ast}$. 


We say that $J$ is a left ideal of $A$  if and only if :
:for each $a \in A$ and $x \in J$ we have $a \ast x \in J$.

=== Right Ideal ===
Let $R$ be a ring.

Let $\struct {A, \ast}$ be an $R$-algebra.

Let $\struct {J, \ast}$ be a subalgebra of $\struct {A, \ast}$. 


We say that $J$ is a left ideal of $A$  if and only if :
:for each $a \in A$ and $x \in J$ we have $x \ast a \in J$.

=== Proper Ideal ===
Let $R$ be a ring.

Let $\struct {A, \ast}$ be an $R$-algebra.

Let $\struct {J, \ast}$ be a subalgebra of $\struct {A, \ast}$. 


We say that $J$ is a proper ideal of $A$  if and only if :
:$J \ne A$",Definition:Ideal of Algebra,['Definitions/Algebras']
Definition:Ideal,Ideal,"Let $R$ be a ring.

Let $\struct {A, \ast}$ be an $R$-algebra.

Let $\struct {J, \ast}$ be a subalgebra of $\struct {A, \ast}$. 


We say that $J$ is a left ideal of $A$  if and only if :
:for each $a \in A$ and $x \in J$ we have $a \ast x \in J$.",Definition:Ideal of Algebra/Left Ideal,['Definitions/Ideals of Algebras']
Definition:Ideal,Ideal,"Let $R$ be a ring.

Let $\struct {A, \ast}$ be an $R$-algebra.

Let $\struct {J, \ast}$ be a subalgebra of $\struct {A, \ast}$. 


We say that $J$ is a left ideal of $A$  if and only if :
:for each $a \in A$ and $x \in J$ we have $x \ast a \in J$.",Definition:Ideal of Algebra/Right Ideal,['Definitions/Ideals of Algebras']
Definition:Ideal,Ideal,"Let $\struct {S, \preceq}$ be an ordered set.

Let $I \subseteq S$ be a non-empty subset of $S$.


Then $I$ is an ideal of $S$  if and only if  $I$ satisifies the ideal axioms:
 

=== Proper Ideal ===
Let $\struct {S, \preccurlyeq}$ be an ordered set.

Let $\II$ be an ideal on $\struct {S, \preccurlyeq}$.


Then:
:$\II$ is a proper ideal on $S$
 if and only if :
:$\II \ne S$

That is,  if and only if  $\II$ is a proper subset of $S$.

=== Join Semilattice ===
Let $\struct {S, \vee, \preceq}$ be a join semilattice.

Let $I \subseteq S$ be a non-empty subset of $S$.


Then $I$ is a join semilattice ideal of $S$  if and only if  $I$ satisifies the join semilattice ideal axioms:
 

=== Lattice ===
Let $\struct {S, \vee, \wedge, \preceq}$ be a lattice.

Let $I \subseteq S$ be a non-empty subset of $S$.



$I$ is a lattice ideal of $S$  if and only if  $I$ satisifes the lattice ideal axioms:
 ",Definition:Ideal (Order Theory),['Definitions/Order Theory']
Definition:Ideal,Ideal,"Let $\struct {S, \vee, \preceq}$ be a join semilattice.

Let $I \subseteq S$ be a non-empty subset of $S$.


Then $I$ is a join semilattice ideal of $S$  if and only if  $I$ satisifies the join semilattice ideal axioms:
 ",Definition:Join Semilattice Ideal,['Definitions/Lattice Theory']
Definition:Ideal,Ideal,"Let $\struct {S, \vee, \wedge, \preceq}$ be a lattice.

Let $I \subseteq S$ be a non-empty subset of $S$.


=== Definition 1 ===
Let $\struct {S, \vee, \wedge, \preceq}$ be a lattice.

Let $I \subseteq S$ be a non-empty subset of $S$.



$I$ is a lattice ideal of $S$  if and only if  $I$ satisifes the lattice ideal axioms:
 

=== Definition 2 ===
Let $\struct {S, \vee, \wedge, \preceq}$ be a lattice.

Let $I \subseteq S$ be a non-empty subset of $S$.



$I$ is a lattice ideal of $S$  if and only if  $I$ is a join semilattice ideal",Definition:Lattice Ideal,"['Definitions/Lattice Ideals', 'Definitions/Lattice Theory']"
Definition:Ideal,Ideal,An ideal (or idealized) object is one in which certain attributes are approximated to zero or infinity.,Definition:Ideal (Physics),"['Definitions/Physics', 'Definitions/Applied Mathematics', 'Definitions/Ideals in Physics']"
Definition:Identity,Identity,"An identity is an equation which is true for all values attained by the variables it contains.


=== Symbol ===
",Definition:Identity (Equation),"['Definitions/Identities (Equations)', 'Definitions/Equations', 'Definitions/Identities']"
Definition:Identity,Identity,"Let $\struct {S, \circ}$ be an algebraic structure.


=== Left Identity ===
Let $\struct {S, \circ}$ be an algebraic structure.

An element $e_L \in S$ is called a left identity (element)  if and only if :
:$\forall x \in S: e_L \circ x = x$

=== Right Identity ===
Let $\struct {S, \circ}$ be an algebraic structure.

An element $e_R \in S$ is called a right identity (element)  if and only if :
:$\forall x \in S: x \circ e_R = x$

=== Two-Sided Identity ===
Let $\struct {S, \circ}$ be an algebraic structure.

An element $e \in S$ is called an identity (element)  if and only if  it is both a left identity and a right identity:

:$\forall x \in S: x \circ e = x = e \circ x$


In Identity is Unique it is established that an identity element, if it exists, is unique within $\struct {S, \circ}$.

Thus it is justified to refer to it as the identity (of a given algebraic structure).


This identity is often denoted $e_S$, or $e$ if it is clearly understood what structure is being discussed.",Definition:Identity (Abstract Algebra),"['Definitions/Identity Elements', 'Definitions/Abstract Algebra']"
Definition:Image,Image,"Let $f: S \to T$ be a mapping.


=== Definition 1 ===
The image of a mapping $f: S \to T$ is the set:

:$\Img f = \set {t \in T: \exists s \in S: \map f s = t}$

That is, it is the set of values taken by $f$.

=== Definition 2 ===
The image of a mapping $f: S \to T$ is the set:

:$\Img f = f \sqbrk S$

where $f \sqbrk S$ is the image of $S$ under $f$.


=== Class Theory ===

 
Let $V$ be a basic universe.

Let $A \subseteq V$ and $B \subseteq V$ be classes.

Let $f: A \to B$ be a class mapping.

The image of $\RR$ is defined and denoted as:
:$\Img \RR := \set {y \in V: \exists x \in V: \tuple {x, y} \in \RR}$

That is, it is the class of all $y$ such that $\tuple {x, y} \in \RR$ for at least one $x$.",Definition:Image (Relation Theory)/Mapping/Mapping,['Definitions/Images']
Definition:Image,Image,"Let $\RR \subseteq S \times T$ be a relation.


The image of $\RR$ is defined as:

:$\Img \RR := \RR \sqbrk S = \set {t \in T: \exists s \in S: \tuple {s, t} \in \RR}$


=== General Definition ===
Let $\ds \prod_{i \mathop = 1}^n S_i$ be the cartesian product of sets $S_1$ to $S_n$.

Let $\ds \RR \subseteq \prod_{i \mathop = 1}^n S_i$ be an $n$-ary relation on $\ds \prod_{i \mathop = 1}^n S_i$.

The image of $\RR$ is the set defined as:
:$\Img \RR := \set {s_n \in S_n: \exists \tuple {s_1, s_2, \ldots, s_{n - 1} } \in \ds \prod_{i \mathop = 1}^{n - 1} S_i: \tuple {s_1, s_2, \ldots, s_n} \in \RR}$


The concept is usually encountered when $\RR$ is an endorelation on $S$:
:$\Img \RR := \set {s_n \in S: \exists \tuple {s_1, s_2, \ldots, s_{n - 1} } \in S^{n - 1}: \tuple {s_1, s_2, \ldots, s_n} \in \RR}$

=== Class Theory ===

 
Let $V$ be a basic universe.

Let $\RR \subseteq V \times V$ be a relation in $V$.

The image of $\RR$ is defined and denoted as:
:$\Img \RR := \set {y \in V: \exists x \in V: \tuple {x, y} \in \RR}$

That is, it is the class of all $y$ such that $\tuple {x, y} \in \RR$ for at least one $x$.",Definition:Image (Relation Theory)/Relation/Relation,"['Definitions/Images', 'Definitions/Relations']"
Definition:Image,Image,"Let $R$ be a ring.

Let:

:$\mathbf A_{m \times n} = \begin{bmatrix}
a_{1 1} & a_{1 2} & \cdots & a_{1 n} \\
a_{2 1} & a_{2 2} & \cdots & a_{2 n} \\
\vdots & \vdots & \ddots & \vdots \\
a_{m 1} & a_{m 2} & \cdots & a_{m n} \\
\end{bmatrix}$

be a matrix over $R$ such that every column is defined as a vector:

:$\forall i: 1 \le i \le m: \begin {bmatrix} a_{1 i} \\ a_{2 i} \\ \vdots \\ a_{m i} \end {bmatrix} \in \mathbf V$

where $\mathbf V$ is some vector space.


Then the column space of $\mathbf A$ is the linear span of all such column vectors:

:$\map {\mathrm C} {\mathbf A} = \map \span {\begin {bmatrix} a_{1 1} \\ a_{2 1} \\ \vdots \\ a_{m 1} \end {bmatrix}, \begin {bmatrix} a_{1 2} \\ a_{2 2} \\ \vdots \\ a_{m 2} \end {bmatrix}, \cdots, \begin {bmatrix} a_{1 n} \\ a_{2 n} \\ \vdots \\ a_{m n} \end {bmatrix} }$",Definition:Column Space,"['Definitions/Column Space', 'Definitions/Matrix Theory', 'Definitions/Linear Algebra']"
Definition:Image,Image,"=== Real Cartesian Space ===
 

=== Complex Plane ===
Let $C$ be a contour in $\C$ defined by the (finite) sequence $\sequence {C_1, \ldots, C_n}$ of directed smooth curves in $\C$.

Let $C_k$ be parameterized by the smooth path $\gamma_k: \closedint {a_k} {b_k} \to \C$ for all $k \in \set {1, \ldots, n}$.


The image of $C$ is defined as:

:$\ds \Img C := \bigcup_{k \mathop = 1}^n \Img {\gamma_k}$

where $\Img {\gamma_k}$ denotes the image of $\gamma_k$.


If $\Img C \subseteq D$, where $D$ is a subset of $\C$, we say that $C$ is a contour in $D$.

Category:Definitions/Vector Analysis",Definition:Image of Contour,['Definitions/Vector Analysis']
Definition:Image Set,Image Set,"Let $f: S \to T$ be a mapping.


=== Definition 1 ===
The image of a mapping $f: S \to T$ is the set:

:$\Img f = \set {t \in T: \exists s \in S: \map f s = t}$

That is, it is the set of values taken by $f$.

=== Definition 2 ===
The image of a mapping $f: S \to T$ is the set:

:$\Img f = f \sqbrk S$

where $f \sqbrk S$ is the image of $S$ under $f$.


=== Class Theory ===

 
Let $V$ be a basic universe.

Let $A \subseteq V$ and $B \subseteq V$ be classes.

Let $f: A \to B$ be a class mapping.

The image of $\RR$ is defined and denoted as:
:$\Img \RR := \set {y \in V: \exists x \in V: \tuple {x, y} \in \RR}$

That is, it is the class of all $y$ such that $\tuple {x, y} \in \RR$ for at least one $x$.",Definition:Image (Relation Theory)/Mapping/Mapping,['Definitions/Images']
Definition:Image Set,Image Set,"Let $\RR \subseteq S \times T$ be a relation.


The image of $\RR$ is defined as:

:$\Img \RR := \RR \sqbrk S = \set {t \in T: \exists s \in S: \tuple {s, t} \in \RR}$


=== General Definition ===
Let $\ds \prod_{i \mathop = 1}^n S_i$ be the cartesian product of sets $S_1$ to $S_n$.

Let $\ds \RR \subseteq \prod_{i \mathop = 1}^n S_i$ be an $n$-ary relation on $\ds \prod_{i \mathop = 1}^n S_i$.

The image of $\RR$ is the set defined as:
:$\Img \RR := \set {s_n \in S_n: \exists \tuple {s_1, s_2, \ldots, s_{n - 1} } \in \ds \prod_{i \mathop = 1}^{n - 1} S_i: \tuple {s_1, s_2, \ldots, s_n} \in \RR}$


The concept is usually encountered when $\RR$ is an endorelation on $S$:
:$\Img \RR := \set {s_n \in S: \exists \tuple {s_1, s_2, \ldots, s_{n - 1} } \in S^{n - 1}: \tuple {s_1, s_2, \ldots, s_n} \in \RR}$

=== Class Theory ===

 
Let $V$ be a basic universe.

Let $\RR \subseteq V \times V$ be a relation in $V$.

The image of $\RR$ is defined and denoted as:
:$\Img \RR := \set {y \in V: \exists x \in V: \tuple {x, y} \in \RR}$

That is, it is the class of all $y$ such that $\tuple {x, y} \in \RR$ for at least one $x$.",Definition:Image (Relation Theory)/Relation/Relation,"['Definitions/Images', 'Definitions/Relations']"
Definition:Improper,Improper,"An improper fraction is a fraction representing a rational number whose absolute value is greater than $1$.

Specifically, when expressed in the form $r = \dfrac p q$, where $p$ and $q$ are integers such that (the absolute value of) the numerator is greater than (the absolute value of) the denominator: $\size p > \size q$.",Definition:Fraction/Improper,"['Definitions/Improper Fractions', 'Definitions/Fractions']"
Definition:Improper,Improper,"An improper integral is a definite integral over an interval which is not closed, that is, open or half open, and whose limits of integration are the end points of that interval.

When the end point is not actually in the interval, the conventional definition of the definite integral is not valid.

Therefore we use the technique of limits to specify the integral.


Note: In the below, in all cases the necessary limits must exist in order for the definition to hold.",Definition:Improper Integral,"['Definitions/Improper Integrals', 'Definitions/Definite Integrals', 'Definitions/Integral Calculus']"
Definition:Inconsistent,Inconsistent,"Let $\LL$ be a logical language.

Let $\mathscr P$ be a proof system for $\LL$.

=== Definition 1 ===
Let $\LL$ be a logical language.

Let $\mathscr P$ be a proof system for $\LL$.


A set $\FF$ of logical formulas is inconsistent for $\mathscr P$  if and only if :

:For every logical formula $\phi$, $\FF \vdash_{\mathscr P} \phi$.

That is, every logical formula $\phi$ is a provable consequence of $\FF$.

=== Definition 2 ===
Let $\LL$ be a logical language.

Let $\mathscr P$ be a proof system for $\LL$.


A set $\FF$ of logical formulas is inconsistent for $\mathscr P$  if and only if :

:There exists a logical formula $\phi$ such that both
::$\FF \vdash_{\mathscr P} \paren {\phi \land \neg \phi}$",Definition:Inconsistent (Logic),"['Definitions/Inconsistent (Logic)', 'Definitions/Proof Systems', 'Definitions/Logic']"
Definition:Inconsistent,Inconsistent,"A set of equations is described as inconsistent  if and only if  they are not consistent

That is, there exists no set of values for its variables such that all the equations are satisfied.",Definition:Consistent Simultaneous Equations/Inconsistent,"['Definitions/Consistent Simultaneous Equations', 'Definitions/Simultaneous Equations']"
Definition:Independent,Independent,"=== Real Function ===
Let $f: \R \to \R$ be a real function.

Let $\map f x = y$.

Then $x$ is referred to as an independent variable.

=== Complex Function ===
Let $f: \C \to \C$ be a complex function.

Let $\map f z = w$.


Then $z$ is referred to as an independent variable (of $f$).",Definition:Independent Variable,"['Definitions/Independent Variables', 'Definitions/Analysis', 'Definitions/Mapping Theory']"
Definition:Independent,Independent,"Let $\EE$ be an experiment with probability space $\struct {\Omega, \Sigma, \Pr}$.

Let $A, B \in \Sigma$ be events of $\EE$ such that $\map \Pr A > 0$ and $\map \Pr B > 0$.


=== Definition 1 ===
Let $\EE$ be an experiment with probability space $\struct {\Omega, \Sigma, \Pr}$.

Let $A, B \in \Sigma$ be events of $\EE$ such that $\map \Pr A > 0$ and $\map \Pr B > 0$.


The events $A$ and $B$ are defined as independent (of each other)  if and only if  the occurrence of one of them does not affect the probability of the occurrence of the other one.


Formally, $A$ is independent of $B$  if and only if :
:$\condprob A B = \map \Pr A$
where $\condprob A B$ denotes the conditional probability of $A$ given $B$.

=== Definition 2 ===
Let $\EE$ be an experiment with probability space $\struct {\Omega, \Sigma, \Pr}$.

Let $A, B \in \Sigma$ be events of $\EE$ such that $\map \Pr A > 0$ and $\map \Pr B > 0$.


The events $A$ and $B$ are defined as independent (of each other)  if and only if  the occurrence of both of them together has the same probability as the product of the probabilities of each of them occurring on their own.


Formally, $A$ and $B$ are independent  if and only if :
:$\map \Pr {A \cap B} = \map \Pr A \map \Pr B$",Definition:Independent Events,"['Definitions/Independent Events', 'Definitions/Probability Theory']"
Definition:Independent,Independent,"Let $G$ be a group whose identity is $e$.

Let $\left \langle {H_n} \right \rangle$ be a sequence of subgroups of $G$.


=== Definition 1 ===
Let $G$ be a group whose identity is $e$.

Let $\sequence {H_n}$ be a sequence of subgroups of $G$.


The subgroups $H_1, H_2, \ldots, H_n$ are independent  if and only if :

:$\ds \prod_{k \mathop = 1}^n h_k = e \iff \forall k \in \set {1, 2, \ldots, n}: h_k = e$
where $h_k \in H_k$ for all $k \in \set {1, 2, \ldots, n}$.


That is, the product of any elements from different $H_k$ instances forms the identity  if and only if  all of those elements are the identity.

=== Definition 2 ===
Let $G$ be a group whose identity is $e$.

Let $\sequence {H_n}$ be a sequence of subgroups of $G$.


The subgroups $H_1, H_2, \ldots, H_n$ are independent  if and only if :

:$\ds \forall k \in \set {2, 3, \ldots, n}: \paren {\prod_{j \mathop = 1}^{k - 1} H_j} \cap H_k = \set e$


That is, the product of any elements from different $H_k$ instances forms the identity  if and only if  all of those elements are the identity.",Definition:Independent Subgroups,['Definitions/Group Theory']
Definition:Independent,Independent,"Let $G$ be an abelian group whose identity is $e$.

Let $R$ be a ring with unity whose zero is $0_R$ and whose unity is $1_R$.


Let $\struct {G, +_G, \circ}_R$ be a unitary $R$-module.


=== Sequence ===
Let $G$ be an abelian group whose identity is $e$.

Let $R$ be a ring with unity whose zero is $0_R$ and whose unity is $1_R$.


Let $\struct {G, +_G, \circ}_R$ be a unitary $R$-module.


Let $\sequence {a_n}$ be a sequence of elements of $G$ such that:
:$\ds \forall \sequence {\lambda_n} \subseteq R: \sum_{k \mathop = 1}^n \lambda_k \circ a_k = e \implies \lambda_1 = \lambda_2 = \cdots = \lambda_n = 0_R$

That is, the only way to make $e$ with a linear combination of $\sequence {a_n}$ is by making all the terms of $\sequence {\lambda_n}$ equal to $0_R$.


Such a sequence is linearly independent.


=== Linearly Independent Sequence on a Real Vector Space ===
Let $\struct {\R^n, +, \cdot}_\R$ be a real vector space.

Let $\sequence {\mathbf v_n}$ be a sequence of vectors in $\R^n$.

Then $\sequence {\mathbf v_n}$ is linearly independent  if and only if :

:$\ds \forall \sequence {\lambda_n} \subseteq \R: \sum_{k \mathop = 1}^n \lambda_k \mathbf v_k = \mathbf 0 \implies \lambda_1 = \lambda_2 = \cdots = \lambda_n = 0$

where $\mathbf 0 \in \R^n$ is the zero vector and $0 \in \R$ is the zero scalar.

=== Set ===
Let $G$ be an abelian group whose identity is $e$.

Let $R$ be a ring with unity whose zero is $0_R$ and whose unity is $1_R$.


Let $\struct {G, +_G, \circ}_R$ be a unitary $R$-module.


Let $S \subseteq G$.


Then $S$ is a linearly independent set (over $R$)  if and only if  every finite sequence of distinct terms in $S$ is a linearly independent sequence.

That is, such that:
:$\ds \forall \sequence {\lambda_n} \subseteq R: \sum_{k \mathop = 1}^n \lambda_k \circ a_k = e \implies \lambda_1 = \lambda_2 = \cdots = \lambda_n = 0_R$
where $a_1, a_2, \ldots, a_k$ are distinct elements of $S$.


=== Linearly Independent Set on a Real Vector Space ===
Let $\struct {\R^n, +, \cdot}_\R$ be a real vector space.

Let $S \subseteq \R^n$.


Then $S$ is a linearly independent set of real vectors  if and only if  every finite sequence of distinct terms in $S$ is a linearly independent sequence.

That is, such that:
:$\ds \forall \set {\lambda_k: 1 \le k \le n} \subseteq \R: \sum_{k \mathop = 1}^n \lambda_k \mathbf v_k = \mathbf 0 \implies \lambda_1 = \lambda_2 = \cdots = \lambda_n = 0$
where $\mathbf v_1, \mathbf v_2, \ldots, \mathbf v_n$ are distinct elements of $S$.

=== Linearly Independent Set on a Complex Vector Space ===
Let $\struct {\C^n, +, \cdot}_\C$ be a complex vector space.

Let $S \subseteq \C^n$.


Then $S$ is a linearly independent set of complex vectors  if and only if  every finite sequence of distinct terms in $S$ is a linearly independent sequence.

That is, such that:
:$\ds \forall \set {\lambda_k: 1 \le k \le n} \subseteq \C: \sum_{k \mathop = 1}^n \lambda_k \mathbf v_k = \mathbf 0 \implies \lambda_1 = \lambda_2 = \cdots = \lambda_n = 0$
where $\mathbf v_1, \mathbf v_2, \ldots, \mathbf v_n$ are distinct elements of $S$.",Definition:Linearly Independent,"['Definitions/Linear Independence', 'Definitions/Vector Spaces', 'Definitions/Module Theory', 'Definitions/Linear Algebra', 'Definitions/Linearity']"
Definition:Independent,Independent,"Let $p$ and $q$ be statements.

Let it be the case that:
:$(1): \quad p$ and $q$ are not contrary
:$(2): \quad p$ and $q$ are not subcontrary
:$(3): \quad p$ is not superimplicant to $q$
:$(4): \quad p$ is not subimplicant to $q$
:$(5): \quad p$ and $q$ are not equivalent
:$(6): \quad p$ and $q$ are not contradictory.


Then $p$ and $q$ are independent statements.",Definition:Independent Statements,['Definitions/Logic']
Definition:Index,Index,"Let $I$ and $S$ be sets.

Let $x: I \to S$ be a mapping.

Let $x_i$ denote the image of an element $i \in I$ of the domain $I$ of $x$.

Let $\family {x_i}_{i \mathop \in I}$ denote the set of the images of all the element $i \in I$ under $x$.


An element of the domain $I$ of $x$ is called an index.",Definition:Indexing Set/Index,"['Definitions/Indexed Families', 'Definitions/Indices']"
Definition:Index,Index,"Let $I$ and $S$ be sets.

Let $x: I \to S$ be a mapping.

Let $x_i$ denote the image of an element $i \in I$ of the domain $I$ of $x$.

Let $\family {x_i}_{i \mathop \in I}$ denote the set of the images of all the elements $i \in I$ under $x$.


When a mapping is used in this context, the domain $I$ of $x$ is called the indexing set of the terms $\family {x_i}_{i \mathop \in I}$.


=== Index ===
Let $I$ and $S$ be sets.

Let $x: I \to S$ be a mapping.

Let $x_i$ denote the image of an element $i \in I$ of the domain $I$ of $x$.

Let $\family {x_i}_{i \mathop \in I}$ denote the set of the images of all the element $i \in I$ under $x$.


An element of the domain $I$ of $x$ is called an index.

=== Indexed Set ===
Let $I$ and $S$ be sets.

Let $x: I \to S$ be a mapping.

Let $x_i$ denote the image of an element $i \in I$ of the domain $I$ of $x$.

Let $\family {x_i}_{i \mathop \in I}$ denote the set of the images of all the elements $i \in I$ under $x$.


The image of $x$, that is, $x \sqbrk I$ or $\Img x$, is called an indexed set.

That is, it is the set indexed by $I$.

=== Indexing Function ===
Let $I$ and $S$ be sets.

Let $x: I \to S$ be a mapping.

Let $x_i$ denote the image of an element $i \in I$ of the domain $I$ of $x$.

Let $\family {x_i}_{i \mathop \in I}$ denote the set of the images of all the element $i \in I$ under $x$.


When used in this context, the mapping $x$ is referred to as an indexing function for $S$.


=== Notation ===
The family of elements $x$ of $S$ indexed by $I$ is often seen with one of the following notations:

:$\family {x_i}_{i \mathop \in I}$

:$\paren {x_i}_{i \mathop \in I}$

:$\set {x_i}_{i \mathop \in I}$


There is little consistency in the literature, but $\paren {x_i}_{i \mathop \in I}$ is perhaps most common.

The preferred notation on   is $\family {x_i}_{i \mathop \in I}$.

The subscripted $i \in I$ is often left out, if it is obvious in the particular context.


Note the use of $x_i$ to denote the image of the index $i$ under the indexing function $x$.

As $x$ is actually a mapping, one would expect the conventional notation $\map x i$.

However, this is generally not used, and $x_i$ is used instead.


Category:Definitions/Indexed Families

=== Family ===
Let $I$ and $S$ be sets.

Let $x: I \to S$ be an indexing function for $S$.

Let $x_i$ denote the image of an element $i \in I$ of the domain $I$ of $x$.

Let $\family {x_i}_{i \mathop \in I}$ denote the set of the images of all the elements $i \in I$ under $x$.


The image $\Img x$, consisting of the terms $\family {x_i}_{i \mathop \in I}$, along with the indexing function $x$ itself, is called a family of elements of $S$ indexed by $I$.

=== Term ===
Let $I$ and $S$ be sets.

Let $x: I \to S$ be a mapping.

Let $x_i$ denote the image of an element $i \in I$ of the domain $I$ of $x$.

Let $\family {x_i}_{i \mathop \in I}$ denote the set of the images of all the element $i \in I$ under $x$.


The image of $x$ at an index $i$ is referred to as a term of the (indexed) family, and is denoted $x_i$.


=== Notation ===
The family of elements $x$ of $S$ indexed by $I$ is often seen with one of the following notations:

:$\family {x_i}_{i \mathop \in I}$

:$\paren {x_i}_{i \mathop \in I}$

:$\set {x_i}_{i \mathop \in I}$


There is little consistency in the literature, but $\paren {x_i}_{i \mathop \in I}$ is perhaps most common.

The preferred notation on   is $\family {x_i}_{i \mathop \in I}$.

The subscripted $i \in I$ is often left out, if it is obvious in the particular context.


Note the use of $x_i$ to denote the image of the index $i$ under the indexing function $x$.

As $x$ is actually a mapping, one would expect the conventional notation $\map x i$.

However, this is generally not used, and $x_i$ is used instead.


Category:Definitions/Indexed Families

=== Family of Distinct Elements ===
Let $I$ and $S$ be sets.

Let $x: I \to S$ be an indexing function for $S$.

Let $\family {x_i}_{i \mathop \in I}$ denote the family of elements of $S$ indexed by $x$.


Let $x$ be an injection, that is:
:$\forall \alpha, \beta \in I: \alpha \ne \beta \implies x_\alpha \ne x_\beta$

Then $\family {x_i} _{i \mathop \in I}$ is called a family of distinct elements of $S$.

=== Family of Sets ===
Let $\SS$ be a set of sets.

Let $I$ be an indexing set.

Let $\family {S_i}_{i \mathop \in I}$ be a family of elements of $\SS$ indexed by $I$.


Then $\family {S_i}_{i \mathop \in I}$ is referred to as an indexed family of sets.

=== Family of Subsets ===
Let $S$ be a set.

Let $I$ be an indexing set.

For each $i \in I$, let $S_i$ be a corresponding subset of $S$.

Let $\family {S_i}_{i \mathop \in I}$ be a family of subsets of $S$ indexed by $I$.


Then $\family {S_i}_{i \mathop \in I}$ is referred to as an indexed family of subsets (of $S$ by $I$).",Definition:Indexing Set,"['Definitions/Indexed Families', 'Definitions/Set Theory', 'Definitions/Mapping Theory']"
Definition:Index,Index,"Let $\sequence {x_n}$ be a sequence.

Let $x_k$ be the $k$th term of $\sequence {x_n}$.

Then the integer $k$ is known as the index of $x_k$.",Definition:Term of Sequence/Index,"['Definitions/Sequences', 'Definitions/Indices']"
Definition:Index,Index,"Let $G$ be a group.

Let $H$ be a subgroup of $G$.

The index of $H$ (in $G$), denoted $\index G H$, is the cardinality of the left (or right) coset space $G / H$.


=== Finite Index ===
Let $G$ be a group.

Let $H$ be a subgroup of $G$.

Let $\index G H$ denote the index of $H$ in $G$, that is, the cardinality of the left (or right) coset space $G / H$.


If $G / H$ is a finite set, then $\index G H$ is finite, and $H$ is of finite index in $G$.

=== Infinite Index ===
Let $G$ be a group.

Let $H$ be a subgroup of $G$.

Let $\index G H$ denote the index of $H$ in $G$, that is, the cardinality of the left (or right) coset space $G / H$.


If $G / H$ is an infinite set, then $\index G H$ is infinite, and $H$ is of infinite index in $G$.",Definition:Index of Subgroup,"['Definitions/Index of Subgroups', 'Definitions/Group Theory', 'Definitions/Indices']"
Definition:Index,Index,"Let $\mathbf A$ be an $m \times n$ matrix.

Let $a_{i j}$ be the element in row $i$ and column $j$ of $\mathbf A$.


Then the subscripts $i$ and $j$ are referred to as the indices (singular: index) of $a_{i j}$.",Definition:Matrix/Indices,"['Definitions/Matrices', 'Definitions/Indices']"
Definition:Index,Index,"Scientific notation is an implementation of floating-point representation for representing approximations to (usually large) numbers by presenting them in the form:
:$n \approx m \times 10^e$

where:
:$m$ is a rational number such that $1 \le m < 10$, expressed in decimal notation
:$e$ is an integer.


=== Base ===
Let $n$ be a number expressed in scientific notation as:
:$n \approx m \times 10^e$


The number $10$, in this context, is referred to as the base.

=== Mantissa ===
Let $n$ be a number expressed in scientific notation as:
:$n \approx m \times 10^e$


The number $m$ is known as the mantissa.

=== Exponent ===
Let $n$ be a number expressed in scientific notation as:
:$n \approx m \times 10^e$


The number $e$ is known as the exponent.",Definition:Scientific Notation,"['Definitions/Scientific Notation', 'Definitions/Floating-Point Representation', 'Definitions/Numbers']"
Definition:Index,Index,"In the power operation $x^r$, the number $r$ is known as the exponent of $x$, particularly for $r \in \R$.",Definition:Power (Algebra)/Exponent,"['Definitions/Exponents', 'Definitions/Powers', 'Definitions/Indices']"
Definition:Index,Index,"Let $\sqrt [n] x$ denote the $n$th root of $x$.

The number $n$ is known as the index of the root.


If $n$ is not specified, that is $\sqrt x$ is presented, this means the square root.",Definition:Root of Number/Index,"['Definitions/Roots of Numbers', 'Definitions/Indices']"
Definition:Index,Index,"Consider the summation, in either of the three forms:

:$\ds \sum_{j \mathop = 1}^n a_j \qquad \sum_{1 \mathop \le j \mathop \le n} a_j \qquad \sum_{\map R j} a_j$


The variable $j$, an example of a bound variable, is known as the index variable of the summation.",Definition:Summation/Index Variable,"['Definitions/Summations', 'Definitions/Indices']"
Definition:Index,Index,"An index number in the context of statistics and economics is a measure of a change in some business activity over time.

It is constructed based on reliable information on relevant components, usually weighted according to importance.

An index number is usually defined relative to a specific base year, at which time the value of the index number would be assigned a round number, usually $100$ but sometimes $1000$.


=== Base Year ===
The base year of an index number is the year from which the index number is first calculated.


The index number for that year is usually assigned a round number, for example $100$ of $1000$.

=== Relative ===
Let $I$ be an index number composed of a weighted mean of a number of other index numbers.

Each of those contributing index numbers are referred to as relatives of $I$",Definition:Index Number,"['Definitions/Index Numbers', 'Definitions/Statistics', 'Definitions/Economics', 'Definitions/Indices']"
Definition:Inertia,Inertia,"Inertia is the tendency of a body to maintain the same velocity in the absence of an external force, in accordance with Newton's First Law of Motion.

Equivalently put, inertia is the resistance of a body to a change in its motion.

Inertia is equivalent to mass.",Definition:Inertia (Physics),"['Definitions/Inertia (Physics)', 'Definitions/Physics', 'Definitions/Inertia']"
Definition:Inertia,Inertia,"Let $\mathbf H$ be a Hermitian matrix.

The inertia of $\mathbf H$ is an ordered triple of integers comprising:
:the number of positive eigenvalues of $\mathbf H$
:the number of negative eigenvalues of $\mathbf H$
:the number of zero eigenvalues of $\mathbf H$
in that order.",Definition:Inertia of Hermitian Matrix,"['Definitions/Inertia of Hermitian Matrices', 'Definitions/Hermitian Matrices', 'Definitions/Inertia']"
Definition:Infimum,Infimum,"Let $\struct {S, \preccurlyeq}$ be an ordered set.

Let $T \subseteq S$.


An element $c \in S$ is the infimum of $T$ in $S$  if and only if :

:$(1): \quad c$ is a lower bound of $T$ in $S$
:$(2): \quad d \preccurlyeq c$ for all lower bounds $d$ of $T$ in $S$.


If there exists an infimum of $T$ (in $S$), we say that:
:$T$ admits an infimum (in $S$) or
:$T$ has an infimum (in $S$).


=== Subset of Real Numbers ===

The concept is often encountered where $\struct {S, \preccurlyeq}$ is the set of real numbers under the usual ordering $\struct {\R, \le}$:

Let $T \subseteq \R$.


A real number $c \in \R$ is the infimum of $T$ in $\R$  if and only if :

:$(1): \quad c$ is a lower bound of $T$ in $\R$
:$(2): \quad d \le c$ for all lower bounds $d$ of $T$ in $\R$.


If there exists an infimum of $T$ (in $\R$), we say that $T$ admits an infimum (in $\R$).


The infimum of $T$ is denoted $\inf T$ or $\map \inf T$.

The infimum of $T$ is denoted $\inf T$ or $\map \inf T$.


=== Finite Infimum ===
Let $\struct {S, \preccurlyeq}$ be an ordered set.

Let $T \subseteq S$ admit a infimum $\map \inf T$.


If $T$ is finite, $\map \inf T$ is called a finite infimum.",Definition:Infimum of Set,['Definitions/Infima']
Definition:Infimum,Infimum,"Let $T \subseteq \R$.


A real number $c \in \R$ is the infimum of $T$ in $\R$  if and only if :

:$(1): \quad c$ is a lower bound of $T$ in $\R$
:$(2): \quad d \le c$ for all lower bounds $d$ of $T$ in $\R$.


If there exists an infimum of $T$ (in $\R$), we say that $T$ admits an infimum (in $\R$).


The infimum of $T$ is denoted $\inf T$ or $\map \inf T$.",Definition:Infimum of Set/Real Numbers,['Definitions/Infima']
Definition:Infimum,Infimum,"Let $S$ be a set.

Let $\struct {T, \preceq}$ be an ordered set.

Let $f: S \to T$ be a mapping from $S$ to $T$.

Let $f \sqbrk S$, the image of $f$, admit an infimum.


Then the infimum of $f$ (on $S$) is defined by:
:$\ds \inf_{x \mathop \in S} \map f x = \inf f \sqbrk S$


=== Real-Valued Function ===
Let $f: S \to \R$ be a real-valued function.

Let $f$ be bounded below on $S$.


=== Definition 1 ===
Let $f: S \to \R$ be a real-valued function.

Let $f$ be bounded below on $S$.


The infimum of $f$ on $S$ is defined by:
:$\ds \inf_{x \mathop \in S} \map f x = \inf f \sqbrk S$
where
:$\inf f \sqbrk S$ is the infimum in $\R$ of the image of $S$ under $f$.

=== Definition 2 ===
Let $f: S \to \R$ be a real-valued function.

Let $f$ be bounded below on $S$.


The infimum of $f$ on $S$ is defined as $\ds \inf_{x \mathop \in S} \map f x := k \in \R$ such that:

:$(1): \quad \forall x \in S: k \le \map f x$
:$(2): \quad \forall \epsilon \in \R_{>0}: \exists x \in S: \map f x < k + \epsilon$",Definition:Infimum of Mapping,['Definitions/Infima']
Definition:Infimum,Infimum,"Let $f: S \to \R$ be a real-valued function.

Let $f$ be bounded below on $S$.


=== Definition 1 ===
Let $f: S \to \R$ be a real-valued function.

Let $f$ be bounded below on $S$.


The infimum of $f$ on $S$ is defined by:
:$\ds \inf_{x \mathop \in S} \map f x = \inf f \sqbrk S$
where
:$\inf f \sqbrk S$ is the infimum in $\R$ of the image of $S$ under $f$.

=== Definition 2 ===
Let $f: S \to \R$ be a real-valued function.

Let $f$ be bounded below on $S$.


The infimum of $f$ on $S$ is defined as $\ds \inf_{x \mathop \in S} \map f x := k \in \R$ such that:

:$(1): \quad \forall x \in S: k \le \map f x$
:$(2): \quad \forall \epsilon \in \R_{>0}: \exists x \in S: \map f x < k + \epsilon$",Definition:Infimum of Mapping/Real-Valued Function,['Definitions/Infima']
Definition:Infimum,Infimum,"A special case of an infimum of a mapping is an infimum of a sequence, where the domain of the mapping is $\N$.

Let $\struct {T, \preceq}$ be an ordered set.

Let $\sequence {x_n}$ be a sequence in $T$.


Let $\set {x_n: n \in \N}$ admit an infimum.


Then the infimum of $\sequence {x_n}$) is defined as:
:$\map \inf {\sequence {x_n} } = \map \inf {\set {x_n: n \in \N} }$",Definition:Infimum of Sequence,"['Definitions/Sequences', 'Definitions/Infima']"
Definition:Infimum,Infimum,"Let $\sequence {x_n}$ be a real sequence.


Let $\set {x_n: n \in \N}$ admit an infimum.


Then the infimum of $\sequence {x_n}$) is defined as:
:$\map \inf {\sequence {x_n} } = \map \inf {\set {x_n: n \in \N} }$",Definition:Infimum of Real Sequence,"['Definitions/Sequences', 'Definitions/Infima']"
Definition:Integral,Integral,"Let $A$ be an extension of a commutative ring with unity $R$.


Let $C$ be the set of all elements of $A$ that are integral over $R$.

Then $C$ is called the integral closure of $R$ in $A$.",Definition:Integral Closure,"['Definitions/Algebraic Number Theory', 'Definitions/Commutative Algebra']"
Definition:Integral,Integral,"=== Ring Extension ===
Let $\phi : A \hookrightarrow B$ be a ring extension.

Let $C$ be the integral closure of $A$ in $B$.


Then $A$ is integrally closed in $B$  if and only if  $C = \phi(A)$.

=== Integral Domain ===
Let $R$ be an integral domain.


Then $R$ is integrally closed  if and only if  it is integrally closed in its field of fractions.",Definition:Integrally Closed,"['Definitions/Algebraic Number Theory', 'Definitions/Commutative Algebra']"
Definition:Integral,Integral,"Let $A$ be a commutative ring with unity.

Let $R \subseteq A$ be a subring.


Then $a \in A$ is said to be integral over $R$  if and only if  is is a root of a monic nonzero polynomial over $R$.",Definition:Integral Element of Ring Extension,"['Definitions/Algebraic Number Theory', 'Definitions/Commutative Algebra']"
Definition:Integral,Integral,"Let $A$ be a commutative ring with unity.

Let $R\subset A$ be a subring.


The ring extension $R \subseteq A$ is said to be integral  if and only if  for all $a \in A$, $a$ is integral over $R$.",Definition:Integral Ring Extension,['Definitions/Commutative Algebra']
Definition:Integral,Integral,"=== Definition 1 ===
An integral domain $\struct {D, +, \circ}$ is:

:a commutative ring which is non-null
:with a unity
:in which there are no (proper) zero divisors, that is:
::: $\forall x, y \in D: x \circ y = 0_D \implies x = 0_D \text{ or } y = 0_D$

that is, in which all non-zero elements are cancellable.

=== Definition 2 ===
An integral domain $\struct {D, +, \circ}$ is a commutative ring such that $\struct {D^*, \circ}$ is a monoid, all of whose elements are cancellable.

In this context, $D^*$ denotes the ring $D$ without zero: $D \setminus \set {0_D}$.

=== Integral Domain Axioms ===
 ",Definition:Integral Domain,"['Definitions/Integral Domains', 'Definitions/Ring Theory']"
Definition:Integral,Integral,An integral polynomial is a polynomial over the ring of integers $\Z$.,Definition:Integral Polynomial,"['Definitions/Integral Polynomials', 'Definitions/Polynomial Theory', 'Definitions/Integers']"
Definition:Integral,Integral,"=== Rings and Fields ===
Let $\struct {F, +, \times}$ be a ring or a field whose zero is $0_F$.

Let $a \in F$.

Let $n \in \Z$ be an integer.

Then $n \cdot a$ is an integral multiple of $a$ where $n \cdot a$ is defined as:

:$n \cdot a := \begin {cases}
0_F & : n = 0 \\
\paren {\paren {n - 1} \cdot a} + a & : n > 1 \\
\size n \cdot \paren {-a} & : n < 0 \\
\end {cases}$
where $\size n$ is the absolute value of $n$.


Using sum notation:
:$\ds n \cdot a := \sum_{j \mathop = 1}^n a$

=== Real Numbers ===

This concept is often seen when $F$ is the set of real numbers $\R$.

Let $x, y \in \R$ be real numbers.

Then $x$ is an integral multiple of $y$  if and only if  $x$ is congruent to $0$ modulo $y$:
:$x \equiv 0 \pmod y$

That is:
:$\exists k \in \Z: x = 0 + k y$",Definition:Integral Multiple,"['Definitions/Discrete Mathematics', 'Definitions/Number Theory', 'Definitions/Field Theory']"
Definition:Integral,Integral,"Let $x$ be a real number.

Informally, the floor function of $x$ is the greatest integer less than or equal to $x$.

=== Definition 1 ===
Let $x$ be a real number.


The floor function of $x$ is defined as the supremum of the set of integers no greater than $x$:
:$\floor x := \sup \set {m \in \Z: m \le x}$
where $\le$ is the usual ordering on the real numbers.

=== Definition 2 ===
Let $x \in \R$ be a real number.


The floor function of $x$, denoted $\floor x$, is defined as the greatest element of the set of integers:
:$\set {m \in \Z: m \le x}$
where $\le$ is the usual ordering on the real numbers.

=== Definition 3 ===
Let $x$ be a real number.


The floor function of $x$ is the unique integer $\floor x$ such that:
:$\floor x \le x < \floor x + 1$",Definition:Floor Function,"['Definitions/Floor Function', 'Definitions/Real Analysis', 'Definitions/Number Theory', 'Definitions/Discrete Mathematics']"
Definition:Integral,Integral,"Integral calculus is a subfield of calculus which is concerned with the study of the rates at which quantities accumulate.

Equivalently, given the rate of change of a quantity integral calculus provides techniques of providing the quantity itself.

The equivalence of the two uses are demonstrated in the Fundamental Theorem of Calculus.

The technique is also frequently used for the purpose of calculating areas and volumes of curved geometric figures.",Definition:Calculus/Integral,"['Definitions/Integral Calculus', 'Definitions/Calculus', 'Definitions/Branches of Mathematics']"
Definition:Integral,Integral,"=== Indefinite Integral ===
Let $F$ be a real function which is continuous on the closed interval $\closedint a b$ and differentiable on the open interval $\openint a b$.

Let $f$ be a real function which is continuous on the open interval $\openint a b$.


Let:
:$\forall x \in \openint a b: \map {F'} x = \map f x$
where $F'$ denotes the derivative of $F$   $x$.


Then $F$ is a primitive of $f$, and is denoted:
:$\ds F = \int \map f x \rd x$

=== Definite Integral ===
Let $\closedint a b$ be a closed real interval.

Let $f: \closedint a b \to \R$ be a real function.

Let $\Delta$ be a finite subdivision of $\closedint a b$, $\Delta = \set {x_0, \ldots, x_n}$, $x_0 = a$ and $x_n = b$.

Let there for $\Delta$ be a corresponding sequence $C$ of sample points $c_i$, $C = \tuple {c_1, \ldots, c_n}$, where $c_i \in \closedint {x_{i - 1} } {x_i}$ for every $i \in \set {1, \ldots, n}$.

Let $\map S {f; \Delta, C}$ denote the Riemann sum of $f$ for the subdivision $\Delta$ and the sample point sequence $C$.


Then $f$ is said to be (properly) Riemann integrable on $\closedint a b$  if and only if :
:$\exists L \in \R: \forall \epsilon \in \R_{>0}: \exists \delta \in \R_{>0}: \forall$ finite subdivisions $\Delta$ of $\closedint a b: \forall$ sample point sequences $C$ of $\Delta: \norm \Delta < \delta \implies \size {\map S {f; \Delta, C} - L} < \epsilon$
where $\norm \Delta$ denotes the norm of $\Delta$.


The real number $L$ is called the Riemann integral of $f$ over $\closedint a b$ and is denoted:
:$\ds \int_a^b \map f x \rd x$


More usually (and informally), we say:
:$f$ is (Riemann) integrable over $\closedint a b$.


=== Riemann Integral as Integral Operator ===
Let $C \closedint a b$ be the space of continuous functions.

Let $x \in C \closedint a b$ be a Riemann integrable function.

Let $\R$ be the set of real numbers.


The Riemann integral operator, denoted by $I$, is the mapping $I : C \closedint a b \to \R$ such that:

:$\ds \map I x := \int_a^b \map x t \rd t$

where $\ds \int_a^b \map x t \rd t$ is the Riemann integral.",Definition:Integral (Calculus),['Definitions/Integral Calculus']
Definition:Integral,Integral,"Let $\closedint a b$ be a closed real interval.

Let $f: \closedint a b \to \R$ be a real function.


=== Riemann Integral ===
Let $\closedint a b$ be a closed real interval.

Let $f: \closedint a b \to \R$ be a real function.

Let $\Delta$ be a finite subdivision of $\closedint a b$, $\Delta = \set {x_0, \ldots, x_n}$, $x_0 = a$ and $x_n = b$.

Let there for $\Delta$ be a corresponding sequence $C$ of sample points $c_i$, $C = \tuple {c_1, \ldots, c_n}$, where $c_i \in \closedint {x_{i - 1} } {x_i}$ for every $i \in \set {1, \ldots, n}$.

Let $\map S {f; \Delta, C}$ denote the Riemann sum of $f$ for the subdivision $\Delta$ and the sample point sequence $C$.


Then $f$ is said to be (properly) Riemann integrable on $\closedint a b$  if and only if :
:$\exists L \in \R: \forall \epsilon \in \R_{>0}: \exists \delta \in \R_{>0}: \forall$ finite subdivisions $\Delta$ of $\closedint a b: \forall$ sample point sequences $C$ of $\Delta: \norm \Delta < \delta \implies \size {\map S {f; \Delta, C} - L} < \epsilon$
where $\norm \Delta$ denotes the norm of $\Delta$.


The real number $L$ is called the Riemann integral of $f$ over $\closedint a b$ and is denoted:
:$\ds \int_a^b \map f x \rd x$


More usually (and informally), we say:
:$f$ is (Riemann) integrable over $\closedint a b$.


=== Riemann Integral as Integral Operator ===
Let $C \closedint a b$ be the space of continuous functions.

Let $x \in C \closedint a b$ be a Riemann integrable function.

Let $\R$ be the set of real numbers.


The Riemann integral operator, denoted by $I$, is the mapping $I : C \closedint a b \to \R$ such that:

:$\ds \map I x := \int_a^b \map x t \rd t$

where $\ds \int_a^b \map x t \rd t$ is the Riemann integral.

=== Darboux Integral ===
Let $\closedint a b$ be a closed real interval.

Let $f: \closedint a b \to \R$ be a real function.

Let $f$ be bounded on $\closedint a b$.


Suppose that:
:$\ds \underline {\int_a^b} \map f x \rd x = \overline {\int_a^b} \map f x \rd x$
where $\ds \underline {\int_a^b}$ and $\ds \overline {\int_a^b}$ denote the lower Darboux integral and upper Darboux integral, respectively.


Then the definite (Darboux) integral of $f$ over $\closedint a b$ is defined as:
:$\ds \int_a^b \map f x \rd x = \underline {\int_a^b} \map f x \rd x = \overline {\int_a^b} \map f x \rd x$


$f$ is formally defined as (properly) integrable over $\closedint a b$ in the sense of Darboux, or (properly) Darboux integrable over $\closedint a b$.


More usually (and informally), we say:
:$f$ is (Darboux) integrable over $\closedint a b$.",Definition:Definite Integral,"['Definitions/Definite Integrals', 'Definitions/Integral Calculus']"
Definition:Integral,Integral,"=== Primitive of Real Function ===
Let $F$ be a real function which is continuous on the closed interval $\closedint a b$ and differentiable on the open interval $\openint a b$.

Let $f$ be a real function which is continuous on the open interval $\openint a b$.


Let:
:$\forall x \in \openint a b: \map {F'} x = \map f x$
where $F'$ denotes the derivative of $F$   $x$.


Then $F$ is a primitive of $f$, and is denoted:
:$\ds F = \int \map f x \rd x$

=== Primitive of Complex Function ===
Let $F: D \to \C$ be a complex function which is complex-differentiable on a connected domain $D$.

Let $f: D \to \C$ be a continuous complex function.


Let:
:$\forall z \in D: \map {F'} z = \map f z$
where $F'$ denotes the derivative of $F$   $z$.


Then $F$ is a primitive of $f$, and is denoted:
:$\ds F = \int \map f z \rd z$

=== Primitive of Vector-Valued Function ===
Let $U \subset \R$ be an open set in $\R$.

Let $\mathbf f: U \to \R^n$ be a vector-valued function on $U$:

:$\forall x \in U: \map {\mathbf f} x = \ds \sum_{k \mathop = 1}^n \map {f_k} x \mathbf e_k$

where:
:$f_1, f_2, \ldots, f_n$ are real functions from $U$ to $\R$
:$\tuple {\mathbf e_1, \mathbf e_2, \ldots, \mathbf e_k}$ denotes the standard ordered basis on $\R^n$.

Let $\mathbf f$ be differentiable on $U$.


Let $\map {\mathbf g} x := \dfrac \d {\d x} \map {\mathbf f} x$ be the derivative of $\mathbf f$   $x$.


The primitive of $\mathbf g$   $x$ is defined as:

:$\ds \int \map {\mathbf g} x \rd x := \map {\mathbf f} x + \mathbf c$

where $\mathbf c$ is a arbitrary constant vector.",Definition:Primitive (Calculus),"['Definitions/Primitives', 'Definitions/Integral Calculus']"
Definition:Integral,Integral,"Let $\Phi$ be a differential equation defined on a domain $D$.

Let $\phi$ be a function which satisfies $\Phi$ on the whole of $D$.


Then $\phi$ is known as a solution of $\Phi$.


Note that, in general, there may be more than one solution to a given differential equation.

On the other hand, there may be none at all.


=== General Solution ===
Let $\Phi$ be a differential equation.

The general solution to $\Phi$ is the set of all functions $\phi$ that satisfy $\Phi$.


 

=== Particular Solution ===
Let $\Phi$ be a differential equation.

Let $S$ denote the solution set of $\Phi$.

A particular solution of $\Phi$ is the element of $S$, or subset of $S$, which satisfies a particular boundary condition of $\Phi$.

=== Weak Solution ===
A weak solution is a solution to a non-standard formulation of a differential equation.

 ",Definition:Differential Equation/Solution,"['Definitions/Solutions to Differential Equations', 'Definitions/Differential Equations']"
Definition:Interior,Interior,"Let $T = \struct {S, \tau}$ be a topological space.

Let $H \subseteq S$.


=== Definition 1 ===
Let $T = \struct {S, \tau}$ be a topological space.

Let $H \subseteq S$.


The interior of $H$ is the union of all subsets of $H$ which are open in $T$.


That is, the interior of $H$ is defined as:
:$\ds H^\circ := \bigcup_{K \mathop \in \mathbb K} K$
where $\mathbb K = \set {K \in \tau: K \subseteq H}$.

=== Definition 2 ===
Let $T = \struct {S, \tau}$ be a topological space.

Let $H \subseteq S$.


The interior of $H$ is defined as the largest open set of $T$ which is contained in $H$.

=== Definition 3 ===
Let $T = \struct {S, \tau}$ be a topological space.

Let $H \subseteq S$.


The interior of $H$ is the set of all interior points of $H$.",Definition:Interior (Topology),"['Definitions/Set Interiors', 'Definitions/Topology']"
Definition:Interior,Interior,"Let $H \subseteq \C$ be a subset of the complex plane.

The interior of $H$ is the subset of $H$ which consists of the interior points of $H$.",Definition:Interior (Complex Analysis),['Definitions/Complex Analysis']
Definition:Interior,Interior,"Let $S \subseteq \C$ be a subset of the complex plane.

Let $z \in S$.


$z$ is an interior point of $S$  if and only if  $z$ has an $\epsilon$-neighborhood $\map {N_\epsilon} z$ such that $\map {N_\epsilon} z \subseteq S$.",Definition:Interior Point (Complex Analysis),['Definitions/Complex Analysis']
Definition:Interior,Interior,"=== Metric Space ===
Let $M = \struct {A, d}$ be a metric space.

A region of $M$ is a subset $U$ of $M$ such that $U$ is:

:$(1): \quad$ non-empty
:$(2): \quad$ path-connected.

=== Complex ===
Let $D \subseteq \C$ be a subset of the set of complex numbers.

$D$ is a region of $\C$  if and only if :
:$(1): \quad$ $D$ is non-empty
:$(2): \quad$ $D$ is path-connected.

=== Region in the Plane ===

The usual usage of region is in the real number plane or complex plane.

A point set $R$ in the plane is a region  if and only if :

:$(1): \quad$ Each point of $R$ is the center of a circle all of whose elements consist of points of $R$
:$(2): \quad$ Each point of $R$ can be joined by a curve consisting entirely of points of $R$.

==== Interior ====

The boundary of a region separates its interior from the exterior.

The interior consists of the points of the plane which are the elements of the region.

Such points are called interior points of the region.


It is ""usual"" that the interior is the ""smaller bit"" which is visually apparently on the inside as it appears on the page or screen, but this is of course not necessarily the case.


Also see the definition of interior and boundary from a topological perspective.


==== Bounded ====

A region in the the plane is bounded if there is a circle in the plane which encloses it.


Also see the definition of bounded in the context of a metric space.",Definition:Region,"['Definitions/Regions', 'Definitions/Metric Spaces', 'Definitions/Geometry', 'Definitions/Analysis']"
Definition:Interior,Interior,":


An interior angle of a transversal is an angle which is between the two lines cut by that transversal.

In the above figure, the interior angles with respect to the transversal $EF$ are:
:$\angle AHJ$
:$\angle CJH$
:$\angle BHJ$
:$\angle DJH$",Definition:Transversal (Geometry)/Interior Angle,['Definitions/Transversals (Geometry)']
Definition:Interior,Interior,"The internal angle of a vertex of a polygon is the size of the angle between the sides adjacent to that vertex, as measured inside the polygon.",Definition:Polygon/Internal Angle,"['Definitions/Internal Angles', 'Definitions/Polygons']"
Definition:Interior,Interior,"Let $f: \closedint 0 1 \to \R^2$ be a Jordan curve.


It follows from the Jordan Curve Theorem that $\R^2 \setminus \Img f$ is a union of two disjoint connected components, one of which is bounded.

This bounded component is called the interior of $f$, and is denoted as $\Int f$.",Definition:Jordan Curve/Interior,['Definitions/Jordan Curves']
Definition:Interior Angle,Interior Angle,"The internal angle of a vertex of a polygon is the size of the angle between the sides adjacent to that vertex, as measured inside the polygon.",Definition:Polygon/Internal Angle,"['Definitions/Internal Angles', 'Definitions/Polygons']"
Definition:Interior Angle,Interior Angle,":


An interior angle of a transversal is an angle which is between the two lines cut by that transversal.

In the above figure, the interior angles with respect to the transversal $EF$ are:
:$\angle AHJ$
:$\angle CJH$
:$\angle BHJ$
:$\angle DJH$",Definition:Transversal (Geometry)/Interior Angle,['Definitions/Transversals (Geometry)']
Definition:Intersection,Intersection,"Let $S$ and $T$ be sets.


The (set) intersection of $S$ and $T$ is written $S \cap T$.

It means the set which consists of all the elements which are contained in both of $S$ and $T$:

:$x \in S \cap T \iff x \in S \land x \in T$

or, more formally:

:$A = S \cap T \iff \forall z: \paren {z \in A \iff z \in S \land z \in T}$


We can write:

:$S \cap T := \set {x: x \in S \land x \in T}$

and can voice it $S$ intersect $T$.


It can be seen that, in this form, $\cap$ is a binary operation which acts on sets.


One often says that two sets intersect  if and only if  they have non-empty intersection.


=== Set of Sets ===
Let $\Bbb S$ be a set of sets.

The intersection of $\Bbb S$ is:
:$\bigcap \Bbb S := \set {x: \forall S \in \Bbb S: x \in S}$
That is, the set of all objects that are elements of all the elements of $\Bbb S$.


Thus:
:$\bigcap \set {S, T} := S \cap T$

=== Family of Sets ===
Let $I$ be an indexing set.

Let $\family {S_i}_{i \mathop \in I}$ be a family of sets indexed by $I$.


Then the intersection of $\family {S_i}$ is defined as:

:$\ds \bigcap_{i \mathop \in I} S_i := \set {x: \forall i \in I: x \in S_i}$


=== In the context of the Universal Set ===

In treatments of set theory in which the concept of the universal set is recognised, this can be expressed as follows.

Let $\mathbb U$ be a universal set.

Let $I$ be an indexing set.

Let $\family {S_i}_{i \mathop \in I}$ be an indexed family of subsets of $\mathbb U$.


Then the intersection of $\family {S_i}$ is defined and denoted as:

:$\ds \bigcap_{i \mathop \in I} S_i := \set {x \in \mathbb U: \forall i \in I: x \in S_i}$

=== Subsets of General Set ===
This definition is the same when the universal set $\mathbb U$ is replaced by any set $X$, which may or may not be a universal set:

Let $I$ be an indexing set.

Let $\family {S_i}_{i \mathop \in I}$ be an indexed family of subsets of a set $X$.


Then the intersection of $\family {S_i}$ is defined as:

:$\ds \bigcap_{i \mathop \in I} S_i := \set {x \in X: \exists i \in I: x \in S_i}$
where $i$ is a bound variable.

=== Countable Intersection ===
Let $\mathbb S$ be a set of sets.

Let $\sequence {S_n}_{n \mathop \in \N}$ be a sequence in $\mathbb S$.

Let $S$ be the intersection of $\sequence {S_n}_{n \mathop \in \N}$:
:$\ds S = \bigcap_{n \mathop \in \N} S_n$


Then $S$ is a countable intersection of sets in $\mathbb S$.

=== Finite Intersection ===
Let $S = S_1 \cap S_2 \cap \ldots \cap S_n$.

Then:
:$\ds S = \bigcap_{i \mathop \in \N^*_n} S_i := \set {x: \forall i \in \N^*_n: x \in S_i}$
where $\N^*_n = \set {1, 2, 3, \ldots, n}$.


If it is clear from the context that $i \in \N^*_n$, we can also write $\ds \bigcap_{\N^*_n} S_i$.",Definition:Set Intersection,"['Definitions/Set Theory', 'Definitions/Set Intersection']"
Definition:Intersection,Intersection,"The intersection of two lines $AB$ and $CD$ is denoted by $AB \cap CD$.

The intersection of two geometric figures is the set of points shared by both figures.


Note that this use of $\cap$ is consistent with that of its more usual context of set intersection.


When two lines intersect, they are said to cut each other.",Definition:Intersection (Geometry),['Definitions/Geometry']
Definition:Inverse,Inverse,"The inverse of the conditional:
: $p \implies q$
is the statement:
:$\neg p \implies \neg q$",Definition:Inverse Statement,['Definitions/Conditional']
Definition:Inverse,Inverse,"Let $\struct {S, \circ}$ be an algebraic structure with an identity element $e_S$.

Let $x, y \in S$ be elements.


The element $y$ is an inverse of $x$  if and only if :
:$y \circ x = e_S = x \circ y$
that is,  if and only if  $y$ is both:
:a left inverse of $x$
and:
:a right inverse of $x$.",Definition:Inverse (Abstract Algebra)/Inverse,['Definitions/Inverse Elements']
Definition:Inverse,Inverse,"Let $\struct {R, +, \circ}$ be a ring with unity.

Let $U_R$ denotes the group of units of $R$.

The inverse of $x \in U_R$ by $\circ$ is called the (ring) product inverse of $x$.


The usual means of denoting the product inverse of an element $x$ is by $x^{-1}$.

Thus it is distinguished from the additive inverse of $x$, that is, the (ring) negative of $x$, which is usually denoted $-x$.",Definition:Product Inverse,['Definitions/Ring Theory']
Definition:Inverse,Inverse,"An inverse semigroup is a semigroup $\struct {S, \circ}$ such that:

:$\forall a \in S: \exists! b \in S: a = a \circ b \circ a, b = b \circ a \circ b$


=== Inverse ===
Let $(S, \circ)$ be an inverse semigroup.

Let $a\in S$.


The inverse of $a$ is the unique element $b\in S$ such that:
:$a = a \circ b \circ a$ and $b = b \circ a \circ b$


Category:Definitions/Inverse Semigroups",Definition:Inverse Semigroup,"['Definitions/Algebraic Structures', 'Definitions/Semigroups', 'Definitions/Inverse Semigroups']"
Definition:Inverse,Inverse,"Let $(S, \circ)$ be an inverse semigroup.

Let $a\in S$.


The inverse of $a$ is the unique element $b\in S$ such that:
:$a = a \circ b \circ a$ and $b = b \circ a \circ b$


Category:Definitions/Inverse Semigroups",Definition:Inverse in Inverse Semigroup,['Definitions/Inverse Semigroups']
Definition:Inverse,Inverse,"Let $V$ and $U$ be vector spaces.

Let $A : V \to U$ be an invertible (in the sense of a mapping) linear transformation with inverse mapping $A^{-1} : U \to V$.


We say that $A^{-1}$ is the inverse linear transformation of $A$.",Definition:Inverse Linear Transformation,['Definitions/Linear Transformations']
Definition:Inverse,Inverse,"Let $X$ be a vector space.

Let $A : X \to X$ be an invertible (in the sense of a mapping) linear transformation with inverse mapping $A^{-1} : X \to X$.


We say that $A^{-1}$ is the inverse linear operator of $A$.",Definition:Inverse Linear Operator,['Definitions/Linear Operators']
Definition:Inverse,Inverse,"Let $\RR \subseteq S \times T$ be a relation.


The inverse relation to (or of) $\RR$ is defined as:

:$\RR^{-1} := \set {\tuple {t, s}: \tuple {s, t} \in \RR}$


That is, $\RR^{-1} \subseteq T \times S$ is the relation which satisfies:

:$\forall s \in S: \forall t \in T: \tuple {t, s} \in \RR^{-1} \iff \tuple {s, t} \in \RR$


=== Class Theoretical Definition ===
 
Let $V$ be a basic universe.

Let $A$ and $B$ be subclasses of $V$.

Let $\RR \subseteq A \times B$ be a relation on $A \times B$.


The inverse relation to (or of) $\RR$ is defined as the class of all ordered pairs $\tuple {b, a}$ such that $\tuple {a, b} \in \RR$:

:$\RR^{-1} := \set {\tuple {b, a}: \tuple {a, b} \in \RR}$",Definition:Inverse Relation,"['Definitions/Inverse Relations', 'Definitions/Relation Theory']"
Definition:Inverse,Inverse,"=== Definition 1 ===
Let $f: S \to T$ be a mapping.

Let $f^{-1} \subseteq T \times S$ be the inverse of $f$:
:$f^{-1} := \set {\tuple {t, s}: \map f s = t}$


Let $f^{-1}$ itself be a mapping:
:$\forall y \in T: \tuple {y, x_1} \in f^{-1} \land \tuple {y, x_2} \in f^{-1} \implies x_1 = x_2$
and
:$\forall y \in T: \exists x \in S: \tuple {y, x} \in f$


Then $f^{-1}$ is called the inverse mapping of $f$.

=== Definition 2 ===
Let $f: S \to T$ and $g: T \to S$ be mappings.

Let:
:$g \circ f = I_S$
:$f \circ g = I_T$
where:
:$g \circ f$ and $f \circ g$ denotes the composition of $f$ with $g$ in either order
:$I_S$ and $I_T$ denote the identity mappings on $S$ and $T$ respectively.

That is, $f$ and $g$ are both left inverse mappings and right inverse mappings of each other.


Then:
:$g$ is the inverse (mapping) of $f$
:$f$ is the inverse (mapping) of $g$.",Definition:Inverse Mapping,"['Definitions/Inverse Mappings', 'Definitions/Inverses of Mappings', 'Definitions/Mapping Theory', 'Definitions/Inverses']"
Definition:Inverse,Inverse,"Let $f: S \to T$ be a mapping.


The inverse of $f$ is its inverse relation, defined as:
:$f^{-1} := \set {\tuple {t, s}: \map f s = t}$

That is:
:$f^{-1} := \set {\tuple {t, s}: \tuple {s, t} \in f}$


That is, $f^{-1} \subseteq T \times S$ is the relation which satisfies:

:$\forall s \in S: \forall t \in T: \tuple {t, s} \in f^{-1} \iff \tuple {s, t} \in f$",Definition:Inverse of Mapping,"['Definitions/Inverses of Mappings', 'Definitions/Inverse Relations', 'Definitions/Mapping Theory', 'Definitions/Relation Theory', 'Definitions/Inverses']"
Definition:Inverse,Inverse,"Let $n \in \Z_{>0}$ be a (strictly) positive integer.

Let $\mathbf A$ be a square matrix of order $n$.


Let there exist a square matrix $\mathbf B$ of order $n$ such that:
:$\mathbf A \mathbf B = \mathbf I_n = \mathbf B \mathbf A$

where $\mathbf I_n$ denotes the unit matrix of order $n$.


Then $\mathbf B$ is called the inverse of $\mathbf A$ and is usually denoted $\mathbf A^{-1}$.


=== Left Inverse Matrix ===
Let $m, n \in \Z_{>0}$ be a (strictly) positive integer.


Let $\mathbf A = \sqbrk a_{m n}$ be a matrix of order $m \times n$.

Let $\mathbf B = \sqbrk b_{n m}$ be a matrix of order $n \times m$ such that:
:$\mathbf B \mathbf A = \mathbf I_n$

where $\mathbf I_n$ denotes the unit matrix of order $n$.


Then $\mathbf B$ is known as a left inverse (matrix) of $\mathbf A$.

=== Right Inverse Matrix ===
Let $m, n \in \Z_{>0}$ be a (strictly) positive integer.


Let $\mathbf A = \sqbrk a_{m n}$ be a matrix of order $m \times n$.

Let $\mathbf B = \sqbrk b_{n m}$ be a matrix of order $n \times m$ such that:
:$\mathbf A \mathbf B = \mathbf I_m$

where $\mathbf I_m$ denotes the unit matrix of order $m$.


Then $\mathbf B$ is known as a right inverse (matrix) of $\mathbf A$.",Definition:Inverse Matrix,"['Definitions/Inverse Matrices', 'Definitions/Matrix Theory']"
Definition:Inverse Image,Inverse Image,"Let $S$ and $T$ be sets.

Let $\powerset S$ and $\powerset T$ be their power sets.


=== Relation ===

Let $\RR \subseteq S \times T$ be a relation on $S \times T$.

Let $S$ and $T$ be sets.

Let $\powerset S$ and $\powerset T$ be their power sets.

Let $\RR \subseteq S \times T$ be a relation on $S \times T$.


The inverse image mapping of $\RR$ is the mapping $\RR^\gets: \powerset T \to \powerset S$ that sends a subset $Y \subseteq T$ to its preimage $\map {\RR^{-1} } Y$ under $\RR$:

:$\forall Y \in \powerset T: \map {\RR^\gets} Y = \begin {cases} \set {s \in S: \exists t \in Y: \tuple {t, s} \in \RR^{-1} } & : \Img \RR \cap Y \ne \O \\ \O & : \Img \RR \cap Y = \O \end {cases}$

=== Mapping ===

Let $f: S \to T$ be a mapping.

Let $S$ and $T$ be sets.

Let $\powerset S$ and $\powerset T$ be their power sets.

Let $f: S \to T$ be a mapping.


The inverse image mapping of $f$ is the mapping $f^\gets: \powerset T \to \powerset S$ that sends a subset $Y \subseteq T$ to its preimage $f^{-1} \paren T$ under $f$:

:$\forall Y \in \powerset T: \map {f^\gets} Y = \begin {cases} \set {s \in S: \exists t \in Y: \map f s = t} & : \Img f \cap Y \ne \O \\ \O & : \Img f \cap Y = \O \end {cases}$",Definition:Inverse Image Mapping,"['Definitions/Induced Mappings', 'Definitions/Preimages', 'Definitions/Inverse Image Mappings']"
Definition:Inverse Image,Inverse Image,"Let $S$ and $T$ be sets.

Let $\powerset S$ and $\powerset T$ be their power sets.

Let $f: S \to T$ be a mapping.


=== Definition 1 ===
Let $S$ and $T$ be sets.

Let $\powerset S$ and $\powerset T$ be their power sets.

Let $f: S \to T$ be a mapping.


The inverse image mapping of $f$ is the mapping $f^\gets: \powerset T \to \powerset S$ that sends a subset $Y \subseteq T$ to its preimage $f^{-1} \paren T$ under $f$:

:$\forall Y \in \powerset T: \map {f^\gets} Y = \begin {cases} \set {s \in S: \exists t \in Y: \map f s = t} & : \Img f \cap Y \ne \O \\ \O & : \Img f \cap Y = \O \end {cases}$

=== Definition 2 ===
Let $S$ and $T$ be sets.

Let $\powerset S$ and $\powerset T$ be their power sets.

Let $f: S \to T$ be a mapping.


The inverse image mapping of $f$ is the direct image mapping of the inverse $f^{-1}$ of $f$:
:$f^\gets = \paren {f^{-1} }^\to: \powerset T \to \powerset S$:

That is:
:$\forall Y \in \powerset T: \map {f^\gets} Y = \set {s \in S: \exists t \in Y: \map f s = t}$",Definition:Inverse Image Mapping/Mapping,['Definitions/Inverse Image Mappings']
Definition:Inverse Image,Inverse Image,"Let $S$ and $T$ be sets.

Let $\powerset S$ and $\powerset T$ be their power sets.

Let $\RR \subseteq S \times T$ be a relation on $S \times T$.


=== Definition 1 ===
Let $S$ and $T$ be sets.

Let $\powerset S$ and $\powerset T$ be their power sets.

Let $\RR \subseteq S \times T$ be a relation on $S \times T$.


The inverse image mapping of $\RR$ is the mapping $\RR^\gets: \powerset T \to \powerset S$ that sends a subset $Y \subseteq T$ to its preimage $\map {\RR^{-1} } Y$ under $\RR$:

:$\forall Y \in \powerset T: \map {\RR^\gets} Y = \begin {cases} \set {s \in S: \exists t \in Y: \tuple {t, s} \in \RR^{-1} } & : \Img \RR \cap Y \ne \O \\ \O & : \Img \RR \cap Y = \O \end {cases}$

=== Definition 2 ===
Let $S$ and $T$ be sets.

Let $\powerset S$ and $\powerset T$ be their power sets.

Let $\RR \subseteq S \times T$ be a relation on $S \times T$.


The inverse image mapping of $\RR$ is the direct image mapping of the inverse $\RR^{-1}$ of $\RR$:
:$\RR^\gets = \paren {\RR^{-1} }^\to: \powerset T \to \powerset S$

That is:
:$\forall Y \in \powerset T: \map {\RR^\gets} Y = \set {s \in S: \exists t \in Y: \tuple {t, s} \in \RR^{-1} }$",Definition:Inverse Image Mapping/Relation,['Definitions/Inverse Image Mappings']
Definition:Inverse Image,Inverse Image,"Let $T_1 = \struct {S_1, \tau_1}$ and $T_2 = \struct {S_2, \tau_2}$ be topological spaces.

Let $f: T_1 \to T_2$ be continuous.

Let $\mathbf C$ be a category which has all small inductive limits.

Let $\FF$ be a $\mathbf C$-valued presheaf on $T_2$.


The inverse image presheaf of $\FF$ via $f$ is the presheaf $f^{-1}_{\operatorname {Psh} } \FF$ on $T_1$ with:
:$\map {\paren {f^{-1}_{\operatorname{Psh} } \FF} } U = \ds \varinjlim_{V \mathop \supseteq \map f U} \map \FF V$ where the inductive limit goes over open $V \subseteq Y$
:$\operatorname {res}^U_W$ is the induced map on the inductive limit of the subset $\set {V: V \supseteq \map f U} \subseteq \set {V : V \supseteq \map f W}$",Definition:Inverse Image Presheaf,['Definitions/Sheaf Theory']
Definition:Invertible,Invertible,"=== Definition 1 ===
Let $f: S \to T$ be a mapping.

Let $f^{-1} \subseteq T \times S$ be the inverse of $f$:
:$f^{-1} := \set {\tuple {t, s}: \map f s = t}$


Let $f^{-1}$ itself be a mapping:
:$\forall y \in T: \tuple {y, x_1} \in f^{-1} \land \tuple {y, x_2} \in f^{-1} \implies x_1 = x_2$
and
:$\forall y \in T: \exists x \in S: \tuple {y, x} \in f$


Then $f^{-1}$ is called the inverse mapping of $f$.

=== Definition 2 ===
Let $f: S \to T$ and $g: T \to S$ be mappings.

Let:
:$g \circ f = I_S$
:$f \circ g = I_T$
where:
:$g \circ f$ and $f \circ g$ denotes the composition of $f$ with $g$ in either order
:$I_S$ and $I_T$ denote the identity mappings on $S$ and $T$ respectively.

That is, $f$ and $g$ are both left inverse mappings and right inverse mappings of each other.


Then:
:$g$ is the inverse (mapping) of $f$
:$f$ is the inverse (mapping) of $g$.",Definition:Inverse Mapping,"['Definitions/Inverse Mappings', 'Definitions/Inverses of Mappings', 'Definitions/Mapping Theory', 'Definitions/Inverses']"
Definition:Invertible,Invertible,"Let $\struct {S, \circ}$ be an algebraic structure which has an identity $e_S$.

If $x \in S$ has an inverse, then $x$ is said to be invertible for $\circ$.


That is, $x$ is invertible  if and only if :

:$\exists y \in S: x \circ y = e_S = y \circ x$


=== Invertible Operation ===
Let $\left({S, \circ}\right)$ be an algebraic structure.


The operation $\circ$ is invertible  if and only if :
:$\forall a, b \in S: \exists r, s \in S: a \circ r = b = s \circ a$",Definition:Invertible Element,['Definitions/Abstract Algebra']
Definition:Invertible,Invertible,"Let $\struct {R, +, \circ}$ be a ring with unity.

Let $n \in \Z_{>0}$ be a (strictly) positive integer.

Let $\mathbf A$ be an element of the ring of square matrices $\struct {\map {\MM_R} n, +, \times}$.


Then $\mathbf A$ is invertible  if and only if :
:$\exists \mathbf B \in \struct {\map {\MM_R} n, +, \times}: \mathbf A \mathbf B = \mathbf I_n = \mathbf B \mathbf A$
where $\mathbf I_n$ denotes the unit matrix of order $n$.


Such a $\mathbf B$ is the inverse of $\mathbf A$.

It is usually denoted $\mathbf A^{-1}$.",Definition:Invertible Matrix,"['Definitions/Inverse Matrices', 'Definitions/Matrices']"
Definition:Invertible,Invertible,"=== Normed Vector Space ===
Let $\struct {X, \norm \cdot}$ be a normed vector space.

Let $T : X \to X$ be an invertible bounded linear transformation.


We say that $A$ is a bounded linear operator. 

=== Inner Product Space ===
Let $\struct {X, \innerprod \cdot \cdot}$ be an inner product  space.

Let $T : X \to X$ be an invertible bounded linear transformation.


We say that $A$ is a bounded linear operator.",Definition:Invertible Bounded Linear Operator,"['Definitions/Invertible Bounded Linear Operator', 'Definitions/Invertible Bounded Linear Operators', 'Definitions/Bounded Linear Operators', 'Definitions/Invertible Bounded Linear Operators']"
Definition:Invertible,Invertible,"=== Normed Vector Space ===
Let $\struct {V, \norm \cdot_V}$ and $\struct {U, \norm \cdot_U}$ be normed vector spaces.

Let $A : V \to U$ be a bounded linear transformation.


We say that $A$ is invertible as a bounded linear transformation  if and only if :

:$A$ has an inverse mapping that is a bounded linear transformation.


That is:

:there exists a bounded linear transformation $B : U \to V$ such that: 

::$A \circ B = I_U$
::$B \circ A = I_V$

where $I_U$ and $I_V$ are the identity mappings on $U$ and $V$ respectively.

We say that $B$ is the inverse of $A$ and write $B = A^{-1}$. 

The process of finding an $A^{-1}$ given $A$ is called inverting.

=== Inner Product Space ===
Let $\struct {V, \innerprod \cdot \cdot}$ and $\struct {U, \innerprod \cdot \cdot}$ be inner product spaces.

Let $A : V \to U$ be a bounded linear transformation.


We say that $A$ is invertible as a bounded linear transformation  if and only if :

:$A$ has an inverse mapping that is a bounded linear transformation.


That is:

:there exists a bounded linear transformation $B : U \to V$ such that: 

::$A \circ B = I_U$
::$B \circ A = I_V$

where $I_U$ and $I_V$ are the identity mappings on $U$ and $V$ respectively.

We say that $B$ is the inverse of $A$ and write $B = A^{-1}$. 

The process of finding an $A^{-1}$ given $A$ is called inverting.",Definition:Invertible Bounded Linear Transformation,"['Definitions/Bounded Linear Transformations', 'Definitions/Invertible Bounded Linear Transformations']"
Definition:Involution,Involution,"=== Natural Numbers ===
Let $\N$ denote the natural numbers.


For each $m \in \N$, recursively define $e_m: \N \to \N$ to be the mapping:
:$e_m \left({n}\right) = \begin{cases}
1 & : n = 0 \\
m \times e_m \left({x}\right) & : n = x + 1
\end{cases}$
where:
: $+$ denotes natural number addition.
: $\times$ denotes natural number multiplication.


$e_m \left({n}\right)$ is then expressed as a binary operation in the form:
:$m^n := e_m \left({n}\right)$

and is called $m$ to the power of $n$.

=== Integers ===
Let $x \in \R$ be a real number.

Let $n \in \Z$ be an integer.

The expression $x^n$ is called $x$ to the power of $n$.

$x^n$ is defined recursively as:


:$x^n = \begin {cases} 1 & : n = 0 \\ & \\ x \times x^{n - 1} & : n > 0 \\ & \\ \dfrac {x^{n + 1} } x & : n < 0 \end {cases}$

where $\dfrac {x^{n + 1} } x$ denotes division.


=== Even Power ===
Let $x \in \R$ be a real number.

Let $n \in \Z$ be an even integer.


Then $x^n$ is called an even power of $x$.


Category:Definitions/Integer Powers
Category:Definitions/Even Integers

=== Odd Power ===
Let $x \in \R$ be a real number.

Let $n \in \Z$ be an odd integer.


Then $x^n$ is called an odd power of $x$


Category:Definitions/Integer Powers
Category:Definitions/Odd Integers

=== Rational Numbers ===
Let $x \in \R$ be a real number such that $x > 0$.

Let $m \in \Z$ be an integer.

Let $y = \sqrt [m] x$ be the $m$th root of $x$.


Then we can write $y = x^{1/m}$ which means the same thing as $y = \sqrt [m] x$.


Thus we can define the power to a positive rational number:

Let $r = \dfrac p q \in \Q$ be a positive rational number where $p \in \Z_{\ge 0}, q \in \Z_{> 0}$.

Then $x^r$ is defined as:
:$x^r = x^{p/q} = \paren {\sqrt [q] x}^p = \sqrt [q] {\paren {x^p} }$


When $r = \dfrac {-p} q \in \Q: r < 0$ we define:
:$x^r = x^{-p/q} = \dfrac 1 {x^{p/q} }$
analogously for the negative integer definition.

=== Real Numbers ===
Let $x \in \R_{>0}$ be a (strictly) positive real number.

Let $r \in \R$ be a real number.


We define $x^r$ as:

:$x^r := \map \exp {r \ln x}$
where $\exp$ denotes the exponential function.


This definition is an extension of the definition for rational $r$.

This follows from Logarithms of Powers and Exponential of Natural Logarithm: it can be seen that:
:$\forall r \in \Q: \map \exp {r \ln x} = \map \exp {\map \ln {x^r} } = x^r$

 

=== Complex Numbers ===
Let $z, k \in \C$ be complex numbers.


$z$ to the power of $k$ is defined as the multifunction:

:$z^k := e^{k \ln \paren z}$

where:
:$e^z$ is the exponential function
:$\ln$ is the  natural logarithm multifunction.


=== Principal Branch ===
The principal branch of a complex number raised to a complex power is defined as:

:$z^k = e^{k \Ln z}$

where $\Ln z$ is the principal branch of the natural logarithm.


=== Positive Real Base ===
Let $t > 0$ be a real number and let $k$ be a complex number.


The principal branch of a positive real number raised to a complex power is defined as:

:$t^k = e^{k \ln t}$

where $\ln$ is the natural logarithm of a positive real number.


Category:Definitions/Complex Powers

Category:Definitions/Complex Powers

=== Multiindices ===
Let $k = \left \langle {k_j}\right \rangle_{j = 1, \ldots, n}$ be a multiindex indexed by $\set {1, \ldots, n}$.

Let $x = \tuple {x_1, \ldots, x_n} \in \R^n$ be an ordered tuple of real numbers.


Then $x^k$ is defined as:

:$\ds x^k := \prod_{j \mathop = 1}^n x_j^{k_j}$
where the powers on the   are integer powers.


Category:Definitions/Analysis

=== Power of Zero ===
Let $r \in \R$ be a real number.

(This includes the situation where $r \in \Z$ or $r \in \Q$.)

When $x=0$, $x^r$ is defined as follows:

:$0^r = \begin{cases}
1 & : r = 0 \\
0 & : r > 0 \\
\text{Undefined} & : r < 0 \\
\end{cases}$

This takes account of the awkward case $0^0$: it is ""generally accepted"" that $0^0 = 1$ as this convention agrees with certain general results which would otherwise need a special case.",Definition:Power (Algebra),"['Definitions/Powers', 'Definitions/Algebra', 'Definitions/Numbers', 'Definitions/Real Analysis', 'Definitions/Complex Analysis', 'Definitions/Involution']"
Definition:Involution,Involution,"Let $A$ be a set.

Let $f: A \to A$ be a mapping on $A$.


=== Definition 1 ===
Let $A$ be a set.

Let $f: A \to A$ be a mapping on $A$.


$f$ is an involution  if and only if :
:$\forall x \in A: \map f {\map f x} = x$

That is:
:$f \circ f = I_A$
where $I_A$ denotes the identity mapping on $A$.

=== Definition 2 ===
Let $A$ be a set.

Let $f: A \to A$ be a mapping on $A$.


$f$ is an involution  if and only if :
:$\forall x, y \in A: \map f x = y \implies \map f y = x$

=== Definition 3 ===
Let $A$ be a set.

Let $f: A \to A$ be a mapping on $A$.


$f$ is an involution  if and only if  $f$ is both a bijection and a symmetric relation.

That is,  if and only if  $f$ is a bijection such that:
:$f = f^{-1}$",Definition:Involution (Mapping),"['Definitions/Mapping Theory', 'Definitions/Involutions', 'Definitions/Involution']"
Definition:Irreducible,Irreducible,"Let $\struct {D, +, \circ}$ be an integral domain whose zero is $0_D$.

Let $\struct {U_D, \circ}$ be the group of units of $\struct {D, +, \circ}$.


Let $x \in D: x \notin U_D, x \ne 0_D$, that is, $x$ is non-zero and not a unit.


=== Definition 1 ===
Let $\struct {D, +, \circ}$ be an integral domain whose zero is $0_D$.

Let $\struct {U_D, \circ}$ be the group of units of $\struct {D, +, \circ}$.

Let $x \in D: x \notin U_D, x \ne 0_D$, that is, $x$ is non-zero and not a unit.


$x$ is defined as irreducible  if and only if  it has no non-trivial factorization in $D$.

That is,  if and only if  $x$ cannot be written as a product of two non-units.

=== Definition 2 ===
Let $\struct {D, +, \circ}$ be an integral domain whose zero is $0_D$.

Let $\struct {U_D, \circ}$ be the group of units of $\struct {D, +, \circ}$.

Let $x \in D: x \notin U_D, x \ne 0_D$, that is, $x$ is non-zero and not a unit.


$x$ is defined as irreducible  if and only if  the only divisors of $x$ are its associates and the units of $D$.

That is,  if and only if  $x$ has no proper divisors.",Definition:Irreducible Element of Ring,"['Definitions/Factorization', 'Definitions/Ring Theory', 'Definitions/Irreducible Elements of Rings']"
Definition:Irreducible,Irreducible,"An irreducible polynomial is a polynomial which is not reducible.


=== Definition 1 ===
Let $R$ be an integral domain.


An irreducible polynomial over $R$ is an irreducible element of the polynomial ring $R \sqbrk X$.

=== Definition 2: for fields ===
Let $K$ be a field.


An irreducible polynomial over $K$ is a nonconstant polynomial over $K$ that is not the product of two polynomials of smaller degree.

=== Definition 3: for fields ===
Let $K$ be a field.


An irreducible polynomial over $K$ is a polynomial over $K$ that is not the product of two nonconstant polynomials.",Definition:Irreducible Polynomial,"['Definitions/Irreducible Polynomials', 'Definitions/Factorization', 'Definitions/Polynomial Theory']"
Definition:Irreducible,Irreducible,"=== Definition 1 ===
A topological space $T = \struct {S, \tau}$ is irreducible  if and only if  there exists no cover of $T$ by two proper closed sets of $T$.

=== Definition 2 ===
A topological space $T = \struct {S, \tau}$ is irreducible  if and only if  there is no finite cover of $T$ by proper closed sets of $T$.

=== Definition 3 ===
A topological space $T = \struct {S, \tau}$ is irreducible  if and only if  every two non-empty open sets of $T$ have non-empty intersection:

:$\forall U, V \in \tau: U, V \ne \O \implies U \cap V \ne \O$

=== Definition 4 ===
A topological space $T = \struct {S, \tau}$ is irreducible  if and only if  every non-empty open set of $T$ is (everywhere) dense in $T$.

=== Definition 5 ===
A topological space $T = \struct {S, \tau}$ is irreducible  if and only if  the interior of every proper closed set of $T$ is empty.

=== Definition 6 ===
A topological space $T = \struct {S, \tau}$ is irreducible  if and only if  the closure of every non-empty open set is the whole space:
:$\forall U \in \tau: U \ne \O \implies U^- = S$

=== Definition 7 ===
A topological space $T = \struct {S, \tau}$ is irreducible  if and only if  every open set of $T$ is connected.",Definition:Irreducible Space,"['Definitions/Topology', 'Definitions/Connected Spaces', 'Definitions/Irreducible Spaces']"
Definition:Irreducible,Irreducible,"Let $T = \struct {S, \tau}$ be a topological space.

Let $A$ be a subset of $S$.


Then $A$ is irreducible (subset)  if and only if 
:$A$ is non-empty and closed and for all closed subsets $B, C$ of $S$: $A = B \cup C \implies A = B$ or $A = C$",Definition:Irreducible Subset (Topology),['Definitions/Topology']
Definition:Irreducible,Irreducible,"Let $\rho: G \to \GL V$ be a linear representation.

Then $\rho$ is irreducible  if and only if  it is not reducible.


That is,  if and only if  there exists no non-trivial proper vector subspace $W$ of $V$ such that:
: $\forall g \in G: \map {\map \rho g} W \subseteq W$


Category:Definitions/Representation Theory",Definition:Irreducible (Representation Theory)/Linear Representation,['Definitions/Representation Theory']
Definition:Irreducible,Irreducible,"=== Linear Representation ===
Let $\rho: G \to \GL V$ be a linear representation.

Then $\rho$ is irreducible  if and only if  it is not reducible.


That is,  if and only if  there exists no non-trivial proper vector subspace $W$ of $V$ such that:
: $\forall g \in G: \map {\map \rho g} W \subseteq W$


Category:Definitions/Representation Theory

=== G-Module ===
A $G$-module is irreducible  if and only if  the corresponding linear representation is irreducible.


That is, any proper $G$-submodule is trivial.",Definition:Irreducible (Representation Theory),['Definitions/Representation Theory']
Definition:Irreducible,Irreducible,"Let $\left({S, \wedge, \preceq}\right)$ be a meet semilattice.

Let $g \in S$.


Then $g$ is meet irreducible  if and only if :
:$\forall x, y \in S: g = x \wedge y \implies g = x$ or $g = y$",Definition:Meet Irreducible,['Definitions/Order Theory']
Definition:Irreducible,Irreducible,"Let $\struct {S, \preceq}$ be an ordered set.

Let $p \in S$.

An element $p$ is completely irreducible  if and only if 
:$p^\succeq \setminus \set p$ admits a minimum element
where $p^\succeq$ denotes the upper closure of $p$.",Definition:Completely Irreducible,['Definitions/Order Theory']
Definition:Irreducible,Irreducible,"Let $\struct {X, \OO_X}$ be a scheme.


Then $\struct {X, \OO_X}$ is irreducible  if and only if  $X$ is an irreducible space.",Definition:Irreducible Scheme,['Definitions/Schemes']
Definition:Isometry,Isometry,"=== Definition 1 ===
Let $M_1 = \tuple {A_1, d_1}$ and $M_2 = \tuple {A_2, d_2}$ be metric spaces or pseudometric spaces.


Let $\phi: A_1 \to A_2$ be a bijection such that:
:$\forall a, b \in A_1: \map {d_1} {a, b} = \map {d_2} {\map \phi a, \map \phi b}$


Then $\phi$ is called an isometry.

That is, an isometry is a distance-preserving bijection.

=== Definition 2 ===
Let $M_1 = \struct {A_1, d_1}$ and $M_2 = \struct {A_2, d_2}$ be metric spaces or pseudometric spaces.


:$M_1$ and $M_2$ are isometric  if and only if  there exist inverse mappings $\phi: A_1 \to A_2$ and $\phi^{-1}: A_2 \to A_1$ such that:

::$\forall a, b \in A_1: \map {d_1} {a, b} = \map {d_2} {\map \phi a, \map \phi b}$
:and:
::$\forall u, v \in A_2: \map {d_2} {u, v} = \map {d_1} {\map {\phi^{-1} } u, \map {\phi^{-1} } v}$

Such metric spaces $M_1$ and $M_2$ are defined as being isometric.


=== Isometry Into ===
Let $M_1 = \struct {A_1, d_1}$ and $M_2 = \struct {A_2, d_2}$ be metric spaces or pseudometric spaces.


Let $\phi: A_1 \to A_2$ be an injection such that:
:$\forall a, b \in A_1: \map {d_1} {a, b} = \map {d_2} {\map \phi a, \map \phi b}$


Then $\phi$ is called an isometry (from $M_1$) into $M_2$.


That is, an isometry (from $M_1$) into $M_2$ is an isometry which is not actually a surjection, but satisfies the other conditions for being an isometry.",Definition:Isometry (Metric Spaces),"['Definitions/Isometries (Metric Spaces)', 'Definitions/Metric Spaces', 'Definitions/Pseudometric Spaces', 'Definitions/Isometries']"
Definition:Isometry,Isometry,"Let $\EE$ be a real Euclidean space.


Let $\phi: \EE \to \EE$ be a bijection such that:
:$\forall P, Q \in \EE: PQ = P'Q'$
where:
:$P$ and $Q$ are arbitrary points in $\EE$
:$P'$ and $Q'$ are the images of $P$ and $Q$ respectively
:$PQ$ and $P'Q'$ denote the lengths of the straight line segments $PQ$ and $P'Q'$ respectively.


Then $\phi$ is an isometry.


That is, an isometry is a bijection which preserves distance between points.


=== Context ===
An isometry is defined usually for either:
:$n = 2$, representing the plane
or:
:$n = 3$, representing ordinary space.",Definition:Isometry (Euclidean Geometry),"['Definitions/Isometries (Euclidean Geometry)', 'Definitions/Euclidean Geometry', 'Definitions/Isometries']"
Definition:Isometry,Isometry,"Let $V$ and $W$ be inner product spaces.

Let their inner products be $\innerprod \cdot \cdot_V$ and $\innerprod \cdot \cdot_W$ respectively.

Let the mapping $F : V \to W$ be a vector space isomorphism that preserves inner products:

:$\forall v_1, v_2 \in V : \innerprod {v_1} {v_2}_V = \innerprod {\map F {v_1}} {\map F {v_2}}_W$


Then $F$ is called a (linear) isometry.


=== Hilbert Spaces ===
Let $H$ and $K$ be Hilbert spaces.

Let their inner products be $\innerprod \cdot \cdot_H$ and $\innerprod \cdot \cdot_K$ respectively.


A linear map $U: H \to K$ is called an isometry  if and only if :

:$\forall g,h \in H: \innerprod g h_H = \innerprod {U g} {U h}_K$


 ",Definition:Isometry (Inner Product Spaces),"['Definitions/Isometries (Inner Product Spaces)', 'Definitions/Inner Product Spaces', 'Definitions/Isometries']"
Definition:Isometry,Isometry,"Let $\struct {M, g}$ and $\struct {\tilde M, \tilde g}$ be Riemannian manifolds with Riemannian metrics $g$ and $\tilde g$ respectively.

Let the mapping $\phi : M \to \tilde M$ be a diffeomorphism such that:

:$\phi^* \tilde g = g$


Then $\phi$ is called an isometry from $\struct {M, g}$ to $\struct {\tilde M, \tilde g}$.",Definition:Isometry (Riemannian Manifolds),"['Definitions/Isometries (Riemannian Manifolds)', 'Definitions/Riemannian Manifolds', 'Definitions/Isometries']"
Definition:Isomorphism,Isomorphism,"An isomorphism is a homomorphism which is a bijection.

That is, it is a mapping which is both a monomorphism and an epimorphism.


An algebraic structure $\struct {S, \circ}$ is isomorphic to another algebraic structure $\struct {T, *}$  if and only if  there exists an isomorphism from $\struct {S, \circ}$ to $\struct {T, *}$, and we can write $S \cong T$ (although notation may vary).


=== Semigroup Isomorphism ===
Let $\struct {S, \circ}$ and $\struct {T, *}$ be semigroups.

Let $\phi: S \to T$ be a (semigroup) homomorphism.


Then $\phi$ is a semigroup isomorphism  if and only if  $\phi$ is a bijection.

That is, $\phi$ is a semigroup isomorphism  if and only if  $\phi$ is both a monomorphism and an epimorphism.


If $S$ is isomorphic to $T$, then the notation $S \cong T$ can be used (although notation varies).

=== Monoid Isomorphism ===
Let $\struct {S, \circ}$ and $\struct {T, *}$ be monoids.

Let $\phi: S \to T$ be a (monoid) homomorphism.


Then $\phi$ is a monoid isomorphism  if and only if  $\phi$ is a bijection.


That is, $\phi$ is a monoid isomorphism  if and only if  $\phi$ is both a monomorphism and an epimorphism.


If $S$ is isomorphic to $T$, then the notation $S \cong T$ can be used (although notation varies).

=== Group Isomorphism ===
Let $\struct {G, \circ}$ and $\struct {H, *}$ be groups.

Let $\phi: G \to H$ be a (group) homomorphism.


Then $\phi$ is a group isomorphism  if and only if  $\phi$ is a bijection.


That is, $\phi$ is a group isomorphism  if and only if  $\phi$ is both a monomorphism and an epimorphism.


If $G$ is isomorphic to $H$, then the notation $G \cong H$ can be used (although notation varies).

=== Ring Isomorphism ===
Let $\struct {R, +, \circ}$ and $\struct {S, \oplus, *}$ be rings.

Let $\phi: R \to S$ be a (ring) homomorphism.


Then $\phi$ is a ring isomorphism  if and only if  $\phi$ is a bijection.

That is, $\phi$ is a ring isomorphism  if and only if  $\phi$ is both a monomorphism and an epimorphism.

=== $F$-Isomorphism ===
Let $R, S$ be rings with unity.

Let $F$ be a subfield of both $R$ and $S$.


Let $\phi: R \to S$ be an $F$-homomorphism such that $\phi$ is bijective.


Then $\phi$ is an $F$-isomorphism.


The relationship between $R$ and $S$ is denoted $R \cong_F S$.

=== Field Isomorphism ===
Let $\struct {F, +, \circ}$ and $\struct {K, \oplus, *}$ be fields.

Let $\phi: F \to K$ be a (field) homomorphism.


Then $\phi$ is a field isomorphism  if and only if  $\phi$ is a bijection.

That is, $\phi$ is a field isomorphism  if and only if  $\phi$ is both a monomorphism and an epimorphism.

=== $R$-Algebraic Structure Isomorphism ===
Let $\struct {S, \ast_1, \ast_2, \ldots, \ast_n, \circ}_R$ and $\struct {T, \odot_1, \odot_2, \ldots, \odot_n, \otimes}_R$ be $R$-algebraic structures.

Let $\phi: S \to T$ be an $R$-algebraic structure homomorphism.


Then $\phi$ is an $R$-algebraic structure isomorphism  if and only if  $\phi$ is a bijection.


=== Module Isomorphism ===
Let $R$ be a ring.

Let $\struct {G, +_G, \circ}_R$ and $\struct {H, +_H, \circ}_R$ be $R$-modules.

Let $\phi: G \to H$ be a module homomorphism.


Then $\phi$ is a module isomorphism  if and only if  $\phi$ is a bijection.

=== Vector Space Isomorphism ===
Let $\struct {V, +, \circ}$ and $\struct {W, +', \circ'}$ be $K$-vector spaces.

Then $\phi: V \to W$ is a vector space isomorphism  if and only if :

:$(1): \quad \phi$ is a bijection
:$(2): \quad \forall \mathbf x, \mathbf y \in V: \map \phi {\mathbf x + \mathbf y} = \map \phi {\mathbf x} +' \map \phi {\mathbf y}$
:$(3): \quad \forall \mathbf x \in V: \forall \lambda \in K: \map \phi {\lambda \mathbf x} = \lambda \map \phi {\mathbf x}$


Category:Definitions/Isomorphisms (Abstract Algebra)
Category:Definitions/R-Algebraic Structure Isomorphisms
Category:Definitions/Linear Algebra

=== Ordered Structure Isomorphism ===
An ordered structure isomorphism from an ordered structure $\struct {S, \circ, \preceq}$ to another $\struct {T, *, \preccurlyeq}$ is a mapping $\phi: S \to T$ that is both:

:$(1): \quad$ An isomorphism, that is a bijective homomorphism, from the structure $\struct {S, \circ}$ to the structure $\struct {T, *}$
:$(2): \quad$ An order isomorphism from the ordered set $\struct {S, \preceq}$ to the ordered set $\struct {T, \preccurlyeq}$.


=== Ordered Semigroup Isomorphism ===
Let $\struct {S, \circ, \preceq}$ and $\struct {T, *, \preccurlyeq}$ be ordered semigroups.


An ordered semigroup isomorphism from $\struct {S, \circ, \preceq}$ to $\struct {T, *, \preccurlyeq}$ is a mapping $\phi: S \to T$ that is both:

:$(1): \quad$ A semigroup isomorphism from the semigroup $\struct {S, \circ}$ to the semigroup $\struct {T, *}$

:$(2): \quad$ An order isomorphism from the ordered set $\struct {S, \preceq}$ to the ordered set $\struct {T, \preccurlyeq}$.

=== Ordered Group Isomorphism ===
Let $\left({S, \circ, \preceq}\right)$ and $\left({T, *, \preccurlyeq}\right)$ be ordered groups.


An ordered group isomorphism from $\left({S, \circ, \preceq}\right)$ to $\left({T, *, \preccurlyeq}\right)$ is a mapping $\phi: S \to T$ that is both:

:$(1): \quad$ A group isomorphism from the group $\left({S, \circ}\right)$ to the group $\left({T, *}\right)$

:$(2): \quad$ An order isomorphism from the ordered set $\left({S, \preceq}\right)$ to the ordered set $\left({T, \preccurlyeq}\right)$.

=== Ordered Ring Isomorphism ===
Let $\struct {S, +, \circ, \preceq}$ and $\struct {T, \oplus, *, \preccurlyeq}$ be ordered rings.


An ordered ring isomorphism from $\struct {S, +, \circ, \preceq}$ to $\struct {T, \oplus, *, \preccurlyeq}$ is a mapping $\phi: S \to T$ that is both:

:$(1): \quad$ An ordered group isomorphism from the ordered group $\struct {S, +, \preceq}$ to the ordered group $\struct {T, \oplus, \preccurlyeq}$

:$(2): \quad$ A semigroup isomorphism from the semigroup $\struct {S, \circ}$ to the semigroup $\struct {T, *}$.

=== Ordered Field Isomorphism ===
Let $\struct {S, +, \circ, \preceq}$ and $\struct {T, \oplus, *, \preccurlyeq}$ be ordered fields.


An ordered field isomorphism from $\struct {S, +, \circ, \preceq}$ to $\struct {T, \oplus, *, \preccurlyeq}$ is a mapping $\phi: S \to T$ that is both:

:$(1): \quad$ An ordered group isomorphism from the ordered group $\struct {S, +, \preceq}$ to the ordered group $\struct {T, \oplus, \preccurlyeq}$

:$(2): \quad$ A group isomorphism from the group $\struct {S_{\ne 0}, \circ}$ to the semigroup $\struct {T_{\ne 0}, *}$

where $S_{\ne 0}$ and $T_{\ne 0}$ denote the sets $S$ and $T$ without the zeros of $S$ and $T$ respectively.",Definition:Isomorphism (Abstract Algebra),"['Definitions/Isomorphisms (Abstract Algebra)', 'Definitions/Bijections', 'Definitions/Epimorphisms (Abstract Algebra)', 'Definitions/Monomorphisms (Abstract Algebra)', 'Definitions/Homomorphisms (Abstract Algebra)', 'Definitions/Abstract Algebra', 'Definitions/Isomorphisms']"
Definition:Isomorphism,Isomorphism,"Let $\struct {G, \circ}$ and $\struct {H, *}$ be groups.

Let $\phi: G \to H$ be a (group) homomorphism.


Then $\phi$ is a group isomorphism  if and only if  $\phi$ is a bijection.


That is, $\phi$ is a group isomorphism  if and only if  $\phi$ is both a monomorphism and an epimorphism.


If $G$ is isomorphic to $H$, then the notation $G \cong H$ can be used (although notation varies).",Definition:Isomorphism (Abstract Algebra)/Group Isomorphism,['Definitions/Group Homomorphisms']
Definition:Isomorphism,Isomorphism,"Let $\struct {R, +, \circ}$ and $\struct {S, \oplus, *}$ be rings.

Let $\phi: R \to S$ be a (ring) homomorphism.


Then $\phi$ is a ring isomorphism  if and only if  $\phi$ is a bijection.

That is, $\phi$ is a ring isomorphism  if and only if  $\phi$ is both a monomorphism and an epimorphism.",Definition:Isomorphism (Abstract Algebra)/Ring Isomorphism,"['Definitions/Isomorphisms (Abstract Algebra)', 'Definitions/Ring Homomorphisms']"
Definition:Isomorphism,Isomorphism,"Let $\struct {S, \ast_1, \ast_2, \ldots, \ast_n, \circ}_R$ and $\struct {T, \odot_1, \odot_2, \ldots, \odot_n, \otimes}_R$ be $R$-algebraic structures.

Let $\phi: S \to T$ be an $R$-algebraic structure homomorphism.


Then $\phi$ is an $R$-algebraic structure isomorphism  if and only if  $\phi$ is a bijection.


=== Module Isomorphism ===
Let $R$ be a ring.

Let $\struct {G, +_G, \circ}_R$ and $\struct {H, +_H, \circ}_R$ be $R$-modules.

Let $\phi: G \to H$ be a module homomorphism.


Then $\phi$ is a module isomorphism  if and only if  $\phi$ is a bijection.

=== Vector Space Isomorphism ===
Let $\struct {V, +, \circ}$ and $\struct {W, +', \circ'}$ be $K$-vector spaces.

Then $\phi: V \to W$ is a vector space isomorphism  if and only if :

:$(1): \quad \phi$ is a bijection
:$(2): \quad \forall \mathbf x, \mathbf y \in V: \map \phi {\mathbf x + \mathbf y} = \map \phi {\mathbf x} +' \map \phi {\mathbf y}$
:$(3): \quad \forall \mathbf x \in V: \forall \lambda \in K: \map \phi {\lambda \mathbf x} = \lambda \map \phi {\mathbf x}$


Category:Definitions/Isomorphisms (Abstract Algebra)
Category:Definitions/R-Algebraic Structure Isomorphisms
Category:Definitions/Linear Algebra",Definition:Isomorphism (Abstract Algebra)/R-Algebraic Structure Isomorphism,"['Definitions/R-Algebraic Structure Isomorphisms', 'Definitions/R-Algebraic Structure Homomorphisms', 'Definitions/Isomorphisms (Abstract Algebra)', 'Definitions/Linear Algebra']"
Definition:Isomorphism,Isomorphism,"Let $\struct {S_1, \RR_1}$ and $\struct {S_2, \RR_2}$ be relational structures.

Let there exist a bijection $\phi: S_1 \to S_2$ such that:
:$(1): \quad \forall \tuple {s_1, t_1} \in \RR_1: \tuple {\map \phi {s_1}, \map \phi {t_1} } \in \RR_2$
:$(2): \quad \forall \tuple {s_2, t_2} \in \RR_2: \tuple {\map {\phi^{-1} } {s_2}, \map {\phi^{-1} } {t_2} } \in \RR_1$


Then $\struct {S_1, \RR_1}$ and $\struct {S_2, \RR_2}$ are isomorphic, and this is denoted $S_1 \cong S_2$.


The function $\phi$ is called a relation isomorphism, or just an isomorphism, from $\struct {S_1, \RR_1}$ to $\struct {S_2, \RR_2}$.",Definition:Relation Isomorphism,"['Definitions/Relational Structures', 'Definitions/Mapping Theory']"
Definition:Isomorphism,Isomorphism,"Let $\struct {S, \preceq_1}$ and $\struct {T, \preceq_2}$ be ordered sets.

=== Definition 1 ===
Let $\struct {S, \preceq_1}$ and $\struct {T, \preceq_2}$ be ordered sets.


Let $\phi: S \to T$ be a bijection such that:

:$\phi: S \to T$ is order-preserving
:$\phi^{-1}: T \to S$ is order-preserving.

Then $\phi$ is an order isomorphism.


That is, $\phi$ is an order isomorphism  if and only if :

:$\phi$ is bijective
:$\forall x, y \in S: x \preceq_1 y \implies \map \phi x \preceq_2 \map \phi y$
:$\forall p, q \in T: p \preceq_2 q \implies \map {\phi^{-1} } p \preceq_1 \map {\phi^{-1} } q$

So an order isomorphism can be described as a bijection that preserves ordering in both directions.

=== Definition 2 ===
Let $\struct {S, \preceq_1}$ and $\struct {T, \preceq_2}$ be ordered sets.


Let $\phi: S \to T$ be a surjective order embedding.

Then $\phi$ is an order isomorphism.


That is, $\phi$ is an order isomorphism  if and only if :

:$(1): \quad \phi$ is surjective
:$(2): \quad \forall x, y \in S: x \preceq_1 y \iff \map \phi x \preceq_2 \map \phi y$

=== Definition 3 ===
Let $\struct {S, \preceq_1}$ and $\struct {T, \preceq_2}$ be ordered sets.


Let $\phi: S \to T$ be a bijection such that:
:$\forall x, y \in S: x \preceq_1 y \iff \map \phi x \preceq_2 \map \phi y$

Then $\phi$ is an order isomorphism.

=== Well-Orderings ===

When $\preceq_1$ and $\preceq_2$ are well-orderings, the condition on the order preservation can be relaxed:

Let $\struct {S, \preceq_1}$ and $\struct {T, \preceq_2}$ be well-ordered sets.

Let $\phi: S \to T$ be a bijection such that $\phi: S \to T$ is order-preserving:
:$\forall x, y \in S: x \preceq_1 y \implies \map \phi x \preceq_2 \map \phi y$


Then $\phi$ is an order isomorphism.


Two well-ordered sets $\struct {S, \preceq_1}$ and $\struct {T, \preceq_2}$ are (order) isomorphic  if and only if  there exists such an order isomorphism between them.

Thus $\struct {S, \preceq_1}$ is described as (order) isomorphic to (or with) $\struct {T, \preceq_2}$, and vice versa.

This may be written $\struct {S, \preceq_1} \cong \struct {T, \preceq_2}$.


Where no confusion is possible, it may be abbreviated to $S \cong T$.


=== Class-Theoretical Definition ===
 
Let $\struct {A, \preccurlyeq_1}$ and $\struct {B, \preccurlyeq_2}$ be well-ordered classes.

Let $\phi: A \to B$ be a bijection such that $\phi: A \to B$ is order-preserving:
:$\forall x, y \in S: x \preccurlyeq_1 y \implies \map \phi x \preccurlyeq_2 \map \phi y$


Then $\phi$ is an order isomorphism.",Definition:Order Isomorphism,"['Definitions/Order Theory', 'Definitions/Order Isomorphisms']"
Definition:Isomorphism,Isomorphism,"An ordered structure isomorphism from an ordered structure $\struct {S, \circ, \preceq}$ to another $\struct {T, *, \preccurlyeq}$ is a mapping $\phi: S \to T$ that is both:

:$(1): \quad$ An isomorphism, that is a bijective homomorphism, from the structure $\struct {S, \circ}$ to the structure $\struct {T, *}$
:$(2): \quad$ An order isomorphism from the ordered set $\struct {S, \preceq}$ to the ordered set $\struct {T, \preccurlyeq}$.


=== Ordered Semigroup Isomorphism ===
Let $\struct {S, \circ, \preceq}$ and $\struct {T, *, \preccurlyeq}$ be ordered semigroups.


An ordered semigroup isomorphism from $\struct {S, \circ, \preceq}$ to $\struct {T, *, \preccurlyeq}$ is a mapping $\phi: S \to T$ that is both:

:$(1): \quad$ A semigroup isomorphism from the semigroup $\struct {S, \circ}$ to the semigroup $\struct {T, *}$

:$(2): \quad$ An order isomorphism from the ordered set $\struct {S, \preceq}$ to the ordered set $\struct {T, \preccurlyeq}$.

=== Ordered Group Isomorphism ===
Let $\left({S, \circ, \preceq}\right)$ and $\left({T, *, \preccurlyeq}\right)$ be ordered groups.


An ordered group isomorphism from $\left({S, \circ, \preceq}\right)$ to $\left({T, *, \preccurlyeq}\right)$ is a mapping $\phi: S \to T$ that is both:

:$(1): \quad$ A group isomorphism from the group $\left({S, \circ}\right)$ to the group $\left({T, *}\right)$

:$(2): \quad$ An order isomorphism from the ordered set $\left({S, \preceq}\right)$ to the ordered set $\left({T, \preccurlyeq}\right)$.

=== Ordered Ring Isomorphism ===
Let $\struct {S, +, \circ, \preceq}$ and $\struct {T, \oplus, *, \preccurlyeq}$ be ordered rings.


An ordered ring isomorphism from $\struct {S, +, \circ, \preceq}$ to $\struct {T, \oplus, *, \preccurlyeq}$ is a mapping $\phi: S \to T$ that is both:

:$(1): \quad$ An ordered group isomorphism from the ordered group $\struct {S, +, \preceq}$ to the ordered group $\struct {T, \oplus, \preccurlyeq}$

:$(2): \quad$ A semigroup isomorphism from the semigroup $\struct {S, \circ}$ to the semigroup $\struct {T, *}$.

=== Ordered Field Isomorphism ===
Let $\struct {S, +, \circ, \preceq}$ and $\struct {T, \oplus, *, \preccurlyeq}$ be ordered fields.


An ordered field isomorphism from $\struct {S, +, \circ, \preceq}$ to $\struct {T, \oplus, *, \preccurlyeq}$ is a mapping $\phi: S \to T$ that is both:

:$(1): \quad$ An ordered group isomorphism from the ordered group $\struct {S, +, \preceq}$ to the ordered group $\struct {T, \oplus, \preccurlyeq}$

:$(2): \quad$ A group isomorphism from the group $\struct {S_{\ne 0}, \circ}$ to the semigroup $\struct {T_{\ne 0}, *}$

where $S_{\ne 0}$ and $T_{\ne 0}$ denote the sets $S$ and $T$ without the zeros of $S$ and $T$ respectively.",Definition:Isomorphism (Abstract Algebra)/Ordered Structure Isomorphism,"['Definitions/Order Isomorphisms', 'Definitions/Ordered Structures']"
Definition:Isomorphism,Isomorphism,"Let $\mathbf C$ be a category, and let $X, Y$ be objects of $\mathbf C$.


A morphism $f: X \to Y$ is an isomorphism  if and only if  there exists a morphism $g: Y \to X$ such that:

:$g \circ f = I_X$
:$f \circ g = I_Y$

where $I_X$ denotes the identity morphism on $X$.

It can be seen that this is equivalent to $g$ being both a retraction and a section of $f$.


=== Inverse Morphism ===
Let $\mathbf C$ be a metacategory.

Let $f: X \to Y$ be a morphism of $\mathbf C$.


A morphism $g: Y \to X$ is said to be an inverse (morphism) for $f$  if and only if :

:$g \circ f = I_X$
:$f \circ g = I_Y$

where $I_X$ denotes the identity morphism on $X$.


It follows that $f$ is an isomorphism  if and only if  it has an inverse morphism.",Definition:Isomorphism (Category Theory),['Definitions/Category Theory']
Definition:Isomorphism,Isomorphism,"Let $\mathbf C$ and $\mathbf D$ be metacategories.

Let $F: \mathbf C \to \mathbf D$ be a functor.


Then $F$ is an isomorphism (of categories)  if and only if  there exists a functor $G: \mathbf C \to \mathbf D$ such that:

:$G F: \mathbf C \to \mathbf C$ is the identity functor $I_{\mathbf C}$
:$F G: \mathbf D \to \mathbf D$ is the identity functor $I_{\mathbf D}$


=== Isomorphic Categories ===
Let $\mathbf C$ and $\mathbf D$ be metacategories.

Let $F: \mathbf C \to \mathbf D$ be an isomorphism of categories.


Then $\mathbf C$ and $\mathbf D$ are said to be isomorphic, and we write $\mathbf C \cong \mathbf D$.",Definition:Isomorphism of Categories,"['Definitions/Category Theory', 'Definitions/Isomorphisms']"
Definition:Isomorphism,Isomorphism,"Let $G = \struct {\map V G, \map E G}$ and $H = \struct {\map V H, \map E H}$ be graphs.

Let there exist a bijection $F: \map V G \to \map V H$ such that:
:for each edge $\set {u, v} \in \map E G$, there exists an edge $\set {\map F u, \map F v} \in \map E H$.


That is, that:
:$F: \map V G \to \map V H$ is a homomorphism, and

:$F^{-1}: \map V H \to \map V G$ is a homomorphism.


Then $G$ and $H$ are isomorphic, and this is denoted $G \cong H$.

The function $F$ is called an isomorphism from $G$ to $H$.",Definition:Isomorphism (Graph Theory),['Definitions/Graph Theory']
Definition:Isomorphism,Isomorphism,"Let $H, K$ be Hilbert spaces.

Denote by $\innerprod \cdot \cdot_H$ and $\innerprod \cdot \cdot_K$ their respective inner products.

An isomorphism between $H$ and $K$ is a map $U: H \to K$, such that:

:$(1): \quad U$ is a linear map
:$(2): \quad U$ is surjective
:$(3): \quad \forall g, h \in H: \innerprod g h_H = \innerprod {U g} {U h}_K$

These three requirements may be summarized by stating that $U$ be a surjective isometry.

Furthermore, Surjection that Preserves Inner Product is Linear shows that requirement $(1)$ is superfluous.


If such an isomorphism $U$ exists, $H$ and $K$ are said to be isomorphic.


As the name isomorphism suggests, Hilbert Space Isomorphism is Equivalence Relation.",Definition:Isomorphism (Hilbert Spaces),['Definitions/Hilbert Spaces']
Definition:Isomorphism,Isomorphism,"Let $T_\alpha = \struct {S_\alpha, \tau_\alpha}$ and $T_\beta = \struct {S_\beta, \tau_\beta}$ be topological spaces.

Let $f: T_\alpha \to T_\beta$ be a bijection.


=== Definition 1 ===
Let $T_\alpha = \struct {S_\alpha, \tau_\alpha}$ and $T_\beta = \struct {S_\beta, \tau_\beta}$ be topological spaces.

Let $f: T_\alpha \to T_\beta$ be a bijection.


$f$ is a homeomorphism  if and only if  both $f$ and $f^{-1}$ are continuous.

=== Definition 2 ===
Let $T_\alpha = \struct {S_\alpha, \tau_\alpha}$ and $T_\beta = \struct {S_\beta, \tau_\beta}$ be topological spaces.

Let $f: T_\alpha \to T_\beta$ be a bijection.


$f$ is a homeomorphism  if and only if :
:$\forall U \subseteq S_\alpha: U \in \tau_\alpha \iff f \sqbrk U \in \tau_\beta$


That is, $f$ is a homeomorphism  if and only if :
:for all subsets $U$ of $S_\alpha$, $U$ is open in $T_\alpha$  if and only if  $f \sqbrk U$ is open in $T_\beta$.

=== Definition 3 ===
Let $T_\alpha = \struct {S_\alpha, \tau_\alpha}$ and $T_\beta = \struct {S_\beta, \tau_\beta}$ be topological spaces.

Let $f: T_\alpha \to T_\beta$ be a bijection.


$f$ is a homeomorphism  if and only if  $f$ is both an open mapping and a continuous mapping.

=== Definition 4 ===
Let $T_\alpha = \struct {S_\alpha, \tau_\alpha}$ and $T_\beta = \struct {S_\beta, \tau_\beta}$ be topological spaces.

Let $f: T_\alpha \to T_\beta$ be a bijection.


$f$ is a homeomorphism  if and only if  $f$ is both a closed mapping and a continuous mapping.",Definition:Homeomorphism/Topological Spaces,"['Definitions/Homeomorphisms (Topological Spaces)', 'Definitions/Homeomorphisms', 'Definitions/Bijections', 'Definitions/Continuous Mappings', 'Definitions/Topology']"
Definition:Isotropy,Isotropy,"Isotropy is a property of a physical system such that measurements of that system are independent of the direction in which those measurements are taken.

 ",Definition:Isotropy (Physics),['Definitions/Physics']
Definition:Isotropy,Isotropy,"Let $\struct {M, g}$ be a Riemannian manifold.

Let $\map {\text {Iso}} {M, g}$ be the set of all isometries from $M$ to itself.

Let $\map {\text {Iso}_p} {M, g}$ be the isotropy subgroup at $p \in M$.

That is, let $\map {\text {Iso}_p} {M, g}$ be the subgroup of $\map {\text {Iso}} {M, g}$ consisting of isometries that fix $p \in M$.

Let $T_p M$ be the tangent space of $M$ at $p \in M$.

For each $\phi \in \map {\text {Iso}_p} {M, g}$ let $\rd \phi_p$ be a linear mapping such that:

:$\rd \phi_p : T_p M \to T_p M$

Let $\GL {T_p M}$ be the general linear group over $T_p M$.

Let $I_p : \map {\text {Iso}_p} {M, g} \to \GL {T_p M}$ be a mapping such that $\map {I_p} \phi = \rd \phi_p$.


Then the mapping $I_p$ is a representation of $\map {\text {Iso}_p} {M, g}$ and is called the isotropy representation.",Definition:Isotropy Representation,['Definitions/Riemannian Geometry']
Definition:Join,Join,"Let $\struct {G, \circ}$ be a group.

Let $A$ and $B$ be subgroups of $G$.


The join of $A$ and $B$ is written and defined as:
:$A \vee B := \gen {A \cup B}$
where $\gen {A \cup B}$ is the subgroup generated by $A \cup B$.


By the definition of subgroup generator, this can alternatively be written:

:$\ds A \vee B := \bigcap \set {T: T \text { is a subgroup of } G: A \cup B \subseteq T}$


=== General Definition ===
Let $\struct {G, \circ}$ be a group.

Let $H_1, H_2, \ldots, H_n$ be subgroups of $G$.


Then the join of $H_1, H_2, \ldots, H_n$ is defined as:
:$\ds \bigvee_{k \mathop = 1}^n H_k := \gen {\bigcup_{k \mathop = 1}^n H_k}$
or:
:$\ds \bigvee_{k \mathop = 1}^n H_k := \bigcap \set {T: T \text { is a subgroup of } G: \bigcup_{k \mathop = 1}^n H_k \subseteq T}$",Definition:Join of Subgroups,['Definitions/Group Theory']
Definition:Join,Join,"Consider the Boolean algebra $\struct {S, \vee, \wedge, \neg}$


The operation $\vee$ is called join.",Definition:Boolean Algebra/Join,['Definitions/Boolean Algebras']
Definition:Join,Join,"Let $\struct {S, \preceq}$ be an ordered set.

Let $a, b \in S$.

Let their supremum $\sup \set {a, b}$ exist in $S$.


Then the join of $a$ and $b$ is defined as:

:$a \vee b = \sup \set {a, b}$


Expanding the definition of supremum, one sees that $c = a \vee b$  if and only if :

:$(1): \quad a \preceq c$ and $b \preceq c$
:$(2): \quad \forall s \in S: a \preceq s$ and $b \preceq s \implies c \preceq s$",Definition:Join (Order Theory),"['Definitions/Order Theory', 'Definitions/Lattice Theory']"
Definition:Join,Join,"Let $G = \struct {V, E}$ be a graph.

Let $u$ and $v$ be vertices of $G$.

Let $e = u v$ be an edge of $G$.


Then $e$ joins the vertices $u$ and $v$.",Definition:Graph (Graph Theory)/Edge/Join,"['Definitions/Edges of Graphs', 'Definitions/Vertices of Graphs']"
Definition:Kernel,Kernel,"=== Kernel of Magma Homomorphism ===
Let $\struct {S, \circ}$ be a magma.

Let $\struct {T, *}$ be an algebraic structure with an identity element $e$.

Let $\phi: \struct {S, \circ} \to \struct {T, *}$ be a homomorphism.


The kernel of $\phi$ is the subset of the domain of $\phi$ defined as:
:$\map \ker \phi = \set {x \in S: \map \phi x = e}$


That is, $\map \ker \phi$ is the subset of $S$ that maps to the identity of $T$.

=== Kernel of Group Homomorphism ===
Let $\struct {G, \circ}$ and $\struct {H, *}$ be groups.

Let $\phi: \struct {G, \circ} \to \struct {H, *}$ be a group homomorphism.


The kernel of $\phi$ is the subset of the domain of $\phi$ defined as:
:$\map \ker \phi := \phi^{-1} \sqbrk {e_H} = \set {x \in G: \map \phi x = e_H}$
where $e_H$ is the identity of $H$.


That is, $\map \ker \phi$ is the subset of $G$ that maps to the identity of $H$.

=== Kernel of Ring Homomorphism ===
Let $\struct {R_1, +_1, \circ_1}$ and $\struct {R_2, +_2, \circ_2}$ be rings.

Let $\phi: \struct {R_1, +_1, \circ_1} \to \struct {R_2, +_2, \circ_2}$ be a ring homomorphism.


The kernel of $\phi$ is the subset of the domain of $\phi$ defined as:
:$\map \ker \phi = \set {x \in R_1: \map \phi x = 0_{R_2} }$
where $0_{R_2}$ is the zero of $R_2$.


That is, $\map \ker \phi$ is the subset of $R_1$ that maps to the zero of $R_2$.


From Ring Homomorphism Preserves Zero it follows that $0_{R_1} \in \map \ker \phi$ where $0_{R_1}$ is the zero of $R_1$.

=== Kernel of Linear Transformation ===
Let $\phi: G \to H$ be a linear transformation where $G$ and $H$ are $R$-modules.

Let $e_H$ be the identity of $H$.


The kernel of $\phi$ is defined as:

:$\map \ker \phi := \phi^{-1} \sqbrk {\set {e_H} }$

where $\phi^{-1} \sqbrk S$ denotes the preimage of $S$ under $\phi$.


=== In Vector Space ===
Let $\struct {\mathbf V, +, \times}$ be a vector space.

Let $\struct {\mathbf V', +, \times}$ be a vector space whose zero vector is $\mathbf 0'$.

Let $T: \mathbf V \to \mathbf V'$ be a linear transformation.


Then the kernel of $T$ is defined as:

:$\map \ker T := T^{-1} \sqbrk {\set {\mathbf 0'} } = \set {\mathbf x \in \mathbf V: \map T {\mathbf x} = \mathbf 0'}$",Definition:Kernel (Abstract Algebra),"['Definitions/Kernels (Abstract Algebra)', 'Definitions/Abstract Algebra']"
Definition:Kernel,Kernel,"Let $G$ be a group with identity $e$.

Let $X$ be a set.

Let $* : G\times X\to X$ be a group action.


=== Definition 1 ===
Let $G$ be a group with identity $e$.

Let $X$ be a set.

Let $* : G\times X\to X$ be a group action.


The kernel of the group action is the set:
:$G_0 = \set {g \in G: \forall x \in X: g * x = x}$

=== Definition 2 ===
Let $G$ be a group with identity $e$.

Let $X$ be a set.

Let $* : G \times X \to X$ be a group action.


The kernel of the group action is the kernel of its permutation representation.",Definition:Kernel of Group Action,['Definitions/Group Actions']
Definition:Kernel,Kernel,"Let $\phi: G \to H$ be a linear transformation where $G$ and $H$ are $R$-modules.

Let $e_H$ be the identity of $H$.


The kernel of $\phi$ is defined as:

:$\map \ker \phi := \phi^{-1} \sqbrk {\set {e_H} }$

where $\phi^{-1} \sqbrk S$ denotes the preimage of $S$ under $\phi$.


=== In Vector Space ===
Let $\struct {\mathbf V, +, \times}$ be a vector space.

Let $\struct {\mathbf V', +, \times}$ be a vector space whose zero vector is $\mathbf 0'$.

Let $T: \mathbf V \to \mathbf V'$ be a linear transformation.


Then the kernel of $T$ is defined as:

:$\map \ker T := T^{-1} \sqbrk {\set {\mathbf 0'} } = \set {\mathbf x \in \mathbf V: \map T {\mathbf x} = \mathbf 0'}$",Definition:Kernel of Linear Transformation,"['Definitions/Kernels of Linear Transformations', 'Definitions/Linear Algebra', 'Definitions/Kernels (Abstract Algebra)']"
Definition:Kernel,Kernel,"Let $\struct {R, +, \cdot}$ be a ring.

Let:
:$M: \quad \cdots \longrightarrow M_i \stackrel {d_i} \longrightarrow M_{i + 1} \stackrel {d_{i + 1} } \longrightarrow M_{i + 2} \stackrel {d_{i + 2} } \longrightarrow \cdots$
and
:$N: \quad \cdots \longrightarrow N_i \stackrel {d'_i} \longrightarrow N_{i + 1} \stackrel {d'_{i + 1} } \longrightarrow N_{i + 2} \stackrel {d'_{i + 2} } \longrightarrow \cdots$
be two differential complexes of $R$-modules.

Let $\phi = \set {\phi_i : i \in \Z}$ be a homomorphism $M \to N$.

For each $i \in \Z$ let $K_i$ be the kernel of $\phi_i$.

For each $i \in \Z$ let $f_i$ be the restriction of $d_i$ to $K_i$.


Then the kernel of $\phi$ is:

:$\ker \phi : \quad \cdots \longrightarrow K_i \stackrel {f_i} \longrightarrow K_{i + 1} \stackrel {f_{i + 1} } \longrightarrow K_{i + 2} \stackrel {f_{i + 2} } \longrightarrow \cdots$",Definition:Kernel of Homomorphism of Differential Complexes,['Definitions/Homological Algebra']
Definition:Kernel,Kernel,"Let $\map F p$ be an integral transform:

:$\map F p = \ds \int_a^b \map f x \map K {p, x} \rd x$


The function $\map K {p, x}$ is the kernel of $\map F p$.",Definition:Integral Transform/Kernel,"['Definitions/Integral Transforms', 'Definitions/Kernels']"
Definition:Kernel,Kernel,"Consider the integral equation:

:of the first kind:
::$\map f x = \lambda \ds \int_{\map a x}^{\map b x} \map K {x, y} \map g y \rd x$

:of the second kind:
::$\map g x = \map f x + \lambda \ds \int_{\map a x}^{\map b x} \map K {x, y} \map g y \rd x$

:of the third kind:
::$\map u x \map g x = \map f x + \lambda \ds \int_{\map a x}^{\map b x} \map K {x, y} \map g y \rd x$


The function $\map K {x, y}$ is known as the kernel of the integral equation.",Definition:Integral Equation/Kernel,"['Definitions/Integral Equations', 'Definitions/Kernels']"
Definition:Kernel,Kernel,"Let $\struct {X, \Sigma}$ be a measurable space.

Let $\overline \R_{\ge 0}$ be the set of positive extended real numbers.


A kernel is a mapping $N: X \times \Sigma \to \overline{\R}_{\ge0}$ such that:

:$(1): \quad \forall x \in X: N_x: \Sigma \to \overline \R_{\ge 0}, E \mapsto \map N {x, E}$ is a measure
:$(2): \quad \forall E \in \Sigma: N_E: X \to \overline \R_{\ge 0}, x \mapsto \map N {x, E}$ is a positive $\Sigma$-measurable function

 ",Definition:Kernel (Measure Theory),['Definitions/Measure Theory']
Definition:Kernel,Kernel,"Let $\struct {X, \Sigma, \mu}$ be a measure space.

Let $N: X \times \Sigma \to \overline {\R_{\ge 0} }$ be a kernel.


The transformation of $\mu$ by $N$ is the mapping $\mu N: \Sigma \to \overline \R$ defined by:

:$\ds \forall E \in \Sigma: \map {\mu N} E := \int \map {N_E} x \map {\rd \mu} x$

where $\map {N_E} x = \map N {x, E}$.",Definition:Kernel Transformation of Measure,['Definitions/Measure Theory']
Definition:Kernel,Kernel,"Let $\struct {X, \Sigma, \mu}$ be a measure space.

Let $N: X \times \Sigma \to \overline \R_{\ge 0}$ be a kernel.

Let $f: X \to \overline \R$ be a positive measurable function.


The transformation of $f$ by $N$ is the mapping $N f: X \to \overline \R$ defined by:

:$\forall x \in X: N \map f x := \ds \int f \rd N_x$

where $N_x$ is the measure $E \mapsto \map N {x, E}$.",Definition:Kernel Transformation of Positive Measurable Function,['Definitions/Measure Theory']
Definition:Kernel,Kernel,"Let $\mathbf C$ be a category.

Let $A$ and $B$ be objects of $\mathbf C$.

Let $f : A \to B $ be a morphism in $\mathbf C$.


=== Definition 1: for categories with initial objects ===
Let $\mathbf C$ be a category.

Let $A$ and $B$ be objects of $\mathbf C$.

Let $f : A \to B $ be a morphism in $\mathbf C$.

Let $\mathbf C$ have an initial object $0$.


A kernel of $f$ is a morphism $\map \ker f \to A$ which is a pullback of the unique morphism $0 \to B$ via $f$ to $A$.



=== Uniqueness ===


=== Definition 2: for categories with zero objects ===
Let $\mathbf C$ be a category.

Let $A$ and $B$ be objects of $\mathbf C$.

Let $f: A \to B $ be a morphism in $\mathbf C$.

Let $\mathbf C$ have a zero object $0$.


A kernel of $f$ is a morphism $\ker(f) \to A$, which is an equalizer of $f$ and the zero morphism $0: A \to B$.


=== Uniqueness ===


=== Uniqueness ===
",Definition:Kernel (Category Theory),['Definitions/Category Theory']
Definition:Knot,Knot,Knot theory is the branch of geometry which studies the embedding of knots in $3$-dimensional space.,Definition:Knot Theory,"['Definitions/Knot Theory', 'Definitions/Topology', 'Definitions/Geometry', 'Definitions/Branches of Mathematics']"
Definition:Knot,Knot,"Let $Y$ be a manifold and $X \subset Y$ a submanifold of $Y$.

Let $i: X \to Y$ be an inclusion, that is a mapping such that $i \sqbrk X = X$.


Then a knotted embedding is an embedding $\phi: X \to Y$ (or the image of such an embedding) such that $\phi \sqbrk X$ is not freely homotopic to $i \sqbrk X$.


=== Sphere Knot ===
A knotted $n$-sphere is a knotted embedding:
: $\phi: \Bbb S^n \to \R^{n + 2}$


Category:Definitions/Knot Theory

=== Circle Knot ===
The description of the sphere is dropped for $\Bbb S^1$ and the term knot is used without qualification for knotted embeddings $\phi: \Bbb S^1 \to \R^3$.

 


Category:Definitions/Knot Theory

=== Elementary Knot ===
Circle knots can often be quite wild and unwieldy - most of modern knot theory concerns itself with a specific kind of knot.

These knots are described as a finite set of points in $\R^3$ called $\set {x_1, x_2, \dots, x_n}$, together with line segments from $x_i$ to $x_{i + 1}$ and a line segment from $x_n$ to $x_1$.

The union of all these line segments is clearly a circle knot, or an unknot, an embedding of the circle which is homotopic to a circle.


 

Category:Definitions/Knot Theory",Definition:Knot (Knot Theory),['Definitions/Knot Theory']
Definition:Knot,Knot,"The knot is a unit of speed which is used for air and sea navigation.

It is defined as $1$ nautical mile per hour.

It is now defined as exactly $1 \, 852$ metres per hour.


=== Conversion Factors ===
",Definition:Knot (Unit of Measurement),"['Definitions/Knot (Unit of Measurement)', 'Definitions/Velocity', 'Definitions/Units of Measurement']"
Definition:Knot,Knot,"Let $\closedint a b$ be a closed real interval.

Let $T := \set {a = t_0, t_1, t_2, \ldots, t_{n - 1}, t_n = b}$ form a subdivision of $\closedint a b$.

Let $S: \closedint a b \to \R$ be a spline function on $\closedint a b$ on $T$.


The points $T := \set {t_0, t_1, t_2, \ldots, t_{n - 1}, t_n}$ of $S$ are known as the knots.


=== Knot Vector ===
Let $\closedint a b$ be a closed real interval.

Let $T := \set {a = t_0, t_1, t_2, \ldots, t_{n - 1}, t_n = b}$ form a subdivision of $\closedint a b$.

Let $S: \closedint a b \to \R$ be a spline function on $\closedint a b$ on $T$.


The ordered $n + 1$-tuple $\mathbf t := \tuple {t_0, t_1, t_2, \ldots, t_{n - 1}, t_n}$ of $S$ is known as the knot vector.


Category:Definitions/Knots of Splines",Definition:Spline Function/Knot,"['Definitions/Knots of Splines', 'Definitions/Splines']"
Definition:Lattice,Lattice,"=== Definition 1 ===
Let $\struct {S, \preceq}$ be an ordered set.

Suppose that $S$ admits all finite non-empty suprema and finite non-empty infima.

Denote with $\vee$ and $\wedge$ the join and meet operations on $S$, respectively.


Then the ordered structure $\struct {S, \vee, \wedge, \preceq}$ is called a lattice.

=== Definition 2 ===
Let $\struct {S, \vee, \wedge, \preceq}$ be an ordered structure.


Then $\struct {S, \vee, \wedge, \preceq}$ is called a lattice  if and only if :

:$(1): \quad \struct {S, \vee, \preceq}$ is a join semilattice
and:
:$(2): \quad \struct {S, \wedge, \preceq}$ is a meet semilattice.


That is, for all $a, b \in S$:

:$a \vee b$ is the supremum of $\set {a, b}$
and:
:$a \wedge b$ is the infimum of $\set {a, b}$

=== Definition 3 ===
Let $\struct {S, \vee}$ and $\struct {S, \wedge}$ be semilattices on a set $S$.

Suppose that $\vee$ and $\wedge$ satisfy the absorption laws, that is, for all $a, b \in S$:

:$a \vee \paren {a \wedge b} = a$
:$a \wedge \paren {a \vee b} = a$

Let $\preceq$ be the ordering on $S$ defined by:

:$\forall a, b \in S: a \preceq b$  if and only if  $a \vee b = b$

as on Semilattice Induces Ordering.


Then the ordered structure $\struct {S, \vee, \wedge, \preceq}$ is called a lattice.


Thus $\struct {S, \vee, \wedge, \preceq}$ is called a lattice  if and only if  the lattice axioms are satisfied and $\preceq$ is defined as above:
 ",Definition:Lattice (Order Theory),"['Definitions/Lattices (Order Theory)', 'Definitions/Lattice Theory', 'Definitions/Order Theory', 'Definitions/Ordered Structures']"
Definition:Lattice,Lattice,"=== Definition 1 ===
A point lattice is a discrete subgroup of $\R^m$ under vector addition.

=== Definition 2 ===
Let $\R^m$ be the $m$-dimensional real Euclidean space.

Let $\set {\mathbf v_1, \mathbf v_2, \ldots, \mathbf v_n}$ be a linearly independent set of vectors of $\R^m$.

A point lattice in $\R^m$ is the set of all integer linear combinations of such vectors.


That is:
:$\ds \map \LL {\mathbf v_1, \mathbf v_2, \ldots, \mathbf v_n} = \set {\sum_{i \mathop = 1}^n a_i \mathbf v_i : a_i \in \Z}$",Definition:Point Lattice,"['Definitions/Point Lattices', 'Definitions/Group Theory', 'Definitions/Lattice Theory']"
Definition:Leading Coefficient,Leading Coefficient,"Let $R$ be a commutative ring with unity.

Let $P \in R \sqbrk X$ be a nonzero polynomial over $R$.

Let $n$ be the degree of $P$.


The leading coefficient of $P$ is the coefficient of $x^n$ in $P$.

 

Let $\struct {R, +, \circ}$ be a ring.

Let $\struct {S, +, \circ}$ be a subring of $R$.

Let $\ds f = \sum_{k \mathop = 0}^n a_k \circ x^k$ be a polynomial in $x$ over $S$.


The coefficient $a_n \ne 0_R$ is called the leading coefficient of $f$.


=== Polynomial Form ===
Let $R$ be a commutative ring with unity.

Let $f = a_0 + a_1 X + \cdots + a_{r-1} X^{r-1} + a_r X^r$ be a polynomial form in the single indeterminate $X$ over $R$.


Then the ring element $a_r$ is called the leading coefficient of $f$.",Definition:Leading Coefficient of Polynomial,['Definitions/Polynomial Theory']
Definition:Leading Coefficient,Leading Coefficient,"Let $\mathbf A = \sqbrk a_{m n}$ be an $m \times n$ matrix.

The leading coefficient of each row of $\mathbf A$ is the leftmost non-zero element of that row.


A zero row has no leading coefficient.",Definition:Leading Coefficient of Matrix,['Definitions/Matrix Theory']
Definition:Left,Left,"The direction left is that way:
:$\gets$",Definition:Left (Direction),['Definitions/Language Definitions']
Definition:Left,Left,"In an equation:
:$\text {Expression $1$} = \text {Expression $2$}$
the term $\text {Expression $1$}$ is the left hand side.",Definition:Left Hand Side,['Definitions/Language Definitions']
Definition:Left,Left,"Let $\RR \subseteq S \times T$ be a relation.


Then $\RR$ is left-total  if and only if :
:$\forall s \in S: \exists t \in T: \tuple {s, t} \in \RR$


That is,  if and only if  every element of $S$ relates to some element of $T$.",Definition:Left-Total Relation,"['Definitions/Relation Theory', 'Definitions/Left-Total Relations']"
Definition:Left,Left,"Let $\RR \subseteq S \times S$ be a relation in $S$.


=== Definition 1 ===
Let $\RR \subseteq S \times S$ be a relation in $S$.


$\RR$ is left quasi-reflexive  if and only if :

:$\forall x, y \in S: \tuple {x, y} \in \RR \implies \tuple {x, x} \in \RR$

=== Definition 2 ===
Let $\RR \subseteq S \times S$ be a relation in $S$.


$\RR$ is left quasi-reflexive  if and only if :

:$\forall x \in \Dom \RR: \tuple {x, x} \in \RR$

where $\Dom \RR$ denotes the domain of $\RR$.",Definition:Left Quasi-Reflexive Relation,"['Definitions/Reflexive Relations', 'Definitions/Quasi-Reflexive Relations', 'Definitions/Left Quasi-Reflexive Relations']"
Definition:Left,Left,"Let $\RR \subseteq S \times S$ be a relation in $S$.


$\RR$ is left-Euclidean  if and only if :

:$\tuple {x, z} \in \RR \land \tuple {y, z} \in \RR \implies \tuple {x, y} \in \RR$",Definition:Euclidean Relation/Left-Euclidean,['Definitions/Euclidean Relations']
Definition:Left,Left,"Let $A$ be a class.

Let $\RR$ be a relation on $A$.


An element $x$ of $A$ is left normal with respect to $\RR$  if and only if :
:$\forall y \in A: \map \RR {x, y}$ holds.",Definition:Left Normal Element of Relation,['Definitions/Relations']
Definition:Left,Left,"A mapping $f: Y \to Z$ is left cancellable (or left-cancellable)  if and only if :

:$\forall X: \forall \struct {g_1, g_2: X \to Y}: f \circ g_1 = f \circ g_2 \implies g_1 = g_2$

That is, for any set $X$, if $g_1$ and $g_2$ are mappings from $X$ to $Y$:
:If $f \circ g_1 = f \circ g_2$
:then $g_1 = g_2$.",Definition:Left Cancellable Mapping,"['Definitions/Mapping Theory', 'Definitions/Cancellability']"
Definition:Left,Left,"Let $S, T$ be sets where $S \ne \O$, that is, $S$ is not empty.

Let $f: S \to T$ be a mapping.


Let $g: T \to S$ be a mapping such that:
:$g \circ f = I_S$
where:
:$g \circ f$ denotes the composite mapping $f$ followed by $g$;
:$I_S$ is the identity mapping on $S$.


Then $g: T \to S$ is called a left inverse (mapping).",Definition:Left Inverse Mapping,['Definitions/Inverse Mappings']
Definition:Left,Left,"Let $\struct {S, \preccurlyeq}$ be an ordered set.

Let $a, b \in S$.


The left half-open interval between $a$ and $b$ is the set:

:$\hointl a b := a^\succ \cap b^\preccurlyeq = \set {s \in S: \paren {a \prec s} \land \paren {s \preccurlyeq b} }$

where:
:$a^\succ$ denotes the strict upper closure of $a$
:$b^\preccurlyeq$ denotes the lower closure of $b$.",Definition:Interval/Ordered Set/Left Half-Open,['Definitions/Intervals']
Definition:Left,Left,"Let $B$ be a Banach space over the set of real numbers $\R$.

Let $f: \R \to B$ be a mapping from $\R$ to $B$.


The left-hand derivative of $f$ is defined as the left-hand limit:
:$\ds \map {f'_-} x = \lim_{h \mathop \to 0^-} \frac {\map f {x + h} - \map f x} h$

If the left-hand derivative exists, then $f$ is said to be left-hand differentiable at $x$.


=== Real Functions ===
Let $f: \R \to \R$ be a real function.


The left-hand derivative of $f$ is defined as the left-hand limit:
:$\ds \map {f'_-} x = \lim_{h \mathop \to 0^-} \frac {\map f {x + h} - \map f x} h$

If the left-hand derivative exists, then $f$ is said to be left-hand differentiable at $x$.",Definition:Left-Hand Derivative,['Definitions/Derivatives']
Definition:Left,Left,"Let $\openint a b$ be an open real interval.

Let $f: \openint a b \to \R$ be a real function.

Let $L \in \R$.


Suppose that:
:$\forall \epsilon \in \R_{>0}: \exists \delta \in \R_{>0}: \forall x \in \R: b - \delta < x < b \implies \size {\map f x - L} < \epsilon$

where $\R_{>0}$ denotes the set of strictly positive real numbers.

That is, for every real strictly positive $\epsilon$ there exists a real strictly positive $\delta$ such that every real number in the domain of $f$, less than $b$ but within $\delta$ of $b$, has an image within $\epsilon$ of $L$.


:

Then $\map f x$ is said to tend to the limit $L$ as $x$ tends to $b$ from the left, and we write:
:$\map f x \to L$ as $x \to b^-$
or
:$\ds \lim_{x \mathop \to b^-} \map f x = L$


This is voiced:
:the limit of $\map f x$ as $x$ tends to $b$ from the left
and such an $L$ is called:
:a limit from the left.",Definition:Limit of Real Function/Left,['Definitions/Limits of Real Functions']
Definition:Left,Left,"Let $V$ be a vector space over the real numbers $\R$.

Let $f: \R \to V$ be a function.


A left difference quotient is an expression of the form:
:$\dfrac {\map f {x + h} - \map f x} h$
where $h < 0$ is a strictly negative real number.",Definition:Difference Quotient/Left,['Definitions/Difference Quotients']
Definition:Left,Left,"Let $A \subseteq \R$ be an open subset of the real numbers $\R$.

Let $f: A \to \R$ be a real function.


Let $x_0 \in A$. 

Then $f$ is said to be left-continuous at $x_0$  if and only if  the limit from the left of $\map f x$ as $x \to x_0$ exists and:

:$\ds \lim_{\substack {x \mathop \to x_0^- \\ x_0 \mathop \in A} } \map f x = \map f {x_0}$

where $\ds \lim_{x \mathop \to x_0^-}$ is a limit from the left.


Furthermore, $f$ is said to be left-continuous  if and only if :

:$\forall x_0 \in A$, $f$ is left-continuous at $x_0$",Definition:Continuous Real Function/Left-Continuous,['Definitions/Continuous Real Functions']
Definition:Left,Left,"Let $f: \R \to \R$ be a real function.


The left-hand derivative of $f$ is defined as the left-hand limit:
:$\ds \map {f'_-} x = \lim_{h \mathop \to 0^-} \frac {\map f {x + h} - \map f x} h$

If the left-hand derivative exists, then $f$ is said to be left-hand differentiable at $x$.",Definition:Left-Hand Derivative/Real Function,['Definitions/Derivatives']
Definition:Left,Left,"Let $a, b \in \R$ be real numbers.


The left half-open (real) interval from $a$ to $b$ is the subset:
:$\hointl a b := \set {x \in \R: a < x \le b}$",Definition:Real Interval/Half-Open/Left,[]
Definition:Left,Left,"There are two unbounded closed intervals involving a real number $a \in \R$, defined as:

 
 
 
 ",Definition:Real Interval/Unbounded Closed,['Definitions/Real Intervals']
Definition:Left,Left,"There are two unbounded open intervals involving a real number $a \in \R$, defined as:

 
 
 
 ",Definition:Real Interval/Unbounded Open,['Definitions/Real Intervals']
Definition:Left,Left,"Let $\struct {S, \circ}$ be an algebraic structure.

An element $z_L \in S$ is called a left zero element (or just left zero)  if and only if :
:$\forall x \in S: z_L \circ x = z_L$",Definition:Left Zero,['Definitions/Zero Elements']
Definition:Left,Left,"Let $\struct {S, \circ}$ be an algebraic structure.

An element $e_L \in S$ is called a left identity (element)  if and only if :
:$\forall x \in S: e_L \circ x = x$",Definition:Identity (Abstract Algebra)/Left Identity,['Definitions/Identity Elements']
Definition:Left,Left,"Let $S$ be a set.

For any $x, y \in S$, the left operation on $S$ is the binary operation defined as:
:$\forall x, y \in S: x \gets y = x$",Definition:Left Operation,"['Definitions/Abstract Algebra', 'Definitions/Left Operation']"
Definition:Left,Left,"Let $\struct {S, \circ}$ be an algebraic structure.


An element $x \in \struct {S, \circ}$ is left cancellable  if and only if :

:$\forall a, b \in S: x \circ a = x \circ b \implies a = b$",Definition:Cancellable Element/Left Cancellable,['Definitions/Cancellability']
Definition:Left,Left,"Let $\struct {S, \circ}$ be an algebraic structure.


The operation $\circ$ in $\struct {S, \circ}$ is left cancellable  if and only if :
:$\forall a, b, c \in S: a \circ b = a \circ c \implies b = c$

That is,  if and only if  all elements of $\struct {S, \circ}$ are left cancellable.",Definition:Left Cancellable Operation,['Definitions/Cancellability']
Definition:Left,Left,"Let $S$ be a set on which is defined two binary operations, defined on all the elements of $S \times S$, denoted here as $\circ$ and $*$.

The operation $\circ$ is left distributive over the operation $*$  if and only if :

:$\forall a, b, c \in S: a \circ \paren {b * c} = \paren {a \circ b} * \paren {a \circ c}$",Definition:Distributive Operation/Left,['Definitions/Distributive Operations']
Definition:Left,Left,"Let $\struct {S, \circ}$ be a monoid whose identity is $e_S$.

An element $x_L \in S$ is called a left inverse of $x$  if and only if :
:$x_L \circ x = e_S$",Definition:Inverse (Abstract Algebra)/Left Inverse,['Definitions/Inverse Elements']
Definition:Left,Left,"Let $\struct {S, \circ}$ be a magma. 


$\struct {S, \circ}$ is a left quasigroup  if and only if :
:for all $a \in S$, the left regular representation $\lambda_a$ is a permutation on $S$.

That is:
:$\forall a, b \in S: \exists ! x \in S: a \circ x = b$",Definition:Quasigroup/Left Quasigroup,['Definitions/Quasigroups']
Definition:Left,Left,"Let $\struct {S, \circ}$ be a magma.

The mapping $\lambda_a: S \to S$ is defined as:

:$\forall x \in S: \map {\lambda_a} x = a \circ x$


This is known as the left regular representation of $\struct {S, \circ}$ with respect to $a$.",Definition:Regular Representations/Left Regular Representation,"['Definitions/Left Regular Representation', 'Definitions/Regular Representations']"
Definition:Left,Left,"Let $x$ and $y$ be elements which are operated on by a given operation $\circ$.

The left-hand product of $x$ by $y$ is the product $y \circ x$.",Definition:Operation/Binary Operation/Product/Left,['Definitions/Operations']
Definition:Left,Left,"Let $\struct {S, \circ, \preceq}$ be a positively totally ordered semigroup.


Then $\struct {S, \circ, \preceq}$ is a left naturally totally ordered semigroup  if and only if :

:$a \prec b \implies \exists x \in S: b = x \circ a$",Definition:Left Naturally Totally Ordered Semigroup,['Definitions/Naturally Ordered Semigroup']
Definition:Left,Left,"Let $\struct {S, \circ}$ be an algebraic structure.

Let $\struct {H, \circ}$ be a subgroup of $\struct {S, \circ}$.


The left coset of $x$ modulo $H$, or left coset of $H$ by $x$, is:

:$x \circ H = \set {y \in S: \exists h \in H: y = x \circ h}$


That is, it is the subset product with singleton:

:$x \circ H = \set x \circ H$",Definition:Coset/Left Coset,['Definitions/Cosets']
Definition:Left,Left,"Let $G$ be a group.

Let $H$ be a subgroup of $G$.


The left coset space (of $G$ modulo $H$) is the quotient set of $G$ by left congruence modulo $H$, denoted $G / H^l$.

It is the set of all the left cosets of $H$ in $G$.

 ",Definition:Coset Space/Left Coset Space,['Definitions/Cosets']
Definition:Left,Left,"Let $G$ be a group.

Let $H$ be a subgroup of $G$.

Let $S \subseteq G$ be a subset of $G$.


$S$ is a left transversal for $H$ in $G$  if and only if  every left coset of $H$ contains exactly one element of $S$.",Definition:Transversal (Group Theory)/Left Transversal,['Definitions/Transversals (Group Theory)']
Definition:Left,Left,"Let $X$ be a set.

Let $\struct {G, \circ}$ be a group whose identity is $e$.


A (left) group action is an operation $\phi: G \times X \to X$ such that:

:$\forall \tuple {g, x} \in G \times X: g * x := \map \phi {g, x} \in X$

in such a way that the group action axioms are satisfied:
 ",Definition:Group Action/Left Group Action,['Definitions/Group Actions']
Definition:Left,Left,"Let $\struct {G, \circ}$ be a group.

Let $\powerset G$ be the power set of $G$.


The (left) subset product action of $G$ is the group action $*: G \times \powerset G \to \powerset G$:
:$\forall g \in G, S \in \powerset G: g * S = g \circ S$",Definition:Subset Product Action/Left,['Definitions/Subset Product Action']
Definition:Left,Left,"Let $G$ be a group.

Let $H$ be a subgroup of $G$.


We can use $H$ to define a relation on $G$ as follows:

:$\RR^l_H := \set {\tuple {x, y} \in G \times G: x^{-1} y \in H}$

This is called left congruence modulo $H$.",Definition:Congruence Modulo Subgroup/Left Congruence,['Definitions/Congruence Modulo Subgroup']
Definition:Left,Left,"Let $\struct {R, +, \circ}$ be a ring.


A left zero divisor (in $R$) is an element $x \in R$ such that:
:$\exists y \in R^*: x \circ y = 0_R$

where $R^*$ is defined as $R \setminus \set {0_R}$.",Definition:Left Zero Divisor,['Definitions/Zero Divisors']
Definition:Left,Left,"Let $R$ be a ring.

Let $M$ be an abelian group.

Let $\circ : R \times M \to M$ be a mapping from the cartesian product $R \times M$.


$\circ$ is a left linear ring action of $R$ on $M$  if and only if  $\circ$ satisfies the left ring action axioms:
 ",Definition:Linear Ring Action/Left,['Definitions/Linear Ring Actions']
Definition:Left,Left,"Let $\struct {R, +, \circ}$ be a ring.

Let $\struct {J, +}$ be a subgroup of $\struct {R, +}$.


$J$ is a left ideal of $R$  if and only if :
:$\forall j \in J: \forall r \in R: r \circ j \in J$

that is,  if and only if :
:$\forall r \in R: r \circ J \subseteq J$",Definition:Ideal of Ring/Left Ideal,['Definitions/Ideal Theory']
Definition:Left,Left,"Let $R$ be a ring.


A left ideal $J$ of $R$ is a maximal left ideal  if and only if :

:$(1): \quad J \subsetneq R$
:$(2): \quad$ There is no left ideal $K$ of $R$ such that $J \subsetneq K \subsetneq R$.


Category:Definitions/Maximal Ideals of Rings",Definition:Maximal Ideal of Ring/Left,['Definitions/Maximal Ideals of Rings']
Definition:Left,Left,"Let $\struct {R, +_R, \times_R}$ be a ring.

Let $\struct {G, +_G}$ be an abelian group.


A left module over $R$ is an $R$-algebraic structure $\struct {G, +_G, \circ}_R$ with one operation $\circ$, the (left) ring action, which satisfies the left module axioms:
 ",Definition:Left Module,"['Definitions/Left Modules', 'Definitions/Module Theory']"
Definition:Left,Left,"Let $R$ be a ring.

Let $\mathbf A$ be a matrix in the matrix space $\map {\MM_{m, n} } R$.

Let $\mathbf A^\intercal$ be the transpose of $\mathbf A$.

The left null space $\mathbf A$ is defined as the null space of $\mathbf A^\intercal$.",Definition:Left Null Space,"['Definitions/Linear Algebra', 'Definitions/Null Spaces']"
Definition:Left,Left,"A left-truncatable prime is a prime number which remains prime when any number of digits are removed from the left hand end.

Zeroes are excluded, in order to eliminate, for example, prime numbers of the form $10^n + 3$ for arbitrarily large $n$.


=== Sequence ===
 ",Definition:Left-Truncatable Prime,"['Definitions/Prime Numbers', 'Definitions/Recreational Mathematics', 'Definitions/Left-Truncatable Primes']"
Definition:Left,Left,"Let $R$ be a ring.


The category of left $R$-modules is the category $\mathbf {R-Mod}$ with:

 ",Definition:Category of Left Modules,"['Definitions/Examples of Categories', 'Definitions/Module Theory']"
Definition:Left,Left,"A Cartesian plane is defined as being left-handed if it has the following property:

Let a left hand be placed, with palm uppermost, such that the thumb points along the $x$-axis in the positive direction, such that the thumb and index finger are at right-angles to each other.

Then the index finger is pointed along the $y$-axis in the positive direction.


:",Definition:Orientation of Coordinate Axes/Cartesian Plane/Left-Handed,['Definitions/Orientation (Coordinate Axes)']
Definition:Left,Left,"A Cartesian $3$-Space is defined as being left-handed if it has the following property:

Let a left hand be placed such that:
:the thumb and index finger are at right-angles to each other
:the $3$rd finger is at right-angles to the thumb and index finger, upwards from the palm
:the thumb points along the $x$-axis in the positive direction
:the index finger points along the $y$-axis in the positive direction.

Then the $3$rd finger is pointed along the $z$-axis in the positive direction.


:",Definition:Orientation of Coordinate Axes/Cartesian 3-Space/Left-Handed,['Definitions/Orientation (Coordinate Axes)']
Definition:Left Cancellable,Left Cancellable,"Let $\struct {S, \circ}$ be an algebraic structure.


An element $x \in \struct {S, \circ}$ is left cancellable  if and only if :

:$\forall a, b \in S: x \circ a = x \circ b \implies a = b$",Definition:Cancellable Element/Left Cancellable,['Definitions/Cancellability']
Definition:Left Cancellable,Left Cancellable,"Let $\struct {S, \circ}$ be an algebraic structure.


The operation $\circ$ in $\struct {S, \circ}$ is left cancellable  if and only if :
:$\forall a, b, c \in S: a \circ b = a \circ c \implies b = c$

That is,  if and only if  all elements of $\struct {S, \circ}$ are left cancellable.",Definition:Left Cancellable Operation,['Definitions/Cancellability']
Definition:Left Cancellable,Left Cancellable,"A mapping $f: Y \to Z$ is left cancellable (or left-cancellable)  if and only if :

:$\forall X: \forall \struct {g_1, g_2: X \to Y}: f \circ g_1 = f \circ g_2 \implies g_1 = g_2$

That is, for any set $X$, if $g_1$ and $g_2$ are mappings from $X$ to $Y$:
:If $f \circ g_1 = f \circ g_2$
:then $g_1 = g_2$.",Definition:Left Cancellable Mapping,"['Definitions/Mapping Theory', 'Definitions/Cancellability']"
Definition:Left Inverse,Left Inverse,"Let $S, T$ be sets where $S \ne \O$, that is, $S$ is not empty.

Let $f: S \to T$ be a mapping.


Let $g: T \to S$ be a mapping such that:
:$g \circ f = I_S$
where:
:$g \circ f$ denotes the composite mapping $f$ followed by $g$;
:$I_S$ is the identity mapping on $S$.


Then $g: T \to S$ is called a left inverse (mapping).",Definition:Left Inverse Mapping,['Definitions/Inverse Mappings']
Definition:Left Inverse,Left Inverse,"Let $\struct {S, \circ}$ be a monoid whose identity is $e_S$.

An element $x_L \in S$ is called a left inverse of $x$  if and only if :
:$x_L \circ x = e_S$",Definition:Inverse (Abstract Algebra)/Left Inverse,['Definitions/Inverse Elements']
Definition:Left Inverse,Left Inverse,"Let $m, n \in \Z_{>0}$ be a (strictly) positive integer.


Let $\mathbf A = \sqbrk a_{m n}$ be a matrix of order $m \times n$.

Let $\mathbf B = \sqbrk b_{n m}$ be a matrix of order $n \times m$ such that:
:$\mathbf B \mathbf A = \mathbf I_n$

where $\mathbf I_n$ denotes the unit matrix of order $n$.


Then $\mathbf B$ is known as a left inverse (matrix) of $\mathbf A$.",Definition:Inverse Matrix/Left,['Definitions/Inverse Matrices']
Definition:Length,Length,"Length is linear measure taken in a particular direction.

Usually, in multi-dimensional figures, the dimension in which the linear measure is greatest is referred to as length.

It is the most widely used term for linear measure, as it is the standard term used when only one dimension is under consideration.


Length is the fundamental notion of Euclidean geometry, never defined but regarded as an intuitive concept at the basis of every geometrical theorem.


=== Vector Definition ===
Let $A$ and $B$ be points in a real Euclidean space $\R^n$.

Let $\mathbf a$ and $\mathbf b$ be the position vectors of $A$ and $B$ respectively.


The length of the straight line $AB$ is given by:
:$\map L {AB} := \size {\mathbf a - \mathbf b}$
where:
:$\mathbf a - \mathbf b$ denotes the vector subtraction operation
:$\size {\mathbf a - \mathbf b}$ denotes the magnitude of $\mathbf a - \mathbf b$.",Definition:Linear Measure/Length,"['Definitions/Length', 'Definitions/Geometry', 'Definitions/Fundamental Dimensions']"
Definition:Length,Length,"Let:
:$\closedint a b$
or
:$\hointr a b$
or
:$\hointl a b$
or
:$\openint a b$
be a real interval.


The difference $b - a$ between the endpoints is called the length of the interval.",Definition:Real Interval/Length,['Definitions/Real Intervals']
Definition:Length,Length,The length of a curve is defined as the limit of the length of a polygonal line inscribed within the curve as the maximum length of the chords which form that polygonal line tends to zero.,Definition:Length of Curve,['Definitions/Analytic Geometry']
Definition:Length,Length,"The length of a finite sequence is the number of terms it contains, or equivalently, the cardinality of its domain.


=== Sequence of $n$ Terms ===
A sequence of $n$ terms is a (finite) sequence whose length is $n$.",Definition:Length of Sequence,['Definitions/Sequences']
Definition:Length,Length,"Let $C$ be a contour in $\C$ defined by the (finite) sequence $\sequence {C_1, \ldots, C_n}$ of directed smooth curves in $\C$.

Let $C_k$ be parameterized by the smooth path $\gamma_k: \closedint {a_k} {b_k} \to \C$ for all $k \in \set {1, \ldots, n}$.


The length of $C$ is defined as:

:$\ds \map L C := \sum_{k \mathop = 1}^n \int_{a_k}^{b_k} \size {\map {\gamma_k'} t} \rd t$


It follows from Length of Contour is Well-Defined that $\map L C$ is defined and independent of the parameterizations of $C_1, \ldots, C_n$.",Definition:Contour/Length/Complex Plane,['Definitions/Complex Analysis']
Definition:Length,Length,"The length of a vector $\mathbf v$ in a normed vector space $\struct {V, \norm {\, \cdot \,} }$ is defined as $\norm {\mathbf v}$, the norm of $\mathbf v$.


=== Arrow Representation ===
Let $\mathbf v$ be a vector quantity represented as an arrow in a real vector space $\R^n$.

The length of $\mathbf v$ is the length of the line segment representing $\mathbf v$ in $\R^n$.",Definition:Vector Length,"['Definitions/Vectors', 'Definitions/Vector Length']"
Definition:Length,Length,"Let $\struct {S, \preceq}$ be an ordered set.

Let $T$ be a chain in $S$.

Let $T$ be finite and non-empty.


The length of the chain $T$ is its cardinality minus $1$.


Category:Definitions/Chains (Order Theory)",Definition:Length of Chain,['Definitions/Chains (Order Theory)']
Definition:Length,Length,"Let $\alpha$ be an ordinal.

Let $\theta$ be an ordinal sequence whose domain is $\alpha$.


Then $\alpha$ can be referred to as the length of $\theta$.


The length of $\theta$ can be denoted $\size \theta$.",Definition:Ordinal Sequence/Length,['Definitions/Ordinal Sequences']
Definition:Length,Length,The number of partial denominators in a cycle of a periodic continued fraction is called the cycle length.,Definition:Periodic Continued Fraction/Cycle/Length,['Definitions/Continued Fractions']
Definition:Length,Length,"Let $k$ be a field.

Let $C$ be a continued fraction in $k$, either finite or infinite.


The length of $C$ is an extended natural number equal to:
*$\infty$ if $C$ is an infinite continued fraction.
*$n$ if $C$ is a finite continued fraction with domain the integer interval $\left[0 \,.\,.\, n\right]$.",Definition:Length of Continued Fraction,['Definitions/Continued Fractions']
Definition:Length,Length,"The length of a walk (or a path, or a trail) is the number of edges it has, counting repeated edges as many times as they appear.


A walk is said to be of infinite length  if and only if  it has infinitely many edges.


=== Zero Length Walk ===
A zero length walk is a walk which consists of one vertex.


Category:Definitions/Walks",Definition:Walk (Graph Theory)/Length,['Definitions/Walks']
Definition:Length,Length,"Let $T$ be a rooted tree with root node $r_T$.

Let $\Gamma$ be a finite branch of $T$.


The length of $\Gamma$ is defined as the number of ancestors of the leaf at the end of that branch.",Definition:Rooted Tree/Branch/Length,['Definitions/Rooted Trees']
Definition:Length,Length,"The length of a finite string in a formal language is the number of symbols it contains.


The length of a string $S$ can be denoted $\map \len S$ or $\size S$.",Definition:Length of String,['Definitions/Collations']
Definition:Length,Length,"The length of a tableau proof is the number of lines it has.


Category:Definitions/Proof Systems",Definition:Tableau Proof (Formal Systems)/Length,['Definitions/Proof Systems']
Definition:Length,Length,"Let $G$ be a group acting on a set $X$.

Let $x \in X$.


Let $\Orb x$ be the orbit of $x$.

The length of the orbit $\Orb x$ of $x$ is the number of elements of $X$ it contains:
:$\size {\Orb x}$


Category:Definitions/Group Actions",Definition:Orbit (Group Theory)/Length,['Definitions/Group Actions']
Definition:Length,Length,"Let $G$ be a group whose identity is $e$.

Let $\sequence {G_i}_{i \mathop \in \closedint 0 n}$ be a normal series for $G$:
:$\sequence {G_i}_{i \mathop \in \closedint 0 n} = \tuple {\set e = G_0 \lhd G_1 \lhd \cdots \lhd G_{n-1} \lhd G_n = G}$


The length of $\sequence {G_i}_{i \mathop \in \closedint 0 n}$ is the number of (normal) subgroups which make it.

In this context, the length of $\sequence {G_i}_{i \mathop \in \closedint 0 n}$ is $n$.


If such a normal series is infinite, then its length is not defined.


Category:Definitions/Normal Series",Definition:Normal Series/Length,['Definitions/Normal Series']
Definition:Length,Length,"Let $G$ be a finite group.

The length of $G$ is the length of a composition series for $G$.

That is, the length of $G$ is the number of factors in a composition series for $G$ (not including $G$ itself).


The length of $G$ can be denoted $\map l G$.


By the Jordan-Hlder Theorem, all composition series for $G$ have the same length.

Therefore, the length of a finite group $G$ is well-defined.",Definition:Length of Group,"['Definitions/Composition Series', 'Definitions/Group Theory']"
Definition:Length,Length,"Let $S_n$ denote the symmetric group on $n$ letters.

Let $\rho \in S_n$ be a permutation on $S$.


Then $\rho$ is a cyclic permutation of length $k$  if and only if  there exists $k \in \Z: k > 0$ and $i \in \Z$ such that:
:$(1): \quad k$ is the smallest such that $\map {\rho^k} i = i$

:$(2): \quad \rho$ fixes each $j$ not in $\set {i, \map \rho i, \ldots, \map {\rho^{k - 1} } i}$.


$\rho$ is usually denoted using cycle notation as:
:$\begin{pmatrix} i & \map \rho i & \ldots & \map {\rho^{k - 1} } i \end{pmatrix}$

but some sources introduce it using two-row notation:

:$\begin{pmatrix} a_1 & a_2 & \cdots & a_k & \cdots & i & \cdots \\ a_2 & a_3 & \cdots & a_1 & \cdots & i & \cdots \end{pmatrix}$",Definition:Cyclic Permutation,"['Definitions/Cyclic Permutations', 'Definitions/Permutation Theory']"
Definition:Like,Like,"Let $\mathbf a$ and $\mathbf b$ be vector quantities.

Then $\mathbf a$ and $\mathbf b$ are known as like vector quantities  if and only if  they have the same direction.",Definition:Like Vector Quantities,['Definitions/Vectors']
Definition:Like,Like,"If $2$ electric charges are of the same polarity, they are referred to as being like (electric) charges.


The usage is archaic; the word one would expect is alike.",Definition:Electric Charge/Polarity/Like,['Definitions/Electric Charge']
Definition:Limit,Limit,"=== Topological Space ===
Let $T = \struct {S, \tau}$ be a topological space.

Let $A \subseteq S$.

Let $\sequence {x_n}$ be a sequence in $A$.

Let $\sequence {x_n}$ converge to a value $\alpha \in S$.


Then $\alpha$ is known as a limit (point) of $\sequence {x_n}$ (as $n$ tends to infinity).

=== Metric Space ===
Let $M = \struct {A, d}$ be a metric space or pseudometric space.

Let $\sequence {x_n}$ be a sequence in $M$.

Let $\sequence {x_n}$ converge to a value $l \in A$.


Then $l$ is a limit of $\sequence {x_n}$ as $n$ tends to infinity.


If $M$ is a metric space, this is usually written:
:$\ds l = \lim_{n \mathop \to \infty} x_n$

=== Normed Division Ring ===
Let $\struct {R, \norm {\, \cdot \,} }$ be a normed division ring.

Let $\sequence {x_n} $ be a sequence in $R$.

Let $\sequence {x_n}$ converge to $x \in R$.

Then $x$ is a limit of $\sequence {x_n}$ as $n$ tends to infinity which is usually written:
:$\ds x = \lim_{n \mathop \to \infty} x_n$

=== Normed Vector Space ===
Let $M = \struct {X, \norm {\, \cdot \,}}$ be a normed vector space.

Let $L \in X$.

Let $\sequence {x_n}_{n \mathop \in \N}$ be a sequence in $X$.

Let $\sequence {x_n}_{n \mathop \in \N}$ converge to $L$. 


Then $L$ is a limit of  $\sequence {x_n}_{n \mathop \in \N}$ as $n$ tends to infinity which is usually written:
:$\ds L = \lim_{n \mathop \to \infty} x_n$

=== Test Function Space ===
Let $\map \DD {\R^d}$ be the test function space.

Let $\phi \in \map \DD {\R^d}$ be a test function.

Let $\sequence {\phi_n}_{n \mathop \in \N}$ be a sequence of test functions in $\map \DD {\R^d}$.

Let $\sequence {\phi_n}_{n \mathop \in \N}$ converge to $\phi$ in $\map \DD {\R^d}$. 


Then $\phi$ is a limit of  $\sequence {\phi_n}_{n \mathop \in \N}$ in $\map \DD {\R^d}$ as $n$ tends to infinity which is usually written:
:$\phi_n \stackrel \DD {\longrightarrow} \phi$",Definition:Limit of Sequence,['Definitions/Limits of Sequences']
Definition:Limit,Limit,"Let $M_1 = \struct {A_1, d_1}$ and $M_2 = \struct {A_2, d_2}$ be metric spaces.

Let $c$ be a limit point of $M_1$.

Let $f: A_1 \to A_2$ be a mapping from $A_1$ to $A_2$ defined everywhere on $A_1$ except possibly at $c$.


Let $L \in M_2$.


$\map f x$ is said to tend to the limit $L$ as $x$ tends to $c$ and is written:
:$\map f x \to L$ as $x \to c$
or:
:$\ds \lim_{x \mathop \to c} \map f x = L$

 if and only if  the following equivalent conditions hold:


=== $\epsilon$-$\delta$ Condition ===
Let $M_1 = \struct {A_1, d_1}$ and $M_2 = \struct {A_2, d_2}$ be metric spaces.

Let $c$ be a limit point of $M_1$.

Let $f: A_1 \to A_2$ be a mapping from $A_1$ to $A_2$ defined everywhere on $A_1$ except possibly at $c$.


Let $L \in M_2$.


$\map f x$ is said to tend to the limit $L$ as $x$ tends to $c$ and is written:
:$\map f x \to L$ as $x \to c$
or
:$\ds \lim_{x \mathop \to c} \map f x = L$

 if and only if :

:$\forall \epsilon \in \R_{>0}: \exists \delta \in \R_{>0}: 0 < \map {d_1} {x, c} < \delta \implies \map {d_2} {\map f x, L} < \epsilon$

That is, for every real positive $\epsilon$ there exists a real positive $\delta$ such that every point in the domain of $f$ within $\delta$ of $c$ has an image within $\epsilon$ of some point $L$ in the codomain of $f$.


This is voiced:
:the limit of $\map f x$ as $x$ tends to $c$.

=== $\epsilon$-Ball Condition ===
Let $M_1 = \struct {A_1, d_1}$ and $M_2 = \struct {A_2, d_2}$ be metric spaces.

Let $c$ be a limit point of $M_1$.

Let $f: A_1 \to A_2$ be a mapping from $A_1$ to $A_2$ defined everywhere on $A_1$ except possibly at $c$.


Let $L \in M_2$.


$\map f x$ is said to tend to the limit $L$ as $x$ tends to $c$ and is written:
:$\map f x \to L$ as $x \to c$
or
:$\ds \lim_{x \mathop \to c} \map f x = L$

 if and only if :

:$\forall \epsilon \in \R_{>0}: \exists \delta \in \R_{>0}: f \sqbrk {\map {B_\delta} {c; d_1} \setminus \set c} \subseteq \map {B_\epsilon} {L; d_2}$

where:
:$\map {B_\delta} {c; d_1} \setminus \set c$ is the deleted $\delta $-neighborhood of $c$ in $M_1$
:$\map {B_\epsilon} {L; d_2}$ is the open $\epsilon$-ball of $L$ in $M_2$.


That is, for every open $\epsilon$-ball of $L$ in $M_2$, there exists a deleted $\delta $-neighborhood of $c$ in $M_1$ whose image is a subset of that open $\epsilon$-ball.


This is voiced:
: the limit of $\map f x$ as $x$ tends to $c$.

This is voiced:
: the limit of $\map f x$ as $x$ tends to $c$.",Definition:Limit of Mapping between Metric Spaces,"['Definitions/Limits of Mappings between Metric Spaces', 'Definitions/Limits of Mappings', 'Definitions/Metric Spaces']"
Definition:Limit,Limit,"Let $\openint a b$ be an open real interval.

Let $c \in \openint a b$.

Let $f: \openint a b \setminus \set c \to \R$ be a real function.

Let $L \in \R$.


=== Definition 1 ===
Let $\openint a b$ be an open real interval.

Let $c \in \openint a b$.

Let $f: \openint a b \setminus \set c \to \R$ be a real function.

Let $L \in \R$.


$\map f x$ tends to the limit $L$ as $x$ tends to $c$  if and only if :

:$\forall \epsilon \in \R_{>0}: \exists \delta \in \R_{>0}: \forall x \in \R: 0 < \size {x - c} < \delta \implies \size {\map f x - L} < \epsilon$

where $\R_{>0}$ denotes the set of strictly positive real numbers.

=== Definition 2 ===
Let $\openint a b$ be an open real interval.

Let $c \in \openint a b$.

Let $f: \openint a b \setminus \set c \to \R$ be a real function.

Let $L \in \R$.


$\map f x$ tends to the limit $L$ as $x$ tends to $c$  if and only if :
:$\forall \epsilon \in \R_{>0}: \exists \delta \in \R_{>0}: x \in \map {N_\delta} c \setminus \set c \implies \map f x \in \map {N_\epsilon} L$
where:
:$\map {N_\epsilon} L$ denotes the $\epsilon$-neighborhood of $L$
:$\map {N_\delta} c \setminus \set c$ denotes the deleted $\delta$-neighborhood of $c$
:$\R_{>0}$ denotes the set of strictly positive real numbers.


That is:
:For every (strictly) positive real number $\epsilon$, there exists a (strictly) positive real number $\delta$ such that every real number $x \ne c$ in the domain of $f$ within $\delta$ of $c$ has an image within $\epsilon$ of $L$.


$\epsilon$ is usually considered as having the connotation of being ""small"" in magnitude, but this is a misunderstanding of its intent: the point is that (in this context) $\epsilon$ can be made arbitrarily small.


:

It can directly be seen that this definition is the same as that for a general metric space.",Definition:Limit of Real Function,"['Definitions/Limits of Real Functions', 'Definitions/Limits of Mappings', 'Definitions/Limits', 'Definitions/Real Analysis']"
Definition:Limit,Limit,"The definition for the limit of a complex function is exactly the same as that for the general metric space.


Let $A_1, A_2 \subseteq \C$ be subsets of the complex plane.

Let $c$ be a limit point of $A_1$.

Let $f: A_1 \to A_2$ be a complex function from $A_1$ to $A_2$ defined everywhere on $A_1$ except possibly at $c$.


Let $L \in A_2$.


Then $\map f z$ is said to tend to the limit $L$ as $z$ tends to $c$, and we write:
:$\map f z \to L$ as $z \to c$

or
:$\ds \lim_{z \mathop \to c} \map f z = L$

if the following equivalent conditions hold.


This is voiced:
: the limit of $\map f z$ as $z$ tends to $c$.


=== Epsilon-Delta Condition ===

:$\forall \epsilon \in \R_{>0}: \exists \delta \in \R_{>0}: \forall z \in A_1: 0 < \cmod {z - c} < \delta \implies \cmod {\map f z - L} < \epsilon$


That is, for every real positive $\epsilon$ there exists a real positive $\delta$ such that every point in the domain of $f$ within $\delta$ of $c$ has an image within $\epsilon$ of some point $L$ in the codomain of $f$.


=== Epsilon-Neighborhood Condition ===

:$\forall \map {N_\epsilon} L: \exists \map {N_\delta} c \setminus \set c: \map f {\map {N_\delta} c \setminus \set c} \subseteq \map {N_\epsilon} L$

where:
:$\map {N_\delta} c \setminus \set c$ is the deleted $\delta $-neighborhood of $c$ in $M_1$;
:$\map {N_\epsilon} L$ is the $\epsilon$-neighborhood of $L$ in $M_2$.


That is, for every $\epsilon$-neighborhood of $L$ in $A_2$, there exists a deleted $\delta$-neighborhood of $c$ in $A_1$ whose image is a subset of that $\epsilon$-neighborhood.",Definition:Limit of Complex Function,"['Definitions/Limits of Complex Functions', 'Definitions/Limits of Mappings', 'Definitions/Limits', 'Definitions/Complex Analysis']"
Definition:Limit,Limit,"Let $\sequence {x_n}$ be a bounded sequence in $\R$.


=== Definition 1 ===
Let $\sequence {x_n}$ be a bounded sequence in $\R$.

Let $L$ be the set of all real numbers which are the limit of some subsequence of $\sequence {x_n}$.


From Existence of Maximum and Minimum of Bounded Sequence, $L$ has a maximum.

This maximum is called the limit superior.

It can be denoted:
:$\ds \map {\limsup_{n \mathop \to \infty} } {x_n} = \overline l$

=== Definition 2 ===
Let $\sequence {x_n}$ be a bounded sequence in $\R$.


The limit superior of $\sequence {x_n}$ is defined and denoted as:
:$\ds \map {\limsup_{n \mathop \to \infty} } {x_n} = \inf \set {\sup_{m \mathop \ge n} x_m: n \in \N}$",Definition:Limit Superior,"['Definitions/Limits Superior', 'Definitions/Real Analysis']"
Definition:Limit,Limit,"Let $\sequence {x_n}$ be a bounded sequence in $\R$.


=== Definition 1 ===
Let $\sequence {x_n}$ be a bounded sequence in $\R$.

Let $L$ be the set of all real numbers which are the limit of some subsequence of $\sequence {x_n}$.


From Existence of Maximum and Minimum of Bounded Sequence, $L$ has a minimum.

This minimum is called the limit inferior.

It can be denoted:
:$\ds \map {\liminf_{n \mathop \to \infty} } {x_n} = \underline l$

=== Definition 2 ===
Let $\sequence {x_n}$ be a bounded sequence in $\R$.


The limit inferior of $\sequence {x_n}$ is defined and denoted as:
:$\ds \map {\liminf_{n \mathop \to \infty} } {x_n} = \sup \set {\inf_{m \mathop \ge n} x_m: n \in \N}$",Definition:Limit Inferior,"['Definitions/Limits Inferior', 'Definitions/Real Analysis']"
Definition:Limit,Limit,"Let $\xi \in \R$ be a real number.

Let $\ds \map S x = \sum_{n \mathop = 0}^\infty a_n \paren {x - \xi}^n$ be a power series about $\xi$.

Let $I$ be the interval of convergence of $\map S x$.

Let the endpoints of $I$ be $\xi - R$ and $\xi + R$.

(This follows from the fact that $\xi$ is the midpoint of $I$.)


Then $R$ is called the radius of convergence of $\map S x$.


If $\map S x$ is convergent over the whole of $\R$, then $I = \R$ and thus the radius of convergence is infinite.",Definition:Radius of Convergence/Real Domain,"['Definitions/Convergence', 'Definitions/Power Series', 'Definitions/Real Analysis']"
Definition:Limit,Limit,"Let $\Bbb S = \set {E_n : n \in \N}$ be a sequence of sets.

Let the limit superior of $\Bbb S$ be equal to the limit inferior of $\Bbb S$.


Then the limit of $\Bbb S$, denoted $\ds \lim_{n \mathop \to \infty} E_n$, is defined as:

:$\ds \lim_{n \mathop \to \infty} E_n := \limsup_{n \mathop \to \infty} E_n$
and so also:
:$\ds \lim_{n \mathop \to \infty} E_n := \liminf_{n \mathop \to \infty} E_n$

and $\Bbb S$ converges to the limit.",Definition:Limit of Sets,['Definitions/Measure Theory']
Definition:Limit,Limit,"Let $\mathbf C$ be a metacategory.

Let $D: \mathbf J \to \mathbf C$ be a $\mathbf J$-diagram in $\mathbf C$.

Let $\mathbf{Cone} \left({D}\right)$ be the category of cones to $D$.


A limit for $D$ is a terminal object in $\mathbf{Cone} \left({D}\right)$.
 

It is denoted by $\varprojlim_j D_j$; the associated morphisms $p_i: \varprojlim_j D_j \to D_i$ are usually left implicit.


=== Finite Limit ===
Let $\mathbf C$ be a metacategory.

Let $D: \mathbf J \to \mathbf C$ be a $\mathbf J$-diagram in $\mathbf C$.

Let $\varprojlim_j D_j$ be a limit for $D$.


Then $\varprojlim_j D_j$ is called a finite limit  if and only if  $\mathbf J$ is a finite category.",Definition:Limit (Category Theory),"['Definitions/Category Theory', 'Definitions/Limits and Colimits']"
Definition:Limit,Limit,"Let $\struct {F, \norm {\,\cdot\,} }$ be a valued field.

Let $C = \sequence {a_n}_{n \mathop \ge 0}$ be a infinite continued fraction in $F$.


Let $C$ converge to $x \in F$:

Then $x$ is the value of $C$.",Definition:Value of Continued Fraction/Infinite,['Definitions/Continued Fractions']
Definition:Limit,Limit,"=== Definition 1 ===
An ordinal $\lambda$ is a limit ordinal  if and only if  it is a limit element in the well-ordering on the class of all ordinals $\On$ that is the subset relation.

=== Definition 2 ===
An ordinal $\lambda$ is a limit ordinal  if and only if  it is neither the zero ordinal nor a successor ordinal.",Definition:Limit Ordinal,"['Definitions/Ordinals', 'Definitions/Limit Ordinals']"
Definition:Limit,Limit,"Let $A$ be a class.

Let $\preccurlyeq$ be a well-ordering on $A$.

Let $x$ be neither the smallest element of $A$ nor an immediate successor of any element of $A$.


Then $x$ is a limit element of $A$ (under $\preccurlyeq$).",Definition:Limit Element under Well-Ordering,['Definitions/Well-Orderings']
Definition:Linear Algebra,Linear Algebra,Linear algebra is the branch of algebra which studies vector spaces and linear transformations between them.,Definition:Linear Algebra (Mathematical Branch),"['Definitions/Linear Algebra', 'Definitions/Algebra', 'Definitions/Linearity', 'Definitions/Branches of Mathematics']"
Definition:Linear Algebra,Linear Algebra,"Let $F$ be a field.


An algebra over $F$ is an ordered pair $\struct {A, *}$ where:
:$A$ is a vector space over $F$
:$* : A^2 \to A$ is a bilinear mapping


That is, it is an algebra $\struct {A, *}$ over the ring $F$ where:
:$F$ is a field
:the $F$-module $A$ is a vector space.


The symbol $A$ is often used for such an algebra, more so as the level of abstraction increases.


=== Multiplication ===
Let $F$ be a field.

Let $\struct {A, *}$ be an algebra over $F$ such that:
:$A$ is a vector space over $F$
:$* : A^2 \to A$ is a bilinear mapping.


The bilinear mapping $*$ is often referred to as multiplication.",Definition:Algebra over Field,"['Definitions/Algebras over Fields', 'Definitions/Algebras', 'Definitions/Field Theory']"
Definition:Linear Form,Linear Form,"Let $\struct {R, +, \times}$ be a commutative ring.

Let $\struct {R, +_R, \circ}_R$ denote the $R$-module $R$.

Let $\struct {G, +_G, \circ}_R$ be a module over $R$.


Let $\phi: \struct {G, +_G, \circ}_R \to \struct {R, +_R, \circ}_R$ be a linear transformation from $G$ to the $R$-module $R$.


$\phi$ is called a linear form on $G$.",Definition:Linear Form (Linear Algebra),"['Definitions/Linear Forms (Linear Algebra)', 'Definitions/Linear Algebra', 'Definitions/Linear Transformations', 'Definitions/Linear Forms', 'Definitions/Linearity']"
Definition:Linear Form,Linear Form,A linear form is a form whose variables are of degree $1$.,Definition:Linear Form (Polynomial Theory),"['Definitions/Linear Forms (Polynomial Theory)', 'Definitions/Forms', 'Definitions/Polynomial Theory', 'Definitions/Linear Forms', 'Definitions/Linearity']"
Definition:Locally Finite,Locally Finite,"Let $T = \struct {S, \tau}$ be a topological space.

Let $\FF$ be a set of subsets of $S$.


Then $\FF$ is locally finite  if and only if  each element of $S$ has a neighborhood which intersects a finite number of sets in $\FF$.",Definition:Locally Finite Set of Subsets,['Definitions/Topology']
Definition:Locally Finite,Locally Finite,"Let $T = \struct {S, \tau}$ be a topological space.

Let $\CC$ be a cover of $S$.


Then $\CC$ is locally finite  if and only if  each element of $S$ has a neighborhood which intersects a finite number of sets in $\CC$.",Definition:Locally Finite Cover,['Definitions/Covers']
Definition:Locally Finite,Locally Finite,A locally finite graph $G$ is an infinite graph where every vertex of $G$ has finite degree.,Definition:Locally Finite Graph,['Definitions/Graph Theory']
Definition:Loop,Loop,"A loop is a part of a plane curve that intersects itself.

Hence it encloses a bounded set of points.",Definition:Loop (Plane Geometry),"['Definitions/Loops (Plane Geometry)', 'Definitions/Plane Curves']"
Definition:Loop,Loop,"Let $G = \struct {V, E}$ be a loop-graph.

A loop is an edge $e$ of $G$ whose endvertices are the same vertex.


Thus a loop $e$ on the vertex $v$ would be written:
:$e = vv$",Definition:Loop (Graph Theory),"['Definitions/Loops (Graph Theory)', 'Definitions/Loop-Graphs']"
Definition:Loop,Loop,"An algebra loop $\struct {S, \circ}$ is a quasigroup with an identity element.
:$\exists e \in S: \forall x \in S: x \circ e = x = e \circ x$",Definition:Algebra Loop,['Definitions/Abstract Algebra']
Definition:Loop,Loop,"Let $\RR$ be a relation on a set $S$.

Let $a_1, a_2, \ldots a_n$ be elements of $S$.


A relational loop on $S$ takes the form:

:$\tuple {a_1 \mathrel \RR a_2 \land a_2 \mathrel \RR a_3 \dots \land a_{n - 1} \mathrel \RR a_n \land a_n \mathrel \RR a_1}$

That is, it is a subset of $\RR$ of the form:
:$\set {\tuple {a_1, a_2}, \tuple {a_2, a_3}, \ldots, \tuple {a_{n - 1}, a_n}, \tuple {a_n, a_1} }$",Definition:Relational Loop,['Definitions/Relation Theory']
Definition:Loop,Loop,"Let $T = \struct {S, \tau}$ be a topological space.

Let $\gamma: \closedint 0 1 \to S$ be a path in $T$.

Let $\map \gamma 0 = \map \gamma 1$.


Then $\gamma$ is called a loop (in $T$).


=== Simple Loop ===
Let $T = \struct {S, \tau}$ be a topological space.

Let $\gamma: \closedint 0 1 \to S$ be a path in $T$.


$\gamma$ is a simple loop (in $T$)  if and only if :
:$\map \gamma {t_1} \ne \map \gamma {t_2}$ for all $t_1 ,t_2 \in \hointr 0 1$ with $t_1 \ne t_2$
:$\map \gamma 0 = \map \gamma 1$

=== Base Point ===
Let $X$ be a topological space.

Let $\gamma: \closedint 0 1 \to X$ be a loop.


The base point of $\gamma$ is $\map \gamma 0$.

In other words, $\gamma$ is said to be based at $\map \gamma 0$.

=== Set of All Loops ===
Let $T$ be a topological space.


The set of all loops based at $p \in T$ is denoted by $\map \Omega {T, p}$.

=== Constant Loop ===
Let $T$ be a topological space.

Let $p \in T$.

Let $\map \Omega {T, p}$ denote the set of all loops based at $p$.


A constant loop $c_p$ is the loop $c_p \in \map \Omega {T, p}$ such that:

:$\forall t \in \closedint 0 1 : \map {c_p} t = p$

=== Null-Homotopic Loop ===
Let $T = \struct {S, \tau}$ be a topological space.

Let $\gamma$ be a loop in $T$.

Suppose $\gamma$ is path-homotopic to a constant loop.


Then $\gamma$ is said to be null-homotopic.

=== Circle Representative of Loop ===
Let $T = \struct {S, \tau}$ be a topological space.

Let $\gamma: \closedint 0 1 \to S$ be a loop in $T$.

Let $\Bbb S^1 \subseteq \C$ be the unit circle in $\C$:

:$\Bbb S^1 = \set {z \in \C : \size z = 1}$

Suppose $\omega : \closedint 0 1 \to \Bbb S^1$ such that $\map \omega s = \map \exp {2 \pi i s}$.


Then the unique map $\tilde f : \Bbb S^1 \to T$ such that $\tilde f \circ \omega = f$ is called the circle representative of $f$.

=== Loop in Topological Manifold ===
Let $M$ be a topological manifold.

Let $\sigma : \closedint 0 1 \to M$ be a continuous path.

Let $\map \sigma 0 = \map \sigma 1$.


Then $\sigma$ is called a loop.",Definition:Loop (Topology),"['Definitions/Loops (Topology)', 'Definitions/Topology']"
Definition:Loop,Loop,"Let $M = \struct {S, \mathscr I}$ be a matroid.


A loop of $M$ is an element $x$ of $S$ such that $\set x$ is a dependent subset of $S$.

That is, $x \in S$ is a loop  if and only if  $\set x \not \in \mathscr I$.",Definition:Loop (Matroid),['Definitions/Matroid Theory']
Definition:Lower Bound,Lower Bound,"Let $\struct {S, \preceq}$ be an ordered set.

Let $T$ be a subset of $S$.


A lower bound for $T$ (in $S$) is an element $m \in S$ such that:
:$\forall t \in T: m \preceq t$

That is, $m$ precedes every element of $T$.


=== Subset of Real Numbers ===

The concept is usually encountered where $\struct {S, \preceq}$ is the set of real numbers under the usual ordering $\struct {\R, \le}$:

Let $\R$ be the set of real numbers.

Let $T$ be a subset of $S$.


A lower bound for $T$ (in $\R$) is an element $m \in \R$ such that:
:$\forall t \in T: m \le t$",Definition:Lower Bound of Set,['Definitions/Boundedness']
Definition:Lower Bound,Lower Bound,"Let $\R$ be the set of real numbers.

Let $T$ be a subset of $S$.


A lower bound for $T$ (in $\R$) is an element $m \in \R$ such that:
:$\forall t \in T: m \le t$",Definition:Lower Bound of Set/Real Numbers,['Definitions/Boundedness']
Definition:Lower Bound,Lower Bound,"Let $f: S \to T$ be a mapping whose codomain is an ordered set $\struct {T, \preceq}$.


Let $f$ be bounded below in $T$ by $H \in T$.


Then $H$ is a lower bound of $f$.


=== Real-Valued Function ===

The concept is usually encountered where $\struct {T, \preceq}$ is the set of real numbers under the usual ordering $\struct {\R, \le}$:

Let $f: S \to \R$ be a real-valued function.


Let $f$ be bounded below in $T$ by $L \in T$.


Then $L$ is a lower bound of $f$.


=== Lower Bound of Number ===
When considering the lower bound of a set of numbers, it is commonplace to ignore the set and instead refer just to the number itself.

Thus the construction:

:The set of numbers which fulfil the propositional function $P \left({n}\right)$ is bounded below with the lower bound $N$

would be reported as:

:The number $n$ such that $P \left({n}\right)$ has the lower bound $N$.


This construct obscures the details of what is actually being stated. Its use on   is considered an abuse of notation and so discouraged.


This also applies in the case where it is the lower bound of a mapping which is under discussion.


Category:Definitions/Numbers",Definition:Lower Bound of Mapping,['Definitions/Boundedness']
Definition:Lower Bound,Lower Bound,"Let $f: S \to \R$ be a real-valued function.


Let $f$ be bounded below in $T$ by $L \in T$.


Then $L$ is a lower bound of $f$.


=== Lower Bound of Number ===
When considering the lower bound of a set of numbers, it is commonplace to ignore the set and instead refer just to the number itself.

Thus the construction:

:The set of numbers which fulfil the propositional function $P \left({n}\right)$ is bounded below with the lower bound $N$

would be reported as:

:The number $n$ such that $P \left({n}\right)$ has the lower bound $N$.


This construct obscures the details of what is actually being stated. Its use on   is considered an abuse of notation and so discouraged.


This also applies in the case where it is the lower bound of a mapping which is under discussion.


Category:Definitions/Numbers",Definition:Lower Bound of Mapping/Real-Valued,['Definitions/Boundedness']
Definition:Lower Bound,Lower Bound,"A special case of a lower bound of a mapping is a lower bound of a sequence, where the domain of the mapping is $\N$.

Let $\struct {T, \preceq}$ be an ordered set.

Let $\sequence {x_n}$ be a sequence in $T$.


Let $\sequence {x_n}$ be bounded below in $T$ by $L \in T$.


Then $L$ is a lower bound of $\sequence {x_n}$.


=== Real Sequence ===

The concept is usually encountered where $\struct {T, \preceq}$ is the set of real numbers under the usual ordering $\struct {\R, \le}$:

Let $\sequence {x_n}$ be a real sequence.


Let $\sequence {x_n}$ be bounded below in $T$ by $L \in \R$.


Then $L$ is a lower bound of $\sequence {x_n}$.


=== Lower Bound of Number ===
When considering the lower bound of a set of numbers, it is commonplace to ignore the set and instead refer just to the number itself.

Thus the construction:

:The set of numbers which fulfil the propositional function $P \left({n}\right)$ is bounded below with the lower bound $N$

would be reported as:

:The number $n$ such that $P \left({n}\right)$ has the lower bound $N$.


This construct obscures the details of what is actually being stated. Its use on   is considered an abuse of notation and so discouraged.


This also applies in the case where it is the lower bound of a mapping which is under discussion.


Category:Definitions/Numbers",Definition:Lower Bound of Sequence,['Definitions/Boundedness']
Definition:Lower Bound,Lower Bound,"Let $\sequence {x_n}$ be a real sequence.


Let $\sequence {x_n}$ be bounded below in $T$ by $L \in \R$.


Then $L$ is a lower bound of $\sequence {x_n}$.


=== Lower Bound of Number ===
When considering the lower bound of a set of numbers, it is commonplace to ignore the set and instead refer just to the number itself.

Thus the construction:

:The set of numbers which fulfil the propositional function $P \left({n}\right)$ is bounded below with the lower bound $N$

would be reported as:

:The number $n$ such that $P \left({n}\right)$ has the lower bound $N$.


This construct obscures the details of what is actually being stated. Its use on   is considered an abuse of notation and so discouraged.


This also applies in the case where it is the lower bound of a mapping which is under discussion.


Category:Definitions/Numbers",Definition:Lower Bound of Sequence/Real,"['Definitions/Boundedness', 'Definitions/Sequences']"
Definition:Machine,Machine,"A machine, in the context of mechanics, is a physical artefact which takes energy in a particular form and converts it into energy in another form for a specific purpose.",Definition:Machine (Mechanics),['Definitions/Machines']
Definition:Machine,Machine,"A finite state machine is an ordered tuple:

: $F = \left({ S, A, I, \Sigma, T }\right)$

where:

: $S$ is the (finite) set of states
: $A \subseteq S$ is the set of accepting states
: $I \in S$ is the initial state
: $\Sigma$ is the alphabet of symbols that can be fed into the machine
: $T : \left({ S \times \Sigma }\right) \rightarrow S$ is the transition function.


A finite state machine operates as follows:

:$(1): \quad$ At the beginning, the current state $s$ of the finite state machine is $I$.
:$(2): \quad$ One by one, the input (a sequence of symbols from $\Sigma$) is fed into the machine.
:$(3): \quad$ After each input symbol $\sigma$, the current state $s$ is set to the result of $T\left({s, \sigma}\right)$.


If, at the end of processing an input word $w$, $s \in A$, the finite state machine is said to accept $w$, otherwise to reject it.


The set of words $w$ accepted by the machine $F$ is called the accepted language $L\left({F}\right)$.

Category:Definitions/Abstract Machines",Definition:Finite State Machine,['Definitions/Abstract Machines']
Definition:Machine,Machine,"An unlimited register machine, abbreviated URM, is an abstract machine with the following characteristics:


=== Registers ===
A URM has a sequence of registers which can store natural numbers: $\set {0, 1, 2, \ldots}$.

Any given URM program may make use of only a finite number of these registers.


Registers are usually referred to by the subscripted uppercase letters $R_1, R_2, R_3, \ldots$.

The number held at any one time by a register is usually referred to by the corresponding lowercase letter $r_1, r_2, r_3, \ldots$.


The registers are unlimited in the following two senses:
:$(1): \quad$ Although a URM program may make use of only a finite number of registers, there is no actual upper bound on how many a particular URM program can actually use.
:$(2): \quad$ There is no upper bound on the size of the natural numbers that may be stored in any register.


=== Index of Register ===
The subscript (which is a natural number) appended to a URM register is called the index of that register.

Hence, for example, the index of register $R_5$ is $5$.

=== Program ===
The numbers held in the registers of a URM are manipulated according to a program.

A URM program is a finite sequence of basic instructions.


=== Basic Instruction ===
The basic instructions of a URM program form a finite sequence and hence can be considered a set indexed by the (positive) integer $1, 2, 3, \ldots$.


The basic instructions are as follows:


Name     	Notation          	Effect                     	Description
Zero     	$\map Z n$        	$0 \to R_n$                	Replace the number in $R_n$ by $0$.
Successor	$\map S n$        	$r_n + 1 \to R_n$          	Add $1$ to the number in $R_n$.
Copy     	$\map C {m, n}$   	$r_m \to R_n$              	Replace the number in $R_n$ by the number in $R_m$ (leaving the one in $R_m$ as it was).
Jump     	$\map J {m, n, q}$	$r_m = r_n ? \Rightarrow q$	If the numbers in $R_m$ and $R_n$ are equal, go to instruction number $q$, otherwise go to the next instruction.



=== Execution ===
The operation of carrying out a basic URM instruction is referred to as execution.

=== Execution ===
The operation of carrying out a basic URM instruction is referred to as execution.

=== Line Number ===
For historical reasons, the index of an instruction in a given URM program is called its line number.

We can refer either to the line of the program or the line in the URM.

=== Set of All URM Programs ===
It is convenient to use $\Bbb U$ to stand for the set of all URM programs.

=== Length of Program ===
Let $\Bbb U$ denote the set of all URM programs.

Let $P \in \Bbb U$ be a URM program.

By definition, $P$ is a finite sequence of basic instructions.


We define the function $\lambda: \Bbb U \to \N$ as follows:
:$\forall P \in \Bbb U: \map \lambda P = $ the number of basic instructions that comprise $P$

Thus $\map \lambda P$ is referred to as the length of $P$.

=== Highest Register ===
Let $\Bbb U$ denote the set of all URM programs.

Let $P \in \Bbb U$ be a URM program.

By definition, $P$ uses a finite number of registers.


We define the function $\rho: \Bbb U \to \N$ as follows:
:$\forall P \in \Bbb U: \map \rho P =$ the highest register number used by $P$

That is, in any URM program $P$, no instruction refers to any register with index greater than $\map \rho P$.

=== Highest Register ===
Let $\Bbb U$ denote the set of all URM programs.

Let $P \in \Bbb U$ be a URM program.

By definition, $P$ uses a finite number of registers.


We define the function $\rho: \Bbb U \to \N$ as follows:
:$\forall P \in \Bbb U: \map \rho P =$ the highest register number used by $P$

That is, in any URM program $P$, no instruction refers to any register with index greater than $\map \rho P$.

=== Termination ===
A URM program terminates when there are no more instructions to execute.

This can happen in either of two ways:
:$(1): \quad$ If the program executes the last instruction, and this does not involve a Jump to an earlier instruction, the program will stop.
:$(2): \quad$ If the program executes a Jump instruction to a non-existent instruction, the program will stop.


=== Exit Jump ===
 

Such a Jump instruction is known as an exit jump .

=== Exit Line ===
 

The line on which a particular run of a URM program stops is called the exit line.


=== Endless Loop ===
 

If a URM program, when running, never reaches a state where it terminates, then it is said to be in an endless loop and will never terminate.

 

Note that whether a program terminates or not may depend on its input.

It may terminate perfectly well for one input, but go into an endless loop on another.

=== Input ===

The input to a URM program is:
:either an ordered $k$-tuple $\tuple {n_1, n_2, \ldots, n_k} \in \N^k$
:or a natural number $n \in \N$.


In the latter case, it is convenient to consider a single natural number as an ordered $1$-tuple $\tuple {n_1} \in \N^1 = \N$.

Hence we can discuss inputs to URM programs solely as instances of tuples, and not be concerned with cumbersome repetition for the cases where $k = 1$ and otherwise.


The convention usually used is for a URM program $P$ to start computation with:
:the input $\left({n_1, n_2, \ldots, n_k}\right)$ in registers $R_1, R_2, \ldots, R_k$
:$0$ in all other registers used by $P$.


That is, the initial state of the URM is:
:$\forall i \in \closedint 1 k: r_i = n_i$
:$\forall i > k: r_i = 0$.


It is usual for the input (either all or part) to be overwritten during the course of the operation of a program. That is, at the end of a program, $R_1, R_2, \ldots, R_k$ are not guaranteed still to contain $n_1, n_2, \ldots, n_k$ unless the program has been explicitly written so as to ensure that this is the case.


=== Output ===

At the end of the running of a URM program, the output will be found in register $R_1$.

=== Operation ===
When a URM runs a program, it always starts by executing the first instruction of the program.

When it has executed an instruction, it moves to the next instruction and executes that one, unless required otherwise by a Jump instruction.


=== Execution ===
The operation of carrying out a basic URM instruction is referred to as execution.

=== Instruction Pointer ===
The line number of the instruction which is currently about to be executed is known as the instruction pointer.

It can be imagined as a special-purpose register in the URM whose purpose is to hold that line number.

=== Stage of Computation ===
The stage of computation (or just stage) of a URM program is the count of how many instructions have been executed.

Thus each stage corresponds to the processing of one instruction.

=== State ===
The state of a URM program at a particular stage is defined as:
:$(1): \quad$ the value of the instruction pointer
:$(2): \quad$ the values contained by each of the registers that are used by the URM program.

=== Instruction Pointer ===
The line number of the instruction which is currently about to be executed is known as the instruction pointer.

It can be imagined as a special-purpose register in the URM whose purpose is to hold that line number.

=== Stage of Computation ===
The stage of computation (or just stage) of a URM program is the count of how many instructions have been executed.

Thus each stage corresponds to the processing of one instruction.

=== State ===
The state of a URM program at a particular stage is defined as:
:$(1): \quad$ the value of the instruction pointer
:$(2): \quad$ the values contained by each of the registers that are used by the URM program.

=== Input ===

The input to a URM program is:
:either an ordered $k$-tuple $\tuple {n_1, n_2, \ldots, n_k} \in \N^k$
:or a natural number $n \in \N$.


In the latter case, it is convenient to consider a single natural number as an ordered $1$-tuple $\tuple {n_1} \in \N^1 = \N$.

Hence we can discuss inputs to URM programs solely as instances of tuples, and not be concerned with cumbersome repetition for the cases where $k = 1$ and otherwise.


The convention usually used is for a URM program $P$ to start computation with:
:the input $\left({n_1, n_2, \ldots, n_k}\right)$ in registers $R_1, R_2, \ldots, R_k$
:$0$ in all other registers used by $P$.


That is, the initial state of the URM is:
:$\forall i \in \closedint 1 k: r_i = n_i$
:$\forall i > k: r_i = 0$.


It is usual for the input (either all or part) to be overwritten during the course of the operation of a program. That is, at the end of a program, $R_1, R_2, \ldots, R_k$ are not guaranteed still to contain $n_1, n_2, \ldots, n_k$ unless the program has been explicitly written so as to ensure that this is the case.


=== Output ===

At the end of the running of a URM program, the output will be found in register $R_1$.


=== Null Program ===
The null URM program is a URM program which contains no instructions.

That is, a URM program whose length is zero.",Definition:Unlimited Register Machine,"['Definitions/Examples of Abstract Machines', 'Definitions/Unlimited Register Machines']"
Definition:Machine,Machine,"A Turing machine is an abstract machine which works by manipulating symbols on an imaginary piece of paper by means of a specific set of algorithmic rules.

To simplify things, the piece of paper being worked on is in the form of a series of boxes on a one-dimensional ""tape"" divided into squares.

Each square can be either blank or can contain a symbol taken from a finite set, e.g. $s_1, s_2, \ldots, s_\alpha$.


The machine examines one square at a time, and carries out an action determined by both:
:$(1): \quad$ the symbol in the square
:$(2): \quad$ the current internal state of the machine.

The internal state of the machine is a way of providing a device that can keep track of the symbols in other squares.

There can be only a finite set of these states, say $q_1, q_2, \ldots, q_\beta$.


The actions that the machine can take are as follows:
:$(1): \quad$ Replace the symbol in the square with another symbol
:$(2): \quad$ Move to examine the square in the immediate left of the current square being looked at
:$(3): \quad$ Move to examine the square in the immediate right of the current square being looked at.

After carrying out an action, the machine may change to a different internal state.


The program for the machine is a set of instructions which specify:
:$(1): \quad$ what action to take in some possible combinations of the internal state and symbol in the square currently being read
:$(2): \quad$ which internal state the machine moves into after carrying out that action.

Thus the instructions have the following form:
:$q_i \quad s_j \quad A \quad q_t$
which is interpreted as:

""If:
:* the machine is in internal state $q_i$
: the symbol in the square currently being examined is $s_j$
then:
: Carry out action $A$
: Move into internal state $q_t$.


The actions can be abbreviated to:
: $L$: Move one square to the left
: $R$: Move one square to the right
: $s_k$: Replace the symbol in the square currently being read with symbol $s_k$.


The computation stops when there is no instruction which specifies what should be done in the current combination of internal state and symbol being read.

=== Formal Definition ===
 

A Turing machine is a 7-tuple $\paren {Q, \Sigma, \Gamma, \delta, q_0, B, F}$ that satisfies the following:
* $Q$ is a finite set, the states of the machine.
* $\Sigma$ is a finite set, the input symbols.
* $\Gamma \supsetneq \Sigma$ is a finite superset of the input symbols, called the tape symbols.
** For convenience, we also require that $\Gamma$ and $Q$ are disjoint.
* $\delta : Q \times \Gamma \to Q \times \Gamma \times \set {L, R}$ is a partial mapping, the transition function.
** $L$ and $R$ are arbitrary constants called directions.
* $q_0 \in Q$ is a distinguished state called the start state.
* $B \in \Gamma$ is a distinguished tape symbol called the blank symbol. $B$ must not be an element of $\Sigma$.
* $F \subset Q$ be a designated subset of the states called accepting states.


An instantaneous description of a Turing machine is a finite sequence of elements of $\Gamma \cup Q$, subject to the following conditions:
* There is exactly one element of $Q$ in the sequence.
* The first entry in the sequence is not $B$.
* The last entry in the sequence is not in $Q$.
* If the last entry in the sequence is $B$, then the second-to-last is in $Q$.

By this definition, an instantaneous description can always be written as:
:$X_m X_{m-1} \dotsm X_2 X_1 q Y Z_1 Z_2 \dotsm Z_{n-1} Z_n$
where $m$ or $n$ may be $0$; $X_i$, $Y$, and $Z_j$ are all elements of $\Gamma$; and $q$ is an element of $Q$.

Additionally, $X_m$ and $Z_n$ are not $B$ if they exist; that is, if $m$ and $n$ are not $0$, respectively.


A move reduces one instantaneous description into another by applying the transition function.

We write $A \vdash B$ if a machine with instantaneous description $A$ has, after a single move, instantaneous description $B$.

Let $\map \delta {q, Y} = \paren{q', Y', d}$.

Then there are seven cases to consider:

* If $d = L$ and $m > 0$, and either $n > 0$ or $Y' \neq B$ then:
:$X_m \dotsm X_2 X_1 q Y Z_1 \dotsm Z_n \vdash X_m \dotsm X_2 q' X_1 Y' Z_1 \dotsm Z_n$

* If $d = L$ and $m > 0$, but $n = 0$ and $Y' = B$ then:
:$X_m \dotsm X_2 X_1 q Y \vdash X_m \dotsm X_2 q' X_1$

* If $d = L$ but $m = 0$, and either $n > 0$ or $Y' \neq B$ then:
:$q Y Z_1 \dotsm Z_n \vdash q' B Y' Z_1 \dotsm Z_n$

* If $d = R$ and $n > 0$, and either $m > 0$ or $Y' \neq B$ then:
:$X_m \dotsm X_1 q Y Z_1 Z_2 \dotsm Z_n \vdash X_m \dotsm X_1 Y' q' Z_1 Z_2 \dotsm Z_n$

* If $d = R$ and $n > 0$, but $m = 0$ and $Y' = B$ then:
:$q Y Z_1 Z_2 \dotsm Z_n \vdash q' Z_1 Z_2 \dotsm Z_n$

* If $d = R$ but $n = 0$, and either $m > 0$ or $Y' \neq B$ then:
:$X_m \dotsm X_1 q Y \vdash X_m \dotsm X_1 Y' q' B$

* If $m = 0$, $n = 0$, and $Y' = B$ then regardless of $d$:
:$q Y \vdash q' B$


Let $A \vdash^* B$ indicate that there exists a finite sequence $\sequence {A_i}_{0 \leq i \leq n}$ such that:
:$A = A_0 \vdash A_1 \vdash A_2 \vdash \dotso \vdash A_n = B$


The language $\map L M$ accepted by the machine $M$ is the set of strings $w \in \Sigma^*$ for which, for some $\alpha, \beta \in \Gamma^*$ and $p \in F$:
:$q_0 w \vdash^* \alpha p \beta$

As a special case, the null string is in the language  if and only if  the above holds for $w = B$.


A machine $M$ halts on an input $w$, using the same special case for the null string as above,  if and only if  for some $\alpha, \beta \in \Gamma^*$, $X \in \Gamma$, and $q \in Q$:
:$q_0 w \vdash^* \alpha q X \beta$
where $\map \delta {q, X}$ is undefined.",Definition:Turing Machine,"['Definitions/Turing Machines', 'Definitions/Examples of Abstract Machines']"
Definition:Machine,Machine,"A Nondeterministic (or Non-deterministic) Turing Machine (NTM) is a variation of the classical Turing machine that relaxes the restriction that all the steps in the machine must be definite.

That is, given a single internal state and a single character being read on the tape the machine may have more than one possible response.

If any sequence of choices puts the machine into a halting state, then the machine stops after $n$ steps, where $n$ is the minimum number of steps needed to put the machine into a halting state.

 

=== Formal Definition ===

A nondeterministic Turing machine is a 7-tuple $\paren {Q, \Sigma, \Gamma, \delta, q_0, B, F}$ that satisfies the following:
* $Q$ is a finite set, the states of the machine.
* $\Sigma$ is a finite set, the input symbols.
* $\Gamma \supsetneq \Sigma$ is a finite superset of the input symbols, called the tape symbols.
** For convenience, we also require that $\Gamma$ and $Q$ are disjoint.
* $\delta : Q \times \Gamma \to \powerset {Q \times \Gamma \times \set {L, R} }$ is a mapping, the transition function.
** $L$ and $R$ are arbitrary constants called directions.
* $q_0 \in Q$ is a distinguished state called the start state.
* $B \in \Gamma$ is a distinguished tape symbol called the blank symbol. $B$ must not be an element of $\Sigma$.
* $F \subset Q$ be a designated subset of the states called accepting states.


An instantaneous description of a nondeterministic Turing machine is a finite sequence of elements of $\Gamma \cup Q$, subject to the following conditions:
* There is exactly one element of $Q$ in the sequence.
* The first entry in the sequence is not $B$.
* The last entry in the sequence is not in $Q$.
* If the last entry in the sequence is $B$, then the second-to-last is in $Q$.

By this definition, an instantaneous description can always be written as:
:$X_m X_{m-1} \dotsm X_2 X_1 q Y Z_1 Z_2 \dotsm Z_{n-1} Z_n$
where $m$ or $n$ may be $0$; $X_i$, $Y$, and $Z_j$ are all elements of $\Gamma$; and $q$ is an element of $Q$.

Additionally, $X_m$ and $Z_n$ are not $B$ if they exist; that is, if $m$ and $n$ are not $0$, respectively.


A move reduces one instantaneous description into another by applying the transition function. There may be many possible moves from any given instantaneous description.

We write $A \vdash B$ if a machine with instantaneous description $A$ may have, after a single move, instantaneous description $B$.

Let $\paren{q', Y', d} \in \map \delta {q, Y}$.

Then there are seven cases to consider:

* If $d = L$ and $m > 0$, and either $n > 0$ or $Y' \neq B$ then:
:$X_m \dotsm X_2 X_1 q Y Z_1 \dotsm Z_n \vdash X_m \dotsm X_2 q' X_1 Y' Z_1 \dotsm Z_n$

* If $d = L$ and $m > 0$, but $n = 0$ and $Y' = B$ then:
:$X_m \dotsm X_2 X_1 q Y \vdash X_m \dotsm X_2 q' X_1$

* If $d = L$ but $m = 0$, and either $n > 0$ or $Y' \neq B$ then:
:$q Y Z_1 \dotsm Z_n \vdash q' B Y' Z_1 \dotsm Z_n$

* If $d = R$ and $n > 0$, and either $m > 0$ or $Y' \neq B$ then:
:$X_m \dotsm X_1 q Y Z_1 Z_2 \dotsm Z_n \vdash X_m \dotsm X_1 Y' q' Z_1 Z_2 \dotsm Z_n$

* If $d = R$ and $n > 0$, but $m = 0$ and $Y' = B$ then:
:$q Y Z_1 Z_2 \dotsm Z_n \vdash q' Z_1 Z_2 \dotsm Z_n$

* If $d = R$ but $n = 0$, and either $m > 0$ or $Y' \neq B$ then:
:$X_m \dotsm X_1 q Y \vdash X_m \dotsm X_1 Y' q' B$

* If $m = 0$, $n = 0$, and $Y' = B$ then regardless of $d$:
:$q Y \vdash q' B$


Let $A \vdash^* B$ indicate that there exists a finite sequence $\sequence {A_i}_{0 \leq i \leq n}$ such that:
:$A = A_0 \vdash A_1 \vdash A_2 \vdash \dotso \vdash A_n = B$


The language $\map L M$ accepted by the machine $M$ is the set of strings $w \in \Sigma^*$ for which, for some $\alpha, \beta \in \Gamma^*$ and $p \in F$:
:$q_0 w \vdash^* \alpha p \beta$

As a special case, the null string is in the language  if and only if  the above holds for $w = B$.",Definition:Nondeterministic Turing Machine,['Definitions/Turing Machines']
Definition:Major,Major,"Let $a, b \in \R_{>0}$ in the forms:
:$a = \dfrac \rho {\sqrt 2} \sqrt {1 + \dfrac k {\sqrt {1 + k^2} } }$
:$b = \dfrac \rho {\sqrt 2} \sqrt {1 - \dfrac k {\sqrt {1 + k^2} } }$

where:
: $\rho$ is a rational number
: $k$ is a rational number whose square root is irrational.


Then $a + b$ is a major.


 ",Definition:Major (Euclidean),['Definitions/Euclidean Number Theory']
Definition:Major,Major,":


Consider an ellipse $K$ whose foci are $F_1$ and $F_2$.


=== Definition $1$ ===
:


Consider an ellipse $K$ whose foci are $F_1$ and $F_2$.

The major axis of $K$ is the line segment passing through both $F_1$ and $F_2$ whose endpoints are where it intersects $K$.


In the above diagram, $V_1 V_2$ is the major axis of $K$.

=== Definition $2$ ===
:


Consider an ellipse $K$ whose foci are $F_1$ and $F_2$.

The major axis of $K$ is the diameter of $K$ which has the greatest length.


In the above diagram, $V_1 V_2$ is the major axis of $K$.

In the above diagram, $V_1 V_2$ is the major axis of $K$.


=== Semi-Major Axis ===
:

Consider an ellipse $K$.


A semi-major axis of $K$ is either half of the major axis of $K$ from its midpoint to its endpoint.


In the above diagram, $O V_1$ and $O V_2$ are the semi-major axes of $K$.",Definition:Ellipse/Major Axis,"['Definitions/Major Axis of Ellipse', 'Definitions/Major Axis', 'Definitions/Ellipses']"
Definition:Major,Major,":

Consider an ellipse $K$.


A semi-major axis of $K$ is either half of the major axis of $K$ from its midpoint to its endpoint.


In the above diagram, $O V_1$ and $O V_2$ are the semi-major axes of $K$.",Definition:Ellipse/Major Axis/Semi-Major Axis,"['Definitions/Semi-Major Axis of Ellipse', 'Definitions/Major Axis', 'Definitions/Ellipses']"
Definition:Major,Major,"Consider the lemniscate of Bernoulli defined as the locus $M$ described by the equation:
:$P_1 M \times P_2 M = \paren {\dfrac {P_1 P_2} 2}^2$


:


The line $P_1 P_2$ is the major axis of the lemniscate.


Category:Definitions/Lemniscate of Bernoulli",Definition:Lemniscate of Bernoulli/Major Axis,['Definitions/Lemniscate of Bernoulli']
Definition:Major,Major,"Consider the lemniscate of Bernoulli defined as the locus $M$ described by the equation:
:$P_1 M \times P_2 M = \paren {\dfrac {P_1 P_2} 2}^2$
where $O$ is the point at the center where the branches cross.


:


Each of the lines $O P_1$ and $O P_2$ is a major semiaxis of the lemniscate.


Category:Definitions/Lemniscate of Bernoulli",Definition:Lemniscate of Bernoulli/Major Semiaxis,['Definitions/Lemniscate of Bernoulli']
Definition:Major,Major,"The major premise of a categorical syllogism is conventionally stated first.

It is a categorical statement which expresses the logical relationship between the primary term and the middle term of the syllogism.",Definition:Categorical Syllogism/Premises/Major Premise,['Definitions/Categorical Syllogisms']
Definition:Manifold,Manifold,"Let $M$ be a Hausdorff second-countable locally Euclidean space of dimension $d$. 


Then $M$ is a topological manifold of dimension $d$.


=== Differentiable Manifold ===
Let $M$ be a second-countable locally Euclidean space of dimension $d$. 

Let $\mathscr F$ be a $d$-dimensional differentiable structure on $M$ of class $\CC^k$, where $k \ge 1$.


Then $\struct {M, \mathscr F}$ is a differentiable manifold of class $\CC^k$ and dimension $d$.

=== Smooth Manifold ===
Let $M$ be a second-countable locally Euclidean space of dimension $d$. 

Let $\mathscr F$ be a smooth differentiable structure on $M$.


Then $\struct {M, \mathscr F}$ is called a smooth manifold of dimension $d$.

=== Complex Manifold ===
Let $M$ be a second-countable, complex locally Euclidean space of dimension $d$. 

Let $\mathscr F$ be a complex analytic differentiable structure on $M$.


Then $\struct {M, \mathscr F}$ is called a complex manifold of dimension $d$.",Definition:Topological Manifold,"['Definitions/Topological Manifolds', 'Definitions/Manifolds', 'Definitions/Topology']"
Definition:Manifold,Manifold,"Let $M$ be a second-countable locally Euclidean space of dimension $d$. 

Let $\mathscr F$ be a $d$-dimensional differentiable structure on $M$ of class $\CC^k$, where $k \ge 1$.


Then $\struct {M, \mathscr F}$ is a differentiable manifold of class $\CC^k$ and dimension $d$.",Definition:Topological Manifold/Differentiable Manifold,"['Definitions/Topological Manifolds', 'Definitions/Differentiable Manifolds']"
Definition:Manifold,Manifold,"Let $M$ be a second-countable locally Euclidean space of dimension $d$. 

Let $\mathscr F$ be a smooth differentiable structure on $M$.


Then $\struct {M, \mathscr F}$ is called a smooth manifold of dimension $d$.",Definition:Topological Manifold/Smooth Manifold,"['Definitions/Smooth Manifolds', 'Definitions/Topological Manifolds', 'Definitions/Differentiable Manifolds']"
Definition:Manifold,Manifold,"Let $M$ be a second-countable, complex locally Euclidean space of dimension $d$. 

Let $\mathscr F$ be a complex analytic differentiable structure on $M$.


Then $\struct {M, \mathscr F}$ is called a complex manifold of dimension $d$.",Definition:Topological Manifold/Complex Manifold,['Definitions/Topological Manifolds']
Definition:Manifold,Manifold,"Let $K$ be a division ring.

Let $\left({S, +, \circ}\right)_K$ be a $K$-algebraic structure with one operation.


Let $T$ be a closed subset of $S$.

Let $\left({T, +_T, \circ_T}\right)_K$ be a $K$-vector space where:
: $+_T$ is the restriction of $+$ to $T \times T$ and
: $\circ_T$ is the restriction of $\circ$ to $K \times T$.


Then $\left({T, +_T, \circ_T}\right)_K$ is a (vector) subspace of $\left({S, +, \circ}\right)_K$.


When considering Hilbert spaces, one wants to deal with projections onto subspaces.

These projections however require the linear subspace to be closed in topological sense in order to be well-defined.

Therefore, in treatises of Hilbert spaces, one encounters the terminology linear manifold for the concept of vector subspace defined above.

The adapted definition of linear subspace is then that it is a topologically closed linear manifold.",Definition:Vector Subspace/Hilbert Spaces,"['Definitions/Linear Algebra', 'Definitions/Hilbert Spaces']"
Definition:Maximal Ideal,Maximal Ideal,"Let $R$ be a ring.


An ideal $J$ of $R$ is maximal  if and only if :

:$(1): \quad J \subsetneq R$
:$(2): \quad$ There is no ideal $K$ of $R$ such that $J \subsetneq K \subsetneq R$.


That is,  if and only if  $J$ is a maximal element of the set of all proper ideals of $R$ ordered by the subset relation.


=== Maximal Left Ideal ===
Let $R$ be a ring.


A left ideal $J$ of $R$ is a maximal left ideal  if and only if :

:$(1): \quad J \subsetneq R$
:$(2): \quad$ There is no left ideal $K$ of $R$ such that $J \subsetneq K \subsetneq R$.


Category:Definitions/Maximal Ideals of Rings

=== Maximal Right Ideal ===
Let $R$ be a ring.


A right ideal $J$ of $R$ is a maximal right ideal  if and only if :

:$(1): \quad J \subsetneq R$
:$(2): \quad$ There is no right ideal $K$ of $R$ such that $J \subsetneq K \subsetneq R$.


Category:Definitions/Maximal Ideals of Rings

It follows that in a commutative ring, a maximal left ideal, a maximal right ideal and a maximal ideal are the same thing.",Definition:Maximal Ideal of Ring,"['Definitions/Ideal Theory', 'Definitions/Maximal Ideals of Rings']"
Definition:Maximal Ideal,Maximal Ideal,"Let $R$ be a ring.

Let $\struct {A, \ast}$ be an $R$-algebra.

Let $\struct {J, \ast}$ be a proper ideal of $\struct {A, \ast}$. 


We say that $J$ is a maximal ideal of $A$  if and only if :
:there is no ideal $I$ such that $J \subsetneq I$. 


=== Maximal Left Ideal ===
Let $R$ be a ring.

Let $\struct {A, \ast}$ be an $R$-algebra.

Let $\struct {J, \ast}$ be a proper left ideal of $\struct {A, \ast}$. 


We say that $J$ is a maximal left ideal of $A$  if and only if :
:there is no left ideal $I$ such that $J \subsetneq I$.

=== Maximal Right Ideal ===
Let $R$ be a ring.

Let $\struct {A, \ast}$ be an $R$-algebra.

Let $\struct {J, \ast}$ be a proper right ideal of $\struct {A, \ast}$. 


We say that $J$ is a maximal right ideal of $A$  if and only if :
:there is no right ideal $I$ such that $J \subsetneq I$.",Definition:Maximal Ideal of Algebra,['Definitions/Algebras']
Definition:Mean,Mean,"Let $x_1, x_2, \ldots, x_n \in \R$ be real numbers.

The arithmetic mean of $x_1, x_2, \ldots, x_n$ is defined as:

:$\ds A_n := \dfrac 1 n \sum_{k \mathop = 1}^n x_k$

That is, to find out the arithmetic mean of a set of numbers, add them all up and divide by how many there are.",Definition:Arithmetic Mean,"['Definitions/Arithmetic Mean', 'Definitions/Pythagorean Means', 'Definitions/Algebra', 'Definitions/Measures of Central Tendency']"
Definition:Mean,Mean,"Let $x_1, x_2, \ldots, x_n \in \R_{>0}$ be (strictly) positive real numbers.

The geometric mean of $x_1, x_2, \ldots, x_n$ is defined as:

:$\ds G_n := \paren {\prod_{k \mathop = 1}^n x_k}^{1/n}$


That is, to find out the geometric mean of a set of $n$ numbers, multiply them together and take the $n$th root.


=== Mean Proportional ===
In the language of  , the geometric mean of two magnitudes is called the mean proportional.

Thus the mean proportional of $a$ and $b$ is defined as that magnitude $c$ such that:
:$a : c = c : b$
where $a : c$ denotes the ratio between $a$ and $c$.


From the definition of ratio it is seen that $\dfrac a c = \dfrac c b$ from which it follows that $c = \sqrt {a b}$ demonstrating that the definitions are logically equivalent.


=== General Definition ===
In the language of  , the terms of a (finite) geometric sequence of positive integers between (and not including) the first and last terms are called mean proportionals.",Definition:Geometric Mean,"['Definitions/Geometric Mean', 'Definitions/Pythagorean Means', 'Definitions/Algebra', 'Definitions/Measures of Central Tendency']"
Definition:Mean,Mean,"The arithmetic-geometric mean of two numbers $a$ and $b$ is the limit of the sequences obtained by the arithmetic-geometric mean iteration.

This is denoted $\map M {a, b}$.


=== Arithmetic-Geometric Mean Iteration ===
Let $a$ and $b$ be numbers.

Let $\sequence {a_n}$ and $\sequence {b_n}$ be defined as the recursive sequences:

 
 
 
 
 

where:

 
 
 
 

The above process is known as the arithmetic-geometric mean iteration.",Definition:Arithmetic-Geometric Mean,"['Definitions/Arithmetic-Geometric Mean', 'Definitions/Measures of Central Tendency']"
Definition:Mean,Mean,"Let $x_1, x_2, \ldots, x_n \in \R$ be real numbers which are all strictly positive.

The harmonic mean $H_n$ of $x_1, x_2, \ldots, x_n$ is defined as:

:$\ds \dfrac 1 {H_n} := \frac 1 n \paren {\sum_{k \mathop = 1}^n \frac 1 {x_k} }$

That is, to find the harmonic mean of a set of $n$ numbers, take the reciprocal of the arithmetic mean of their reciprocals.",Definition:Harmonic Mean,"['Definitions/Harmonic Mean', 'Definitions/Pythagorean Means', 'Definitions/Measures of Central Tendency', 'Definitions/Algebra', 'Definitions/Number Theory', 'Definitions/Analysis']"
Definition:Mean,Mean,"Let $x_1, x_2, \ldots, x_n \in \R$ be real numbers.

The quadratic mean of $x_1, x_2, \ldots, x_n$ is defined as:

:$Q_n := \ds \sqrt {\frac 1 n \sum_{k \mathop = 1}^n x_k^2}$",Definition:Quadratic Mean,"['Definitions/Algebra', 'Definitions/Measures of Central Tendency']"
Definition:Mean,Mean,"Let $S = \sequence {x_1, x_2, \ldots, x_n}$ be a sequence of real numbers.

Let $W$ be a weight function to be applied to the terms of $S$.


The weighted mean of $S$   $W$ is defined as:
:$\bar x := \dfrac {\ds \sum_{i \mathop = 1}^n \map W {x_i} x_i} {\ds \sum_{i \mathop = 1}^n \map W {x_i} }$

This means that elements of $S$ with a larger weight contribute more to the weighted mean than those with a smaller weight.


If we write:
:$\forall i: 1 \le i \le n: w_i = \map W {x_i}$
we can write this weighted mean as:
:$\bar x := \dfrac {w_1 x_1 + w_2 x_2 + \cdots + w_n x_n} {w_1 + w_2 + \cdots + w_n}$


From the definition of the weight function, none of the weights can be negative.

While some of the weights may be zero, not all of them can, otherwise we would be dividing by zero.


=== Normalized Weighted Mean ===
Let $S = \sequence {x_1, x_2, \ldots, x_n}$ be a sequence of real numbers.

Let $\map W x$ be a weight function to be applied to the terms of $S$.

Let the weights be normalized.

Then the weighted mean of $S$   $W$ can be expressed in the form:
:$\ds \bar x := \sum_{i \mathop = 1}^n \map W {x_i} x_i$

as by definition of normalized weight function all the weights add up to $1$.

This weighted mean is known as a  normalized weighted mean.


 ",Definition:Weighted Mean,"['Definitions/Weighted Means', 'Definitions/Measures of Central Tendency', 'Definitions/Algebra']"
Definition:Mean,Mean,"The Heronian mean of two numbers $x$ and $y$ is defined as:

:$H = \dfrac {x + \sqrt {x y} + y} 3$


It can also be defined as:

:$H = \dfrac 2 3 \paren {\dfrac {x + y} 2} + \dfrac 1 3 \sqrt {x y}$


Thus it is seen to be a weighted mean of their arithmetic mean and geometric mean.

 ",Definition:Heronian Mean,['Definitions/Algebra']
Definition:Mean,Mean,"Let $x_1, x_2, \ldots, x_n \in \R_{\ge 0}$ be positive real numbers.

Let $p$ be an extended real number.


The Hlder mean with exponent $p$ of $x_1, x_2, \ldots, x_n$ is denoted $\map {M_p} {x_1, x_2, \ldots, x_n}$.


=== Non-Zero Exponent ===
Let $x_1, x_2, \ldots, x_n \in \R_{\ge 0}$ be positive real numbers.

Let $p$ be an extended real number.


Let $\map {M_p} {x_1, x_2, \ldots, x_n}$ denote the Hlder mean with exponent $p$ of $x_1, x_2, \ldots, x_n$.


For $p \in \R_{\ne 0}$, the Hlder mean is defined as:
:$\ds \map {M_p} {x_1, x_2, \ldots, x_n} = \paren {\frac 1 n \sum_{k \mathop = 1}^n {x_k}^p}^{1 / p}$
whenever the above expression is defined.

=== Negative Exponent with Zero Parameter ===
Let $x_1, x_2, \ldots, x_n \in \R_{\ge 0}$ be positive real numbers.

Let $p$ be an extended real number.


Let $\map {M_p} {x_1, x_2, \ldots, x_n}$ denote the Hlder mean with exponent $p$ of $x_1, x_2, \ldots, x_n$.


For $p < 0$ and at least one $a_k = 0$, the Hlder mean is defined as:
:$\ds \map {M_p} {x_1, x_2, \ldots, x_n} = 0$

=== Zero Exponent ===
Let $x_1, x_2, \ldots, x_n \in \R_{\ge 0}$ be positive real numbers.

Let $p$ be an extended real number.


Let $\map {M_p} {x_1, x_2, \ldots, x_n}$ denote the Hlder mean with exponent $p$ of $x_1, x_2, \ldots, x_n$.


For $p = 0$, the Hlder mean is defined as:
:$\map {M_0} {x_1, x_2, \ldots, x_n} = \paren {x_1 x_2 \cdots x_n}^{1 / n}$
which is the geometric mean of $x_1, x_2, \ldots, x_n$.

=== Positive Infinite Exponent ===
Let $x_1, x_2, \ldots, x_n \in \R_{\ge 0}$ be positive real numbers.

Let $p$ be an extended real number.


Let $\map {M_p} {x_1, x_2, \ldots, x_n}$ denote the Hlder mean with exponent $p$ of $x_1, x_2, \ldots, x_n$.


For $p = \infty$, the Hlder mean is defined as:
:$\map {M_\infty} {x_1, x_2, \ldots, x_n} = \max \set {x_1, x_2, \ldots, x_n}$

=== Negative Infinite Exponent ===
Let $x_1, x_2, \ldots, x_n \in \R_{\ge 0}$ be positive real numbers.

Let $p$ be an extended real number.


Let $\map {M_p} {x_1, x_2, \ldots, x_n}$ denote the Hlder mean with exponent $p$ of $x_1, x_2, \ldots, x_n$.


For $p = -\infty$, the Hlder mean is defined as:
:$\map {M_{-\infty} } {x_1, x_2, \ldots, x_n} = \min \set {x_1, x_2, \ldots, x_n}$",Definition:Hlder Mean,"['Definitions/Hlder Mean', 'Definitions/Measures of Central Tendency', 'Definitions/Real Analysis', 'Definitions/Algebra']"
Definition:Mean,Mean,"Let $f$ be an integrable function on some closed interval $\closedint a b$.

The mean value of $f$ on $\closedint a b$ is defined as:

:$\ds \frac 1 {b - a} \int_a^b \map f x \rd x$",Definition:Mean Value of Function,"['Definitions/Mean Value of Function', 'Definitions/Integral Calculus', 'Definitions/Measures of Central Tendency']"
Definition:Mean,Mean,"=== Definition 1 ===
Let a line segment $AB$ be divided at $C$ such that:
:$AB : AC = AC : BC$

Then the golden mean $\phi$ is defined as:
:$\phi := \dfrac {AB} {AC}$

=== Definition 2 ===
The golden mean is the unique positive real number $\phi$ satisfying:
:$\phi = \dfrac {1 + \sqrt 5} 2$

=== Definition 3 ===
The golden mean is the unique positive real number $\phi$ satisfying:
:$\phi = \dfrac 1 {\phi - 1}$",Definition:Golden Mean,"['Definitions/Golden Mean', 'Definitions/Fibonacci Numbers', 'Definitions/Real Analysis', 'Definitions/Number Theory', 'Definitions/Algebra', 'Definitions/Geometry', 'Definitions/Specific Numbers']"
Definition:Mean,Mean,"=== Centroid of Set of Points ===
Let $S = \set {A_1, A_2, \ldots, A_n}$ be a set of $n$ points in Euclidean space.


=== Definition 1 ===
Let $S = \set {A_1, A_2, \ldots, A_n}$ be a set of $n$ points in Euclidean space.

Let the position vectors of the elements of $S$ be given by $\mathbf a_1, \mathbf a_2, \dotsc, \mathbf a_n$ respectively.

Let $G$ be the point whose position vector is given by:

:$\vec {OG} = \dfrac 1 n \paren {\mathbf a_1 + \mathbf a_2 + \dotsb + \mathbf a_n}$


Then $G$ is known as the centroid of $S$.

=== Definition 2 ===
Let $S = \set {A_1, A_2, \ldots, A_n}$ be a set of $n$ points in Euclidean space.

Let the Cartesian coordinates of the elements of $S$ be $\tuple {x_j, y_j, z_j}$ for each $j \in \set {1, 2, \ldots, n}$.

Let $G$ be the point whose Cartesian coordinates are given by:

:$G = \tuple {\dfrac 1 n \ds \sum_{j \mathop = 1}^n x_j, \dfrac 1 n \ds \sum_{j \mathop = 1}^n y_j, \dfrac 1 n \ds \sum_{j \mathop = 1}^n z_j}$

That is, the arithmetic mean of the Cartesian coordinates of the elements of $S$


Then $G$ is known as the centroid of $S$.

=== Centroid of Weighted Set of Points ===
Let $S = \set {A_1, A_2, \ldots, A_n}$ be a set of $n$ points in Euclidean space whose position vectors are given by $\mathbf a_1, \mathbf a_2, \dotsc, \mathbf a_n$ repectively.

Let $W: S \to \R$ be a weight function on $S$.

Let $G$ be the point whose position vector is given by:

:$\vec {OG} = \dfrac {w_1 \mathbf a_1 + w_2 \mathbf a_2 + \dotsb + w_n \mathbf a_n} {w_1 + w_2 + \dotsb + w_n}$

where $w_i = \map W {A_i}$ for each $i$.


Then $G$ is known as the centroid of $S$ with weights $w_i, w_2, \dotsc, w_n$.

=== Centroid of Surface ===
Let $S$ be a surface.

Let $S$ be divided into a large number $n$ of small elements.

Consider one point of each of these elements.

Let a weight function be associated with this set of points.

Let $G$ be the centroid of each of these weighted points.

Let $n$ increase indefinitely, such that each element of $S$ converges to a point.

Then the limiting position of $G$ is the centroid of $S$.

=== Centroid of Solid Figure ===
Let $F$ be a solid figure.

Let $F$ be divided into a large number $n$ of small elements.

Consider one point of each of these elements.

Let a weight function be associated with this set of points.

Let $G$ be the centroid of each of these weighted points.

Let $n$ increase indefinitely, such that each element of $F$ converges to a point.

Then the limiting position of $G$ is the centroid of $F$.

=== Centroid of Triangle ===
Let $\triangle ABC$ be a triangle.


The centroid of $\triangle ABC$ is the point $G$ where its three medians $AL$, $MB$ and $CN$ meet.


:",Definition:Centroid,"['Definitions/Centroids', 'Definitions/Physics', 'Definitions/Applied Mathematics']"
Definition:Mean,Mean,"Let $S = \set {x_1, x_2, \ldots, x_n}$ be a set of observations.

Let $\bar x$ denote a measure of central tendency of $S$.


The mean deviation   $\bar x$ of $S$ is defined as the arithmetic mean of the deviation of the elements of $S$ from $\bar x$:

:$\ds \sum_{i \mathop = 1}^n \dfrac 1 n \paren {x_i - \bar x}$


=== Discrete Random Variable ===
Let $X$ be a discrete random variable.

Let $\bar x$ denote a measure of central tendency of $X$.


The mean deviation of $X$ is the first moment of $X$ about $\bar x$.

=== Continuous Random Variable ===
Let $X$ be a continuous random variable.

Let $m$ denote the median of $X$.

Let the frequency function of $X$ be $f$.


The mean deviation of $X$ is defined as:
:$\ds \int_{-\infty}^{+\infty} \paren {x - m} \map f x \rd x$",Definition:Mean Deviation,"['Definitions/Mean Deviation', 'Definitions/Deviations']"
Definition:Mean,Mean,"Let $S = \set {x_1, x_2, \ldots, x_n}$ be a set of observations.

Let $\bar x$ denote a measure of central tendency of $S$.


The mean absolute deviation   $\bar x$ of $S$ is defined as the arithmetic mean of the absolute values of the deviation of the elements of $S$ from $\bar x$ :

:$\ds \sum_{i \mathop = 1}^n \dfrac 1 n \size {x_i - \bar x}$


=== Discrete Random Variable ===
Let $X$ be a discrete random variable.

Let $\bar x$ denote a measure of central tendency of $X$.


The mean absolute deviation of $X$ is the first absolute moment of $X$ about $\bar x$.

=== Continuous Random Variable ===
Let $X$ be a continuous random variable.

Let $m$ denote the median of $X$.

Let the frequency function of $X$ be $f$.


The mean absolute deviation of $X$ is defined as:
:$\ds \int_{-\infty}^{+\infty} \size {x - m} \map f x \rd x$",Definition:Mean Absolute Deviation,"['Definitions/Mean Absolute Deviation', 'Definitions/Deviations']"
Definition:Mean,Mean,"Let $S = \set {x_1, x_2, \ldots, x_n}$ be a set of observations.

Let $\bar x$ denote a measure of central tendency of $S$.


The mean square deviation   $\bar x$ of $S$ is defined as the arithmetic mean of the square of the deviation of the elements of $S$ from $\bar x$:

:$\ds \sum_{i \mathop = 1}^n \dfrac 1 n \paren {x_i - \bar x}^2$


=== Discrete Random Variable ===
Let $X$ be a discrete random variable.

Let $\bar x$ denote a measure of central tendency of $X$.


The mean square deviation of $X$ is the second moment of $X$ about $\bar x$.

=== Continuous Random Variable ===
Let $X$ be a continuous random variable.

Let $m$ denote the median of $X$.

Let the frequency function of $X$ be $f$.


The mean square deviation of $X$ is defined as:
:$\ds \int_{-\infty}^{+\infty} \paren {x - m}^2 \map f x \rd x$",Definition:Mean Square Deviation,"['Definitions/Mean Square Deviation', 'Definitions/Deviations']"
Definition:Mean,Mean,The mean squared error is the expected value of the square of the difference between an estimator $T$ and the true parameter value $\theta$.,Definition:Mean Squared Error,"['Definitions/Mean Squared Error', 'Definitions/Variance', 'Definitions/Statistics']"
Definition:Measure,Measure,"Measure theory is the subfield of analysis concerned with the properties of measures, particularly the Lebesgue measure.",Definition:Measure Theory,"['Definitions/Branches of Mathematics', 'Definitions/Measure Theory', 'Definitions/Analysis']"
Definition:Measure,Measure,"Let $\struct {X, \Sigma}$ be a measurable space.

Let $\mu: \Sigma \to \overline \R$ be a mapping, where $\overline \R$ denotes the set of extended real numbers.


=== Definition 1 ===
Let $\struct {X, \Sigma}$ be a measurable space.

Let $\mu: \Sigma \to \overline \R$ be a mapping, where $\overline \R$ denotes the set of extended real numbers.


$\mu$ is called a measure on $\Sigma$  if and only if  $\mu$ fulfils the following axioms:
 

=== Definition 2 ===
Let $\struct {X, \Sigma}$ be a measurable space.

Let $\mu: \Sigma \to \overline \R$ be a mapping, where $\overline \R$ denotes the set of extended real numbers.


$\mu$ is called a measure on $\Sigma$  if and only if  $\mu$ fulfils the following axioms:
 

=== Definition 3 ===
Let $\struct {X, \Sigma}$ be a measurable space.

Let $\mu: \Sigma \to \overline \R$ be a mapping, where $\overline \R$ denotes the set of extended real numbers.


$\mu$ is called a measure on $\Sigma$  if and only if  $\mu$ fulfils the following axioms:
 ",Definition:Measure (Measure Theory),"['Definitions/Measures', 'Definitions/Measure Theory']"
Definition:Measure,Measure,"Let $\JJ_{ho}^n$ be the set of half-open $n$-rectangles.

Let $\map \BB {\R^n}$ be the Borel $\sigma$-algebra on $\R^n$.


Let $\lambda^n$ be the $n$-dimensional Lebesgue pre-measure on $\JJ_{ho}^n$.

A measure $\mu$ extending $\lambda^n$ to $\map \BB {\R^n}$ is called $n$-dimensional Lebesgue measure.

That is, $\mu$ is an $n$-dimensional Lebesgue measure  if and only if  it satisfies:

:$\mu \restriction_{\JJ_{ho}^n} = \lambda^n$

where $\restriction$ denotes restriction.


Usually, this measure is also denoted by $\lambda^n$, even though this may be considered abuse of notation.


=== Lebesgue Measure on the Reals ===
For a given set $S \subseteq \R$, let $\set {I_n}$ be a countable set of open intervals such that:

:$S \subseteq \bigcup \set {I_n}$

For the power set $\powerset \R$ of the real numbers $\R$, construct a function $\mu^*: \powerset \R \to \R_{>0}$ as:

:$\ds \map {\mu^*} S = \inf \set {\sum_{n \mathop \in \N} \map l {I_n} : \set {I_n} : S \subseteq \bigcup_{n \mathop \in \N} I_n}$

where:
:the infimum ranges over all such sets $\set {I_n}$
:$\map l {I_n}$ is the length of the interval $I_n$.

Then $\mu^*$ is known as the Lebesgue outer measure and can be shown to be an outer measure.",Definition:Lebesgue Measure,"['Definitions/Lebesgue Measures', 'Definitions/Measure Theory']"
Definition:Measure,Measure,"A geometric quantity $A$ is said to measure another quantity $B$ when the size of $A$ is a divisor of the size of $B$.

Category:Definitions/Geometry",Definition:Measure (Geometry),['Definitions/Geometry']
Definition:Measure,Measure,"Measurement is the process of determining the quantity of a measurable property according to some given scale of measurement.

A measurement is reported as a (real) number multiplied by a unit of measurement for that quantity.",Definition:Measurable Property/Measurement,"['Definitions/Measurement', 'Definitions/Measurable Properties']"
Definition:Median,Median,"Let $\triangle ABC$ be a triangle.

:

A median is a cevian which bisects the opposite.


In the above diagram, $CD$ is a median.",Definition:Median of Triangle,"['Definitions/Medians of Triangles', 'Definitions/Cevians', 'Definitions/Triangles']"
Definition:Median,Median,"Let $\Box ABCD$ be a trapezium.

:

The median of $\Box ABCD$ is the straight line through the midpoints of the legs of $\Box ABCD$.


In the above diagram, $EF$ is the median of the trapezium $\Box ABCD$.",Definition:Median of Trapezium,"['Definitions/Medians of Trapezia', 'Definitions/Trapezia']"
Definition:Median,Median,"Let $S$ be a set of quantitative data.

Let $S$ be arranged in order of size.

The median is the element of $S$ that is in the middle of that ordered set.


Suppose there are an odd number of elements of $S$ such that $S$ has cardinality $2 n - 1$.

The median of $S$ in that case is the $n$th element of $S$.


Suppose there are an even number of elements of $S$ such that $S$ has cardinality $2 n$.

Then the middle of $S$ is not well-defined, and so the median of $S$ in that case is the arithmetic mean of the $n$th and $n + 1$th elements of $S$.


=== Continuous Random Variable ===
Let $X$ be a continuous random variable on a probability space $\struct {\Omega, \Sigma, \Pr}$.

Let $X$ have probability density function $f_X$. 

A median of $X$ is defined as a real number $m_X$ such that: 

:$\ds \map \Pr {X < m_X} = \int_{-\infty}^{m_X} \map {f_X} x \rd x = \frac 1 2$

That is, $m_X$ is the first $2$-quantile of $X$.",Definition:Median (Statistics),"['Definitions/Medians', 'Definitions/Measures of Central Tendency']"
Definition:Median,Median,"Let $X$ be a continuous random variable on a probability space $\struct {\Omega, \Sigma, \Pr}$.

Let $X$ have probability density function $f_X$. 

A median of $X$ is defined as a real number $m_X$ such that: 

:$\ds \map \Pr {X < m_X} = \int_{-\infty}^{m_X} \map {f_X} x \rd x = \frac 1 2$

That is, $m_X$ is the first $2$-quantile of $X$.",Definition:Median of Continuous Random Variable,['Definitions/Medians']
Definition:Meet,Meet,"Let $\struct {S, \preceq}$ be an ordered set.

Let $a, b \in S$, and suppose that their infimum $\inf \set {a, b}$ exists in $S$.


Then $a \wedge b$, the meet of $a$ and $b$, is defined as:

:$a \wedge b = \inf \set {a, b}$


Expanding the definition of infimum, one sees that $c = a \wedge b$  if and only if :

:$(1): \quad c \preceq a$ and $c \preceq b$
:$(2): \quad \forall s \in S: s \preceq a$ and $s \preceq b \implies s \preceq c$",Definition:Meet (Order Theory),"['Definitions/Order Theory', 'Definitions/Lattice Theory']"
Definition:Meet,Meet,"Consider the Boolean algebra $\struct {S, \vee, \wedge, \neg}$


The operation $\wedge$ is called meet.",Definition:Boolean Algebra/Meet,['Definitions/Boolean Algebras']
Definition:Meet,Meet,"Let $\family {S_i}_{i \mathop \in I}$ be an family of sets indexed by some  indexing set $I$.

The sets in $\family {S_i}$ are said to meet  if and only if  their intersection is not empty.


That is,  if and only if :
:$\ds \bigcap_{i \mathop \in I} \family {S_i} \ne \O$


That is,  if and only if  $\family {S_i}_{i \mathop \in I}$ is not disjoint.",Definition:Set Meeting Set,['Definitions/Set Intersection']
Definition:Metrizable,Metrizable,"Let $T = \struct {S, \tau}$ be a topological space.

=== Definition 1 ===
Let $T = \struct {S, \tau}$ be a topological space.


$T$ is said to be metrizable  if and only if  there exists a metric $d$ on $S$ such that:
:$\tau$ is the topology induced by $d$ on $S$.

=== Definition 2 ===
Let $T = \struct {S, \tau}$ be a topological space.


$T$ is said to be metrizable  if and only if  there exists a metric space $M = \struct{A, d}$ such that:
:$T$ is homeomorphic to the topological space $\struct{A, \tau_d}$ 
where $\tau_d$ is the topology induced by $d$ on $A$.",Definition:Metrizable Topology,"['Definitions/Metrizable Topologies', 'Definitions/Topology', 'Definitions/Metric Spaces', 'Definitions/Metrizable']"
Definition:Metrizable,Metrizable,"Let $M = \struct {A, d}$ be a metric space.

Let $\UU$ be the uniformity on $X$ defined as:
:$\UU := \set {u_\epsilon: \epsilon \in \R_{>0} }$
where:
:$\R_{>0}$ is the set of strictly positive real numbers
:$u_\epsilon$ is defined as:
::$u_\epsilon := \set {\paren {x, y}: \map d {x, y} < \epsilon}$


Then $\UU$ is defined as metrizable.",Definition:Metrizable Uniformity,"['Definitions/Metric Spaces', 'Definitions/Uniformities', 'Definitions/Metrizable']"
Definition:Minor,Minor,"Let $a, b \in \R_{>0}$ in the forms:
:$a = \dfrac \rho {\sqrt 2} \sqrt {1 + \dfrac k {\sqrt {1 + k^2} } }$
:$b = \dfrac \rho {\sqrt 2} \sqrt {1 - \dfrac k {\sqrt {1 + k^2} } }$

where:
:$\rho$ is a rational number
:$k$ is a rational number whose square root is irrational.


Then $a - b$ is a minor.


 ",Definition:Minor (Euclidean),['Definitions/Euclidean Number Theory']
Definition:Minor,Minor,":

Consider an ellipse $K$ whose foci are $F_1$ and $F_2$.


=== Definition $1$ ===
:


Consider an ellipse $K$ whose foci are $F_1$ and $F_2$.

The minor axis of $K$ is the line segment through the center of $K$ perpendicular to the major axis of $K$ such that its endpoints are the points of intersection with $K$.


In the above diagram, $C_1 C_2$ is the minor axis of $K$.

=== Definition $2$ ===
:


Consider an ellipse $K$ whose foci are $F_1$ and $F_2$.

The minor axis of $K$ is the diameter of $K$ which has the smallest length.


In the above diagram, $C_1 C_2$ is the minor axis of $K$.

In the above diagram, $C_1 C_2$ is the minor axis of $K$.


=== Semi-Minor Axis ===
:

Consider an ellipse $K$.


A semi-minor axis of $K$ is either half of the minor axis of $K$ from its midpoint to its endpoint.


In the above diagram, $O C_1$ and $O C_2$ are the semi-minor axes of $K$.",Definition:Ellipse/Minor Axis,"['Definitions/Minor Axis of Ellipse', 'Definitions/Minor Axis', 'Definitions/Ellipses']"
Definition:Minor,Minor,":

Consider an ellipse $K$.


A semi-minor axis of $K$ is either half of the minor axis of $K$ from its midpoint to its endpoint.


In the above diagram, $O C_1$ and $O C_2$ are the semi-minor axes of $K$.",Definition:Ellipse/Minor Axis/Semi-Minor Axis,"['Definitions/Semi-Minor Axis of Ellipse', 'Definitions/Minor Axis', 'Definitions/Ellipses']"
Definition:Minor,Minor,"The minor premise of a categorical syllogism is conventionally stated second.

It is a categorical statement which expresses the logical relationship between the secondary term and the middle term of the syllogism.",Definition:Categorical Syllogism/Premises/Minor Premise,['Definitions/Categorical Syllogisms']
Definition:Minor,Minor,"Let $\mathbf A = \sqbrk a_n$ be a square matrix of order $n$.

Consider the order $k$ square submatrix $\mathbf B$ obtained by deleting $n - k$ rows and $n - k$ columns from $\mathbf A$.


Let $\map \det {\mathbf B}$ denote the determinant of $\mathbf B$.

Then $\map \det {\mathbf B}$ is an order-$k$ minor of $\map \det {\mathbf A}$.


Thus a minor is a determinant formed from the elements (in the same relative order) of $k$ specified rows and columns.",Definition:Minor of Determinant,['Definitions/Determinants']
Definition:Model,Model,"Let $\mathscr M$ be a formal semantics for a logical language $\LL$.

Let $\MM$ be a structure of $\mathscr M$.


=== Model of Logical Formula ===
Let $\mathscr M$ be a formal semantics for a logical language $\LL$.

Let $\MM$ be a structure of $\mathscr M$.


Let $\phi$ be a logical formula of $\LL$.

Then $\MM$ is a model of $\phi$  if and only if :

:$\MM \models_{\mathscr M} \phi$

that is,  if and only if  $\phi$ is valid in $\MM$.


 

Category:Definitions/Model Theory
Category:Definitions/Formal Semantics

=== Model of Set of Logical Formulas ===
Let $\mathscr M$ be a formal semantics for a logical language $\LL$.

Let $\MM$ be a structure of $\mathscr M$.


Let $\FF$ be a set of logical formulas of $\LL$.

Then $\MM$ is a model of $\FF$  if and only if :

:$\MM \models_{\mathscr M} \phi$ for every $\phi \in \FF$

that is,  if and only if  it is a model of every logical formula $\phi \in \FF$.",Definition:Model (Logic),"['Definitions/Model Theory', 'Definitions/Symbolic Logic', 'Definitions/Formal Semantics']"
Definition:Model,Model,"Let $\LL_1$ be the language of predicate logic.

Let $\AA$ be a structure for predicate logic.


Then $\AA$ models a sentence $\mathbf A$  if and only if :

:$\map {\operatorname{val}_\AA} {\mathbf A} = \T$

where $\map {\operatorname{val}_\AA} {\mathbf A}$ denotes the value of $\mathbf A$ in $\AA$.


This relationship is denoted:

:$\AA \models_{\mathrm{PL} } \mathbf A$


When pertaining to a collection of sentences $\FF$, one says $\AA$ models $\FF$  if and only if :

:$\forall \mathbf A \in \FF: \AA \models_{\mathrm{PL} } \mathbf A$

that is,  if and only if  it models all elements of $\FF$.

This can be expressed symbolically as:

:$\AA \models_{\mathrm {PL} } \FF$",Definition:Model (Predicate Logic),['Definitions/Model Theory for Predicate Logic']
Definition:Model,Model,"A mathematical model is an equation, or a system of equations, whose purpose is to provide an approximation to the behavior of a real-world phenomenon.


=== Constant ===
Let $P$ be a stochastic process which is being modelled by means of a model $M$.

A constant of $M$ is a number which implements some aspect of $P$ which is not changed during the evolution of $P$.

=== Parameter ===
Let $P$ be a stochastic process which is being modelled by means of a model $M$.

A parameter of $M$ is a number which implements some aspect of $P$ which is designed so as to be able to be modified during the evolution of $P$ as a result of analysis of the behaviour of $P$ over time.",Definition:Mathematical Model,"['Definitions/Applied Mathematics', 'Definitions/Statistics', 'Definitions/Mathematical Models']"
Definition:Modulus,Modulus,"Let $k = \sequence {k_j}_{j \mathop \in J}$ be a multiindex.


The modulus of such a multiindex $k$  is defined by:
:$\ds \size k = \sum_{j \mathop \in J} k_j$


 

Note that, since by definition all but finitely many of the $k_j$ are zero, this summation is convergent.",Definition:Multiindex/Modulus,['Definitions/Polynomial Theory']
Definition:Modulus,Modulus,"Let $z = a + i b$ be a complex number, where $a, b \in \R$.


The (complex) modulus of $z$ is written $\cmod z$, and is defined as the square root of the sum of the squares of the real and imaginary parts:

:$\cmod z := \sqrt {a^2 + b^2}$


The (complex) modulus is a real-valued function, and, as and when appropriate, can be referred to as the (complex) modulus function.",Definition:Complex Modulus,"['Definitions/Complex Modulus', 'Definitions/Complex Numbers', 'Definitions/Complex Analysis', 'Definitions/Polar Form of Complex Number', 'Definitions/Examples of Norms']"
Definition:Modulus,Modulus,"Let $f: S \to \C$ be a complex-valued function.


Then the (complex) modulus of $f$ is written $\cmod f : S \to \R$ and is the real-valued function defined as:

:$\forall z \in S: \map {\cmod f} z = \cmod {\map f z}$",Definition:Modulus of Complex-Valued Function,['Definitions/Complex Analysis']
Definition:Modulus,Modulus,"In geometric function theory, the term modulus is used to denote certain conformal invariants of configurations or curve families.

More precisely, the modulus of a curve family $\Gamma$ is the reciprocal of its extremal length:
:$\mod \Gamma := \dfrac 1 {\map \lambda \Gamma}$


=== Modulus of a Quadrilateral ===

Consider a quadrilateral; that is, a Jordan domain $Q$ in the complex plane (or some other Riemann surface), together with two disjoint closed boundary arcs $\alpha$ and $\alpha'$.

Then the modulus of the quadrilateral $\map Q {\alpha, \alpha'}$ is the extremal length of the family of curves in $Q$ that connect $\alpha$ and $\alpha'$.

Equivalently, there exists a rectangle $R = \set {x + i y: \cmod x < a, \cmod y < b}$ and a conformal isomorphism between $Q$ and $R$ under which $\alpha$ and $\alpha'$ correspond to the vertical sides of $R$.

Then the modulus of $\map Q {\alpha, \alpha'}$ is equal to the ratio $a/b$.


See Modulus of a Quadrilateral.


=== Modulus of an Annulus ===

Consider an annulus $A$; that is, a domain whose boundary consists of two Jordan curves.

Then the modulus $\mod A$ is the extremal length of the family of curves in $A$ that connect the two boundary components of $A$.

Equivalently, there is a round annulus $\tilde A = \set {z \in \C: r < \cmod z < R}$ that is conformally equivalent to $A$.

Then:
:$\mod A := \dfrac 1 {2 \pi} \map \ln {\dfrac R r}$

The modulus of $A$ can also be denoted $\map M R$.",Definition:Modulus (Geometric Function Theory),['Definitions/Geometric Function Theory']
Definition:Modulus,Modulus,"Let $x$ be congruent to $y$ modulo $m$.

The number $m$ in this congruence is known as the modulus of the congruence.",Definition:Congruence (Number Theory)/Modulus,['Definitions/Congruence (Number Theory)']
Definition:Modulus,Modulus,"An elastic modulus is a ratio of stress to strain for a body or a substance which obeys Hooke's Law.

Hence an elastic modulus is the slope of the linear region of a stress-strain diagram.

The type of elastic modulus is dependent on the type of strain under discussion.


=== Young's Modulus ===
 

=== Bulk Modulus ===
Let $B$ be an elastic body.

The bulk modulus of $B$ is a physical property of $B$ which measures its resistance to change in volume without change of shape.

It is defined as the ratio of compressive stress per surface area of $B$ to its change of volume per unit volume associated with this stress, assuming uniform pressure over the surface of $B$.

=== Rigidity Modulus ===
 ",Definition:Elastic Modulus,"['Definitions/Elastic Moduli', 'Definitions/Elasticity']"
Definition:Modulus,Modulus,"Let $B$ be an elastic body.

The bulk modulus of $B$ is a physical property of $B$ which measures its resistance to change in volume without change of shape.

It is defined as the ratio of compressive stress per surface area of $B$ to its change of volume per unit volume associated with this stress, assuming uniform pressure over the surface of $B$.",Definition:Bulk Modulus,"['Definitions/Bulk Modulus', 'Definitions/Physical Quantities']"
Definition:Monomorphism,Monomorphism,"A homomorphism which is an injection is descibed as monic, or called a monomorphism.


=== Semigroup Monomorphism ===
Let $\struct {S, \circ}$ and $\struct {T, *}$ be semigroups.

Let $\phi: S \to T$ be a (semigroup) homomorphism.


Then $\phi$ is a semigroup monomorphism  if and only if  $\phi$ is an injection.

=== Group Monomorphism ===
Let $\struct {G, \circ}$ and $\struct {H, *}$ be groups.

Let $\phi: G \to H$ be a (group) homomorphism.


Then $\phi$ is a group monomorphism  if and only if  $\phi$ is an injection.

=== Ring Monomorphism ===
Let $\struct {R, +, \circ}$ and $\struct {S, \oplus, *}$ be rings.

Let $\phi: R \to S$ be a (ring) homomorphism.


Then $\phi$ is a ring monomorphism  if and only if  $\phi$ is an injection.

=== Field Monomorphism ===
Let $\struct {F, +, \circ}$ and $\struct {K, \oplus, *}$ be fields.

Let $\phi: F \to K$ be a (field) homomorphism.


Then $\phi$ is a field monomorphism  if and only if  $\phi$ is an injection.

=== $R$-Algebraic Structure Monomorphism ===
Let $\struct {S, \ast_1, \ast_2, \ldots, \ast_n, \circ}_R$ and $\struct {T, \odot_1, \odot_2, \ldots, \odot_n, \otimes}_R$ be $R$-algebraic structures.

Then $\phi: S \to T$ is an $R$-algebraic structure monomorphism  if and only if :

:$(1): \quad \phi$ is an injection
:$(2): \quad \forall k: k \in \closedint 1 n: \forall x, y \in S: \map \phi {x \ast_k y} = \map \phi x \odot_k \map \phi y$
:$(3): \quad \forall x \in S: \forall \lambda \in R: \map \phi {\lambda \circ x} = \lambda \otimes \map \phi x$.


That is,  if and only if :

:$(1): \quad \phi$ is an injection
:$(2): \quad \phi$ is an $R$-algebraic structure homomorphism.


This definition continues to apply when $S$ and $T$ are modules, and also when they are vector spaces.


=== Vector Space Monomorphism ===
Let $V$ and $W$ be $K$-vector spaces.

Then $\phi: V \to W$ is a vector space monomorphism  if and only if :

:$(1): \quad \phi$ is an injection
:$(2): \quad \forall \mathbf x, \mathbf y \in V: \map \phi {\mathbf x + \mathbf y} = \map \phi {\mathbf x} + \map \phi {\mathbf y}$
:$(3): \quad \forall \mathbf x \in V: \forall \lambda \in K: \map \phi {\lambda \mathbf x} = \lambda \map \phi {\mathbf x}$

=== Ordered Structure Monomorphism ===
Let $\struct {S, \circ, \preceq}$ and $\struct {T, *, \preccurlyeq}$ be ordered structures.


An ordered structure monomorphism from $\struct {S, \circ, \preceq}$ to $\struct {T, *, \preccurlyeq}$ is a mapping $\phi: S \to T$ that is both:

:$(1): \quad$ A monomorphism, i.e. an injective homomorphism, from the structure $\struct {S, \circ}$ to the structure $\struct {T, *}$

:$(2): \quad$ An order embedding from the ordered set $\struct {S, \preceq}$ to the ordered set $\struct {T, \preccurlyeq}$.


=== Ordered Semigroup Monomorphism ===
Let $\struct {S, \circ, \preceq}$ and $\struct {T, *, \preccurlyeq}$ be ordered semigroups.


An ordered semigroup monomorphism from $\struct {S, \circ, \preceq}$ to $\struct {T, *, \preccurlyeq}$ is a mapping $\phi: S \to T$ that is both:

:$(1): \quad$ A (semigroup) monomorphism from the semigroup $\struct {S, \circ}$ to the semigroup $\struct {T, *}$

:$(2): \quad$ An order embedding from the ordered set $\struct {S, \preceq}$ to the ordered set $\struct {T, \preccurlyeq}$.

=== Ordered Group Monomorphism ===
Let $\struct {S, \circ, \preceq}$ and $\struct {T, *, \preccurlyeq}$ be ordered groups.


An ordered group monomorphism from $\struct {S, \circ, \preceq}$ to $\struct {T, *, \preccurlyeq}$ is a mapping $\phi: S \to T$ that is both:

:$(1): \quad$ A group monomorphism from the group $\struct {S, \circ}$ to the group $\struct {T, *}$

:$(2): \quad$ An order embedding from the ordered set $\struct {S, \preceq}$ to the ordered set $\struct {T, \preccurlyeq}$.

=== Ordered Ring Monomorphism ===
Let $\struct {S, +, \circ, \preceq}$ and $\struct {T, \oplus, *, \preccurlyeq}$ be ordered rings.


An ordered ring monomorphism from $\struct {S, +, \circ, \preceq}$ to $\struct {T, \oplus, *, \preccurlyeq}$ is a mapping $\phi: S \to T$ that is both:

:$(1): \quad$ An ordered group monomorphism from the ordered group $\struct {S, +, \preceq}$ to the ordered group $\struct {T, \oplus, \preccurlyeq}$

:$(2): \quad$ A semigroup monomorphism from the semigroup $\struct {S, \circ}$ to the semigroup $\struct {T, *}$.",Definition:Monomorphism (Abstract Algebra),"['Definitions/Monomorphisms (Abstract Algebra)', 'Definitions/Injections', 'Definitions/Homomorphisms (Abstract Algebra)', 'Definitions/Monomorphisms']"
Definition:Monomorphism,Monomorphism,"Let $\mathbf C$ be a metacategory.

A monomorphism is a morphism $f \in \mathbf C_1$ such that:

:$f \circ g = f \circ h \implies g = h$

for all morphisms $g, h \in \mathbf C_1$ for which these compositions are defined.


That is, a monomorphism is a morphism which is left cancellable. 


One writes $f: C \rightarrowtail D$ to denote that $f$ is a monomorphism.",Definition:Monomorphism (Category Theory),"['Definitions/Category Theory', 'Definitions/Monomorphisms']"
Definition:Monotone,Monotone,"=== Ordered Sets ===
Let $\struct {S, \preceq_1}$ and $\struct {T, \preceq_2}$ be ordered sets.

Let $\phi: \struct {S, \preceq_1} \to \struct {T, \preceq_2}$ be a mapping.


Then $\phi$ is monotone  if and only if  it is either increasing or decreasing.


Note that this definition also holds if $S = T$.

=== Real Functions ===
This definition continues to hold when $S = T = \R$.

Thus, let $f$ be a real function.

Then $f$ is monotone  if and only if  it is either increasing or decreasing.

=== Sequences ===
Let $\struct {S, \preceq}$ be a totally ordered set.


A sequence $\sequence {a_k}_{k \mathop \in A}$ of elements of $S$ is monotone  if and only if  it is either increasing or decreasing.


=== Real Sequence ===

The above definition for sequences is usually applied to real number sequences:

Let $\sequence {x_n}$ be a sequence in $\R$.


Then $\sequence {x_n}$ is monotone  if and only if  it is either increasing or decreasing.",Definition:Monotone (Order Theory),['Definitions/Order Theory']
Definition:Monotone,Monotone,"Let $\SS$ be an algebra of sets.

Let $f: \SS \to \overline \R$ be an extended real-valued function, where $\overline \R$ denotes the set of extended real numbers.


Then $f$ is defined as monotone or monotonic  if and only if :
:$\forall A, B \in \SS: A \subseteq B \iff \map f A \le \map f B$

Category:Definitions/Set Systems",Definition:Monotone (Measure Theory),['Definitions/Set Systems']
Definition:Monotone,Monotone,"Let $X$ be a set, and let $\powerset X$ be its power set.

Let $\MM \subseteq \powerset X$ be a collection of subsets of $X$.


Then $\MM$ is said to be a monotone class (on $X$)  if and only if  for every countable, nonempty, index set $I$, it holds that:

:$\ds \family {A_i}_{i \mathop \in I} \in \MM \implies \bigcup_{i \mathop \in I} A_i \in \MM$
:$\ds \family {A_i}_{i \mathop \in I} \in \MM \implies \bigcap_{i \mathop \in I} A_i \in \MM$

that is,  if and only if  $\MM$ is closed under countable unions and countable intersections.",Definition:Monotone Class,['Definitions/Set Systems']
Definition:Multiplicative Function,Multiplicative Function,"Let $K$ be a field.

Let $f: K \to K$ be a function on $K$.


Then $f$ is described as completely multiplicative  if and only if :

:$\forall m, n \in K: \map f {m n} = \map f m \map f n$


That is, a completely multiplicative function is one where the value of a product of two numbers equals the product of the value of each one individually.",Definition:Completely Multiplicative Function,"['Definitions/Multiplicative Functions', 'Definitions/Field Theory', 'Definitions/Number Theory']"
Definition:Multiplicative Function,Multiplicative Function,"Let $\struct {R, +, \circ}$ be a ring.

Let $f: R \to \R$ be a (real-valued) function on $R$.


$f$ is a multiplicative function on $R$  if and only if :

:$\forall x, y \in R: \map f {x \circ y} = \map f x \times \map f y$


That is, a multiplicative function on $R$ is one where the value of the product of two elements of $R$ equals the product of their values.",Definition:Multiplicative Function on Ring,"['Definitions/Multiplicative Functions', 'Definitions/Ring Theory', 'Definitions/Number Theory']"
Definition:Multiplicative Function,Multiplicative Function,"Let $R$ be a unique factorization domain.

Let $f : R \to \C$ be a complex-valued function.


Then $f$ is multiplicative  if and only if :
:For all coprime $x, y\in R$: $f \left({x y}\right) = f \left({x}\right) f \left({y}\right)$


=== Arithmetic Function ===
Let $f : \N \to \C$ be an arithmetic function.


Then $f$ is multiplicative  if and only if :
:$m \perp n \implies \map f {m n} = \map f m \map f n$
where $m \perp n$ denotes that $m$ is coprime to $n$.


That is, a multiplicative arithmetic function is one where the value of a product of two coprime numbers equals the product of the value of each one individually.",Definition:Multiplicative Function on UFD,['Definitions/Ring Theory']
Definition:Multiplicity,Multiplicity,The multiplicity of a multiple root is the number of times it appears.,Definition:Multiple Root/Multiplicity,['Definitions/Multiple Roots']
Definition:Multiplicity,Multiplicity,"Let $f: \C \to \C$ be a function.

Suppose there is $a \in \C$ such that $\map f a = 0$.

Then $a$ is said to be a zero of multiplicity $k$  if and only if  there exists non-zero $L \in \R$ such that:

:$\ds \lim_{z \mathop \to a} \dfrac {\cmod {\map f z} } {\cmod {z - a}^k} = L$

 

Category:Definitions/Complex Analysis",Definition:Multiplicity (Complex Analysis),['Definitions/Complex Analysis']
Definition:Multiplicity,Multiplicity,"Let $G = \struct {V, E}$ be a multigraph.

The multiplicity of an edge is the number of edges having the same pair of endvertices.


For example, simple edges have multiplicity $1$.

Thus, an edge is a multiple edge  if and only if  its multiplicity exceeds $1$.

Category:Definitions/Multigraphs",Definition:Multiple Edge/Multiplicity,['Definitions/Multigraphs']
Definition:Multiplicity,Multiplicity,"The multiplicity of a multigraph is the maximum multiplicity of its (multiple) edges.


Category:Definitions/Multigraphs",Definition:Multigraph/Multiplicity,['Definitions/Multigraphs']
Definition:Multiplicity,Multiplicity,"Let $n > 1 \in \Z$.

Let:
:$n = p_1^{k_1} p_2^{k_2} \cdots p_r^{k_r}$
be the prime decomposition of $n$, where:
:$p_1 < p_2 < \cdots < p_r$ are distinct primes
:$k_1, k_2, \ldots, k_r$ are (strictly) positive integers.


For each $p_j \in \set {p_1, p_2, \ldots, p_r}$, its power $k_j$ is known as the multiplicity of $p_j$.",Definition:Prime Decomposition/Multiplicity,['Definitions/Prime Decompositions']
Definition:Negation,Negation,"The logical not or (logical) negation operator is a unary connective whose action is to reverse the truth value of the statement on which it operates.

:$\neg p$ is defined as:
:$p$ is not true
:It is not the case that $p$ is true
:It is false that $p$
:$p$ is false.


Thus the statement $\neg p$ is called the negation of $p$.


$\neg p$ is voiced not $p$.


=== Truth Function ===
The logical not connective defines the truth function $f^\neg$ as follows:

 
 
 
 

=== Truth Table ===
The characteristic truth table of the negation operator $\neg p$ is as follows:

:$\begin {array} {|c||c|} \hline p & \neg p \\ \hline \F & \T \\ \T & \F \\ \hline \end {array}$

=== Boolean Interpretation ===
Let $\mathbf A$ be a propositional formula.

Let $\neg$ denote the negation operator.


The truth value of $\neg \mathbf A$ under a boolean interpretation $v$ is given by:

:$\map v {\neg \mathbf A} = \begin {cases} \T & : \map v {\mathbf A} = \F \\ \F & : \map v {\mathbf A} = \T \end {cases}$",Definition:Logical Not,"['Definitions/Logical Negation', 'Definitions/Propositional Logic']"
Definition:Negation,Negation,"The negation function is the function defined on the various standard number systems as follows:


=== Integer Negation Function ===
The negation function $h: \Z \to \Z$ is defined on the set of integers as:
:$\forall n \in \Z: \map h n = -n$


Category:Definitions/Negation Functions
Category:Definitions/Integers

=== Rational Negation Function ===
The negation function $h: \Q \to \Q$ is defined on the set of rational numbers as:
:$\forall x \in \Q: \map h x = -x$


Category:Definitions/Negation Functions
Category:Definitions/Rational Numbers

=== Real Negation Function ===
The negation function $h: \R \to \R$ is defined on the set of real numbers as:
:$\forall x \in \R: \map h x = -x$


Category:Definitions/Negation Functions
Category:Definitions/Real Numbers

=== Complex Negation Function ===
The negation function $h: \R \to \R$ is defined on the set of complex numbers as:
:$\forall z = x + i y \in \C: \map h z = -x - i y$


Category:Definitions/Negation Functions
Category:Definitions/Complex Numbers

Category:Definitions/Numbers
Category:Definitions/Negation Functions",Definition:Negation Function,"['Definitions/Numbers', 'Definitions/Negation Functions']"
Definition:Net,Net,"Let $X$ be a nonempty set.

Let $\struct {\Lambda, \precsim}$ be a preordered set.

Let $F: \Lambda \to X$ be a mapping.


Then $F$ is referred to as a net.",Definition:Net (Preordered Set),['Definitions/Nets (Topology)']
Definition:Net,Net,"Let $M = \struct {A, d}$ be a metric space.

Let $\epsilon \in \R_{>0}$ be a strictly positive real number.


An $\epsilon$-net for $M$ is a subset $S \subseteq A$ such that:
:$\ds A \subseteq \bigcup_{x \mathop \in S} \map {B_\epsilon} x$
where $\map {B_\epsilon} x$ denotes the open $\epsilon$-ball of $x$ in $M$.


That is, it is a subset of $A$ such that the set of all open $\epsilon$-balls of elements of that set forms a cover for $A$.


=== Finite Net ===
Let $M$ be a metric space.

Let $\epsilon \in \R_{>0}$ be a strictly positive real number.


A finite $\epsilon$-net for $M$ is an $\epsilon$-net for $M$ which is a finite set.",Definition:Net (Metric Space),['Definitions/Metric Spaces']
Definition:Net,Net,"Net means what is left after deductions.


=== Net Profit ===
Net profit is profit after subtracting the operating costs.

=== Net Weight ===
The net weight of an object is the weight remaining after subtracting the tare for the weight of whatever wrapper, container or shipping means in which the object is contained when its weight is measured.",Definition:Net (Accounting),"['Definitions/Net (Accounting)', 'Definitions/Accounting']"
Definition:Neutral,Neutral,A body which has no electric charge on it is described as (electrically) neutral.,Definition:Electric Charge/Neutral,['Definitions/Electric Charge']
Definition:Neutral,Neutral,"Let $B$ be a particle, a system of particles, or a body which is in equilibrium.

Let $B$ be such that if a small displacement is applied, $B$ remains in its new position.


$B$ is then said to be in neutral equilibrium.",Definition:Equilibrium (Mechanics)/Neutral,"['Definitions/Neutral Equilibrium', 'Definitions/Equilibrium (Mechanics)']"
Definition:Neutral,Neutral,"Let $\struct {S, \circ}$ be an algebraic structure.

An element $e \in S$ is called an identity (element)  if and only if  it is both a left identity and a right identity:

:$\forall x \in S: x \circ e = x = e \circ x$


In Identity is Unique it is established that an identity element, if it exists, is unique within $\struct {S, \circ}$.

Thus it is justified to refer to it as the identity (of a given algebraic structure).


This identity is often denoted $e_S$, or $e$ if it is clearly understood what structure is being discussed.",Definition:Identity (Abstract Algebra)/Two-Sided Identity,"['Definitions/Identity Elements', 'Definitions/Identities']"
Definition:Node,Node,"=== Vertex ===

For graphs which are not trees, the term vertex is generally used instead:

 

Let $G = \struct {V, E}$ be a graph.

The vertices (singular: vertex) are the elements of $V$.

Informally, the vertices are the points that are connected by the edges.


In the above, the vertices are the points $A, B, C, D, E, F, G$ which are marked as dots.

=== Node ===
The vertices of a tree are called its nodes.",Definition:Node (Graph Theory),"['Definitions/Tree Theory', 'Definitions/Graph Theory']"
Definition:Node,Node,An acnode is a singular point of the locus of an equation describing a curve which is not actually on that curve.,Definition:Acnode,"['Definitions/Acnodes', 'Definitions/Analytic Geometry']"
Definition:Node,Node,A crunode is a double point $P$ of the locus of an equation describing a curve which intersects itself in such a way that there are $2$ distinct tangents at $P$.,Definition:Crunode,"['Definitions/Crunodes', 'Definitions/Double Points', 'Definitions/Analytic Geometry']"
Definition:Node,Node,"A cusp is a singular point on a curve at which there are two different tangents which coincide.

Thus a cusp is a special case of a double point in which the tangents are coincident.


=== Cusp of the First Kind ===
A cusp of the first kind is a cusp in which both parts of the curve lie on opposite sides of the coincident tangents defining that cusp.


=== Single Cusp of the First Kind ===
A single cusp of the first kind is a single cusp in which both parts of the curve lie on opposite sides of the coincident tangents defining that single cusp.
:

=== Double Cusp of the First Kind ===
A double cusp of the first kind is a double cusp in which both parts of the curve lie on opposite sides of the coincident tangents defining that double cusp.
:

=== Cusp of the Second Kind ===
A cusp of the second kind is a cusp in which both parts of the curve lie on the same side of the coincident tangents defining that cusp.


=== Single Cusp of the Second Kind ===
A single cusp of the second kind is a single cusp in which both parts of the curve lie on the same side of the coincident tangents defining that single cusp.
:

=== Double Cusp of the Second Kind ===
A double cusp of the second kind is a double cusp in which both parts of the curve lie on the same side of the coincident tangents defining that double cusp.
:",Definition:Cusp,"['Definitions/Cusps', 'Definitions/Double Points', 'Definitions/Analytic Geometry']"
Definition:Node,Node,"A double cusp is a cusp in which $2$ or more parts of the curve are both continuous through the point.

That is, the curve is tangential to itself.


=== Double Cusp of the First Kind ===
A double cusp of the first kind is a double cusp in which both parts of the curve lie on opposite sides of the coincident tangents defining that double cusp.
:

=== Double Cusp of the Second Kind ===
A double cusp of the second kind is a double cusp in which both parts of the curve lie on the same side of the coincident tangents defining that double cusp.
:

=== Point of Osculoinflection ===
A point of osculoinflection is a double cusp in which one or both parts of the curve has a point of inflection at the point of contact with the tangents defining that double cusp.
:",Definition:Double Cusp,"['Definitions/Double Cusps', 'Definitions/Cusps']"
Definition:Node,Node,"A metagraph $\GG$ consists of:

* objects $X, Y, Z, \ldots$
* morphisms $f, g, h, \ldots$ between its objects

These are subjected to the following two axioms:

 
 
 
 

 
A metagraph is purely axiomatic, and does not use set theory.

For example, the objects are not ""elements of the set of objects"", because these axioms are (without further interpretation) unfounded in set theory.",Definition:Metagraph,['Definitions/Category Theory']
Definition:Node,Node,"Let $\closedint a b$ be a closed real interval.

Let $T := \set {a = t_0, t_1, t_2, \ldots, t_{n - 1}, t_n = b}$ form a subdivision of $\closedint a b$.

Let $S: \closedint a b \to \R$ be a spline function on $\closedint a b$ on $T$.


The points $T := \set {t_0, t_1, t_2, \ldots, t_{n - 1}, t_n}$ of $S$ are known as the knots.


=== Knot Vector ===
Let $\closedint a b$ be a closed real interval.

Let $T := \set {a = t_0, t_1, t_2, \ldots, t_{n - 1}, t_n = b}$ form a subdivision of $\closedint a b$.

Let $S: \closedint a b \to \R$ be a spline function on $\closedint a b$ on $T$.


The ordered $n + 1$-tuple $\mathbf t := \tuple {t_0, t_1, t_2, \ldots, t_{n - 1}, t_n}$ of $S$ is known as the knot vector.


Category:Definitions/Knots of Splines",Definition:Spline Function/Knot,"['Definitions/Knots of Splines', 'Definitions/Splines']"
Definition:Noetherian,Noetherian,"Let $A$ be a commutative ring with unity.

Let $M$ be an $A$-module.


=== Definition 1 ===
Let $A$ be a commutative ring with unity.

Let $M$ be an $A$-module.


$M$ is a Noetherian module  if and only if  every submodule of $M$ is finitely generated.

=== Definition 2 ===
Let $A$ be a commutative ring with unity.

Let $M$ be an $A$-module.


$M$ is a Noetherian module  if and only if  it satisfies the ascending chain condition on submodules.

=== Definition 3 ===
Let $A$ be a commutative ring with unity.

Let $M$ be an $A$-module.


$M$ is a Noetherian module  if and only if  it satisfies the maximal condition on submodules.",Definition:Noetherian Module,"['Definitions/Noetherian Modules', 'Definitions/Examples of Modules']"
Definition:Noetherian,Noetherian,"=== Definition 1 ===
A commutative ring with unity $A$ is Noetherian  if and only if  every ideal of $A$ is finitely generated.

=== Definition 2 ===
A commutative ring with unity $A$ is Noetherian  if and only if  it satisfies the ascending chain condition on ideals.

=== Definition 3 ===
A commutative ring with unity $A$ is Noetherian  if and only if  it satisfies the maximal condition on ideals.

=== Definition 4 ===
A commutative ring with unity $A$ is Noetherian  if and only if  it is Noetherian as an $A$-module.",Definition:Noetherian Ring,"['Definitions/Noetherian Rings', 'Definitions/Ring Theory']"
Definition:Noetherian,Noetherian,"=== Definition 1 ===
A topological space $T = \struct {S, \tau}$ is Noetherian  if and only if  its set of closed sets, ordered by the subset relation, satisfies the descending chain condition.

=== Definition 2 ===
A topological space $T = \struct {S, \tau}$ is Noetherian  if and only if  its set of open sets, ordered by the subset relation, satisfies the ascending chain condition.

=== Definition 3 ===
A topological space $T = \struct {S, \tau}$ is Noetherian  if and only if  each non-empty set of closed sets has a minimal element with respect to the subset relation.

=== Definition 4 ===
A topological space $T = \struct {S, \tau}$ is Noetherian  if and only if  each non-empty set of open sets has a maximal element   the subset relation.",Definition:Noetherian Topological Space,"['Definitions/Noetherian Topological Spaces', 'Definitions/Topology', 'Definitions/Algebraic Geometry']"
Definition:Noetherian,Noetherian,"Let $\struct {X, \OO_X}$ be a scheme.


Then $\struct {X, \OO_X}$ is Noetherian  if and only if  $\struct {X, \OO_X}$ is locally Noetherian and quasi-compact.

 ",Definition:Noetherian Scheme,"['Definitions/Algebraic Geometry', 'Definitions/Schemes']"
Definition:Normal,Normal,"Let $X = \struct {M_1, d_1}$ and $Y = \struct {M_2, d_2}$ be complete metric spaces.

Let $\FF = \family {f_i}_{i \mathop \in I}$ be a family of continuous mappings $f_i: X \to Y$.


Then $\FF$ is a normal family  if and only if :
:every sequence of mappings in $\FF$ contains a subsequence which converges uniformly on compact subsets of $X$ to a continuous function $f: X \to Y$.",Definition:Normal Family,['Definitions/Metric Spaces']
Definition:Normal,Normal,"Let $\HH$ be a Hilbert space.

Let $\mathbf T: \HH \to \HH$ be a bounded linear operator.


Then $\mathbf T$ is said to be normal  if and only if :

:$\mathbf T^* \mathbf T = \mathbf T \mathbf T^*$

where $\mathbf T^*$ denotes the adjoint of $\mathbf T$.",Definition:Normal Operator,"['Definitions/Adjoints', 'Definitions/Linear Operators', 'Definitions/Linear Transformations on Hilbert Spaces']"
Definition:Normal,Normal,"Let $C$ be a curve embedded in the plane.

The normal to $C$ at a point $P$ is defined as the straight line which lies perpendicular to the tangent at $P$ and in the same plane as $P$.",Definition:Normal to Curve,['Definitions/Analytic Geometry']
Definition:Normal,Normal,"Let $S$ be a surface in ordinary $3$-space.

Let $P$ be a point of $S$.


Let $\mathbf n$ be a vector whose initial point is at $P$ such that $\mathbf n$ is perpendicular to $S$ at $P$.


Then $\mathbf n$ is a normal vector to $S$ at $P$.",Definition:Normal Vector,"['Definitions/Normal Vectors', 'Definitions/Vectors']"
Definition:Normal,Normal,"Let $M$ be a differentiable manifold.

Let $p \in M$ be a point in $M$.

Let $N_p M$ be the normal space at $p$.


The normal bundle of $M$ is the disjoint union of all the normal spaces of $M$:

:$\ds N M = \coprod_{p \mathop \in M} N_p M$",Definition:Normal Bundle,['Definitions/Topology']
Definition:Normal,Normal,"Let $G$ be a group.

Let $N$ be a subgroup of $G$.


$N$ is a normal subgroup of $G$  if and only if :


=== Definition 1 ===
Let $G$ be a group.

Let $N$ be a subgroup of $G$.


$N$ is a normal subgroup of $G$  if and only if :

:$\forall g \in G: g \circ N = N \circ g$

where $g \circ N$ denotes the subset product of $g$ with $N$.

=== Definition 2 ===
Let $G$ be a group.

Let $N$ be a subgroup of $G$.


$N$ is a normal subgroup of $G$  if and only if :

: Every right coset of $N$ in $G$ is a left coset
that is:
: The right coset space of $N$ in $G$ equals its left coset space.

=== Definition 3 ===
Let $G$ be a group.

Let $N$ be a subgroup of $G$.


$N$ is a normal subgroup of $G$  if and only if :

 
 
 
 

where $g \circ N$ etc. denotes the subset product of $g$ with $N$.

=== Definition 4 ===
Let $G$ be a group.

Let $N$ be a subgroup of $G$.


$N$ is a normal subgroup of $G$  if and only if :

 
 
 
 

where $g \circ N$ etc. denotes the subset product of $g$ with $N$.

=== Definition 5 ===
Let $G$ be a group.

Let $N$ be a subgroup of $G$.


$N$ is a normal subgroup of $G$  if and only if :

 
 
 
 

where $g \circ N$ etc. denotes the subset product of $g$ with $N$.


That is:
:$\forall g \in G: \map {\kappa_g} N = N$

where $\map {\kappa_g} N$ denotes the inner automorphism of $N$ by $g$.

=== Definition 6 ===
Let $G$ be a group.

Let $N$ be a subgroup of $G$.


$N$ is a normal subgroup of $G$  if and only if :

 
 
 
 

=== Definition 7 ===
Let $G$ be a group.

Let $N$ be a subgroup of $G$.

$N$ is a normal subgroup of $G$  if and only if :

:$N$ is a normal subset of $G$.",Definition:Normal Subgroup,"['Definitions/Normal Subgroups', 'Definitions/Normal Subsets', 'Definitions/Normality in Groups', 'Definitions/Subgroups']"
Definition:Normal,Normal,"Let $\struct {G, \circ}$ be a group.

Let $S \subseteq G$ be a general subset of $G$.


Then $S$ is a normal subset of $G$  if and only if :


=== Definition 1===
Let $\struct {G, \circ}$ be a group.

Let $S \subseteq G$ be a general subset of $G$.


Then $S$ is a normal subset of $G$  if and only if :

:$\forall g \in G: g \circ S = S \circ g$

=== Definition 2===
Let $\struct {G, \circ}$ be a group.

Let $S \subseteq G$ be a general subset of $G$.


Then $S$ is a normal subset of $G$  if and only if :

:$\forall g \in G: g \circ S \circ g^{-1} = S$
or, equivalently:
:$\forall g \in G: g^{-1} \circ S \circ g = S$

=== Definition 3===
Let $\struct {G, \circ}$ be a group.

Let $S \subseteq G$ be a general subset of $G$.


Then $S$ is a normal subset of $G$  if and only if :

:$\forall g \in G: g \circ S \circ g^{-1} \subseteq S$
or, equivalently:
:$\forall g \in G: g^{-1} \circ S \circ g \subseteq S$

=== Definition 4===
Let $\struct {G, \circ}$ be a group.

Let $S \subseteq G$ be a general subset of $G$.


Then $S$ is a normal subset of $G$  if and only if :

: $\forall g \in G: S \subseteq g \circ S \circ g^{-1}$
or, equivalently:
: $\forall g \in G: S \subseteq g^{-1} \circ S \circ g$

=== Definition 5 ===
Let $\struct {G, \circ}$ be a group.

Let $S \subseteq G$ be a general subset of $G$.


Then $S$ is a normal subset of $G$  if and only if :

:$\forall x, y \in G: x \circ y \in S \implies y \circ x \in S$

=== Definition 6 ===
Let $\struct {G, \circ}$ be a group.

Let $S \subseteq G$ be a general subset of $G$.


Then $S$ is a normal subset of $G$  if and only if :

:$\map {N_G} S = G$
where $\map {N_G} S$ denotes the normalizer of $S$ in $G$.

=== Definition 7 ===
Let $\struct {G, \circ}$ be a group.

Let $S \subseteq G$ be a general subset of $G$.


Then $S$ is a normal subset of $G$  if and only if :

:$\forall g \in G: g \circ S \subseteq S \circ g$
or:
:$\forall g \in G: S \circ g \subseteq g \circ S$",Definition:Normal Subset,"['Definitions/Normal Subsets', 'Definitions/Normality in Groups', 'Definitions/Subsets']"
Definition:Normal,Normal,"Let $G$ be a group whose identity is $e$.


A normal series for $G$ is a sequence of (normal) subgroups of $G$:
:$\set e = G_0 \lhd G_1 \lhd \cdots \lhd G_n = G$

where $G_{i - 1} \lhd G_i$ denotes that $G_{i - 1}$ is a proper normal subgroup of $G_i$.


=== Factors ===
Let $G$ be a group whose identity is $e$.

Let $\sequence {G_i}_{i \mathop \in \closedint 0 n}$ be a normal series for $G$:
:$\sequence {G_i}_{i \mathop \in \closedint 0 n} = \tuple {\set e = G_0 \lhd G_1 \lhd \cdots \lhd G_{n - 1} \lhd G_n = G}$


The factor groups of $\sequence {G_i}_{i \mathop \in \closedint 0 n}$:
:$\set e = G_0 \lhd G_1 \lhd \cdots \lhd G_n = G$

are the quotient groups:
:$G_1 / G_0, G_2 / G_1, \ldots, G_i / G_{i - 1}, \ldots, G_n / G_{n-1}$

=== Normal Series as Sequence of Homomorphisms ===
Let $G$ be a group whose identity is $e$.

Let $\sequence {G_i}_{i \mathop \in \closedint 0 n}$ be a normal series for $G$:
:$\sequence {G_i}_{i \mathop \in \closedint 0 n} = \tuple {\set e = G_0 \lhd G_1 \lhd \cdots \lhd G_{n - 1} \lhd G_n = G}$
whose factor groups are:
:$H_1 = G_1 / G_0, H_2 = G_2 / G_1, \ldots, H_i = G_i / G_{i - 1}, \ldots, H_n = G_n / G_{n - 1}$


By Kernel of Group Homomorphism Corresponds with Normal Subgroup of Domain, such a series can also be expressed as a sequence $\phi_1, \ldots, \phi_n$ of group homomorphisms:

:$\set e \stackrel {\phi_1} {\to} H_1 \stackrel {\phi_2} {\to} H_2 \stackrel {\phi_3} {\to} \cdots \stackrel {\phi_n} {\to} H_n$

=== Infinite Normal Series ===
A normal series may or may not terminate at either end:
:$\cdots \stackrel {\phi_{i - 1} } {\longrightarrow} H_{i - 1} \stackrel {\phi_i} {\longrightarrow} H_i \stackrel {\phi_{i + 1} } {\longrightarrow} H_{i + 1} \stackrel {\phi_{i + 2} } {\longrightarrow} \cdots$

Such a series is referred to as an infinite normal series.

The context will determine which end, if either, it terminates.

=== Length ===
Let $G$ be a group whose identity is $e$.

Let $\sequence {G_i}_{i \mathop \in \closedint 0 n}$ be a normal series for $G$:
:$\sequence {G_i}_{i \mathop \in \closedint 0 n} = \tuple {\set e = G_0 \lhd G_1 \lhd \cdots \lhd G_{n-1} \lhd G_n = G}$


The length of $\sequence {G_i}_{i \mathop \in \closedint 0 n}$ is the number of (normal) subgroups which make it.

In this context, the length of $\sequence {G_i}_{i \mathop \in \closedint 0 n}$ is $n$.


If such a normal series is infinite, then its length is not defined.


Category:Definitions/Normal Series",Definition:Normal Series,"['Definitions/Normal Series', 'Definitions/Normality in Groups']"
Definition:Normal,Normal,"Let $G$ be a group.

Let $S \subseteq G$ be a subset.


=== Definition 1 ===
Let $G$ be a group. 

Let $S \subseteq G$ be a subset.


The normal subgroup generated by $S$, denoted $\gen {S^G}$,  is the intersection of all normal subgroups of $G$ containing $S$.

=== Definition 2 ===
 

Let $G$ be a group. 

Let $S \subseteq G$ be a subset.


The normal subgroup generated by $S$, denoted $\gen {S^G}$,  is the smallest normal subgroup of $G$ containing $S$:
:$\gen {S^G} = \gen {x S x^{-1}: x \in G}$",Definition:Generated Normal Subgroup,"['Definitions/Generated Normal Subgroups', 'Definitions/Normal Subgroups', 'Definitions/Normality in Groups', 'Definitions/Generated Subgroups', 'Definitions/Generators of Groups', 'Definitions/Subgroups']"
Definition:Normal,Normal,"=== Definition 1 ===
Let $L / K$ be a field extension.

Then $L / K$ is a normal extension  if and only if :
:for every irreducible polynomial $f \in K \sqbrk x$ with at least one root in $L$, $f$ splits completely in $L$.

=== Definition 2 ===
Let $L / K$ be a field extension.

Let $\overline K$ be the algebraic closure of $K$.

Let $\Gal {L / K}$ denote the set of embeddings of $L$ in $\overline K$ which fix $K$ pointwise.


Then $L / K$ is a normal extension  if and only if :
: $\map \sigma L = L$
for each $\sigma \in \Gal {L / K}$.",Definition:Normal Extension,['Definitions/Field Extensions']
Definition:Normal,Normal,"Let $x$ be an ordinal.


The Cantor normal form of $x$ is an ordinal summation:

:$x = \omega^{a_1} n_1 + \dots + \omega^{a_k} n_k$

where:

:$k \in \N$ is a natural number
:$\omega$ is the minimally inductive set
:$\sequence {a_i}$ is a strictly decreasing finite sequence of ordinals.
:$\sequence {n_i}$ is a finite sequence of finite ordinals


In summation notation:

:$x = \ds \sum_{i \mathop = 1}^k \omega^{a_i} n_i$

 ",Definition:Cantor Normal Form,['Definitions/Ordinal Arithmetic']
Definition:Normal,Normal,"A real number $r$ is normal with respect to a number base $b$  if and only if  its basis expansion in number base $b$ is such that:

:no finite sequence of digits of $r$ of length $n$ occurs more frequently than any other such finite sequence of length $n$.


In particular, for number base $b$, all digits of $r$ have the same natural density in the basis expansion of $r$.",Definition:Normal Number,"['Definitions/Normal Numbers', 'Definitions/Numbers']"
Definition:Normal,Normal,"Let $X$ be a continuous random variable on a probability space $\struct {\Omega, \Sigma, \Pr}$.


Then $X$ has a Gaussian distribution  if and only if  the probability density function of $X$ is:

:$\map {f_X} x = \dfrac 1 {\sigma \sqrt {2 \pi} } \map \exp {-\dfrac {\paren {x - \mu}^2} {2 \sigma^2} }$

for $\mu \in \R, \sigma \in \R_{> 0}$.


This is written: 

:$X \sim \Gaussian \mu {\sigma^2}$",Definition:Gaussian Distribution,"['Definitions/Gaussian Distribution', 'Definitions/Examples of Probability Distributions']"
Definition:Normal,Normal,"A propositional formula $P$ is in conjunctive normal form  if and only if  it consists of a conjunction of:
:$(1): \quad$ disjunctions of literals
and/or:
:$(2): \quad$ literals.",Definition:Conjunctive Normal Form,"['Definitions/Conjunctive Normal Form', 'Definitions/Conjunction', 'Definitions/Propositional Logic']"
Definition:Normal,Normal,"A propositional formula $P$ is in disjunctive normal form  if and only if  it consists of a disjunction of:
:$(1): \quad$ conjunctions of literals
and/or:
:$(2): \quad$ literals.",Definition:Disjunctive Normal Form,"['Definitions/Disjunctive Normal Form', 'Definitions/Propositional Logic']"
Definition:Normal,Normal,"A propositional formula $P$ is in negation normal form (NNF)  if and only if :
* The only logical connectives connecting substatements of $P$ are Not, And and Or, that is, elements of the set $\left\{{\neg, \land, \lor}\right\}$;
* The Not sign $\neg$ appears only in front of atomic statements.

That is $P$ is in negation normal form iff it consists of literals, conjunctions and disjunctions.",Definition:Negation Normal Form,['Definitions/Propositional Logic']
Definition:Normal,Normal,"Let $T = \struct {S, \tau}$ be a topological space.


$\struct {S, \tau}$ is a normal space  if and only if :
:$\struct {S, \tau}$ is a $T_4$ space
:$\struct {S, \tau}$ is a $T_1$ (Frchet) space.


That is:
:$\forall A, B \in \map \complement \tau, A \cap B = \O: \exists U, V \in \tau: A \subseteq U, B \subseteq V, U \cap V = \O$ 

:$\forall x, y \in S$, both:
::$\exists U \in \tau: x \in U, y \notin U$
::$\exists V \in \tau: y \in V, x \notin V$

 

This space is also referred to as normal Hausdorff.",Definition:Normal Space,"['Definitions/Normal Spaces', 'Definitions/T4 Spaces', 'Definitions/T1 Spaces', 'Definitions/Separation Axioms']"
Definition:Normal,Normal,"Let $T = \struct {S, \tau}$ be a topological space.


$\struct {S, \tau}$ is a completely normal space  if and only if :
:$\struct {S, \tau}$ is a $T_5$ space
:$\struct {S, \tau}$ is a $T_1$ (Frchet) space.


That is:

:$\forall A, B \subseteq S, A^- \cap B = A \cap B^- = \O: \exists U, V \in \tau: A \subseteq U, B \subseteq V, U \cap V = \O$ 

:$\forall x, y \in S$, both:
::$\exists U \in \tau: x \in U, y \notin U$
::$\exists V \in \tau: y \in V, x \notin V$

 ",Definition:Completely Normal Space,"['Definitions/Completely Normal Spaces', 'Definitions/Normal Spaces', 'Definitions/T5 Spaces', 'Definitions/T1 Spaces', 'Definitions/Separation Axioms']"
Definition:Normal,Normal,"Let $T = \struct {S, \tau}$ be a topological space.


$T$ is fully normal  if and only if :
:Every open cover of $S$ has a star refinement
:All points of $T$ are closed.


That is, $T$ is fully normal  if and only if :
:$T$ is fully $T_4$
:$T$ is a $T_1$ (Frchet) space.

 ",Definition:Fully Normal Space,"['Definitions/Fully Normal Spaces', 'Definitions/Fully T4 Spaces', 'Definitions/T1 Spaces', 'Definitions/Compact Spaces', 'Definitions/Separation Axioms']"
Definition:Normal,Normal,"Let $T = \struct {S, \tau}$ be a topological space.


$\struct {S, \tau}$ is a perfectly normal space  if and only if :
:$\struct {S, \tau}$ is a perfectly $T_4$ space
:$\struct {S, \tau}$ is a $T_1$ (Frchet) space.


That is:

:Every closed set in $T$ is a $G_\delta$ set.

:$\forall x, y \in S$, both:
::$\exists U \in \tau: x \in U, y \notin U$
::$\exists V \in \tau: y \in V, x \notin V$",Definition:Perfectly Normal Space,"['Definitions/Perfectly Normal Spaces', 'Definitions/Normal Spaces', 'Definitions/Perfectly T4 Spaces', 'Definitions/T1 Spaces', 'Definitions/Separation Axioms']"
Definition:Normal,Normal,"Let $S$ be a set.

Then $S$ is an ordinary set  if and only if :
:$S \notin S$

That is,  if and only if  $S$ is not an element of itself.",Definition:Ordinary Set,['Definitions/Set Theory']
Definition:Normal,Normal,"Let $A$ be a class.

Let $\RR$ be a relation on $A$.


An element $x$ of $A$ is left normal with respect to $\RR$  if and only if :
:$\forall y \in A: \map \RR {x, y}$ holds.",Definition:Left Normal Element of Relation,['Definitions/Relations']
Definition:Normal,Normal,"Let $A$ be a class.

Let $\RR$ be a relation on $A$.


An element $x$ of $A$ is right normal with respect to $\RR$  if and only if :
:$\forall y \in A: \map \RR {y, x}$ holds.",Definition:Right Normal Element of Relation,['Definitions/Relations']
Definition:Normal,Normal,"Let $G$ be a group.

Let $S$ be a subset of $G$.


Then the normalizer of $S$ in $G$ is the set $\map {N_G} S$ defined as:
:$\map {N_G} S := \set {a \in G: S^a = S}$

where $S^a$ is the $G$-conjugate of $S$ by $a$.


If $S$ is a singleton such that $S = \set s$, we may also write $\map {N_G} s$ for $\map {N_G} S = \map {N_G} {\set s}$, as long as there is no possibility of confusion.",Definition:Normalizer,['Definitions/Normality in Groups']
Definition:Null,Null,"Let $\struct {X, \Sigma, \mu}$ be a measure space.

A set $N \in \Sigma$ is called a ($\mu$-)null set  if and only if  $\map \mu N = 0$.


 

=== Family of Null Sets ===

The family of $\mu$-null sets, $\set {N \in \Sigma: \map \mu N = 0}$, is denoted $\NN_\mu$. 


=== Signed Measure ===
Let $\struct {X, \Sigma}$ be a measurable space.

Let $\mu$ be a signed measure on $\struct {X, \Sigma}$. 

Let $N \in \Sigma$.


We say that $N$ is a $\mu$-null set  if and only if :

:for each $A \in \Sigma$ with $A \subseteq N$, we have $\map \mu A = 0$


Category:Definitions/Signed Measures",Definition:Null Set,"['Definitions/Null Sets', 'Definitions/Measure Theory', 'Definitions/Topology']"
Definition:Null,Null,"The null relation is a relation $\RR$ in $S$ to $T$ such that $\RR$ is the empty set:
:$\RR \subseteq S \times T: \RR = \O$


That is, no element of $S$ relates to any element in $T$:
:$\RR: S \times T: \forall \tuple {s, t} \in S \times T: \neg s \mathrel \RR t$",Definition:Null Relation,"['Definitions/Null Relation', 'Definitions/Empty Set', 'Definitions/Examples of Relations']"
Definition:Null,Null,"A ring with one element is called the null ring.

That is, the null ring is $\struct {\set {0_R}, +, \circ}$, where ring addition and the ring product are defined as:

 
 
 
 ",Definition:Null Ring,"['Definitions/Ring Theory', 'Definitions/Examples of Rings']"
Definition:Null,Null,"Let $\left({R, +_R, \circ_R}\right)$ be a ring.

Let $G$ be the trivial group.


Then the $R$-module $\left({G, +_G, \circ}\right)_R$ is known as the null module.",Definition:Null Module,['Definitions/Module Theory']
Definition:Null,Null,"Let $\struct {X, \Sigma}$ be a measurable space.


Then the null measure is the measure defined by:

:$\mu: \Sigma \to \overline \R: \map \mu E := 0$

where $\overline \R$ denotes the extended real numbers.",Definition:Null Measure,"['Definitions/Null Measure', 'Definitions/Measures']"
Definition:Null,Null,"A null sequence is a sequence which converges to zero.


=== Normed Division Ring ===
Let $\struct {R, \norm {\,\cdot\,} }$ be a normed division ring with zero $0_R$.

Let $\sequence {x_n}$ be a sequence in $R$ which converges to the limit $0_R$:

:$\ds \lim_{n \mathop \to \infty} x_n = 0_R$


Then $\sequence {x_n}$ is called a null sequence.",Definition:Null Sequence,"['Definitions/Null Sequences', 'Definitions/Convergence', 'Definitions/Sequences', 'Definitions/Analysis', 'Definitions/Real Analysis', 'Definitions/Complex Analysis', 'Definitions/Normed Division Rings']"
Definition:Null,Null,"The null URM program is a URM program which contains no instructions.

That is, a URM program whose length is zero.",Definition:Unlimited Register Machine/Null Program,"['Definitions/Null URM Program', 'Definitions/URM Programs', 'Definitions/Unlimited Register Machines']"
Definition:Null,Null,"The null graph is the graph which has no vertices.

That is, the null graph is the graph of order zero.


It is called the null graph because, from Empty Set is Unique, there is only one such entity.",Definition:Null Graph,"['Definitions/Null Graph', 'Definitions/Graph Theory']"
Definition:Null,Null,"A null string is a string with no symbols in it.

In particular, the null string is a word.


The null string can be denoted $\epsilon$.",Definition:Null String,['Definitions/Collations']
Definition:Null,Null,"Let:
$\quad \mathbf A_{m \times n} = \begin {bmatrix}
a_{11} & a_{12} & \cdots & a_{1n} \\
a_{21} & a_{22} & \cdots & a_{2n} \\
\vdots & \vdots & \ddots & \vdots \\
a_{m1} & a_{m2} & \cdots & a_{mn} \\
\end {bmatrix}$,  $\mathbf x_{n \times 1} = \begin {bmatrix} x_1 \\ x_2 \\ \vdots \\ x_n \end {bmatrix}$, $\mathbf 0_{m \times 1} = \begin {bmatrix} 0 \\ 0 \\ \vdots \\ 0 \end {bmatrix}$

be matrices where each column is a member of a real vector space.

The set of all solutions to $\mathbf A \mathbf x = \mathbf 0$:

:$\map {\mathrm N} {\mathbf A} = \set {\mathbf x \in \R^n : \mathbf {A x} = \mathbf 0}$

is called the null space of $\mathbf A$.


 ",Definition:Null Space,"['Definitions/Null Spaces', 'Definitions/Linear Algebra']"
Definition:Null,Null,"Let $\struct {R, +, \circ}$ be a ring whose zero is $0_R$.


The null ring $\struct {\set {0_R}, +, \circ}$ is called the null ideal of $R$.",Definition:Null Ideal,['Definitions/Ideal Theory']
Definition:Null,Null,"Let $\mathbf C$ be a category.


A zero object of $\mathbf C$ is an object which is initial and terminal.",Definition:Zero Object,['Definitions/Category Theory']
Definition:One-to-One,One-to-One,"=== Definition 1 ===
A mapping $f$ is an injection, or injective  if and only if :
:$\forall x_1, x_2 \in \Dom f: \map f {x_1} = \map f {x_2} \implies x_1 = x_2$


That is, an injection is a mapping such that the output uniquely determines its input.


=== Definition 1 a ===

This can otherwise be put:
A mapping $f$ is an injection, or injective  if and only if :

:$\forall x_1, x_2 \in \Dom f: x_1 \ne x_2 \implies \map f {x_1} \ne \map f {x_2}$


That is, distinct elements of the domain are mapped to distinct elements of the image.

=== Definition 2 ===
An injection is a relation which is both one-to-one and left-total.


Thus, a relation $f$ is an injection  if and only if :

 
 
 
 
 

=== Definition 3 ===
Let $f$ be a mapping.

Then $f$ is an injection  if and only if :
:$f^{-1} {\restriction_{\Img f} }: \Img f \to \Dom f$ is a mapping
where $f^{-1} {\restriction_{\Img f} }$ is the restriction of the inverse of $f$ to the image set of $f$.

=== Definition 4 ===
Let $f$ be a mapping.

$f$ is an injection  if and only if :
:$\forall y \in \Img f: \card {\map {f^{-1} } y} = \card {\set {f^{-1} \sqbrk {\set y} } } = 1$
where:
:$\Img f$ denotes the image set of $f$
:$\card {\, \cdot \,}$ denotes the cardinality of a set
:$\map {f^{-1} } y$ is the preimage of $y$
:$f^{-1} \sqbrk {\set y}$ is the preimage of the subset $\set y \subseteq \Img f$.


That is,  if and only if  the preimage of $y$ is a singleton for all $y$ in the image set of $f$.

=== Definition 5 ===
Let $f: S \to T$ be a mapping where $S \ne \O$.

Then $f$ is an injection  if and only if :
:$\exists g: T \to S: g \circ f = I_S$
where $g$ is a mapping.


That is,  if and only if  $f$ has a left inverse.

=== Definition 6 ===
Let $f: S \to T$ be a mapping where $S \ne \O$.

Then $f$ is an injection  if and only if  $f$ is left cancellable:
:$\forall X: \forall g_1, g_2: X \to S: f \circ g_1 = f \circ g_2 \implies g_1 = g_2$
where $g_1$ and $g_2$ are arbitrary mappings from an arbitrary set $X$ to the domain $S$ of $f$.",Definition:Injection,"['Definitions/Mapping Theory', 'Definitions/Injections']"
Definition:One-to-One,One-to-One,"A relation $\RR \subseteq S \times T$ is one-to-one if it is both many-to-one and one-to-many.


That is, every element of the domain of $\RR$ relates to no more than one element of its codomain, and every element of the image is related to by exactly one element of its domain.",Definition:One-to-One Relation,['Definitions/Relation Theory']
Definition:Opposite,Opposite,"When a polygon has an even number of sides, each side has an opposite side, and each vertex likewise has an opposite vertex.

When a polygon has an odd number of sides, each side has an opposite vertex.


The opposite side (or opposite vertex) to a given side (or vertex) is that side (or vertex) which has the same number of sides between it and the side (or vertex) in question.",Definition:Polygon/Opposite,['Definitions/Polygons']
Definition:Opposite,Opposite,"The side of a triangle which is not one of the sides adjacent to a particular vertex is referred to as its opposite.

Thus, each vertex has an opposite side, and each side has an opposite vertex.",Definition:Triangle (Geometry)/Opposite,['Definitions/Triangles']
Definition:Opposite,Opposite,":

The opposite face of the face $F$ of a parallelepiped $P$ is the face of $P$ which is parallel to $F$.

In the above example, the pairs of parallel planes are:

:Face $ABCD$ is opposite $HGFE$
:Face $ADEH$ is opposite $BCFG$
:Face $ABGH$ is opposite $DCFE$",Definition:Parallelepiped/Opposite Face,['Definitions/Parallelepipeds']
Definition:Opposite,Opposite,"Let $\struct {R, +, \times}$ be a ring.


Let $* : R \times R \to R$ be the binary operation on $S$ defined by:
:$\forall x, y \in S: x * y = y \times x$

The opposite ring of $R$ is the algebraic structure $\struct {R, +, *}$.",Definition:Opposite Ring,['Definitions/Examples of Rings']
Definition:Opposite,Opposite,"Let $\mathbf C$ be a metacategory.


Its dual category, denoted $\mathbf C^{\text{op} }$, is defined as follows:

 

It can be seen that this comes down to the metacategory obtained by reversing the direction of all morphisms of $\mathbf C$.",Definition:Dual Category,"['Definitions/Category Theory', 'Definitions/Examples of Categories']"
Definition:Opposite,Opposite,"Let $\mathbf v$ and $\mathbf w$ be vectors in space.

We say that $\mathbf v$ is in the opposite direction to $\mathbf w$  if and only if :

:the lines of action of $\mathbf v$ and $\mathbf w$ are parallel
but at the same time:
:the lines of action of $\mathbf v$ and $\mathbf w$ are not the same.

Category:Definitions/Vectors",Definition:Opposite Direction,['Definitions/Vectors']
Definition:Order,Order,"Two sets (either finite or infinite) which are equivalent are said to have the same cardinality.

The cardinality of a set $S$ is written $\card S$.


=== Cardinality of Finite Set ===
Let $S$ be a finite set.

The cardinality $\card S$ of $S$ is the number of elements in $S$.

That is, if:
:$S \sim \N_{< n}$

where:
:$\sim$ denotes set equivalence
:$\N_{

Also note that from the definition of finite:
:$\exists n \in \N: \card S = n \iff S$ is finite.

=== Cardinality of Infinite Set ===
Let $S$ be an infinite set.

The cardinality $\card S$ of $S$ can be indicated as:
:$\card S = \infty$

However, it needs to be noted that this just means that the cardinality of $S$ cannot be assigned a number $n \in \N$.


It means that $\card S$ is at least $\aleph_0$ (aleph null).",Definition:Cardinality,"['Definitions/Set Theory', 'Definitions/Cardinality']"
Definition:Order,Order,"The order of an algebraic structure $\struct {S, \circ}$ is the cardinality of its underlying set, and is denoted $\order S$.


Thus, for a finite set $S$, the order of $\struct {S, \circ}$ is the number of elements in $S$.


=== Infinite Structure ===
Let $\struct {S, \circ}$ be an algebraic structure.

Let the underlying set $S$ of $\struct {S, \circ}$ be infinite.

Then $\struct {S, \circ}$ is an infinite structure.


That is, $\struct {S, \circ}$ is an infinite structure  if and only if  $\struct {S, \circ}$ is not a finite structure.

=== Finite Structure ===
Let $\struct {S, \circ}$ be an algebraic structure.

Let the underlying set $S$ of $\struct {S, \circ}$ be finite.

Then $\struct {S, \circ}$ a finite structure.


That is, $\struct {S, \circ}$ is a finite structure  if and only if  $\struct {S, \circ}$ is not an infinite structure.",Definition:Order of Structure,"['Definitions/Order of Groups', 'Definitions/Group Theory', 'Definitions/Abstract Algebra']"
Definition:Order,Order,"Let $G$ be a group whose identity is $e_G$.

Let $x \in G$ be an element of $G$.


=== Definition 1 ===
Let $G$ be a group whose identity is $e_G$.

Let $x \in G$ be an element of $G$.


The order of $x$ (in $G$), denoted $\order x$, is the smallest $k \in \Z_{> 0}$ such that $x^k = e_G$.

=== Definition 2 ===
Let $G$ be a group whose identity is $e_G$.

Let $x \in G$ be an element of $G$.


The order of $x$ (in $G$), denoted $\order x$, is the order of the group generated by $x$:
:$\order x := \order {\gen x}$

=== Definition 3 ===
Let $G$ be a group whose identity is $e_G$.

Let $x \in G$ be an element of $G$.


The order of $x$ (in $G$), denoted $\left\vert{x}\right\vert$, is the largest $k \in \Z_{\gt 0}$ such that:
:$\forall i, j \in \Z: 0 \le i < j < k \implies x^i \ne x^j$",Definition:Order of Group Element,"['Definitions/Group Theory', 'Definitions/Order of Group Elements']"
Definition:Order,Order,"Let $\sqbrk a_{m n}$ be an $m \times n$ matrix.

Then the parameters $m$ and $n$ are known as the order of the matrix.


=== Square Matrix ===
Let $\mathbf A$ be an $n \times n$ square matrix.

That is, let $\mathbf A$ have $n$ rows (and by definition $n$ columns).


Then the order of $\mathbf A$ is defined as being $n$.

=== Column Matrix ===
Let $\mathbf A$ be an $n \times 1$ column matrix.

Then the order of $\mathbf A$ is defined as being $n$.


Category:Definitions/Orders of Matrices
Category:Definitions/Column Matrices

=== Row Matrix ===
Let $\mathbf A$ be a $1 \times n$ row matrix.

Then the order of $\mathbf A$ is defined as being $n$.


Category:Definitions/Orders of Matrices
Category:Definitions/Row Matrices",Definition:Matrix/Order,"['Definitions/Orders of Matrices', 'Definitions/Matrices']"
Definition:Order,Order,"Let $\mathbf A$ be an $n \times n$ square matrix.

That is, let $\mathbf A$ have $n$ rows (and by definition $n$ columns).


Then the order of $\mathbf A$ is defined as being $n$.",Definition:Matrix/Square Matrix/Order,"['Definitions/Orders of Matrices', 'Definitions/Square Matrices']"
Definition:Order,Order,The order of a determinant is defined as the order of the square matrix on which it is defined.,Definition:Determinant/Matrix/Order,['Definitions/Determinants']
Definition:Order,Order,"Let $a$ and $n$ be integers.

Let there exist a positive integer $c$ such that:

:$a^c \equiv 1 \pmod n$


Then the least such integer is called order of $a$ modulo $n$.",Definition:Multiplicative Order of Integer,['Definitions/Number Theory']
Definition:Order,Order,"Let $m$ be a positive integer.

Let $s \left({m}\right)$ be the aliquot sum of $m$.


Let a sequence $\left\langle{a_k}\right\rangle$ be a sociable chain.

The order of $a_k$ is the smallest $r \in \Z_{>0}$ such that
:$a_r = a_0$


Category:Definitions/Sociable Numbers",Definition:Sociable Chain/Order,['Definitions/Sociable Numbers']
Definition:Order,Order,"The order of a derivative is the number of times it has been differentiated.

For example:
:a first derivative is of first order, or order $1$
:a second derivative is of second order, or order $2$

and so on.",Definition:Derivative/Higher Derivatives/Order of Derivative,"['Definitions/Order of Derivative', 'Definitions/Higher Derivatives']"
Definition:Order,Order,"Let $f: \C \to \C$ be an entire function.

Let $\ln$ denote the natural logarithm.


=== Definition 1 ===
Let $f: \C \to \C$ be an entire function.


The order $\alpha \in \closedint 0 {+\infty}$ of $f$ is the infimum of the $\beta \ge 0$ for which:
:$\map f z = \map \OO {\map \exp {\size z^\beta} }$
or $\infty$ if no such $\beta$ exists, where $\OO$ denotes big-$\OO$ notation.

=== Definition 2 ===
Let $f: \C \to \C$ be an entire function.

Let $f$ be not identically zero.


The order $\alpha \in \closedint 0 {+\infty}$ of $f$ is the infimum of the $\beta \ge 0$ for which:
:$\ds \map \ln {\max_{\size z \mathop \le R} \size {\map f z} } = \map \OO {R^\beta}$

or $\infty$ if no such $\beta$ exists, where $\OO$ denotes big-$\OO$ notation

The order of $0$ is $0$.

=== Definition 3 ===
Let $f: \C \to \C$ be an entire function.

Let $f$ be non-constant.


The order $\alpha \in \closedint 0 {+\infty}$ of $f$ is the limit superior:
:$\ds \limsup_{R \mathop \to \infty} \frac {\ds \ln \ln \max_{\cmod z \mathop \le R} \cmod f} {\ln R}$

The order of a constant function is $0$.",Definition:Order of Entire Function,"['Definitions/Order of Entire Function', 'Definitions/Entire Functions']"
Definition:Order,Order,"Let $f: \R \to \F$ be a function, where $\F \in \set {\R, \C}$.

Let $f$ be continuous on the real interval $\hointr 0 \to$, except possibly for some finite number of discontinuities of the first kind in every finite subinterval of $\hointr 0 \to$.

 


Then $f$ is said to be of exponential order, denoted $f \in \EE$,  if and only if  it is of exponential order $a$ for some $a > 0$.


=== Exponential Order $a$ ===
Let $f: \R \to \mathbb F$ be a function, where $\mathbb F \in \set {\R, \C}$.

Let $f$ be continuous on the real interval $\hointr 0 \to$, except possibly for some finite number of discontinuities of the first kind in every finite subinterval of $\hointr 0 \to$.

 

Let  $\size {\, \cdot \,}$ be the absolute value if $f$ is real-valued, or the modulus if $f$ is complex-valued.

Let $e^{a t}$ be the exponential function, where $a \in \R$ is constant.


Then $\map f t$ is said to be of exponential order $a$, denoted $f \in \EE_a$,  if and only if  there exist strictly positive real numbers $M, K$ such that:

:$\forall t \ge M: \size {\map f t} < K e^{a t}$",Definition:Exponential Order,"['Definitions/Real Analysis', 'Definitions/Complex Analysis']"
Definition:Order,Order,The order of a differential equation is defined as being the order of the highest order derivative that is present in the equation.,Definition:Differential Equation/Order,"['Definitions/Order of Differential Equation', 'Definitions/Differential Equations']"
Definition:Order,Order,"Let $G = \struct {V, E}$ be a graph.

The order of $G$ is the cardinality of its vertex set.


That is, the order of $G$ is $\card V$.",Definition:Graph (Graph Theory)/Order,['Definitions/Graphs (Graph Theory)']
Definition:Order,Order,"Let $\mathbf L$ be an $n \times n$ Latin square.

The order of $\mathbf L$ is $n$.


Category:Definitions/Latin Squares",Definition:Latin Square/Order,['Definitions/Latin Squares']
Definition:Order,Order,"Let $\closedint a b$ be a closed real interval.

Let $T := \set {a = t_0, t_1, t_2, \ldots, t_{n - 1}, t_n = b}$ form a subdivision of $\closedint a b$.

Let $S: \closedint a b \to \R$ be a spline function on $\closedint a b$ on $T$.


Some sources, instead of referring to the degree of a spline, use the order.

Let the maximum degree of the polynomials $P_k$ fitted between $t_k$ and $t_{k + 1}$ be $n$.

The order of $S$ is then $n + 1$.


Category:Definitions/Splines",Definition:Spline Function/Order,['Definitions/Splines']
Definition:Order,Order,"Let $S$ be a set.

=== Definition 1 ===
Let $S$ be a set.

Let $\RR$ be a relation $\RR$ on $S$.


$\RR$ is an ordering on $S$  if and only if  $\RR$ satisfies the ordering axioms:
 

=== Definition 2 ===
Let $S$ be a set.

Let $\RR$ be a relation $\RR$ on $S$.


$\RR$ is an ordering on $S$  if and only if  $\RR$ satisfies the ordering axioms:
 

=== Class Theory ===
 
Let $V$ be a basic universe.

Let $\RR \subseteq V \times V$ be a relation on $V$.


$\RR$ is an ordering in $V$  if and only if  $\RR$ satisfies the ordering axioms:
 ",Definition:Ordering,"['Definitions/Order Theory', 'Definitions/Orderings']"
Definition:Orientation,Orientation,"Let $G = \struct {V, E}$ be a simple graph.

Let $H = \struct {V, A}$ be a digraph.


Then $H$ is an orientation of $G$  if and only if  both of the following hold:

:$(1): \quad H$ is a simple digraph. That is, $A$ is antisymmetric.
:$(2): \quad \forall x, y \in V: \paren {\set {x, y} \in E \iff \tuple {x, y} \in A \lor \tuple {y, x} \in A}$


That is, $H$ is formed from $G$ by replacing every edge of $G$ with an arc.",Definition:Orientation (Graph Theory),['Definitions/Digraphs']
Definition:Orientation,Orientation,"The orientation of a coordinate system is the disposition of the coordinate axes relative to each other.


=== Cartesian Plane ===
There are $2$ different orientations of a Cartesian plane:


: $\qquad \qquad \qquad$ 


=== Right-Handed ===
A Cartesian plane is defined as being right-handed if it has the following property:

Let a right hand be placed, with palm uppermost, such that the thumb points along the $x$-axis in the positive direction, such that the thumb and index finger are at right-angles to each other.

Then the index finger is pointed along the $y$-axis in the positive direction.


:

=== Left-Handed ===
A Cartesian plane is defined as being left-handed if it has the following property:

Let a left hand be placed, with palm uppermost, such that the thumb points along the $x$-axis in the positive direction, such that the thumb and index finger are at right-angles to each other.

Then the index finger is pointed along the $y$-axis in the positive direction.


:

=== Cartesian $3$-Space ===
There are $2$ different orientations of a Cartesian $3$-space:


: $\qquad \qquad \qquad$ 


=== Right-Handed ===
A Cartesian $3$-Space is defined as being right-handed if it has the following property:

Let a right hand be placed such that:
:the thumb and index finger are at right-angles to each other
:the $3$rd finger is at right-angles to the thumb and index finger, upwards from the palm
:the thumb points along the $x$-axis in the positive direction
:the index finger points along the $y$-axis in the positive direction.

Then the $3$rd finger is pointed along the $z$-axis in the positive direction.


:

=== Left-Handed ===
A Cartesian $3$-Space is defined as being left-handed if it has the following property:

Let a left hand be placed such that:
:the thumb and index finger are at right-angles to each other
:the $3$rd finger is at right-angles to the thumb and index finger, upwards from the palm
:the thumb points along the $x$-axis in the positive direction
:the index finger points along the $y$-axis in the positive direction.

Then the $3$rd finger is pointed along the $z$-axis in the positive direction.


:",Definition:Orientation of Coordinate Axes,"['Definitions/Analytic Geometry', 'Definitions/Orientation (Coordinate Axes)']"
Definition:Orthogonal,Orthogonal,"=== Orthogonal Curves ===
Two curves are orthogonal if they intersect at right angles.

The term perpendicular can also be used, but the latter term is usual when the intersecting lines are straight.


=== Orthogonal Circles ===
Two circles are orthogonal if their angle of intersection is a right angle.


:

=== Orthogonal Surfaces ===
 ",Definition:Orthogonal (Analytic Geometry),['Definitions/Orthogonality (Geometry)']
Definition:Orthogonal,Orthogonal,"Two curves are orthogonal if they intersect at right angles.

The term perpendicular can also be used, but the latter term is usual when the intersecting lines are straight.


=== Orthogonal Circles ===
Two circles are orthogonal if their angle of intersection is a right angle.


:",Definition:Orthogonal Curves,"['Definitions/Orthogonal Curves', 'Definitions/Orthogonality (Geometry)', 'Definitions/Analytic Geometry']"
Definition:Orthogonal,Orthogonal,"An orthogonal coordinate system is a coordinate system in which the coordinate axes are pairwise perpendicular.


=== Orthogonal Curvilinear Coordinates ===
Let $\KK$ be a curvilinear coordinate system in $3$-space.

Let $\QQ_1$, $\QQ_2$ and $\QQ_3$ denote the one-parameter families that define the curvilinear coordinates.



Let the relation between those curvilinear coordinates and Cartesian coordinates be expressed as:

 
 
 
 
 

where:
:$\tuple {x, y, z}$ denotes the Cartesian coordinates of an arbitrary point $P$
:$\tuple {q_1, q_2, q_3}$ denotes the curvilinear coordinates of $P$.


Let these equations have the property that the metric of $\KK$ between coordinate surfaces of $\QQ_i$ and $\QQ_j$ is zero where $i \ne j$.


That is, for every point $P$ expressible as $\tuple {x, y, z}$ and $\tuple {q_1, q_2, q_3}$:

:$\dfrac {\partial x} {\partial q_i} \dfrac {\partial x} {\partial q_j} + \dfrac {\partial y} {\partial q_i} \dfrac {\partial y} {\partial q_j} + \dfrac {\partial z} {\partial q_i} \dfrac {\partial z} {\partial q_j} = 0$

wherever $i \ne j$.


Then $\KK$ is an orthogonal curvilinear coordinate system.

=== Rectangular Coordinate System ===
A rectangular coordinate system is a Cartesian coordinate system in which each of the coordinate axes are perpendicular to each other.",Definition:Orthogonal Coordinate System,"['Definitions/Orthogonal Coordinate Systems', 'Definitions/Coordinate Systems']"
Definition:Orthogonal,Orthogonal,"Let $\KK$ be a curvilinear coordinate system in $3$-space.

Let $\QQ_1$, $\QQ_2$ and $\QQ_3$ denote the one-parameter families that define the curvilinear coordinates.

=== Definition 1 ===
Let $\KK$ be a curvilinear coordinate system in $3$-space.

Let $\QQ_1$, $\QQ_2$ and $\QQ_3$ denote the one-parameter families that define the curvilinear coordinates.



Let the relation between those curvilinear coordinates and Cartesian coordinates be expressed as:

 
 
 
 
 

where:
:$\tuple {x, y, z}$ denotes the Cartesian coordinates of an arbitrary point $P$
:$\tuple {q_1, q_2, q_3}$ denotes the curvilinear coordinates of $P$.


Let these equations have the property that the metric of $\KK$ between coordinate surfaces of $\QQ_i$ and $\QQ_j$ is zero where $i \ne j$.


That is, for every point $P$ expressible as $\tuple {x, y, z}$ and $\tuple {q_1, q_2, q_3}$:

:$\dfrac {\partial x} {\partial q_i} \dfrac {\partial x} {\partial q_j} + \dfrac {\partial y} {\partial q_i} \dfrac {\partial y} {\partial q_j} + \dfrac {\partial z} {\partial q_i} \dfrac {\partial z} {\partial q_j} = 0$

wherever $i \ne j$.


Then $\KK$ is an orthogonal curvilinear coordinate system.

=== Definition 2 ===
Let $\KK$ be a curvilinear coordinate system in $3$-space.

Let $\QQ_1$, $\QQ_2$ and $\QQ_3$ denote the one-parameter families that define the curvilinear coordinates.

Let $\tuple {q_1, q_2, q_3}$ denote a set of curvilinear coordinates.

Let $\KK$ have the property that for every arbitrary pair of coordinate surfaces $q_i \in \QQ_i$ and $q_j \in \QQ_j$ where $i \ne j$:

:$q_i$ and $q_j$ are orthogonal.


Then $\KK$ is an orthogonal curvilinear coordinate system.

=== Scale Factor ===
 ",Definition:Orthogonal Curvilinear Coordinates,"['Definitions/Orthogonal Curvilinear Coordinates', 'Definitions/Orthogonal Coordinate Systems', 'Definitions/Curvilinear Coordinates']"
Definition:Orthogonal,Orthogonal,"Let $\map f {x, y, c}$ define a one-parameter family of curves $F$.

Let $\map g {x, y, c}$ also define a one-parameter family of curves $G$, with the property that:

:Every curve in $F$ is orthogonal to every curve in $G$.


Then $F$ is a family of (reciprocal) orthogonal trajectories of $G$, and contrariwise.",Definition:Orthogonal Trajectories,"['Definitions/Orthogonal Trajectories', 'Definitions/Orthogonal Curves']"
Definition:Orthogonal,Orthogonal,"Let $\struct {V, \innerprod \cdot \cdot}$ be an inner product space.

Let $u, v \in V$.


We say that $u$ and $v$ are orthogonal  if and only if :
:$\innerprod u v = 0$

We denote this: 

:$u \perp v$


=== Orthogonal Set ===
Let $\struct {V, \innerprod \cdot \cdot}$ be an inner product space.

Let $S = \set {u_1, \ldots, u_n}$ be a subset of $V$.


Then $S$ is an orthogonal set  if and only if  its elements are pairwise orthogonal:

:$\forall i \ne j: \innerprod {u_i} {u_j} = 0$

=== Orthogonality of Sets ===
Let $\struct {V, \innerprod \cdot \cdot}$ be an inner product space.

Let $A, B \subseteq V$.

We say that $A$ and $B$ are orthogonal  if and only if :

:$\forall a \in A, b \in B: a \perp b$

That is, if $a$ and $b$ are orthogonal elements of $A$ and $B$ for all $a \in A$ and $b \in B$.


We write: 

:$A \perp B$

=== Orthogonal Complement ===
Let $\mathbb K$ be a field.

Let $V$ be a vector space over $\mathbb K$.

Let $\struct {V, \innerprod \cdot \cdot}$ be an inner product space.

Let $S\subseteq V$ be a subset.


We define the orthogonal complement of $S$ (with respect to $\innerprod \cdot \cdot$), written $S^\perp$ as the set of all $v \in V$ which are orthogonal to all $s \in S$.

That is: 

:$S^\perp = \set {v \in V : \innerprod v s = 0 \text { for all } s \in S}$


If $S = \set v$ is a singleton, we may write $S^\perp$ as $v^\perp$.

=== Vectors in $\R^n$ ===
Let $\mathbf u$, $\mathbf v$ be vectors in $\R^n$.


Then $\mathbf u$ and $\mathbf v$ are said to be orthogonal  if and only if  their dot product is zero:

:$\mathbf u \cdot \mathbf v = 0$


As Dot Product is Inner Product, this is a special case of the definition of orthogonal vectors.",Definition:Orthogonal (Linear Algebra),"['Definitions/Vector Algebra', 'Definitions/Linear Algebra', 'Definitions/Inner Product Spaces', 'Definitions/Orthogonality (Linear Algebra)']"
Definition:Orthogonal,Orthogonal,"Let $\mathbb K$ be a field.

Let $V$ be a vector space over $\mathbb K$.

Let $b: V \times V \to \mathbb K$ be a reflexive bilinear form on $V$.

Let $v,w\in V$.


Then $v$ and $w$ are orthogonal (with respect to $b$)  if and only if  $\map b {v, w} = \map b {w, v} = 0$


This is denoted: $v \perp w$.


=== Orthogonal Subsets ===
Let $\mathbb K$ be a field.

Let $V$ be a vector space over $\mathbb K$.

Let $b: V \times V \to \mathbb K$ be a reflexive bilinear form on $V$.

Let $S, T \subset V$ be subsets.


Then $S$ and $T$ are orthogonal  if and only if  for all $s \in S$ and $t \in T$, $s$ and $t$ are orthogonal: $s \perp t$.

=== Orthogonal Complement ===
Let $\mathbb K$ be a field.

Let $V$ be a vector space over $\mathbb K$.

Let $b : V\times V \to \mathbb K$ be a reflexive bilinear form on $V$.

Let $S\subset V$ be a subset.


The orthogonal complement of $S$ (with respect to $b$) is the set of all $v \in V$ which are orthogonal to all $s \in S$.


This is denoted: $S^\perp$.

If $S = \set v$ is a singleton, we also write $v^\perp$.

=== Radical ===
Let $\mathbb K$ be a field.

Let $V$ be a vector space over $\mathbb K$.

Let $b : V\times V \to \mathbb K$ be a reflexive bilinear form on $V$.


The radical of $V$ is the orthogonal complement of $V$:
:$\map {\operatorname {rad} } V = V^\perp$",Definition:Orthogonal (Bilinear Form),['Definitions/Bilinear Forms (Linear Algebra)']
Definition:Orthogonal,Orthogonal,"Let $H$ be a Hilbert space.

Let $M, N$ be closed linear subspaces of $H$.


Then the orthogonal difference of $M$ and $N$, denoted $M \ominus N$, is the set $M \cap N^\perp$.

 ",Definition:Orthogonal Difference,['Definitions/Hilbert Spaces']
Definition:Orthogonal,Orthogonal,"Let $R$ be a ring with unity.

Let $\mathbf Q$ be an invertible square matrix over $R$.


=== Definition 1 ===
Let $R$ be a ring with unity.

Let $\mathbf Q$ be an invertible square matrix over $R$.


Then $\mathbf Q$ is orthogonal  if and only if :
:$\mathbf Q^{-1} = \mathbf Q^\intercal$
where:
:$\mathbf Q^{-1}$ is the inverse of $\mathbf Q$
:$\mathbf Q^\intercal$ is the transpose of $\mathbf Q$

=== Definition 2 ===
Let $R$ be a ring with unity.

Let $\mathbf Q$ be an invertible square matrix over $R$.


Then $\mathbf Q$ is orthogonal  if and only if :
:$\mathbf Q^\intercal \mathbf Q = \mathbf I$
where:
:$\mathbf Q^\intercal$ is the transpose of $\mathbf Q$
:$\mathbf I$ is the identity matrix of the same order as $\mathbf Q$.

=== Definition 3 ===
Let $R$ be a ring with unity.

Let $\mathbf Q$ be an invertible square matrix over $R$.


Then $\mathbf Q$ is orthogonal  if and only if :
:$\mathbf Q = \paren {\mathbf Q^\intercal}^{-1}$
where:
:$\mathbf Q^\intercal$ is the transpose of $\mathbf Q$
:$\paren {\mathbf Q^\intercal}^{-1}$ is the inverse of $\mathbf Q^\intercal$.",Definition:Orthogonal Matrix,"['Definitions/Orthogonal Matrices', 'Definitions/Matrix Algebra']"
Definition:Orthogonal,Orthogonal,"Let $k$ be a field.


The ($n$th) orthogonal group (on $k$), denoted $\map {\mathrm O} {n, k}$, is the following subset of the general linear group $\GL {n, k}$:

:$\map {\mathrm O} {n, k} := \set {M \in \GL {n, k}: M^\intercal = M^{-1} }$

where $M^\intercal$ denotes the transpose of $M$.


Further, $\map {\mathrm O} {n, k}$ is considered to be endowed with conventional matrix multiplication.


That is, the ($n$th) orthogonal group (on $k$) is the set of all orthogonal order-$n$ square matrices over $k$ under (conventional) matrix multiplication.


=== Orthogonal Group of Bilinear Form ===

Let $V$ be a vector space over a field $\mathbb K$.

Let $B: V \times V \to \mathbb K$ be a nondegenerate bilinear form.


Its orthogonal group $\map {\mathrm O} B$ is the group of invertible linear transformations $g \in \GL V$ such that:
:$\forall v, w \in V : \map B {g v, g w} = \map B {v, w}$


=== Orthogonal Group of Inner Product Space ===

Let $V$ be an inner product space.


Its orthogonal group $\map {\mathrm O} V$ is the group of invertible linear transformations $g \in \GL V$ such that:
:$\forall v, w \in V: \innerprod {g v} {g w} = \innerprod v w$

That is, it is the orthogonal group of its inner product.",Definition:Orthogonal Group,['Definitions/Matrix Groups']
Definition:Orthogonal Complement,Orthogonal Complement,"Let $\mathbb K$ be a field.

Let $V$ be a vector space over $\mathbb K$.

Let $\struct {V, \innerprod \cdot \cdot}$ be an inner product space.

Let $S\subseteq V$ be a subset.


We define the orthogonal complement of $S$ (with respect to $\innerprod \cdot \cdot$), written $S^\perp$ as the set of all $v \in V$ which are orthogonal to all $s \in S$.

That is: 

:$S^\perp = \set {v \in V : \innerprod v s = 0 \text { for all } s \in S}$


If $S = \set v$ is a singleton, we may write $S^\perp$ as $v^\perp$.",Definition:Orthogonal (Linear Algebra)/Orthogonal Complement,['Definitions/Orthogonality (Linear Algebra)']
Definition:Orthogonal Complement,Orthogonal Complement,"Let $\mathbb K$ be a field.

Let $V$ be a vector space over $\mathbb K$.

Let $b : V\times V \to \mathbb K$ be a reflexive bilinear form on $V$.

Let $S\subset V$ be a subset.


The orthogonal complement of $S$ (with respect to $b$) is the set of all $v \in V$ which are orthogonal to all $s \in S$.


This is denoted: $S^\perp$.

If $S = \set v$ is a singleton, we also write $v^\perp$.",Definition:Orthogonal (Bilinear Form)/Orthogonal Complement,['Definitions/Bilinear Forms (Linear Algebra)']
Definition:Pair,Pair,"The definition of a set does not take any account of the order in which the elements are listed.

That is, $\set {a, b} = \set {b, a}$, and the elements $a$ and $b$ have the same status - neither is distinguished above the other as being more ""important"".


=== Informal Definition ===
The definition of a set does not take any account of the order in which the elements are listed.

That is, $\set {a, b} = \set {b, a}$, and the elements $a$ and $b$ have the same status -- neither is distinguished above the other as being more ""important"".


An ordered pair is a two-element set together with an ordering.

In other words, one of the elements is distinguished above the other - it comes first.

Such a structure is written:
:$\tuple {a, b}$
and it means:
:first $a$, then $b$.


=== Coordinates ===
Let $\tuple {a, b}$ be an ordered pair.

The following terminology is used:
:$a$ is called the first coordinate
:$b$ is called the second coordinate.

This definition is compatible with the equivalent definition in the context of Cartesian coordinate systems.

=== Kuratowski Formalization ===
The definition of a set does not take any account of the order in which the elements are listed.

That is, $\set {a, b} = \set {b, a}$, and the elements $a$ and $b$ have the same status - neither is distinguished above the other as being more ""important"".


The concept of an ordered pair can be formalized by the definition:

:$\tuple {a, b} := \set {\set a, \set {a, b} }$

This formalization justifies the existence of ordered pairs in Zermelo-Fraenkel set theory.


=== Coordinates ===
Let $\tuple {a, b}$ be an ordered pair.

The following terminology is used:
:$a$ is called the first coordinate
:$b$ is called the second coordinate.

This definition is compatible with the equivalent definition in the context of Cartesian coordinate systems.

=== Empty Set Formalization ===
The concept of an ordered pair can be formalized by the definition:

:$\tuple {a, b} := \set {\set {\O, a}, \set {\set \O, b} }$

=== Wiener Formalization ===
The concept of an ordered pair can be formalized by the definition:

:$\tuple {a, b} := \set {\set {\O, \set a}, \set {\set b} }$",Definition:Ordered Pair,"['Definitions/Set Theory', 'Definitions/Cartesian Product', 'Definitions/Ordered Pairs', 'Definitions/Ordered Tuples']"
Definition:Pair,Pair,"A doubleton is a set that contains exactly two elements.


The doubleton containing the distinct elements $a$ and $b$ can be written $\set {a, b}$.


The set $\set {a, b}$ is known as the doubleton of $a$ and $b$.


=== Class Theory Definition ===
Let $a$ and $b$ be sets.

The class $\set {a, b}$ is a doubleton (class).

It is defined as the class of all $x$ such that $x = a$ or $x = b$:

:$\set {a, b} = \set {x: x = a \lor x = b: a \ne b}$",Definition:Doubleton,"['Definitions/Set Theory', 'Definitions/Doubletons']"
Definition:Parallel,Parallel,"=== Lines ===
 
: 
:Parallel straight lines are straight lines which, being in the same plane and being produced indefinitely in either direction, do not meet one another in either direction.
 ''
 

The contemporary interpretation of the concept of parallelism declares that a straight line is parallel to itself.

=== Planes ===
Two planes are parallel  if and only if , when produced indefinitely, do not intersect at any point.


 


The contemporary interpretation of the concept of parallelism declares that a plane is parallel to itself.

=== Line Parallel to Plane ===
Let $L$ be a straight line.

Let $P$ be a plane.

Then $L$ and $P$ are parallel  if and only if , when produced indefinitely, they do not intersect at any point.


Category:Definitions/Parallel

=== Surfaces ===
Let $S_1$ and $S_2$ be surfaces in ordinary space.

Let $S_1$ and $S_2$ have the property that:

:for every point $P$ on $S_1$, a normal vector passing through $P$ is also a normal vector to $S_2$
and:
:for every point $Q$ on $S_2$, a normal vector passing through $Q$ is also a normal vector to $S_1$.


Then $S_1$ and $S_2$ are parallel.


 ",Definition:Parallel (Geometry),"['Definitions/Euclidean Geometry', 'Definitions/Parallel']"
Definition:Parallel,Parallel," 
: 
:Parallel straight lines are straight lines which, being in the same plane and being produced indefinitely in either direction, do not meet one another in either direction.
 ''
 

The contemporary interpretation of the concept of parallelism declares that a straight line is parallel to itself.",Definition:Parallel (Geometry)/Lines,"['Definitions/Parallel Lines', 'Definitions/Parallel']"
Definition:Parallel,Parallel,"Two planes are parallel  if and only if , when produced indefinitely, do not intersect at any point.


 


The contemporary interpretation of the concept of parallelism declares that a plane is parallel to itself.",Definition:Parallel (Geometry)/Planes,"['Definitions/Parallel Planes', 'Definitions/Parallel']"
Definition:Parallel,Parallel,"Let $L$ be a straight line.

Let $P$ be a plane.

Then $L$ and $P$ are parallel  if and only if , when produced indefinitely, they do not intersect at any point.


Category:Definitions/Parallel",Definition:Parallel (Geometry)/Line to Plane,['Definitions/Parallel']
Definition:Parallel,Parallel,"Let $M = \struct {S, \mathscr I}$ be a matroid.


Two elements $x, y \in S$ are said to be parallel in $M$  if and only if  they are not loops but $\set {x, y}$ is a dependent subset of $S$.


That is, $x, y \in S$ are parallel  if and only if :
:$\set x, \set y \in \mathscr I$ and $\set {x, y} \notin \mathscr I$.",Definition:Parallel (Matroid),['Definitions/Matroid Theory']
Definition:Parameter,Parameter,"Consider the integral equation:

:of the first kind:
::$\map f x = \lambda \ds \int_{\map a x}^{\map b x} \map K {x, y} \map g y \rd x$

:of the second kind:
::$\map g x = \map f x + \lambda \ds \int_{\map a x}^{\map b x} \map K {x, y} \map g y \rd x$

:of the third kind:
::$\map u x \map g x = \map f x + \lambda \ds \int_{\map a x}^{\map b x} \map K {x, y} \map g y \rd x$


The number $\lambda$ is known as the parameter of the integral equation.",Definition:Integral Equation/Parameter,"['Definitions/Integral Equations', 'Definitions/Parameters']"
Definition:Parameter,Parameter,A population parameter is a numerical description of a population.,Definition:Population Parameter,"['Definitions/Population Parameters', 'Definitions/Descriptive Statistics']"
Definition:Parameter,Parameter,"Let $f$ be a differential equation with general solution $F$.

A parameter of $F$ is an arbitrary constant arising from the solving of a primitive during the course of obtaining the solution of $f$.",Definition:Parameter of Differential Equation,['Definitions/Differential Equations']
Definition:Parameter,Parameter,"Let $\map f {x, y, c}$ define a one-parameter family of curves $F$.

The value $c$ is the parameter of $F$.",Definition:Family of Curves/One-Parameter/Parameter,"['Definitions/One-Parameter Families of Curves', 'Definitions/One-Parameter Families']"
Definition:Parity,Parity,"Let $z \in \Z$ be an integer.

The parity of $z$ is whether it is even or odd.


That is:
:an integer of the form $z = 2 n$, where $n$ is an integer, is of even parity;
:an integer of the form $z = 2 n + 1$, where $n$ is an integer, is of odd parity.


:If $z_1$ and $z_2$ are either both even or both odd, $z_1$ and $z_2$ have the same parity. 
:If $z_1$ is even and $z_2$ is odd, then $z_1$ and $z_2$ have opposite parity.",Definition:Parity of Integer,['Definitions/Integers']
Definition:Parity,Parity,"Let $n \in \N$ be a natural number.

Let $S_n$ denote the symmetric group on $n$ letters.

Let $\rho \in S_n$, that is, let $\rho$ be a permutation of $S_n$.

The parity of $\rho$ is defined as follows:


=== Even Permutation ===
Let $n \in \N$ be a natural number.

Let $S_n$ denote the symmetric group on $n$ letters.

Let $\rho \in S_n$ be a permutation in $S_n$.


$\rho$ is an even permutation  if and only if :
:$\map \sgn \rho = 1$
where $\sgn$ denotes the sign function.

=== Odd Permutation ===
Let $n \in \N$ be a natural number.

Let $S_n$ denote the symmetric group on $n$ letters.

Let $\rho \in S_n$ be a permutation in $S_n$.


$\rho$ is an odd permutation  if and only if :

:$\map \sgn \rho = -1$

where $\sgn $ denotes the sign function.

where $\map \sgn \rho$ denotes the sign of $\rho$.",Definition:Parity of Permutation,['Definitions/Permutation Theory']
Definition:Path,Path,"Let $T = \struct {S, \tau}$ be a topological space.

Let $I \subset \R$ be the closed real interval $\closedint a b$.


A path in $T$ is a continuous mapping $\gamma: I \to S$.


The mapping $\gamma$ can be referred as:
:a path (in $T$) joining $\map \gamma a$ and $\map \gamma b$
or:
:a path (in $T$) from $\map \gamma a$ to $\map \gamma b$.


It is common to refer to a point $z = \map \gamma t$ as a point on the path $\gamma$, even though $z$ is in fact on the image of $\gamma$.


=== Initial Point ===
Let $T$ be a topological space.

Let $I \subset \R$ be the closed real interval $\closedint a b$.

Let $\gamma: I \to T$ be a path in $T$.


The initial point of $\gamma$ is $\map \gamma a$.

That is, the path starts (or begins) at $\map \gamma a$.

=== Final Point ===
Let $T$ be a topological space.

Let $I \subset \R$ be the closed real interval $\closedint a b$.

Let $\gamma: I \to T$ be a path in $T$.


The final point of $\gamma$ is $\map \gamma b$.

That is, the path ends (or finishes) at $\map \gamma b$.

=== Endpoint ===
Let $T$ be a topological space.

Let $I \subset \R$ be the closed real interval $\closedint a b$.

Let $\gamma: I \to T$ be a path in $T$.


The initial point and final point of $\gamma$ can be referred to as the endpoints of $\gamma$


Category:Definitions/Path-Connected Spaces

=== Composable Paths ===
Let $T$ be a topological space.

Let $f, g: \closedint 0 1 \to T$ be paths.


$f$ and $g$ are said to be composable paths if:

:$\map f 1 = \map g 0$.",Definition:Path (Topology),"['Definitions/Paths (Topology)', 'Definitions/Path-Connected Spaces', 'Definitions/Complex Analysis', 'Definitions/Topology']"
Definition:Path,Path,"Let $G$ be an undirected graph.

A path in $G$ is a trail in $G$ in which all vertices (except perhaps the first and last ones) are distinct.


A path between two vertices $u$ and $v$ is called a $u$-$v$ path.


=== Subgraph ===
The set of vertices and edges which go to make up a path in a graph $G$ form a subgraph of $G$.

This subgraph itself is also referred to as a path in $G$.

=== Open Path ===
An open path is a path in which the first and last vertices are distinct.



=== Endpoint of Open Path ===
Let $P$ be an open path in a graph $G$.

The endpoints of $P$ are its first and last vertices.


Category:Definitions/Paths (Graph Theory)",Definition:Path (Graph Theory),"['Definitions/Paths (Graph Theory)', 'Definitions/Graph Theory']"
Definition:Path,Path,"Let $D = \struct {V, E}$ be a digraph.

A path $P$ in $D$ is:
:a sequence of vertices $v_1, v_2, \ldots, v_n$ in $V$ and a sequence of arcs $e_1, e_2, \ldots{}, e_{n - 1}$ in $E$ such that:
:$P$ begins with $v_1$ and ends with $v_n$
:in which each arc $e_j$ is incident from $v_j$ and incident to $v_{j + 1}$
:all arcs are distinct
:all vertices (except perhaps the first and last ones) are distinct.

A path between two vertices $u$ and $v$ is called a path from $u$ to $v$.


=== Predecessor ===
Let $D = \struct {V, E}$ be a digraph.

Let $P$ be a path in $D$ such that the vertices of $P$ are $v_1, v_2, \ldots, v_n$.

Let $v_j$ be a vertex of $P$ such that $j > 1$.

Then the predecessor (vertex) of $v_j$ is the vertex $v_{j - 1}$.


That is, if $v \to w$ is an arc in $P$, $v$ is the predecessor (vertex) of $w$.

=== Successor ===
Let $D = \struct {V, E}$ be a digraph.

Let $P$ be a path in $D$ such that the vertices of $P$ are $v_1, v_2, \ldots, v_n$.

Let $v_j$ be a vertex of $P$ such that $j < n$.

Then the successor (vertex) of $v_j$ is the vertex $v_{j + 1}$.


That is, if $v \to w$ is an arc in $P$, $w$ is the successor (vertex) of $v$.",Definition:Path (Graph Theory)/Digraph,"['Definitions/Paths in Digraphs', 'Definitions/Paths (Graph Theory)', 'Definitions/Digraphs']"
Definition:Path,Path,"A cycle is a circuit in which no vertex except the first (which is also the last) appears more than once.


An $n$-cycle is a cycle with $n$ vertices.


=== Subgraph ===
The set of vertices and edges which go to make up a cycle form a subgraph.

This subgraph itself is also referred to as a cycle.

=== Odd Cycle ===
An odd cycle is a cycle with odd length, that is, with an odd number of edges.

=== Even Cycle ===
An even cycle is a cycle with even length, that is, with an even number of edges.",Definition:Cycle (Graph Theory),"['Definitions/Cycles (Graph Theory)', 'Definitions/Circuits (Graph Theory)', 'Definitions/Paths (Graph Theory)', 'Definitions/Graph Theory']"
Definition:Perfect,Perfect,"=== Definition 1 ===
A perfect number is a (strictly) positive integer equal to its aliquot sum.

=== Definition 2 ===
A perfect number $n$ is a (strictly) positive integer such that:
:$\map {\sigma_1} n= 2 n$
where $\sigma_1: \Z_{>0} \to \Z_{>0}$ is the divisor sum function.

=== Definition 3 ===
Let $n \in \Z_{\ge 0}$ be a positive integer.


Let $A \left({n}\right)$ denote the abundance of $n$.

$n$ is perfect  if and only if  $A \left({n}\right) = 0$.

=== Definition 4 ===
A perfect number $n$ is a (strictly) positive integer such that:
:$\dfrac {\map {\sigma_1} n} n = 2$
where $\sigma_1: \Z_{>0} \to \Z_{>0}$ is the divisor sum function.",Definition:Perfect Number,"['Definitions/Number Theory', 'Definitions/Abundance', 'Definitions/Abundancy', 'Definitions/Perfect Numbers']"
Definition:Perfect,Perfect,"=== Definition 1 ===
A perfect set of a topological space $T = \left({S, \tau}\right)$ is a subset $H \subseteq S$ such that:
:$H = H'$
where $H'$ is the derived set of $H$.

That is, where:
:every point of $H$ is a limit point of $H$
and
:every limit point of $H$ is a point of $H$.

=== Definition 2 ===
A perfect set of a topological space $T = \left({S, \tau}\right)$ is a subset $H \subseteq S$ such that:
: $H$ is a closed set of $T$
: $H$ has no isolated points.

=== Definition 3 ===
A perfect set of a topological space $T = \struct {S, \tau}$ is a subset $H \subseteq S$ such that:
:$H$ is dense-in-itself.
:$H$ contains all its limit points.",Definition:Perfect Set,"['Definitions/Topology', 'Definitions/Perfect Sets']"
Definition:Perfect,Perfect,A graph is perfect if no two vertices have the same degree.,Definition:Perfect Graph,['Definitions/Graph Theory']
Definition:Perfect,Perfect,"Let $F$ be a field.


=== Definition 1 ===
Let $F$ be a field.


$F$ is a perfect field   if and only if  $F$ has no inseparable extensions.

=== Definition 2 ===
Let $F$ be a field.


$F$ is a perfect field  if and only if  one of the following holds:
:$\Char F = 0$
:$\Char F = p$ with $p$ prime and $\Frob$ is an automorphism of $F$

where:
:$\Char F$ denotes the characteristic of $F$
:$\Frob$ denotes the Frobenius endomorphism on $F$",Definition:Perfect Field,"['Definitions/Field Theory', 'Definitions/Perfect Fields']"
Definition:Period,Period,"Let $f: \R \to \R$ be a periodic real function.


The period of $f$ is the smallest value $L \in \R_{>0}$ such that:
:$\forall x \in \R: \map f x = \map f {x + L}$",Definition:Periodic Real Function/Period,['Definitions/Periodic Functions']
Definition:Period,Period,"Let $f: \R \to \R$ be a real function.


Then $f$ is periodic  if and only if :
:$\exists L \in \R_{\ne 0}: \forall x \in \R: \map f x = \map f {x + L}$


=== Period ===
Let $f: \R \to \R$ be a periodic real function.


The period of $f$ is the smallest value $L \in \R_{>0}$ such that:
:$\forall x \in \R: \map f x = \map f {x + L}$

=== Frequency ===
Let $f: \R \to \R$ be a periodic real function.

The frequency $\nu$ of $f$ is the reciprocal of the period $L$ of $f$:
:$\nu = \dfrac 1 L$

where:
:$\forall x \in X: \map f x = \map f {x + L}$

=== Amplitude ===
Let $f: \R \to \R$ be a periodic real function.


The amplitude of $f$ is the maximum absolute difference of the value of $f$ from a reference level.",Definition:Periodic Function/Real,"['Definitions/Real Analysis', 'Definitions/Periodic Functions']"
Definition:Permutable,Permutable,"A permutable prime is a prime number $p$ which has the property that all anagrams of $p$ are prime.


=== Sequence ===
The sequence of permutable primes begins:
:$2, 3, 5, 7, 11, 13, 17, 31, 37, 71, 73, 79, 97, 113, 131, 199, 311, 337, 373, 733, 919, 991, R_{19}, R_{23}, R_{317}, R_{1091}, \ldots$
where $R_n$ denotes the repunit of $n$ digits.

 

The smallest 
elements of the permutation sets of these are:
:$2, 3, 5, 7, 11, 13, 17, 37, 79, 113, 199, 337, R_{19}, R_{23}, R_{317}, R_{1091}, \ldots$

 

 ",Definition:Permutable Prime,"['Definitions/Permutable Primes', 'Definitions/Number Theory', 'Definitions/Recreational Mathematics', 'Definitions/Prime Numbers']"
Definition:Permutable,Permutable,"Let $\struct {G, \circ}$ be a group.

Let $H$ and $K$ be subgroups of $G$.

Let $H \circ K$ denote the subset product of $H$ and $K$.


Then $H$ and $K$ are permutable  if and only if :
:$H \circ K = K \circ H$",Definition:Permutable Subgroups,['Definitions/Group Theory']
Definition:Permutable,Permutable,"Let $\circ$ be a binary operation.


Two elements $x, y$ are said to commute (with each other)  if and only if :
:$x \circ y = y \circ x$


Thus $x$ and $y$ can be described as commutative (elements) under $\circ$.",Definition:Commutative/Elements,['Definitions/Commutativity']
Definition:Polar,Polar,"Polar coordinates are a technique for unique identification of points on the plane.

A distinct point $O$ is identified.


=== Pole ===
Consider a system of polar coordinates used to identify points on the plane.

Let $O$ be the distinct point identified as the center of the frame.


The point $O$ is referred to as the pole of the polar coordinate plane.

=== Polar Axis ===
Let $O$ be the pole of the polar coordinate plane.


A ray is drawn from $O$, usually to the right, and referred to as the polar axis.",Definition:Polar Coordinates,"['Definitions/Polar Coordinates', 'Definitions/Examples of Coordinate Systems', 'Definitions/Curvilinear Coordinates']"
Definition:Polar,Polar,"Let $O$ be the pole of the polar coordinate plane.


A ray is drawn from $O$, usually to the right, and referred to as the polar axis.",Definition:Polar Coordinates/Polar Axis,"['Definitions/Polar Coordinates', 'Definitions/Polar Axes']"
Definition:Polar,Polar,"A polar equation is an equation defining the locus of a set of points in the polar coordinate plane.

Such an equation is generally presented in terms of the variables:
:$r$: the radial coordinate
:$\theta$: the angular coordinate",Definition:Polar Equation,"['Definitions/Polar Equations', 'Definitions/Polar Coordinates']"
Definition:Polar,Polar,"Let $\KK$ be a conic section embedded in a Euclidean plane.

Let $P$ be an arbitrary point in that plane.

Let a secant line pass through $P$ and intersect $\KK$ at $L$ and $M$.


The polar of $P$ with respect to $\KK$ is the straight line upon which the tangents to $\KK$ intersect.


=== Circle ===
Let $\CC$ be a circle whose radius is $r$ and whose center is at the origin of a Cartesian plane.

Let $P = \tuple {x_0, y_0}$ be an arbitrary point in the Cartesian plane.


The polar of $P$ with respect to $\CC$ is the straight line whose equation is given by:

:$x x_0 + y y_0 = r^2$


=== Pole ===
Let $\CC$ be a circle embedded in the plane.

Let $P$ be an arbitrary point in the plane.


Let $\LL$ be the polar of $P$ with respect to $\CC$.

Then $P$ is known as the pole of $\LL$.

=== Ellipse ===
Let $\EE$ be an ellipse embedded in a Cartesian plane in reduced form with the equation:
:$\dfrac {x^2} {a^2} + \dfrac {y^2} {b^2} = 1$


Let $P = \tuple {x_0, y_0}$ be an arbitrary point in the Cartesian plane.


The polar of $P$ with respect to $\EE$ is the straight line whose equation is given by:

:$\dfrac {x x_0} {a^2} + \dfrac {y y_0} {b^2} = 1$


=== Pole ===
Let $\EE$ be an ellipse embedded in the plane.

Let $P$ be an arbitrary point in the plane.


Let $\LL$ be the polar of $P$ with respect to $\EE$.

Then $P$ is known as the pole of $\LL$.",Definition:Polar of Point,"['Definitions/Polars of Points', 'Definitions/Conic Sections', 'Definitions/Tangents', 'Definitions/Projective Geometry', 'Definitions/Analytic Geometry']"
Definition:Polar,Polar,The polar axis of a spherical coordinate system is the vertical straight line which passes through the origin $O$.,Definition:Spherical Coordinate System/Polar Axis,['Definitions/Spherical Coordinates']
Definition:Polar,Polar,"Let $\triangle ABC$ be a spherical triangle on the surface of a sphere whose center is $O$.

Let the sides $a, b, c$ of $\triangle ABC$ be measured by the angles subtended at $O$, where $a, b, c$ are opposite $A, B, C$ respectively.


Let $A'$, $B'$ and $C'$ be the poles of the sides $BC$, $AC$ and $AB$ respectively which are in the same hemisphere as the points $A$, $B$ and $C$ respectively.

:

Then the spherical triangle $\triangle A'B'C'$ is the polar triangle of $\triangle ABC$.",Definition:Polar Triangle,['Definitions/Spherical Triangles']
Definition:Polar,Polar,A polar vector is a vector quantity whose action is along a line drawn in the direction of the vector itself.,Definition:Polar Vector,"['Definitions/Vectors', 'Definitions/Mechanics']"
Definition:Positive Definite,Positive Definite,"Let $\struct {R, +, \times}$ be a ring whose zero is denoted $0_R$.

Let $f: R \to \R$ be a (real-valued) function on $R$.


Then $f$ is positive definite  if and only if :

:$\forall x \in R: \begin {cases} \map f x = 0 & : x = 0_R \\ \map f x > 0 & : x \ne 0_R \end {cases}$

Category:Definitions/Ring Theory",Definition:Positive Definite (Ring),['Definitions/Ring Theory']
Definition:Positive Definite,Positive Definite,"Let $\mathbf A$ be a symmetric square matrix of order $n$.

=== Definition 1 ===
Let $\mathbf A$ be a symmetric square matrix of order $n$.

$\mathbf A$ is positive definite  if and only if :
:for all nonzero column matrices $\mathbf x$ of order $n$, $\mathbf x^\intercal \mathbf A \mathbf x$ is strictly positive.

=== Definition 2 ===
Let $\mathbf A$ be a symmetric square matrix of order $n$.

$\mathbf A$ is positive definite  if and only if :
:all the eigenvalues of $\mathbf A$ are strictly positive.",Definition:Positive Definite Matrix,"['Definitions/Positive Definite Matrices', 'Definitions/Matrix Algebra', 'Definitions/Matrix Theory']"
Definition:Power,Power,"=== Natural Numbers ===
Let $\N$ denote the natural numbers.


For each $m \in \N$, recursively define $e_m: \N \to \N$ to be the mapping:
:$e_m \left({n}\right) = \begin{cases}
1 & : n = 0 \\
m \times e_m \left({x}\right) & : n = x + 1
\end{cases}$
where:
: $+$ denotes natural number addition.
: $\times$ denotes natural number multiplication.


$e_m \left({n}\right)$ is then expressed as a binary operation in the form:
:$m^n := e_m \left({n}\right)$

and is called $m$ to the power of $n$.

=== Integers ===
Let $x \in \R$ be a real number.

Let $n \in \Z$ be an integer.

The expression $x^n$ is called $x$ to the power of $n$.

$x^n$ is defined recursively as:


:$x^n = \begin {cases} 1 & : n = 0 \\ & \\ x \times x^{n - 1} & : n > 0 \\ & \\ \dfrac {x^{n + 1} } x & : n < 0 \end {cases}$

where $\dfrac {x^{n + 1} } x$ denotes division.


=== Even Power ===
Let $x \in \R$ be a real number.

Let $n \in \Z$ be an even integer.


Then $x^n$ is called an even power of $x$.


Category:Definitions/Integer Powers
Category:Definitions/Even Integers

=== Odd Power ===
Let $x \in \R$ be a real number.

Let $n \in \Z$ be an odd integer.


Then $x^n$ is called an odd power of $x$


Category:Definitions/Integer Powers
Category:Definitions/Odd Integers

=== Rational Numbers ===
Let $x \in \R$ be a real number such that $x > 0$.

Let $m \in \Z$ be an integer.

Let $y = \sqrt [m] x$ be the $m$th root of $x$.


Then we can write $y = x^{1/m}$ which means the same thing as $y = \sqrt [m] x$.


Thus we can define the power to a positive rational number:

Let $r = \dfrac p q \in \Q$ be a positive rational number where $p \in \Z_{\ge 0}, q \in \Z_{> 0}$.

Then $x^r$ is defined as:
:$x^r = x^{p/q} = \paren {\sqrt [q] x}^p = \sqrt [q] {\paren {x^p} }$


When $r = \dfrac {-p} q \in \Q: r < 0$ we define:
:$x^r = x^{-p/q} = \dfrac 1 {x^{p/q} }$
analogously for the negative integer definition.

=== Real Numbers ===
Let $x \in \R_{>0}$ be a (strictly) positive real number.

Let $r \in \R$ be a real number.


We define $x^r$ as:

:$x^r := \map \exp {r \ln x}$
where $\exp$ denotes the exponential function.


This definition is an extension of the definition for rational $r$.

This follows from Logarithms of Powers and Exponential of Natural Logarithm: it can be seen that:
:$\forall r \in \Q: \map \exp {r \ln x} = \map \exp {\map \ln {x^r} } = x^r$

 

=== Complex Numbers ===
Let $z, k \in \C$ be complex numbers.


$z$ to the power of $k$ is defined as the multifunction:

:$z^k := e^{k \ln \paren z}$

where:
:$e^z$ is the exponential function
:$\ln$ is the  natural logarithm multifunction.


=== Principal Branch ===
The principal branch of a complex number raised to a complex power is defined as:

:$z^k = e^{k \Ln z}$

where $\Ln z$ is the principal branch of the natural logarithm.


=== Positive Real Base ===
Let $t > 0$ be a real number and let $k$ be a complex number.


The principal branch of a positive real number raised to a complex power is defined as:

:$t^k = e^{k \ln t}$

where $\ln$ is the natural logarithm of a positive real number.


Category:Definitions/Complex Powers

Category:Definitions/Complex Powers

=== Multiindices ===
Let $k = \left \langle {k_j}\right \rangle_{j = 1, \ldots, n}$ be a multiindex indexed by $\set {1, \ldots, n}$.

Let $x = \tuple {x_1, \ldots, x_n} \in \R^n$ be an ordered tuple of real numbers.


Then $x^k$ is defined as:

:$\ds x^k := \prod_{j \mathop = 1}^n x_j^{k_j}$
where the powers on the   are integer powers.


Category:Definitions/Analysis

=== Power of Zero ===
Let $r \in \R$ be a real number.

(This includes the situation where $r \in \Z$ or $r \in \Q$.)

When $x=0$, $x^r$ is defined as follows:

:$0^r = \begin{cases}
1 & : r = 0 \\
0 & : r > 0 \\
\text{Undefined} & : r < 0 \\
\end{cases}$

This takes account of the awkward case $0^0$: it is ""generally accepted"" that $0^0 = 1$ as this convention agrees with certain general results which would otherwise need a special case.",Definition:Power (Algebra),"['Definitions/Powers', 'Definitions/Algebra', 'Definitions/Numbers', 'Definitions/Real Analysis', 'Definitions/Complex Analysis', 'Definitions/Involution']"
Definition:Power,Power,"=== Magma ===
Let $\struct {S, \circ}$ be a magma which has no identity element.

Let $a \in S$.


Let the mapping $\circ^n a: \N_{>0} \to S$ be recursively defined as:

:$\forall n \in \N_{>0}: \circ^n a = \begin{cases}
a & : n = 1 \\
\paren {\circ^r a} \circ a & : n = r + 1
\end{cases}$


The mapping $\circ^n a$ is known as the $n$th power of $a$ (under $\circ$).


=== Notation ===


=== Magma with Identity ===
Let $\struct {S, \circ}$ be a magma with an identity element $e$.

Let $a \in S$.


Let the mapping $\circ^n a: \N \to S$ be recursively defined as:

:$\forall n \in S: \circ^n a = \begin{cases}
e & : n = 0 \\
\paren {\circ^r a} \circ a & : n = r + 1
\end{cases}$


The mapping $\circ^n a$ is known as the $n$th power of $a$ (under $\circ$).


=== Notation ===


Furthermore:
:$a^0 = \circ^0 a = e$

=== Semigroup ===
Let $\struct {S, \circ}$ be a semigroup which has no identity element.

Let $a \in S$.


For $n \in \N_{>0}$, the $n$th power of $a$ (under $\circ$) is defined as:

:$\circ^n a = \begin{cases} a & : n = 1 \\ \paren {\circ^m a} \circ a & : n = m + 1 \end{cases}$

That is:
:$a^n = \underbrace {a \circ a \circ \cdots \circ a}_{n \text{ copies of } a}$

which from the General Associativity Theorem is unambiguous.


=== Notation ===


=== Monoid ===
Let $\struct {S, \circ}$ be a monoid whose identity element is $e$.

Let $a \in S$.

Let $n \in \N$.


The definition $a^n = \map {\circ^n} a$ as the $n$th power of $a$ in a semigroup can be extended to allow an exponent of $0$:

:$a^n = \begin {cases}
e & : n = 0 \\
a^{n - 1} \circ a & : n > 0
\end{cases}$

or:

:$n \cdot a = \begin {cases}
e & : n = 0 \\
\paren {\paren {n - 1} \cdot a} \circ a & : n > 0
\end{cases}$


The validity of this definition follows from the fact that a monoid has an identity element.


=== Invertible Element ===
Let $\struct {S, \circ}$ be a monoid whose identity element is $e$.

Let $b \in S$ be invertible for $\circ$.

Let $n \in \Z$.


The definition $b^n = \map {\circ^n} b$ as the $n$th power of $b$ in $\left({S, \circ}\right)$ can be extended to include the inverse of $b$:

:$b^{-n} = \paren {b^{-1} }^n$


Category:Definitions/Monoids

=== Group ===
Let $\struct {G, \circ}$ be a group whose identity element is $e$.

Let $g \in G$.

Let $n \in \Z$.


The definition $g^n = \map {\circ^n} g$ as the $n$th power of $g$ in a monoid can be extended to allow negative values of $n$:

:$g^n = \begin{cases}
e & : n = 0 \\
g^{n - 1} \circ g & : n > 0 \\
\paren {g^{-n} }^{-1} & : n < 0
\end{cases}$

or

:$n \cdot g = \begin{cases}
e & : n = 0 \\
\paren {\paren {n - 1} \cdot g} \circ g & : n > 0 \\
-\paren {-n \cdot g} & : n < 0
\end{cases}$

The validity of this definition follows from the group axioms: $g$ has an inverse element.

=== Ring ===
Let $\struct {R, +, \circ}$ be a ring.

Let $r \in R$.

Let $n \in \Z_{>0}$ be the set of strictly positive integers.

The $n$th power of $r$ in $R$ is defined as the $n$th power of $r$ with respect to the semigroup $\struct {R, \circ}$:

:$\forall n \in \Z_{>0}: r^n = \begin {cases}
r & : n = 1 \\
r^{n - 1} \circ r & : n > 1 
\end {cases}$


If $R$ is a ring with unity where $1_R$ is that unity, the definition extends to $n \in \Z_{\ge 0}$:

:$\forall n \in \Z_{\ge 0}: r^n = \begin {cases}
1_R & : n = 0 \\
r^{n - 1} \circ r & : n > 0
\end {cases}$

=== Field ===
Let $\struct {F, +, \circ}$ be a field with zero $0_F$ and unity $1_F$.


Let $a \in F^*$ where $F^*$ denotes the set of elements of $F$ without the zero $0_F$.

Let $n \in \Z$ be an integer.


The $n$th power of $a$ in $F$ is defined as the $n$th power of $a$ with respect to the Abelian group $\struct {F^*, \circ}$:
:$\forall n \in \Z: a^n = \begin {cases}
1_F & : n = 0 \\
a^{n - 1} \circ a & : n > 0 \\
\paren{a^{-1}}^{-n} & : n < 0
\end {cases}$


The definition of $n$th power of $a$ in $F$ as the the $n$th power of $a$ with respect to the monoid $\struct {F, \circ}$ can be extended to $0_F$ for positive values of $n$.


For all $n \in \Z_{\ge 0}$ the $n$th power of $0_F$ in $F$ is defined:
:$\paren{0_F}^n = \begin {cases}
1_F & : n = 0 \\
0_F & : n > 0
\end {cases}$


It should be noted that for all $n < 0$ the $n$th power of $0_F$ is not defined.",Definition:Power of Element,"['Definitions/Abstract Algebra', 'Definitions/Powers (Abstract Algebra)']"
Definition:Power,Power,"Let $\struct {S, \circ}$ be a magma which has no identity element.

Let $a \in S$.


Let the mapping $\circ^n a: \N_{>0} \to S$ be recursively defined as:

:$\forall n \in \N_{>0}: \circ^n a = \begin{cases}
a & : n = 1 \\
\paren {\circ^r a} \circ a & : n = r + 1
\end{cases}$


The mapping $\circ^n a$ is known as the $n$th power of $a$ (under $\circ$).


=== Notation ===
",Definition:Power of Element/Magma,"['Definitions/Magmas', 'Definitions/Powers (Abstract Algebra)']"
Definition:Power,Power,"Let $\struct {S, \circ}$ be a magma with an identity element $e$.

Let $a \in S$.


Let the mapping $\circ^n a: \N \to S$ be recursively defined as:

:$\forall n \in S: \circ^n a = \begin{cases}
e & : n = 0 \\
\paren {\circ^r a} \circ a & : n = r + 1
\end{cases}$


The mapping $\circ^n a$ is known as the $n$th power of $a$ (under $\circ$).


=== Notation ===


Furthermore:
:$a^0 = \circ^0 a = e$",Definition:Power of Element/Magma with Identity,"['Definitions/Magmas', 'Definitions/Powers (Abstract Algebra)']"
Definition:Power,Power,"Let $\struct {S, \circ}$ be a semigroup which has no identity element.

Let $a \in S$.


For $n \in \N_{>0}$, the $n$th power of $a$ (under $\circ$) is defined as:

:$\circ^n a = \begin{cases} a & : n = 1 \\ \paren {\circ^m a} \circ a & : n = m + 1 \end{cases}$

That is:
:$a^n = \underbrace {a \circ a \circ \cdots \circ a}_{n \text{ copies of } a}$

which from the General Associativity Theorem is unambiguous.


=== Notation ===
",Definition:Power of Element/Semigroup,"['Definitions/Semigroups', 'Definitions/Powers (Abstract Algebra)']"
Definition:Power,Power,"Let $\struct {S, \circ}$ be a monoid whose identity element is $e$.

Let $a \in S$.

Let $n \in \N$.


The definition $a^n = \map {\circ^n} a$ as the $n$th power of $a$ in a semigroup can be extended to allow an exponent of $0$:

:$a^n = \begin {cases}
e & : n = 0 \\
a^{n - 1} \circ a & : n > 0
\end{cases}$

or:

:$n \cdot a = \begin {cases}
e & : n = 0 \\
\paren {\paren {n - 1} \cdot a} \circ a & : n > 0
\end{cases}$


The validity of this definition follows from the fact that a monoid has an identity element.


=== Invertible Element ===
Let $\struct {S, \circ}$ be a monoid whose identity element is $e$.

Let $b \in S$ be invertible for $\circ$.

Let $n \in \Z$.


The definition $b^n = \map {\circ^n} b$ as the $n$th power of $b$ in $\left({S, \circ}\right)$ can be extended to include the inverse of $b$:

:$b^{-n} = \paren {b^{-1} }^n$


Category:Definitions/Monoids",Definition:Power of Element/Monoid,['Definitions/Monoids']
Definition:Power,Power,"Let $\struct {G, \circ}$ be a group whose identity element is $e$.

Let $g \in G$.

Let $n \in \Z$.


The definition $g^n = \map {\circ^n} g$ as the $n$th power of $g$ in a monoid can be extended to allow negative values of $n$:

:$g^n = \begin{cases}
e & : n = 0 \\
g^{n - 1} \circ g & : n > 0 \\
\paren {g^{-n} }^{-1} & : n < 0
\end{cases}$

or

:$n \cdot g = \begin{cases}
e & : n = 0 \\
\paren {\paren {n - 1} \cdot g} \circ g & : n > 0 \\
-\paren {-n \cdot g} & : n < 0
\end{cases}$

The validity of this definition follows from the group axioms: $g$ has an inverse element.",Definition:Power of Element/Group,['Definitions/Group Theory']
Definition:Power,Power,"Let $\struct {R, +, \circ}$ be a ring.

Let $r \in R$.

Let $n \in \Z_{>0}$ be the set of strictly positive integers.

The $n$th power of $r$ in $R$ is defined as the $n$th power of $r$ with respect to the semigroup $\struct {R, \circ}$:

:$\forall n \in \Z_{>0}: r^n = \begin {cases}
r & : n = 1 \\
r^{n - 1} \circ r & : n > 1 
\end {cases}$


If $R$ is a ring with unity where $1_R$ is that unity, the definition extends to $n \in \Z_{\ge 0}$:

:$\forall n \in \Z_{\ge 0}: r^n = \begin {cases}
1_R & : n = 0 \\
r^{n - 1} \circ r & : n > 0
\end {cases}$",Definition:Power of Element/Ring,['Definitions/Ring Theory']
Definition:Power,Power,"Let $\struct {X, \circ}$ be a $B$-algebra.

For any $x \in X$ and $n \in \N$, define the $n$th power of $x$, denoted $x^n$, inductively:

:$x^n = \begin{cases}
0 & \text {if $n = 0$} \\
x^{n - 1} \circ \paren {0 \circ x} & \text {if $n \ge 1$}
\end{cases}$",Definition:Power (B-Algebra),['Definitions/B-Algebras']
Definition:Power,Power,"Two sets (either finite or infinite) which are equivalent are said to have the same cardinality.

The cardinality of a set $S$ is written $\card S$.


=== Cardinality of Finite Set ===
Let $S$ be a finite set.

The cardinality $\card S$ of $S$ is the number of elements in $S$.

That is, if:
:$S \sim \N_{< n}$

where:
:$\sim$ denotes set equivalence
:$\N_{

Also note that from the definition of finite:
:$\exists n \in \N: \card S = n \iff S$ is finite.

=== Cardinality of Infinite Set ===
Let $S$ be an infinite set.

The cardinality $\card S$ of $S$ can be indicated as:
:$\card S = \infty$

However, it needs to be noted that this just means that the cardinality of $S$ cannot be assigned a number $n \in \N$.


It means that $\card S$ is at least $\aleph_0$ (aleph null).",Definition:Cardinality,"['Definitions/Set Theory', 'Definitions/Cardinality']"
Definition:Power,Power,"The power set of a set $S$ is the set defined and denoted as:

:$\powerset S := \set {T: T \subseteq S}$

That is, the set whose elements are all of the subsets of $S$.


=== Class Theory ===
The power set of a set $x$ is the class of all the subsets of $x$:

:$\powerset x := \set {y: y \subseteq x}$


It is clear from the definition that:
:$y \in \powerset x \iff y \subseteq x$


=== Axiom of Powers ===

The concept of the power set is axiomatised in the Axiom of Powers in class theory:

 ",Definition:Power Set,"['Definitions/Set Theory', 'Definitions/Power Set']"
Definition:Power,Power,"Power the amount of energy transferred or converted per unit time.

That is, the rate at which energy is transformed.


Power is a scalar quantity.


=== Instantaneous ===
The instantaneous power transformed at an instant of time is defined as:
:$\map p t = \dfrac {\d \map E t} {\d t}$
where:
:$\map p t$ is the power as a function of time
:$\map E t$ is the energy as a function of time.

=== Symbol ===


=== Dimension ===
The dimension of measurement of power is $\mathsf {M L}^2 \mathsf T^{-3}$.

This derives from its definition as:
:$\dfrac {\text{Energy} } {\text {Time} }$


Category:Definitions/Dimensions of Measurement
Category:Definitions/Power (Physics)

=== Unit ===
The SI unit of power  is the watt $\mathrm W$.


=== Watt ===
The watt is the SI unit of power.


It is defined as being the rate at which $1$ joule of energy is uniformly transformed in $1$ second:

:$1 \, \mathrm W = 1 \, \mathrm J \, \mathrm s^{-1}$


Hence it can be understood as:
:the rate at which work is done when the velocity of a body is held constant at $1$ metre per second against a constant opposing force of $1$ newton
:the rate at which work is performed when an electric current of $1$ ampere flows across an electrical potential difference of $1$ volt.

=== Conversion Factors ===


=== Symbol ===
 

=== Base Units ===
The SI base units of the watt are:

:$\mathrm W := \mathrm {kg} \, \mathrm m^2 \mathrm s^{-3}$
where:
:$\mathrm {kg}$ denotes kilograms
:$\mathrm m$ denotes metres
:$\mathrm s$ denotes seconds (of time).

This arises from the definition of the watt as $\mathrm J \, \mathrm s^{-1}$, that is, joules per second.
 ",Definition:Power (Physics),"['Definitions/Power (Physics)', 'Definitions/Physics', 'Definitions/Mechanics']"
Definition:Prime Element,Prime Element,"Let $R$ be a commutative ring.

Let $p \in R \setminus \set 0$ be any non-zero element of $R$.


Then $p$ is a prime element of $R$  if and only if :
:$(1): \quad p$ is not a unit of $R$ 
:$(2): \quad$ whenever $a, b \in R$ such that $p$ divides $a b$, then either $p$ divides $a$ or $p$ divides $b$.",Definition:Prime Element of Ring,"['Definitions/Ring Theory', 'Definitions/Factorization', 'Definitions/Prime Elements of Rings']"
Definition:Prime Element,Prime Element,"Let $\struct {S, \wedge, \preceq}$ be a meet semilattice.

Let $p \in S$.


Then $p$ is a prime element (of $\struct {S, \wedge, \preceq}$)  if and only if :
:$\forall x, y \in S: \paren {x \wedge y \preceq p \implies x \preceq p \text { or } y \preceq p}$",Definition:Prime Element (Order Theory),['Definitions/Order Theory']
Definition:Prime Ideal,Prime Ideal,"Let $R$ be a ring.


A prime ideal of $R$ is a proper ideal $P$ such that:
:$I \circ J \subseteq P \implies I \subseteq P \text { or } J \subseteq P$
for any ideals $I$ and $J$ of $R$.",Definition:Prime Ideal of Ring,"['Definitions/Ideal Theory', 'Definitions/Prime Ideals of Rings']"
Definition:Prime Ideal,Prime Ideal,"Let $I$ be an ideal in an ordered set $S$.


Then $I$ is a prime ideal in $S$  if and only if  $S \setminus I$ is a filter.",Definition:Prime Ideal (Order Theory),['Definitions/Order Theory']
Definition:Primitive,Primitive,"=== Primitive of Real Function ===
Let $F$ be a real function which is continuous on the closed interval $\closedint a b$ and differentiable on the open interval $\openint a b$.

Let $f$ be a real function which is continuous on the open interval $\openint a b$.


Let:
:$\forall x \in \openint a b: \map {F'} x = \map f x$
where $F'$ denotes the derivative of $F$   $x$.


Then $F$ is a primitive of $f$, and is denoted:
:$\ds F = \int \map f x \rd x$

=== Primitive of Complex Function ===
Let $F: D \to \C$ be a complex function which is complex-differentiable on a connected domain $D$.

Let $f: D \to \C$ be a continuous complex function.


Let:
:$\forall z \in D: \map {F'} z = \map f z$
where $F'$ denotes the derivative of $F$   $z$.


Then $F$ is a primitive of $f$, and is denoted:
:$\ds F = \int \map f z \rd z$

=== Primitive of Vector-Valued Function ===
Let $U \subset \R$ be an open set in $\R$.

Let $\mathbf f: U \to \R^n$ be a vector-valued function on $U$:

:$\forall x \in U: \map {\mathbf f} x = \ds \sum_{k \mathop = 1}^n \map {f_k} x \mathbf e_k$

where:
:$f_1, f_2, \ldots, f_n$ are real functions from $U$ to $\R$
:$\tuple {\mathbf e_1, \mathbf e_2, \ldots, \mathbf e_k}$ denotes the standard ordered basis on $\R^n$.

Let $\mathbf f$ be differentiable on $U$.


Let $\map {\mathbf g} x := \dfrac \d {\d x} \map {\mathbf f} x$ be the derivative of $\mathbf f$   $x$.


The primitive of $\mathbf g$   $x$ is defined as:

:$\ds \int \map {\mathbf g} x \rd x := \map {\mathbf f} x + \mathbf c$

where $\mathbf c$ is a arbitrary constant vector.",Definition:Primitive (Calculus),"['Definitions/Primitives', 'Definitions/Integral Calculus']"
Definition:Primitive,Primitive,"Let $\Q \sqbrk X$ be the ring of polynomial forms over the field of rational numbers in the indeterminate $X$.

Let $f \in \Q \sqbrk X$ be such that:
:$\cont f = 1$
where $\cont f$ is the content of $f$.


That is:
:The greatest common divisor of the coefficients of $f$ is equal to $1$.
 


Then $f$ is described as primitive.",Definition:Primitive Polynomial (Ring Theory),['Definitions/Polynomial Theory']
Definition:Primitive,Primitive,"Let $\tuple {x, y, z}$ be a Pythagorean triple such that $x \perp y$ (that is, $x$ and $y$ are coprime).

Then $\tuple {x, y, z}$ is a primitive Pythagorean triple.


=== Canonical Form ===
Let $\tuple {x, y, z}$ be a primitive Pythagorean triple.


The convention for representing $\tuple {x, y, z}$ as a (primitive) Pythagorean triple is that $x$ is the even element, while $y$ and $z$ are both odd.

This is the canonical form of a Pythagorean triple.",Definition:Pythagorean Triple/Primitive,['Definitions/Pythagorean Triples']
Definition:Primitive,Primitive,"A primitive abundant number is an abundant number whose aliquot parts are all deficient.


=== Sequence of Primitive Abundant Numbers ===
The sequence of primitive abundant numbers begins:
:$20, 70, 88, 104, 272, 304, 368, 464, 550, 572, 650, 748, 836, 945, 1184, 1312, \ldots$

 ",Definition:Primitive Abundant Number,"['Definitions/Abundance', 'Definitions/Abundancy', 'Definitions/Abundant Numbers', 'Definitions/Primitive Abundant Numbers']"
Definition:Primitive,Primitive,"A primitive semiperfect number is a semiperfect number which is not a multiple of a smaller semiperfect number.


=== Sequence of Primitive Semiperfect Numbers ===
The sequence of primitive semiperfect numbers begins:
:$6, 20, 28, 88, 104, 272, 304, 350, 368, 464, 490, 496, 550, 572, \ldots$

 ",Definition:Primitive Semiperfect Number,"['Definitions/Semiperfect Numbers', 'Definitions/Primitive Semiperfect Numbers']"
Definition:Primitive,Primitive,"=== Primitive Recursion on Several Variables ===
Let $f: \N^k \to \N$ and $g: \N^{k + 2} \to \N$ be functions.

Let $\tuple {n_1, n_2, \ldots, n_k} \in \N^k$.

Then the function $h: \N^{k + 1} \to \N$ is obtained from $f$ and $g$ by primitive recursion  if and only if :
:$\forall n \in \N: \map h {n_1, n_2, \ldots, n_k, n} = \begin {cases}
\map f {n_1, n_2, \ldots, n_k} & : n = 0 \\
\map g {n_1, n_2, \ldots, n_k, n - 1, \map h {n_1, n_2, \ldots, n_k, n - 1} } & : n > 0 
\end {cases}$


Category:Definitions/Recursion Theory

=== Primitive Recursion on One Variable ===
Let $a \in \N$ be a natural number.

Let $g: \N^2 \to \N$ be a function.

Then the function $h: \N \to \N$ is obtained from the constant $a$ and $g$ by primitive recursion  if and only if :
:$\forall n \in \N: \map h n = \begin {cases}
a & : n = 0 \\
\map g {n - 1, \map h {n - 1} } & : n > 0 
\end{cases}$

=== Primitive Recursion on Partial Functions ===
Let $f: \N^k \to \N$ and $g: \N^{k+2} \to \N$ be partial functions.

Let $\tuple {n_1, n_2, \ldots, n_k} \in \N^k$.

Then the partial function $h: \N^{k + 1} \to \N$ is obtained from $f$ and $g$ by primitive recursion  if and only if :
:$\forall n \in \N: \map h {n_1, n_2, \ldots, n_k, n} \approx \begin {cases}
\map f {n_1, n_2, \ldots, n_k} & : n = 0 \\
\map g {n_1, n_2, \ldots, n_k, n - 1, \map h {n_1, n_2, \ldots, n_k, n - 1} } & : n > 0 
\end{cases}$

where $\approx$ is as defined in Partial Function Equality.


Note that $\map h {n_1, n_2, \ldots, n_k, n}$ is defined only when:
:$\map h {n_1, n_2, \ldots, n_k, n - 1}$ is defined
:$\map g {n_1, n_2, \ldots, n_k, n - 1, \map h {n_1, n_2, \ldots, n_k, n - 1} }$ is defined.


Category:Definitions/Recursion Theory

Category:Definitions/Recursion Theory",Definition:Primitive Recursion,['Definitions/Recursion Theory']
Definition:Primitive,Primitive,"=== Function ===
A function is primitive recursive  if and only if  it can be obtained from basic primitive recursive functions using the operations of substitution and primitive recursion a finite number of times.


Category:Definitions/Recursion Theory
Category:Definitions/Primitive Recursive Functions

=== Set ===
Let $A \subseteq \N$.


Then $A$ is a primitive recursive set  if and only if  its characteristic function $\chi_A$ is a primitive recursive function.


Category:Definitions/Recursion Theory

=== Relation ===
Let $\RR \subseteq \N^k$ be an $n$-ary relation on $\N^k$.


Then $\RR$ is a primitive recursive relation  if and only if  its characteristic function $\chi_\RR$ is a primitive recursive function.


Category:Definitions/Recursion Theory

Category:Definitions/Recursion Theory",Definition:Primitive Recursive,['Definitions/Recursion Theory']
Definition:Primitive,Primitive,"For a definition to not be circular, the definer must use already defined terms. 

However, this process cannot go on indefinitely. If we were to insist on everything being defined only using previously defined terms, we would enter an infinite regress.


Concepts that are not defined in terms of previously defined concepts are called undefined terms.

An undefined term is frequently explained by using an ostensive definition: that is, a statement that shows what something is, rather than explains.",Definition:Undefined Term,"['Definitions/Logic', 'Definitions/Definitions']"
Definition:Primitive,Primitive,"Let $F / K$ be a simple field extension such that $F = \map K \alpha$.


Then $\alpha$ is a primitive element of $F$.",Definition:Primitive Element of Field Extension,['Definitions/Field Extensions']
Definition:Primitive Root,Primitive Root,"Let $a, n \in \Z_{>0}$, that is, let $a$ and $n$ be strictly positive integers.

Let the multiplicative order of $a$ modulo $n$ be $\map \phi n$, where $\map \phi n$ is the Euler phi function of $n$.


Then $a$ is a primitive root of $n$ or a primitive root modulo $n$.",Definition:Primitive Root (Number Theory),['Definitions/Number Theory']
Definition:Primitive Root,Primitive Root,"Let $n \in \Z_{> 0}$ be a strictly positive integer.

Let $F$ be a field.

Let $U_n$ denote the set of all $n$-th roots of unity.


=== Definition 1 ===
Let $n \in \Z_{> 0}$ be a strictly positive integer.

Let $F$ be a field.

Let $U_n$ denote the set of all $n$-th roots of unity.


A primitive $n$th root of unity of $F$ is an element $\alpha \in U_n$ such that:

:$U_n = \set {1, \alpha, \ldots, \alpha^{n - 1} }$

=== Definition 2 ===
Let $n \in \Z_{> 0}$ be a strictly positive integer.

Let $F$ be a field.

Let $U_n$ denote the set of all $n$-th roots of unity.


A primitive $n$th root of unity of $F$ is an element $\alpha \in U_n$ such that:
:$\forall m : 0 < m < n : \alpha^m \ne 1$",Definition:Root of Unity/Primitive,['Definitions/Roots of Unity']
Definition:Principal,Principal,"In a compound statement, exactly one of its logical connectives has the largest scope.

That connective is called the main connective.

The scope of the main connective comprises the entire compound statement.


 ",Definition:Main Connective,"['Definitions/Main Connective', 'Definitions/Propositional Logic', 'Definitions/Logic']"
Definition:Principal,Principal,"Let $\struct {S, \preceq}$ be a preordered set.

Let $I$ be an ideal in $S$.

=== Definition 1 ===
Let $\struct {S, \preceq}$ be a preordered set.

Let $I$ be an ideal in $S$.


Then $I$ is a principal ideal  if and only if :
:$\exists x \in I: x$ is upper bound for $I$

=== Definition 2 ===
Let $\struct {S, \preceq}$ be a preordered set.

Let $I$ be an ideal in $S$.


Then $I$ is a principal ideal  if and only if :
:$\exists x \in S: I = x^\preceq$
where $x^\preceq$ denotes the lower closure of $x$.",Definition:Principal Ideal of Preordered Set,"['Definitions/Principal Ideals of Preordered Sets', 'Definitions/Preorder Theory']"
Definition:Principal,Principal,"Let $\mathbf A = \sqbrk a_{m n}$ be a matrix.

The elements $a_{j j}: j \in \closedint 1 {\min \set {m, n} }$ constitute the main diagonal of $\mathbf A$.

That is, the main diagonal of $\mathbf A$ is the diagonal of $\mathbf A$ from the top left corner, that is, the element $a_{1 1}$, running towards the lower right corner.


=== Diagonal Elements ===
The elements of the main diagonal of a matrix or a determinant are called the diagonal elements.",Definition:Matrix/Diagonal/Main,"['Definitions/Main Diagonal', 'Definitions/Matrix Diagonals', 'Definitions/Matrices']"
Definition:Principal,Principal,"Let $A$ and $B$ be sets.

Let $f: A \to B$ be a multifunction on $A$.

Let $\sequence {S_i}_{i \mathop \in I}$ be a partitioning of the codomain of $f$ into branches.


It is usual to distinguish one such branch of $f$ from the others, and label it the principal branch of $f$.


=== Principal Value ===
Let $A$ and $B$ be sets.

Let $f: A \to B$ be a multifunction on $A$.

Let $x \in A$ be an element of the domain of $f$.

The principal value of $x$ is the element $y$ of the principal branch of $f$ such that $\map f x = y$.",Definition:Multifunction/Principal Branch,['Definitions/Multifunctions']
Definition:Principal,Principal,"It is understood that the argument of a complex number $z$ is unique only up to multiples of $2 k \pi$.

With this understanding, we can limit the choice of what $\theta$ can be for any given $z$ by requiring that $\theta$ lie in some half open interval of length $2 \pi$.

The most usual of these are:
:$\hointr 0 {2 \pi}$
:$\hointl {-\pi} \pi$

but in theory any such interval may be used.

This interval is known as the principal range.",Definition:Argument of Complex Number/Principal Range,['Definitions/Argument of Complex Number']
Definition:Principal,Principal,"Let $R$ be the principal range of the complex numbers $\C$.

The unique value of $\theta$ in $R$ is known as the principal argument, of $z$.

This is denoted $\Arg z$.

Note the capital $A$.

The standard practice is for $R$ to be $\hointl {-\pi} \pi$.

This ensures that the principal argument is continuous on the real axis for positive numbers.

Thus, if $z$ is represented in the complex plane, the principal argument $\Arg z$ is intuitively defined as the angle which $z$ yields with the real ($y = 0$) axis.


 ",Definition:Argument of Complex Number/Principal Argument,['Definitions/Argument of Complex Number']
Definition:Principal,Principal,"Let $z \in \C$ be a complex number.

Let $z^{1/2} = \set {w \in \C: w^2 = z}$ be the square root of $z$.


The principal square root of $z$ is the principal branch of the $2$nd power of $w$.


Hence, by the conventional definition of the principal branch of the natural logarithm of $z$, it is the element $w$ of $z^{1/2}$ such that:
:$-\dfrac \pi 2 < \arg w \le \dfrac \pi 2$",Definition:Square Root/Complex Number/Principal Square Root,['Definitions/Complex Square Roots']
Definition:Principal,Principal,"The principal branch of a complex number raised to a complex power is defined as:

:$z^k = e^{k \Ln z}$

where $\Ln z$ is the principal branch of the natural logarithm.


=== Positive Real Base ===
Let $t > 0$ be a real number and let $k$ be a complex number.


The principal branch of a positive real number raised to a complex power is defined as:

:$t^k = e^{k \ln t}$

where $\ln$ is the natural logarithm of a positive real number.


Category:Definitions/Complex Powers

Category:Definitions/Complex Powers",Definition:Power (Algebra)/Complex Number/Principal Branch,['Definitions/Complex Powers']
Definition:Principal,Principal,"Let $a, b \in \Z$ be integers such that $b \ne 0$.

From the Division Theorem, we have that:

:$\forall a, b \in \Z, b \ne 0: \exists_1 q, r \in \Z: a = q b + r, 0 \le r < \size b$


The value $r$ is defined as the remainder of $a$ on division by $b$, or the remainder of $\dfrac a b$'''.


=== Real Arguments ===

When $x, y \in \R$ the remainder is still defined:

Let $x, y \in \R$ be real numbers such that $y \ne 0$.

The remainder of $x$ on division by $y$ is defined as the value of $r$ in the expression:

:$\forall x, y \in \R, y \ne 0: \exists! q \in \Z, r \in \R: x = q y + r, 0 \le r < \size y$


From the definition of the Modulo Operation:

:$x \bmod y := x - y \floor {\dfrac x y}$

it can be seen that the remainder of $x$ on division by $y$ is defined as:
:$r = x \bmod y$",Definition:Remainder,"['Definitions/Integer Division', 'Definitions/Integers', 'Definitions/Number Theory', 'Definitions/Discrete Mathematics']"
Definition:Principal,Principal,"Let $G$ be a finite abelian group.

The character $\chi_0: G \to \C_{\ne 0}$ defined as:

:$\forall g \in G: \map {\chi_0} g = 1$

is the trivial character on $G$.",Definition:Trivial Character,['Definitions/Analytic Number Theory']
Definition:Principal,Principal,"Let $\struct {R, +, \circ}$ be a ring with unity.

Let $a \in R$.


We define:

=== Definition 1 ===
Let $\struct {R, +, \circ}$ be a ring with unity.

Let $a \in R$.

We define:

:$\ideal a = \ds \set {\sum_{i \mathop = 1}^n r_i \circ a \circ s_i: n \in \N, r_i, s_i \in R}$


The ideal $\ideal a$ is called the principal ideal of $R$ generated by $a$.

=== Definition 2 ===
Let $\struct {R, +, \circ}$ be a ring with unity.

Let $a \in R$.

We define:

:$\ideal a$ is the smallest ideal of $\struct {R, +, \circ}$ containing $a$ as an element.


The ideal $\ideal a$ is called the principal ideal of $R$ generated by $a$.

=== Definition 3 ===
Let $\struct {R, +, \circ}$ be a ring with unity.

Let $a \in R$.

We define:

:$\ideal a$ is the intersection of all ideals of $\struct {R, +, \circ}$ which contain $a$ as an element.


The ideal $\ideal a$ is called the principal ideal of $R$ generated by $a$.

=== Definition 4 ===
Let $\struct {R, +, \circ}$ be a ring with unity.

Let $a \in R$.

We define:

:$\ideal a$ is an ideal of $\struct {R, +, \circ}$ such that every element of $\ideal a$ is of the form $a \circ r$, where $r \in R$


The ideal $\ideal a$ is called the principal ideal of $R$ generated by $a$.

The ideal $\ideal a$ is called the principal ideal of $R$ generated by $a$.",Definition:Principal Ideal of Ring,"['Definitions/Principal Ideals of Rings', 'Definitions/Ideal Theory', 'Definitions/Commutative Algebra', 'Definitions/Integral Domains']"
Definition:Principal,Principal,A principal ideal domain is an integral domain in which every ideal is a principal ideal.,Definition:Principal Ideal Domain,"['Definitions/Principal Ideal Domains', 'Definitions/Ideal Theory', 'Definitions/Integral Domains']"
Definition:Principal,Principal,"Let $S$ be a set.

Let $\powerset S$ denote the power set of $S$.

Let $\FF \subset \powerset S$ be an ultrafilter on $S$ with a cluster point.


Then $\FF$ is a principal ultrafilter on $S$.


=== Nonprincipal Ultrafilter ===
Let $S$ be a set.

Let $\powerset S$ denote the power set of $S$.


Let $\FF \subset \powerset S$ be an ultrafilter on $S$ which does not have a cluster point.

Then $\FF$ is a nonprincipal ultrafilter  on $S$.",Definition:Principal Ultrafilter,['Definitions/Filter Theory']
Definition:Principal,Principal,Principal is defined as a quantity of money that is either borrowed or invested.,Definition:Principal (Economics),"['Definitions/Principal (Economics)', 'Definitions/Economics']"
Definition:Principal Ideal,Principal Ideal,"Let $\struct {R, +, \circ}$ be a ring with unity.

Let $a \in R$.


We define:

=== Definition 1 ===
Let $\struct {R, +, \circ}$ be a ring with unity.

Let $a \in R$.

We define:

:$\ideal a = \ds \set {\sum_{i \mathop = 1}^n r_i \circ a \circ s_i: n \in \N, r_i, s_i \in R}$


The ideal $\ideal a$ is called the principal ideal of $R$ generated by $a$.

=== Definition 2 ===
Let $\struct {R, +, \circ}$ be a ring with unity.

Let $a \in R$.

We define:

:$\ideal a$ is the smallest ideal of $\struct {R, +, \circ}$ containing $a$ as an element.


The ideal $\ideal a$ is called the principal ideal of $R$ generated by $a$.

=== Definition 3 ===
Let $\struct {R, +, \circ}$ be a ring with unity.

Let $a \in R$.

We define:

:$\ideal a$ is the intersection of all ideals of $\struct {R, +, \circ}$ which contain $a$ as an element.


The ideal $\ideal a$ is called the principal ideal of $R$ generated by $a$.

=== Definition 4 ===
Let $\struct {R, +, \circ}$ be a ring with unity.

Let $a \in R$.

We define:

:$\ideal a$ is an ideal of $\struct {R, +, \circ}$ such that every element of $\ideal a$ is of the form $a \circ r$, where $r \in R$


The ideal $\ideal a$ is called the principal ideal of $R$ generated by $a$.

The ideal $\ideal a$ is called the principal ideal of $R$ generated by $a$.",Definition:Principal Ideal of Ring,"['Definitions/Principal Ideals of Rings', 'Definitions/Ideal Theory', 'Definitions/Commutative Algebra', 'Definitions/Integral Domains']"
Definition:Principal Ideal,Principal Ideal,"Let $\struct {S, \preceq}$ be a preordered set.

Let $I$ be an ideal in $S$.

=== Definition 1 ===
Let $\struct {S, \preceq}$ be a preordered set.

Let $I$ be an ideal in $S$.


Then $I$ is a principal ideal  if and only if :
:$\exists x \in I: x$ is upper bound for $I$

=== Definition 2 ===
Let $\struct {S, \preceq}$ be a preordered set.

Let $I$ be an ideal in $S$.


Then $I$ is a principal ideal  if and only if :
:$\exists x \in S: I = x^\preceq$
where $x^\preceq$ denotes the lower closure of $x$.",Definition:Principal Ideal of Preordered Set,"['Definitions/Principal Ideals of Preordered Sets', 'Definitions/Preorder Theory']"
Definition:Product,Product,"Let $a \times b$ denote the operation of multiplication on two objects $a$ and $b$.

Then the result $a \times b$ is referred to as the product of $a$ and $b$.


Note that the nature of $a$ and $b$ has deliberately been left unspecified.

They could be, for example, numbers, matrices or more complex expressions constructed from such elements.",Definition:Multiplication/Product,['Definitions/Multiplication']
Definition:Product,Product,"Let $\struct {S, \times}$ be an algebraic structure where the operation $\times$ is an operation derived from, or arising from, the multiplication operation on the natural numbers.

Let $\tuple {a_1, a_2, \ldots, a_n} \in S^n$ be an ordered $n$-tuple in $S$.


=== Definition by Index ===
Let $\struct {S, \times}$ be an algebraic structure where the operation $\times$ is an operation derived from, or arising from, the multiplication operation on the natural numbers.

Let $\tuple {a_1, a_2, \ldots, a_n} \in S^n$ be an ordered $n$-tuple in $S$.


The composite is called the continued product of $\tuple {a_1, a_2, \ldots, a_n}$, and is written:

:$\ds \prod_{j \mathop = 1}^n a_j = \paren {a_1 \times a_2 \times \cdots \times a_n}$

=== Definition by Inequality ===
Let $\struct {S, \times}$ be an algebraic structure where the operation $\times$ is an operation derived from, or arising from, the multiplication operation on the natural numbers.

Let $\tuple {a_1, a_2, \ldots, a_n} \in S^n$ be an ordered $n$-tuple in $S$.


The continued product of $\tuple {a_1, a_2, \ldots, a_n}$ can be written:
:$\ds \prod_{1 \mathop \le j \mathop \le n} a_j = \paren {a_1 \times a_2 \times \cdots \times a_n}$

=== Definition by Propositional Function ===
Let $\struct {S, \times}$ be an algebraic structure where the operation $\times$ is an operation derived from, or arising from, the multiplication operation on the natural numbers.

Let $\tuple {a_1, a_2, \ldots, a_n} \in S^n$ be an ordered $n$-tuple in $S$.


Let $\map R j$ be a propositional function of $j$.

Then we can write:

:$\ds \prod_{\map R j} a_j = \text { the product of all $a_j$ such that $\map R j$ holds}$.


If more than one propositional function is written under the product sign, they must all hold.


Such an operation on an ordered tuple is known as a continued product.


Note that the definition by inequality form $1 \le j \le n$ is a special case of such a propositional function.

Also note that the definition by index form $\ds \prod_{j \mathop = 1}^n$ is merely another way of writing $\ds \prod_{1 \mathop \le j \mathop \le n}$.

Hence all instances of a continued product can be expressed in terms of a propositional function.


=== Iverson's Convention ===
Let $\ds \prod_{\map R j} a_j$ be the continued product over all $a_j$ such that $j$ satisfies $R$.


This can also be expressed:
:$\ds \prod_{j \mathop \in \Z} a_j^{\sqbrk {\map R j} }$
where $\sqbrk {\map R j}$ is Iverson's convention.",Definition:Continued Product,"['Definitions/Continued Products', 'Definitions/Algebra', 'Definitions/Abstract Algebra']"
Definition:Product,Product,"Let $S$ and $T$ be sets.


The cartesian product $S \times T$ of $S$ and $T$ is the set of ordered pairs $\tuple {x, y}$ with $x \in S$ and $y \in T$:

:$S \times T = \set {\tuple {x, y}: x \in S \land y \in T}$


Another way of defining it is by:

:$\tuple {x, y} \in S \times T \iff x \in S, y \in T$

More specifically:
:$\forall p: \paren {p \in S \times T \iff \exists x: \exists y: x \in S \land y \in T \land p = \tuple {x, y} }$


$S \times T$ can be voiced $S$ cross $T$.


=== Class Theory ===
Let $A$ and $B$ be classes.


The cartesian product $A \times B$ of $A$ and $B$ is the class of ordered pairs $\tuple {x, y}$ with $x \in A$ and $y \in B$:

:$A \times B = \set {\tuple {x, y}: x \in A \land y \in B}$


Thus:
:$\forall p: \paren {p \in A \times B \iff \exists x: \exists y: x \in A \land y \in B \land p = \tuple {x, y} }$


$A \times B$ can be voiced $A$ cross $B$.

=== Diagram ===


=== Finite Cartesian Product ===
Let $\sequence {S_n}$ be a sequence of sets. 

The (finite) cartesian product of $\sequence {S_n}$ is defined as:

:$\ds \prod_{k \mathop = 1}^n S_k = \set {\tuple {x_1, x_2, \ldots, x_n}: \forall k \in \N^*_n: x_k \in S_k}$


It is also denoted $S_1 \times S_2 \times \cdots \times S_n$.

Thus $S_1 \times S_2 \times \cdots \times S_n$ is the set of all ordered $n$-tuples $\tuple {x_1, x_2, \ldots, x_n}$ with $x_k \in S_k$.


In particular:
:$\ds \prod_{k \mathop = 1}^2 S_k = S_1 \times S_2$

=== Countable Cartesian Product ===

The same notation can be used to define the (countable) cartesian product of an infinite sequence:

Let $\sequence {S_n}_{n \mathop \in \N}$ be an infinite sequence of sets. 

The cartesian product of $\sequence {S_n}$ is defined as:

:$\ds \prod_{k \mathop = 1}^\infty S_k = \set {\tuple {x_1, x_2, \ldots, x_n, \ldots}: \forall k \in \N: x_k \in S_k}$


It defines the concept:
:$S_1 \times S_2 \times \cdots \times S_n \times \cdots$

Thus $\ds \prod_{k \mathop = 1}^\infty S_k$ is the set of all infinite sequences $\tuple {x_1, x_2, \ldots, x_n, \ldots}$ with $x_k \in S_k$.

=== Family of Sets ===
Let $I$ be an indexing set.

Let $\family {S_i}_{i \mathop \in I}$ be a family of sets indexed by $I$.

The Cartesian product of $\family {S_i}_{i \mathop \in I}$ is the set of all families $\family {s_i}_{i \mathop \in I}$ with $s_i \in S_i$ for each $i \in I$.


This can be denoted $\ds \prod_{i \mathop \in I} S_i$ or, if $I$ is understood, $\ds \prod_i S_i$.",Definition:Cartesian Product,"['Definitions/Set Theory', 'Definitions/Cartesian Product']"
Definition:Product,Product,"Let $S$ and $T$ be sets.

Let $P$ be a set and let $\phi_1: P \to S$ and $\phi_2: P \to T$ be mappings such that:

:For all sets $X$ and all mappings $f_1: X \to S$ and $f_2: X \to T$ there exists a unique mapping $h: X \to P$ such that:
::$\phi_1 \circ h = f_1$
::$\phi_2 \circ h = f_2$

:that is, such that:

$\quad\quad\begin{xy}\xymatrix@+1em@L+3px{
&
X
\ar[ld]_*+{f_1}
\ar@{-->}[d]^*+{h}
\ar[rd]^*+{f_2}
\\
S
&
P
\ar[l]^*+{\phi_1}
\ar[r]_*+{\phi_2}
&
T
}\end{xy}$

:is a commutative diagram.


Then $P$, together with the mappings $\phi_1$ and $\phi_2$, is called a product of $S$ and $T$.


This product of $S$ and $T$ can be denoted $\struct {P, \phi_1, \phi_2}$.


=== Family of Sets ===
Let $\family {S_i}_{i \mathop \in I}$ be an indexed family of sets.

Let $P$ be a set.

Let $\family {\phi_i}_{i \mathop \in I}$ be an indexed family of mappings $\phi_i: P \to S_i$ for all $i \in I$ such that:

:For all sets $X$ and all indexed families $\family {f_i}_{i \mathop \in I}$ of mappings $f_i: X \to S_i$ there exists a unique mapping $h: X \to P$ such that:
::$\forall i \in I: \phi_i \circ h = f_i$

:that is, such that for all $i \in I$:

$\quad \quad \begin {xy} \xymatrix@+1em@L+3px {
X
\ar@{-->}[d]_*+{h}
\ar[dr]^*+{f_i}
\\
P
\ar[r]_*{\phi_i}
&
S_i
} \end {xy}$

:is a commutative diagram.


Then $P$, together with the family of mappings $\family {\phi_i}_{i \mathop \in I}$, is called a product of (the family) $\family {S_i}_{i \mathop \in I}$.


This product of $\family {S_i}_{i \mathop \in I}$ can be denoted $\struct {P, \family {\phi_i}_{i \mathop \in I} }$.

=== Projection ===
Let $\family {S_i}_{i \mathop \in I}$ be an indexed family of sets.

Let $\struct {P, \family {\phi_i}_{i \mathop \in I} }$ be a set product of $\family {S_i}_{i \mathop \in I}$.


The mappings $\phi_i$ are the projections of $P$.",Definition:Set Product,['Definitions/Set Theory']
Definition:Product,Product,"Let $A$ and $B$ be sets.

Let $\mathbf a$ and $\mathbf b$ be the cardinals associated respectively with $A$ and $B$.


Then the product of $\mathbf a$ and $\mathbf b$ is defined as:
:$\mathbf a \mathbf b := \map \Card {A \times B}$
where:
:$A \times B$ denotes the Cartesian product of $A$ and $B$
:$\map \Card {A \times B}$ denotes the cardinal associated with $A \times B$.",Definition:Product of Cardinals,['Definitions/Cardinals']
Definition:Product,Product,"Let $\struct {S, \circ}$ be an algebraic structure.

Let $\circ$ be the operation on $\struct {S, \circ}$.


=== General Operation ===
Let $\struct {S, \circ}$ be an algebraic structure.

Let $\circ$ be the operation on $\struct {S, \circ}$.


Let $z = x \circ y$.

Then $z$ is called the product of $x$ and $y$.

This is an extension of the normal definition of product that is encountered in conventional arithmetic.


=== Left-Hand Product ===
Let $x$ and $y$ be elements which are operated on by a given operation $\circ$.

The left-hand product of $x$ by $y$ is the product $y \circ x$.

=== Right-Hand Product ===
Let $x$ and $y$ be elements which are operated on by a given operation $\circ$.

The right-hand product of $x$ by $y$ is the product $x \circ y$.

=== Group Product ===

Let $\struct {G, \circ}$ be a group.
Let $\struct {G, \circ}$ be a group.


The operation $\circ$ can be referred to as the group law.

=== Ring Product ===

Let $\struct {R, *, \circ}$ be a ring.
Let $\struct {R, *, \circ}$ be a ring.


The distributive operation $\circ$ in $\struct {R, *, \circ}$ is known as the (ring) product.

=== Field Product ===

Let $\struct {F, +, \times}$ be a field.
Let $\struct {F, +, \times}$ be a field.


The distributive operation $\times$ in $\struct {F, +, \times}$ is known as the (field) product.

Category:Definitions/Operations
Category:Definitions/Abstract Algebra
Category:Definitions/Multiplication",Definition:Product (Abstract Algebra),"['Definitions/Operations', 'Definitions/Abstract Algebra', 'Definitions/Multiplication']"
Definition:Product,Product,"Let $\struct {G, \circ}$ be a group.

The term group product can have two different interpretations:


=== Group Law ===
Let $\struct {G, \circ}$ be a group.


The operation $\circ$ can be referred to as the group law.

=== Product Element ===
Let $\struct {G, \circ}$ be a group.


Let $a, b \in G$ such that $ = a \circ b$.

Then $g$ is known as the product of $a$ and $b$.",Definition:Group Product,['Definitions/Group Theory']
Definition:Product,Product,"Let $\struct {G, \circ}$ be a group.


Let $a, b \in G$ such that $ = a \circ b$.

Then $g$ is known as the product of $a$ and $b$.",Definition:Group Product/Product Element,['Definitions/Group Theory']
Definition:Product,Product,"Let $\struct {R, *, \circ}$ be a ring.


The distributive operation $\circ$ in $\struct {R, *, \circ}$ is known as the (ring) product.",Definition:Ring (Abstract Algebra)/Product,['Definitions/Ring Theory']
Definition:Product,Product,"Let $\struct {F, +, \times}$ be a field.


The distributive operation $\times$ in $\struct {F, +, \times}$ is known as the (field) product.",Definition:Field (Abstract Algebra)/Product,"['Definitions/Field Theory', 'Definitions/Multiplication']"
Definition:Product,Product,"=== Matrix Product (Conventional) ===
Let $\struct {R, +, \circ}$ be a ring.

Let $\mathbf A = \sqbrk a_{m n}$ be an $m \times n$ matrix over $R$.

Let $\mathbf B = \sqbrk b_{n p}$ be an $n \times p$ matrix over $R$.

Then the matrix product of $\mathbf A$ and $\mathbf B$ is written $\mathbf A \mathbf B$ and is defined as follows.

Let $\mathbf A \mathbf B = \mathbf C = \sqbrk c_{m p}$.


Then:
:$\ds \forall i \in \closedint 1 m, j \in \closedint 1 p: c_{i j} = \sum_{k \mathop = 1}^n a_{i k} \circ b_{k j}$


Thus $\sqbrk c_{m p}$ is the $m \times p$ matrix where each entry $c_{i j}$ is built by forming the (ring) product of each entry in the $i$'th row of $\mathbf A$ with the corresponding entry in the $j$'th column of $\mathbf B$ and adding up all those products.


This operation is called matrix multiplication, and $\mathbf C$ is the matrix product of $\mathbf A$ with $\mathbf B$.


It follows that matrix multiplication is defined whenever the first matrix has the same number of columns as the second matrix has rows.


=== Pre-Multiplication ===
Let $\struct {R, +, \circ}$ be a ring.

Let $\mathbf A = \sqbrk a_{m n}$ be an $m \times n$ matrix over $R$.

Let $\mathbf B = \sqbrk b_{n p}$ be an $n \times p$ matrix over $R$.


Let $\mathbf A \mathbf B$ be the product of $\mathbf A$ with $\mathbf B$.

Then $\mathbf B$ is pre-multiplied by $\mathbf A$.

=== Post-Multiplication ===
Let $\struct {R, +, \circ}$ be a ring.

Let $\mathbf A = \sqbrk a_{m n}$ be an $m \times n$ matrix over $R$.

Let $\mathbf B = \sqbrk b_{n p}$ be an $n \times p$ matrix over $R$.


Let $\mathbf A \mathbf B$ be the product of $\mathbf A$ with $\mathbf B$.

Then $\mathbf A$ is post-multiplied by $\mathbf B$.

=== Using Einstein Summation Convention ===
Let $\struct {R, +, \circ}$ be a ring.

Let $\mathbf A = \sqbrk a_{m n}$ be an $m \times n$ matrix over $R$.

Let $\mathbf B = \sqbrk b_{n p}$ be an $n \times p$ matrix over $R$.


The matrix product of $\mathbf A$ and $\mathbf B$ can be expressed using the Einstein summation convention as:

Then:
:$c_{i j} := a_{i k} \circ b_{k j}$


The index which appears twice in the expressions on the   is the entry $k$, which is the one summated over.

=== Conformable Matrices ===
Let $\mathbf A$ and $\mathbf B$ be matrices.

It needs to be emphasised that matrix product can be defined on $\mathbf A$ and $\mathbf B$  if and only if  $\mathbf A$ and $\mathbf B$ are conformable.


That is, if the number of rows of one is equal to the number of columns of the other.

=== Matrix Scalar Product ===
Let $\GF$ denote one of the standard number systems.

Let $\map \MM {m, n}$ be the $m \times n$ matrix space over $\GF$.

Let $\mathbf A = \sqbrk a_{m n} \in \map \MM {m, n}$.

Let $\lambda \in \GF$ be any element of $\Bbb F$.


The operation of scalar multiplication of $\mathbf A$ by $\lambda$ is defined as follows.

Let $\lambda \mathbf A = \mathbf C$.

Then:
:$\forall i \in \closedint 1 m, j \in \closedint 1 n: c_{i j} = \lambda a_{i j}$

$\lambda \mathbf A$ is the scalar product of $\lambda$ and $\mathbf A$.


Thus $\mathbf C = \sqbrk c_{m n}$ is the $m \times n$ matrix composed of the product of $\lambda$ with the corresponding elements of $\mathbf A$.


=== Ring ===
Let $\struct {R, +, \circ}$ be a ring.

Let $\mathbf A = \sqbrk a_{m n}$ be an $m \times n$ matrix over $\struct {R, +, \circ}$.

Let $\lambda \in R$ be any element of $R$.


The scalar product of $\lambda$ and $\mathbf A$ is defined as follows.

Let $\lambda \circ \mathbf A = \mathbf C$.

Then:
:$\forall i \in \closedint 1 m, j \in \closedint 1 n: c_{i j} = \lambda \circ a_{i j}$


Thus $\sqbrk c_{m n}$ is the $m \times n$ matrix composed of the product of $\lambda$ with the corresponding elements of $\mathbf A$.


=== Scalar ===
Let $\map \MM {m, n}$ be a matrix space of order $m \times n$ on which scalar multiplication is defined.

Let $\mathbf A = \sqbrk a_{m n} \in \map \MM {m, n}$.


Let $\lambda$ be an element of the underlying structure such that:

:$\mathbf C = \lambda \mathbf A$

where the notation denotes scalar multiplication.


The element $\lambda$ of the underlying structure of $\map \MM {m, n}$ is known as a scalar.


Category:Definitions/Matrix Scalar Product

=== Scalar ===
Let $\map \MM {m, n}$ be a matrix space of order $m \times n$ on which scalar multiplication is defined.

Let $\mathbf A = \sqbrk a_{m n} \in \map \MM {m, n}$.


Let $\lambda$ be an element of the underlying structure such that:

:$\mathbf C = \lambda \mathbf A$

where the notation denotes scalar multiplication.


The element $\lambda$ of the underlying structure of $\map \MM {m, n}$ is known as a scalar.


Category:Definitions/Matrix Scalar Product

=== Commutative Matrix Product ===
 

=== Kronecker Product ===

Also known as matrix direct product:

Let $\mathbf A = \sqbrk a_{m n}$ and $\mathbf B = \sqbrk b_{p q}$ be matrices.

The Kronecker product of $\mathbf A$ and $\mathbf B$ is denoted $\mathbf A \otimes \mathbf B$ and is defined as the block matrix:

:$\mathbf A \otimes \mathbf B = \begin{bmatrix}
a_{11} \mathbf B & a_{12} \mathbf B & \cdots & a_{1n} \mathbf B \\
a_{21} \mathbf B & a_{22} \mathbf B & \cdots & a_{2n} \mathbf B \\
\vdots & \vdots & \ddots & \vdots \\
a_{m1} \mathbf B & a_{m2} \mathbf B & \cdots & a_{mn} \mathbf B
\end{bmatrix}$


Writing this out in full:

:$\mathbf A \otimes \mathbf B = \begin{bmatrix}
a_{11} b_{11} & a_{11} b_{12} & \cdots & a_{11} b_{1q} & \cdots & \cdots & a_{1n} b_{11} & a_{1n} b_{12} & \cdots & a_{1n} b_{1q} \\
a_{11} b_{21} & a_{11} b_{22} & \cdots & a_{11} b_{2q} & \cdots & \cdots & a_{1n} b_{21} & a_{1n} b_{22} & \cdots & a_{1n} b_{2q} \\
\vdots & \vdots & \ddots & \vdots & & & \vdots & \vdots & \ddots & \vdots \\
a_{11} b_{p1} & a_{11} b_{p2} & \cdots & a_{11} b_{pq} & \cdots & \cdots & a_{1n} b_{p1} & a_{1n} b_{p2} & \cdots & a_{1n} b_{pq} \\
\vdots & \vdots & & \vdots & \ddots & & \vdots & \vdots & & \vdots \\
\vdots & \vdots & & \vdots & & \ddots & \vdots & \vdots & & \vdots \\
a_{m1} b_{11} & a_{m1} b_{12} & \cdots & a_{m1} b_{1q} & \cdots & \cdots & a_{mn} b_{11} & a_{mn} b_{12} & \cdots & a_{mn} b_{1q} \\
a_{m1} b_{21} & a_{m1} b_{22} & \cdots & a_{m1} b_{2q} & \cdots & \cdots & a_{mn} b_{21} & a_{mn} b_{22} & \cdots & a_{mn} b_{2q} \\
\vdots & \vdots & \ddots & \vdots & & & \vdots & \vdots & \ddots & \vdots \\
a_{m1} b_{p1} & a_{m1} b_{p2} & \cdots & a_{m1} b_{pq} & \cdots & \cdots & a_{mn} b_{p1} & a_{mn} b_{p2} & \cdots & a_{mn} b_{pq} 
\end{bmatrix}$


Thus, if:
:$\mathbf A$ is a matrix with order $m \times n$
:$\mathbf B$ is a matrix with order $p \times q$

then $\mathbf A \otimes \mathbf B$ is a matrix with order $m p \times n q$.

=== Hadamard Product ===

Also known as Matrix Entrywise Product or Schur Product:

Let $\mathbf A = \sqbrk a_{m n}$ and $\mathbf B = \sqbrk b_{m n}$ be $m \times n$ matrices over a ring $\struct {R, +, \times}$.


The Hadamard product of $\mathbf A$ and $\mathbf B$ is written $\mathbf A \circ \mathbf B$ and is defined as follows:

:$\mathbf A \circ \mathbf B := \mathbf C = \sqbrk c_{m n}$

where:

:$\forall i \in \closedint 1 m, j \in \closedint 1 n: c_{i j} = a_{i j} \times b_{i j}$


=== Defined Operation ===


=== Frobenius Inner Product ===
 

=== Cracovian ===
 ",Definition:Matrix Product,"['Definitions/Matrix Products', 'Definitions/Matrix Theory', 'Definitions/Matrix Algebra', 'Definitions/Vector Algebra']"
Definition:Product,Product,"=== Binary Product ===
Let $\mathbf C$ be a metacategory.

Let $A$ and $B$ be objects of $\mathbf C$.


A (binary) product diagram for $A$ and $B$ comprises an object $P$ and morphisms $p_1: P \to A$, $p_2: P \to B$:

::$\begin{xy}\xymatrix@+1em@L+3px{
 A
&
 P
  \ar[l]_*+{p_1}
  \ar[r]^*+{p_2}
&
 B
}\end{xy}$

subjected to the following universal mapping property:


:For any object $X$ and morphisms $x_1, x_2$ like so:

::$\begin{xy}\xymatrix@+1em@L+3px{
 A
&
 X
  \ar[l]_*+{x_1}
  \ar[r]^*+{x_2}
&
 B
}\end{xy}$

:there is a unique morphism $u: X \to P$ such that:

::$\begin{xy}\xymatrix@+1em@L+3px{
&
 X
  \ar[ld]_*+{x_1}
  \ar@{-->}[d]^*+{u}
  \ar[rd]^*+{x_2}

\\
 A
&
 P
  \ar[l]^*+{p_1}
  \ar[r]_*+{p_2}
&
 B
}\end{xy}$

:is a commutative diagram, i.e., $x_1 = p_1 \circ u$ and $x_2 = p_2 \circ u$.


In this situation, $P$ is called a (binary) product of $A$ and $B$ and may be denoted $A \times B$.

Generally, one writes $\left\langle{x_1, x_2}\right\rangle$ for the unique morphism $u$ determined by above diagram.


The morphisms $p_1$ and $p_2$ are often taken to be implicit.

They are called projections; if necessary, $p_1$ can be called the first projection and $p_2$ the second projection.
 

=== General Definition ===
Let $\mathbf C$ be a metacategory.

Let $\CC$ be any collection of objects of $\mathbf C$.

Let $\map {\mathbf {Dis} } \CC$ be the discrete category on $\CC$, considered as a subcategory of $\mathbf C$.


A product for $\CC$, denoted $\ds \prod \CC$, is a limit for the inclusion functor $D: \map {\mathbf {Dis} } \CC \to \mathbf C$, considered as a diagram.


For an object $C$ in $\CC$, the associated morphism $\ds \prod \CC \to C$ is denoted $\pr_C$ and called the projection on $C$.

The whole construction is pictured in the following commutative diagram:

::$\begin{xy}\xymatrix@R-.5em@L+3px{
& &
 A
  \ar@{-->}[dd]
  \ar[dddl]_*+{a_C}
  \ar[dddr]^*+{a_C'}

\\ \\ & &
 \ds \prod \CC
  \ar[dl]^*{\pr_C}
  \ar[dr]_*{\pr_{C'}}

\\
 \map {\mathbf {Dis} } \CC
&
 C
&
 \dots \quad \dots
&
 C'
}\end{xy}$


=== Finite Product ===
Let $\mathbf C$ be a metacategory.

Let $\ds \prod \CC$ be a product for a finite set $\CC$ of objects of $\mathbf C$.


Then $\ds \prod \CC$ is called a finite product.",Definition:Product (Category Theory),['Definitions/Category Theory']
Definition:Projection,Projection,"Let $S_1, S_2, \ldots, S_j, \ldots, S_n$ be sets.

Let $\ds \prod_{i \mathop = 1}^n S_i$ be the Cartesian product of $S_1, S_2, \ldots, S_n$.

For each $j \in \set {1, 2, \ldots, n}$, the $j$th projection on $\ds S = \prod_{i \mathop = 1}^n S_i$ is the mapping $\pr_j: S \to S_j$ defined by:
:$\map {\pr_j} {s_1, s_2, \ldots, s_j, \ldots, s_n} = s_j$

for all $\tuple {s_1, s_2, \ldots, s_n} \in S$.


=== Family of Sets ===
Let $\family {S_i}_{i \mathop \in I}$ be a family of sets.

Let $\ds \prod_{i \mathop \in I} S_i$ be the Cartesian product of $\family {S_i}_{i \mathop \in I}$.


For each $j \in I$, the $j$th projection on $\ds S = \prod_{i \mathop \in I} S_i$ is the mapping $\pr_j: S \to S_j$ defined by:
:$\map {\pr_j} {\family {s_i}_{i \mathop \in I} } = s_j$

where $\family {s_i}_{i \mathop \in I}$ is an arbitrary element of $\ds \prod_{i \mathop \in I} S_i$.",Definition:Projection (Mapping Theory),"['Definitions/Mapping Theory', 'Definitions/Cartesian Product', 'Definitions/Metric Spaces', 'Definitions/Topology']"
Definition:Projection,Projection,"Let $\family {S_i}_{i \mathop \in I}$ be an indexed family of sets.

Let $\struct {P, \family {\phi_i}_{i \mathop \in I} }$ be a set product of $\family {S_i}_{i \mathop \in I}$.


The mappings $\phi_i$ are the projections of $P$.",Definition:Set Product/Projection,['Definitions/Projections']
Definition:Projection,Projection,"=== Binary Product ===
Let $\mathbf C$ be a metacategory.

Let $A$ and $B$ be objects of $\mathbf C$.


A (binary) product diagram for $A$ and $B$ comprises an object $P$ and morphisms $p_1: P \to A$, $p_2: P \to B$:

::$\begin{xy}\xymatrix@+1em@L+3px{
 A
&
 P
  \ar[l]_*+{p_1}
  \ar[r]^*+{p_2}
&
 B
}\end{xy}$

subjected to the following universal mapping property:


:For any object $X$ and morphisms $x_1, x_2$ like so:

::$\begin{xy}\xymatrix@+1em@L+3px{
 A
&
 X
  \ar[l]_*+{x_1}
  \ar[r]^*+{x_2}
&
 B
}\end{xy}$

:there is a unique morphism $u: X \to P$ such that:

::$\begin{xy}\xymatrix@+1em@L+3px{
&
 X
  \ar[ld]_*+{x_1}
  \ar@{-->}[d]^*+{u}
  \ar[rd]^*+{x_2}

\\
 A
&
 P
  \ar[l]^*+{p_1}
  \ar[r]_*+{p_2}
&
 B
}\end{xy}$

:is a commutative diagram, i.e., $x_1 = p_1 \circ u$ and $x_2 = p_2 \circ u$.


In this situation, $P$ is called a (binary) product of $A$ and $B$ and may be denoted $A \times B$.

Generally, one writes $\left\langle{x_1, x_2}\right\rangle$ for the unique morphism $u$ determined by above diagram.


The morphisms $p_1$ and $p_2$ are often taken to be implicit.

They are called projections; if necessary, $p_1$ can be called the first projection and $p_2$ the second projection.
 

=== General Definition ===
Let $\mathbf C$ be a metacategory.

Let $\CC$ be any collection of objects of $\mathbf C$.

Let $\map {\mathbf {Dis} } \CC$ be the discrete category on $\CC$, considered as a subcategory of $\mathbf C$.


A product for $\CC$, denoted $\ds \prod \CC$, is a limit for the inclusion functor $D: \map {\mathbf {Dis} } \CC \to \mathbf C$, considered as a diagram.


For an object $C$ in $\CC$, the associated morphism $\ds \prod \CC \to C$ is denoted $\pr_C$ and called the projection on $C$.

The whole construction is pictured in the following commutative diagram:

::$\begin{xy}\xymatrix@R-.5em@L+3px{
& &
 A
  \ar@{-->}[dd]
  \ar[dddl]_*+{a_C}
  \ar[dddr]^*+{a_C'}

\\ \\ & &
 \ds \prod \CC
  \ar[dl]^*{\pr_C}
  \ar[dr]_*{\pr_{C'}}

\\
 \map {\mathbf {Dis} } \CC
&
 C
&
 \dots \quad \dots
&
 C'
}\end{xy}$


=== Finite Product ===
Let $\mathbf C$ be a metacategory.

Let $\ds \prod \CC$ be a product for a finite set $\CC$ of objects of $\mathbf C$.


Then $\ds \prod \CC$ is called a finite product.",Definition:Product (Category Theory),['Definitions/Category Theory']
Definition:Projection,Projection,"=== Projection in Plane ===
Let $M$ and $N$ be distinct lines in the plane.

:

The projection on $M$ along $N$ is the mapping $\pr_{M, N}$ such that:
:$\forall x \in \R^2: \map {\pr_{M, N} } x =$ the intersection of $M$ with the line through $x$ parallel to $N$.

 ",Definition:Projection (Geometry),"['Definitions/Geometric Projections', 'Definitions/Geometry']"
Definition:Projection,Projection,"Let $\mathbf u$ and $\mathbf v$ be vector quantities.


=== Definition 1 ===
Let $\mathbf u$ and $\mathbf v$ be vector quantities.


The (vector) projection of $\mathbf u$ onto $\mathbf v$, denoted $\proj_\mathbf v \mathbf u$, is the orthogonal projection of $\mathbf u$ onto a straight line which is parallel to $\mathbf v$.


Hence $\proj_\mathbf v \mathbf u$ is a like vector to $\mathbf v$ whose length is $\norm {\mathbf u} \cos \theta$, where:
:$\norm {\mathbf u}$ is the magnitude of $\mathbf u$
:$\cos \theta$ is the angle between $\mathbf u$ and $\mathbf v$.


:

=== Definition 2 ===
Let $\mathbf u$ and $\mathbf v$ be vector quantities.


The (vector) projection of $\mathbf u$ onto $\mathbf v$ is defined and denoted:

:$\proj_\mathbf v \mathbf u = \dfrac {\mathbf u \cdot \mathbf v} {\norm {\mathbf v}^2} \mathbf v$

where:
:$\cdot$ denotes the dot product
:$\norm {\mathbf v}$ denotes the magnitude of $\mathbf v$.


:

=== Definition 3 ===
Let $\mathbf u$ and $\mathbf v$ be vector quantities.


The (vector) projection of $\mathbf u$ onto $\mathbf v$ is defined and denoted:

:$\proj_\mathbf v \mathbf u = u_{\parallel \mathbf v} \mathbf {\hat v}$

where:
:$u_{\parallel \mathbf v}$ denotes the scalar projection of $\mathbf u$ on $\mathbf v$
:$\mathbf {\hat v}$ denotes the unit vector in the direction of $\mathbf v$.


:

:",Definition:Vector Projection,"['Definitions/Vector Projections', 'Definitions/Vector Algebra']"
Definition:Projection,Projection,"Let $\mathbf u$ and $\mathbf v$ be vector quantities.


=== Definition 1 ===
Let $\mathbf u$ and $\mathbf v$ be vector quantities.


The scalar projection of $\mathbf u$ onto $\mathbf v$, denoted $u_{\parallel \mathbf v}$, is the magnitude of the orthogonal projection of $\mathbf u$ onto a straight line which is parallel to $\mathbf v$.


Hence $u_{\parallel \mathbf v}$ is the magnitude $\norm {\mathbf u} \cos \theta$, where:
:$\norm {\mathbf u}$ is the magnitude of $\mathbf u$
:$\cos \theta$ is the angle between $\mathbf u$ and $\mathbf v$.


:

=== Definition 2 ===
Let $\mathbf u$ and $\mathbf v$ be vector quantities.


The scalar projection of $\mathbf u$ onto $\mathbf v$ is defined and denoted:

:$u_{\parallel \mathbf v} = \dfrac {\mathbf u \cdot \mathbf v} {\norm {\mathbf v} }$

where:
:$\cdot$ denotes the dot product
:$\norm {\mathbf v}$ denotes the magnitude of $\mathbf v$.


:

=== Definition 3 ===
Let $\mathbf u$ and $\mathbf v$ be vector quantities.


The scalar projection of $\mathbf u$ onto $\mathbf v$ is defined and denoted:

:$u_{\parallel \mathbf v} = \mathbf u \cdot \mathbf {\hat v}$

where:
:$\cdot$ denotes the dot product
:$\mathbf {\hat v}$ denotes the unit vector in the direction of $\mathbf v$.


:

:",Definition:Scalar Projection,"['Definitions/Scalar Projections', 'Definitions/Vector Algebra']"
Definition:Projection,Projection,"Let $H$ be a Hilbert space.

Let $P \in \map B H$ be an idempotent operator.


Then $P$ is said to be a projection  if and only if :

:$\ker P = \paren {\Img P}^\perp$

where:
:$\ker P$ denotes the kernel of $P$
:$\Img P$ denotes the image of $P$
:$\perp$ denotes orthocomplementation.",Definition:Projection (Hilbert Spaces),['Definitions/Linear Transformations on Hilbert Spaces']
Definition:Projection,Projection,"Let $H$ be a Hilbert space.

Let $K$ be a closed linear subspace of $H$.


Then the orthogonal projection on $K$ is the mapping $P_K: H \to H$ defined by

:$k = \map {P_K} h \iff k \in K$ and $\map d {h, k} = \map d {h, K}$

where the latter $d$ signifies distance to a set.


That $P_K$ is indeed a mapping is proved on Orthogonal Projection is Mapping.

 
 
 
The name orthogonal projection stems from the fact that $\paren {h - \map {P_K} h} \perp K$.",Definition:Orthogonal Projection,"['Definitions/Hilbert Spaces', 'Definitions/Linear Transformations on Hilbert Spaces']"
Definition:Proper,Proper,"Let $\struct {K, +, \circ}$ be a subfield of $\struct {F, +, \circ}$.


Then $\struct {K, +, \circ}$ is a proper subfield of $\struct {F, +, \circ}$  if and only if  $K \ne F$.


That is, $\struct {K, +, \circ}$ is a proper subfield of $\struct {F, +, \circ}$  if and only if :
:$(1): \quad \struct {K, +, \circ}$ is a subfield of $\struct {F, +, \circ}$
:$(2): \quad K$ is a proper subset of $F$.",Definition:Subfield/Proper Subfield,['Definitions/Subfields']
Definition:Proper,Proper,"Let $\struct {G, \circ}$ be a group.


Then $\struct {H, \circ}$ is a proper subgroup of $\struct {G, \circ}$  if and only if :

: $(1): \quad \struct {H, \circ}$ is a subgroup of $\struct {G, \circ}$
: $(2): \quad H \ne G$, i.e. $H \subset G$.


The notation $H < G$, or $G > H$, means:
: $H$ is a proper subgroup of $G$.


If $H$ is a subgroup of $G$, but it is not specified whether $H = G$ or not, then we write $H \le G$, or $G \ge H$.


=== Non-Trivial Proper Subgroup ===
Let $\struct {G, \circ}$ be a group.

Let $\struct {H, \circ}$ be a subgroup of $\struct {G, \circ}$ such that $\set e \subset H \subset G$, that is:
:$H \ne \set e$
:$H \ne G$

Then $\struct {H, \circ}$ is a non-trivial proper subgroup of $\struct {G, \circ}$.",Definition:Proper Subgroup,['Definitions/Subgroups']
Definition:Proper,Proper,"Let $\struct {R, +, \circ}$ be a ring.


A subring $S$ of $R$ is a proper subring of $R$  if and only if  $S$ is neither the null ring nor $R$ itself.",Definition:Proper Subring,['Definitions/Ring Theory']
Definition:Proper,Proper,"Let $T = \struct {S, \tau}$ be a topological space.

Let $H \subseteq S$ be a continuum in $T$.


Let $K$ be a subcontinuum of $H$ such that $K \ne H$.

That is, let $K$ be a proper subset of $H$.


Then $K$ is a proper subcontinuum of $H$.",Definition:Subcontinuum/Proper Subcontinuum,['Definitions/Continua (Topology)']
Definition:Proper,Proper,"Let $K$ be a division ring.

Let $\struct {S, +, \circ}_K$ be a $K$-algebraic structure with one operation.


Let $T$ be a closed subset of $S$.

Let $\struct {T, +_T, \circ_T}_K$ be a $K$-vector space where:
:$+_T$ is the restriction of $+$ to $T \times T$ and
:$\circ_T$ is the restriction of $\circ$ to $K \times T$.


Then $\struct {T, +_T, \circ_T}_K$ is a (vector) subspace of $\struct {S, +, \circ}_K$.


=== Proper Subspace ===
Let $K$ be a division ring.

Let $\struct {S, +, \circ}_K$ be a $K$-algebraic structure with one operation.

Let $\struct {T, +_T, \circ_T}_K$ be a vector subspace of $\struct {S, +, \circ}_K$.


If $T$ is a proper subset of $S$, then $\struct {T, +_T, \circ_T}_K$ is a proper (vector) subspace of $\struct {S, +, \circ}_K$.

=== Hilbert Spaces ===
Let $K$ be a division ring.

Let $\left({S, +, \circ}\right)_K$ be a $K$-algebraic structure with one operation.


Let $T$ be a closed subset of $S$.

Let $\left({T, +_T, \circ_T}\right)_K$ be a $K$-vector space where:
: $+_T$ is the restriction of $+$ to $T \times T$ and
: $\circ_T$ is the restriction of $\circ$ to $K \times T$.


Then $\left({T, +_T, \circ_T}\right)_K$ is a (vector) subspace of $\left({S, +, \circ}\right)_K$.


When considering Hilbert spaces, one wants to deal with projections onto subspaces.

These projections however require the linear subspace to be closed in topological sense in order to be well-defined.

Therefore, in treatises of Hilbert spaces, one encounters the terminology linear manifold for the concept of vector subspace defined above.

The adapted definition of linear subspace is then that it is a topologically closed linear manifold.",Definition:Vector Subspace,"['Definitions/Linear Algebra', 'Definitions/Vector Algebra']"
Definition:Proper,Proper,"Let $\struct {R, +, \circ}$ be a ring.

Let $\struct {G, +_G, \circ_G}_R$ be an $R$-module.

Let $\struct {H, +_H, \circ_H}_R$ be a submodule of $\struct {G, +_G, \circ_G}_R$.

Let $H$ be a proper subset of $G$.


Then $\struct {H, +_H, \circ_H}_R$ is a proper submodule of $\struct {G, +_G, \circ_G}_R$.",Definition:Submodule/Proper,['Definitions/Module Theory']
Definition:Proper,Proper,"Let $\struct {R, +, \circ}$ be a ring.


A proper ideal $J$ of $\struct {R, +, \circ}$ is an ideal of $R$ such that $J$ is a proper subset of $R$.

That is, such that $J \subseteq R$ and $J \ne R$.",Definition:Ideal of Ring/Proper Ideal,['Definitions/Ideal Theory']
Definition:Proper,Proper,"Let $\left({V, \phi}\right)$ be a $G$-module.


A $G$-submodule of $V$ is called proper iff it is a proper vector subspace of $V$.

",Definition:Proper G-Submodule,['Definitions/Representation Theory']
Definition:Proper,Proper,"Let $G = \struct {V, E}$ be a simple graph.


=== Proper Vertex Coloring ===
Let $G = \struct {V, E}$ be a simple graph.


A proper (vertex) $k$-coloring of $G$ is defined as a vertex coloring from a set of $k$ colors such that no two adjacent vertices share a common color.

That is, a proper $k$-coloring of $G$ is a mapping $c: V \to \set {1, 2, \ldots k}$ such that:
:$\forall e = \set {u, v} \in E: \map c u \ne \map c v$

=== Proper Edge Coloring ===
Let $G = \struct {V, E}$ be a simple graph.


A proper (edge) $k$-coloring of $G$ is defined as an edge coloring from a set of $k$ colors such that no two adjacent edges share a common color.

That is, a proper $k$-coloring of $G$ is a mapping $c: E \to \set {1, 2, \ldots k}$ such that:
:$\forall v \in V: \forall e = \set {u_k, v} \in E: \map c {\set {u_i, v} } \ne \map c {\set {u_j, v} }$

Category:Definitions/Graph Colorings",Definition:Proper Coloring,['Definitions/Graph Colorings']
Definition:Proper,Proper,"Let $G = \struct {V, E}$ be a simple graph.


A proper (vertex) $k$-coloring of $G$ is defined as a vertex coloring from a set of $k$ colors such that no two adjacent vertices share a common color.

That is, a proper $k$-coloring of $G$ is a mapping $c: V \to \set {1, 2, \ldots k}$ such that:
:$\forall e = \set {u, v} \in E: \map c u \ne \map c v$",Definition:Proper Coloring/Vertex Coloring,"['Definitions/Graph Colorings', 'Definitions/Vertices of Graphs']"
Definition:Proper,Proper,"Let $T$ be a rooted tree with root $r_T$.

Let $t$ be a node of $T$.


A proper ancestor node of $t$ is an ancestor node of $t$ that is not $t$ itself.",Definition:Rooted Tree/Ancestor Node/Proper,['Definitions/Ancestor Nodes']
Definition:Proper,Proper,"A proper refinement of a normal series is a refinement which is not equal to the original normal series.


That is, it contains extra (normal) subgroups which are not present in the original normal series.",Definition:Refinement of Normal Series/Proper Refinement,['Definitions/Normal Series']
Definition:Proper,Proper,"Let $\mathbf Q$ be an orthogonal matrix.


Then $\mathbf Q$ is a proper orthogonal matrix  if and only if :
:$\map \det {\mathbf Q} = 1$
where $\map \det {\mathbf Q}$ is the determinant of $\mathbf Q$.",Definition:Proper Orthogonal Matrix,['Definitions/Matrix Algebra']
Definition:Proper,Proper,"Let $\FF$ be a formal language with alphabet $\AA$.

Let $\mathbf A$ be a well-formed formula of $\FF$.

Let $\mathbf B$ be a well-formed part of $\mathbf A$.


Then $\mathbf B$ is a proper well-formed part of $\mathbf A$  if and only if  $\mathbf B$ is not equal to $\mathbf A$.",Definition:Well-Formed Part/Proper Well-Formed Part,['Definitions/Formal Languages']
Definition:Proper,Proper,"Let $\struct {\Z, +, \times}$ be the ring of integers.

Let $x, y \in \Z$.


Then $x$ divides $y$ is defined as:
:$x \divides y \iff \exists t \in \Z: y = t \times x$


Then $x$ is a proper divisor of $y$  if and only if :

:$(1): \quad x \divides y$
:$(2): \quad \size x \ne \size y$
:$(3): \quad x \ne \pm 1$

That is:
:$(1): \quad x$ is a divisor of $y$
:$(2): \quad x$ and $y$ are not equal in absolute value
:$(3): \quad x$ is not equal to either $1$ or $-1$.",Definition:Proper Divisor/Integer,"['Definitions/Proper Divisors', 'Definitions/Divisors']"
Definition:Proper,Proper,A non-zero element of a ring which does not have a product inverse is called a proper element.,Definition:Proper Element of Ring,['Definitions/Ring Theory']
Definition:Proper,Proper,"Let $\struct {R, +, \circ}$ be a ring.


A proper zero divisor of $R$ is an element $x \in R^*$ such that:

:$\exists y \in R^*: x \circ y = 0_R$

where $R^*$ is defined as $R \setminus \set {0_R}$.


That is, it is a zero divisor of $R$ which is specifically not $0_R$.


The presence of a proper zero divisor in a ring means that the product of two elements of the ring may be zero even if neither factor is zero.

That is, if $R$ has proper zero divisors, then $\struct {R^*, \circ}$ is not closed.",Definition:Proper Zero Divisor,['Definitions/Zero Divisors']
Definition:Proper,Proper,"Let $\struct {D, +, \circ}$ be an integral domain whose zero is $0_D$ and whose unity is $1_D$.

Let $U$ be the group of units of $D$.

Let $x, y \in D$.


Then $x$ is a proper divisor of $y$  if and only if :

:$(1): \quad x \divides y$
:$(2): \quad y \nmid x$
:$(3): \quad x \notin U$

That is:
:$(1): \quad x$ is a divisor of $y$
:$(2): \quad x$ is not an associate of $y$
:$(3): \quad x$ is not a unit of $D$


=== Integers ===

As the set of integers form an integral domain, the concept of a proper divisor is fully applicable to the integers.

Let $\struct {\Z, +, \times}$ be the ring of integers.

Let $x, y \in \Z$.


Then $x$ divides $y$ is defined as:
:$x \divides y \iff \exists t \in \Z: y = t \times x$


Then $x$ is a proper divisor of $y$  if and only if :

:$(1): \quad x \divides y$
:$(2): \quad \size x \ne \size y$
:$(3): \quad x \ne \pm 1$

That is:
:$(1): \quad x$ is a divisor of $y$
:$(2): \quad x$ and $y$ are not equal in absolute value
:$(3): \quad x$ is not equal to either $1$ or $-1$.",Definition:Proper Divisor,"['Definitions/Proper Divisors', 'Definitions/Ring Theory']"
Definition:Proper,Proper,"Let $S$ and $T$ be sets such that $S$ is a subset of $T$.

Let $S \ne T$.

Then $S$ is referred to as a proper subset of $T$, and we write:
:$S \subsetneq T$
or:
:$S \subsetneqq T$


=== Proper Superset ===
If $S$ is a proper subset of $T$, then $T$ is a proper superset of $S$.

This can be expressed by the notation $T \supsetneqq S$.",Definition:Proper Subset,['Definitions/Subsets']
Definition:Proper,Proper,"A proper class is a class which is not a set.

That is, $A$ is a proper class  if and only if :
:$\neg \exists x: x = A$
where $x$ is a set.",Definition:Class (Class Theory)/Proper Class,['Definitions/Class Theory']
Definition:Proper,Proper,"Let $\struct {S, \preccurlyeq}$ be an ordered set.

Let $\FF$ be a filter on $\struct {S, \preccurlyeq}$.


Then:
:$\FF$ is a proper filter on $S$
 if and only if :
:$\FF \ne S$

That is,  if and only if  $\FF$ is a proper subset of $S$.",Definition:Filter/Proper Filter,['Definitions/Filter Theory']
Definition:Proper,Proper,"Let $V$ be a basic universe.

Let $A$ be a class.

Let $\RR$ be a well-ordering on $A$.


Then $\RR$ is a proper well-ordering  if and only if :
:every proper lower section of $A$ is a set.",Definition:Proper Well-Ordering,"['Definitions/Well-Orderings', 'Definitions/Class Theory']"
Definition:Proper,Proper,"Let $X$ and $Y$ be topological spaces.


A mapping $f: X \to Y$ is proper  if and only if  for every compact subspace $K \subset Y$, its preimage $f^{-1} \sqbrk K$ is also compact.",Definition:Proper Mapping,"['Definitions/Proper Mappings', 'Definitions/Topology', 'Definitions/Compact Spaces']"
Definition:Proper,Proper,"Let $G$ be a topological group.

Let $X$ be a topological space.


A group action $\phi: G \times X \to X$ is called proper  if and only if  $\phi$ is a proper mapping.


Here $G\times X$ is equipped with the product topology.",Definition:Proper Group Action,"['Definitions/Topology', 'Definitions/Group Theory', 'Definitions/Group Actions', 'Definitions/Topological Groups']"
Definition:Proper,Proper,"A proper name (or just name) is a symbol or collection of symbols used to identify a particular object uniquely.


In contrast with natural language, a proper name has a wider range than being the particular identifying label attached to a particular single entity (be it a person, or a place, or whatever else).

For example:
:Sloth is a proper name for the concept of being lazy.
:Rain is a proper name for the meteorological phenomenon of water falling from the sky.",Definition:Proper Name,"['Definitions/Proper Names', 'Definitions/Predicate Logic']"
Definition:Pullback,Pullback,"Let $\mathbf C$ be a metacategory.

Let $f: A \to C$ and $g: B \to C$ be morphisms with common codomain.


A pullback of $f$ and $g$ is a commutative diagram:

::$\begin{xy}\xymatrix{
 P
  \ar[r]^*+{p_1}
  \ar[d]_*+{p_2}
&
 A
  \ar[d]^*+{f}

\\
 B
  \ar[r]_*+{g}
&
 C
}\end{xy}$

such that $f \circ p_1 = g \circ p_2$, subject to the following UMP:


:For any commutative diagram:

:::$\begin{xy}\xymatrix{
 Q
  \ar[r]^*+{q_1}
  \ar[d]_*+{q_2}
&
 A
  \ar[d]^*+{f}

\\
 B
  \ar[r]_*+{g}
&
 C
}\end{xy}$

:there is a unique morphism $u: Q \to P$ making the following diagram commute:

:::$\begin{xy}\xymatrix@+1em{
 Q
  \ar@/^/[rrd]^*+{q_1}
  \ar@/_/[ddr]_*+{q_2}
  \ar@{-->}[rd]^*+{u}

\\
&
 P
  \ar[r]_*+{p_1}
  \ar[d]^*+{p_2}
&
 A
  \ar[d]^*+{f}

\\
&
 B
  \ar[r]_*+{g}
&
 C
}\end{xy}$


In this situation, $p_1$ is called the pullback of $f$ along $g$ and may be denoted as $g^* f$.

Similarly, $p_2$ is called the pullback of $g$ along $f$ and may be denoted $f^* g$.",Definition:Pullback (Category Theory),"['Definitions/Category Theory', 'Definitions/Pullbacks']"
Definition:Pullback,Pullback,"Let $\mathbf C$ be a metacategory having all pullbacks.

Let $f: C \to D$ be a morphism of $\mathbf C$.

Let $\mathbf C \mathop / C$ and $\mathbf C \mathop / D$ be the slice categories over $C$ and $D$, respectively.


The pullback functor $f^* : \mathbf C \mathop / D \to \mathbf C \mathop / C$ associated to $f$ is defined by:

 
 
 
 

Explicitly, $f^* \gamma$ is defined as the unique morphism fitting:

::$\begin{xy}\xymatrix@+1em@L+4px{
 A'
  \ar[rr]^*{f_\alpha}
  \ar[dd]_*{f^* \alpha}
  \ar@{-->}[rd]_*{f^* \gamma}
& &
 A
  \ar[rd]^*{\gamma}
  \ar[dd]^(.4)*{\alpha}

\\ &
 B'
  \ar[ld]^*{f^* \beta}
  \ar[rr] |{\hole} ^(.3)*{f_\beta}
& &
 B
  \ar[ld]^*{\beta}

\\
 C
  \ar[rr]_*{f}
& &
 D
}\end{xy}$",Definition:Pullback Functor,"['Definitions/Pullbacks', 'Definitions/Slice Categories']"
Definition:Pullback,Pullback,"Let $G, H$ be groups.

Let $N \lhd G, K \lhd H$ be normal subgroups of $G$ and $H$ respectively.

Let:
:$G / N \cong H / K$
where:
:$G / N$ denotes the quotient of $G$ by $N$
:$\cong$ denotes group isomorphism.

Let $\theta: G / N \to H / K$ be such a group isomorphism.


The pullback $G \times^\theta H$ of $G$ and $H$ via $\theta$ is the subset of $G \times H$ of elements of the form $\tuple {g, h}$ where $\map \theta {g N} = h K$.",Definition:Pullback of Quotient Group Isomorphism,"['Definitions/Group Theory', 'Definitions/Normality in Groups']"
Definition:Quadratic Form,Quadratic Form,"Let $\mathbb K$ be a field of characteristic $\Char {\mathbb K} \ne 2$.

Let $V$ be a vector space over $\mathbb K$.


A quadratic form on $V$ is a mapping $q : V \mapsto \mathbb K$ such that:
:$\forall v \in V : \forall \kappa \in \mathbb K : \map q {\kappa v} = \kappa^2 \map q v$
:$b: V \times V \to \mathbb K: \tuple {v, w} \mapsto \map q {v + w} - \map q v - \map q w$ is a bilinear form


 ",Definition:Quadratic Form (Linear Algebra),"['Definitions/Quadratic Forms (Linear Algebra)', 'Definitions/Bilinear Forms (Linear Algebra)', 'Definitions/Linear Algebra', 'Definitions/Module Theory', 'Definitions/Vector Spaces', 'Definitions/Quadratic Forms']"
Definition:Quadratic Form,Quadratic Form,A quadratic form is a form whose variables are of degree $2$.,Definition:Quadratic Form (Polynomial Theory),"['Definitions/Quadratic Forms (Polynomial Theory)', 'Definitions/Forms', 'Definitions/Polynomial Theory', 'Definitions/Quadratic Forms']"
Definition:Quasi-Compact,Quasi-Compact,"=== Definition 1 ===
A topological space $T = \struct {S, \tau}$ is compact  if and only if  every open cover for $S$ has a finite subcover.

=== Definition 2 ===
A topological space $T = \struct {S, \tau}$ is compact  if and only if  it satisfies the Finite Intersection Axiom.

=== Definition 3 ===
A topological space $T = \struct {S, \tau}$ is compact  if and only if  $\tau$ has a sub-basis $\BB$ such that:
:from every cover of $S$ by elements of $\BB$, a finite subcover of $S$ can be selected.

=== Definition 4 ===
A topological space $T = \struct {S, \tau}$ is compact  if and only if  every filter on $S$ has a limit point in $S$.

=== Definition 5 ===
A topological space $T = \struct {S, \tau}$ is compact  if and only if  every ultrafilter on $S$ converges.",Definition:Compact Space/Topology,"['Definitions/Compact Spaces', 'Definitions/Topology']"
Definition:Quasi-Compact,Quasi-Compact,"Let $\struct {X, \OO_X}$ be a scheme.


Then $\struct {X, \OO_X}$ is quasi-compact  if and only if  $X$ is compact.",Definition:Quasi-Compact Scheme,"['Definitions/Algebraic Geometry', 'Definitions/Schemes']"
Definition:Quasi-Compact,Quasi-Compact,"Let $\struct {X, \OO_X}$ and $\struct {Y, \OO_Y}$ be schemes.

Let $f : \struct {X, \OO_X} \to \struct {Y, \OO_Y}$ be a morphism of schemes.


$f$ is quasi-compact  if and only if  for all quasi-compact open subsets $U \subset Y$, the set $\map {f^{-1} } U \subset X$ is quasi-compact.",Definition:Quasi-Compact Morphism of Schemes,"['Definitions/Algebraic Geometry', 'Definitions/Schemes']"
Definition:Radical,Radical,"Let $x, y \in \R_{\ge 0}$ be positive real numbers.

Let $n \in \Z$ be an integer such that $n \ne 0$.


Then $y$ is the positive $n$th root of $x$  if and only if :
:$y^n = x$

and we write:
:$y = \sqrt[n] x$


Using the power notation, this can also be written:
:$y = x^{1/n}$


When $n = 2$, we write $y = \sqrt x$ and call $y$ the positive square root of $x$.

When $n = 3$, we write $y = \sqrt [3] x$ and call $y$ the cube root of $x$.


Note the special case where $x = 0 = y$:
:$0 = \sqrt [n] 0$


=== Index ===
Let $\sqrt [n] x$ denote the $n$th root of $x$.

The number $n$ is known as the index of the root.


If $n$ is not specified, that is $\sqrt x$ is presented, this means the square root.

=== Extraction of Root ===
The process of evaluating roots of a given real number is referred to as extraction.",Definition:Root of Number,"['Definitions/Roots of Numbers', 'Definitions/Real Analysis']"
Definition:Radical,Radical,"Let $L / F$ be a field extension.

Then $L$ is a radical extension of $F$  if and only if  there exist $\alpha_1, \ldots, \alpha_m \in F$ and $n_1, \ldots, n_2 \in \Z_{>0}$ such that:

:$(1): \quad L = K \sqbrk {\alpha_1, \ldots, \alpha_m}$

:$(2): \quad \alpha_1^{n_1} \in F$

:$(3): \quad \forall i \in \N_m: \alpha_i^{n_i} \in F \sqbrk {\alpha_1, \ldots, \alpha_{i-1} }$

where $K \sqbrk {\alpha_1, \ldots, \alpha_m}$ and $F \sqbrk {\alpha_1, \ldots, \alpha_{i-1} }$ are generated field extensions.

Category:Definitions/Field Extensions",Definition:Radical Extension,['Definitions/Field Extensions']
Definition:Radical,Radical,"Let $\mathbb K$ be a field.

Let $V$ be a vector space over $\mathbb K$.

Let $b : V\times V \to \mathbb K$ be a reflexive bilinear form on $V$.


The radical of $V$ is the orthogonal complement of $V$:
:$\map {\operatorname {rad} } V = V^\perp$",Definition:Orthogonal (Bilinear Form)/Radical,['Definitions/Bilinear Forms (Linear Algebra)']
Definition:Radical,Radical,"Let $n \in \Z$ be an integer.

=== Definition 1 ===
Let $n \in \Z$ be an integer.

The radical of $n$ is the product of the individual prime factors of $n$.

=== Definition 2 ===
Let $n \in \Z$ be an integer.

The radical of $n$ is the largest square-free integer which divides $n$.",Definition:Radical of Integer,"['Definitions/Radicals of Integers', 'Definitions/Number Theory']"
Definition:Radical,Radical,"Let $R$ be a commutative ring with unity.

Let $\map {\operatorname{maxspec}} R$ be the set of maximal ideals of $R$.


Then the Jacobson radical of $R$ is:
:$\ds \map {\operatorname {Jac} } R = \bigcap_{m \mathop \in \map {\operatorname{maxspec}} R} m$

That is, it is the intersection of all maximal ideals of $R$.",Definition:Jacobson Radical,['Definitions/Ring Theory']
Definition:Radical,Radical,"Let $A$ be a commutative ring with unity.


=== Definition 1 ===
Let $A$ be a commutative ring with unity.


The nilradical of $A$ is the subset consisting of all nilpotent elements of $A$.

=== Definition 2 ===
Let $A$ be a commutative ring with unity.

Let $\Spec A$ denote the prime spectrum of $A$.


The nilradical of $A$ is:
:$\ds \Nil A = \bigcap_{\mathfrak p \mathop \in \Spec A} \mathfrak p$

That is, it is the intersection of all prime ideals of $A$.",Definition:Nilradical of Ring,['Definitions/Ring Theory']
Definition:Radical,Radical,"Let $A$ be a commutative ring with unity.

Let $I$ be an ideal of $A$.


=== Definition 1 ===
Let $A$ be a commutative ring with unity.

Let $I$ be an ideal of $A$.


The radical of $I$ is the ideal of elements of which some power is in $I$:
:$\map \Rad I := \set {a \in A: \exists n \in \N_{>0} : a^n \in I}$

=== Definition 2 ===
Let $A$ be a commutative ring with unity.

Let $I$ be an ideal of $A$.


Let $A / I$ be the quotient ring of $A$ by $I$.

Let $\Nil {A / I}$ be its nilradical.

Let $\pi: A \to A / I$ be the quotient epimorphism from $A$ onto $A / I$.


The radical of $I$ is the preimage of $\Nil {A / I}$ under $\pi$:
:$\map \Rad I = \pi^{-1} \sqbrk {\Nil {A / I} }$",Definition:Radical of Ideal of Ring,['Definitions/Ideal Theory']
Definition:Radical,Radical,"Let $A$ be a commutative ring with unity.

Let $I$ be an ideal of $A$.


Then $I$ is a radical ideal  if and only if  it is equal to its radical.",Definition:Radical Ideal of Ring,['Definitions/Ideal Theory']
Definition:Radius,Radius,":

A radius of a circle is a straight line segment whose endpoints are the center and the circumference of the circle.

In the above diagram, the line $AB$ is a radius.",Definition:Circle/Radius,['Definitions/Circles']
Definition:Radius,Radius,"A radius of a sphere is a straight line segment whose endpoints are the center and the surface of the sphere.

The radius of a sphere is the length of one such radius.",Definition:Sphere/Geometry/Radius,['Definitions/Spheres']
Definition:Radius,Radius,"The radius of curvature of a curve $C$ at a point $P$ is defined as the reciprocal of the absolute value of its curvature:

:$\rho = \dfrac 1 {\size k}$


=== Cartesian Coordinates ===
Let $C$ be a curve defined by a real function which is twice differentiable.

Let $C$ be embedded in a cartesian plane.


The radius of curvature of $C$ at a point $P$ can be expressed in cartesian coordinates as:
:$\rho = \size {\dfrac {\paren {1 + y'^2}^{3/2} } {y} }$

where:
 
 
 
 

=== Parametric Cartesian Form ===
Let $C$ be a curve defined by a real function which is twice differentiable.

Let $C$ be embedded in a cartesian plane and defined by the parametric equations:
:$\begin{cases} x = \map x t \\ y = \map y t \end{cases}$


The radius of curvature $\rho$ of $C$ at a point $P = \tuple {x, y}$ is given by:

:$\rho = \dfrac {\paren {x'^2 + y'^2}^{3/2} } {\size {x' y - y' x} }$
where:
:$x' = \dfrac {\d x} {\d t}$ is the derivative of $x$   $t$ at $P$
:$y' = \dfrac {\d y} {\d t}$ is the derivative of $y$   $t$ at $P$
:$x$ and $y$ are the second derivatives of $x$ and $y$   $t$ at $P$.",Definition:Radius of Curvature,['Definitions/Curvature']
Definition:Radius,Radius,"Let $P$ be a point in a given frame of reference whose origin is $O$.

The position vector $\mathbf p$ of $P$ is the displacement vector of $P$ from $O$.


=== Notation ===
",Definition:Position Vector,"['Definitions/Position Vectors', 'Definitions/Displacement', 'Definitions/Vectors']"
Definition:Radius,Radius,"Let $M = \struct {A, d}$ be a metric space or pseudometric space.

Let $a \in A$.

Let $\map {B_\epsilon} a$ be the open $\epsilon$-ball of $a$.


In $\map {B_\epsilon} a$, the value $\epsilon$ is referred to as the radius of the open $\epsilon$-ball.",Definition:Open Ball/Radius,['Definitions/Open Balls']
Definition:Radius,Radius,"=== Real Domain ===
Let $\xi \in \R$ be a real number.

Let $\ds \map S x = \sum_{n \mathop = 0}^\infty a_n \paren {x - \xi}^n$ be a power series about $\xi$.

Let $I$ be the interval of convergence of $\map S x$.

Let the endpoints of $I$ be $\xi - R$ and $\xi + R$.

(This follows from the fact that $\xi$ is the midpoint of $I$.)


Then $R$ is called the radius of convergence of $\map S x$.


If $\map S x$ is convergent over the whole of $\R$, then $I = \R$ and thus the radius of convergence is infinite.

=== Complex Domain ===
Let $\xi \in \C$ be a complex number.

For $z \in \C$, let:
:$\ds \map f z = \sum_{n \mathop = 0}^\infty a_n \paren {z - \xi}^n$
be a power series about $\xi$.


The radius of convergence is the extended real number $R \in \overline \R$ defined by:

:$R = \ds \inf \set {\cmod {z - \xi}: z \in \C, \sum_{n \mathop = 0}^\infty a_n \paren {z - \xi}^n \text{ is divergent} }$

where a divergent series is a series that is not convergent.

As usual, $\inf \O = +\infty$.",Definition:Radius of Convergence,"['Definitions/Power Series', 'Definitions/Convergence']"
Definition:Range,Range,"Let $\RR \subseteq S \times T$ be a relation, or (usually) a mapping (which is, of course, itself a relation).


The range of $\RR$, denoted  is defined as one of two things, depending on the source.

On   it is denoted $\Rng \RR$, but this may be non-standard.


=== Range as Codomain ===
Let $\RR \subseteq S \times T$ be a relation, or (usually) a mapping (which is, of course, itself a relation).


The range of $\RR$ can be defined as $T$.

As such, it is the same thing as the term codomain of $\RR$.

=== Range as Image ===
Let $\RR \subseteq S \times T$ be a relation, or (usually) a mapping (which is, of course, itself a relation).


The range of $\RR$ can be defined as:
:$\Rng \RR = \set {t \in T: \exists s \in S: \tuple {s, t} \in \RR}$

Defined like this, it is the same as what is defined as the image of $\RR$.",Definition:Range of Relation,"['Definitions/Relation Theory', 'Definitions/Mapping Theory', 'Definitions/Ranges (Relation Theory)']"
Definition:Range,Range,"Let $S \subseteq \R$.

Let $f: S \to \R$ be a real function.

The range of $f$ is the set of values that the dependent variable can take.


That is, it is the image set of $f$.",Definition:Real Function/Range,['Definitions/Real Functions']
Definition:Range,Range,"Let $S$ be a set of observations of a quantitative variable.


The range of $S$ is defined as:
:$\map R S := \map \max S - \map \min S$

where $\map \max S$ and $\map \min S$ are the greatest value of $S$ and the least value of $S$ respectively.",Definition:Range (Statistics),['Definitions/Descriptive Statistics']
Definition:Rank,Rank,"Let $A$ be a set.

Let $V$ denote the von Neumann hierarchy.


Then the rank of $A$ is the smallest ordinal $x$ such that $A \in \map V {x + 1}$, given that $x$ exists.",Definition:Rank (Set Theory),['Definitions/Axiom of Foundation']
Definition:Rank,Rank,"=== Linear Transformation ===
Let $\phi$ be a linear transformation from one vector space to another.

Let the image of $\phi$ be finite-dimensional.


Then its dimension is called the rank of $\phi$ and is denoted $\map \rho \phi$.

=== Matrix ===
=== Definition 1 ===
Let $K$ be a field.

Let $\mathbf A$ be an $m \times n$ matrix over $K$.


Then the rank of $\mathbf A$, denoted $\map \rho {\mathbf A}$, is the dimension of the subspace of $K^m$ generated by the columns of $\mathbf A$.


That is, it is the dimension of the column space of $\mathbf A$.

=== Definition 2 ===
Let $K$ be a field.

Let $\mathbf A$ be an $m \times n$ matrix over $K$.

Let $\mathbf A$ be converted to echelon form $\mathbf B$.

Let $\mathbf B$ have exactly $k$ non-zero rows.

Then the rank of $\mathbf A$, denoted $\map \rho {\mathbf A}$, is $k$.

=== Definition 3 ===
Let $K$ be a field.

Let $\mathbf A$ be an $m \times n$ matrix over $K$.

The rank of $\mathbf A$, denoted $\map \rho {\mathbf A}$ is the largest number of elements in a linearly independent set of rows of $\mathbf A$.

 
",Definition:Rank (Linear Algebra),['Definitions/Linear Algebra']
Definition:Rank,Rank,"=== Definition 1 ===
Let $K$ be a field.

Let $\mathbf A$ be an $m \times n$ matrix over $K$.


Then the rank of $\mathbf A$, denoted $\map \rho {\mathbf A}$, is the dimension of the subspace of $K^m$ generated by the columns of $\mathbf A$.


That is, it is the dimension of the column space of $\mathbf A$.

=== Definition 2 ===
Let $K$ be a field.

Let $\mathbf A$ be an $m \times n$ matrix over $K$.

Let $\mathbf A$ be converted to echelon form $\mathbf B$.

Let $\mathbf B$ have exactly $k$ non-zero rows.

Then the rank of $\mathbf A$, denoted $\map \rho {\mathbf A}$, is $k$.

=== Definition 3 ===
Let $K$ be a field.

Let $\mathbf A$ be an $m \times n$ matrix over $K$.

The rank of $\mathbf A$, denoted $\map \rho {\mathbf A}$ is the largest number of elements in a linearly independent set of rows of $\mathbf A$.",Definition:Rank/Matrix,"['Definitions/Matrix Theory', 'Definitions/Rank of Matrix']"
Definition:Rank,Rank,"Let $\phi$ be a linear transformation from one vector space to another.

Let the image of $\phi$ be finite-dimensional.


Then its dimension is called the rank of $\phi$ and is denoted $\map \rho \phi$.",Definition:Rank/Linear Transformation,['Definitions/Linear Algebra']
Definition:Rank,Rank,"Let $f: \C \to \C$ be an entire function.

Let $\sequence {a_n}$ be the sequence of non-zero zeroes of $f$, repeated according to multiplicity.


The rank of $f$ is:
:the smallest positive integer $p \ge 0$ for which the series $\ds \sum_{n \mathop = 1}^\infty \size {a_n}^{-p - 1}$ converges
or:
:$\infty$ if there is no such integer.


If $f$ has finitely many zeroes, its rank is $0$.",Definition:Rank of Entire Function,['Definitions/Entire Functions']
Definition:Rank,Rank,"Let $F$ be a free group.


The rank of $F$ is the dimension of its abelianization as a module over $\Z$.",Definition:Rank of Free Group,['Definitions/Group Theory']
Definition:Rank,Rank,"Let $S$ be a set of sample data which has been assigned a ranking $\RR$.

The index of a given $x \in S$ according to $\RR$ is known as its rank.",Definition:Rank (Statistics),['Definitions/Rankings']
Definition:Rank,Rank,"A rank correlation coefficient is a statistical coefficient which measures the degree of agreement of a set of subjective rankings.


=== Spearman's Rank Correlation Coefficient ===
Let $X$ and $Y$ be two rankings assigned to the same set of entities.

The Spearman's rank correlation coefficient is the Pearson correlation coefficient between $X$ and $Y$.

=== Kendall's Rank Correlation Coefficient ===
Kendall's rank correlation coefficient is a test for consistency of $2$ sets of rankings $\sequence a_n$ and $\sequence b_n$ on a set $S$ of $n$ objects.

The set $R$ of ordered pairs $\tuple {a_i, b_i}$ is assembled:
:$R = \set {\tuple {a_i, b_i}: i \in \set {1, 2, \ldots, n} }$
and ordered according to $\sequence a$.

The number $Q$ of elements of $S$ out of ranking order from $\sequence b$ is counted.

Kendall's rank correlation coefficient is then formed:
:$K = 1 - \dfrac {4 Q} {n \paren {n + 1} }$
which takes values between $-1$ (complete disagreement) and $+1$ (complete agreement).

Complete disagreement happens when $\sequence a_n$ is in reverse order to $\sequence b_n$.

=== Kendall's Coefficient of Concordance ===
Kendall's coefficient of concordance is a test for consistency of more than $2$ sets of rankings.

Let $m$ judges independently award ranks $1$ to $n$ to a set of $n$ competitors.

Let $s_i$ be the sum of the rankings awarded to competitor $i$.

The mean $M$ of the values of $s_i$ is $\dfrac 1 2 m \paren {n + 1}$.

The sum of the squares of the deviations from $M$ is given by:
:$S = \ds \sum_{i \mathop = 1}^n \paren {s_i - M}^2$

and Kendall's coefficient of concordance is given by:
:$W = \dfrac {12 S} {m^2 n \paren {n^2 - 1} }$


=== Kendall's Coefficient of Concordance is between $0$ and $1$ ===
 ",Definition:Rank Correlation Coefficient,"['Definitions/Rank Correlation Coefficients', 'Definitions/Correlation Coefficients', 'Definitions/Coefficients (Statistics)']"
Definition:Rank Function,Rank Function,"Let $\struct {S, \RR}$ be a relational structure.

Let $\struct {T, \prec}$ be a strictly well-ordered set.

Let $\operatorname {rk}: S \to T$ be a mapping such that:
:$\forall x, y \in S: \paren {x \ne y \text { and } \tuple {x, y} \in \RR} \implies \map {\operatorname {rk} } x \prec \map {\operatorname {rk} } y$


$\operatorname {rk}$ is known as a rank function for $\RR$.",Definition:Rank Function for Relation,"['Definitions/Well-Founded Relations', 'Definitions/Relation Theory']"
Definition:Rank Function,Rank Function,"Let $M = \struct {S, \mathscr I}$ be a matroid.


The rank function of $M$ is the mapping $\rho : \powerset S \to \Z$ from the power set of $S$ into the integers defined by:
:$\forall A \subseteq S : \map \rho A = \max \set {\size X : X \subseteq A \land X \in \mathscr I}$
where $\size A$ denotes the cardinality of $A$.",Definition:Rank Function (Matroid),['Definitions/Matroid Theory']
Definition:Rational,Rational,"A number in the form $\dfrac p q$, where both $p$ and $q$ are integers ($q$ non-zero), is called a rational number.

The set of all rational numbers is usually denoted $\Q$.


Thus:
:$\Q = \set {\dfrac p q: p \in \Z, q \in \Z_{\ne 0} }$


=== Formal Definition ===
The field $\struct {\Q, +, \times}$ of rational numbers is the field of quotients of the integral domain $\struct {\Z, +, \times}$ of integers.

This is shown to exist in Existence of Field of Quotients.


In view of Field of Quotients is Unique, we construct the field of quotients of $\Z$, give it a label $\Q$ and call its elements rational numbers.

=== Canonical Form of Rational Number ===
Let $r \in \Q$ be a rational number.

The canonical form of $r$ is the expression $\dfrac p q$, where:
:$r = \dfrac p q: p \in \Z, q \in \Z_{>0}, p \perp q$
where $p \perp q$ denotes that $p$ and $q$ have no common divisor except $1$.


That is, in its canonical form, $r$ is expressed as $\dfrac p q$ where:

:$p$ is an integer
:$q$ is a strictly positive integer
:$p$ and $q$ are coprime.

=== Geometrical Definition ===
The definitions of rational numbers and irrational numbers as specified in   is different from the contemporary definitions:

 
: 
:With these hypotheses, it is proved that there exist straight lines infinite in multitude which are commensurable and incommensurable respectively, some in length only, and others in square also, with an assigned straight line. Let then the assigned straight line be called rational, and those straight lines which are commensurable with it, whether in length and in square or square only, rational, but those which are incommensurable with it irrational.
 ''
 
: 
: And let the square on the assigned straight line be called rational and those areas which are commensurable with it rational, but those which are incommensurable with it irrational, and the straight lines which produce them irrational, that is, in case the areas are squares, the sides themselves, but in case they are any other rectilineal figures, the straight lines on which are described squares equal to them.
 ''
 ",Definition:Rational Number,"['Definitions/Rational Numbers', 'Definitions/Standard Number Fields', 'Definitions/Numbers', 'Definitions/Fields of Quotients', 'Definitions/Field Theory']"
Definition:Rational,Rational,"Let $F$ be a field.

Let $P: F \to F$ and $Q: F \to F$ be polynomial functions on $F$.

Let $S$ be the set $F$ from which all the roots of $Q$ have been removed.

That is:
:$S = F \setminus \set {x \in F: \map Q x = 0}$


Then the equation $y = \dfrac {\map P x} {\map Q x}$ defines a mapping from $S$ to $F$.


Such a mapping is called a rational function.


The concept is usually encountered where the polynomial functions $P$ and $Q$ are either real or complex:


=== Real Domain ===
Let $P: \R \to \R$ and $Q: \R \to \R$ be polynomial functions on the set of real numbers.

Let $S$ be the set $\R$ from which all the roots of $Q$ have been removed.

That is:
:$S = \R \setminus \set {x \in \R: \map Q x = 0}$.


Then the equation $y = \dfrac {\map P x} {\map Q x}$ defines a function from $S$ to $\R$.


Such a function is known as a rational function.

=== Complex Domain ===
Let $P: \C \to \C$ and $Q: \C \to \C$ be polynomial functions on the set of complex numbers.

Let $S$ be the set $\C$ from which all the roots of $Q$ have been removed.

That is:
:$S = \C \setminus \set {z \in \C: \map Q z = 0}$


Then the equation $y = \dfrac {\map P z} {\map Q z}$ defines a function from $S$ to $\C$.


Such a function is a rational (algebraic) function.",Definition:Rational Function,"['Definitions/Rational Functions', 'Definitions/Polynomial Theory', 'Definitions/Analysis']"
Definition:Ray,Ray,"An infinite half-line is a line which terminates at an endpoint at one end, but has no such endpoint at the other.


=== Start Point ===
The endpoint of an infinite half-line is a known as its start point.


Category:Definitions/Lines",Definition:Line/Infinite Half-Line,"['Definitions/Infinite Half-Lines', 'Definitions/Lines']"
Definition:Ray,Ray,"Let $T = \struct {S, \tau}$ be a topological space.


A ray in $T$ is an embedding $R_{>0} \to S$.

Category:Definitions/Topology",Definition:Ray (Topology),['Definitions/Topology']
Definition:Ray,Ray,"Let $\struct {S, \preccurlyeq}$ be a totally ordered set.

Let $\prec$ be the reflexive reduction of $\preccurlyeq$.

Let $a \in S$ be any point in $S$.


=== Open Ray ===
Let $\struct {S, \preccurlyeq}$ be a totally ordered set.

Let $\prec$ be the reflexive reduction of $\preccurlyeq$.

Let $a \in S$ be any point in $S$.


The following sets are called open rays or open half-lines:

:$\set {x \in S: a \prec x}$ (the strict upper closure of $a$), denoted $a^\succ$
:$\set {x \in S: x \prec a}$ (the strict lower closure of $a$), denoted $a^\prec$.

=== Closed Ray ===
Let $\struct {S, \preccurlyeq}$ be a totally ordered set.

Let $a \in S$ be any point in $S$.


The following sets are called closed rays or closed half-lines:

:$\set {x \in S: a \preccurlyeq x}$ (the upper closure of $a$), denoted $a^\succcurlyeq$
:$\set {x \in S: x \preccurlyeq a}$ (the lower closure of $a$), denoted $a^\preccurlyeq$.

=== Upward-Pointing Ray ===
Let $\struct {S, \preccurlyeq}$ be a totally ordered set.

Let $\prec$ be the reflexive reduction of $\preccurlyeq$.

Let $a \in S$ be any point in $S$.


An upward-pointing ray is a ray which is bounded below:

:an open ray $a^\succ:= \set {x \in S: a \prec x}$
:a closed ray $a^\succcurlyeq: \set {x \in S: a \preccurlyeq x}$

=== Downward-Pointing Ray ===
Let $\struct {S, \preccurlyeq}$ be a totally ordered set.

Let $\prec$ be the reflexive reduction of $\preccurlyeq$.

Let $a \in S$ be any point in $S$.


A downward-pointing ray is a ray which is bounded above:

:an open ray $a^\prec := \set {x \in S: x \prec a}$
:a closed ray $a^\preccurlyeq : \set {x \in S: x \preccurlyeq a}$",Definition:Ray (Order Theory),"['Definitions/Order Theory', 'Definitions/Rays (Order Theory)']"
Definition:Realization,Realization,"Let $S$ be a stochastic process.

Let $T$ be a time series of observations of $S$ which has been acquired as $S$ evolves, according to the underlying probability distribution of $S$.


Then $T$ is referred to as a realization of $S$.


Thus we can regard the observation $z_t$ at some timestamp $t$, for example $t = 25$, as the realization of a random variable with probability mass function $\map p {z_t}$.

Similarly the observations $z_{t_1}$ and $z_{t_2}$ at times $t_1$ and $t_2$ can be regarded as the realizations of two random variables with joint probability mass function $\map p {z_{t_1} }$ and $\map p {z_{t_2} }$.


Similarly, the observations making an equispaced time series can be described by an $N$-dimensional random variable $\tuple {z_1, z_2, \dotsc, z_N}$ with associated probability mass function $\map p {z_1, z_2, \dotsc, z_N}$.",Definition:Realization of Stochastic Process,['Definitions/Stochastic Processes']
Definition:Realization,Realization,"Let $\MM$ be an $\LL$-structure.

Let $A$ be a subset of the universe of $\MM$.

Let $\LL_A$ be the language consisting of $\LL$ along with constant symbols for each element of $A$.

Viewing $\MM$ as an $\LL_A$-structure by interpreting each new constant as the element for which it is named, let $\map {\operatorname {Th}_A} \MM$ be the set of $\LL_A$-sentences satisfied by $\MM$.


An $n$-type over $A$ is a set $p$ of $\LL_A$-formulas in $n$ free variables such that $p \cup \map {\operatorname {Th}_A} \MM$ is satisfiable by some $\LL_A$-structure.

 


=== Complete Type ===

We say that an $n$-type $p$ is complete (over $A$)  if and only if :
:for every $\LL_A$-formula $\phi$ in $n$ free variables, either $\phi \in p$ or $\phi \notin p$.


The set of complete $n$-types over $A$ is often denoted by $\map {S_n^\MM} A$.


Given an $n$-tuple $\bar b$ of elements from $\MM$, the type of $\bar b$ over $A$ is the complete $n$-type consisting of those $\LL_A$-formulas $\map \phi {x_1, \dotsc, x_n}$ such that $\MM \models \map \phi {\bar b}$.

It is often denoted by $\map {\operatorname {tp}^\MM} {\bar b / A}$.


=== Realization ===

Given an $\LL_A$-structure $\NN$, a type $p$ is realized by an element $\bar b$ of $\NN$  if and only if :
:$\forall \phi \in p: \NN \models \map \phi {\bar b}$.

Such an element $\bar b$ of $\NN$ is a realization of $p$.


=== Omission ===

We say that $\NN$ omits $p$  if and only if  $p$ is not realized in $\NN$.

Then $p$ is an omission from $\NN$.",Definition:Type,['Definitions/Model Theory for Predicate Logic']
Definition:Reducible,Reducible,"Let $q = \dfrac a b$ be a vulgar fraction.

Then $q$ is defined as being reducible  if and only if  $q$ is not in canonical form.

That is,  if and only if  there exists $r \in \Z: r \ne 1$ such that $r$ is a divisor of both $a$ and $b$.

Such a fraction can therefore be reduced by dividing both $a$ and $b$ by $r$.",Definition:Reducible Fraction,"['Definitions/Reducible Fractions', 'Definitions/Vulgar Fractions', 'Definitions/Fractions']"
Definition:Reducible,Reducible,"=== Definition 1 ===
Let $K$ be a field.


A reducible polynomial over $K$ is a nonconstant polynomial over $K$ that can be expressed as the product of two polynomials over $K$ of smaller degree.

=== Definition 2 ===
Let $K$ be a field.


A reducible polynomial over $K$ is a polynomial over $K$ that can be expressed as the product of two nonconstant polynomials.",Definition:Reducible Polynomial,"['Definitions/Reducible Polynomials', 'Definitions/Factorization', 'Definitions/Polynomial Theory']"
Definition:Reducible,Reducible,"Let $\rho: G \to \GL V$ be a linear representation.


$\rho$ is reducible  if and only if  there exists a non-trivial proper vector subspace $W$ of $V$ such that:
:$\forall g \in G: \map {\map \rho g} W \subseteq W$


That is, such that $W$ is invariant for every linear operator in the set $\set {\map \rho g: g \in G}$.",Definition:Reducible Linear Representation,['Definitions/Representation Theory']
Definition:Reducible,Reducible,"Let $M$ be a $G$-module.

Then $M$ is reducible  if and only if  the corresponding linear representation is reducible.",Definition:Reducible G-Module,['Definitions/Representation Theory']
Definition:Reducible,Reducible,"Let $\Sigma, \Sigma'$ be finite sets.

Let:

 
 
 
 

be sets of finite strings over $\Sigma$ and $\Sigma'$ respectively, where:
:$\Sigma^*$ denotes the set of all finite strings over the alphabet $\Sigma$.

Let $f : \Sigma^* \to \Sigma'^*$ be a computable function such that, for all $w \in \Sigma^*$:
:$w \in L \iff \map f w \in L'$

Then, $f$ is a mapping reduction from $L$ to $L'$.


If any such $f$ exists, then $L$ is mapping reducible to $L'$, which is denoted as:
:$L \le_m L'$",Definition:Mapping Reduction,"['Definitions/Mapping Reductions', 'Definitions/Turing Machines']"
Definition:Regular,Regular,"Let $\kappa$ be an infinite cardinal.


Then $\kappa$ is a regular cardinal  if and only if :
:$\map {\mathrm {cf} } \kappa = \kappa$

That is,  if and only if  the cofinality of $\kappa$ is equal to itself.

 ",Definition:Regular Cardinal,['Definitions/Cardinals']
Definition:Regular,Regular,"Let $U \subset \C$ be an open set.

Let $f : U \to \C$ be a complex function.


Then $f$ is analytic in $U$  if and only if  for every $z_0 \in U$ there exists a sequence $\sequence {a_n}: \N \to \C$ such that the series:
:$\ds \sum_{n \mathop = 0}^\infty a_n \paren {z - z_0}^n$
converges to $\map f z$ in a neighborhood of $z_0$ in $U$.",Definition:Analytic Function/Complex Plane,['Definitions/Analytic Complex Functions']
Definition:Regular,Regular,"Let $\struct {R, +, \circ}$ be a ring with unity.

Let $n \in \Z_{>0}$ be a (strictly) positive integer.

Let $\mathbf A$ be an element of the ring of square matrices $\struct {\map {\MM_R} n, +, \times}$.


Then $\mathbf A$ is invertible  if and only if :
:$\exists \mathbf B \in \struct {\map {\MM_R} n, +, \times}: \mathbf A \mathbf B = \mathbf I_n = \mathbf B \mathbf A$
where $\mathbf I_n$ denotes the unit matrix of order $n$.


Such a $\mathbf B$ is the inverse of $\mathbf A$.

It is usually denoted $\mathbf A^{-1}$.",Definition:Invertible Matrix,"['Definitions/Inverse Matrices', 'Definitions/Matrices']"
Definition:Regular,Regular,"Let $\struct {S, \circ}$ be a magma.


=== Left Regular Representation ===
Let $\struct {S, \circ}$ be a magma.

The mapping $\lambda_a: S \to S$ is defined as:

:$\forall x \in S: \map {\lambda_a} x = a \circ x$


This is known as the left regular representation of $\struct {S, \circ}$ with respect to $a$.

=== Right Regular Representation ===
Let $\struct {S, \circ}$ be a magma.

The mapping $\rho_a: S \to S$ is defined as:

:$\forall x \in S: \map {\rho_a} x = x \circ a$


This is known as the right regular representation of $\struct {S, \circ}$ with respect to $a$.",Definition:Regular Representations,"['Definitions/Abstract Algebra', 'Definitions/Group Theory', 'Definitions/Regular Representations']"
Definition:Regular,Regular,"Let $\struct {S, \circ}$ be a magma.

The mapping $\lambda_a: S \to S$ is defined as:

:$\forall x \in S: \map {\lambda_a} x = a \circ x$


This is known as the left regular representation of $\struct {S, \circ}$ with respect to $a$.",Definition:Regular Representations/Left Regular Representation,"['Definitions/Left Regular Representation', 'Definitions/Regular Representations']"
Definition:Regular,Regular,"Let $\struct {S, \circ}$ be a magma.

The mapping $\rho_a: S \to S$ is defined as:

:$\forall x \in S: \map {\rho_a} x = x \circ a$


This is known as the right regular representation of $\struct {S, \circ}$ with respect to $a$.",Definition:Regular Representations/Right Regular Representation,"['Definitions/Right Regular Representation', 'Definitions/Regular Representations']"
Definition:Regular,Regular,"Let $A$ be a commutative ring with unity.

Let $a \in A$.


Then $a$ is regular  if and only if  it is not a zero divisor.",Definition:Regular Element of Ring,['Definitions/Ring Theory']
Definition:Regular,Regular,"A regular polygon is a polygon which is both equilateral and equiangular.

That is, in which all the sides are the same length, and all the vertices have the same angle:

:


=== Center ===
The center of a regular polygon $P$ is defined as the point which is the center of the circumcircle of $P$.

:300px

In the above, $O$ is the center of the regular polygon.

=== Long Radius ===
The long radius of a regular polygon $P$ is defined as the distance from the center of $P$ to one of its vertices.

:

In the above, the length of $OA$ is the long radius of the regular polygon.

=== Apothem ===
The apothem of a regular polygon $P$ is defined as the perpendicular distance from the center of $P$ to one of its sides.

:

In the above, the length of $OM$ is the apothem of the regular polygon.",Definition:Polygon/Regular,"['Definitions/Polygons', 'Definitions/Regular Polygons']"
Definition:Regular,Regular,"A regular pentagon is a pentagon which is both equilateral and equiangular.

That is, a regular polygon with $5$ sides.

That is, a pentagon in which all the sides are the same length, and all the vertices have the same angle:

:",Definition:Pentagon/Regular,['Definitions/Regular Polygons']
Definition:Regular,Regular,"A regular hexagon is a hexagon which is both equilateral and equiangular.

That is, a regular polygon with $6$ sides.

That is, a hexagon in which all the sides are the same length, and all the vertices have the same angle:

:",Definition:Hexagon/Regular,['Definitions/Regular Polygons']
Definition:Regular,Regular,"A regular polyhedron is a polyhedron:
:$(1): \quad$ whose faces are congruent regular polygons
:$(2): \quad$ each of whose vertices is the common vertex of the same number of faces.",Definition:Regular Polyhedron,['Definitions/Polyhedra']
Definition:Regular,Regular,"Let $T$ be a topological space.

Let $A \subseteq T$.


Then $A$ is regular open in $T$  if and only if :
:$A = A^{- \circ}$

That is,  if and only if  $A$ equals the interior of its closure.",Definition:Regular Open Set,['Definitions/Topology']
Definition:Regular,Regular,"Let $T$ be a topological space.

Let $A \subseteq T$.


Then $A$ is regular closed in $T$  if and only if :
:$A = A^{\circ -}$

That is,  if and only if  $A$ equals the closure of its interior.",Definition:Regular Closed Set,['Definitions/Topology']
Definition:Regular,Regular,"Let $X$ and $Y$ be smooth manifolds.

Let $f: X \to Y$ be a smooth mapping.


Then a point $y \in Y$ is called a regular value of $f$  if and only if  the pushforward of $f$ at $x$:
: $f_* \vert_x: T_x X \to T_y Y$
 
is surjective for every $x \in \map {f^{-1} } y \subseteq X$.",Definition:Regular Value,['Definitions/Topology']
Definition:Regular,Regular,"Let $T = \struct {S, \tau}$ be a topological space.


$\struct {S, \tau}$ is a regular space  if and only if :
:$\struct {S, \tau}$ is a $T_3$ space
:$\struct {S, \tau}$ is a $T_0$ (Kolmogorov) space.


That is:
:$\forall F \subseteq S: \relcomp S F \in \tau, y \in \relcomp S F: \exists U, V \in \tau: F \subseteq U, y \in V: U \cap V = \O$ 

:$\forall x, y \in S$, either:
::$\exists U \in \tau: x \in U, y \notin U$
::$\exists U \in \tau: y \in U, x \notin U$

 ",Definition:Regular Space,"['Definitions/Regular Spaces', 'Definitions/T3 Spaces', 'Definitions/T0 Spaces', 'Definitions/Separation Axioms']"
Definition:Regular,Regular,"Let $T = \struct {S, \tau}$ be a topological space.


$\struct {S, \tau}$ is a Tychonoff Space or completely regular space  if and only if :
:$\struct {S, \tau}$ is a $T_{3 \frac 1 2}$ space
:$\struct {S, \tau}$ is a $T_0$ (Kolmogorov) space.


That is:

:For any closed set $F \subseteq S$ and any point $y \in S$ such that $y \notin F$, there exists an Urysohn function for $F$ and $\set y$.

:$\forall x, y \in S$, either:
::$\exists U \in \tau: x \in U, y \notin U$
::$\exists U \in \tau: y \in U, x \notin U$

 ",Definition:Tychonoff Space,"['Definitions/Tychonoff Spaces', 'Definitions/Separation Axioms']"
Definition:Regular,Regular,"Let $G = \struct {V, E}$ be an simple graph whose vertices all have the same degree $r$.

Then $G$ is called regular of degree $r$, or $r$-regular.",Definition:Regular Graph,"['Definitions/Regular Graphs', 'Definitions/Graph Theory']"
Definition:Regular,Regular,"A regular expression is an algebraic structure on an alphabet $\Sigma$ defined as follows:

* The empty-set regular expression, $\O$, is a regular expression.

* The empty-word regular expression, $\epsilon$, is a regular expression.

* Every $\sigma \in \Sigma$ is a regular expression. (These are called literals.)

* If $R_1$ and $R_2$ are regular expressions, $R_1 R_2$ is a regular expression (concatenation).

* If $R_1$ and $R_2$ are regular expressions, $R_1 \mid R_2$ is a regular expression (alternation).

* If $R$ is a regular expression, $R^*$ is a regular expression (Kleene star).


Every regular expression $R$ on an alphabet $\Sigma$ defines a language $\map L R \subseteq \Sigma^*$, where $\Sigma^*$ is the set of all (finite-length) words (sequences) of symbols in $\Sigma$:

* $\map L \O = \O$ (the empty set).

* $\map L \epsilon = \set {\sqbrk \,}$ (the set containing only the empty word).

* If $R$ is a literal $\sigma$, $\map L R = \set {\sqbrk \sigma}$ (i.e., the set containing only the single-symbol word $\sigma$).

* If $R$ is a concatenation $R_1 R_2$, $\map L R = \set {w_1 w_2: w_1 \in \map L {R_1}, w_2 \in \map L {R_2} }$, where $w_1 w_2$ is the concatenation of the words $w_1$ and $w_2$.

* If $R$ is an alternation $R_1 \mid R_2$, $\map L R = L {R_1} \cup \map L {R_2}$.

* If $R$ is a Kleene star $R_1^*$, $\map L R$ is the smallest set satisfying the following:
** $\sqbrk \, \in \map L R$ (the empty word is in the set);
** if $w_1 \in \map L R$ and $w_2 \in \map L {R_1}$, then $w_1 w_2 \in \map L R$.

Category:Definitions/Formal Systems",Definition:Regular Expression,['Definitions/Formal Systems']
Definition:Representation,Representation,"Let $G$ be a group.

Let $X$ be a set.

Let $\struct {\map \Gamma X, \circ}$ be the symmetric group on $X$.


A permutation representation of $G$ is a group homomorphism from $G$ to $\struct {\map \Gamma X, \circ}$.


=== Associated to Group Action ===
Let $G$ be a group.

Let $X$ be a set.

Let $\struct {\map \Gamma X, \circ}$ be the symmetric group on $X$.


Let $\phi: G \times X \to X$ be a group action.

Define for $g \in G$ the mapping $\phi_g : X \to X$ by:
:$\map {\phi_g} x = \map \phi {g, x}$


The permutation representation of $G$ associated to the group action is the group homomorphism $G \to \struct {\map \Gamma X, \circ}$ which sends $g$ to $\phi_g$.",Definition:Permutation Representation,['Definitions/Group Actions']
Definition:Representation,Representation,"=== Groups ===
Let $\struct {K, +, \circ}$ be a field.

Let $V$ be a vector space over $K$ of finite dimension.

Let $\GL V$ be the general linear group of $V$.

Let $\struct {G, \cdot}$ be a finite group.


A linear representation of $G$ on $V$ is a group homomorphism $\rho: G \to \GL V$.


=== Module associated to representation ===

Let $K \sqbrk G$ be the group ring.

Let $\map {\operatorname{End} } V$ be the endomorphism ring of $V$.

Let $K \sqbrk G \to \map {\operatorname{End} } V$ be the ring homomorphism given by $\rho : G \to \GL V$ and the Universal Property of Group Ring.


The $K \sqbrk G$-module induced by the representation is the module induced by this homomorphism.

=== Algebras ===
Let $K$ be a field.

Let $A$ be an associative unitary algebra over $K$.

Then a (linear) representation of $A$ is a vector space $V$ over $K$ equipped with a homomorphism of algebras:

:$\rho: A \to \map {\operatorname {End} } V$

where $\map {\operatorname {End} } V$ is the endomorphism ring of $V$.


Category:Definitions/Representation Theory",Definition:Linear Representation,['Definitions/Representation Theory']
Definition:Representation,Representation,"Let $R$ be a ring.

Let $M$ be an abelian group.


A ring representation of $R$ on $M$ is a ring homomorphism from $R$ to the endomorphism ring $\map {\operatorname {End} } M$.


=== Unital Ring Representation ===
Let $R$ be a ring with unity.

Let $M$ be an abelian group.


A unital ring representation of $R$ on $M$ is a ring representation $R \to \map {\operatorname {End} } M$ which is unital.

That is, it is a unital ring homomorphism from $R$ to the endomorphism ring $\map {\operatorname {End} } M$.",Definition:Ring Representation,['Definitions/Module Theory']
Definition:Representation,Representation,"Let $\mathbf C$ be a locally small category.

Let $\mathbf{Set}$ be the category of sets.

Let $F : \mathbf C \to \mathbf{Set}$ be a covariant functor.


A representation of $F$ is a pair $\tuple {C, \eta}$ where $\eta : \map {\operatorname {Hom} } {C, \cdot} \to F$ is a natural isomorphism with the covariant hom functor of $C$.",Definition:Representation of Functor,['Definitions/Category Theory']
Definition:Residue,Residue,"Let $f: \C \to \C$ be a complex function.

Let $z_0 \in U \subset \C$ such that $f$ is analytic in $U \setminus \set {z_0}$.


Then by Existence of Laurent Series, there is a Laurent series:
:$\ds \sum_{j \mathop = -\infty}^\infty a_j \paren {z - z_0}^j$
such that the sum converges to $f$ in $U - \set {z_0}$.  


The residue at a point $z = z_0$ of $f$ is defined as $a_{-1}$ in that Laurent series.

It is denoted $\Res f {z_0}$ or just $\map {\mathrm {Res} } {z_0}$ when $f$ is understood.",Definition:Residue (Complex Analysis),['Definitions/Complex Analysis']
Definition:Residue,Residue,"Let $m, n \in \N$ be natural numbers.

Let $a \in \Z$ be an integer such that $a$ is not divisible by $m$.

Then $a$ is a residue of $m$ of order $n$  if and only if :
:$\exists x \in \Z: x^n \equiv a \pmod m$
where $\equiv$ denotes modulo congruence.


=== Nonresidue ===
Let $m, n \in \N$ be natural numbers.

Let $a \in \Z$ be an integer such that $a$ is not divisible by $m$.


$a$ is a nonresidue of $m$ of order $n$  if and only if  there does not exist $x \in \Z$ such that:
:$x^n \equiv a \pmod m$

where $\equiv$ denotes modulo congruence.",Definition:Residue (Number Theory),"['Definitions/Residues (Number Theory)', 'Definitions/Number Theory']"
Definition:Residue,Residue,"Let $m \in \Z_{\ne 0}$ be a non-zero integer.

Let $a, b \in \Z$.

Let $a \equiv b \pmod m$.


Then $b$ is a residue of $a$ modulo $m$.

Residue is another word for remainder, and is any integer congruent to $a$ modulo $m$.",Definition:Congruence (Number Theory)/Residue,"['Definitions/Congruence (Number Theory)', 'Definitions/Residue Classes']"
Definition:Residue,Residue,"Let $R$ be a commutative local ring.

Let $m$ be its maximal ideal.


The residue field of $R$ is the quotient ring $R / m$.",Definition:Residue Field of Local Ring,['Definitions/Local Rings']
Definition:Resultant,Resultant,"Let $\mathbf u$ and $\mathbf v$ be vector quantities of the same physical property.


=== Component Definition ===
Let $\mathbf u$ and $\mathbf v$ be vector quantities of the same physical property.


Let $\mathbf u$ and $\mathbf v$ be represented by their components considered to be embedded in a real $n$-space:

 
 
 
 


Then the (vector) sum of $\mathbf u$ and $\mathbf v$ is defined as:
:$\mathbf u + \mathbf v := \tuple {u_1 + v_1, u_2 + v_2, \ldots, u_n + v_n}$


Note that the $+$ on the   is conventional addition of numbers, while the $+$ on the   takes on a different meaning.

The distinction is implied by which operands are involved.

=== Triangle Law ===
Let $\mathbf u$ and $\mathbf v$ be vector quantities of the same physical property.

Let $\mathbf u$ and $\mathbf v$ be represented by arrows embedded in the plane such that:

:$\mathbf u$ is represented by $\vec {AB}$
:$\mathbf v$ is represented by $\vec {BC}$

that is, so that the initial point of $\mathbf v$ is identified with the terminal point of $\mathbf u$.

:

Then their (vector) sum $\mathbf u + \mathbf v$ is represented by the arrow $\vec {AC}$.",Definition:Vector Sum,"['Definitions/Vector Addition', 'Definitions/Addition', 'Definitions/Vectors']"
Definition:Resultant,Resultant,"Let $S$ be a system of simultaneous linear equations.

The eliminant of $S$ is the determinant formed by removing the variables between the equations.",Definition:Eliminant,"['Definitions/Eliminants', 'Definitions/Simultaneous Linear Equations', 'Definitions/Linear Algebra']"
Definition:Right,Right,"The direction right is that way:
:$\to$",Definition:Right (Direction),['Definitions/Language Definitions']
Definition:Right,Right,"A right angle is an angle that is equal to half of a straight angle.


=== Measurement of Right Angle ===
 ",Definition:Right Angle,"['Definitions/Right Angles', 'Definitions/Angles']"
Definition:Right,Right,"In an equation:
:$\text {Expression $1$} = \text {Expression $2$}$
the term $\text {Expression $2$}$ is the right hand side.",Definition:Right Hand Side,['Definitions/Language Definitions']
Definition:Right,Right,"Let $\RR \subseteq S \times T$ be a relation.


Then $\RR$ is right-total  if and only if :
:$\forall t \in T: \exists s \in S: \tuple {s, t} \in \RR$


That is,  if and only if  every element of $T$ is related to by some element of $S$.


That is,  if and only if :
:$\Img \RR = T$
where $\Img \RR$ denotes the image of $\RR$.",Definition:Right-Total Relation,['Definitions/Relation Theory']
Definition:Right,Right,"Let $\RR \subseteq S \times S$ be a relation in $S$.


=== Definition 1 ===
Let $\RR \subseteq S \times S$ be a relation in $S$.


$\RR$ is right quasi-reflexive  if and only if :

:$\forall x, y \in S: \tuple {x, y} \in \RR \implies \tuple {y, y} \in \RR$

=== Definition 2 ===
Let $\RR \subseteq S \times S$ be a relation in $S$.


$\RR$ is right quasi-reflexive  if and only if :

:$\forall y \in \Img \RR: \tuple {y, y} \in \RR$

where $\Img \RR$ denotes the image set of $\RR$.",Definition:Right Quasi-Reflexive Relation,"['Definitions/Reflexive Relations', 'Definitions/Quasi-Reflexive Relations', 'Definitions/Right Quasi-Reflexive Relations']"
Definition:Right,Right,"Let $\RR \subseteq S \times S$ be a relation in $S$.


$\RR$ is right-Euclidean  if and only if :

:$\tuple {x, y} \in \RR \land \tuple {x, z} \in \RR \implies \tuple {y, z} \in \RR$",Definition:Euclidean Relation/Right-Euclidean,['Definitions/Euclidean Relations']
Definition:Right,Right,"Let $A$ be a class.

Let $\RR$ be a relation on $A$.


An element $x$ of $A$ is right normal with respect to $\RR$  if and only if :
:$\forall y \in A: \map \RR {y, x}$ holds.",Definition:Right Normal Element of Relation,['Definitions/Relations']
Definition:Right,Right,"A mapping $f: X \to Y$ is right cancellable (or right-cancellable)  if and only if :

:$\forall Z: \forall \paren {h_1, h_2: Y \to Z}: h_1 \circ f = h_2 \circ f \implies h_1 = h_2$

That is,  if and only if  for any set $Z$:
:If $h_1$ and $h_2$ are mappings from $Y$ to $Z$
:then $h_1 \circ f = h_2 \circ f$ implies $h_1 = h_2$.",Definition:Right Cancellable Mapping,"['Definitions/Mapping Theory', 'Definitions/Cancellability']"
Definition:Right,Right,"Let $S, T$ be sets where $S \ne \O$, that is, $S$ is not empty.

Let $f: S \to T$ be a mapping.


Let $g: T \to S$ be a mapping such that:
:$f \circ g = I_T$
where:
:$f \circ g$ denotes the composite mapping $g$ followed by $f$
:$I_T$ is the identity mapping on $T$.


Then $g: T \to S$ is called a right inverse (mapping) of $f$.",Definition:Right Inverse Mapping,['Definitions/Inverse Mappings']
Definition:Right,Right,"Let $\struct {S, \preccurlyeq}$ be an ordered set.

Let $a, b \in S$.


The right half-open interval between $a$ and $b$ is the set:

:$\hointr a b := a^\succcurlyeq \cap b^\prec = \set {s \in S: \paren {a \preccurlyeq s} \land \paren {s \prec b} }$

where:
:$a^\succcurlyeq$ denotes the upper closure of $a$
:$b^\prec$ denotes the strict lower closure of $b$.",Definition:Interval/Ordered Set/Right Half-Open,['Definitions/Intervals']
Definition:Right,Right,"A Cartesian plane is defined as being right-handed if it has the following property:

Let a right hand be placed, with palm uppermost, such that the thumb points along the $x$-axis in the positive direction, such that the thumb and index finger are at right-angles to each other.

Then the index finger is pointed along the $y$-axis in the positive direction.


:",Definition:Orientation of Coordinate Axes/Cartesian Plane/Right-Handed,['Definitions/Orientation (Coordinate Axes)']
Definition:Right,Right,"A Cartesian $3$-Space is defined as being right-handed if it has the following property:

Let a right hand be placed such that:
:the thumb and index finger are at right-angles to each other
:the $3$rd finger is at right-angles to the thumb and index finger, upwards from the palm
:the thumb points along the $x$-axis in the positive direction
:the index finger points along the $y$-axis in the positive direction.

Then the $3$rd finger is pointed along the $z$-axis in the positive direction.


:",Definition:Orientation of Coordinate Axes/Cartesian 3-Space/Right-Handed,"['Definitions/Orientation (Coordinate Axes)', 'Definitions/Right-Hand Rule']"
Definition:Right,Right,"Let $B$ be a Banach space over the set of real numbers $\R$.

Let $f: \R \to B$ be a mapping from $\R$ to $B$.


The right-hand derivative of $f$ is defined as the right-hand limit:
:$\ds \map {f'_+} x = \lim_{h \mathop \to 0^+} \frac {\map f {x + h} - \map f x} h$

If the right-hand derivative exists, then $f$ is said to be right-hand differentiable at $x$.


=== Real Functions ===
Let $f: \R \to \R$ be a real function.


The right-hand derivative of $f$ is defined as the right-hand limit:
:$\ds \map {f'_+} x = \lim_{h \mathop \to 0^+} \frac {\map f {x + h} - \map f x} h$

If the right-hand derivative exists, then $f$ is said to be right-hand differentiable at $x$.",Definition:Right-Hand Derivative,['Definitions/Derivatives']
Definition:Right,Right,"Let $\Bbb I = \openint a b$ be an open real interval.

Let $f: \Bbb I \to \R$ be a real function.

Let $L \in \R$.


Suppose that:
:$\forall \epsilon \in \R_{>0}: \exists \delta \in \R_{>0}: \forall x \in \Bbb I: a < x < a + \delta \implies \size {\map f x - L} < \epsilon$
where $\R_{>0}$ denotes the set of strictly positive real numbers.

That is, for every real strictly positive $\epsilon$ there exists a real strictly positive $\delta$ such that every real number in the domain of $f$, greater than $a$ but within $\delta$ of $a$, has an image within $\epsilon$ of $L$.


:

Then $\map f x$ is said to tend to the limit $L$ as $x$ tends to $a$ from the right, and we write:
:$\map f x \to L$ as $x \to a^+$
or
:$\ds \lim_{x \mathop \to a^+} \map f x = L$


This is voiced
:the limit of $\map f x$ as $x$ tends to $a$ from the right
and such an $L$ is called:
:a limit from the right.",Definition:Limit of Real Function/Right,['Definitions/Limits of Real Functions']
Definition:Right,Right,"Let $V$ be a vector space over the real numbers $\R$.

Let $f: \R \to V$ be a function.


A right difference quotient is an expression of the form:
:$\dfrac {\map f {x + h} - \map f x} h$
where $h > 0$ is a strictly positive real number.",Definition:Difference Quotient/Right,['Definitions/Difference Quotients']
Definition:Right,Right,"Let $S \subseteq \R$ be an open subset of the real numbers $\R$.

Let $f: S \to \R$ be a real function.


Let $x_0 \in S$. 

Then $f$ is said to be right-continuous at $x_0$  if and only if  the limit from the right of $\map f x$ as $x \to x_0$ exists and:

:$\ds \lim_{\substack {x \mathop \to x_0^+ \\ x_0 \mathop \in A}} \map f x = \map f {x_0}$

where $\ds \lim_{x \mathop \to x_0^+}$ is a limit from the right.


Furthermore, $f$ is said to be right-continuous  if and only if :

:$\forall x_0 \in S$, $f$ is right-continuous at $x_0$",Definition:Continuous Real Function/Right-Continuous,['Definitions/Continuous Real Functions']
Definition:Right,Right,"Let $f: \R \to \R$ be a real function.


The right-hand derivative of $f$ is defined as the right-hand limit:
:$\ds \map {f'_+} x = \lim_{h \mathop \to 0^+} \frac {\map f {x + h} - \map f x} h$

If the right-hand derivative exists, then $f$ is said to be right-hand differentiable at $x$.",Definition:Right-Hand Derivative/Real Function,['Definitions/Derivatives']
Definition:Right,Right,"Let $a, b \in \R$ be real numbers.


The right half-open (real) interval from $a$ to $b$ is the subset:
:$\hointr a b := \set {x \in \R: a \le x < b}$",Definition:Real Interval/Half-Open/Right,['Definitions/Real Intervals']
Definition:Right,Right,"There are two unbounded closed intervals involving a real number $a \in \R$, defined as:

 
 
 
 ",Definition:Real Interval/Unbounded Closed,['Definitions/Real Intervals']
Definition:Right,Right,"There are two unbounded open intervals involving a real number $a \in \R$, defined as:

 
 
 
 ",Definition:Real Interval/Unbounded Open,['Definitions/Real Intervals']
Definition:Right,Right,"Let $\struct {S, \circ}$ be an algebraic structure.

An element $z_R \in S$ is called a right zero element (or just right zero)  if and only if :
:$\forall x \in S: x \circ z_R = z_R$",Definition:Right Zero,['Definitions/Zero Elements']
Definition:Right,Right,"Let $\struct {S, \circ}$ be an algebraic structure.

An element $e_R \in S$ is called a right identity (element)  if and only if :
:$\forall x \in S: x \circ e_R = x$",Definition:Identity (Abstract Algebra)/Right Identity,['Definitions/Identity Elements']
Definition:Right,Right,"Let $S$ be a set.

For any $x, y \in S$, the right operation on $S$ is the binary operation defined as:
:$\forall x, y \in S: x \to y = y$",Definition:Right Operation,"['Definitions/Abstract Algebra', 'Definitions/Right Operation']"
Definition:Right,Right,"Let $\struct {S, \circ}$ be an algebraic structure.


An element $x \in \struct {S, \circ}$ is right cancellable  if and only if :

:$\forall a, b \in S: a \circ x = b \circ x \implies a = b$",Definition:Cancellable Element/Right Cancellable,['Definitions/Cancellability']
Definition:Right,Right,"Let $\struct {S, \circ}$ be an algebraic structure.


The operation $\circ$ in $\struct {S, \circ}$ is right cancellable  if and only if :
:$\forall a, b, c \in S: a \circ c = b \circ c \implies a = b$

That is,  if and only if  all elements of $\struct {S, \circ}$ are right cancellable.",Definition:Right Cancellable Operation,['Definitions/Cancellability']
Definition:Right,Right,"Let $S$ be a set on which is defined two binary operations, defined on all the elements of $S \times S$, denoted here as $\circ$ and $*$.

The operation $\circ$ is right distributive over the operation $*$  if and only if :

:$\forall a, b, c \in S: \paren {a * b} \circ c = \paren {a \circ c} * \paren {b \circ c}$",Definition:Distributive Operation/Right,['Definitions/Distributive Operations']
Definition:Right,Right,"Let $\struct {S, \circ}$ be a monoid whose identity is $e_S$.

An element $x_R \in S$ is called a right inverse of $x$  if and only if :
:$x \circ x_R = e_S$",Definition:Inverse (Abstract Algebra)/Right Inverse,['Definitions/Inverse Elements']
Definition:Right,Right,"Let $\struct {S, \circ}$ be a magma. 


$\struct {S, \circ}$ is a right quasigroup  if and only if :
:for all $a \in S$, the right regular representation $\rho_a$ is a permutation on $S$.

That is:
:$\forall a, b \in S: \exists ! x \in S: x \circ a = b$",Definition:Quasigroup/Right Quasigroup,['Definitions/Quasigroups']
Definition:Right,Right,"Let $\struct {S, \circ}$ be a magma.

The mapping $\rho_a: S \to S$ is defined as:

:$\forall x \in S: \map {\rho_a} x = x \circ a$


This is known as the right regular representation of $\struct {S, \circ}$ with respect to $a$.",Definition:Regular Representations/Right Regular Representation,"['Definitions/Right Regular Representation', 'Definitions/Regular Representations']"
Definition:Right,Right,"Let $x$ and $y$ be elements which are operated on by a given operation $\circ$.

The right-hand product of $x$ by $y$ is the product $x \circ y$.",Definition:Operation/Binary Operation/Product/Right,['Definitions/Operations']
Definition:Right,Right,"Let $\struct {S, \circ, \preceq}$ be a positively totally ordered semigroup.


Then $\struct {S, \circ, \preceq}$ is a right naturally totally ordered semigroup  if and only if :

:$\forall a, b \in S: a \prec b \implies \exists x \in S: b = a \circ x$",Definition:Right Naturally Totally Ordered Semigroup,['Definitions/Naturally Ordered Semigroup']
Definition:Right,Right,"Let $\struct {S, \circ}$ be an algebraic structure.

Let $\struct {H, \circ}$ be a subgroup of $\struct {S, \circ}$.


The right coset of $y$ modulo $H$, or right coset of $H$ by $y$, is:

:$H \circ y = \set {x \in S: \exists h \in H: x = h \circ y}$


That is, it is the subset product with singleton:

:$H \circ y = H \circ \set y$",Definition:Coset/Right Coset,['Definitions/Cosets']
Definition:Right,Right,"Let $G$ be a group.

Let $H$ be a subgroup of $G$.


The right coset space (of $G$ modulo $H$) is the quotient set of $G$ by right congruence modulo $H$, denoted $G / H^r$.

It is the set of all the right cosets of $H$ in $G$.",Definition:Coset Space/Right Coset Space,['Definitions/Cosets']
Definition:Right,Right,"Let $G$ be a group.

Let $H$ be a subgroup of $G$.

Let $S \subseteq G$ be a subset of $G$.


$S$ is a right transversal for $H$ in $G$  if and only if  every right coset of $H$ contains exactly one element of $S$.",Definition:Transversal (Group Theory)/Right Transversal,['Definitions/Transversals (Group Theory)']
Definition:Right,Right,"Let $X$ be a set.

Let $\struct {G, \circ}$ be a group whose identity is $e$.


A right group action is a mapping $\phi: X \times G \to X$ such that:

:$\forall \tuple {x, g} \in X \times G : x * g := \map \phi {x, g} \in X$

in such a way that the right group action axioms are satisfied:
 ",Definition:Group Action/Right Group Action,['Definitions/Group Actions']
Definition:Right,Right,"Let $\struct {G, \circ}$ be a group.

Let $\powerset G$ be the power set of $G$.


The (right) subset product action of $G$ is the group action $*: G \times \powerset G \to \powerset G$:
:$\forall g \in G, S \in \powerset G: g * S = S \circ g$",Definition:Subset Product Action/Right,['Definitions/Subset Product Action']
Definition:Right,Right,"Let $G$ be a group.

Let $H$ be a subgroup of $G$.


We can use $H$ to define a relation on $G$ as follows:

:$\RR^r_H = \set {\tuple {x, y} \in G \times G: x y^{-1} \in H}$

This is called right congruence modulo $H$.",Definition:Congruence Modulo Subgroup/Right Congruence,['Definitions/Congruence Modulo Subgroup']
Definition:Right,Right,"Let $\struct {R, +, \circ}$ be a ring.


A right zero divisor (in $R$) is an element $x \in R$ such that:
: $\exists y \in R^*: y \circ x = 0_R$

where $R^*$ is defined as $R \setminus \set {0_R}$.",Definition:Right Zero Divisor,['Definitions/Zero Divisors']
Definition:Right,Right,"Let $R$ be a ring.

Let $M$ be an abelian group.

Let $\circ : M \times R \to M$ be a mapping from the cartesian product $M \times R$.


$\circ$ is a right linear ring action of $R$ on $M$  if and only if  $\circ$ satisfies the right ring action axioms:
 ",Definition:Linear Ring Action/Right,['Definitions/Linear Ring Actions']
Definition:Right,Right,"Let $\struct {R, +, \circ}$ be a ring.

Let $\struct {J, +}$ be a subgroup of $\struct {R, +}$.


$J$ is a right ideal of $R$  if and only if :
:$\forall j \in J: \forall r \in R: j \circ r \in J$

that is,  if and only if :
:$\forall r \in R: J \circ r \subseteq J$",Definition:Ideal of Ring/Right Ideal,['Definitions/Ideal Theory']
Definition:Right,Right,"Let $R$ be a ring.


A right ideal $J$ of $R$ is a maximal right ideal  if and only if :

:$(1): \quad J \subsetneq R$
:$(2): \quad$ There is no right ideal $K$ of $R$ such that $J \subsetneq K \subsetneq R$.


Category:Definitions/Maximal Ideals of Rings",Definition:Maximal Ideal of Ring/Right,['Definitions/Maximal Ideals of Rings']
Definition:Right,Right,"Let $\struct {R, +_R, \times_R}$ be a ring.

Let $\struct {G, +_G}$ be an abelian group.


A right module over $R$ is an $R$-algebraic structure $\struct {G, +_G, \circ}_R$ with one operation $\circ$, the (right) ring action, which satisfies the right module axioms:
 ",Definition:Right Module,"['Definitions/Right Modules', 'Definitions/Module Theory']"
Definition:Right,Right,"A right-truncatable prime is a prime number which remains prime when any number of digits are removed from the right hand end.


=== Sequence ===
 ",Definition:Right-Truncatable Prime,"['Definitions/Right-Truncatable Primes', 'Definitions/Prime Numbers', 'Definitions/Recreational Mathematics']"
Definition:Right,Right,"Let $R$ be a ring.


The category of right $R$-modules is the category $\mathbf {Mod-R}$ with:

 ",Definition:Category of Right Modules,"['Definitions/Examples of Categories', 'Definitions/Module Theory']"
Definition:Right Cancellable,Right Cancellable,"Let $\struct {S, \circ}$ be an algebraic structure.


An element $x \in \struct {S, \circ}$ is right cancellable  if and only if :

:$\forall a, b \in S: a \circ x = b \circ x \implies a = b$",Definition:Cancellable Element/Right Cancellable,['Definitions/Cancellability']
Definition:Right Cancellable,Right Cancellable,"Let $\struct {S, \circ}$ be an algebraic structure.


The operation $\circ$ in $\struct {S, \circ}$ is right cancellable  if and only if :
:$\forall a, b, c \in S: a \circ c = b \circ c \implies a = b$

That is,  if and only if  all elements of $\struct {S, \circ}$ are right cancellable.",Definition:Right Cancellable Operation,['Definitions/Cancellability']
Definition:Right Cancellable,Right Cancellable,"A mapping $f: X \to Y$ is right cancellable (or right-cancellable)  if and only if :

:$\forall Z: \forall \paren {h_1, h_2: Y \to Z}: h_1 \circ f = h_2 \circ f \implies h_1 = h_2$

That is,  if and only if  for any set $Z$:
:If $h_1$ and $h_2$ are mappings from $Y$ to $Z$
:then $h_1 \circ f = h_2 \circ f$ implies $h_1 = h_2$.",Definition:Right Cancellable Mapping,"['Definitions/Mapping Theory', 'Definitions/Cancellability']"
Definition:Right Inverse,Right Inverse,"Let $S, T$ be sets where $S \ne \O$, that is, $S$ is not empty.

Let $f: S \to T$ be a mapping.


Let $g: T \to S$ be a mapping such that:
:$f \circ g = I_T$
where:
:$f \circ g$ denotes the composite mapping $g$ followed by $f$
:$I_T$ is the identity mapping on $T$.


Then $g: T \to S$ is called a right inverse (mapping) of $f$.",Definition:Right Inverse Mapping,['Definitions/Inverse Mappings']
Definition:Right Inverse,Right Inverse,"Let $\struct {S, \circ}$ be a monoid whose identity is $e_S$.

An element $x_R \in S$ is called a right inverse of $x$  if and only if :
:$x \circ x_R = e_S$",Definition:Inverse (Abstract Algebra)/Right Inverse,['Definitions/Inverse Elements']
Definition:Right Inverse,Right Inverse,"Let $m, n \in \Z_{>0}$ be a (strictly) positive integer.


Let $\mathbf A = \sqbrk a_{m n}$ be a matrix of order $m \times n$.

Let $\mathbf B = \sqbrk b_{n m}$ be a matrix of order $n \times m$ such that:
:$\mathbf A \mathbf B = \mathbf I_m$

where $\mathbf I_m$ denotes the unit matrix of order $m$.


Then $\mathbf B$ is known as a right inverse (matrix) of $\mathbf A$.",Definition:Inverse Matrix/Right,['Definitions/Inverse Matrices']
Definition:Root,Root,"Let $x, y \in \R_{\ge 0}$ be positive real numbers.

Let $n \in \Z$ be an integer such that $n \ne 0$.


Then $y$ is the positive $n$th root of $x$  if and only if :
:$y^n = x$

and we write:
:$y = \sqrt[n] x$


Using the power notation, this can also be written:
:$y = x^{1/n}$


When $n = 2$, we write $y = \sqrt x$ and call $y$ the positive square root of $x$.

When $n = 3$, we write $y = \sqrt [3] x$ and call $y$ the cube root of $x$.


Note the special case where $x = 0 = y$:
:$0 = \sqrt [n] 0$


=== Index ===
Let $\sqrt [n] x$ denote the $n$th root of $x$.

The number $n$ is known as the index of the root.


If $n$ is not specified, that is $\sqrt x$ is presented, this means the square root.

=== Extraction of Root ===
The process of evaluating roots of a given real number is referred to as extraction.",Definition:Root of Number,"['Definitions/Roots of Numbers', 'Definitions/Real Analysis']"
Definition:Root,Root,"Let $\map E x$ be a mathematical expression representing an equation which is dependent upon a variable $x$.

A root of $\map E x$ is a constant which, when substituted for $x$ in $\map E x$, makes $\map E x$ a true statement.


=== Extraction of Root ===
The process of finding roots of a given equation is referred to as extraction.",Definition:Root of Equation,"['Definitions/Roots of Equations', 'Definitions/Roots', 'Definitions/Algebra']"
Definition:Root,Root,"Let $f: R \to R$ be a mapping on a ring $R$.

Let $x \in R$.


Then the values of $x$ for which $\map f x = 0_R$ are known as the roots of the mapping $f$.",Definition:Root of Mapping,"['Definitions/Roots of Mappings', 'Definitions/Ring Theory', 'Definitions/Field Theory', 'Definitions/Real Analysis', 'Definitions/Complex Analysis']"
Definition:Root,Root,"Let $R$ be a commutative ring with unity.

Let $f \in R \sqbrk x$ be a polynomial over $R$.


A root in $R$ of $f$ is an element $x \in R$ for which $\map f x = 0$, where $\map f x$ denotes the image of $f$ under the evaluation homomorphism at $x$.",Definition:Root of Polynomial,"['Definitions/Roots of Polynomials', 'Definitions/Polynomial Theory', 'Definitions/Ring Theory']"
Definition:Root,Root,"Let $T$ be a rooted tree.

The root node of $T$ is the node of $T$ which is distinguished from the others by being the ancestor node of every node of $T$.",Definition:Rooted Tree/Root Node,['Definitions/Root Nodes']
Definition:Root,Root,"A rooted tree is a tree with a countable number of nodes, in which a particular node is distinguished from the others and called the root node:

:

=== Root Node ===
Let $T$ be a rooted tree.

The root node of $T$ is the node of $T$ which is distinguished from the others by being the ancestor node of every node of $T$.

=== Parent ===
Let $T$ be a rooted tree whose root is $r_T$.

Let $t$ be a node of $T$.

From Path in Tree is Unique, there is only one path from $t$ to $r_T$.

Let $\pi: T \setminus \set {r_T} \to T$ be the mapping defined by:

:$\map \pi t := \text {the node adjacent to $t$ on the path to $r_T$}$


Then $\map \pi t$ is known as the parent node of $t$.

The mapping $\pi$ is called the parent mapping.

=== Ancestor ===
Let $T$ be a rooted tree with root $r_T$.

Let $t$ be a node of $T$.


An ancestor node of $t$ is a node in the path from $t$ to $r_T$.


=== Proper Ancestor ===
Let $T$ be a rooted tree with root $r_T$.

Let $t$ be a node of $T$.


A proper ancestor node of $t$ is an ancestor node of $t$ that is not $t$ itself.

=== Child Node ===
Let $T$ be a rooted tree with root $r_T$.

Let $t$ be a node of $T$.

The child nodes of $t$ are the elements of the set:
:$\set {s \in T: \map \pi s = t}$
where $\map \pi s$ denotes the parent mapping of $s$.

That is, the children of $t$ are all the nodes of $T$ of which $t$ is the parent.


=== Grandchild Node ===
Let $T$ be a rooted tree with root $r_T$.

Let $t$ be a node of $T$.


A child of a child node of a node $t$ can be referred to as a grandchild node of $t$.

In terms of the parent mapping $\pi$ of $T$, a grandchild node of $t$ is a node $s$ such that:

:$\map \pi {\map \pi s} = t$


Category:Definitions/Descendant Nodes

=== Descendant ===
Let $T$ be a rooted tree with root $r_T$.

Let $t$ be a node of $T$.


A descendant node $s$ of a $t$ is a node such that $t$ is in the path from $s$ to $r_T$.

That is, the descendant nodes of $t$ are all the nodes of $T$ of which $t$ is an ancestor node.


=== Proper Descendant ===
Let $T$ be a rooted tree with root $r_T$.

Let $t$ be a node of $T$.


A proper descendant node of $t$ is a descendant of $t$ which is not $t$ itself.


Category:Definitions/Descendant Nodes

=== Sibling ===
Let $T$ be a rooted tree with root $r_T$.

Two children of the same node of $T$ are called siblings.

That is, siblings are nodes which both have the same parent.


Category:Definitions/Rooted Trees

=== Leaf Node ===
Let $v$ be a node of a tree $T$.

Then $v$ is a leaf node of a $T$  if and only if  $v$ is of degree $1$.


If $T$ is a rooted tree, this is equivalent to saying that $v$ has no child nodes.

=== Branch ===
Let $T$ be a rooted tree with root node $r_T$.

A subset $\Gamma$ of $T$ is a branch  if and only if  all the following conditions hold:
:$(1): \quad$ The root node $r_T$ belongs to $\Gamma$
:$(2): \quad$ The parent of each node in $\Gamma \setminus \set {r_T}$ is in $\Gamma$
:$(3): \quad$ Each node in $\Gamma$ either:
::$\text {(a)}: \quad$ is a leaf node of $T$
:or:
::$\text {(b)}: \quad$ has exactly one child node in $\Gamma$.


Informally, a branch of a rooted tree is the path from the root to a leaf.


=== Finite ===
Let $T$ be a rooted tree with root node $r_T$.

Let $\Gamma$ be a branch of $T$.


Then $\Gamma$ is finite  if and only if  it has exactly one leaf node.

=== Infinite ===
Let $T$ be a rooted tree with root node $r_T$.

Let $\Gamma$ be a branch of $T$.


Then $\Gamma$ is infinite  if and only if  it has no leaf node at the end.",Definition:Rooted Tree,"['Definitions/Rooted Trees', 'Definitions/Graph Theory', 'Definitions/Tree Theory']"
Definition:Root,Root,"Let $\left({T, \mathbf H, \Phi}\right)$ be a labeled tree for propositional logic.


The countable set $\mathbf H$ of WFFs of propositional logic is called the hypothesis set.

The elements of $\mathbf H$ are known as hypothesis WFFs.


The hypothesis set $\mathbf H$ is considered to be attached to the root node of $T$.",Definition:Labeled Tree for Propositional Logic/Hypothesis Set,['Definitions/Propositional Tableaus']
Definition:Rotation,Rotation,"A rotation in the context of Euclidean geometry is an isometry on a Euclidean Space $\R^n$ as follows.

A rotation is defined usually for either:
:$n = 2$, representing the plane
or:
:$n = 3$, representing ordinary space.


=== Rotation in the Plane ===
A rotation $r_\alpha$ in the plane is an isometry on the Euclidean Space $\Gamma = \R^2$ as follows.


Let $O$ be a distinguished point in $\Gamma$, which has the property that:
:$\map {r_\alpha} O = O$

That is, $O$ maps to itself.


Let $P \in \Gamma$ such that $P \ne O$.

Let $OP$ be joined by a straight line.

Let a straight line $OP'$ be constructed such that:
:$(1): \quad OP' = OP$
:$(2): \angle POP' = \alpha$ such that $OP \to OP'$ is in the anticlockwise direction:

:


Then:
:$\map {r_\alpha} P = P'$

Thus $r_\alpha$ is a rotation (in the plane) of (angle) $\alpha$ about (the axis) $O$.

=== Rotation in Space ===
A rotation $r_\theta$ in space is an isometry on the Euclidean Space $\Gamma = \R^3$ as follows.


Let $AB$ be a distinguished straight line in $\Gamma$, which has the property that:
:$\forall P \in AB: \map {r_\theta} P = P$

That is, all points on $AB$ map to themselves.


Let $P \in \Gamma$ such that $P \notin AB$.

Let a straight line be constructed from $P$ to $O$ on $AB$ such that $OP$ is perpendicular to $AB$.

Let a straight line $OP'$ be constructed perpendicular to $AB$ such that:
:$(1): \quad OP' = OP$
:$(2): \quad \angle POP' = \theta$ such that $OP \to OP'$ is in the anticlockwise direction:


:


Then:
:$\map {r_\theta} P = P'$

Thus $r_\theta$ is a rotation (in space) of (angle) $\theta$ about (the axis) $O$.


 

=== Axis of Rotation ===
Let $r_\theta$ be a rotation in the Euclidean Space $\Gamma = \R^n$.


The set $A$ of points in $\Gamma$ such that:
:$\forall P \in A: \map {r_\theta} P = P$

is called the axis of rotation of $r_\theta$.

=== Vector Form ===
A space rotation $r_\theta$ can be expressed as an axial vector $\mathbf r_\theta$ such that:
:the direction of $\mathbf r_\theta$ is defined to be its axis of rotation
:the length of $\mathbf r_\theta$ specifies its angle of rotation of $\mathbf r_\theta$ to an appropriate scale.

:

=== Right-Hand Rule ===
Let $\mathbf V$ be an axial vector acting with respect to an axis of rotation $R$.

Consider a right hand with its fingers curled round $R$ so that the fingers are pointed in the direction of rotation of $\mathbf V$ around $R$.


The right-hand rule is the convention that the direction of $\mathbf V$ is the direction in which the thumb is pointing:


:",Definition:Rotation (Geometry),"['Definitions/Geometric Rotations', 'Definitions/Isometries (Euclidean Geometry)', 'Definitions/Euclidean Geometry', 'Definitions/Analytic Geometry']"
Definition:Rotation,Rotation,"Let $\tuple {a_1, \ldots, a_n}$ be a string over an alphabet $A$.

A rotation is a mapping $r: A^n \to A^n$ given by:

:$\tuple {a_1, \ldots, a_n} \mapsto \tuple {a_{\map \phi 1}, \cdots, a_{\map \phi n} }$

where $\phi$ is a permutation on n letters.

Category:Definitions/Permutation Theory",Definition:Rotation (Permutation Theory),['Definitions/Permutation Theory']
Definition:Row,Row,"A row of a truth table is one of the horizontal lines that consists of instances of the symbols $T$ and $F$.

Each row contains the truth values of each of the boolean interpretations of the statement forms according to the propositional variables that comprise them.

There are as many rows in a truth table as there are combinations of $T$ and $F$ for all the propositional variables that constitute the statement forms.",Definition:Truth Table/Row,['Definitions/Truth Tables']
Definition:Row,Row,"Let $\mathbf A$ be an $m \times n$ matrix.

For each $i \in \closedint 1 m$, the rows of $\mathbf A$ are the ordered $n$-tuples:
:$r_i = \tuple {a_{i 1}, a_{i 2}, \ldots, a_{i n} }$

where $r_i$ is called the $i$th row of $\mathbf A$.


A row of an $m \times n$ matrix can also be treated as a $1 \times n$ row matrix in its own right:
:$r_i = \begin {bmatrix} a_{i 1} & a_{i 2} & \cdots & a_{i n} \end {bmatrix}$
for $i = 1, 2, \ldots, m$.",Definition:Matrix/Row,['Definitions/Matrices']
Definition:Row,Row,"Let $\mathbf L$ be a Latin square.

The rows of $\mathbf L$ are the lines of elements reading across the page.",Definition:Latin Square/Row,['Definitions/Latin Squares']
Definition:Scalar Field,Scalar Field,"Let $\struct {G, +_G, \circ}_K$ be a vector space, where:

:$\struct {K, +_K, \times_K}$ is a field

:$\struct {G, +_G}$ is an abelian group $\struct {G, +_G}$

:$\circ: K \times G \to G$ is a binary operation.


Then the field $\struct {K, +_K, \times_K}$ is called the scalar field of $\struct {G, +_G, \circ}_K$.


If the scalar field is understood, then $\struct {G, +_G, \circ}_K$ can be rendered $\struct {G, +_G, \circ}$.",Definition:Scalar Field (Linear Algebra),"['Definitions/Vector Algebra', 'Definitions/Linear Algebra']"
Definition:Scalar Field,Scalar Field,"Let $F$ be a field which acts on a region of space $S$.

Let the point-function giving rise to $F$ be a scalar quantity.


Then $F$ is a scalar field.",Definition:Scalar Field (Physics),"['Definitions/Scalar Fields (Physics)', 'Definitions/Fields (Physics)', 'Definitions/Physics']"
Definition:Secant,Secant,"=== Definition from Triangle ===
:

In the above right triangle, we are concerned about the angle $\theta$.

The secant of $\angle \theta$ is defined as being $\dfrac{\text{Hypotenuse}} {\text{Adjacent}}$.

=== Definition from Circle ===
=== First Quadrant ===
Consider a unit circle $C$ whose center is at the origin of a cartesian plane.


:


Let $P$ be the point on $C$ in the first quadrant such that $\theta$ is the angle made by $OP$ with the $x$-axis.

Let a tangent line be drawn to touch $C$ at $A = \tuple {1, 0}$.

Let $OP$ be produced to meet this tangent line at $B$.


Then the secant of $\theta$ is defined as the length of $OB$.

Hence in the first quadrant, the secant is positive.

=== Second Quadrant ===
Consider a unit circle $C$ whose center is at the origin of a cartesian plane.


:


Let $P$ be the point on $C$ in the second quadrant such that $\theta$ is the angle made by $OP$ with the $x$-axis.

Let a tangent line be drawn to touch $C$ at $A = \tuple {1, 0}$.

Let $OP$ be produced to meet this tangent line at $B$.


Then the secant of $\theta$ is defined as the length of $OB$.

As $OP$ needs to be produced in the opposite direction to $P$, the secant is therefore a negative function in the second quadrant.

=== Third Quadrant ===
Consider a unit circle $C$ whose center is at the origin of a cartesian plane.


:


Let $P$ be the point on $C$ in the third quadrant such that $\theta$ is the angle made by $OP$ with the $x$-axis.

Let a tangent line be drawn to touch $C$ at $A = \tuple {1, 0}$.

Let $OP$ be produced to meet this tangent line at $B$.


Then the secant of $\theta$ is defined as the length of $OB$.


As $OP$ needs to be produced in the opposite direction to $P$, the secant is therefore a negative function in the third quadrant.

=== Fourth Quadrant ===
Consider a unit circle $C$ whose center is at the origin of a cartesian plane.


:


Let $P$ be the point on $C$ in the fourth quadrant such that $\theta$ is the angle made by $OP$ with the $x$-axis.

Let a tangent line be drawn to touch $C$ at $A = \tuple {1, 0}$.

Let $OP$ be produced to meet this tangent line at $B$.


Then the secant of $\theta$ is defined as the length of $OB$.

Hence in the fourth quadrant, the secant is positive.

=== Real Function ===
Let $x \in \R$ be a real number.

The real function $\sec x$ is defined as:

:$\sec x = \dfrac 1 {\cos x}$

where $\cos x$ is the cosine of $x$.


The definition is valid for all $x \in \R$ such that $\cos x \ne 0$.


Category:Definitions/Secant Function

=== Complex Function ===
Let $z \in \C$ be a complex number.

The complex function $\sec z$ is defined as:

:$\sec z = \dfrac 1 {\cos z}$

where $\cos z$ is the cosine of $z$.


The definition is valid for all $z \in \C$ such that $\cos z \ne 0$.


Category:Definitions/Secant Function",Definition:Secant Function,['Definitions/Secant Function']
Definition:Secant,Secant,"Let $f: \R \to \R$ be a real function.

Let the graph of $f$ be depicted on a Cartesian plane.


:


A secant to $f$ is a straight line which intersects the graph of $f$ in (at least) two points.


In the above diagram, the secant is the line $AB$ in  .",Definition:Secant Line,['Definitions/Analytic Geometry']
Definition:Section,Section,"Let $\struct {S, \preceq}$ be an ordered set.

Let $U \subseteq S$.


=== Definition 1 ===
Let $\struct {S, \preceq}$ be an ordered set.

Let $U \subseteq S$.


$U$ is an upper section in $S$  if and only if :

:$\forall u \in U: \forall s \in S: u \preceq s \implies s \in U$

=== Definition 2 ===
Let $\struct {S, \preceq}$ be an ordered set.

Let $U \subseteq S$.


$U$ is an upper section in $S$  if and only if :

:$U^\succeq \subseteq U$

where $U^\succeq$ is the upper closure of $U$.

=== Definition 3 ===
Let $\struct {S, \preceq}$ be an ordered set.

Let $U \subseteq S$.


$U$ is an upper section in $S$  if and only if :

:$U^\succeq = U$

where $U^\succeq$ is the upper closure of $U$.",Definition:Upper Section,"['Definitions/Order Theory', 'Definitions/Upper Sections']"
Definition:Section,Section,"Let $\struct {S, \preceq}$ be an ordered set.

Let $L \subseteq S$.


=== Definition 1 ===
Let $\struct {S, \preceq}$ be an ordered set.

Let $L \subseteq S$.


$L$ is a lower section in $S$  if and only if :
:$\forall l \in L, s \in S: s \preceq l \implies s \in L$

=== Definition 2 ===
Let $\struct {S, \preceq}$ be an ordered set.

Let $L \subseteq S$.


$L$ is a lower section in $S$  if and only if :
:$L^\preceq \subseteq L$

where $L^\preceq$ is the lower closure of $L$.

=== Definition 3 ===
Let $\struct {S, \preceq}$ be an ordered set.

Let $L \subseteq S$.


$L$ is a lower section in $S$  if and only if :
:$L^\preceq = L$

where $L^\preceq$ is the lower closure of $L$.

=== Class Theory ===
 
Let $A$ be a class under a total ordering $\preccurlyeq$.

Let $L$ be a subclass of $A$ such that:
:$\forall x \in L: \forall a \in A \setminus L: x \preccurlyeq a$
where $A \setminus L$ is the difference between $A$ and $L$.

Then $L$ is known as a lower section of $A$.",Definition:Lower Section,"['Definitions/Order Theory', 'Definitions/Lower Sections']"
Definition:Section,Section,"=== Definition 1 ===
Let a line segment $AB$ be divided at $C$ such that:
:$AB : AC = AC : BC$

Then the golden mean $\phi$ is defined as:
:$\phi := \dfrac {AB} {AC}$

=== Definition 2 ===
The golden mean is the unique positive real number $\phi$ satisfying:
:$\phi = \dfrac {1 + \sqrt 5} 2$

=== Definition 3 ===
The golden mean is the unique positive real number $\phi$ satisfying:
:$\phi = \dfrac 1 {\phi - 1}$",Definition:Golden Mean,"['Definitions/Golden Mean', 'Definitions/Fibonacci Numbers', 'Definitions/Real Analysis', 'Definitions/Number Theory', 'Definitions/Algebra', 'Definitions/Geometry', 'Definitions/Specific Numbers']"
Definition:Section,Section,"Let $\struct {S, \preceq}$ be a totally ordered set.


=== Definition 1 ===
Let $\struct {S, \preceq}$ be a totally ordered set.


A Dedekind cut of $\struct {S, \preceq}$ is a non-empty proper subset $L \subsetneq S$ such that:
:$(1): \quad \forall x \in L: \forall y \in S: y \prec x \implies y \in L$ ($L$ is a lower section in $S$)
:$(2): \quad \forall x \in L: \exists y \in L: x \prec y$

=== Definition 2 ===
Let $\struct {S, \preceq}$ be a totally ordered set.


A Dedekind cut of $\struct {S, \preceq}$ is an ordered pair $\tuple {L, R}$ such that:
:$(1): \quad \set {L, R}$ is a partition of $S$.
:$(2): \quad L$ does not have a greatest element.
:$(3): \quad \forall x \in L: \forall y \in R: x \prec y$.",Definition:Dedekind Cut,"['Definitions/Order Theory', 'Definitions/Real Analysis', 'Definitions/Dedekind Cuts']"
Definition:Section,Section,"=== Section of Line by Line ===
Let a geometrical line $A$ cross over (or intersect) another line $B$.

The point where they cross is called the section of the $B$ by $A$ (or equivalently, of $A$ by $B$).


Category:Definitions/Geometry

=== Plane Section ===
Let $F$ be a $3$-dimensional figure.

A plane section of $F$ is the intersection of $F$ with a plane.

Category:Definitions/Geometry",Definition:Section (Geometry),['Definitions/Geometry']
Definition:Section,Section,"Let a geometrical line $A$ cross over (or intersect) another line $B$.

The point where they cross is called the section of the $B$ by $A$ (or equivalently, of $A$ by $B$).


Category:Definitions/Geometry",Definition:Section of Line by Line,['Definitions/Geometry']
Definition:Section,Section,"The intersection of two lines $AB$ and $CD$ is denoted by $AB \cap CD$.

The intersection of two geometric figures is the set of points shared by both figures.


Note that this use of $\cap$ is consistent with that of its more usual context of set intersection.


When two lines intersect, they are said to cut each other.",Definition:Intersection (Geometry),['Definitions/Geometry']
Definition:Section,Section,"Let $F$ be a $3$-dimensional figure.

A plane section of $F$ is the intersection of $F$ with a plane.",Definition:Plane Section,"['Definitions/Plane Sections', 'Definitions/Solid Geometry']"
Definition:Section,Section,A cross-section of a 3-dimensional figure $F$ is a plane section of $F$ with a plane which is perpendicular to an axis of $F$.,Definition:Cross-Section,"['Definitions/Plane Sections', 'Definitions/Solid Geometry']"
Definition:Section,Section,"Let $M, E$ be topological spaces. 

Let $\pi: E \to M$ be a continuous surjection. 

Let $I_M: M \to M$ be the identity mapping on $M$. 


Then a section of $E$ is a continuous mapping $s: M \to E$ such that $\pi \circ s = I_M$.",Definition:Section (Topology),['Definitions/Topology']
Definition:Section,Section,"To bisect a finite geometrical object is to cut it in half, that is, into two equal parts.

The act of cutting in half is known as bisection.


=== Bisector ===
A bisector is an object which bisects another object.


=== Angle Bisector ===
:

Let $\angle ABC$ be an angle.

The angle bisector of $\angle ABC$ is the straight line which bisects $\angle ABC$.


In the above diagram, $BD$ is the angle bisector of $\angle ABC$.

Thus $\angle ABD \cong \angle DBC$ and $\angle ABD + \angle DBC = \angle ABC$.


=== Internal ===
:

Let $\angle APB$ be an angle.

The internal angle bisector of $\angle APB$ is the straight line which bisects $\angle APB$.


In the above diagram, $PC$ is the internal angle bisector of $\angle APB$.

Thus $\angle APC \cong \angle BPC$ and $\angle APC + \angle BPC = \angle APB$.

=== External ===
:

Let $\angle APB$ be an angle.

Let $BP$ be produced beyond $P$ to $B'$.

The external angle bisector of $\angle APB$ is the straight line which bisects $\angle APB'$.


In the above diagram, $PD$ is the external angle bisector of $\angle APB$.

Thus $\angle APD \cong \angle B'PD$ and $\angle APD + \angle B'PD = \angle APB'$.

=== Line Bisector ===
 

=== Perpendicular Bisector ===
Let $AB$ be a line segment.

The perpendicular bisector of $AB$ is the straight line which:

:is perpendicular to $AB$

:passes through the point which bisects $AB$.


:

=== Midpoint of Line ===
Let $L = AB$ be a line segment whose endpoints are $A$ and $B$.

Let $M$ be a point on $L$ such that the line segment $AM$ is equal to the line segment $MB$.

That is, let $M$ be the bisector of $L$.


Then $M$ is the midpoint of $L$.",Definition:Bisection,"['Definitions/Bisection', 'Definitions/Geometry']"
Definition:Segment,Segment,":

 
: 
:A segment of a circle is the figure contained by a straight line and a circumference of a circle.
 ''
 

=== Base ===
:

The base of a segment of a circle is the straight line forming one of the boundaries of the seqment.

In the above diagram, $AB$ is the base of the highlighted segment.


Category:Definitions/Segments of Circles

=== Angle of a Segment ===
 
: 
:An angle of a segment is that contained by a straight line and a circumference of a circle.
 ''
 
That is, it is the angle the base makes with the circumference where they meet.


It can also be defined as the angle between the base and the tangent to the circle at the end of the base:

:


Category:Definitions/Segments of Circles

=== Angle in a Segment ===
 
: 
:An angle in a segment is the angle which, when a point is taken on the circumference of the segment and straight lines are joined from it to the extremities of the straight line which is the base of the segment, is contained by the straight lines so joined.
 ''
 
: 
:And, when the straight lines containing the angle cut off a circumference, the angle is said to stand upon that circumference.
 ''
 

:

Such a segment is said to admit the angle specified.


Category:Definitions/Segments of Circles

=== Similar Segments ===
 
: 
:Similar segments of circles are those which admit equal angles, or in which the angles are equal to one another.
 ''
 


Category:Definitions/Segments of Circles

=== Major Segment ===
Let $AB$ be a chord of a circle $\CC$ defined by the points $A$ and $B$ on the circumference of $\CC$.

The major segment of $\CC$   $AB$ is the segment between $AB$ and the major arc of $\CC$ between $A$ and $B$.

=== Minor Segment ===
Let $AB$ be a chord of a circle $\CC$ defined by the points $A$ and $B$ on the circumference of $\CC$.

The minor segment of $\CC$   $AB$ is the segment between $AB$ and the minor arc of $\CC$ between $A$ and $B$.",Definition:Segment of Circle,"['Definitions/Segments of Circles', 'Definitions/Circles']"
Definition:Segment,Segment,"A line segment is any line (straight or not) which terminates at two points.


=== Straight Line Segment ===
A straight line segment is a line segment which is straight.


 
:A straight line segment can be drawn joining any two points.
 


Thus a definition for straight line which is frequently encountered is:
:A straight line is the shortest distance between two points.
This is all very well but it assumes that the line in question terminates at two particular endpoints.

=== Endpoint ===
Each of the points at either end of a line segment is called an endpoint of that line segment.

Similarly, the point at which an infinite half-line terminates is called the endpoint of that line.


 
: 
:The extremities of a line are points.
 ''
 

=== Midpoint ===
Let $L = AB$ be a line segment whose endpoints are $A$ and $B$.

Let $M$ be a point on $L$ such that the line segment $AM$ is equal to the line segment $MB$.

That is, let $M$ be the bisector of $L$.


Then $M$ is the midpoint of $L$.",Definition:Line/Segment,['Definitions/Lines']
Definition:Segment,Segment,"Let $\struct {S, \preceq}$ be a well-ordered set.

Let $a \in S$.


The initial segment (of $S$) determined by $a$ is defined as:

:$S_a := \set {b \in S: b \preceq a \land b \ne a}$

which can also be rendered as:

:$S_a := \set {b \in S: b \prec a}$


That is, $S_a$ is the set of all elements of $S$ that strictly precede $a$.

That is, $S_a$ is the strict lower closure of $a$ (in $S$).


By extension, $S_a$ is described as an initial segment (of $S$).


=== Class Theoretical Definition ===
 
Let $A$ be a class.

Let $\preceq$ be a well-ordering on $A$.

Let $a \in A$.


The initial segment (of $A$) determined by $a$ is defined as:

:$A_a := \set {b \in S: b \preceq a \land b \ne a}$

which can also be rendered as:

:$A_a := \set {b \in S: b \prec a}$",Definition:Initial Segment,"['Definitions/Initial Segments', 'Definitions/Order Theory']"
Definition:Segment,Segment,"Let $\struct {S, \preccurlyeq}$ be an ordered set.

Let $a \in S$.


The lower closure of $a$ (in $S$) is defined as:

:$a^\preccurlyeq := \set {b \in S: b \preccurlyeq a}$


That is, $a^\preccurlyeq$ is the set of all elements of $S$ that precede $a$.


=== Class Theory ===
 
Let $A$ be a class under an ordering $\preccurlyeq$.

Let $a \in A$.


The lower closure of $a$ (in $A$) is defined as:

:$a^\preccurlyeq := \set {b \in A: b \preccurlyeq a}$",Definition:Lower Closure/Element,['Definitions/Lower Closures']
Definition:Segment,Segment,"Let $\mathbf A$ be a matrix with $m$ rows and $n$ columns.


A submatrix of $\mathbf A$ is a matrix formed by selecting from $\mathbf A$:
:a subset of the rows
and:
:a subset of the columns
and forming a new matrix by using those entries, in the same relative positions, that appear in both the rows and columns of those selected.",Definition:Submatrix,['Definitions/Matrix Theory']
Definition:Separable,Separable,"A topological space $T = \struct {S, \tau}$ is separable  if and only if  there exists a countable subset of $S$ which is everywhere dense in $T$.


=== Normed Vector Space ===
Let $M = \struct {X, \norm {\, \cdot \,} }$ be a normed vector space.

Let $Y \subseteq X$ be a subset of $X$.

Let $Y$ be countable set and (everywhere) dense in $X$.

In other words, suppose $Y = \set {y_i : i \in \N}$ such that:

:$\forall x \in X : \forall \epsilon \in \R_{> 0} : \epsilon > 0 : \exists y_{n \mathop \in \N} \in Y : \norm {y_n - x} < \epsilon$


Then $X$ is separable.

 ",Definition:Separable Space,"['Definitions/Countability Axioms', 'Definitions/Separable Spaces']"
Definition:Separable,Separable,"A topological space $T = \struct {S, \tau}$ is second-countable or satisfies the Second Axiom of Countability  if and only if  its topology has a countable basis.",Definition:Second-Countable Space,['Definitions/Countability Axioms']
Definition:Separable,Separable,"Let $L/K$ be a field extension.

Let $\alpha\in L$.


Then $\alpha$ is separable over $K$  if and only if  its minimal polynomial over $K$ is separable.",Definition:Separable Element,['Definitions/Field Extensions']
Definition:Separable,Separable,"Let $K$ be a field.

Let $L/K$ be an algebraic field extension.


Then $L/K$ is a separable extension  if and only if  every $\alpha\in L$ is separable over $K$.

That is:
:For every $\alpha \in L$, its minimal polynomial over $K$ is separable.",Definition:Separable Extension,['Definitions/Field Extensions']
Definition:Separable,Separable,"Let $K$ be a field.

Let $\map P X \in K \sqbrk X$ be a polynomial of degree $n$.


=== Definition 1 ===
Let $K$ be a field.

Let $\map P X \in K \sqbrk X$ be a polynomial of degree $n$.


$P$ is separable  if and only if  its roots are distinct in an algebraic closure of $K$.


Category:Definitions/Separable Polynomials

=== Definition 2 ===
Let $K$ be a field.

Let $\map P X \in K \sqbrk X$ be a polynomial of degree $n$.


$P$ is separable  if and only if  it has no double roots in every field extension of $K$.


Category:Definitions/Separable Polynomials

=== Definition 3 ===
Let $K$ be a field.

Let $\map P X \in K \sqbrk X$ be a polynomial of degree $n$.


$P$ is separable  if and only if  it has $n$ distinct roots in every field extension where $P$ splits.


Category:Definitions/Separable Polynomials",Definition:Separable Polynomial,"['Definitions/Separable Polynomials', 'Definitions/Polynomial Theory', 'Definitions/Field Theory']"
Definition:Separable,Separable,"A first order ordinary differential equation which can be expressed in the form:
:$\dfrac {\d y} {\d x} = \map g x \map h y$
is known as a separable differential equation.


Its general solution is found by solving the integration:
:$\ds \int \frac {\d y} {\map h y} = \int \map g x \rd x + C$


=== General Form ===
A first order ordinary differential equation which can be expressed in the form:
:$\map {g_1} x \map {h_1} y + \map {g_2} x \map {h_2} y \dfrac {\d y} {\d x} = 0$
is known as a separable differential equation.


Its general solution is found by solving the integration:
:$\ds \int \frac {\map {g_1} x} {\map {g_2} x} \rd x + \int \frac {\map {h_2} y} {\map {h_1} y} \rd y = C$",Definition:Separable Differential Equation,"['Definitions/Separable Differential Equations', 'Definitions/Ordinary Differential Equations']"
Definition:Separated,Separated,"Let $\struct {S, \tau}$ be a topological space.

Let $x, y \in S$ such that both of the following hold:

:$\exists U \in \tau: x \in U, y \notin U$
:$\exists V \in \tau: y \in V, x \notin V$


Then $x$ and $y$ are separated points.",Definition:Separated Points,['Definitions/Topology']
Definition:Separated,Separated,"Let $T = \struct {S, \tau}$ be a topological space.

Let $A, B \subseteq S$.


=== Definition 1 ===
Let $T = \struct {S, \tau}$ be a topological space.

Let $A, B \subseteq S$.


$A$ and $B$ are separated (in $T$)  if and only if :
:$A^- \cap B = A \cap B^- = \O$
where:
:$A^-$ denotes the closure of $A$ in $T$
:$\O$ denotes the empty set.


$A$ and $B$ are said to be separated sets (of $T$).

=== Definition 2 ===
Let $T = \struct {S, \tau}$ be a topological space.

Let $A, B \subseteq S$.


$A$ and $B$ are separated (in $T$)  if and only if  there exist $U,V\in\tau$ with:
:$A \subset U$ and $U \cap B = \O$
:$B \subset V$ and $V \cap A = \O$
where $\O$ denotes the empty set.


$A$ and $B$ are said to be separated sets (of $T$).

Category:Definitions/Topology

$A$ and $B$ are said to be separated sets (of $T$).",Definition:Separated Sets,['Definitions/Topology']
Definition:Separated,Separated,"Let $T = \struct {S, \tau}$ be a topological space.


=== Sets ===
Let $T = \struct {S, \tau}$ be a topological space.


=== Definition 1 ===

Let $T = \struct {S, \tau}$ be a topological space.


Let $A, B \subseteq S$ such that:
:$\exists N_A, N_B \subseteq S: \exists U, V \in \tau: A \subseteq U \subseteq N_A, B \subseteq V \subseteq N_B: N_A \cap N_B = \O$


That is, that $A$ and $B$ both have neighborhoods in $T$ which are disjoint.


Then $A$ and $B$ are described as separated by neighborhoods.


Category:Definitions/Separated by Neighborhoods


=== Definition 2 ===
Let $T = \struct {S, \tau}$ be a topological space.


Let $A, B \subseteq S$ such that:
:$\exists U, V \in \tau: A \subseteq U, B \subseteq V: U \cap V = \O$


That is, that $A$ and $B$ both have open neighborhoods in $T$ which are disjoint.


Then $A$ and $B$ are described as separated by (open) neighborhoods.



Category:Definitions/Separated by Neighborhoods

=== Points ===
Let $T = \left({S, \tau}\right)$ be a topological space.


=== Definition 1 ===

Let $T = \struct {S, \tau}$ be a topological space.


Let $x, y \in S$ such that:

:$\exists N_x, N_y \subseteq S: \exists U, V \in \tau: x \in U \subseteq N_x, y \in V \subseteq N_y: N_x \cap N_y = \O$


That is, that $x$ and $y$ both have neighborhoods in $T$ which are disjoint.


Then $x$ and $y$ are described as separated by neighborhoods.


Thus two points are separated by neighborhoods $x$ and $y$  if and only if  the two singleton sets $\set x$ and $\set y$ are separated by neighborhoods as sets.

Category:Definitions/Separated by Neighborhoods


=== Definition 2 ===
Let $T = \struct {S, \tau}$ be a topological space.


Let $x, y \in S$ such that:

:$\exists U, V \in \tau: x \in U, y \in V: U \cap V = \O$


That is, that $x$ and $y$ both have open neighborhoods in $T$ which are disjoint.


Then $x$ and $y$ are described as separated by (open) neighborhoods.


Thus two points $x$ and $y$ are separated by neighborhoods  if and only if  the two singleton sets $\set x$ and $\set y$ are separated by open neighborhoods as sets.

Category:Definitions/Separated by Neighborhoods


Thus two points $x$ and $y$ are separated by neighborhoods  if and only if  the two singleton sets $\left\{{x}\right\}$ and $\left\{{y}\right\}$ are separated by neighborhoods as sets.",Definition:Separated by Neighborhoods,"['Definitions/Separated by Neighborhoods', 'Definitions/Separation Axioms']"
Definition:Separated,Separated,"Let $T = \struct {S, \tau}$ be a topological space.


Let $x, y \in S$ such that:

:$\exists N_x, N_y \subseteq S: \exists U, V \in \tau: x \in U \subseteq N_x, y \in V \subseteq N_y: N_x \cap N_y = \O$


That is, that $x$ and $y$ both have neighborhoods in $T$ which are disjoint.


Then $x$ and $y$ are described as separated by neighborhoods.


Thus two points are separated by neighborhoods $x$ and $y$  if and only if  the two singleton sets $\set x$ and $\set y$ are separated by neighborhoods as sets.

Category:Definitions/Separated by Neighborhoods",Definition:Points Separated by Neighborhoods/Neighborhoods,['Definitions/Separated by Neighborhoods']
Definition:Separated,Separated,"Let $T = \struct {S, \tau}$ be a topological space.


Let $A, B \subseteq S$ such that:
:$\exists N_A, N_B \subseteq S: \exists U, V \in \tau: A \subseteq U \subseteq N_A, B \subseteq V \subseteq N_B: N_A \cap N_B = \O$


That is, that $A$ and $B$ both have neighborhoods in $T$ which are disjoint.


Then $A$ and $B$ are described as separated by neighborhoods.


Category:Definitions/Separated by Neighborhoods",Definition:Sets Separated by Neighborhoods/Neighborhoods,['Definitions/Separated by Neighborhoods']
Definition:Separated,Separated,"Let $T = \struct {S, \tau}$ be a topological space.


=== Sets ===
Let $T = \struct {S, \tau}$ be a topological space.


Let $A, B \subseteq S$ such that:
:$\exists N_A, N_B \subseteq S: \exists U, V \in \tau: A \subseteq U \subseteq N_A, B \subseteq V \subseteq N_B: N_A^- \cap N_B^- = \O$
where $N_A^-$ and $N_B^-$ are the closures in $T$ of $N_A$ and $N_B$ respectively.

That is, that $A$ and $B$ both have neighborhoods in $T$ whose closures are disjoint.


Then $A$ and $B$ are described as separated by closed neighborhoods.


Category:Definitions/Separated by Closed Neighborhoods

=== Points ===
Let $T = \struct {S, \tau}$ be a topological space.


Let $x, y \in S$ such that:

:$\exists N_x, N_y \subseteq S: \exists U, V \in \tau: x \subseteq U \subseteq N_x, y \subseteq V \subseteq N_y: N_x^- \cap N_y^- = \O$

where $N_x^-$ and $N_y^-$ are the closures in $T$ of $N_x$ and $N_y$ respectively.


That is, that $x$ and $y$ both have neighborhoods in $T$ whose closures are disjoint.


Then $x$ and $y$ are described as separated by closed neighborhoods.


Thus two points are separated by closed neighborhoods $x$ and $y$  if and only if  the two singleton sets $\set x$ and $\set y$ are separated (as sets) by closed neighborhoods.",Definition:Separated by Closed Neighborhoods,"['Definitions/Separated by Closed Neighborhoods', 'Definitions/Completely Hausdorff Spaces', 'Definitions/Separation Axioms']"
Definition:Separated,Separated,"Let $T = \struct {S, \tau}$ be a topological space.


Let $x, y \in S$ such that:

:$\exists N_x, N_y \subseteq S: \exists U, V \in \tau: x \subseteq U \subseteq N_x, y \subseteq V \subseteq N_y: N_x^- \cap N_y^- = \O$

where $N_x^-$ and $N_y^-$ are the closures in $T$ of $N_x$ and $N_y$ respectively.


That is, that $x$ and $y$ both have neighborhoods in $T$ whose closures are disjoint.


Then $x$ and $y$ are described as separated by closed neighborhoods.


Thus two points are separated by closed neighborhoods $x$ and $y$  if and only if  the two singleton sets $\set x$ and $\set y$ are separated (as sets) by closed neighborhoods.",Definition:Separated by Closed Neighborhoods/Points,['Definitions/Separated by Closed Neighborhoods']
Definition:Separated,Separated,"Let $T = \struct {S, \tau}$ be a topological space.


Let $A, B \subseteq S$ such that:
:$\exists N_A, N_B \subseteq S: \exists U, V \in \tau: A \subseteq U \subseteq N_A, B \subseteq V \subseteq N_B: N_A^- \cap N_B^- = \O$
where $N_A^-$ and $N_B^-$ are the closures in $T$ of $N_A$ and $N_B$ respectively.

That is, that $A$ and $B$ both have neighborhoods in $T$ whose closures are disjoint.


Then $A$ and $B$ are described as separated by closed neighborhoods.


Category:Definitions/Separated by Closed Neighborhoods",Definition:Separated by Closed Neighborhoods/Sets,['Definitions/Separated by Closed Neighborhoods']
Definition:Separated,Separated,"Let $T = \struct {S, \tau}$ be a topological space.


=== Definition 1 ===

Let $T = \struct {S, \tau}$ be a topological space.


$\struct {S, \tau}$ is a Hausdorff space or $T_2$ space  if and only if :
:$\forall x, y \in S, x \ne y: \exists U, V \in \tau: x \in U, y \in V: U \cap V = \O$ 

That is:
:for any two distinct elements $x, y \in S$ there exist disjoint open sets $U, V \in \tau$ containing $x$ and $y$ respectively.


That is:
:$\struct {S, \tau}$ is a $T_2$ space  if and only if  every two elements in $S$ are separated by open sets.

=== Definition 2 ===
Let $T = \struct {S, \tau}$ be a topological space.


$\struct {S, \tau}$ is a Hausdorff space or $T_2$ space  if and only if  each point of $S$ is the intersection of all its closed neighborhoods.

=== Definition 3 ===
Let $T = \struct {S, \tau}$ be a topological space.


$\struct {S, \tau}$ is a Hausdorff space or $T_2$ space  if and only if :
:$\forall x, y \in S, x \ne y: \exists N_x, N_y \subseteq S: \exists U, V \in \tau: x \in U \subseteq N_x, y \in V \subseteq N_y: N_x \cap N_y = \O$

That is:
:for any two distinct elements $x, y \in S$ there exist disjoint neighborhoods $N_x, N_y \subseteq S$ containing $x$ and $y$ respectively.


That is:
:$\struct {S, \tau}$ is a $T_2$ space  if and only if  every two elements in $S$ are separated by neighborhoods.",Definition:Hausdorff Space,"['Definitions/Separation Axioms', 'Definitions/Hausdorff Spaces']"
Definition:Side,Side,":

The line segments which make up a polygon are known as its sides.

Thus, in the polygon above, the sides are identified as $a, b, c, d$ and $e$.",Definition:Polygon/Side,"['Definitions/Sides of Polygons', 'Definitions/Polygons']"
Definition:Side,Side,"From the definition of surface, it follows that a plane locally separates space into two sides.

Thus the sides of a plane are the parts of that space into which the plane separates it.


Category:Definitions/Surfaces",Definition:Plane Surface/Side,['Definitions/Surfaces']
Definition:Side,Side,"Let $S$ be a surface.

By definition, $S$ locally separates space into two sides.

Thus the sides of $S$ are the parts of that space into which $S$ separates it.


Category:Definitions/Surfaces",Definition:Side of Surface,['Definitions/Surfaces']
Definition:Side,Side,"The side of a plane number is one of the (natural) numbers which are its divisors.


=== Example ===


Category:Definitions/Euclidean Number Theory",Definition:Plane Number/Side,['Definitions/Euclidean Number Theory']
Definition:Side,Side,"The side of a solid number is one of the (natural) numbers which are its divisors.


=== Example ===


Category:Definitions/Euclidean Number Theory",Definition:Solid Number/Side,['Definitions/Euclidean Number Theory']
Definition:Sign,Sign,"Let $n \in \N$ be a natural number.

Let $\N_n$ denote the set of natural numbers $\set {1, 2, \ldots, n}$.

Let $\tuple {x_1, x_2, \ldots, x_n}$ be an ordered $n$-tuple of real numbers.

Let $\pi$ be a permutation of $\N_n$.

Let $\map {\Delta_n} {x_1, x_2, \ldots, x_n}$ be the product of differences of $\tuple {x_1, x_2, \ldots, x_n}$.

Let $\pi \cdot \map {\Delta_n} {x_1, x_2, \ldots, x_n}$ be defined as:

:$\pi \cdot \map {\Delta_n} {x_1, x_2, \ldots, x_n} := \map {\Delta_n} {x_{\map \pi 1}, x_{\map \pi 2}, \ldots, x_{\map \pi n} }$


The sign of $\pi \in S_n$ is defined as:

:$\map \sgn \pi = \begin {cases}
\dfrac {\Delta_n} {\pi \cdot \Delta_n} & : \Delta_n \ne 0 \\
0 & : \Delta_n = 0 \end {cases}$",Definition:Sign of Permutation,"['Definitions/Sign of Permutation', 'Definitions/Permutation Theory', 'Definitions/Algebra']"
Definition:Sign,Sign,"In the context of arithmetic and algebra, the term sign is used to mean one of the operators:

* Addition: $+$
* Subtraction: $-$
* Multiplication: $\times$
* Division: $\div$

It can also be used to describe a general operator in the context of abstract algebra: $\circ$ and so on.",Definition:Sign (Arithmetic),"['Definitions/Arithmetic', 'Definitions/Algebra', 'Definitions/Abstract Algebra']"
Definition:Sign,Sign,"The sign of a number is the symbol indicating whether it is:
: positive, denoted by the symbol $+$
or:
: negative, denoted by the symbol $-$.


Hence a number's sign has evolved to define the fact of the number being positive or negative independently of the symbol itself.


Thus:
:the sign of $3.14159$ is positive
and
:the sign of $-75$ is negative.",Definition:Sign of Number,['Definitions/Numbers']
Definition:Sign,Sign,"Let $X \subseteq \R$ be a subset of the real numbers.


The signum function $\sgn: X \to \set {-1, 0, 1}$ is defined as:
:$\forall x \in X: \map \sgn x := \sqbrk {x > 0} - \sqbrk {x < 0}$
where $\sqbrk {x > 0}$ etc. denotes Iverson's convention.


That is:
:$\forall x \in X: \map \sgn x := \begin {cases} -1 & : x < 0 \\ 0 & : x = 0 \\ 1 & : x > 0 \end {cases}$


=== Graph of Signum Function ===
The graph of the signum function is illustrated below:


:

=== Natural Numbers ===
The signum function $\sgn: \N \to \set {0, 1}$ is the restriction of the signum function to the natural numbers, defined as:
:$\forall n \in \N: \map \sgn n := \begin {cases} 0 & : n = 0 \\ 1 & : n > 0 \end{cases}$


=== Signum Complement ===
Let $\sgn: \N \to \set {0, 1}$ be the signum function on the natural numbers.

The signum complement function $\overline \sgn: \N \to \set {0, 1}$ is defined as:
:$\forall n \in \N: \map {\overline \sgn} n := \begin {cases} 1 & : n = 0 \\ 0 & : n > 0 \end {cases}$",Definition:Signum Function,"['Definitions/Signum Function', 'Definitions/Number Theory', 'Definitions/Real Analysis', 'Definitions/Discrete Mathematics']"
Definition:Signature,Signature,"Let $\LL$ be a formal language.

A choice of vocabulary for $\LL$ is called a signature for $\LL$.


=== Signature for Predicate Logic ===
Let $\LL_1$ be the language of predicate logic.


Then a signature for $\LL_1$ is an explicit choice of the alphabet of $\LL_1$.

That is to say, it amounts to choosing, for each $n \in \N$:

:A collection $\FF_n$ of $n$-ary function symbols
:A collection $\PP_n$ of $n$-ary relation symbols

It is often conceptually enlightening to explicitly address the $0$-ary function symbols separately, as constant symbols.",Definition:Signature (Logic),['Definitions/Formal Languages']
Definition:Signature,Signature,"Let $n \in \N$ be a natural number.

Let $\N_n$ denote the set of natural numbers $\set {1, 2, \ldots, n}$.

Let $\tuple {x_1, x_2, \ldots, x_n}$ be an ordered $n$-tuple of real numbers.

Let $\pi$ be a permutation of $\N_n$.

Let $\map {\Delta_n} {x_1, x_2, \ldots, x_n}$ be the product of differences of $\tuple {x_1, x_2, \ldots, x_n}$.

Let $\pi \cdot \map {\Delta_n} {x_1, x_2, \ldots, x_n}$ be defined as:

:$\pi \cdot \map {\Delta_n} {x_1, x_2, \ldots, x_n} := \map {\Delta_n} {x_{\map \pi 1}, x_{\map \pi 2}, \ldots, x_{\map \pi n} }$


The sign of $\pi \in S_n$ is defined as:

:$\map \sgn \pi = \begin {cases}
\dfrac {\Delta_n} {\pi \cdot \Delta_n} & : \Delta_n \ne 0 \\
0 & : \Delta_n = 0 \end {cases}$",Definition:Sign of Permutation,"['Definitions/Sign of Permutation', 'Definitions/Permutation Theory', 'Definitions/Algebra']"
Definition:Similar,Similar,"Two rectilineal figures are similar  if and only if :
:They have corresponding angles, all of which are equal
:They have corresponding sides, all of which are proportional.


=== Informal Definition ===

Two geometric figures are similar if they have the same shape but not necessarily the same size.

It is intuitively understood what it means for two figures to have the same shape.


=== Algebraic Definition ===

Two geometric figures are similar if one can be transformed into the other by means of a similarity mapping.


=== Euclid's Definition ===

 
: 
:Similar rectilineal figures are such as have their angles severally equal and the sides about the equal angles proportional.
 ''
 ",Definition:Similar Figures,['Definitions/Euclidean Geometry']
Definition:Similar,Similar,"Similar triangles are triangles whose corresponding angles are the same, but whose corresponding sides may be of different lengths.

:

Thus $\triangle ABC$ is similar to $\triangle DEF$:
:$\angle ABC = \angle EFD$
:$\angle BCA = \angle EDF$
:$\angle CAB = \angle DEF$",Definition:Similar Triangles,['Definitions/Triangles']
Definition:Similar,Similar," 
: 
:Similar segments of circles are those which admit equal angles, or in which the angles are equal to one another.
 ''
 


Category:Definitions/Segments of Circles",Definition:Segment of Circle/Similar,['Definitions/Segments of Circles']
Definition:Similar,Similar," 

 

Category:Definitions/Solid Geometry",Definition:Similar Solid Figures,['Definitions/Solid Geometry']
Definition:Similar,Similar,"Let $h_1$ and $h_2$ be the lengths of the axes of two right circular cones.

Let $d_1$ and $d_2$ be the lengths of the diameters of the bases of the two right circular cones.

Then the two right circular cones are similar  if and only if :

:$\dfrac {h_1} {h_2} = \dfrac {d_1} {d_2}$


 
: 
:Similar cones and cylinders are those in which the axes and the diameters of the bases are proportional.
 ''
 


Category:Definitions/Right Circular Cones",Definition:Right Circular Cone/Similar Cones,['Definitions/Right Circular Cones']
Definition:Similar,Similar,"Let $h_1$ and $h_2$ be the heights of two cylinders.

Let $d_1$ and $d_2$ be the diameters of the bases of the two cylinders.

Then the two cylinders are similar  if and only if :

:$\dfrac {h_1} {h_2} = \dfrac {d_1} {d_2}$


 
: 
:Similar cones and cylinders are those in which the axes and the diameters of the bases are proportional.
 ''
 


Category:Definitions/Cylinders",Definition:Cylinder/Similar Cylinders,['Definitions/Cylinders']
Definition:Similar,Similar,"Similar planes are plane figures which are similar.

Category:Definitions/Euclidean Geometry",Definition:Similar Planes,['Definitions/Euclidean Geometry']
Definition:Similar,Similar," 

Category:Definitions/Angles
Category:Definitions/Solid Geometry",Definition:Similar Inclination,"['Definitions/Angles', 'Definitions/Solid Geometry']"
Definition:Similar,Similar,Two similar solid figures are said to be in a similar situation  if and only if  corresponding surfaces are similarly inclined and when corresponding edges are parallel.,Definition:Similar Situation,['Definitions/Solid Geometry']
Definition:Similar,Similar,"Let $m$ and $n$ be plane numbers.

Let:
:$m = p_1 \times p_2$ where $p_1 \le p_2$
:$n = q_1 \times q_2$ where $q_1 \le q_2$

Then $m$ and $n$ are similar  if and only if :
:$p_1 : q_1 = p_2 : q_2$


That is:
:$\dfrac {p_1} {q_1} = \dfrac {p_2} {q_2}$


 
: 
:Similar plane and solid numbers are those which have their sides proportional.
 ''
 

Category:Definitions/Euclidean Number Theory",Definition:Plane Number/Similar Numbers,['Definitions/Euclidean Number Theory']
Definition:Similar,Similar,"Let $m$ and $n$ be solid numbers.

Let:
: $m = p_1 \times p_2 \times p_3$ where $p_1 \le p_2 \le p_3$
: $n = q_1 \times q_2 \times q_3$ where $q_1 \le q_2 \le q_3$

Then $m$ and $n$ are similar  if and only if :
:$p_1 : q_1 = p_2 : q_2 = p_3 : q_3$


 
: 
:Similar plane and solid numbers are those which have their sides proportional.
 ''
 

Category:Definitions/Euclidean Number Theory",Definition:Solid Number/Similar Numbers,['Definitions/Euclidean Number Theory']
Definition:Similar,Similar,"Let $R$ be a ring with unity.

Let $n \in \N_{>0}$ be a natural number.

Let $\mathbf A, \mathbf B$ be square matrices of order $n$ over $R$.


=== Definition 1 ===
Let $R$ be a ring with unity.

Let $n \in \N_{>0}$ be a natural number.

Let $\mathbf A, \mathbf B$ be square matrices of order $n$ over $R$.

Let there exist an invertible square matrix $\mathbf P$ of order $n$ over $R$ such that $\mathbf B = \mathbf P^{-1} \mathbf A \mathbf P$.


Then $\mathbf A$ and $\mathbf B$ are similar.


We write:
:$\mathbf A \sim \mathbf B$

=== Definition 2 ===
Let $R$ be a ring with unity.

Let $n \in \N_{>0}$ be a natural number.

Let $\mathbf A, \mathbf B$ be square matrices of order $n$ over $R$.


$\mathbf A$ and $\mathbf B$ are similar  if and only if  they are the relative matrices, to (possibly) different ordered bases, of the same linear operator.


We write:
:$\mathbf A \sim \mathbf B$


We write:
:$\mathbf A \sim \mathbf B$",Definition:Matrix Similarity,"['Definitions/Matrix Similarity', 'Definitions/Matrix Equivalence', 'Definitions/Matrix Theory', 'Definitions/Matrix Algebra']"
Definition:Similar,Similar,"Let $G$ be a vector space over a field $K$.

Let $\beta \in K$.

Let $s_\beta: G \to G$ be the mapping on $G$ defined as:
:$\forall \mathbf x \in G: \map {s_\beta} {\mathbf x} = \beta \mathbf x$


$s_\beta$ is called a similarity (mapping).


=== Scale Factor ===
Let $G$ be a vector space over a field $K$.

Let $\beta \in K$.

Let $s_\beta: G \to G$ be a similarity mapping on $G$:
:$\forall \mathbf x \in G: \map {s_\beta} {\mathbf x} = \beta \mathbf x$


The coefficient $\beta$ is called the scale factor of $s_\beta$.",Definition:Similarity Mapping,"['Definitions/Linear Algebra', 'Definitions/Similarity Mappings']"
Definition:Similar,Similar,"Let $S$ and $T$ be sets.

Then $S$ and $T$ are equivalent  if and only if :
:there exists a bijection $f: S \to T$ between the elements of $S$ and those of $T$.

That is,  if and only if  they have the same cardinality.


This can be written $S \sim T$.


If $S$ and $T$ are not equivalent we write $S \nsim T$.",Definition:Set Equivalence,"['Definitions/Set Equivalence', 'Definitions/Set Theory']"
Definition:Similarity,Similarity,"Let $G$ be a vector space over a field $K$.

Let $\beta \in K$.

Let $s_\beta: G \to G$ be the mapping on $G$ defined as:
:$\forall \mathbf x \in G: \map {s_\beta} {\mathbf x} = \beta \mathbf x$


$s_\beta$ is called a similarity (mapping).


=== Scale Factor ===
Let $G$ be a vector space over a field $K$.

Let $\beta \in K$.

Let $s_\beta: G \to G$ be a similarity mapping on $G$:
:$\forall \mathbf x \in G: \map {s_\beta} {\mathbf x} = \beta \mathbf x$


The coefficient $\beta$ is called the scale factor of $s_\beta$.",Definition:Similarity Mapping,"['Definitions/Linear Algebra', 'Definitions/Similarity Mappings']"
Definition:Similarity,Similarity,"Let $S$ be a geometric object.

$S$ has the property of self-similarity  if and only if :
:every point of $S$ is contained in a copy of $S$ at a smaller scale.


 ",Definition:Self-Similarity,['Definitions/Fractals']
Definition:Similarity,Similarity,"Let $R$ be a ring with unity.

Let $n \in \N_{>0}$ be a natural number.

Let $\mathbf A, \mathbf B$ be square matrices of order $n$ over $R$.


=== Definition 1 ===
Let $R$ be a ring with unity.

Let $n \in \N_{>0}$ be a natural number.

Let $\mathbf A, \mathbf B$ be square matrices of order $n$ over $R$.

Let there exist an invertible square matrix $\mathbf P$ of order $n$ over $R$ such that $\mathbf B = \mathbf P^{-1} \mathbf A \mathbf P$.


Then $\mathbf A$ and $\mathbf B$ are similar.


We write:
:$\mathbf A \sim \mathbf B$

=== Definition 2 ===
Let $R$ be a ring with unity.

Let $n \in \N_{>0}$ be a natural number.

Let $\mathbf A, \mathbf B$ be square matrices of order $n$ over $R$.


$\mathbf A$ and $\mathbf B$ are similar  if and only if  they are the relative matrices, to (possibly) different ordered bases, of the same linear operator.


We write:
:$\mathbf A \sim \mathbf B$


We write:
:$\mathbf A \sim \mathbf B$",Definition:Matrix Similarity,"['Definitions/Matrix Similarity', 'Definitions/Matrix Equivalence', 'Definitions/Matrix Theory', 'Definitions/Matrix Algebra']"
Definition:Similarity,Similarity,"Let $S$ and $T$ be sets.

Then $S$ and $T$ are equivalent  if and only if :
:there exists a bijection $f: S \to T$ between the elements of $S$ and those of $T$.

That is,  if and only if  they have the same cardinality.


This can be written $S \sim T$.


If $S$ and $T$ are not equivalent we write $S \nsim T$.",Definition:Set Equivalence,"['Definitions/Set Equivalence', 'Definitions/Set Theory']"
Definition:Simple,Simple,"A group $G$ is simple  if and only if  it has only $G$ and the trivial group as normal subgroups.

That is,  if and only if  the composition length of $G$ is $1$.",Definition:Simple Group,['Definitions/Normality in Groups']
Definition:Simple,Simple,"Let $E / F$ be a field extension.


Then $E$ is a simple extension over $F$  if and only if :
:$\exists \alpha \in E: E = F \sqbrk \alpha$
where $F \sqbrk \alpha$ is the field extension generated by $\alpha$.",Definition:Simple Field Extension,['Definitions/Field Extensions']
Definition:Simple,Simple,"Let $\R$ be the field of real numbers.


=== Simple Finite Continued Fraction ===
Let $\R$ be the set of real numbers.

Let $n \ge 0$ be a natural number.


A simple finite continued fraction of length $n$ is a finite continued fraction in $\R$ of length $n$ whose partial denominators are integers that are strictly positive, except perhaps the first.

That is, it is a finite sequence $a: \closedint 0 n \to \Z$ with $a_n > 0$ for $n > 0$.

=== Simple Infinite Continued Fraction ===
Let $\R$ be the field of real numbers.


A simple infinite continued fraction is a infinite continued fraction in $\R$ whose partial denominators are integers that are strictly positive, except perhaps the first.

That is, it is a sequence $a: \N_{\ge 0} \to \Z$ with $a_n > 0$ for $n > 0$.",Definition:Continued Fraction/Simple,"['Definitions/Simple Continued Fractions', 'Definitions/Continued Fractions']"
Definition:Simple,Simple,"Let $\R$ be the set of real numbers.

Let $n \ge 0$ be a natural number.


A simple finite continued fraction of length $n$ is a finite continued fraction in $\R$ of length $n$ whose partial denominators are integers that are strictly positive, except perhaps the first.

That is, it is a finite sequence $a: \closedint 0 n \to \Z$ with $a_n > 0$ for $n > 0$.",Definition:Continued Fraction/Simple/Finite,['Definitions/Simple Continued Fractions']
Definition:Simple,Simple,"Let $\R^n$ be a real cartesian space of $n$ dimensions.

Let $C_1, \ldots, C_n$ be directed smooth curves in $\R^n$.

Let $C_i$ be parameterized by the smooth path $\rho_i: \closedint {a_i} {b_i} \to \R^n$ for all $i \in \set {1, 2, \ldots, n}$.

Let $C$ be the contour in $\R^n$ defined by the finite sequence $C_1, \ldots, C_n$.


$C$ is a simple contour  if and only if :

:$(1): \quad$ For all $i, j \in \set {1, \ldots, n}, t_1 \in \hointr {a_i} {b_i}, t_2 \in \hointr {a_j} {b_j}$ with $t_1 \ne t_2$, we have $\map {\rho_i} {t_1} \ne \map {\rho_j} {t_2}$

:$(2): \quad$ For all $k \in \set {1, \ldots, n}, t \in \hointr {a_k} {b_k}$ where either $k \ne 1$ or $t \ne a_1$, we have $\map {\rho_k} t \ne \map {\rho_n} {b_n}$.


Thus a simple contour is a contour that does not intersect itself.


=== Complex Plane ===

The definition carries over to the complex plane, in which context it is usually applied:

Let $C_1, \ldots, C_n$ be directed smooth curves in the complex plane $\C$.

Let $C_k$ be parameterized by the smooth path $\gamma_k: \closedint {a_k} {b_k} \to \C$ for all $k \in \set {1, \ldots, n}$.

Let $C$ be the contour defined by the finite sequence $C_1, \ldots, C_n$.


$C$ is a simple contour  if and only if :

:$(1): \quad$ For all $j, k \in \set {1, \ldots, n}, t_1 \in \hointr {a_j} {b_j}, t_2 \in \hointr {a_k} {b_k}$ with $t_1 \ne t_2$, we have $\map {\gamma_j} {t_1} \ne \map {\gamma_j} {t_2}$.

:$(2): \quad$ For all $k \in \set {1, \ldots, n}, t \in \hointr {a_k} {b_k}$ where either $k \ne 1$ or $t \ne a_1$, we have $\map {\gamma_k} t \ne \map {\gamma_n} {b_n}$.


Thus a simple contour is a contour that does not intersect itself.",Definition:Contour/Simple,['Definitions/Vector Analysis']
Definition:Simple,Simple,"A simple graph is a graph which is:

:An undirected graph, that is, the edges are defined as doubleton sets of vertices and not ordered pairs

:Not a multigraph, that is, there is no more than one edge between each pair of vertices

:Not a loop-graph, that is, there are no loops, that is, edges which start and end at the same vertex

:Not a weighted graph, that is, the edges are not mapped to a number.


=== Formal Definition ===
Let $V$ be a set.

Let $\RR$ be an endorelation on $V$ which is antireflexive and symmetric.

Let $E$ be the set whose elements of the form:
:$\set {\tuple {v_a, v_b}, \tuple {v_b, v_a} }$.
where $\tuple {v_a, v_b}$ and $\tuple {v_b, v_a}$ are elements of $\RR$ 


A simple graph is an ordered pair $G = \struct {V, E}$, where $V$ and $E$ are defined as above.


$V$ is called the vertex set.

$E$ is called the edge set.",Definition:Simple Graph,"['Definitions/Simple Graphs', 'Definitions/Graph Theory']"
Definition:Simple,Simple,"Let $D = \struct {V, E}$ be a digraph.

If the relation $E$ in $D$ is also specifically asymmetric, then $D$ is called a simple digraph.

That is, in a simple digraph there are no pairs of arcs (like there are between $v_1$ and $v_4$ in the diagram above) which go in both directions between two vertices.",Definition:Digraph/Simple Digraph,['Definitions/Digraphs']
Definition:Simple,Simple,"Let $G = \struct {V, E}$ be a multigraph.


A simple edge is an edge $u v$ of $G$ which is the only edge of $G$ which is incident to both $u$ and $v$.


Category:Definitions/Edges of Graphs
Category:Definitions/Multigraphs",Definition:Multigraph/Simple Edge,"['Definitions/Edges of Graphs', 'Definitions/Multigraphs']"
Definition:Simple,Simple,"Let $\struct {X, \Sigma}$ be a measurable space.

A real-valued function $f: X \to \R$ is said to be a simple function  if and only if  it is a finite linear combination of characteristic functions:

:$\ds f = \sum_{k \mathop = 1}^n a_k \chi_{S_k}$

where $a_1, a_2, \ldots, a_n$ are real numbers and each of the sets $S_k$ is $\Sigma$-measurable.


=== Positive Simple Function ===

When all of the $a_i$ are positive, $f$ is also said to be positive.

 

=== Banach Space ===
Let $\GF \in \set {\R, \C}$. 

Let $I$ be a real interval.

Let $X$ be a Banach space over $\GF$. 

Let $f : I \to X$ be a function.


We say that $f$ is simple  if and only if  there exists: 

:Lebesgue measurable subsets $\Omega_1, \ldots, \Omega_r$ of $I$ with finite Lebesgue measure
:$x_1, \ldots, x_r \in X$ 

such that: 

:$\ds \map f t = \sum_{r \mathop = 1}^n x_r \map {\chi_{\Omega_r} } t$

for each $t \in I$.",Definition:Simple Function,['Definitions/Measure Theory']
Definition:Singular,Singular,"A singular statement is a statement whose subject is identified by means of a proper name.

More generally, it is a statement which contains no variables, either bound or free.


=== Individuating Description ===
An individuating description is a predicate whose purpose is to uniquely identify a particular object.

=== Designatory Function ===
A designatory function is a propositional function which, on replacement of the operand with a constant, becomes an individuating description.",Definition:Singular Statement,['Definitions/Predicate Logic']
Definition:Singular,Singular,"Let $\struct {R, +, \circ}$ be a ring with unity.

Let $n \in \Z_{>0}$ be a (strictly) positive integer.

Let $\mathbf A$ be an element of the ring of square matrices $\struct {\map {\MM_R} n, +, \times}$.


=== Definition 1 ===
Let $\struct {R, +, \circ}$ be a ring with unity.

Let $n \in \Z_{>0}$ be a (strictly) positive integer.

Let $\mathbf A$ be an element of the ring of square matrices $\struct {\map {\MM_R} n, +, \times}$.


Let $\mathbf A$ have no inverse.

Then $\mathbf A$ is referred to as non-invertible.

=== Definition 2 ===
Let $\struct {R, +, \circ}$ be a ring with unity.

Let $n \in \Z_{>0}$ be a (strictly) positive integer.

Let $\mathbf A$ be an element of the ring of square matrices $\struct {\map {\MM_R} n, +, \times}$.


Let the determinant of $\mathbf A$ be equal to $0$.

Then $\mathbf A$ is referred to as non-invertible.",Definition:Non-Invertible Matrix,"['Definitions/Matrices', 'Definitions/Non-Invertible Matrices']"
Definition:Singular,Singular,"Let $\kappa$ be an infinite cardinal.


Then $\kappa$ is a singular cardinal  if and only if  $\map {\mathrm {cf} } \kappa < \kappa$.

That is, the cofinality of $\kappa$ is less than itself.

 ",Definition:Singular Cardinal,['Definitions/Cardinals']
Definition:Singular,Singular,"=== Real Analysis ===

Let $C$ be a locus.
Let $C$ be a locus.

A point $P \in C$ is called a singular point  if and only if  $P$ does not have a unique tangent to $C$ which is itself differentiable.

=== Complex Analysis ===
Let $U \subseteq \C$ be an open set.

Let $f : U \to \C$ be a complex function.


A singular point of $f$ is a point at which $f$ is not analytic.",Definition:Singular Point,"['Definitions/Singular Points', 'Definitions/Singularity Theory', 'Definitions/Analysis']"
Definition:Skew,Skew,"Let $L_1$ and $L_2$ be two straight lines in $3$-dimensional Euclidean space.


$L_1$ and $L_2$ are said to be skew  if and only if , when produced, they are neither intersecting nor parallel.",Definition:Skew Lines,"['Definitions/Skew Lines', 'Definitions/Solid Geometry']"
Definition:Skew,Skew,A skew field is a division ring whose ring product is specifically not commutative.,Definition:Skew Field,"['Definitions/Ring Theory', 'Definitions/Field Theory']"
Definition:Skew,Skew,"Skewness is a measure of the asymmetry of a probability distribution about its mean.


Let $X$ be a random variable with mean $\mu$ and standard deviation $\sigma$.

Then the skewness of $X$, usually denoted $\gamma_1$, is defined as: 

:$\gamma_1 = \expect {\paren {\dfrac {X - \mu} \sigma}^3}$

where $\expect X$ denotes the expectation of $X$.


=== Coefficient of Skewness ===
Let $X$ be a random variable with mean $\mu$ and standard deviation $\sigma$.

The coefficient of skewness of $X$ is the coefficient:
:$\gamma_1 = \expect {\paren {\dfrac {X - \mu} \sigma}^3}$
where $\mu_i$ denotes the $i$th central moment of $X$.",Definition:Skewness,"['Definitions/Skewness', 'Definitions/Statistics', 'Definitions/Probability Theory']"
Definition:Small,Small,"Let $A$ denote an arbitrary class.


Then $A$ is said to be small  if and only if :

:$\exists x: x = A$

where $=$ denotes class equality and $x$ is a set variable.


That is, a class is small  if and only if  it is equal to some set variable.


To denote that a class $A$ is small, the notation $\map \MM A$ may be used.

Thus:
:$\map \MM A \iff \exists x: x = A$",Definition:Small Class,"['Definitions/Class Theory', 'Definitions/Set Theory']"
Definition:Small,Small,"Let $\mathbf C$ be a metacategory.


Then $\mathbf C$ is said to be small  if and only if  both of the following hold:

:The collection of objects $\mathbf C_0$ is a set;
:The collection of morphisms $\mathbf C_1$ is a set.",Definition:Small Category,['Definitions/Category Theory']
Definition:Smooth,Smooth,"A real function is smooth  if and only if  it is of differentiability class $C^\infty$.

That is,  if and only if  it admits of continuous derivatives of all orders.


 ",Definition:Smooth Real Function,"['Definitions/Differentiable Real Functions', 'Definitions/Topology', 'Definitions/Differentiability Classes']"
Definition:Smooth,Smooth,"Let $M, N$ be smooth manifolds. 

Denote $m := \dim M$ and $n := \dim N$. 

Let $\phi: M \to N$ be a mapping. 


Then $\phi$ is a smooth mapping  if and only if :
:for every chart $\struct {U, \kappa}$ on $M$ and every chart $\struct {V, \xi}$ on $N$ such that $V \cap \map \phi U \ne \O$, the mapping:
::$\ds \xi \circ \phi \circ \kappa^{-1}: \map \kappa U \subseteq \R^m \to \map \xi {V \cap \map \phi U} \subseteq \R^n$
:is smooth.",Definition:Smooth Mapping,['Definitions/Manifolds']
Definition:Smooth,Smooth,"Let $M$ be a second-countable locally Euclidean space of dimension $d$. 

Let $\mathscr F$ be a smooth differentiable structure on $M$.


Then $\struct {M, \mathscr F}$ is called a smooth manifold of dimension $d$.",Definition:Topological Manifold/Smooth Manifold,"['Definitions/Smooth Manifolds', 'Definitions/Topological Manifolds', 'Definitions/Differentiable Manifolds']"
Definition:Smooth,Smooth,"Let $X$ and $Y$ be topological spaces.

Let $f: X \to Y$, $g: X \to Y$ be smooth mappings.


Then $f$ and $g$ are smoothly homotopic  if and only if  there exists a smooth mapping:
: $H: X \times \left[{0 \,.\,.\, 1}\right] \to Y$
such that:
: $H \left({x, 0}\right) = f \left({x}\right)$
and:
: $H \left({x, 1}\right) = g \left({x}\right)$


$H$ is called a smooth homotopy between $f$ and $g$.",Definition:Smooth Homotopy,['Definitions/Homotopy Theory']
Definition:Solution,Solution,"Let $P: X \to \set {\T, \F}$ be a propositional function defined on a domain $X$.

Let $S = \map {P^{-1} } \T$ be the fiber of truth (under $P$).


Then an element of $S$ is known as a solution of $P$.


This terminology is usual when $P$ is an equation in the context of algebra.",Definition:Fiber of Truth/Solution,"['Definitions/Mapping Theory', 'Definitions/Algebra']"
Definition:Solution,Solution,"Let $\Phi$ be a differential equation defined on a domain $D$.

Let $\phi$ be a function which satisfies $\Phi$ on the whole of $D$.


Then $\phi$ is known as a solution of $\Phi$.


Note that, in general, there may be more than one solution to a given differential equation.

On the other hand, there may be none at all.


=== General Solution ===
Let $\Phi$ be a differential equation.

The general solution to $\Phi$ is the set of all functions $\phi$ that satisfy $\Phi$.


 

=== Particular Solution ===
Let $\Phi$ be a differential equation.

Let $S$ denote the solution set of $\Phi$.

A particular solution of $\Phi$ is the element of $S$, or subset of $S$, which satisfies a particular boundary condition of $\Phi$.

=== Weak Solution ===
A weak solution is a solution to a non-standard formulation of a differential equation.

 ",Definition:Differential Equation/Solution,"['Definitions/Solutions to Differential Equations', 'Definitions/Differential Equations']"
Definition:Solution,Solution,"Let $\Phi$ be a differential equation.

The general solution to $\Phi$ is the set of all functions $\phi$ that satisfy $\Phi$.


 ",Definition:Differential Equation/Solution/General Solution,"['Definitions/General Solutions to Differential Equations', 'Definitions/Solutions to Differential Equations', 'Definitions/Differential Equations']"
Definition:Solution,Solution,"Let $\Phi$ be a differential equation.

Let $S$ denote the solution set of $\Phi$.

A particular solution of $\Phi$ is the element of $S$, or subset of $S$, which satisfies a particular boundary condition of $\Phi$.",Definition:Differential Equation/Solution/Particular Solution,"['Definitions/Particular Solutions to Differential Equations', 'Definitions/Solutions to Differential Equations']"
Definition:Solution,Solution,"Let $P: X \to \set {\T, \F}$ be a propositional function defined on a domain $X$.


The fiber of truth (under $P$) is the preimage, or fiber, of $\T$ under $P$:
:$\map {P^{-1} } \T := \set {x \in X: \map P x = \T}$


That is, the elements of $X$ whose image under $P$ is $\T$.


=== Solution ===
Let $P: X \to \set {\T, \F}$ be a propositional function defined on a domain $X$.

Let $S = \map {P^{-1} } \T$ be the fiber of truth (under $P$).


Then an element of $S$ is known as a solution of $P$.


This terminology is usual when $P$ is an equation in the context of algebra.",Definition:Fiber of Truth,"['Definitions/Mapping Theory', 'Definitions/Symbolic Logic']"
Definition:Solution,Solution,"An ordered $n$-tuple $\tuple {x_1, x_2, \ldots, x_n}$ which satisfies each of the equations in a system of $m$ simultaneous equations in $n$ variables is called a solution of the system.",Definition:Simultaneous Equations/Solution,['Definitions/Simultaneous Equations']
Definition:Solution,Solution,"Let $G$ be a game.

A solution of $G$ is a systematic description of the outcomes that may emerge in a family of games.",Definition:Solution of Game,['Definitions/Game Theory']
Definition:Solution,Solution,"Let:
:$\map P x \equiv 0 \pmod n$
be a polynomial congruence.


A solution of $\map P x \equiv 0 \pmod n$ is a residue class modulo $n$ such that any element of that class satisfies the congruence.",Definition:Polynomial Congruence/Solution,['Definitions/Polynomial Congruences']
Definition:Space,Space,"Let $S$ be a set.

The cartesian $n$th power of $S$, or $S$ to the power of $n$, is defined as:

:$\ds S^n = \prod_{k \mathop = 1}^n S = \set {\tuple {x_1, x_2, \ldots, x_n}: \forall k \in \N^*_n: x_k \in S}$


Thus $S^n = \underbrace {S \times S \times \cdots \times S}_{\text{$n$ times} }$

Alternatively it can be defined recursively:

:$S^n = \begin{cases}
S: & n = 1 \\
S \times S^{n - 1} & n > 1
\end{cases}$


The set $S^n$ called a cartesian space.


An element $x_j$ of an ordered tuple $\tuple {x_1, x_2, \ldots, x_n}$ of a cartesian space $S^n$ is known as a basis element of $S^n$.


=== Two Dimensions ===

$n = 2$ is frequently taken as a special case:

Let $S$ be a set.

The cartesian $2$nd power of $S$ is:

:$S^2 = S \times S = \set {\tuple {x_1, x_2}: x_1, x_2 \in S}$


The set $S^2$ called a cartesian space of $2$ dimensions.


=== Cartesian Plane ===

When $S$ is the set of real numbers $\R$, the cartesian product takes on a special significance.


The Cartesian plane is a Cartesian coordinate system of $2$ dimensions.

Every point on the plane can be identified uniquely by means of an ordered pair of real coordinates $\tuple {x, y}$, as follows:


Identify one distinct point on the plane as the origin $O$.

Select a point $P$ on the plane different from $O$.

Construct an infinite straight line through $O$ and $P$ and call it the $x$-axis.


Identify the $x$-axis with the real number line such that:
:$0$ is identified with the origin $O$
:$1$ is identified with the point $P$

The orientation of the $x$-axis is determined by the relative positions of $O$ and $P$.

It is conventional to locate $P$ to the right of $O$, so as to arrange that:

:to the right of the origin, the numbers on the $x$-axis are positive
:to the left of the origin, the numbers on the $x$-axis are negative.


Construct an infinite straight line through $O$ perpendicular to the $x$-axis and call it the $y$-axis.

Identify the point $P'$ on the $y$-axis such that $OP' = OP$.

Identify the $y$-axis with the real number line such that:
:$0$ is identified with the origin $O$
:$1$ is identified with the point $P'$


The orientation of the $y$-axis is determined by the position of $P'$ relative to $O$.

It is conventional to locate $P'$ such that, if one were to imagine being positioned at $O$ and facing along the $x$-axis towards $P$, then $P'$ is on the left. 

Hence with the conventional orientation of the $x$-axis as horizontal and increasing to the right:

:going vertically ""up"" the page or screen from the origin, the numbers on the $y$-axis are positive
:going vertically ""down"" the page or screen from the origin, the numbers on the $y$-axis are negative.


=== Cartesian Coordinate Pair ===


Hence:
:the point $P$ is identified with the coordinates $\tuple {1, 0}$
:the point $P'$ is identified with the coordinates $\tuple {0, 1}$.


=== $x$ Coordinate ===
Consider a Cartesian coordinate system $C$ with an $x$-axis.

Let a point $Q$ be positioned in $C$.


Let $x$ be the length of the line segment from the origin $O$ to the foot of the perpendicular from $Q$ to the $x$-axis.

Then $x$ is known as the $x$ coordinate.

If $Q$ is in the positive direction along the real number line that is the $x$-axis, then $x$ is positive.

If $Q$ is in the negative direction along the real number line that is the $x$-axis, then $x$ is negative.

=== $y$ Coordinate ===
Consider a Cartesian coordinate system $C$ with a $y$-axis.

Let a point $Q$ be positioned in $C$.


Let $y$ be the length of the line segment from the origin $O$ to the foot of the perpendicular from $Q$ to the $y$-axis.

Then $y$ is known as the $y$ coordinate.

If $Q$ is in the positive direction along the real number line that is the $y$-axis, then $y$ is positive.

If $Q$ is in the negative direction along the real number line that is the $y$-axis, then $y$ is negative.

=== Three Dimensions ===

$n = 3$ is another special case:

Let $S$ be a set.

The cartesian $3$rd power of $S$ is:

:$S^3 = S \times S \times S = \set {\tuple {x_1, x_2, x_3}: x_1, x_2, x_3 \in S}$


The set $S^3$ called a cartesian space of $3$ dimensions.


=== Cartesian 3-Space ===

When $S$ is the set of real numbers $\R$, the cartesian product takes on a special significance.

The Cartesian $3$-space is a Cartesian coordinate system of $3$ dimensions.


=== Definition by Axes ===


Every point in ordinary $3$-space can be identified uniquely by means of an ordered triple of real coordinates  $\tuple {x, y, z}$, as follows:

Construct a Cartesian plane, with origin $O$ and axes identified as the $x$-axis and $y$-axis.

Recall the identification of the point $P$ with the coordinate pair $\tuple {1, 0}$ in the $x$-$y$ plane.


Construct an infinite straight line through $O$ perpendicular to both the $x$-axis and the$y$-axis and call it the $z$-axis.

Identify the point $P' '$ on the $z$-axis such that $OP' ' = OP$.

Identify the $z$-axis with the real number line such that:
:$0$ is identified with the origin $O$
:$1$ is identified with the point $P$


=== Orientation ===


Consider a Cartesian $3$-Space.

Let the $x$-axis, $y$-axis and $z$-axis be defined.


Let a point $P$ be identified on the $x$-axis, different from $O$, with the coordinate pair $\tuple {1, 0}$ in the $x$-$y$ plane.

Let the point $P'$ be identified on the $y$-axis such that $OP' = OP$.


It remains to identify the point $P'  '$ on the $z$-axis such that $OP' ' = OP$.


==== Right-Handed ====


==== Left-Handed ====


Category:Definitions/Cartesian Coordinate Systems

=== Cartesian Coordinate Triple ===


=== Definition by Planes ===


Every point in ordinary $3$-space can be identified uniquely by means of an ordered triple of real coordinates  $\tuple {x, y, z}$, as follows:

Identify one distinct point in space as the origin $O$.

Let $3$ distinct planes be constructed through $O$ such that all are perpendicular.

Each pair of these $3$ planes intersect in a straight line that passes through $O$.

Let $X$, $Y$ and $Z$ be points, other than $O$, one on each of these $3$ lines of intersection.


Then the lines $OX$, $OY$ and $OZ$ are named the $x$-axis, $y$-axis and $z$-axis respectively.


Select a point $P$ on the $x$-axis different from $O$.

Let $P$ be identified with the coordinate pair $\tuple {1, 0}$ in the $x$-$y$ plane.

Identify the point $P'$ on the $y$-axis such that $OP' = OP$.

Identify the point $P' '$ on the $z$-axis such that $OP' ' = OP$.


=== Orientation ===


Consider a Cartesian $3$-Space.

Let the $x$-axis, $y$-axis and $z$-axis be defined.


Let a point $P$ be identified on the $x$-axis, different from $O$, with the coordinate pair $\tuple {1, 0}$ in the $x$-$y$ plane.

Let the point $P'$ be identified on the $y$-axis such that $OP' = OP$.


It remains to identify the point $P'  '$ on the $z$-axis such that $OP' ' = OP$.


==== Right-Handed ====


==== Left-Handed ====


Category:Definitions/Cartesian Coordinate Systems

=== Coordinate Planes ===
Consider the Cartesian $3$-space defined by $3$ distinct perpendicular planes through the origin $O$.

These $3$ planes are known as the coordinate planes of the Cartesian $3$-space.


=== $x$-$y$ Plane ===


=== $y$-$z$ Plane ===


=== $x$-$z$ Plane ===


=== Cartesian Coordinate Triple ===


=== Orientation ===


Consider a Cartesian $3$-Space.

Let the $x$-axis, $y$-axis and $z$-axis be defined.


Let a point $P$ be identified on the $x$-axis, different from $O$, with the coordinate pair $\tuple {1, 0}$ in the $x$-$y$ plane.

Let the point $P'$ be identified on the $y$-axis such that $OP' = OP$.


It remains to identify the point $P'  '$ on the $z$-axis such that $OP' ' = OP$.


==== Right-Handed ====


Consider a Cartesian $3$-Space.

Let the $x$-axis, $y$-axis and $z$-axis be defined.


Let a point $P$ be identified on the $x$-axis, different from $O$, with the coordinate pair $\tuple {1, 0}$ in the $x$-$y$ plane.

Let the point $P'$ be identified on the $y$-axis such that $OP' = OP$.

It remains to identify the point $P' '$ on the $z$-axis such that $OP' ' = OP$.

The orientation of the $z$-axis is determined by the position of $P' '$ relative to $O$.


The Cartesian $3$-Space is defined as right-handed when $P' '$ is located as follows.

Let the coordinate axes be oriented as follows:

Imagine being positioned, standing on the $x$-$y$ plane at $O$, and facing along the $x$-axis towards $P$, with $P'$ on the left. 

Then $P' '$ is then one unit above the $x$-$y$ plane.


Hence, let the coordinate axes be oriented as follows:

:Let the $x$-axis increase from West to East.
:Let the $y$-axis increase from South to North.

Then the $z$-axis increases from below to above.


Simiarly, let the $x$-$y$ plane be identified with the plane of the page or screen such aligned perpendicular to the line of sight such that:

:the $x$-axis increases from left to right.
:the $y$-axis increases from bottom to top.

Then the $z$-axis increases from behind to in front (that is, from further away to closer in).


Category:Definitions/Cartesian Coordinate Systems

==== Left-Handed ====


Consider a Cartesian $3$-Space.

Let the $x$-axis, $y$-axis and $z$-axis be defined.


Let a point $P$ be identified on the $x$-axis, different from $O$, with the coordinate pair $\tuple {1, 0}$ in the $x$-$y$ plane.

Let the point $P'$ be identified on the $y$-axis such that $OP' = OP$.

It remains to identify the point $P' '$ on the $z$-axis such that $OP' ' = OP$.

The orientation of the $z$-axis is determined by the position of $P' '$ relative to $O$.


The Cartesian $3$-Space is defined as left-handed when $P' '$ is located as follows.

Let the coordinate axes be oriented as follows:

Imagine being positioned, standing on the $x$-$y$ plane at $O$, and facing along the $x$-axis towards $P$, with $P'$ on the left. 

$P' '$ is then one unit below the $x$-$y$ plane.


Hence, let the coordinate axes be oriented as follows:

:Let the $x$-axis increase from West to East.
:Let the $y$-axis increase from South to North.

Then the $z$-axis increases from above to below.


Simiarly, let the $x$-$y$ plane be identified with the plane of the page or screen such aligned perpendicular to the line of sight such that:

:the $x$-axis increases from left to right.
:the $y$-axis increases from bottom to top.

Then the $z$-axis increases from in front to behind (that is, from closer in to further away).

Category:Definitions/Cartesian Coordinate Systems

Category:Definitions/Cartesian Coordinate Systems

=== Cartesian Coordinate Triple ===


Hence:
:the point $P$ is identified with the coordinates $\tuple {1, 0, 0}$
:the point $P'$ is identified with the coordinates $\tuple {0, 1, 0}$.
:the point $P' '$ is identified with the coordinates $\tuple {0, 0, 1}$.

=== Family of Sets ===
Let $I$ be an indexing set.

Let $\family {S_i}_{i \mathop \in I}$ be a family of sets indexed by $I$.

Let $\ds \prod_{i \mathop \in I} S_i$ be the Cartesian product of $\family {S_i}_{i \mathop \in I}$.

Let $S$ be a set such that:
:$\forall i \in I: S_i = S$


=== Definition 1 ===
Let $I$ be an indexing set.

Let $\family {S_i}_{i \mathop \in I}$ be an family of sets indexed by $I$.

Let $\ds \prod_{i \mathop \in I} S_i$ be the Cartesian product of $\family {S_i}_{i \mathop \in I}$.

Let $S$ be a set such that:
:$\forall i \in I: S_i = S$


The Cartesian space of $S$ indexed by $I$ is the set of all families $\family {s_i}_{i \mathop \in I}$ with $s_i \in S$ for each $i \in I$:
:$S_I := \ds \prod_I S = \set {\family {s_i}_{i \mathop \in I}: s_i \in S}$

=== Definition 2 ===
Let $I$ be an indexing set.

Let $\family {S_i}_{i \mathop \in I}$ be an family of sets indexed by $I$.

Let $\ds \prod_{i \mathop \in I} S_i$ be the Cartesian product of $\family {S_i}_{i \mathop \in I}$.

Let $S$ be a set such that:
:$\forall i \in I: S_i = S$


The Cartesian space of $S$ indexed by $I$ is defined and denoted as:
:$\ds S^I := \set {f: \paren {f: I \to S} \land \paren {\forall i \in I: \paren {\map f i \in S} } }$

=== Real Cartesian Space ===

When $S$ is the set of real numbers $\R$, the cartesian product takes on a special significance.

Let $n \in \N_{>0}$.

Then $\R^n$ is the cartesian product defined as follows:

:$\ds \R^n = \underbrace {\R \times \R \times \cdots \times \R}_{\text {$n$ times} } = \prod_{k \mathop = 1}^n \R$


Similarly, $\R^n$ can be defined as the set of all real $n$-tuples:

:$\R^n = \set {\tuple {x_1, x_2, \ldots, x_n}: x_1, x_2, \ldots, x_n \in \R}$


=== Cartesian Plane ===

The Cartesian plane is a Cartesian coordinate system of $2$ dimensions.

Every point on the plane can be identified uniquely by means of an ordered pair of real coordinates $\tuple {x, y}$, as follows:


Identify one distinct point on the plane as the origin $O$.

Select a point $P$ on the plane different from $O$.

Construct an infinite straight line through $O$ and $P$ and call it the $x$-axis.


Identify the $x$-axis with the real number line such that:
:$0$ is identified with the origin $O$
:$1$ is identified with the point $P$

The orientation of the $x$-axis is determined by the relative positions of $O$ and $P$.

It is conventional to locate $P$ to the right of $O$, so as to arrange that:

:to the right of the origin, the numbers on the $x$-axis are positive
:to the left of the origin, the numbers on the $x$-axis are negative.


Construct an infinite straight line through $O$ perpendicular to the $x$-axis and call it the $y$-axis.

Identify the point $P'$ on the $y$-axis such that $OP' = OP$.

Identify the $y$-axis with the real number line such that:
:$0$ is identified with the origin $O$
:$1$ is identified with the point $P'$


The orientation of the $y$-axis is determined by the position of $P'$ relative to $O$.

It is conventional to locate $P'$ such that, if one were to imagine being positioned at $O$ and facing along the $x$-axis towards $P$, then $P'$ is on the left. 

Hence with the conventional orientation of the $x$-axis as horizontal and increasing to the right:

:going vertically ""up"" the page or screen from the origin, the numbers on the $y$-axis are positive
:going vertically ""down"" the page or screen from the origin, the numbers on the $y$-axis are negative.


=== Cartesian Coordinate Pair ===


Hence:
:the point $P$ is identified with the coordinates $\tuple {1, 0}$
:the point $P'$ is identified with the coordinates $\tuple {0, 1}$.


=== $x$ Coordinate ===
Consider a Cartesian coordinate system $C$ with an $x$-axis.

Let a point $Q$ be positioned in $C$.


Let $x$ be the length of the line segment from the origin $O$ to the foot of the perpendicular from $Q$ to the $x$-axis.

Then $x$ is known as the $x$ coordinate.

If $Q$ is in the positive direction along the real number line that is the $x$-axis, then $x$ is positive.

If $Q$ is in the negative direction along the real number line that is the $x$-axis, then $x$ is negative.

=== $y$ Coordinate ===
Consider a Cartesian coordinate system $C$ with a $y$-axis.

Let a point $Q$ be positioned in $C$.


Let $y$ be the length of the line segment from the origin $O$ to the foot of the perpendicular from $Q$ to the $y$-axis.

Then $y$ is known as the $y$ coordinate.

If $Q$ is in the positive direction along the real number line that is the $y$-axis, then $y$ is positive.

If $Q$ is in the negative direction along the real number line that is the $y$-axis, then $y$ is negative.

=== Cartesian 3-Space ===
The Cartesian $3$-space is a Cartesian coordinate system of $3$ dimensions.


=== Definition by Axes ===


Every point in ordinary $3$-space can be identified uniquely by means of an ordered triple of real coordinates  $\tuple {x, y, z}$, as follows:

Construct a Cartesian plane, with origin $O$ and axes identified as the $x$-axis and $y$-axis.

Recall the identification of the point $P$ with the coordinate pair $\tuple {1, 0}$ in the $x$-$y$ plane.


Construct an infinite straight line through $O$ perpendicular to both the $x$-axis and the$y$-axis and call it the $z$-axis.

Identify the point $P' '$ on the $z$-axis such that $OP' ' = OP$.

Identify the $z$-axis with the real number line such that:
:$0$ is identified with the origin $O$
:$1$ is identified with the point $P$


=== Orientation ===


Consider a Cartesian $3$-Space.

Let the $x$-axis, $y$-axis and $z$-axis be defined.


Let a point $P$ be identified on the $x$-axis, different from $O$, with the coordinate pair $\tuple {1, 0}$ in the $x$-$y$ plane.

Let the point $P'$ be identified on the $y$-axis such that $OP' = OP$.


It remains to identify the point $P'  '$ on the $z$-axis such that $OP' ' = OP$.


==== Right-Handed ====


==== Left-Handed ====


Category:Definitions/Cartesian Coordinate Systems

=== Cartesian Coordinate Triple ===


=== Definition by Planes ===


Every point in ordinary $3$-space can be identified uniquely by means of an ordered triple of real coordinates  $\tuple {x, y, z}$, as follows:

Identify one distinct point in space as the origin $O$.

Let $3$ distinct planes be constructed through $O$ such that all are perpendicular.

Each pair of these $3$ planes intersect in a straight line that passes through $O$.

Let $X$, $Y$ and $Z$ be points, other than $O$, one on each of these $3$ lines of intersection.


Then the lines $OX$, $OY$ and $OZ$ are named the $x$-axis, $y$-axis and $z$-axis respectively.


Select a point $P$ on the $x$-axis different from $O$.

Let $P$ be identified with the coordinate pair $\tuple {1, 0}$ in the $x$-$y$ plane.

Identify the point $P'$ on the $y$-axis such that $OP' = OP$.

Identify the point $P' '$ on the $z$-axis such that $OP' ' = OP$.


=== Orientation ===


Consider a Cartesian $3$-Space.

Let the $x$-axis, $y$-axis and $z$-axis be defined.


Let a point $P$ be identified on the $x$-axis, different from $O$, with the coordinate pair $\tuple {1, 0}$ in the $x$-$y$ plane.

Let the point $P'$ be identified on the $y$-axis such that $OP' = OP$.


It remains to identify the point $P'  '$ on the $z$-axis such that $OP' ' = OP$.


==== Right-Handed ====


==== Left-Handed ====


Category:Definitions/Cartesian Coordinate Systems

=== Coordinate Planes ===
Consider the Cartesian $3$-space defined by $3$ distinct perpendicular planes through the origin $O$.

These $3$ planes are known as the coordinate planes of the Cartesian $3$-space.


=== $x$-$y$ Plane ===


=== $y$-$z$ Plane ===


=== $x$-$z$ Plane ===


=== Cartesian Coordinate Triple ===


=== Orientation ===


Consider a Cartesian $3$-Space.

Let the $x$-axis, $y$-axis and $z$-axis be defined.


Let a point $P$ be identified on the $x$-axis, different from $O$, with the coordinate pair $\tuple {1, 0}$ in the $x$-$y$ plane.

Let the point $P'$ be identified on the $y$-axis such that $OP' = OP$.


It remains to identify the point $P'  '$ on the $z$-axis such that $OP' ' = OP$.


==== Right-Handed ====


Consider a Cartesian $3$-Space.

Let the $x$-axis, $y$-axis and $z$-axis be defined.


Let a point $P$ be identified on the $x$-axis, different from $O$, with the coordinate pair $\tuple {1, 0}$ in the $x$-$y$ plane.

Let the point $P'$ be identified on the $y$-axis such that $OP' = OP$.

It remains to identify the point $P' '$ on the $z$-axis such that $OP' ' = OP$.

The orientation of the $z$-axis is determined by the position of $P' '$ relative to $O$.


The Cartesian $3$-Space is defined as right-handed when $P' '$ is located as follows.

Let the coordinate axes be oriented as follows:

Imagine being positioned, standing on the $x$-$y$ plane at $O$, and facing along the $x$-axis towards $P$, with $P'$ on the left. 

Then $P' '$ is then one unit above the $x$-$y$ plane.


Hence, let the coordinate axes be oriented as follows:

:Let the $x$-axis increase from West to East.
:Let the $y$-axis increase from South to North.

Then the $z$-axis increases from below to above.


Simiarly, let the $x$-$y$ plane be identified with the plane of the page or screen such aligned perpendicular to the line of sight such that:

:the $x$-axis increases from left to right.
:the $y$-axis increases from bottom to top.

Then the $z$-axis increases from behind to in front (that is, from further away to closer in).


Category:Definitions/Cartesian Coordinate Systems

==== Left-Handed ====


Consider a Cartesian $3$-Space.

Let the $x$-axis, $y$-axis and $z$-axis be defined.


Let a point $P$ be identified on the $x$-axis, different from $O$, with the coordinate pair $\tuple {1, 0}$ in the $x$-$y$ plane.

Let the point $P'$ be identified on the $y$-axis such that $OP' = OP$.

It remains to identify the point $P' '$ on the $z$-axis such that $OP' ' = OP$.

The orientation of the $z$-axis is determined by the position of $P' '$ relative to $O$.


The Cartesian $3$-Space is defined as left-handed when $P' '$ is located as follows.

Let the coordinate axes be oriented as follows:

Imagine being positioned, standing on the $x$-$y$ plane at $O$, and facing along the $x$-axis towards $P$, with $P'$ on the left. 

$P' '$ is then one unit below the $x$-$y$ plane.


Hence, let the coordinate axes be oriented as follows:

:Let the $x$-axis increase from West to East.
:Let the $y$-axis increase from South to North.

Then the $z$-axis increases from above to below.


Simiarly, let the $x$-$y$ plane be identified with the plane of the page or screen such aligned perpendicular to the line of sight such that:

:the $x$-axis increases from left to right.
:the $y$-axis increases from bottom to top.

Then the $z$-axis increases from in front to behind (that is, from closer in to further away).

Category:Definitions/Cartesian Coordinate Systems

Category:Definitions/Cartesian Coordinate Systems

=== Cartesian Coordinate Triple ===


Hence:
:the point $P$ is identified with the coordinates $\tuple {1, 0, 0}$
:the point $P'$ is identified with the coordinates $\tuple {0, 1, 0}$.
:the point $P' '$ is identified with the coordinates $\tuple {0, 0, 1}$.

=== Countable-Dimensional Real Cartesian Space ===
The countable cartesian product defined as:
:$\ds \R^\omega := \R \times \R \times \cdots = \prod_\N \R$

is called the countable-dimensional real cartesian space.

Thus, $\R^\omega$ can be defined as the set of all real sequences:

:$\R^\omega = \set {\sequence {x_1, x_2, \ldots}: x_1, x_2, \ldots \in \R}$
 ",Definition:Cartesian Product/Cartesian Space,['Definitions/Cartesian Product']
Definition:Space,Space,"Let $S$ be a set.

Let $\tau$ be a topology on $S$.

That is, let $\tau \subseteq \powerset S$ satisfy the open set axioms:
 

Then the ordered pair $\struct {S, \tau}$ is called a topological space.

The elements of $\tau$ are called open sets of $\struct {S, \tau}$.


In a topological space $\struct {S, \tau}$, we consider $S$ to be the universal set.",Definition:Topological Space,"['Definitions/Topological Spaces', 'Definitions/Topology', 'Definitions/Abstract Spaces']"
Definition:Space,Space,"Let $T = \struct {S, \tau}$ be a topological space.


=== Definition 1 ===

Let $T = \struct {S, \tau}$ be a topological space.


$\struct {S, \tau}$ is a Hausdorff space or $T_2$ space  if and only if :
:$\forall x, y \in S, x \ne y: \exists U, V \in \tau: x \in U, y \in V: U \cap V = \O$ 

That is:
:for any two distinct elements $x, y \in S$ there exist disjoint open sets $U, V \in \tau$ containing $x$ and $y$ respectively.


That is:
:$\struct {S, \tau}$ is a $T_2$ space  if and only if  every two elements in $S$ are separated by open sets.

=== Definition 2 ===
Let $T = \struct {S, \tau}$ be a topological space.


$\struct {S, \tau}$ is a Hausdorff space or $T_2$ space  if and only if  each point of $S$ is the intersection of all its closed neighborhoods.

=== Definition 3 ===
Let $T = \struct {S, \tau}$ be a topological space.


$\struct {S, \tau}$ is a Hausdorff space or $T_2$ space  if and only if :
:$\forall x, y \in S, x \ne y: \exists N_x, N_y \subseteq S: \exists U, V \in \tau: x \in U \subseteq N_x, y \in V \subseteq N_y: N_x \cap N_y = \O$

That is:
:for any two distinct elements $x, y \in S$ there exist disjoint neighborhoods $N_x, N_y \subseteq S$ containing $x$ and $y$ respectively.


That is:
:$\struct {S, \tau}$ is a $T_2$ space  if and only if  every two elements in $S$ are separated by neighborhoods.",Definition:Hausdorff Space,"['Definitions/Separation Axioms', 'Definitions/Hausdorff Spaces']"
Definition:Space,Space,"=== Definition 1 ===
Let $\struct {K, +_K, \times_K}$ be a field.

Let $\struct {G, +_G}$ be an abelian group.

Let $\struct {G, +_G, \circ}_K$ be a unitary $K$-module.


Then $\struct {G, +_G, \circ}_K$ is a vector space over $K$ or a $K$-vector space.


That is, a vector space is a unitary module whose scalar ring is a field.

=== Definition 2 ===
Let $\struct {K, +_K, \times_K}$ be a field whose unity is $1_K$.

Let $\struct {G, +_G}$ be an abelian group.

Let $\struct {\map {\mathrm {End} } G, +, \circ}$ be the endomorphism ring of $\struct {G, +_G}$ such that $I_G$ is the identity mapping.

Let $\cdot: \struct {K, +_K, \times_K} \to \struct {\map {\mathrm {End} } G, +, \circ}$ be a ring homomorphism from $K$ to $\map {\mathrm {End} } G$ which maps $1_K$ to $I_G$.


Then $\struct {G, +_G, \cdot, K}$ is a vector space over $K$ or a $K$-vector space.


=== Vector Space Axioms ===
The vector space axioms are the defining properties of a vector space.

Let $\struct {G, +_G, \circ}_K$ be a vector space over $K$ where:

:$G$ is a set of objects, called vectors.

:$+_G: G \times G \to G$ is a binary operation on $G$

:$\struct {K, +, \cdot}$ is a division ring whose unity is $1_K$

:$\circ: K \times G \to G$ is a binary operation

The usual situation is for $K$ to be one of the standard number fields $\R$ or $\C$.


The vector space axioms consist of the abelian group axioms:

 
 
 
 
 
 
 


together with the properties of a unitary module:

 
 
 
 
 
 

=== Vector ===
Let $V = \struct {G, +_G, \circ}_K$ be a vector space over $K$, where:

:$\struct {G, +_G}$ is an abelian group

:$\struct {K, +_K, \times_K}$ is the scalar field of $V$.


The elements of the abelian group $\struct {G, +_G}$ are called vectors.

=== Zero Vector ===
Let $\struct {R, +_R, \times_R}$ be a ring.

Let $\struct {G, +_G}$ be an abelian group.

Let $\struct {G, +_G, \circ}_R$ be an $R$-module.


The identity of $\struct {G, +_G}$ is usually denoted $\bszero$, or some variant of this, and called the zero vector:

:$\forall \mathbf a \in \struct {G, +_G, \circ}_R: \bszero +_G \mathbf a = \mathbf a = \mathbf a +_G \bszero$


Note that on occasion it is advantageous to denote the zero vector differently, for example by $e$, or $\bszero_V$ or $\bszero_G$, in order to highlight the fact that the zero vector is not the same object as the zero scalar.


=== Zero Vector in $\R^n$ ===
Let $\struct {\R^n, +, \times}_\R$ be a real vector space.

The zero vector in $\struct {\R^n, +, \times}_\R$ is:

:$\mathbf 0_{n \times 1} := \begin {bmatrix} 0 \\ 0 \\ \vdots \\ 0 \end {bmatrix}$

where $0 \in \R$.

=== Zero Vector Quantity ===
A vector quantity whose magnitude is zero is referred to as a zero vector.

=== Vector Space Axioms ===
The vector space axioms are the defining properties of a vector space.

Let $\struct {G, +_G, \circ}_K$ be a vector space over $K$ where:

:$G$ is a set of objects, called vectors.

:$+_G: G \times G \to G$ is a binary operation on $G$

:$\struct {K, +, \cdot}$ is a division ring whose unity is $1_K$

:$\circ: K \times G \to G$ is a binary operation

The usual situation is for $K$ to be one of the standard number fields $\R$ or $\C$.


The vector space axioms consist of the abelian group axioms:

 
 
 
 
 
 
 


together with the properties of a unitary module:

 
 
 
 
 
 

=== Vector ===
Let $V = \struct {G, +_G, \circ}_K$ be a vector space over $K$, where:

:$\struct {G, +_G}$ is an abelian group

:$\struct {K, +_K, \times_K}$ is the scalar field of $V$.


The elements of the abelian group $\struct {G, +_G}$ are called vectors.

=== Zero Vector ===
Let $\struct {R, +_R, \times_R}$ be a ring.

Let $\struct {G, +_G}$ be an abelian group.

Let $\struct {G, +_G, \circ}_R$ be an $R$-module.


The identity of $\struct {G, +_G}$ is usually denoted $\bszero$, or some variant of this, and called the zero vector:

:$\forall \mathbf a \in \struct {G, +_G, \circ}_R: \bszero +_G \mathbf a = \mathbf a = \mathbf a +_G \bszero$


Note that on occasion it is advantageous to denote the zero vector differently, for example by $e$, or $\bszero_V$ or $\bszero_G$, in order to highlight the fact that the zero vector is not the same object as the zero scalar.


=== Zero Vector in $\R^n$ ===
Let $\struct {\R^n, +, \times}_\R$ be a real vector space.

The zero vector in $\struct {\R^n, +, \times}_\R$ is:

:$\mathbf 0_{n \times 1} := \begin {bmatrix} 0 \\ 0 \\ \vdots \\ 0 \end {bmatrix}$

where $0 \in \R$.

=== Zero Vector Quantity ===
A vector quantity whose magnitude is zero is referred to as a zero vector.",Definition:Vector Space,"['Definitions/Vector Spaces', 'Definitions/Vector Algebra', 'Definitions/Linear Algebra', 'Definitions/Abstract Spaces']"
Definition:Space,Space,"Let $R$ be a ring.

Let:

:$\mathbf A_{m \times n} = \begin{bmatrix}
a_{1 1} & a_{1 2} & \cdots & a_{1 n} \\
a_{2 1} & a_{2 2} & \cdots & a_{2 n} \\
\vdots & \vdots & \ddots & \vdots \\
a_{m 1} & a_{m 2} & \cdots & a_{m n} \\
\end{bmatrix}$

be a matrix over $R$ such that every column is defined as a vector:

:$\forall i: 1 \le i \le m: \begin {bmatrix} a_{1 i} \\ a_{2 i} \\ \vdots \\ a_{m i} \end {bmatrix} \in \mathbf V$

where $\mathbf V$ is some vector space.


Then the column space of $\mathbf A$ is the linear span of all such column vectors:

:$\map {\mathrm C} {\mathbf A} = \map \span {\begin {bmatrix} a_{1 1} \\ a_{2 1} \\ \vdots \\ a_{m 1} \end {bmatrix}, \begin {bmatrix} a_{1 2} \\ a_{2 2} \\ \vdots \\ a_{m 2} \end {bmatrix}, \cdots, \begin {bmatrix} a_{1 n} \\ a_{2 n} \\ \vdots \\ a_{m n} \end {bmatrix} }$",Definition:Column Space,"['Definitions/Column Space', 'Definitions/Matrix Theory', 'Definitions/Linear Algebra']"
Definition:Space,Space,"Let $R$ be a ring.

Let $\mathbf A$ be a matrix over $R$.

Let $\mathbf A^\intercal$ be the transpose of $\mathbf A$.
 
Let the columns of $\mathbf A^\intercal$ be members of a vector space.

The row space of $\mathbf A$ is defined as the column space of $\mathbf A^\intercal$.",Definition:Row Space,['Definitions/Linear Algebra']
Definition:Space,Space,"Let:
$\quad \mathbf A_{m \times n} = \begin {bmatrix}
a_{11} & a_{12} & \cdots & a_{1n} \\
a_{21} & a_{22} & \cdots & a_{2n} \\
\vdots & \vdots & \ddots & \vdots \\
a_{m1} & a_{m2} & \cdots & a_{mn} \\
\end {bmatrix}$,  $\mathbf x_{n \times 1} = \begin {bmatrix} x_1 \\ x_2 \\ \vdots \\ x_n \end {bmatrix}$, $\mathbf 0_{m \times 1} = \begin {bmatrix} 0 \\ 0 \\ \vdots \\ 0 \end {bmatrix}$

be matrices where each column is a member of a real vector space.

The set of all solutions to $\mathbf A \mathbf x = \mathbf 0$:

:$\map {\mathrm N} {\mathbf A} = \set {\mathbf x \in \R^n : \mathbf {A x} = \mathbf 0}$

is called the null space of $\mathbf A$.


 ",Definition:Null Space,"['Definitions/Null Spaces', 'Definitions/Linear Algebra']"
Definition:Space,Space,"Let $\mathbb K$ be a field.


A bilinear space over $\mathbb K$ is a pair $\struct {V, f}$ where:
:$V$ be a vector space over $\mathbb K$ of finite dimension $n > 0$
:$f$ is a bilinear form on $V$.",Definition:Bilinear Space,['Definitions/Bilinear Forms (Linear Algebra)']
Definition:Space,Space,"Let $\mathbb K$ be a field.


A quadratic space over $\mathbb K$ is a pair $\tuple {V, q}$ where:
:$V$ is a vector space over $\mathbb K$ of finite dimension $n > 0$
:$q$ is a quadratic form on $V$.",Definition:Quadratic Space,['Definitions/Quadratic Forms (Linear Algebra)']
Definition:Space,Space,"Ordinary space (or just space) is a word used to mean the universe we live in.

The intuitive belief is that space is $3$-dimensional and therefore isomorphic to the real vector space $\R^3$.


Hence ordinary space is usually taken as an alternative term for Euclidean $3$-dimensional space.",Definition:Ordinary Space,"['Definitions/Ordinary Space', 'Definitions/Geometry', 'Definitions/Projective Geometry', 'Definitions/Physics']"
Definition:Space,Space,"Let $\R$ be the set of real numbers.


Then the $\R$-module $\R^n$ is called the real ($n$-dimensional) vector space.",Definition:Real Vector Space,"['Definitions/Real Vector Spaces', 'Definitions/Examples of Vector Spaces', 'Definitions/Real Analysis', 'Definitions/Analytic Geometry']"
Definition:Spanning Set,Spanning Set,"Let $R$ be a ring.

Let $M$ be an $R$-module.

Let $S \subseteq M$ be a subset.


=== Definition 1 ===
Let $R$ be a ring.

Let $M$ be an $R$-module.

Let $S \subseteq M$ be a subset.


$S$ is a generator of $M$  if and only if  $M$ is the submodule generated by $S$.

=== Definition 2 ===
Let $R$ be a ring.

Let $M$ be an $R$-module.

Let $S \subseteq M$ be a subset.


$S$ is a generator of $M$  if and only if  $M$ has no proper submodule containing $S$.",Definition:Generator of Module,"['Definitions/Generators of Modules', 'Definitions/Module Theory', 'Definitions/Linear Algebra']"
Definition:Spanning Set,Spanning Set,"Let $K$ be a division ring.

Let $\mathbf V$ be a vector space over $K$.

Let $S \subseteq \mathbf V$ be a subset of $\mathbf V$.


$S$ is a generator of $\mathbf V$  if and only if  every element of $\mathbf V$ is a linear combination of elements of $S$.",Definition:Generator of Vector Space,"['Definitions/Generators of Vector Spaces', 'Definitions/Vector Spaces', 'Definitions/Linear Algebra']"
Definition:Spectrum,Spectrum,"Let $A$ be a commutative ring with unity.


The prime spectrum of $A$ is the set of prime ideals $\mathfrak p$ of $A$:

:$\Spec A = \set {\mathfrak p \lhd A: \mathfrak p \text{ is prime} }$

where $\mathfrak p \lhd A$ indicates that $\mathfrak p$ is an ideal of $A$.",Definition:Prime Spectrum of Ring,['Definitions/Commutative Algebra']
Definition:Spectrum,Spectrum,"Let $A$ be a commutative ring with unity.


The maximal spectrum of $A$ is the set of maximal ideals of $A$:

:$\operatorname{Max} \: \Spec A = \set {\mathfrak m \lhd A : \mathfrak m \text { is maximal} }$

where $I \lhd A$ indicates that $I$ is an ideal of $A$.


The notation $\operatorname {Max} \: \Spec A$ is also a shorthand for the locally ringed space
:$\struct {\operatorname {Max} \: \Spec A, \tau, \OO_{\map {\operatorname {Max Spec} } A} }$
where:
:$\tau$ is the Zariski topology on $\map {\operatorname {Max Spec} } A$
:$\OO_{\map {\operatorname {Max Spec} } A}$ is the structure sheaf of $\map {\operatorname {Max Spec} } A$",Definition:Maximal Spectrum of Ring,['Definitions/Commutative Algebra']
Definition:Spectrum,Spectrum,"Let $\struct {X, \norm \cdot_X}$ be a Banach space over $\C$. 

Let $A : X \to X$ be a bounded linear operator.

Let $\map \rho A$ be the resolvent set of $A$. 

Let: 

:$\map \sigma A = \C \setminus \map \rho A$


We say that $\map \sigma A$ is the spectrum of $A$.",Definition:Spectrum (Spectral Theory)/Bounded Linear Operator,"['Spectra (Spectral Theory)', 'Definitions/Spectra (Spectral Theory)', 'Definitions/Bounded Linear Operators', 'Definitions/Banach Spaces', 'Definitions/Spectra (Spectral Theory)']"
Definition:State,State,"Let $G$ be a game whose outcome is determined by the realization of a random variable $X$.

Each of the possible values that can be taken by $X$ is known as a state of $G$.",Definition:State of Game,['Definitions/Game Theory']
Definition:State,State,"Matter can be in one of the following states:

=== Solid ===
Solid is one of the fundamental states of matter.

A body is solid  if and only if  it retains its volume and has a well-definable shape without the need for a container.

=== Liquid ===
Liquid is one of the fundamental states of matter.

A body is liquid  if and only if  it retains its volume, but in order to maintain a specific shape it needs a container.

=== Gas ===
Gas is one of the fundamental states of matter.

A body is gas  if and only if  it can be contained only if it is fully surrounded by a solid, or in a bubble of liquid, or held together by gravitational pull.

=== Plasma ===
Plasma is one of the fundamental states of matter.

A body is plasma  if and only if  it consists of partially ionized gas and electrons.",Definition:State of Matter,"['Definitions/States of Matter', 'Definitions/Matter']"
Definition:Strong,Strong,"In a conditional $p \implies q$, the statement $p$ is stronger than $q$.",Definition:Conditional/Language of Conditional/Strong,['Definitions/Conditional']
Definition:Strong,Strong,"Let $T = \left({S, \tau}\right)$ be a topological space.


=== Definition 1 ===
Let $T = \struct {S, \tau}$ be a topological space.


The space $T$ is strongly locally compact  if and only if :
:every point of $S$ is contained in an open set whose closure is compact.

=== Definition 2 ===
Let $T = \left({S, \tau}\right)$ be a topological space.


The space $T$ is strongly locally compact  if and only if :
:every point has a closed compact neighborhood.
That is:
:every point of $S$ is contained in an open set which is contained in a closed compact subspace.",Definition:Strongly Locally Compact Space,['Definitions/Compact Spaces']
Definition:Structure,Structure,"Let $\LL$ be a formal language.

Part of specifying a formal semantics $\mathscr M$ for $\LL$ is to specify structures $\MM$ for $\mathscr M$.


A structure can in principle be any object one can think of.

However, to get a useful formal semantics, the structures should support a meaningful definition of validity for the WFFs of $\LL$.


It is common that structures are sets, often endowed with a number of relations or functions.


=== Structure for Predicate Logic ===
Let $\LL_1$ be the language of predicate logic.


A structure $\AA$ for $\LL_1$ comprises:

:$(1): \quad$ A non-empty set $A$;
:$(2): \quad$ For each function symbol $f$ of arity $n$, a mapping $f_\AA: A^n \to A$;
:$(3): \quad$ For each predicate symbol $p$ of arity $n$, a mapping $p_\AA: A^n \to \Bbb B$

where $\Bbb B$ denotes the set of truth values.

$A$ is called the underlying set of $\AA$.

$f_\AA$ and $p_\AA$ are called the interpretations of $f$ and $p$ in $\AA$, respectively.


We remark that function symbols of arity $0$ are interpreted as constants in $A$.

To avoid pathological situations with the interpretation of arity-$0$ function symbols, it is essential that $A$ be non-empty.

Also, the predicate symbols may be interpreted as relations via their characteristic functions.


 ",Definition:Formal Semantics/Structure,['Definitions/Formal Semantics']
Definition:Structure,Structure,"Let $\LL_1$ be the language of predicate logic.


A structure $\AA$ for $\LL_1$ comprises:

:$(1): \quad$ A non-empty set $A$;
:$(2): \quad$ For each function symbol $f$ of arity $n$, a mapping $f_\AA: A^n \to A$;
:$(3): \quad$ For each predicate symbol $p$ of arity $n$, a mapping $p_\AA: A^n \to \Bbb B$

where $\Bbb B$ denotes the set of truth values.

$A$ is called the underlying set of $\AA$.

$f_\AA$ and $p_\AA$ are called the interpretations of $f$ and $p$ in $\AA$, respectively.


We remark that function symbols of arity $0$ are interpreted as constants in $A$.

To avoid pathological situations with the interpretation of arity-$0$ function symbols, it is essential that $A$ be non-empty.

Also, the predicate symbols may be interpreted as relations via their characteristic functions.


 ",Definition:Structure for Predicate Logic,"['Definitions/Predicate Logic', 'Definitions/Formal Semantics', 'Definitions/Model Theory for Predicate Logic']"
Definition:Structure,Structure,"A relational structure is an ordered pair $\struct {S, \RR}$, where:
:$S$ is a set
:$\RR$ is an endorelation on $S$.",Definition:Relational Structure,"['Definitions/Relation Theory', 'Definitions/Relational Structures']"
Definition:Structure,Structure,"An algebraic structure with $n$ operations is an ordered tuple:
:$\struct {S, \circ_1, \circ_2, \ldots, \circ_n}$
where:
:$S$ is a set
:$\circ_1, \circ_2, \ldots, \circ_n$ are $n$ binary operations which are defined on all the elements of $S \times S$.


=== One Operation ===
An algebraic structure with $1$ operation is an ordered pair:
:$\struct {S, \circ}$
where:
:$S$ is a set
:$\circ$ is a binary operation defined on all the elements of $S \times S$.

=== Two Operations ===
An algebraic structure with $2$ operations is an ordered triple:
:$\struct {S, \circ, *}$
where:
:$S$ is a set
:$\circ$ and $*$ are binary operations defined on all the elements of $S \times S$.",Definition:Algebraic Structure,"['Definitions/Abstract Algebra', 'Definitions/Algebraic Structures']"
Definition:Structure Sheaf,Structure Sheaf,"A ringed space is a pair $\struct {X, \OO_X}$ where:
:$X$ is a topological space
:$\OO_X$ is a sheaf of commutative rings with unity on $X$.



=== Structure Sheaf ===
Let $\struct {X, \OO_X}$ be a ringed space.


The structure sheaf of $\struct {X, \OO_X}$ is the term $\OO_X$.",Definition:Ringed Space,"['Definitions/Algebraic Geometry', 'Definitions/Ringed Spaces']"
Definition:Structure Sheaf,Structure Sheaf,"Let $A$ be a commutative ring with unity.

Let $\struct {\Spec A, \tau}$ be its spectrum with Zariski topology $\tau$


=== Definition 1 ===

Note that Principal Open Subsets form Basis of Zariski Topology on Prime Spectrum.

We define the structure sheaf of $\Spec A$ to be the sheaf induced by a sheaf on this basis defined as follows:
:For $f \in A$, $\map \OO {\map X f}$ is the localization of $A$ at $f$
:For $f, g \in A$ with $\map X f \supset \map X g$, the restriction is the induced homomorphism of $A$-algebras $A_f \to A_g$.


=== Definition 2 ===

 


=== Definition 3 ===


 ",Definition:Structure Sheaf of Spectrum of Ring,['Definitions/Algebraic Geometry']
Definition:Subadditive Function,Subadditive Function,"Let $\struct {S, +_S}$ and $\struct {T, +_T, \preceq}$ be semigroups such that $\struct {T, +_T, \preceq}$ is ordered.


Let $f: S \to T$ be a mapping from $S$ to $T$ which satisfies the relation:
:$\forall a, b \in S: \map f {a +_S b} \preceq \map f a +_T \map f b$


Then $f$ is defined as being subadditive.


The usual context in which this is encountered is where $S$ and $T$ are both the set of real numbers $\R$ (or a subset of them).",Definition:Subadditive Function (Conventional),"['Definitions/Subadditive Functions', 'Definitions/Abstract Algebra', 'Definitions/Analysis']"
Definition:Subadditive Function,Subadditive Function,"Let $\SS$ be an algebra of sets.

Let $f: \SS \to \overline \R$ be a function, where $\overline \R$ denotes the extended set of real numbers.


Then $f$ is defined to be subadditive (or sub-additive)  if and only if :
:$\forall S, T \in \SS: \map f {S \cup T} \le \map f S + \map f T$


That is, for any two elements of $\SS$, $f$ applied to their union is not greater than the sum of $f$ of the individual elements.",Definition:Subadditive Function (Measure Theory),"['Definitions/Set Systems', 'Definitions/Measure Theory']"
Definition:Subadditive Function,Subadditive Function,"Let $\Sigma$ be a $\sigma$-algebra over a set $X$.

Let $f: \Sigma \to \overline \R$ be a function, where $\overline \R$ denotes the set of extended real numbers.


Then $f$ is defined as countably subadditive  if and only if  for any sequence $\sequence {E_n}_{n \mathop \in \N}$ of elements of $\Sigma$:

:$\ds \map f {\bigcup_{n \mathop = 0}^\infty E_n} \le \sum_{n \mathop = 0}^\infty \map f {E_n}$",Definition:Countably Subadditive Function,"['Definitions/Set Systems', 'Definitions/Measure Theory']"
Definition:Subdivision,Subdivision,"Let $\closedint a b$ be a closed interval of the set $\R$ of real numbers.


=== Finite ===
Let $\closedint a b$ be a closed interval of the set $\R$ of real numbers.


Let $x_0, x_1, x_2, \ldots, x_{n - 1}, x_n$ be points of $\R$ such that:

:$a = x_0 < x_1 < x_2 < \cdots < x_{n - 1} < x_n = b$


Then $\set {x_0, x_1, x_2, \ldots, x_{n - 1}, x_n}$ form a finite subdivision of $\closedint a b$.


=== Normal Subdivision ===
Let $\closedint a b$ be a closed interval of the set $\R$ of real numbers.

Let $P = \set {x_0, x_1, x_2, \ldots, x_{n - 1}, x_n}$ form a (finite) subdivision of $\closedint a b$.


$P$ is a normal subdivision of $\closedint a b$  if and only if :
:the length of every interval of the form $\closedint {x_i} {x_{i + 1} }$ is the same as every other.


That is,  if and only if :

:$\exists c \in \R_{> 0}: \forall i \in \N_{< n}: x_{i + 1} - x_i = c$

=== Infinite ===
Let $\closedint a b$ be a closed interval of the set $\R$ of real numbers.


Let $x_0, x_1, x_2, \ldots$ be an infinite number of points of $\R$ such that:

:$a = x_0 < x_1 < x_2 < \cdots < x_{n - 1} < \ldots \le b$


Then $\set {x_0, x_1, x_2, \ldots}$ forms an infinite subdivision of $\closedint a b$.


 ",Definition:Subdivision (Real Analysis),"['Definitions/Real Analysis', 'Definitions/Subdivisions (Real Analysis)']"
Definition:Subdivision,Subdivision,"Let $G = \struct {V, E}$ be a graph.


=== Edge Subdivision ===
Let $G = \struct {V, E}$ be a graph.


The edge subdivision operation for an edge $\set {u, v} \in E$ is the deletion of $\set {u, v}$ from $G$ and the addition of two edges $\set {u, w}$ and $\set {w, v}$ along with the new vertex $w$. 


This operation generates a new graph $H$:
:$H = \struct {V \cup \set w, \paren {E \setminus \set {u, v} } \cup \set {\set {u, w}, \set {w, v} } }$

=== Graph Subdivision ===
Let $G = \struct {V, E}$ be a graph.


A graph which has been derived from $G$ by a sequence of edge subdivision operations is called a subdivision of $G$.",Definition:Subdivision (Graph Theory),"['Definitions/Subdivisions (Graph Theory)', 'Definitions/Graph Theory']"
Definition:Subdivision,Subdivision,"Let $G = \struct {V, E}$ be a graph.


The edge subdivision operation for an edge $\set {u, v} \in E$ is the deletion of $\set {u, v}$ from $G$ and the addition of two edges $\set {u, w}$ and $\set {w, v}$ along with the new vertex $w$. 


This operation generates a new graph $H$:
:$H = \struct {V \cup \set w, \paren {E \setminus \set {u, v} } \cup \set {\set {u, w}, \set {w, v} } }$",Definition:Subdivision (Graph Theory)/Edge,"['Definitions/Subdivisions (Graph Theory)', 'Definitions/Edges of Graphs']"
Definition:Subdivision,Subdivision,"Let $G = \struct {V, E}$ be a graph.


A graph which has been derived from $G$ by a sequence of edge subdivision operations is called a subdivision of $G$.",Definition:Subdivision (Graph Theory)/Graph,['Definitions/Subdivisions (Graph Theory)']
Definition:Subspace,Subspace,"Let $T = \struct {S, \tau}$ be a topological space.

Let $H \subseteq S$ be a non-empty subset of $S$.


Define:
:$\tau_H := \set {U \cap H: U \in \tau} \subseteq \powerset H$

where $\powerset H$ denotes the power set of $H$.


Then the topological space $T_H = \struct {H, \tau_H}$ is called a (topological) subspace of $T$.


The set $\tau_H$ is referred to as the subspace topology on $H$ (induced by $\tau$).",Definition:Topological Subspace,['Definitions/Topology']
Definition:Subspace,Subspace,"Let $\struct {A, d}$ be a metric space.

Let $H \subseteq A$.

Let $d_H: H \times H \to \R$ be the restriction $d \restriction_{H \times H}$ of $d$ to $H$.

That is, let $\forall x, y \in H: \map {d_H} {x, y} = \map d {x, y}$.


Then $d_H$ is the metric induced on $H$ by $d$ or the subspace metric of $d$ (with respect to $H$).


The metric space $\struct {H, d_H}$ is called a metric subspace of $\struct {A, d}$.",Definition:Metric Subspace,"['Definitions/Metric Subspaces', 'Definitions/Metric Spaces']"
Definition:Subspace,Subspace,"Let $K$ be a division ring.

Let $\struct {S, +, \circ}_K$ be a $K$-algebraic structure with one operation.


Let $T$ be a closed subset of $S$.

Let $\struct {T, +_T, \circ_T}_K$ be a $K$-vector space where:
:$+_T$ is the restriction of $+$ to $T \times T$ and
:$\circ_T$ is the restriction of $\circ$ to $K \times T$.


Then $\struct {T, +_T, \circ_T}_K$ is a (vector) subspace of $\struct {S, +, \circ}_K$.


=== Proper Subspace ===
Let $K$ be a division ring.

Let $\struct {S, +, \circ}_K$ be a $K$-algebraic structure with one operation.

Let $\struct {T, +_T, \circ_T}_K$ be a vector subspace of $\struct {S, +, \circ}_K$.


If $T$ is a proper subset of $S$, then $\struct {T, +_T, \circ_T}_K$ is a proper (vector) subspace of $\struct {S, +, \circ}_K$.

=== Hilbert Spaces ===
Let $K$ be a division ring.

Let $\left({S, +, \circ}\right)_K$ be a $K$-algebraic structure with one operation.


Let $T$ be a closed subset of $S$.

Let $\left({T, +_T, \circ_T}\right)_K$ be a $K$-vector space where:
: $+_T$ is the restriction of $+$ to $T \times T$ and
: $\circ_T$ is the restriction of $\circ$ to $K \times T$.


Then $\left({T, +_T, \circ_T}\right)_K$ is a (vector) subspace of $\left({S, +, \circ}\right)_K$.


When considering Hilbert spaces, one wants to deal with projections onto subspaces.

These projections however require the linear subspace to be closed in topological sense in order to be well-defined.

Therefore, in treatises of Hilbert spaces, one encounters the terminology linear manifold for the concept of vector subspace defined above.

The adapted definition of linear subspace is then that it is a topologically closed linear manifold.",Definition:Vector Subspace,"['Definitions/Linear Algebra', 'Definitions/Vector Algebra']"
Definition:Substatement,Substatement,A substatement of a compound statement is one of the statements that comprise it.,Definition:Compound Statement/Substatement,['Definitions/Compound Statements']
Definition:Substatement,Substatement,A substatement of a statement form $\mathbf A$ is another statement form which occurs as a part of $\mathbf A$.,Definition:Statement Form/Substatement,['Definitions/Symbolic Logic']
Definition:Substitution,Substitution,"Let $S$ and $T$ be non-empty sets such that $T$ is not a subset of $S$.

Let:
:$s \in S$
:$t \in T \setminus S$
where $\setminus$ denotes set difference.

Let $S'$ be the set defined as:
:$S' = \paren {S \setminus \set s} \cup \set t$

That is, $S'$ is the set obtained by removing $s$ and replacing it with $t$ which is not in $S$.


The operation of replacing $s$ with $t$ is known as substitution.


Category:Definitions/Set Theory",Definition:Substitution (Set Theory),['Definitions/Set Theory']
Definition:Substitution,Substitution,"=== Substitution for Well-Formed Part ===
Let $\FF$ be a formal language with alphabet $\AA$.

Let $\mathbf B$ be a well-formed formula of $\FF$.

Let $\mathbf A$ be a well-formed part of $\mathbf B$.

Let $\mathbf A'$ be another well-formed formula.


Then the substitution of $\mathbf A'$ for $\mathbf A$ in $\mathbf B$ is the collation resulting from $\mathbf B$ by replacing all occurrences of $\mathbf A$ in $\mathbf B$ by $\mathbf A'$.

It is denoted as $\map {\mathbf B} {\mathbf A' \mathbin {//} \mathbf A}$.


Note that it is not immediate that $\map {\mathbf B} {\mathbf A' \mathbin {//} \mathbf A}$ is a well-formed formula of $\FF$.

This is either accepted as an axiom or proven as a theorem about the formal language $\FF$.


=== Example ===


=== Substitution for Letter ===
Let $\FF$ be a formal language with alphabet $\AA$.

Let $\mathbf B$ be a well-formed formula of $\FF$.

Let $p$ be a letter of $\FF$.

Let $\mathbf A$ be another well-formed formula.

Then the substitution of $\mathbf A$ for $p$ in $\mathbf B$ is the collation resulting from $\mathbf B$ by replacing all occurrences of $p$ in $\mathbf B$ by $\mathbf A$.

It is denoted as $\map {\mathbf B} {\mathbf A \mathbin {//} p}$.


Note that it is not immediate that $\map {\mathbf B} {\mathbf A \mathbin {//} p}$ is a well-formed formula of $\FF$.

This is either accepted as an axiom or proven as a theorem about the formal language $\FF$.

=== Substitution of Term for Variable ===
=== Substitution of Term in Term ===
Let $\beta, \tau$ be terms of predicate logic.

Let $x \in \mathrm{VAR}$ be a variable.

Let $\beta \left({x \gets \tau}\right)$ be the term resulting from $\beta$ by replacing all occurrences of $x$ by $\tau$.


Then $\beta \left({x \gets \tau}\right)$ is called the substitution instance of $\beta$ substituting $\tau$ for $x$.

=== Substitution of Term in WFF ===
Let $\mathbf A$ be a WFF of predicate logic.

Let $\tau$ be a term of predicate logic.

Let $x \in \mathrm{VAR}$ be a variable.

Let $\mathbf A \left({x \gets \tau}\right)$ be the WFF resulting from $\mathbf A$ by replacing all free occurrences of $x$ by $\tau$.


Then $\mathbf A \left({x \gets \tau}\right)$ is called the substitution instance of $\mathbf A$ substituting $\tau$ for $x$.

Category:Definitions/Language of Predicate Logic

=== Substitution for Metasymbol ===
Let $S_1$ be a statement form.

Let $p$ be a metasymbol which occurs one or more times in $S_1$.

Let $T$ be a statement.

Let $S_2$ be the string formed by replacing every occurrence of $p$ in $S_1$ with $T$.


Then $S_2$ results from the substitution of $p$ by $T$ in $S_1$.

$S_2$ is called a substitution instance of $S_1$.",Definition:Substitution (Formal Systems),['Definitions/Formal Languages']
Definition:Substitution,Substitution,"Let $\FF$ be a formal language with alphabet $\AA$.

Let $\mathbf B$ be a well-formed formula of $\FF$.

Let $\mathbf A$ be a well-formed part of $\mathbf B$.

Let $\mathbf A'$ be another well-formed formula.


Then the substitution of $\mathbf A'$ for $\mathbf A$ in $\mathbf B$ is the collation resulting from $\mathbf B$ by replacing all occurrences of $\mathbf A$ in $\mathbf B$ by $\mathbf A'$.

It is denoted as $\map {\mathbf B} {\mathbf A' \mathbin {//} \mathbf A}$.


Note that it is not immediate that $\map {\mathbf B} {\mathbf A' \mathbin {//} \mathbf A}$ is a well-formed formula of $\FF$.

This is either accepted as an axiom or proven as a theorem about the formal language $\FF$.


=== Example ===
",Definition:Substitution (Formal Systems)/Well-Formed Part,['Definitions/Formal Languages']
Definition:Substitution,Substitution,"Let $\FF$ be a formal language with alphabet $\AA$.

Let $\mathbf B$ be a well-formed formula of $\FF$.

Let $p$ be a letter of $\FF$.

Let $\mathbf A$ be another well-formed formula.

Then the substitution of $\mathbf A$ for $p$ in $\mathbf B$ is the collation resulting from $\mathbf B$ by replacing all occurrences of $p$ in $\mathbf B$ by $\mathbf A$.

It is denoted as $\map {\mathbf B} {\mathbf A \mathbin {//} p}$.


Note that it is not immediate that $\map {\mathbf B} {\mathbf A \mathbin {//} p}$ is a well-formed formula of $\FF$.

This is either accepted as an axiom or proven as a theorem about the formal language $\FF$.",Definition:Substitution (Formal Systems)/Letter,['Definitions/Formal Languages']
Definition:Substitution,Substitution,"Let $\mathbf C$ be a WFF of the language of predicate logic $\LL_1$.

Consider the (abbreviated) WFF $Q x: \mathbf C$ where $Q$ is a quantifier.

Let $y$ be another variable such that:

:$y$ is freely substitutable for $x$ in $\mathbf C$
:$y$ does not occur freely in $\mathbf C$.


Let $\mathbf C'$ be the WFF resulting from substituting $y$ for all free occurrences of $x$ in $\mathbf C$.

The change from $Q x: \mathbf C$ to $Q y: \mathbf C'$ is called alphabetic substitution.",Definition:Alphabetic Substitution,['Definitions/Predicate Logic']
Definition:Substitution,Substitution,"Let $S_1$ be a statement form.

Let $p$ be a metasymbol which occurs one or more times in $S_1$.

Let $T$ be a statement.

Let $S_2$ be the string formed by replacing every occurrence of $p$ in $S_1$ with $T$.


Then $S_2$ results from the substitution of $p$ by $T$ in $S_1$.

$S_2$ is called a substitution instance of $S_1$.",Definition:Substitution (Formal Systems)/Metasymbol,['Definitions/Formal Systems']
Definition:Substitution,Substitution,"=== Mapping ===

Let $S$ be a set.

Let $f: S^t \to S$ be a mapping.

Let $\left\{{g_1: S^k \to S, g_2: S^k \to S, \ldots, g_t: S^k \to S}\right\}$ be a set of mappings.

Let the mapping $h: S^k \to S$ be defined as:
:$h \left({s_1, s_2, \ldots, s_k}\right) = f \left({g_1 \left({s_1, s_2, \ldots, s_k}\right), g_2 \left({s_1, s_2, \ldots, s_k}\right), \ldots, g_t \left({s_1, s_2, \ldots, s_k}\right)}\right)$


Then $h$ is said to be obtained from $f, g_1, g_2, \ldots, g_k$ by substitution.


The definition can be generalized in the following ways:
* It can apply to mappings which operate on variously different sets.
* Each of $g_1, g_2, \ldots, g_t$ may have different arities. If $g$ is a mapping of $m$ variables where $m > k$, we can always consider it a mapping of $k$ variables in which the additional variables play no part. So if $g_i$ is a mapping of $k_i$ variables, we can take $k = \max \left\{{k_i: i = 1, 2, \ldots, t}\right\}$ and then each $g_i$ is then a mapping of $k$ variables.


=== Partial Function ===

Let $f: \N^t \to \N$ be a partial function.

Let $\left\{{g_1: \N^k \to \N, g_2: \N^k \to \N, \ldots, g_t: \N^k \to \N}\right\}$ be a set of partial functions.

Let the partial function $h: \N^k \to \N$ be defined as:
:$h \left({n_1, n_2, \ldots, n_k}\right) \approx f \left({g_1 \left({n_1, n_2, \ldots, n_k}\right), g_2 \left({n_1, n_2, \ldots, n_k}\right), \ldots, g_t \left({n_1, n_2, \ldots, n_k}\right)}\right)$
where $\approx$ is as defined in Partial Function Equality.


Then $h$ is said to be obtained from $f, g_1, g_2, \ldots, g_k$ by substitution.


Note that $h \left({n_1, n_2, \ldots, n_k}\right)$ is defined only when:
* All of $g_1 \left({n_1, n_2, \ldots, n_k}\right), g_2 \left({n_1, n_2, \ldots, n_k}\right), \ldots, g_t \left({n_1, n_2, \ldots, n_k}\right)$ are defined
* $f \left({g_1 \left({n_1, n_2, \ldots, n_k}\right), g_2 \left({n_1, n_2, \ldots, n_k}\right), \ldots, g_t \left({n_1, n_2, \ldots, n_k}\right)}\right)$ is defined.",Definition:Substitution (Mathematical Logic),['Definitions/Mathematical Logic']
Definition:Subtree,Subtree,"Let $T = \struct {V, E}$ be a tree.


A subtree of $T$ is a subgraph of $T$ that is also a tree.",Definition:Subtree (Graph Theory),"['Definitions/Subgraphs', 'Definitions/Tree Theory']"
Definition:Subtree,Subtree,"Let $\struct {T, r_T}$ be a rooted tree.


A rooted subtree of $T$ is a rooted tree $\struct {S, r_S}$ such that:

:$S$ is a subtree of $T$
:$r_S = r_T$

Note that the second condition implies that $r_T \in S$.",Definition:Rooted Subtree,"['Definitions/Subgraphs', 'Definitions/Rooted Trees']"
Definition:Subtree,Subtree,"Let $\struct {T, \preceq}$ be a tree.

A subtree of $\struct {T, \preceq}$ is an ordered subset $\struct {S, \preceq}$ with the property that:
:for every $\forall s \in S: \forall t \in T: t \preceq s \implies t \in S$

That is, such that $\struct {S, \preceq}$ is a lower closure of $\struct {T, \preceq}$.


Category:Definitions/Set Theory",Definition:Tree (Set Theory)/Subtree,['Definitions/Set Theory']
Definition:Successor,Successor,"Let $\preceq$ be an ordering.

Let $a, b$ be such that $a \preceq b$.


Then $b$ succeeds $a$.

$a$ is then described as being a successor of $b$.",Definition:Succeed,"['Definitions/Order Theory', 'Definitions/Successor Elements']"
Definition:Successor,Successor,"Let $\struct {S, \preceq}$ be an ordered set.

Let $a, b \in S$.


Then $a$ is an immediate successor (element) to $b$  if and only if  $b$ is an immediate predecessor (element) to $a$.

That is,  if and only if :
:$(1): \quad b \prec a$
:$(2): \quad \nexists c \in S: b \prec c \prec a$

That is, there exists no element strictly between $b$ and $a$ in the ordering $\preceq$.

That is:
:$a \prec b$ and $\openint a b = \O$
where $\openint a b$ denotes the open interval from $a$ to $b$.


We say that $a$ immediately succeeds $b$.


=== Class Theory ===
 
Let $A$ be an ordered class under an ordering $\preccurlyeq$.

Let $a, b \in A$.


Then $a$ is an immediate successor (element) to $b$  if and only if  $b$ is an immediate predecessor (element) to $a$.

That is,  if and only if :
:$(1): \quad b \prec a$
:$(2): \quad \nexists c \in S: b \prec c \prec a$

We say that $a$ immediately succeeds $b$.",Definition:Immediate Successor Element,['Definitions/Successor Elements']
Definition:Successor,Successor,"Let $V$ be a basic universe.

Let $s: V \to V$ denote the successor mapping on $V$.


For $x \in V$, the result of applying the successor mapping on $x$ is denoted $x^+$:

:$x^+ := \map s x = x \cup \set x$

$x^+$ is referred to as the successor (set) of $x$.",Definition:Successor Mapping/Successor Set,['Definitions/Successor Mapping']
Definition:Successor,Successor,"Let $V$ be a basic universe.

The successor mapping $s$ is the mapping on $V$ defined and denoted:
:$\forall x \in V: \map s x := x \cup \set x$
where $x$ is a set in $V$.


=== Peano Structure ===
Let $\struct {P, s, 0}$ be a Peano structure.


Then the mapping $s: P \to P$ is called the successor mapping on $P$.


=== Successor Element ===
Let $\struct {P, s, 0}$ be a Peano structure.

Let mapping $s: P \to P$ denote the successor mapping on $P$.


The image element $\map s x$ of an element $x$ is called the successor element or just successor of $x$.

=== Successor Mapping on Natural Numbers ===
Let $\N$ be the set of natural numbers.

Let $s: \N \to \N$ be the mapping defined as:

:$s = \set {\tuple {x, y}: x \in \N, y = x + 1}$


Considering $\N$ defined as a Peano structure, this is seen to be an instance of a successor mapping.",Definition:Successor Mapping,"['Definitions/Class Theory', 'Definitions/Successor Mapping']"
Definition:Successor,Successor,"A successor ordinal is the successor set of an ordinal:
:$\alpha^+ := \map s \alpha = \alpha \cup \set \alpha$",Definition:Successor Ordinal,"['Definitions/Ordinals', 'Definitions/Successor Mapping']"
Definition:Sum,Sum,"Let $a + b$ denote the operation of addition on two objects $a$ and $b$.

Then the result $a + b$ is referred to as the sum of $a$ and $b$.


Note that the nature of $a$ and $b$ has deliberately been left unspecified.

They could be, for example, numbers, matrices or more complex expressions constructed from such elements.",Definition:Addition/Sum,['Definitions/Addition']
Definition:Sum,Sum,"Let $\struct {S, +}$ be an algebraic structure where the operation $+$ is an operation derived from, or arising from, the addition operation on the natural numbers.

Let $\tuple {a_1, a_2, \ldots, a_n} \in S^n$ be an ordered $n$-tuple in $S$.


=== Definition by Index ===
Let $\struct {S, +}$ be an algebraic structure where the operation $+$ is an operation derived from, or arising from, the addition operation on the natural numbers.

Let $\tuple {a_1, a_2, \ldots, a_n} \in S^n$ be an ordered $n$-tuple in $S$.


The composite is called the summation of $\tuple {a_1, a_2, \ldots, a_n}$, and is written:

:$\ds \sum_{j \mathop = 1}^n a_j = \paren {a_1 + a_2 + \cdots + a_n}$


=== Summand ===
Let $\struct {S, +}$ be an algebraic structure where the operation $+$ is an operation derived from, or arising from, the addition operation on the natural numbers.

Let $\set {a_1, a_2, \ldots, a_n} \subseteq S$ be a set of elements of $S$.

Let $\map R j$ be a propositional function of $j$.

Let:
:$\ds \sum_{\map R j} a_j$
be an instance of a summation on $\set {a_1, a_2, \ldots, a_n}$.


The set of elements $\set {a_j \in S: 1 \le j \le n, \map R j}$ is called the summand.

=== Definition by Inequality ===
Let $\struct {S, +}$ be an algebraic structure where the operation $+$ is an operation derived from, or arising from, the addition operation on the natural numbers.

Let $\tuple {a_1, a_2, \ldots, a_n} \in S^n$ be an ordered $n$-tuple in $S$.


The summation of $\tuple {a_1, a_2, \ldots, a_n}$ can be written:
:$\ds \sum_{1 \mathop \le j \mathop \le n} a_j = \paren {a_1 + a_2 + \cdots + a_n}$


=== Multiple Indices ===
Let $\ds \sum_{0 \mathop \le j \mathop \le n} a_j$ denote the summation of $\tuple {a_0, a_1, a_2, \ldots, a_n}$.


Summands with multiple indices can be denoted by propositional functions in several variables, for example:

:$\ds \sum_{0 \mathop \le i \mathop \le n} \paren {\sum_{0 \mathop \le j \mathop \le n} a_{i j} } = \sum_{0 \mathop \le i, j \mathop \le n} a_{i j}$


:$\ds \sum_{0 \mathop \le i \mathop \le n} \paren {\sum_{0 \mathop \le j \mathop \le i} a_{i j} } = \sum_{0 \mathop \le j \mathop \le i \mathop \le n} a_{i j}$

=== Definition by Propositional Function ===
Let $\struct {S, +}$ be an algebraic structure where the operation $+$ is an operation derived from, or arising from, the addition operation on the natural numbers.

Let $\tuple {a_1, a_2, \ldots, a_n} \in S^n$ be an ordered $n$-tuple in $S$.


Let $\map R j$ be a propositional function of $j$.

Then we can write the summation as:

:$\ds \sum_{\map R j} a_j = \text{ The sum of all $a_j$ such that $\map R j$ holds}$.


If more than one propositional function is written under the summation sign, they must all hold.


Such an operation on an ordered tuple is known as a summation.


Note that the definition by inequality form $1 \le j \le n$ is a special case of such a propositional function.

Also note that the definition by index form $\ds \sum_{j \mathop = 1}^n$ is merely another way of writing $\ds \sum_{1 \mathop \le j \mathop \le n}$.

Hence all instances of a summation can be expressed in terms of a propositional function.


=== Iverson's Convention ===
Let $\ds \sum_{\map R j} a_j$ be the summation over all $a_j$ such that $j$ satisfies $R$.


This can also be expressed:
:$\ds \sum_{j \mathop \in \Z} a_j \sqbrk {\map R j}$
where $\sqbrk {\map R j}$ is Iverson's convention.

=== Iverson's Convention ===
Let $\ds \sum_{\map R j} a_j$ be the summation over all $a_j$ such that $j$ satisfies $R$.


This can also be expressed:
:$\ds \sum_{j \mathop \in \Z} a_j \sqbrk {\map R j}$
where $\sqbrk {\map R j}$ is Iverson's convention.

=== Summation over Finite Subset ===
Let $\struct {G, +}$ be a commutative monoid.


Let $F \subseteq G$ be a finite subset of $G$.


Let $\set {e_1, e_2, \ldots, e_n}$ be a finite enumeration of $F$.

Let $\tuple {e_1, e_2, \ldots, e_n}$ be the ordered tuple formed from the bijection $e: \closedint 1 n \to F$.


The summation over $F$, denoted $\ds \sum_{g \mathop \in F} g$, is defined as the summation over $\tuple{e_1, e_2, \ldots, e_n}$:
:$\ds \sum_{g \mathop \in F} g = \sum_{i \mathop = 1}^n e_i$

=== Summation over Finite Index ===
Let $\struct {G, +}$ be a commutative monoid.


Let $\family {g_i}_{i \mathop \in I}$ be an indexed subset of $G$ where the indexing set $I$ is finite.


Let $\set {e_1, e_2, \ldots, e_n}$ be a finite enumeration of $I$.

Let $\tuple {g_{e_1}, g_{e_2}, \ldots, g_{e_n} }$ be the ordered tuple formed from the composite mapping $g \circ e: \closedint 1 n \to G$.


The summation over $I$, denoted $\ds \sum_{i \mathop \in I} g_i$, is defined as the summation over $\tuple {g_{e_1}, g_{e_2}, \ldots, g_{e_n} }$:
:$\ds \sum_{i \mathop \in I} g_i = \sum_{k \mathop = 1}^n g_{e_k}$",Definition:Summation,"['Definitions/Summations', 'Definitions/Algebra', 'Definitions/Abstract Algebra']"
Definition:Support,Support,"=== Real-Valued Function on an Abstract Set ===
Let $S$ be a set.

Let $f: S \to \R$ be a real-valued function.


The support of $f$ is the set of elements $x$ of $S$ whose values under $f$ are non-zero.

That is:
:$\map \supp f := \set {x \in S: \map f x \ne 0}$


That is, the support of a function whose codomain is the set of real numbers is generally defined to be the subset of its domain which maps to anywhere that is not $0$.

Category:Definitions/Real Analysis

=== General Real-Valued Function in $\R^n$ ===

=== General Algebraic Structure ===

Let $\struct {A, *}$ be an algebraic structure with an identity element $e$.

Let $S$ be a set.

Let $f: S \to A$ be a mapping.


The support of $f$ is the set:
:$\map \supp f = \set {s \in S : \map f s \ne e}$
 ",Definition:Support of Mapping to Algebraic Structure,"['Definitions/Real Analysis', 'Definitions/Abstract Algebra']"
Definition:Support,Support,"Let $S$ be a set.

Let $f: S \to \R$ be a real-valued function.


The support of $f$ is the set of elements $x$ of $S$ whose values under $f$ are non-zero.

That is:
:$\map \supp f := \set {x \in S: \map f x \ne 0}$


That is, the support of a function whose codomain is the set of real numbers is generally defined to be the subset of its domain which maps to anywhere that is not $0$.

Category:Definitions/Real Analysis",Definition:Support of Mapping to Algebraic Structure/Real-Valued Function,['Definitions/Real Analysis']
Definition:Support,Support,"Let $A$ be a commutative ring with unity.

Let $M$ be a unitary $A$-module.


The support $\map \supp M$ of $M$ is the set of prime ideals $P$ of $A$ such that the localization of $M$ at $P$ is nonzero:
:$\map \supp M = \set {P \in \Spec A : M_P \ne 0}$

where $\Spec A$ is the spectrum of $A$.

Category:Definitions/Module Theory",Definition:Support of Module,['Definitions/Module Theory']
Definition:Support,Support,"Let $\family {\struct {S_i, \circ_i} }_{i \mathop \in I}$ be a family of algebraic structures with identity.

Let $\ds S = \prod_{i \mathop \in I} S_i$ be their direct product.

Let $e_i$ be an identity of $S_i$ for all $i \in I$.

Let $m = \family {m_i}_{i \mathop \in I} \in S$.


The support of $m$ is defined as:
:$\supp \set {i \in I: m_i \ne e_i}$

 


=== Finite Support ===

The element is said to have finite support  if and only if  its support is a finite set.",Definition:Support of Element of Direct Product,"['Definitions/Abstract Algebra', 'Definitions/Direct Products']"
Definition:Support,Support,"=== Continuous Real-Valued Function in $\R^n$ ===
Let $f: \R^n \to \R$ be a continuous real-valued function.

The support of $f$ is the closure of the set of elements $x$ of $\R^n$ whose values under $f$ are non-zero.

That is:
:$\map \supp f = \cl \set {x \in \R^n: \map f x \ne 0}$


Category:Definitions/Real Analysis

=== General topological group ===

Let $X$ be a topological space.

Let $G$ be a topological group with identity $e$.

Let $f : X \to G$ be a continuous mapping.


The support of $f$ is the closure of the set of elements of $X$ that do not map to $e$ under $f$:
:$\map \supp f = \cl \set {x \in X: \map f x \ne e}$",Definition:Support of Continuous Mapping,['Definitions/Topology']
Definition:Support,Support,"Let $f: \R^n \to \R$ be a continuous real-valued function.

The support of $f$ is the closure of the set of elements $x$ of $\R^n$ whose values under $f$ are non-zero.

That is:
:$\map \supp f = \cl \set {x \in \R^n: \map f x \ne 0}$


Category:Definitions/Real Analysis",Definition:Support of Continuous Mapping/Real-Valued,['Definitions/Real Analysis']
Definition:Support,Support,"Let $\Omega \subseteq \R^n$ be an open set.

 

Let $\map \DD \Omega$ be the space of continuous functions compactly supported in $\Omega$.

Let $\map {\DD'} \Omega$ be the distribution space.

Let $T \in \map {\DD'} \Omega$ be a distribution.


The support $\map {\mathrm {supp} } T \subseteq \Omega$ of $T$ is defined by:
:$\ds x \notin \map {\mathrm {supp} } T$  if and only if :
::there exists an open neighborhood $U$ of $x$ such that:
:::for all $\phi \in \map \DD \Omega$ such that $\map {\mathrm {supp} } \phi \subseteq U$:
::::$\map T \phi = 0$


Category:Definitions/Real Analysis",Definition:Support of Distribution,['Definitions/Real Analysis']
Definition:Support,Support,"Let $S$ be a set.

Let $f$ be a permutation on $S$.


The support of $f$ is the subset of moved elements:
:$\map \supp f = \set {x \in S: \map f x \ne x}$


 ",Definition:Support of Permutation,"['Definitions/Mapping Theory', 'Definitions/Permutation Theory']"
Definition:Support,Support,"Let $S$ be a set

Let $E \subseteq S$ be a subset.

Let $\chi_E: S \to \set {0, 1}$ be the characteristic function of $E$.


The support of $\chi_E$, denoted $\map \supp {\chi_E}$, is the set $E$.

That is:

:$\map \supp {\chi_E} = \set {x \in S: \map {\chi_E} x = 1}$",Definition:Characteristic Function (Set Theory)/Set/Support,['Definitions/Characteristic Functions of Sets']
Definition:Supremum,Supremum,"Let $\struct {S, \preccurlyeq}$ be an ordered set.

Let $T \subseteq S$.


An element $c \in S$ is the supremum of $T$ in $S$  if and only if :

:$(1): \quad c$ is an upper bound of $T$ in $S$
:$(2): \quad c \preccurlyeq d$ for all upper bounds $d$ of $T$ in $S$.


If there exists a supremum of $T$ (in $S$), we say that:
:$T$ admits a supremum (in $S$) or
:$T$ has a supremum (in $S$).


=== Finite Supremum ===
Let $\struct {S, \preccurlyeq}$ be an ordered set.

Let $T \subseteq S$ admit a supremum $\sup T$.


If $T$ is finite, $\sup T$ is called a finite supremum.

=== Subset of Real Numbers ===

The concept is usually encountered where $\struct {S, \preccurlyeq}$ is the set of real numbers under the usual ordering $\struct {\R, \le}$:

Let $T \subseteq \R$ be a subset of the real numbers.


A real number $c \in \R$ is the supremum of $T$ in $\R$  if and only if :

:$(1): \quad c$ is an upper bound of $T$ in $\R$
:$(2): \quad c \le d$ for all upper bounds $d$ of $T$ in $\R$.


If there exists a supremum of $T$ (in $\R$), we say that:
:$T$ admits a supremum (in $\R$) or
:$T$ has a supremum (in $\R$).


The supremum of $T$ is denoted $\sup T$ or $\map \sup T$.


=== Definition by Propositional Function ===
Let $\family {a_j}_{j \mathop \in I}$ be a family of elements of the real numbers $\R$ indexed by $I$.

Let $\map R j$ be a propositional function of $j \in I$.


Then we can define the supremum of $\family {a_j}_{j \mathop \in I}$ as:

:$\ds \sup_{\map R j} a_j := \text { the supremum of all $a_j$ such that $\map R j$ holds}$


If more than one propositional function is written under the supremum sign, they must all hold.


=== Finite Range ===
Let $\family {a_j}_{j \mathop \in I}$ be a family of elements of the real numbers $\R$ indexed by $I$.

Let $\map R j$ be a propositional function of $j \in I$.


Let the fiber of truth of $\map R j$ be finite.

Then the supremum of $\family {a_j}_{j \mathop \in I}$ can be expressed as:

:$\ds \max_{\map R j} a_j = \text { the maxmum of all $a_j$ such that $\map R j$ holds}$

and can be referred to as the maximum of $\family {a_j}_{j \mathop \in I}$.


If more than one propositional function is written under the supremum sign, they must all hold.

=== Vacuous Supremum ===
Take the indexed supremum:
:$\ds \sup _{\map \Phi j} a_j$
where $\map \Phi j$ is a propositional function of $j$.

Suppose that there are no values of $j$ for which $\map \Phi j$ is true.

Then $\ds \sup_{\map \Phi j} a_j$ is defined as being $-\infty$.

This supremum is called a vacuous supremum.


This is because:
:$\forall a \in \R: \sup \set {a, -\infty} = a$

Hence for all $j$ for which $\map \Phi j$ is false, the supremum is unaffected.


In this context $-\infty$ is considered as minus infinity, the hypothetical quantity that has the property:

:$\forall n \in \Z: -\infty < n$

The supremum of $T$ is denoted $\sup T$ or $\map \sup T$.",Definition:Supremum of Set,['Definitions/Suprema']
Definition:Supremum,Supremum,"Let $T \subseteq \R$ be a subset of the real numbers.


A real number $c \in \R$ is the supremum of $T$ in $\R$  if and only if :

:$(1): \quad c$ is an upper bound of $T$ in $\R$
:$(2): \quad c \le d$ for all upper bounds $d$ of $T$ in $\R$.


If there exists a supremum of $T$ (in $\R$), we say that:
:$T$ admits a supremum (in $\R$) or
:$T$ has a supremum (in $\R$).


The supremum of $T$ is denoted $\sup T$ or $\map \sup T$.


=== Definition by Propositional Function ===
Let $\family {a_j}_{j \mathop \in I}$ be a family of elements of the real numbers $\R$ indexed by $I$.

Let $\map R j$ be a propositional function of $j \in I$.


Then we can define the supremum of $\family {a_j}_{j \mathop \in I}$ as:

:$\ds \sup_{\map R j} a_j := \text { the supremum of all $a_j$ such that $\map R j$ holds}$


If more than one propositional function is written under the supremum sign, they must all hold.


=== Finite Range ===
Let $\family {a_j}_{j \mathop \in I}$ be a family of elements of the real numbers $\R$ indexed by $I$.

Let $\map R j$ be a propositional function of $j \in I$.


Let the fiber of truth of $\map R j$ be finite.

Then the supremum of $\family {a_j}_{j \mathop \in I}$ can be expressed as:

:$\ds \max_{\map R j} a_j = \text { the maxmum of all $a_j$ such that $\map R j$ holds}$

and can be referred to as the maximum of $\family {a_j}_{j \mathop \in I}$.


If more than one propositional function is written under the supremum sign, they must all hold.

=== Vacuous Supremum ===
Take the indexed supremum:
:$\ds \sup _{\map \Phi j} a_j$
where $\map \Phi j$ is a propositional function of $j$.

Suppose that there are no values of $j$ for which $\map \Phi j$ is true.

Then $\ds \sup_{\map \Phi j} a_j$ is defined as being $-\infty$.

This supremum is called a vacuous supremum.


This is because:
:$\forall a \in \R: \sup \set {a, -\infty} = a$

Hence for all $j$ for which $\map \Phi j$ is false, the supremum is unaffected.


In this context $-\infty$ is considered as minus infinity, the hypothetical quantity that has the property:

:$\forall n \in \Z: -\infty < n$",Definition:Supremum of Set/Real Numbers,['Definitions/Suprema']
Definition:Supremum,Supremum,"Let $S$ be a set.

Let $\struct {T, \preceq}$ be an ordered set.

Let $f: S \to T$ be a mapping from $S$ to $T$.

Let $f \sqbrk S$, the image of $f$, admit a supremum.


Then the supremum of $f$ (on $S$) is defined by:
:$\ds \sup_{x \mathop \in S} \map f x = \sup f \sqbrk S$


=== Real-Valued Function ===
Let $f: S \to \R$ be a real-valued function.

Let $f$ be bounded above on $S$.


=== Definition 1 ===
Let $f: S \to \R$ be a real-valued function.

Let $f$ be bounded above on $S$.


The supremum of $f$ on $S$ is defined by:
:$\ds \sup_{x \mathop \in S} \map f x := \sup f \sqbrk S$
where
:$\sup f \sqbrk S$ is the supremum in $\R$ of the image of $S$ under $f$.

=== Definition 2 ===
Let $f: S \to \R$ be a real-valued function.

Let $f$ be bounded above on $S$.


The supremum of $f$ on $S$ is defined as $\ds \sup_{x \mathop \in S} \map f x := K \in \R$ such that:

:$(1): \quad \forall x \in S: \map f x \le K$
:$(2): \quad \exists x \in S: \forall \epsilon \in \R_{>0}: \map f x > K - \epsilon$",Definition:Supremum of Mapping,['Definitions/Mapping Theory']
Definition:Supremum,Supremum,"Let $f: S \to \R$ be a real-valued function.

Let $f$ be bounded above on $S$.


=== Definition 1 ===
Let $f: S \to \R$ be a real-valued function.

Let $f$ be bounded above on $S$.


The supremum of $f$ on $S$ is defined by:
:$\ds \sup_{x \mathop \in S} \map f x := \sup f \sqbrk S$
where
:$\sup f \sqbrk S$ is the supremum in $\R$ of the image of $S$ under $f$.

=== Definition 2 ===
Let $f: S \to \R$ be a real-valued function.

Let $f$ be bounded above on $S$.


The supremum of $f$ on $S$ is defined as $\ds \sup_{x \mathop \in S} \map f x := K \in \R$ such that:

:$(1): \quad \forall x \in S: \map f x \le K$
:$(2): \quad \exists x \in S: \forall \epsilon \in \R_{>0}: \map f x > K - \epsilon$",Definition:Supremum of Mapping/Real-Valued Function,['Definitions/Suprema']
Definition:Supremum,Supremum,"A special case of a supremum of a mapping is a supremum of a sequence, where the domain of the mapping is $\N$.

Let $\struct {T, \preceq}$ be an ordered set.

Let $\sequence {x_n}$ be a sequence in $T$.


Let $\set {x_n: n \in \N}$ admit a supremum.


Then the supremum of $\sequence {x_n}$) is defined as:
:$\ds \map \sup {\sequence {x_n} } = \map \sup {\set {x_n: n \in \N} }$",Definition:Supremum of Sequence,"['Definitions/Sequences', 'Definitions/Suprema']"
Definition:Supremum,Supremum,"Let $\sequence {x_n}$ be a real sequence.


Let $\set {x_n: n \in \N}$ admit a supremum.


Then the supremum of $\sequence {x_n}$) is defined as:
:$\ds \map \sup {\sequence {x_n} } = \map \sup {\set {x_n: n \in \N} }$",Definition:Supremum of Real Sequence,"['Definitions/Sequences', 'Definitions/Suprema']"
Definition:Symmetry,Symmetry,"Let $\RR \subseteq S \times S$ be a relation in $S$.


=== Symmetric ===
Let $\RR \subseteq S \times S$ be a relation in $S$.


=== Definition 1 ===
Let $\RR \subseteq S \times S$ be a relation in $S$.

$\RR$ is symmetric  if and only if :

:$\tuple {x, y} \in \RR \implies \tuple {y, x} \in \RR$

=== Definition 2 ===
Let $\RR \subseteq S \times S$ be a relation in $S$.


$\RR$ is symmetric  if and only if  it equals its inverse:
:$\RR^{-1} = \RR$

=== Definition 3 ===
Let $\RR \subseteq S \times S$ be a relation in $S$.

$\RR$ is symmetric  if and only if  it is a subset of its inverse:
:$\RR \subseteq \RR^{-1}$

=== Class Theory ===
 
Let $V$ be a basic universe.

Let $\RR \subseteq V \times V$ be a relation in $V$.


$\RR$ is symmetric  if and only if :

:$\tuple {x, y} \in \RR \implies \tuple {y, x} \in \RR$

=== Asymmetric ===
Let $\RR \subseteq S \times S$ be a relation in $S$.


=== Definition 1 ===
Let $\RR \subseteq S \times S$ be a relation in $S$.


$\RR$ is asymmetric  if and only if :

:$\tuple {x, y} \in \RR \implies \tuple {y, x} \notin \RR$

=== Definition 2 ===
Let $\RR \subseteq S \times S$ be a relation in $S$.


$\RR$ is asymmetric  if and only if  it and its inverse are disjoint:
:$\RR \cap \RR^{-1} = \O$

=== Antisymmetric ===
Let $\RR \subseteq S \times S$ be a relation in $S$.


=== Definition 1 ===
Let $S$ be a set.

Let $\RR \subseteq S \times S$ be a relation in $S$.

$\RR$ is antisymmetric  if and only if :
:$\tuple {x, y} \in \RR \land \tuple {y, x} \in \RR \implies x = y$
that is:
:$\set {\tuple {x, y}, \tuple {y, x} } \subseteq \RR \implies x = y$

=== Definition 2 ===
Let $\RR \subseteq S \times S$ be a relation in $S$.

$\RR$ is antisymmetric  if and only if :
:$\tuple {x, y} \in \RR \land x \ne y \implies \tuple {y, x} \notin \RR$

=== Class Theory ===
 
Let $V$ be a basic universe.

Let $\RR \subseteq V \times V$ be a relation in $V$.


=== Definition 1 ===
Let $V$ be a basic universe.

Let $\RR \subseteq V \times V$ be a relation in $V$.


$\RR$ is antisymmetric  if and only if :
:$\tuple {x, y} \in \RR \land \tuple {y, x} \in \RR \implies x = y$
that is:
:$\set {\tuple {x, y}, \tuple {y, x} } \subseteq \RR \implies x = y$

=== Definition 2 ===
Let $V$ be a basic universe.

Let $\RR \subseteq V \times V$ be a relation in $V$.


$\RR$ is antisymmetric  if and only if :
:$\tuple {x, y} \in \RR \land x \ne y \implies \tuple {y, x} \notin \RR$

=== Non-symmetric ===
Let $\RR \subseteq S \times S$ be a relation in $S$.

$\RR$ is non-symmetric  if and only if  it is neither symmetric nor asymmetric.",Definition:Symmetry (Relation),"['Definitions/Relation Theory', 'Definitions/Symmetric Relations']"
Definition:Symmetry,Symmetry,"A symmetry mapping of a geometric figure is a bijection from the figure to itself which preserves the distance between points.

In other words, it is a self-congruence.


Intuitively and informally, a symmetry mapping is a movement of the figure so that it looks exactly the same after it has been moved.",Definition:Symmetry Mapping,"['Definitions/Geometry', 'Definitions/Mapping Theory', 'Definitions/Symmetry Mappings']"
Definition:Symmetry,Symmetry,"Let $P$ be a geometric figure.

Let $S_P$ be the set of all symmetries of $P$.

Let $\struct {S_P, \circ}$ be the algebraic structure such that $\circ$ denotes the composition of mappings.


Then $\struct {S_P, \circ}$ is called the symmetry group of $P$.",Definition:Symmetry Group,"['Definitions/Examples of Groups', 'Definitions/Symmetry Groups']"
Definition:System,System,A system is a configuration of electrical devices designed for a specific utilitarian purpose.,Definition:System (Electronics),"['Definitions/Systems (Electronics)', 'Definitions/Electronics']"
Definition:System,System,"Let $T = \struct {S, \tau}$ be a topological space.

A neighborhood system is a family $\family {\NN_x}_{x \mathop \in S}$ indexed by points of $S$, such that $\NN_x$ is a local basis at $x$ for $x \in S$.",Definition:Neighborhood System,['Definitions/Topology']
Definition:System,System,"Let $M = \struct {A, d}$ be a metric space.

Let $a \in A$.

Let $\NN_a$ be the set of all neighborhoods of $a$ in $M$.


Then $\NN_a$ is the system of neighborhoods of the point $a$.",Definition:System of Neighborhoods,"['Definitions/Systems of Neighborhoods', 'Definitions/Neighborhoods']"
Definition:System,System,"Let $\struct {S, \tau}$ be a topological space.

Let $f: S \to S$ be a continuous mapping.


Then $\struct {S, f}$ is called a topological dynamical system.",Definition:Topological Dynamical System,['Definitions/Dynamical Systems Theory']
Definition:System,System,"Let $\LL$ be a formal language.


A proof system $\mathscr P$ for $\LL$ comprises:

* Axioms and/or axiom schemata;
* Rules of inference for deriving theorems.


It is usual that a proof system does this by declaring certain arguments concerning $\LL$ to be valid.

Informally, a proof system amounts to a precise account of what constitutes a (formal) proof.


=== Rule of Inference ===
Let $\LL$ be a formal language.

Part of defining a proof system $\mathscr P$ for $\LL$ is to specify its rules of inference.


A rule of inference is a specification of a valid means to conclude new theorems in $\mathscr P$ from given theorems and axioms of $\mathscr P$.

Often, the formulation of rules of inference also appeals to the notion of provable consequence to be able to deal with assumptions as part of a proof.

=== Axiom ===
Let $\LL$ be a formal language.

Part of defining a proof system $\mathscr P$ for $\LL$ is to specify its axioms.


An axiom of $\mathscr P$ is a well-formed formula of $\LL$ that $\mathscr P$ approves of by definition.


=== Axiom Schema ===
Let $\LL$ be a formal language.

Part of defining a proof system $\mathscr P$ for $\LL$ is to specify its axiom schemata.


An axiom schema is a well-formed formula $\phi$ of $\LL$, except for it containing one or more variables which are outside $\LL$ itself.

This formula can then be used to represent an infinite number of individual axioms in one statement.


Namely, each of these variables is allowed to take a specified range of values, most commonly WFFs.

Each WFF $\psi$ that results from $\phi$ by a valid choice of values for all the variables is then an axiom of $\mathscr P$.

=== Formal Proof ===
Let $\mathscr P$ be a proof system for a formal language $\LL$.


Let $\phi$ be a WFF of $\LL$.

A formal proof of $\phi$ in $\mathscr P$ is a collection of axioms and rules of inference of $\mathscr P$ that leads to the conclusion that $\phi$ is a theorem of $\mathscr P$.


The term formal proof is also used to refer to specific presentations of such collections.

For example, the term applies to tableau proofs in natural deduction.",Definition:Proof System,"['Definitions/Formal Systems', 'Definitions/Symbolic Logic', 'Definitions/Proof Systems']"
Definition:System,System,"Let $\LL$ be a logical language.

Let $\mathscr P$ be a proof system.


Then $\mathscr P$ is independent if it is not possible to derive one axiom or rule of inference of $\mathscr P$ from the others.

 ",Definition:Independent Proof System,['Definitions/Proof Systems']
Definition:System,System,"Gentzen proof systems are a class of proof systems for propositional and predicate logic.

Their characteristics include:

* The presence of few axioms and many rules of inference.
* Use of formal, sequent-like notation involving the turnstile $\vdash$.
* Proofs whose structure can be viewed as rooted trees.

Specific instances may deviate from this general scheme at some points.


 
 ",Definition:Gentzen Proof System,"['Definitions/Proof Systems', 'Definitions/Propositional Logic', 'Definitions/Predicate Logic']"
Definition:System,System,"Let $\LL$ be a logical language.

Let $\mathscr P$ be a proof system for $\LL$.


Then $\mathscr P$ is consistent  if and only if :

:There exists a logical formula $\phi$ such that $\not \vdash_{\mathscr P} \phi$

That is, some logical formula $\phi$ is not a theorem of $\mathscr P$.


=== Propositional Logic ===
Let $\LL_0$ be the language of propositional logic.

Let $\mathscr P$ be a proof system for $\LL_0$.


=== Definition 1 ===
Let $\LL_0$ be the language of propositional logic.

Let $\mathscr P$ be a proof system for $\LL_0$.


Then $\mathscr P$ is consistent  if and only if :

:There exists a logical formula $\phi$ such that $\not \vdash_{\mathscr P} \phi$

That is, some logical formula $\phi$ is not a theorem of $\mathscr P$.


Category:Definitions/Proof Systems
Category:Definitions/Propositional Logic

=== Definition 2 ===
Let $\LL$ be the language of propositional logic.

Let $\mathscr P$ be a proof system for $\LL_0$.

Suppose that in $\mathscr P$, the Rule of Explosion (Variant 3) holds.


Then $\mathscr P$ is consistent  if and only if :

:For every logical formula $\phi$, not both of $\phi$ and $\neg \phi$ are theorems of $\mathscr P$",Definition:Consistent (Logic)/Proof System,['Definitions/Proof Systems']
Definition:System,System,"Let $\LL$ be a logical language.

Let $\mathscr P$ be a proof system for $\LL$.

Let $\mathscr M$ be a formal semantics for $\LL$.


Then $\mathscr P$ is said to be sound for $\mathscr M$  if and only if :

:Every $\mathscr P$-theorem is an $\mathscr M$-tautology.

Symbolically, this can be expressed as the statement that, for every logical formula $\phi$ of $\LL$:

:$\vdash_{\mathscr P} \phi$ implies $\models_{\mathscr M} \phi$


=== Strongly Sound Proof System ===
Let $\LL$ be a logical language.

Let $\mathscr P$ be a proof system for $\LL$.

let $\mathscr M$ be a formal semantics for $\LL$.


$\mathscr P$ is strongly sound for $\mathscr M$  if and only if :

:Every $\mathscr P$-provable consequence is an $\mathscr M$-semantic consequence.

Symbolically, this can be expressed as the statement that, for every collection of logical formulas $\FF$, and logical formula $\phi$ of $\LL$:

:$\FF \vdash_{\mathscr P} \phi$ implies $\FF \models_{\mathscr M} \phi$",Definition:Sound Proof System,"['Definitions/Sound Proof Systems', 'Definitions/Proof Systems']"
Definition:System,System,"Let $\LL$ be a logical language.

Let $\mathscr P$ be a proof system for $\LL$, and let $\mathscr M$ be a formal semantics for $\LL$.


$\mathscr P$ is strongly complete for $\mathscr M$  if and only if :

:Every $\mathscr M$-semantic consequence is a $\mathscr P$-provable consequence.

Symbolically, this can be expressed as the statement that, for every collection $\FF$ of logical formulas, and every logical formula $\phi$ of $\LL$:

:$\FF \models_{\mathscr M} \phi$ implies $\FF \vdash_{\mathscr P} \phi$",Definition:Complete Proof System/Strongly Complete,"['Definitions/Complete Proof Systems', 'Definitions/Proof Systems']"
Definition:System,System,"Let $\LL$ be a logical language.

Let $\mathscr P$ be a proof system for $\LL$.

let $\mathscr M$ be a formal semantics for $\LL$.


$\mathscr P$ is strongly sound for $\mathscr M$  if and only if :

:Every $\mathscr P$-provable consequence is an $\mathscr M$-semantic consequence.

Symbolically, this can be expressed as the statement that, for every collection of logical formulas $\FF$, and logical formula $\phi$ of $\LL$:

:$\FF \vdash_{\mathscr P} \phi$ implies $\FF \models_{\mathscr M} \phi$",Definition:Sound Proof System/Strongly Sound,['Definitions/Sound Proof Systems']
Definition:System,System,"A key feature of collations is the presence of methods to collate a number of collations into a new one.

A collection of collations, together with a collection of such collation methods may be called a collation system.


For example, words and the method of concatenation.


Category:Definitions/Collations",Definition:Collation/Collation System,['Definitions/Collations']
Definition:System,System,"A formal system is a formal language $\LL$ together with a deductive apparatus for $\LL$.


Let $\FF$ be a formal system consisting of a formal language with deductive apparatus $\DD$.

By applying the formal grammar of $\LL$, one constructs well-formed formulae in $\LL$.

Of such a well-formed formula, one can then use the deductive apparatus $\DD$ to determine whether or not it is a theorem in $\FF$.",Definition:Formal System,['Definitions/Formal Systems']
Definition:System,System,"A system of differential equations is autonomous if all of the differential equations which it comprises are themselves autonomous.


Category:Definitions/Differential Equations",Definition:Differential Equation/System/Autonomous,['Definitions/Differential Equations']
Definition:System,System,"Let $R$ be a ring with unity.

Let $\sequence {a_k}_{1 \mathop \le k \mathop \le n}$ be an ordered basis of a free unitary $R$-module $G$.


Then $\sequence {a_k}_{1 \mathop \le k \mathop \le n}$ can be referred to as a coordinate system.


=== Coordinate Function ===
Let $\left \langle {a_n} \right \rangle$ be a coordinate system of a unitary $R$-module $G$.

For each $x \in G$ let $x_1, x_2, \ldots, x_n$ be the coordinates of $x$ relative to $\left \langle {a_n} \right \rangle$.


Then for $i = 1, \ldots, n$ the mapping $f_i : G \to R$ defined by $f_i \left({x}\right) = x_i$ is called the $i$-th coordinate function on $G$ relative to $\left \langle {a_n} \right \rangle$.


Category:Definitions/Coordinate Systems

=== Coordinates on Affine Space ===
Let $\EE$ be an affine space of dimension $n$ over a field $k$.

Let $\RR = \tuple {p_0, e_1, \ldots, e_n}$ be an affine frame in $\EE$.

Let $p \in \EE$ be a point.

Since Affine Coordinates are Well-Defined, there exists a unique ordered tuple $\tuple {\lambda_1, \ldots, \lambda_n} \in k^n$ such that:
:$\ds p = p_0 + \sum_{i \mathop = 1}^n \lambda_i e_i$


The numbers $\lambda_1, \ldots, \lambda_n$ are the coordinates of $p$ in the frame $\RR$.


Category:Definitions/Affine Geometry

=== Coordinate ===
Let $\sequence {a_n}$ be a coordinate system of a unitary $R$-module $G$.

Let $\ds x \in G: x = \sum_{k \mathop = 1}^n \lambda_k a_k$.

The scalars $\lambda_1, \lambda_2, \ldots, \lambda_n$ can be referred to as the coordinates of $x$ relative to $\sequence {a_n}$.


=== Elements of Ordered Pair ===
Let $\tuple {a, b}$ be an ordered pair.

The following terminology is used:
:$a$ is called the first coordinate
:$b$ is called the second coordinate.

This definition is compatible with the equivalent definition in the context of Cartesian coordinate systems.

=== Origin ===
The origin of a coordinate system is the zero vector.

In the $x y$-plane, it is the point:
:$O = \tuple {0, 0}$
and in general, in the Euclidean space $\R^n$:
:$O = \underbrace {\tuple {0, 0, \ldots, 0} }_{\text{$n$ coordinates} }$


Thus it is the point where the axes cross over each other.",Definition:Coordinate System,['Definitions/Coordinate Systems']
Definition:System,System,A coordinate system whose coordinate axes are straight lines is called a system of rectilinear coordinate system.,Definition:Rectilinear Coordinate System,"['Definitions/Rectilinear Coordinate Systems', 'Definitions/Orthogonal Coordinate Systems', 'Definitions/Coordinate Systems']"
Definition:System,System,"A coordinate system such that at least one of the coordinate axes is a curved line is called a system of curvilinear coordinates.


=== Cartesian Representation ===
The relation between curvilinear coordinates and Cartesian coordinates can be expressed as:

 
 
 
 
 


 
 
 
 
 


where:
:$\tuple {x, y, z}$ denotes the Cartesian coordinates
:$\tuple {q_1, q_2, q_3}$ denotes their curvilinear equivalents.",Definition:Curvilinear Coordinate System,"['Definitions/Coordinate Systems', 'Definitions/Curvilinear Coordinates']"
Definition:System,System,"An orthogonal coordinate system is a coordinate system in which the coordinate axes are pairwise perpendicular.


=== Orthogonal Curvilinear Coordinates ===
Let $\KK$ be a curvilinear coordinate system in $3$-space.

Let $\QQ_1$, $\QQ_2$ and $\QQ_3$ denote the one-parameter families that define the curvilinear coordinates.



Let the relation between those curvilinear coordinates and Cartesian coordinates be expressed as:

 
 
 
 
 

where:
:$\tuple {x, y, z}$ denotes the Cartesian coordinates of an arbitrary point $P$
:$\tuple {q_1, q_2, q_3}$ denotes the curvilinear coordinates of $P$.


Let these equations have the property that the metric of $\KK$ between coordinate surfaces of $\QQ_i$ and $\QQ_j$ is zero where $i \ne j$.


That is, for every point $P$ expressible as $\tuple {x, y, z}$ and $\tuple {q_1, q_2, q_3}$:

:$\dfrac {\partial x} {\partial q_i} \dfrac {\partial x} {\partial q_j} + \dfrac {\partial y} {\partial q_i} \dfrac {\partial y} {\partial q_j} + \dfrac {\partial z} {\partial q_i} \dfrac {\partial z} {\partial q_j} = 0$

wherever $i \ne j$.


Then $\KK$ is an orthogonal curvilinear coordinate system.

=== Rectangular Coordinate System ===
A rectangular coordinate system is a Cartesian coordinate system in which each of the coordinate axes are perpendicular to each other.",Definition:Orthogonal Coordinate System,"['Definitions/Orthogonal Coordinate Systems', 'Definitions/Coordinate Systems']"
Definition:System,System,"A Cartesian coordinate system is a coordinate system in which the position of a point is determined by its relation to a set of perpendicular straight lines.

These straight lines are referred to as coordinate axes.",Definition:Cartesian Coordinate System,"['Definitions/Cartesian Coordinate Systems', 'Definitions/Examples of Coordinate Systems', 'Definitions/Examples of Rectilinear Coordinate Systems', 'Definitions/Analytic Geometry']"
Definition:System,System,A rectangular coordinate system is a Cartesian coordinate system in which each of the coordinate axes are perpendicular to each other.,Definition:Rectangular Coordinate System,['Definitions/Cartesian Coordinate Systems']
Definition:System,System,"An oblique coordinate system is a coordinate system in which the position of a point is determined by its relation to a set of straight coordinate axes which are oblique.

:",Definition:Oblique Coordinate System,"['Definitions/Oblique Coordinate Systems', 'Definitions/Coordinate Systems']"
Definition:System,System,"An algebraic system is a mathematical system $\SS = \struct {E, O}$ where:

:$E$ is a non-empty set of elements

:$O$ is a set of finitary operations on $E$.",Definition:Algebraic System,['Definitions/Abstract Algebra']
Definition:System,System,"A mathematical system is a set $\SS = \struct {E, O, A}$ where:

:$E$ is a non-empty set of elements

:$O$ is a set of relations and operations on the elements of $E$

:$A$ is a set of axioms concerning the elements of $E$ and $O$.


=== Abstract System ===

A mathematical system $\SS = \struct {E, O, A}$ is classed as abstract  if and only if  the elements of $E$ and $O$ are defined only by their properties as specified in $A$.


=== Concrete System ===

A mathematical system $\SS = \struct {E, O, A}$ is classed as concrete  if and only if  the elements of $E$ and $O$ are understood as objects independently of their existence in $\SS$ itself.


The distinction between abstract and concrete is of questionable value from a modern standpoint, as it is a moot point, for example, as to whether the natural numbers exist independently of Peano's axioms or are specifically defined by them.


=== Algebraic System ===

A mathematical system $\SS = \struct {E, O, A}$ is classed as algebraic  if and only if  it has many of the properties of the set of integers.

This is usually because such a system is itself an abstraction of certain properties of the integers.

The axioms are usually not considered as separate entities from the operations, as their nature is implicit in the operations themselves.


Specifically, an algebraic system can be defined as follows:
An algebraic system is a mathematical system $\SS = \struct {E, O}$ where:

:$E$ is a non-empty set of elements

:$O$ is a set of finitary operations on $E$.",Definition:Mathematical System,['Definitions/Abstract Algebra']
Definition:System,System,The solar system is the system of celestial bodies which are under the direct influence of the gravitational field of the sun.,Definition:Solar System,"['Definitions/Astronomy', 'Definitions/Celestial Mechanics', 'Definitions/Solar System']"
Definition:System,System,A number system is a technique for representing numbers.,Definition:Number System,['Definitions/Number Systems']
Definition:System,System,"The vigesimal system is base $20$ notation.

That is, every number $x \in \R$ is expressed in the form:
:$\ds x = \sum_{j \mathop \in \Z} r_j 20^j$
where:
:$\forall j \in \Z: r_j \in \set {0, 1, \ldots, 19}$",Definition:Vigesimal System,['Definitions/Number Bases']
Definition:System,System,"The golden mean number system is a system for representing a non-negative real number $x$ by a sequence of zeroes and  ones using the golden mean $\phi$ as a number base.


=== Equivalent Representations ===
Let $x \in \R_{\ge 0}$ have two representations $S_1$ and $S_2$ in the golden mean number system.

Then $S_1$ and $S_2$ are equivalent representations.


Category:Definitions/Golden Mean Number System

=== Simplest Form ===
Let $x \in \R_{\ge 0}$ have a representation $S$ in the golden mean number system.

Then $S$ is the simplest form for $x$  if and only if :

:$(1): \quad$ No two adjacent $1$s appear in $S$
:$(2): \quad S$ does not end with the infinite sequence $\cdotp \ldots 010101 \ldots$

=== Simplification ===
Consider the golden mean number system.

Let $x \in \R_{\ge 0}$ have a representation which includes the string $011$, say:
:$x = p011q$

where $p$ and $q$ are strings in $\left\{ {0, 1}\right\}$.


From 100 in Golden Mean Number System is Equivalent to 011, $x$ can also be written as:
:$x = p100q$


The expression $p100q$ is a simplification of $p011q$.

=== Expansion ===
Consider the golden mean number system.

Let $x \in \R_{\ge 0}$ have a representation which includes the string $100$, say:
:$x = p100q$

where $p$ and $q$ are strings in $\left\{ {0, 1}\right\}$.


From 100 in Golden Mean Number System is Equivalent to 011, $x$ can also be written as:
:$x = p011q$


The expression $p011q$ is an expansion of $p011q$.",Definition:Golden Mean Number System,"['Definitions/Number Bases', 'Definitions/Golden Mean', 'Definitions/Golden Mean Number System']"
Definition:System,System,The factorial number system is a mixed radix positional numeral system in which the digit in the $i$th column from the right is of base $i$.,Definition:Factorial Number System,['Definitions/Number Bases']
Definition:System,System,A decimal system is a system of measurement in which the standard multiples and fractions of the units of measurement are powers of $10$.,Definition:Decimal System,"['Definitions/Decimal System', 'Definitions/Decimal', 'Definitions/Number Systems', 'Definitions/Units of Measurement']"
Definition:System,System,"The combinatorial number system is a system for representing a positive integer $m$ by a sequence of digits which are the upper coefficient of a sequence of $n$ binomial coefficients for some $n \in \Z_{>0}$:

:$m := k_1 k_2 k_3 \ldots k_n$

where:
:$m = \dbinom {k_1} 1 + \dbinom {k_2} 2 + \dbinom {k_3} 3 + \cdots + \dbinom {k_n} n$
:$0 \le k_1 < k_2 < \cdots < k_n$",Definition:Combinatorial Number System,['Definitions/Binomial Coefficients']
Definition:System,System,"Zeckendorf representation is a system for representing a positive integer $m$ by a sequence of digits which are the indices of a sequence of $r$ Fibonacci numbers:

:$n := k_1 k_2 k_3 \ldots k_r$

where:
:$n = F_{k_1} + F_{k_2} + F_{k_3} + \cdots + F_{k_r}$
:$k_1 \gg k_2 \gg k_3 \gg \cdots \gg k_r \gg 0$

where $n \gg k$ denotes that $n \ge k + 2$.",Definition:Zeckendorf Representation,"['Definitions/Fibonacci Numbers', 'Definitions/Number Bases']"
Definition:System,System,"A positional number system is a number system with the following properties:

:It has a set of numerals which represent a subset of the numbers.

:The number being represented is written as a string of these numerals, which represent a different value according to their position in the numerals.

The design of the positional number system is such that all numbers can be represented by such a string, which may or may not be infinite in length.",Definition:Positional Numeral System,['Definitions/Number Systems']
Definition:System,System,"The number system as used in the   was a positional numeral system where the number base was a combination of decimal (base $10$) and sexagesimal (base $60$).

The characters were written in   by a combination of:
:a thin vertical wedge shape, to indicate the digit $1$
:a fat horizontal wedge shape, to indicate the digit $10$
arranged in groups to indicate the digits $2$ to $9$ and $20$ to $50$.


:


At $59$ the pattern stops, and the number $60$ is represented by the digit $1$ once again.

Thus these groupings were placed side by side:
:the rightmost grouping would indicate a number from $1$ to $59$
:the one to the left of that would indicate a number from $60 \times 1$ to $60 \times 59$
and so on, each grouping further to the left indicating another multiplication by $60$


For fractional numbers there was no actual radix point. Instead, the distinction was inferred by context.


The fact that they had no symbol to indicate the zero digit means that this was not a true positional numeral system as such.


For informal everyday arithmetic, they used a decimal system which was the decimal part of the full sexagesimal system.",Definition:Babylonian Number System,"['Definitions/Numeral Systems', 'Definitions/Babylonian Number System', 'Definitions/Babylonian Mathematics']"
Definition:System,System,"There are various number-naming systems for naming large numbers (that is: greater than $1 \, 000 \, 000$).


=== Short Scale ===
The short scale system is the number-naming system which uses:
:the word million for $10^6 = 1 \, 000 \, 000$
:the Latin-derived prefixes bi-, tri-, quadri-, quint-, etc. for each further multiple of $1 \, 000$, appended to the root -(i)llion, corresponding to the indices $2$, $3$, $4$, $5$, $\ldots$


Thus:

 
 
 
 
 
 

Thus one $n$-illion equals $1000 \times 10^{3 n}$ or $10^{3 n + 3}$

=== Long Scale ===
The long scale system is the number-naming system which uses:
:the word million for $10^6 = 1 \, 000 \, 000$
:the Latin-derived prefixes bi-, tri-, quadri-, quint-, etc. for each further multiple of $1 \, 000 \, 000$, appended to the root -(i)llion, corresponding to the indices $2$, $3$, $4$, $5$, $\ldots$


Thus:

 
 
 
 
 
 

Thus one $n$-illion equals $10^{6 n}$.


Additional terms are occasionally found to fill some of the gaps, but these are rare nowadays:

 
 
 
 ",Definition:Number-Naming System,['Definitions/Number-Naming Systems']
Definition:System,System,"The metric system is the colloquial term for the system of measurement based on the metre.

Its main characteristic is that its units are constructed on a decimal system.",Definition:Metric System,"['Definitions/Metric System', 'Definitions/Units of Measurement']"
Definition:System,System,"A physical system is a portion of the physical universe which has been chosen for investigation for a particular purpose.


Category:Definitions/Physics",Definition:Physical System,['Definitions/Physics']
Definition:System,System,"Let $X$ be a set, and let $\DD \subseteq \powerset X$ be a collection of subsets of $X$.


Then $\DD$ is called a Dynkin system (on $X$)  if and only if  it satisfies the following conditions:

:$(1): \quad X \in \DD$
:$(2): \quad \forall D \in \DD: X \setminus D \in \DD$
:$(3): \quad$ For all pairwise disjoint sequences $\sequence {D_n}_{n \mathop \in \N}$ in $\DD$, $\ds \bigcup_{n \mathop \in \N} D_n \in \DD$",Definition:Dynkin System,"['Definitions/Set Systems', 'Definitions/Measure Theory', 'Definitions/Dynkin Systems']"
Definition:System,System,"Let $X$ be a set.

Let $\GG \subseteq \powerset X$ be a collection of subsets of $X$.


Then the Dynkin system generated by $\GG$, denoted $\map \delta \GG$, is the smallest Dynkin system on $X$ that contains $\GG$.

That is, $\map \delta \GG$ is subject to:

:$(1):\quad \GG \subseteq \map \delta \GG$
:$(2):\quad \GG \subseteq \DD \implies \map \delta \GG \subseteq \DD$ for any Dynkin system $\DD$ on $X$


In fact, $\map \delta \GG$ always exists, and is unique, as proved on Existence and Uniqueness of Dynkin System Generated by Collection of Subsets.


=== Generator ===

One says that $\GG$ is a generator for $\map \delta \GG$.",Definition:Dynkin System Generated by Collection of Subsets,['Definitions/Dynkin Systems']
Definition:System,System,"A dynamical system is a non-linear system in which a function describes the time dependence of a point in a geometrical space.


=== Flow ===
In a dynamical system, a set of time-dependent equations is known as flow.

 ",Definition:Dynamical System,"['Definitions/Dynamical Systems', 'Definitions/Dynamical Systems Theory']"
Definition:System,System,"A numeral system is:
:a set of symbols that is used to represent a specific subset of the set of numbers (usually natural numbers), referred to as numerals
:a set of rules which define how to combine the numerals so as to be able to express other numbers.",Definition:Numeral System,"['Definitions/Numeral Systems', 'Definitions/Numbers']"
Definition:System,System,"Let $S$ be a finite set.

Let $\mathscr F$ be a set of subsets of $S$ satisfying the independence system axioms:
 

The ordered pair $I = \struct {S, \mathscr F}$ is called an independence system on $S$.",Definition:Independence System,['Definitions/Matroid Theory']
Definition:System,System,"Let $L$ be an ordered set.

Let $S$ be a system of $L$.


Then $S$ is closure  if and only if  $S$ inherits infima.",Definition:Closure System,['Definitions/Order Theory']
Definition:System,System,"There are five main classes of number:

:$(1): \quad$ The natural numbers: $\N = \set {0, 1, 2, 3, \ldots}$
::$(1 \text a): \quad$ The non-zero natural numbers: $\N_{>0} = \set {1, 2, 3, \ldots}$
:$(2): \quad$ The integers: $\Z = \set {\ldots, -3, -2, -1, 0, 1, 2, 3, \ldots}$
:$(3): \quad$ The rational numbers: $\Q = \set {p / q: p, q \in \Z, q \ne 0}$
:$(4): \quad$ The real numbers: $\R = \set {x: x = \sequence {s_n} }$ where $\sequence {s_n}$ is a Cauchy sequence in $\Q$
:$(5): \quad$ The complex numbers: $\C = \set {a + i b: a, b \in \R, i^2 = -1}$


It is possible to categorize numbers further, for example:

:The set of algebraic numbers $\mathbb A$ is the subset of the complex numbers which are roots of polynomials with rational coefficients.  The algebraic numbers include the rational numbers, $\sqrt 2$, and the golden section $\varphi$.

:The set of transcendental numbers is the set of all the real numbers which are not algebraic.  The transcendental numbers include $\pi, e$ and $\sqrt 2^{\sqrt 2}$.

:The set of prime numbers (sometimes referred to as $\mathbb P$) is the subset of the integers which have exactly two positive divisors, $1$ and the number itself.  The first several positive primes are $2, 3, 5, 7, 11, 13, \ldots$",Definition:Number,['Definitions/Numbers']
Definition:System,System,"There are three imperial systems of measurement of mass:

* Avoirdupois

* Apothecaries' weights

* Troy


=== Grain ===
The grain is the imperial unit of mass which is used as the basis of all three of the imperial weight systems.",Definition:Imperial/Mass,"['Definitions/Imperial', 'Definitions/Mass']"
Definition:System,System,"A Steiner triple system of order $v$ is a BIBD with block size $3$, and each pair of points occurring together in exactly $1$ block (called a triple).",Definition:Steiner Triple System,['Definitions/Design Theory']
Definition:System,System,A non-linear system is a system of differential equations which are non-linear.,Definition:Non-Linear System,"['Definitions/Non-Linear Systems', 'Definitions/Differential Equations', 'Definitions/Applied Mathematics', 'Definitions/Branches of Mathematics']"
Definition:System,System,"Hilbert proof systems are a class of proof systems for propositional and predicate logic.

Their characteristics include:

* The presence of many axioms;
* Typically only Modus Ponendo Ponens as a rule of inference.

Specific instances may deviate from this general scheme at some points.",Definition:Hilbert Proof System,"['Definitions/Proof Systems', 'Definitions/Propositional Logic', 'Definitions/Predicate Logic']"
Definition:System,System,"Let $m \in \Z_{\ne 0}$ be a non-zero integer.


Let $S := \set {r_1, r_2, \dotsb, r_s}$ be a set of integers with the properties that:

:$(1): \quad i \ne j \implies r_i \not \equiv r_j \pmod m$

:$(2): \quad \forall n \in \Z: \exists r_i \in S: n \equiv r_i \pmod m$


Then $S$ is a complete residue system modulo $m$.",Definition:Complete Residue System,['Definitions/Residue Classes']
Definition:System,System,"Let $m \in \Z_{> 0}$ be a (strictly) positive integer.


The reduced residue system modulo $m$, denoted $\Z'_m$, is the set of all residue classes of $k$ (modulo $m$) which are prime to $m$:

:$\Z'_m = \set {\eqclass k m \in \Z_m: k \perp m}$


Thus $\Z'_m$ is the set of all coprime residue classes modulo $m$:
:$\Z'_m = \set {\eqclass {a_1} m, \eqclass {a_2} m, \ldots, \eqclass {a_{\map \phi m} } m}$
where:
:$\forall k: a_k \perp m$
:$\map \phi m$ denotes the Euler phi function of $m$.",Definition:Reduced Residue System,"['Definitions/Residue Classes', 'Definitions/Reduced Residue Systems', 'Definitions/Modulo Arithmetic']"
Definition:System,System,"A set of sets is a set, whose elements are themselves all sets.


Those elements can themselves be assumed to be subsets of some particular fixed set which is frequently referred to as the universe.",Definition:Set of Sets,"['Definitions/Set Systems', 'Definitions/Set Theory']"
Definition:System,System,"A system of simultaneous equations is a set of equations:

:$\forall i \in \set {1, 2, \ldots, m} : \map {f_i} {x_1, x_2, \ldots x_n} = \beta_i$


That is:

 
 
 
 
 
 


=== Linear Equations ===
A system of simultaneous linear equations is a set of linear equations:

:$\ds \forall i \in \set {1, 2, \ldots, m} : \sum_{j \mathop = 1}^n \alpha_{i j} x_j = \beta_i$


That is:

 
 
 
 
 
 


=== Solution ===
Consider the system of simultaneous linear equations:

:$\ds \forall i \in \set {1, 2, \ldots, m} : \sum_{j \mathop = 1}^n \alpha_{i j} x_j = \beta_i$


That is:

 
 
 
 
 
 


Let $\tuple {x_1, x_2, \ldots, x_n}$ satisfy each of the linear equations in $\ds \sum_{j \mathop = 1}^n \alpha_{i j} x_j = \beta_i$.

Then $\tuple {x_1, x_2, \ldots, x_n}$ is referred to as a solution (to the system of simultaneous linear equations).

=== Matrix Representation ===
A system of simultaneous linear equations can be (and commonly is) expressed in its matrix representation:
:$\mathbf A \mathbf x = \mathbf b$
where:

$\quad \mathbf A = \begin {bmatrix}
\alpha_{1 1} & \alpha_{1 2} & \cdots & \alpha_{1 n} \\
\alpha_{2 1} & \alpha_{2 2} & \cdots & \alpha_{2 n} \\
\vdots & \vdots & \ddots & \vdots \\
\alpha_{m 1} & \alpha_{m 2} & \cdots & \alpha_{m n} \\
\end {bmatrix}$,  $\mathbf x = \begin {bmatrix} x_1 \\ x_2 \\ \vdots \\ x_n \end {bmatrix}$, $\mathbf b = \begin {bmatrix} \beta_1 \\ \beta_2 \\ \vdots \\ \beta_m \end {bmatrix}$

are matrices.


=== Matrix of Coefficients ===
Consider the system of simultaneous linear equations can be expressed as:

:$\ds \forall i \in \set {1, 2, \ldots, m} : \sum_{j \mathop = 1}^n \alpha_{i j} x_j = \beta_i$

expressed in matrix representation as:
:$\mathbf A \mathbf x = \mathbf b$


The matrix $\mathbf A$ is known as the matrix of coeffficients of the system.

=== Augmented Matrix ===
Consider the system of simultaneous linear equations:

:$\ds \forall i \in \set {1, 2, \ldots, m} : \sum_{j \mathop = 1}^n \alpha_{i j} x_j = \beta_i$

expressed in matrix representation as:
:$\mathbf A \mathbf x = \mathbf b$


Let $\begin {bmatrix} \mathbf A & \mathbf b \end {bmatrix}$ be the block matrix formed from $\mathbf A$ and $\mathbf b$.

Then $\begin {bmatrix} \mathbf A & \mathbf b \end {bmatrix}$ is known as the augmented matrix of the system.


Thus:

$\quad \begin {bmatrix} \mathbf A & \mathbf b \end {bmatrix} = \begin {bmatrix}
\alpha_{1 1} & \alpha_{1 2} & \cdots & \alpha_{1 n} & \beta_1 \\
\alpha_{2 1} & \alpha_{2 2} & \cdots & \alpha_{2 n} & \beta_2 \\
\vdots & \vdots & \ddots & \vdots & \vdots \\
\alpha_{m 1} & \alpha_{m 2} & \cdots & \alpha_{m n} & \beta_m \\
\end {bmatrix}$",Definition:Simultaneous Equations,"['Definitions/Linear Algebra', 'Definitions/Simultaneous Equations']"
Definition:System,System,A system of measurement is a set of fundamental units of measurement with which one can measure any measurable physical property.,Definition:System of Measurement,"['Definitions/Physics', 'Definitions/Units of Measurement', 'Definitions/Systems of Measurement']"
Definition:System,System,"The US volume system is a system of measurement based on the imperial system and adapted by the emergent  .


=== Pint ===
The (US) pint is a unit of volume used in the United States.

 
 
 
 
 
 
 
 

=== Quart ===
The (US) quart is a unit of volume used in the United States.

 
 
 
 
 
 
 
 

=== Gallon ===
The (US) gallon is a unit of volume used in the United States.

It is defined as $231$ cubic inches, which is exactly $3.78541 \, 1784$ litres.


=== Conversion Factors ===


=== Symbol ===
 ",Definition:US Volume System,"['Definitions/Systems of Measurement', 'Definitions/US Volume System']"
Definition:System,System,"Let $L = \left({S, \preceq}\right)$ be an ordered set.


The system of $L$ is an ordered subset of $L$.",Definition:System (Order Theory),['Definitions/Order Theory']
Definition:System,System,"CGS is the centimetre-gram-second standard system of units of measurement.


This system is rarely used nowadays, the SI units having largely taken over.


=== CGS Base Units ===
The base units of the CGS system are as follows:

 
 
 
 
 
 
 
| centimetre
| $\mathrm{cm}$
| Length
| $l$
 
| gram
| $\mathrm g$
| Mass
| $m$
 
| second
| $\mathrm s$
| Time
| $t$
 

=== CGS Derived Units ===
The derived units of the CGS system include the following:


==== Dyne ====
The dyne is the CGS unit of force.

It is defined as being:

:The amount of force required to accelerate a mass of one gram at a rate of one centimetre per second squared.


This arises from Newton's Second Law of Motion, which states that such an acceleration exists under the influence of such a force.


=== Conversion Factors ===


=== Symbol ===
 

=== Base Units ===
The CGS base units of the dyne are:

:$\mathrm {dyn} := \mathrm g \, \mathrm {cm} \, \mathrm s^{-2}$
where:
:$\mathrm g$ denotes grams
:$\mathrm {cm}$ denotes centimetres
:$\mathrm s$ denotes seconds (of time).

==== Erg ====
The erg is the CGS unit of energy:


It is defined as being:

:the energy transferred to (or work done on) a body when a force of $1$ dyne acts on that body in the direction of the force's motion through a distance of $1$ centimetre.

Thus:
:$1 \, \mathrm {erg} = 1 \, \mathrm {dyn} \, \mathrm {cm}$


=== Conversion Factors ===


=== Symbol ===
 

=== Base Units ===
The CGS base units of the erg are:

:$1 \ \mathrm {erg} = 1 \ \mathrm g \ \mathrm {cm}^2 \ \mathrm s^{-2}$
where:
:$\mathrm g$ denotes grams
:$\mathrm {cm}$ denotes centimetres
:$\mathrm s$ denotes seconds (of time).

Category:Definitions/Units of Measurement
Category:Definitions/CGS",Definition:CGS,"['Definitions/Systems of Measurement', 'Definitions/CGS']"
Definition:System,System,"Let $m \in \Z_{> 0}$.

The least positive reduced residue system modulo $m$ is the set of integers:
:$\set {a_1, a_2, \ldots, a_{\map \phi m} }$
with the following properties:
:$\map \phi m$ is the Euler $\phi$ function
:$\forall i: 0 < a_i < m$
:each of which is prime to $m$
:no two of which are congruent modulo $m$.",Definition:Reduced Residue System/Least Positive,['Definitions/Residue Classes']
Definition:System,System,"Let $B = \struct {E, M, \pi, F}$ be a fiber bundle. 

Let $\UU = \set {U_\alpha \subseteq M: \alpha \in I}$ be an open cover of $M$ with index set $I$. 

Let $\struct {U_\alpha, \chi_\alpha}$ be local trivializations for all $\alpha \in I$. 


The set $\set {\struct {U_\alpha, \chi_\alpha}: \alpha \in I}$ is called a system of local trivializations of $E$ on $M$.",Definition:Fiber Bundle/System of Local Trivializations,['Definitions/Fiber Bundles']
Definition:System,System,"A system of differential equations is a set of simultaneous differential equations.

The solutions for each of the differential equations are in general expected to be consistent.


=== Autonomous System ===
A system of differential equations is autonomous if all of the differential equations which it comprises are themselves autonomous.


Category:Definitions/Differential Equations

Category:Definitions/Differential Equations",Definition:Differential Equation/System,['Definitions/Differential Equations']
Definition:Table,Table,"A reference table is a set of pages, arranged usually in book form, containing arrays of (usually) numbers arranged in rows and columns for ease of look-up.

They have been generally superseded by computers now, but facility in their use is generally considered advantageous.",Definition:Reference Table,"['Definitions/Reference Tables', 'Definitions/Tools', 'Definitions/Proof Techniques']"
Definition:Table,Table,"A Cayley table is a technique for describing an algebraic structure (usually a finite group) by putting all the products in a square array:

$\qquad \begin {array} {c|cccc}
\circ & a & b & c & d \\
\hline
a & a & a & b & a \\
b & b & c & a & d \\
c & d & e & f & a \\
d & c & d & a & b \\
\end {array}$


The column down the   denotes the first (leading) operand of the operation.

The row across the top denotes the second (following) operand of the operation.

Thus, in the above Cayley table:
:$c \circ a = d$


If desired, the symbol denoting the operation itself can be put in the upper left corner, but this is not essential if there is no ambiguity.


The order in which the rows and columns are placed is immaterial.

However, it is conventional, when representing an algebraic structure with an identity element, to place that element at the head of the first row and column.


=== Entry ===
The occurrences in a Cayley table of the elements of the algebraic structure it defines are called the entries of the Cayley table.


Category:Definitions/Cayley Tables",Definition:Cayley Table,"['Definitions/Cayley Tables', 'Definitions/Abstract Algebra']"
Definition:Tableau Proof,Tableau Proof,"A tableau proof for a proof system is a technique for presenting a logical argument in the form of a formal proof in a straightforward, standard form.

On  , the proof system is usually natural deduction.


A tableau proof is a sequence of lines specifying the order of premises, assumptions, inferences and conclusion in support of an argument.


Each line of a tableau proof has a particular format. It consists of the following parts:

* Line: The line number of the proof. This is a simple numbering from 1 upwards.
* Pool: The list of all the lines containing the pool of assumptions for the formula introduced on this line.
* Formula: The propositional formula introduced on this line.
* Rule: The justification for introducing this line. This should be the rule of inference being used to derive this line.
* Depends on: The lines (if any) upon which this line directly depends. For premises and assumptions, this field will be empty.


Optionally, a comment may be added to explicitly point out possible intricacies.

If any assumptions are discharged on a certain line, for the sake of clarity it is preferred that such be mentioned explicitly in a comment.


At the end of a tableau proof, the only lines upon which the proof depends may be those which contain the premises.


=== Length ===
The length of a tableau proof is the number of lines it has.


Category:Definitions/Proof Systems",Definition:Tableau Proof (Formal Systems),['Definitions/Proof Systems']
Definition:Tableau Proof,Tableau Proof,"Let $\mathbf H$ be a set of WFFs of propositional logic.

Let $\mathbf A$ be a WFF.


A tableau proof of $\mathbf A$ from $\mathbf H$ is a tableau confutation of $\mathbf H \cup \set {\neg \mathbf A}$.


This definition also applies when $\mathbf H = \O$.

Then a tableau proof of $\mathbf A$ is a tableau confutation of $\set {\neg \mathbf A}$.


If there exists a tableau proof of $\mathbf A$ from $\mathbf H$, one can write:
:$\mathbf H \vdash_{\mathrm{PT} } \mathbf A$

Specifically, the notation:
:$\vdash_{\mathrm{PT} } \mathbf A$
means that there exists a tableau proof of $\mathbf A$.


=== Proof System ===
",Definition:Tableau Proof (Propositional Tableaus),"['Definitions/Propositional Tableaus', 'Definitions/Proof Systems']"
Definition:Tangent,Tangent,"Let $f: \R \to \R$ be a real function.

Let the graph of $f$ be depicted on a Cartesian plane.


:


Let $A = \tuple {x, \map f x}$ be a point on $G$.


The tangent to $f$ at $A$ is defined as:
:$\ds \lim_{h \mathop \to 0} \frac {\map f {x + h} - \map f x} h$


Thus the tangent to $f$ at $x$ can be considered as the secant $AB$ to $G$ where:
:$B = \tuple {x + h, \map f {x + h} }$
as $B$ gets closer and closer to $A$.

By taking $h$ smaller and smaller, the secant approaches more and more closely the [[Definition:Tangent Line|tangent]] to $G$ at $A$.


Hence the tangent to $f$ is a straight line which intersects the graph of $f$ locally at a single point.


:


In the above diagram, the tangent is the straight line passing through $A$.


=== Tangent to Circle ===
 
: 
:A straight line is said to touch a circle which, meeting the circle and being produced, does not cut the circle.
 ''
 

:

In the above diagram, the line is tangent to the circle at the point $C$.


=== Tangent Circles ===
 
: 
:Circles are said to touch one another which, meeting one another, do not cut one another.
 ''
 

:

In the above diagram, the two circles are tangent to each other at the point $C$.


Category:Definitions/Circles
Category:Definitions/Tangents

=== Point of Contact ===
Let $f: \R \to \R$ be a real function.

Let the graph of $f$ be depicted on a Cartesian plane.


:


Let $A = \tuple {x, \map f x}$ be a point on $G$.

Let $\LL$ be tangent to $f$ at $A$.

Then $A$ is known as the point of contact of $\LL$ to $f$.",Definition:Tangent Line,"['Definitions/Tangents', 'Definitions/Analytic Geometry']"
Definition:Tangent,Tangent,":

In the above right triangle, we are concerned about the angle $\theta$.

The tangent of $\angle \theta$ is defined as being $\dfrac{\text{Opposite}} {\text{Adjacent}}$.",Definition:Tangent Function/Definition from Triangle,['Definitions/Tangent Function']
Definition:Tangent,Tangent,"Let $x \in \R$ be a real number.

The real function $\tan x$ is defined as:

:$\tan x = \dfrac {\sin x} {\cos x}$

where:
: $\sin x$ is the sine of $x$
: $\cos x$ is the cosine of $x$.

The definition is valid for all $x \in \R$ such that $\cos x \ne 0$.",Definition:Tangent Function/Real,['Definitions/Tangent Function']
Definition:Tangent,Tangent,"Let $z \in \C$ be a complex number.

The complex function $\tan z$ is defined as:

:$\tan z = \dfrac {\sin z} {\cos z}$

where:
: $\sin z$ is the sine of $z$
: $\cos z$ is the cosine of $z$.

The definition is valid for all $z \in \C$ such that $\cos z \ne 0$.",Definition:Tangent Function/Complex,['Definitions/Tangent Function']
Definition:Tensor Product,Tensor Product,"Let $A$ and $B$ be abelian groups.


=== Definition 1: by universal property ===

Their tensor product is a pair $\struct {A \otimes B, \theta}$ where:
:$A \otimes B$ is an abelian group
:$\theta : A \times B \to A \otimes B$ is a biadditive mapping such that, for every ordered pair $\struct {C, \omega}$ where:
:$C$ is an abelian group
:$\omega : A \times B \to C$ is a biadditive mapping
there exists a unique group homomorphism $g : A \otimes B \to C$ such that $\omega = g \circ \theta$.


=== Definition 2: construction ===

Their tensor product is the pair $\struct {A \otimes B, \theta}$ where:
:$A \otimes B$ is the quotient of the free abelian group $\Z^{\paren {A \times B} }$ on the cartesian product $A \times B$ by the subgroup generated by the elements of the form:
:::$\tuple {a_1 + a_2, b} - \tuple {a_1, b} - \tuple {a_2, b}$
:::$\tuple {a, b_1 + b_2} - \tuple {a, b_1} - \tuple {a, b_2}$
::for $a, a_1, a_2 \in A$, $b, b_1, b_2 \in B$, where we denote $\tuple {a, b}$ for its image under the canonical mapping $A \times B \to \Z^{\paren {A \times B} }$.
:$\theta : A \times B \to A \otimes B$ is the composition of the canonical mapping $A \times B \to \Z^{\paren {A \times B} }$ with the quotient group epimorphism $\Z^{\paren {A \times B} } \to A \otimes B$.",Definition:Tensor Product of Abelian Groups,['Definitions/Abelian Groups']
Definition:Tensor Product,Tensor Product,"=== Commutative ring ===

Let $R$ be a commutative ring with unity.

Let $M$ and $N$ be $R$-modules.


=== Definition 1 ===

Their tensor product is a pair $\struct {M \otimes_R N, \theta}$ where:
:$M \otimes_R N$ is an $R$-module
:$\theta : M \times N \to M \otimes_R N$ is an $R$-bilinear mapping
satisfying the following universal property:
:For every pair $\struct {P, \omega}$ of an $R$-module and an $R$-bilinear mapping $\omega : M \times N \to P$, there exists a unique $R$-module homomorphism $f: M \otimes_R N \to P$ with $\omega = f \circ \theta$.


=== Definition 2 ===

Their tensor product is the pair $\struct {M \otimes_R N, \theta}$, where:
:$M \otimes_R N$ is the quotient of the free $R$-module $R^{\paren {M \times N} }$ on the direct product $M \times N$, by the submodule generated by the set of elements of the form:
::$\tuple {\lambda m_1 + m_2, n} - \lambda \tuple {m_1, n} - \tuple {m_2, n}$
::$\tuple {m, \lambda n_1 + n_2} - \lambda \tuple {m, n_1} - \tuple {m, n_2}$
::for $m, m_1, m_2 \in M$, $n, n_1, n_2 \in N$ and $\lambda \in R$, where we denote $\tuple {m, n}$ for its image under the canonical mapping $M \times N \to R^{\paren {M \times N} }$.
:$\theta : M \times N \to M \otimes_R N$ is the composition of the canonical mapping $M \times N \to R^{\paren {M \times N} }$ with the quotient module homomorphism $R^{\paren {M \times N} } \to M \otimes_R N$.


=== Noncommutative ring ===

Let $R$ be a ring.

Let $M$ be a $R$-right module.

Let $N$ be a $R$-left module.


First construct a left module as a direct sum of all free left modules with a basis that is a single ordered pair in $M \times N$ which is denoted $\map R {m, n}$.

:$T = \ds \bigoplus_{s \mathop \in M \mathop \times N} R s$


That this is indeed a module is demonstrated in Tensor Product is Module.


Next for all $m, m' \in M$, $n, n' \in N$ and $r \in R$ we construct the following free left modules.

:$L_{m, m', n}$ with a basis of $\tuple {m + m', n}$, $\tuple {m, n}$ and $\tuple {m', n}$
:$R_{m, n, n'}$ with a basis of $\tuple {m, n + n'}$, $\tuple {m, n}$ and $\tuple {m, n'}$
:$A_{r, m, n}$ with a basis of $r \tuple {m, n}$ and $\tuple {m r, n}$
:$B_{r, m, n}$ with a basis of $r \tuple {m, n}$ and $\tuple {m, r n}$

Let:

:$D = \ds \map {\bigoplus_{r \in R, n, n' \in N, m, m' \in M} } {L_{m, m', n} \oplus R_{m, n, n'} \oplus A_{r, m, n} \oplus B_{r, m, n} }$

The tensor product $M \otimes_R N$ is then our quotient module $T / D$.",Definition:Tensor Product of Modules,"['Definitions/Module Theory', 'Definitions/Tensor Algebra', 'Definitions/Homological Algebra']"
Definition:Term,Term,"Part of specifying the language of predicate logic $\LL_1$ is the introduction of terms.


The terms of $\LL_1$ are identified by the following bottom-up grammar:

 
 
 
 

Colloquially, we can think of a term as an expression signifying an object.",Definition:Language of Predicate Logic/Formal Grammar/Term,"['Definitions/Predicate Logic', 'Definitions/Language of Predicate Logic']"
Definition:Term,Term,"Both the subject and the predicate of a simple statement are referred to as its (logical) terms.


Note that this use of the word term is a more specialized use of the word term as used in algebra.",Definition:Logical Term,['Definitions/Predicate Logic']
Definition:Term,Term,"A term is either a variable or a constant.


Let $a \circ b$ be an expression.

Then each of $a$ and $b$ are known as the terms of the expression.


The word term is usually used when the operation $\circ$ is addition, that is $+$.",Definition:Term of Expression,"['Definitions/Algebra', 'Definitions/Symbolic Logic']"
Definition:Term,Term,"Let $P = a_n x^n + a_{n - 1} x^{n - 1} + \cdots + a_1 x + a_0$ be a polynomial.

Each of the expressions $a_i x^i$, for $0 \le i \le n$, is referred to as a term of $P$.",Definition:Polynomial/Term,['Definitions/Polynomial Theory']
Definition:Term,Term,"The terms of a fraction are referred to as the numerator and the denominator:

=== Numerator ===
Let $\dfrac a b$ be a fraction.

The term $a$ is known as the numerator of $\dfrac a b$.

=== Denominator ===
Let $\dfrac a b$ be a fraction.

The term $b$ is known as the denominator of $\dfrac a b$.

A helpful mnemonic to remember which goes on top and which goes on the bottom is ""Numerator Over Denominator"", which deserves a ""nod"" for being correct.",Definition:Fraction/Term,['Definitions/Fractions']
Definition:Term,Term,"The elements of a sequence are known as its terms.


Let $\sequence {x_n}$ be a sequence.

Then the $k$th term of $\sequence {x_n}$ is the ordered pair $\tuple {k, x_k}$.


=== Index ===
Let $\sequence {x_n}$ be a sequence.

Let $x_k$ be the $k$th term of $\sequence {x_n}$.

Then the integer $k$ is known as the index of $x_k$.",Definition:Term of Sequence,['Definitions/Sequences']
Definition:Term,Term,"Let $I$ and $S$ be sets.

Let $x: I \to S$ be a mapping.

Let $x_i$ denote the image of an element $i \in I$ of the domain $I$ of $x$.

Let $\family {x_i}_{i \mathop \in I}$ denote the set of the images of all the element $i \in I$ under $x$.


The image of $x$ at an index $i$ is referred to as a term of the (indexed) family, and is denoted $x_i$.


=== Notation ===
The family of elements $x$ of $S$ indexed by $I$ is often seen with one of the following notations:

:$\family {x_i}_{i \mathop \in I}$

:$\paren {x_i}_{i \mathop \in I}$

:$\set {x_i}_{i \mathop \in I}$


There is little consistency in the literature, but $\paren {x_i}_{i \mathop \in I}$ is perhaps most common.

The preferred notation on   is $\family {x_i}_{i \mathop \in I}$.

The subscripted $i \in I$ is often left out, if it is obvious in the particular context.


Note the use of $x_i$ to denote the image of the index $i$ under the indexing function $x$.

As $x$ is actually a mapping, one would expect the conventional notation $\map x i$.

However, this is generally not used, and $x_i$ is used instead.


Category:Definitions/Indexed Families",Definition:Indexing Set/Term,['Definitions/Indexed Families']
Definition:Term,Term,"Let $n \in \N_{>0}$.

Let $\sequence {a_k}_{k \mathop \in \N^*_n}$ be an ordered tuple.

The ordered pair $\tuple {k, a_k}$ is called the $k$th term of the ordered tuple for each $k \in \N^*_n$.",Definition:Ordered Tuple/Term,['Definitions/Ordered Tuples']
Definition:Term,Term,"A term is a noun which is assigned a specified definition within mathematics.

In a more specialized context, the word term is used for an element  of an expression.

Category:Definitions/Language Definitions",Definition:Term (Natural Language),['Definitions/Language Definitions']
Definition:Top,Top,"Top is a constant of propositional logic interpreted to mean the canonical, undoubted tautology whose truth nobody could possibly ever question.

The symbol used is $\top$.",Definition:Top (Logic),['Definitions/Top']
Definition:Top,Top,"Let $\struct {S, \vee, \wedge, \preceq}$ be a lattice.


=== Definition 1 ===
Let $\struct {S, \vee, \wedge, \preceq}$ be a lattice.

Let $S$ admit a greatest element $\top$.


Then $\top$ is called the top of $S$.

=== Definition 2 ===
Let $\struct {S, \vee, \wedge, \preceq}$ be a lattice.

Let $\wedge$ have an identity element $\top$.


Then $\top$ is called the top of $S$.",Definition:Top of Lattice,"['Definitions/Top of Lattice', 'Definitions/Lattice Theory']"
Definition:Torsion,Torsion,"Let $G$ be a group.


An element of finite order of $G$ is also known as a torsion element of $G$.


Category:Definitions/Order of Group Elements",Definition:Order of Group Element/Finite/Also known as,['Definitions/Order of Group Elements']
Definition:Torsion,Torsion,"Let $G$ be an abelian group.


Its torsion subgroup $\map T G$ is the subgroup of all torsion elements.",Definition:Torsion Subgroup,['Definitions/Examples of Subgroups']
Definition:Torsion,Torsion,"Let $G$ be a group.


Then $G$ is a torsion group  if and only if  all its elements are torsion elements.",Definition:Torsion Group,['Definitions/Group Theory']
Definition:Torsion,Torsion,"Let $R$ be a commutative ring with unity.

Let $M$ be a unitary module over $R$.

Let $m \in M$.


Then $m$ is a torsion element  if and only if  there exists a regular element $a \in R$ with $a m = 0$.",Definition:Torsion Element of Module,['Definitions/Module Theory']
Definition:Torsion,Torsion,"Let $R$ be a commutative ring with unity.

Let $M$ be a unitary module over $R$.


The torsion submodule $T(M)$ of $M$ is the submodule of all torsion elements of $M$.",Definition:Torsion Submodule,['Definitions/Module Theory']
Definition:Torsion,Torsion,"Let $R$ be a commutative ring with unity.

Let $M$ be a unitary module over $R$.


Then $M$ is a torsion module  if and only if  every element of $M$ is of torsion, that is, $M$ equals its torsion submodule $\map T M$.",Definition:Torsion Module,"['Definitions/Module Theory', 'Definitions/Commutative Algebra']"
Definition:Trace,Trace,"Let $K$ be a field and $L / K$ a finite field extension of $K$.

Then by Vector Space on Field Extension is Vector Space, $L$ is naturally a vector space over $K$.

Let $\alpha \in L$, and $\theta_\alpha$ be the linear operator:

:$\theta_\alpha: L \to L: \beta \mapsto \alpha\beta$


The trace $\map {\operatorname {Tr}_{L / K} } \alpha$ of $\alpha$ is the trace of this linear operator.",Definition:Trace (Field Theory),['Definitions/Field Theory']
Definition:Trace,Trace,"=== Matrix ===
Let $A = \sqbrk a_n$ be a square matrix of order $n$.


The trace of $A$ is:

:$\ds \map \tr A = \sum_{i \mathop = 1}^n a_{ii}$


=== Using Einstein Summation Convention ===
Let $A = \sqbrk {a_{ij} }_{1 \mathop \le i, j \mathop \le n}$ be a matrix.


The trace of $A$, using the Einstein summation convention, is:

:$\map \tr A = a_{ii}$

=== Linear Operator ===
Let $V$ be a vector space.

Let $A: V \to V$ be a linear operator of $V$.


The trace of $A$ is the trace of the matrix of $A$ with respect to some basis.


Category:Definitions/Linear Algebra

Category:Definitions/Linear Algebra
Category:Definitions/Matrix Algebra",Definition:Trace (Linear Algebra),"['Definitions/Linear Algebra', 'Definitions/Matrix Algebra']"
Definition:Trace,Trace,"Let $A = \sqbrk a_n$ be a square matrix of order $n$.


The trace of $A$ is:

:$\ds \map \tr A = \sum_{i \mathop = 1}^n a_{ii}$


=== Using Einstein Summation Convention ===
Let $A = \sqbrk {a_{ij} }_{1 \mathop \le i, j \mathop \le n}$ be a matrix.


The trace of $A$, using the Einstein summation convention, is:

:$\map \tr A = a_{ii}$",Definition:Trace (Linear Algebra)/Matrix,"['Definitions/Traces of Matrices', 'Definitions/Matrix Theory']"
Definition:Trace,Trace,"Let $V$ be a vector space.

Let $A: V \to V$ be a linear operator of $V$.


The trace of $A$ is the trace of the matrix of $A$ with respect to some basis.


Category:Definitions/Linear Algebra",Definition:Trace (Linear Algebra)/Linear Operator,['Definitions/Linear Algebra']
Definition:Trace,Trace,"Let $X$ be a set, and let $\Sigma$ be a $\sigma$-algebra on $X$.

Let $E \subseteq X$ be a subset of $X$.


Then the trace $\sigma$-algebra (of $E$ in $\Sigma$), $\Sigma_E$, is defined as:

:$\Sigma_E := \set {E \cap S: S \in \Sigma}$


It is a $\sigma$-algebra on $E$, as proved on Trace $\sigma$-Algebra is $\sigma$-Algebra.",Definition:Trace Sigma-Algebra,"['Definitions/Trace Sigma-Algebras', 'Definitions/Sigma-Algebras', 'Definitions/Trace Sigma-Algebras']"
Definition:Trace,Trace,"Let $P$ be a URM program.

The trace table of $P$ consists of:
* The stage of computation;
* The number of the instruction of $P$ that is about to be performed;
* A list of the contents of all the registers used by $P$ at this point.

Thus the trace table is a list of the states of the URM program at each stage.",Definition:Trace Table,['Definitions/Mathematical Logic']
Definition:Trace,Trace,"Let $\struct {M, g}$ be a Riemannian manifold.

Let $h$ be a covariant $k$-tensor field with $k \ge 2$.

Let $h^\sharp$ be a $\tuple {1, k - 1}$-tensor field obtained from $h$ by raising its index.


Then the trace of $h$   $g$ is  a covariant $\paren {k - 2}$-tensor field defined as:

:$\tr_g h := \map \tr {h^\sharp}$

where $\tr$ is the trace over a covariant and a contravariant index.
 ",Definition:Trace of Tensor,['Definitions/Riemannian Geometry']
Definition:Transcendental,Transcendental,"=== Field Extension ===
A field extension $E / F$ is said to be transcendental  if and only if :
:$\exists \alpha \in E: \alpha$ is transcendental over $F$

That is, a field extension is transcendental  if and only if  it contains at least one transcendental element.


=== Transcendental Element ===
Let $E / F$ be a field extension.

Let $\alpha \in E$.


Then $\alpha$ is transcendental over $F$  if and only if :
: $\nexists \map f x \in F \sqbrk x \setminus \set 0: \map f \alpha = 0$
where $\map f x$ denotes a polynomial in $x$ over $F$.

=== Transcendental over Integral Domain ===
Let $\struct {R, +, \circ}$ be a commutative ring with unity whose zero is $0_R$ and whose unity is $1_R$.

Let $\struct {D, +, \circ}$ be an integral subdomain of $R$.

Let $x \in R$.


Then $x$ is transcendental over $D$  if and only if :
:$\ds \forall n \in \Z_{\ge 0}: \sum_{k \mathop = 0}^n a_k \circ x^k = 0_R \implies \forall k: 0 \le k \le n: a_k = 0_R$


That is, $x$ is transcendental over $D$  if and only if  the only way to express $0_R$ as a polynomial in $x$ over $D$ is by the null polynomial.",Definition:Transcendental (Abstract Algebra),"['Definitions/Ring Theory', 'Definitions/Field Extensions', 'Definitions/Polynomial Theory']"
Definition:Transcendental,Transcendental,"A field extension $E / F$ is said to be transcendental  if and only if :
:$\exists \alpha \in E: \alpha$ is transcendental over $F$

That is, a field extension is transcendental  if and only if  it contains at least one transcendental element.


=== Transcendental Element ===
Let $E / F$ be a field extension.

Let $\alpha \in E$.


Then $\alpha$ is transcendental over $F$  if and only if :
: $\nexists \map f x \in F \sqbrk x \setminus \set 0: \map f \alpha = 0$
where $\map f x$ denotes a polynomial in $x$ over $F$.",Definition:Transcendental (Abstract Algebra)/Field Extension,"['Definitions/Field Extensions', 'Definitions/Polynomial Theory']"
Definition:Transcendental,Transcendental,"Let $\struct {R, +, \circ}$ be a commutative ring with unity whose zero is $0_R$ and whose unity is $1_R$.

Let $\struct {D, +, \circ}$ be an integral subdomain of $R$.

Let $x \in R$.


Then $x$ is transcendental over $D$  if and only if :
:$\ds \forall n \in \Z_{\ge 0}: \sum_{k \mathop = 0}^n a_k \circ x^k = 0_R \implies \forall k: 0 \le k \le n: a_k = 0_R$


That is, $x$ is transcendental over $D$  if and only if  the only way to express $0_R$ as a polynomial in $x$ over $D$ is by the null polynomial.",Definition:Transcendental (Abstract Algebra)/Ring,"['Definitions/Ring Theory', 'Definitions/Polynomial Theory']"
Definition:Transcendental,Transcendental,"A number (either real or complex) is transcendental  if and only if  it is not algebraic.


=== Transcendental Number over Field ===

Some sources define a transcendental number over a more general field:

Let $F$ be a field.

Let $z$ be a complex number.

$z$ is a transcendental number over $F$  if and only if  $z$ cannot be expressed as a root of a polynomial with coefficients in $F$.",Definition:Transcendental Number,"['Definitions/Transcendental Numbers', 'Definitions/Numbers', 'Definitions/Analysis']"
Definition:Transcendental,Transcendental,"Let $f$ be an entire function that has an essential singularity at $\infty$.

Then $f$ is a transcendental entire function.",Definition:Entire Function/Transcendental,"['Definitions/Entire Functions', 'Definitions/Complex Analysis']"
Definition:Transfer Function,Transfer Function,"Let $C, D \subseteq \C$ with $z \in C \implies z + 1 \in C$.

Let $F: C \to D$ and $H: D \to D$ be holomorphic functions.

Let $\map H {\map F z} = \map F {z + 1}$ for all $z \in C$.

Then $F$ is said to be a superfunction of $H$, and $H$ is called a transfer function of $F$.

That is, superfunctions are iterations of transfer functions.",Definition:Superfunction,['Definitions/Number Theory']
Definition:Transfer Function,Transfer Function,"A transfer function, in the context of time series analysis, is a function of time which theoretically models the future output for each possible input.",Definition:Transfer Function (Time Series Analysis),['Definitions/Time Series Analysis']
Definition:Transformation,Transformation,"A linear transformation is a homomorphism from one module to another.


Hence, let $R$ be a ring.

Let $M = \struct {G, +_G, \circ}_R$ and $N = \struct {H, +_H, \otimes}_R$ be $R$-modules.

Let $\phi: G \to H$ be a mapping.

Then $\phi$ is a linear transformation  if and only if :
:$(1): \quad \forall x, y \in G: \map \phi {x +_G y} = \map \phi x +_H \map \phi y$
:$(2): \quad \forall x \in G: \forall \lambda \in R: \map \phi {\lambda \circ x} = \lambda \otimes \map \phi x$


=== Definition in a Vector Space ===
Let $V, W$ be vector spaces over a field (or, more generally, division ring) $K$.


A mapping $A: V \to W$ is a linear transformation  if and only if :

:$\forall v_1, v_2 \in V, \lambda \in K: \map A {\lambda v_1 + v_2} = \lambda \map A {v_1} + \map A {v_2}$


That is, a homomorphism from one vector space to another.


=== Linear Operator on Vector Space ===
A linear operator on a vector space is a linear transformation from a vector space into itself.

=== Linear Operator ===
A linear operator is a linear transformation from a module into itself.


=== Linear Operator on Vector Space ===
A linear operator on a vector space is a linear transformation from a vector space into itself.",Definition:Linear Transformation,"['Definitions/Linear Transformations', 'Definitions/Linear Algebra', 'Definitions/Vector Spaces', 'Definitions/Linearity']"
Definition:Transformation,Transformation,"Let $R$ be a commutative ring with unity.

Let $G$ be an $R$-module.

Let $G^*$ be the algebraic dual of $G$.

Let $G^{**}$ be the double dual of $G^*$.


For each $x \in G$, we define the mapping $x^\wedge: G^* \to R$ as:
:$\forall t \in G^*: \map {x^\wedge} t = \map t x$


The mapping $J: G \to G^{**}$ defined as:
:$\forall x \in G: \map J x = x^\wedge$
is called the evaluation linear transformation from $G$ into $G^{**}$.


It is usual to denote the mapping $t: G^* \to R$ as follows:

:$\forall x \in G, t \in G^*: \innerprod x t := \map t x$",Definition:Evaluation Linear Transformation/Module Theory,"['Definitions/Evaluation Linear Transformations', 'Definitions/Module Theory']"
Definition:Transformation,Transformation,"A complex transformation is a mapping on the complex plane $f: \C \to \C$ which is specifically not a multifunction.


Let $z = x + i y$ be a complex variable.

Let $w = u + i v = \map f z$.


Then $w$ can be expressed as:
:$u + i v = \map f {x + i y}$

such that:
:$u = \map u {x, y}$
and:
:$v = \map v {x, y}$
are real functions of two variables.


Thus a point $P = \tuple {x, y}$ in the complex plane is transformed to a point $P' = \tuple {\map u {x, y}, \map v {x, y} }$ by $f$.

Thus $P'$ is the image of $P$ under $f$.",Definition:Complex Transformation,['Definitions/Complex Functions']
Definition:Transformation,Transformation,"Let $X$ be a set.

Let $\struct {G, \circ}$ be a group whose identity is $e$.


=== Left Group Action ===
Let $X$ be a set.

Let $\struct {G, \circ}$ be a group whose identity is $e$.


A (left) group action is an operation $\phi: G \times X \to X$ such that:

:$\forall \tuple {g, x} \in G \times X: g * x := \map \phi {g, x} \in X$

in such a way that the group action axioms are satisfied:
 

=== Right Group Action ===
Let $X$ be a set.

Let $\struct {G, \circ}$ be a group whose identity is $e$.


A right group action is a mapping $\phi: X \times G \to X$ such that:

:$\forall \tuple {x, g} \in X \times G : x * g := \map \phi {x, g} \in X$

in such a way that the right group action axioms are satisfied:
 

The group $G$ thus acts on the set $X$.

The group $G$ can be referred to as the group of transformations, or a transformation group.


=== From Permutation Representation ===
Let $G$ be a group.

Let $X$ be a set.

Let $\struct {\map \Gamma X, \circ}$ be the symmetric group on $X$.

Let $\rho: G \to \struct {\map \Gamma X, \circ}$ be a permutation representation.


The group action of $G$ associated to the permutation representation $\rho$ is the group action $\phi: G \times X \to X$ defined by:
:$\map \phi {g, x} = \map {\rho_g} x$

where $\rho_g : X \to X$ is the permutation representation associated to $\rho$ for $g \in G$ by $\map {\rho_g} x = \map \phi {g, x}$.",Definition:Group Action,['Definitions/Group Actions']
Definition:Transformation,Transformation,"Let $G$ be a group whose identity is $e$.

Let $X$ be a set.

Let $\phi: G \times X \to X$ be a group action.


Then $G$ is an effective transformation group for $\phi$  if and only if  $\phi$ is faithful.",Definition:Effective Transformation Group,['Definitions/Group Actions']
Definition:Transformation,Transformation,"Let $\mathbf C$ and $\mathbf D$ be categories.


=== Covariant Functors ===
Let $\mathbf C$ and $\mathbf D$ be categories.

Let $F, G : \mathbf C \to \mathbf D$ be covariant functors.


A natural transformation $\eta$ from $F$ to $G$ is a mapping on $\mathbf C$ such that:
 

:$(1): \quad$ For all $x \in \mathbf C$, $\eta_x$ is a morphism from $\map F x$ to $\map G x$.

:$(2): \quad$ For all $x, y \in C$ and morphism $f: x \to y$, the following diagram commutes:
 

$\quad \quad \xymatrix{
\map F x \ar[d]^{\eta_x} \ar[r]^{\map F f} & \map F y \ar[d]^{\eta_y} \\
\map G x \ar[r]^{\map G f}                 & \map G y}$

=== Contravariant Functors ===
Let $\mathbf C$ and $\mathbf D$ be categories.

Let $F, G: \mathbf C \to \mathbf D$ be contravariant functors.


A natural transformation $\eta$ from $F$ to $G$ is a mapping on $\mathbf C$ such that:

:$(1): \quad$ For all $x \in \mathbf C$, $\eta_x$ is a morphism from $\map F x$ to $\map G x$.

:$(2): \quad$ For all $x, y \in C$ and morphism $f: x \to y$, the following diagram commutes:
 

$\quad \quad \xymatrix{
\map F x \ar[d]^{\eta_x} & \map F y \ar[d]^{\eta_y} \ar[l]^{\map F f}  \\
\map G x                 & \map G y \ar[l]^{\map G f} }$",Definition:Natural Transformation,"['Definitions/Category Theory', 'Definitions/Functors', 'Definitions/Natural Transformations']"
Definition:Transformation,Transformation,"Let $\mathbf C$ and $\mathbf D$ be categories.

Let $F, G : \mathbf C \to \mathbf D$ be covariant functors.


A natural transformation $\eta$ from $F$ to $G$ is a mapping on $\mathbf C$ such that:
 

:$(1): \quad$ For all $x \in \mathbf C$, $\eta_x$ is a morphism from $\map F x$ to $\map G x$.

:$(2): \quad$ For all $x, y \in C$ and morphism $f: x \to y$, the following diagram commutes:
 

$\quad \quad \xymatrix{
\map F x \ar[d]^{\eta_x} \ar[r]^{\map F f} & \map F y \ar[d]^{\eta_y} \\
\map G x \ar[r]^{\map G f}                 & \map G y}$",Definition:Natural Transformation/Covariant Functors,['Definitions/Natural Transformations']
Definition:Transformation,Transformation,"Let $\mathbf C$ and $\mathbf D$ be categories.

Let $F, G: \mathbf C \to \mathbf D$ be contravariant functors.


A natural transformation $\eta$ from $F$ to $G$ is a mapping on $\mathbf C$ such that:

:$(1): \quad$ For all $x \in \mathbf C$, $\eta_x$ is a morphism from $\map F x$ to $\map G x$.

:$(2): \quad$ For all $x, y \in C$ and morphism $f: x \to y$, the following diagram commutes:
 

$\quad \quad \xymatrix{
\map F x \ar[d]^{\eta_x} & \map F y \ar[d]^{\eta_y} \ar[l]^{\map F f}  \\
\map G x                 & \map G y \ar[l]^{\map G f} }$",Definition:Natural Transformation/Contravariant Functors,['Definitions/Natural Transformations']
Definition:Transformation,Transformation,"The Lorentz transformation is a transformation which changes the position and motion in one inertial frame of reference to a different inertial frame of reference.

The equations governing such a transformation must satisfy the postulates of the special theory of relativity.


 ",Definition:Lorentz Transformation,"['Definitions/Lorentz Transformations', 'Definitions/Special Theory of Relativity']"
Definition:Transformation,Transformation,"Let $\map f x$ be a polynomial over a field $k$:

:$\map f x = a_n x^n + a_{n - 1} x^{n - 1} + a_{n - 2} x^{n - 2} + \cdots + a_1 x + a_0$


Then the Tschirnhaus transformation is the linear substitution $x = y - \dfrac {a_{n - 1} } {n a_n}$.

The Tschirnhaus transformation produces a resulting polynomial $\map {f'} y$ which is depressed, as shown on Tschirnhaus Transformation yields Depressed Polynomial.

This technique is used in the derivation of Cardano's Formula for the roots of the general cubic.

 ",Definition:Tschirnhaus Transformation,"['Definitions/Tschirnhaus Transformations', 'Definitions/Polynomial Theory']"
Definition:Transitive,Transitive,"=== Smallest Transitive Superset ===
Let $\RR$ be a relation on a set $S$.


The transitive closure of $\RR$ is defined as the smallest transitive relation on $S$ which contains $\RR$ as a subset.


The transitive closure of $\RR$ is denoted $\RR^+$.

=== Intersection of Transitive Supersets ===
Let $\RR$ be a relation on a set $S$.


The transitive closure of $\RR$ is defined as the intersection of all transitive relations on $S$ which contain $\RR$.


The transitive closure of $\RR$ is denoted $\RR^+$.

=== Finite Chain ===
Let $\RR$ be a relation on a set or class $S$.


The transitive closure of $\RR$ is the relation $\RR^+$ defined as follows:

For $x, y \in S$, $x \mathrel {\RR^+} y$  if and only if  for some $n \in \N_{>0}$ there exist $s_0, s_1, \dots, s_n \in S$ such that $s_0 = x$, $s_n = y$, and:

 
 
 
 
 
 


That is:

:$\forall k \in \N_n: s_k \mathrel \RR s_{k + 1}$

=== Union of Compositions ===
Let $\RR$ be a relation on a set $S$.

Let:

:$\RR^n := \begin{cases}
\RR & : n = 1 \\
\RR^{n-1} \circ \RR & : n > 1
\end{cases}$

where $\circ$ denotes composition of relations.

Finally, let:

:$\ds \RR^+ = \bigcup_{i \mathop = 1}^\infty \RR^i$


Then $\RR^+$ is called the transitive closure of $\RR$.",Definition:Transitive Closure (Relation Theory),"['Definitions/Transitive Closures', 'Definitions/Transitive Relations', 'Definitions/Examples of Closure Operators']"
Definition:Transitive,Transitive,"Let $\RR$ be a relation on a set $S$.


=== Smallest Reflexive Transitive Superset ===
Let $\RR$ be a relation on a set $S$.

The reflexive transitive closure of $\RR$ is denoted $\RR^*$, and is defined as the smallest reflexive and transitive relation on $S$ which contains $\RR$.

=== Reflexive Closure of Transitive Closure ===
Let $\RR$ be a relation on a set $S$.

The reflexive transitive closure of $\RR$ is denoted $\RR^*$, and is defined as the reflexive closure of the transitive closure of $\RR$:

:$\RR^* = \paren {\RR^+}^=$

=== Transitive Closure of Reflexive Closure ===
Let $\RR$ be a relation on a set $S$.

The reflexive transitive closure of $\RR$ is denoted $\RR^*$, and is defined as the transitive closure of the reflexive closure of $\RR$:
:$\RR^* = \paren {\RR^=}^+$",Definition:Reflexive Transitive Closure,"['Definitions/Transitive Relations', 'Definitions/Reflexive Relations', 'Definitions/Reflexive Transitive Closures']"
Definition:Transitive,Transitive,"=== Relation Theory ===
Let $\RR$ be a relation on a set $S$.


A transitive reduction of $\RR$ is denoted $\RR^-$, and is defined as a minimal relation on $S$ which has the same transitive closure as $\RR$.

=== Graph Theory ===

The concept of transitive reduction is usually encountered in the field of graph theory where it has considerable importance:

Let $G = \struct {V, E}$ be a loop-digraph.

Let $G$ be expressed formally as a relational structure $\GG$.

A transitive reduction of $G$ is denoted $G^-$, and is defined as a transitive reduction of the relation $\GG$.

Hence it is a minimal loop-digraph on $V$ which has the same transitive closure as $\GG$.",Definition:Transitive Reduction,"['Definitions/Transitive Reductions', 'Definitions/Transitive Relations', 'Definitions/Graph Theory']"
Definition:Transitive,Transitive,"Let $G$ be a group.

Let $S$ be a set.

Let $*: G \times S \to S$ be a group action.


The group action is transitive  if and only if  for any $x, y \in S$ there exists $g \in G$ such that $g * x = y$.


That is,  if and only if  for all $x \in S$:
:$\Orb x = S$
where $\Orb x$ denotes the orbit of $x \in S$ under $*$.


=== $n$-transitive Action ===
Let $G$ be a group.

Let $S$ be a set.

Let $*: G \times S \to S$ be a group action.

Let $n\geq1$ be a natural number.


The group action is $n$-transitive  if and only if  for any two ordered $n$-tuples $(x_1, \ldots, x_n)$ and $(y_1, \ldots, y_n)$ of pairwise distinct elements of $S$, there exists $g\in G$ such that:
:$\forall i\in \{1, \ldots, n\} : g * x_i = y_i$


Category:Definitions/Group Actions",Definition:Transitive Group Action,['Definitions/Group Actions']
Definition:Transitive,Transitive,"Let $S_n$ denote the symmetric group on $n$ letters for $n \in \N$.

Let $H$ be a subgroup of $S_n$.

Let $H$ be such that:
:for every pair of elements $i, j \in \N_n$ there exists $\pi \in H$ such that $\map \pi i = j$.


Then $H$ is called a transitive subgroup of $S_n$.",Definition:Transitive Subgroup,['Definitions/Symmetric Groups']
Definition:Transitive,Transitive,"Let $A$ denote a class, which can be either a set or a proper class.

Then $A$ is transitive  if and only if  every element of $A$ is also a subclass of $A$.


That is, $A$ is transitive  if and only if :
:$x \in A \implies x \subseteq A$

or:
:$\forall x: \forall y: \paren {x \in y \land y \in A \implies x \in A}$",Definition:Transitive Class,"['Definitions/Class Theory', 'Definitions/Transitive Classes']"
Definition:Transitive,Transitive,"=== Definition 1 ===
Let $x$ be a set.

Then the transitive closure of $x$ is the smallest transitive superset of $x$.

The following is not equivalent to the above, but they are almost the same.

=== Definition 2 ===
Let $x$ be a set.

For each natural number $n \in \N_{\ge 0}$ let:

: $\bigcup^n x = \underbrace{\bigcup \bigcup \cdots \bigcup}_n x$


Then the transitive closure of $x$ is the union of the sets:
:$\left\{ {x}\right\}, x, \bigcup x, \bigcup^2 x, \dots, \bigcup^n x, \dots$


More precisely:

Let $F$ be the mapping on the universal class defined by letting:
:$F \left({a}\right) = \bigcup a$
for each set $a$.

Let $G$ be the mapping on the natural numbers defined recursively by letting:

: $G \left({0}\right) = \left\{ {x}\right\}$
: $G \left({n^+}\right) = F \left({G \left({n}\right)}\right)$

for each natural number $n$.

Then the transitive closure of $x$ is defined as the union of the image of $G$.",Definition:Transitive Closure (Set Theory),['Definitions/Relational Closures']
Definition:Transpose,Transpose,"Let $R$ be a commutative ring.

Let $G$ and $H$ be $R$-modules.

Let $G^*$ and $H^*$ be the algebraic duals of $G$ and $H$ respectively.


Let $\map {\LL_R} {G, H}$ be the set of all linear transformations from $G$ to $H$.

Let $u \in \map {\LL_R} {G, H}$.


The transpose of $u$ is the mapping $u^\intercal: H^* \to G^*$ defined as:
:$\forall y \in H^*: \map {u^\intercal} y = y \circ u$
where $y \circ u$ is the composition of $y$ and $u$.",Definition:Transpose of Linear Transformation,['Definitions/Linear Transformations']
Definition:Transpose,Transpose,"Let $\mathbf A = \sqbrk \alpha_{m n}$ be an $m \times n$ matrix over a set.


Then the transpose of $\mathbf A$ is denoted $\mathbf A^\intercal$ and is defined as:

:$\mathbf A^\intercal = \sqbrk \beta_{n m}: \forall i \in \closedint 1 n, j \in \closedint 1 m: \beta_{i j} = \alpha_{j i}$",Definition:Transpose of Matrix,['Definitions/Matrix Theory']
Definition:Transversal,Transversal,"A transversal of two straight lines lying in the same plane is a straight line which intersects them in two different points.

The transversal is said to cut the two lines that it crosses.

:

In the above diagram, $EF$ is a transversal of the lines $AB$ and $CD$.


It is also apparent that:
:$AB$ is a transversal of the lines $EF$ and $CD$
:$CD$ is a transversal of the lines $EF$ and $AB$
although this is not as obvious.


=== Interior Angle ===
:


An interior angle of a transversal is an angle which is between the two lines cut by that transversal.

In the above figure, the interior angles with respect to the transversal $EF$ are:
:$\angle AHJ$
:$\angle CJH$
:$\angle BHJ$
:$\angle DJH$

=== Exterior Angle ===
:


An exterior angle of a transversal is an angle which is not between the two lines cut by a transversal.

In the above figure, the exterior angles with respect to the transversal $EF$ are:
:$\angle AHE$
:$\angle CJF$
:$\angle BHE$
:$\angle DJF$

=== Alternate Angles ===
:


Alternate angles are interior angles of a transversal which are on opposite sides and different lines.

In the above figure, the pairs of alternate angles with respect to the transversal $EF$ are:
:$\angle AHJ$ and $\angle DJH$
:$\angle CJH$ and $\angle BHJ$

=== Corresponding Angles ===
:


Corresponding angles are the angles in equivalent positions on the two lines cut by a transversal with respect to that transversal.

In the above figure, the corresponding angles with respect to the transversal $EF$ are:
:$\angle AHJ$ and $\angle CJF$
:$\angle AHE$ and $\angle CJH$
:$\angle BHE$ and $\angle DJH$
:$\angle BHJ$ and $\angle DJF$",Definition:Transversal (Geometry),"['Definitions/Straight Lines', 'Definitions/Transversals (Geometry)']"
Definition:Transversal,Transversal,"Let $G$ be a group.

Let $H$ be a subgroup of $G$.

Let $S \subseteq G$ be a subset of $G$.


=== Left Transversal ===
Let $G$ be a group.

Let $H$ be a subgroup of $G$.

Let $S \subseteq G$ be a subset of $G$.


$S$ is a left transversal for $H$ in $G$  if and only if  every left coset of $H$ contains exactly one element of $S$.

=== Right Transversal ===
Let $G$ be a group.

Let $H$ be a subgroup of $G$.

Let $S \subseteq G$ be a subset of $G$.


$S$ is a right transversal for $H$ in $G$  if and only if  every right coset of $H$ contains exactly one element of $S$.

=== Transversal ===

A transversal for $H$ in $G$ is either a left transversal or a right transversal.


 
Clearly if $S$ is a transversal for $H$ it contains $\index G H$ elements, where $\index G H$ denotes the index of $H$ in $G$.",Definition:Transversal (Group Theory),"['Definitions/Group Theory', 'Definitions/Transversals (Group Theory)']"
Definition:Tree,Tree,"=== Definition 1===
A tree is a simple connected graph with no circuits.


:

=== Definition 2===
A tree is a simple connected graph with no cycles.

=== Node ===
The vertices of a tree are called its nodes.

=== Leaf Node ===
Let $v$ be a node of a tree $T$.

Then $v$ is a leaf node of a $T$  if and only if  $v$ is of degree $1$.


If $T$ is a rooted tree, this is equivalent to saying that $v$ has no child nodes.",Definition:Tree (Graph Theory),"['Definitions/Graph Theory', 'Definitions/Tree Theory']"
Definition:Tree,Tree,"A rooted tree is a tree with a countable number of nodes, in which a particular node is distinguished from the others and called the root node:

:

=== Root Node ===
Let $T$ be a rooted tree.

The root node of $T$ is the node of $T$ which is distinguished from the others by being the ancestor node of every node of $T$.

=== Parent ===
Let $T$ be a rooted tree whose root is $r_T$.

Let $t$ be a node of $T$.

From Path in Tree is Unique, there is only one path from $t$ to $r_T$.

Let $\pi: T \setminus \set {r_T} \to T$ be the mapping defined by:

:$\map \pi t := \text {the node adjacent to $t$ on the path to $r_T$}$


Then $\map \pi t$ is known as the parent node of $t$.

The mapping $\pi$ is called the parent mapping.

=== Ancestor ===
Let $T$ be a rooted tree with root $r_T$.

Let $t$ be a node of $T$.


An ancestor node of $t$ is a node in the path from $t$ to $r_T$.


=== Proper Ancestor ===
Let $T$ be a rooted tree with root $r_T$.

Let $t$ be a node of $T$.


A proper ancestor node of $t$ is an ancestor node of $t$ that is not $t$ itself.

=== Child Node ===
Let $T$ be a rooted tree with root $r_T$.

Let $t$ be a node of $T$.

The child nodes of $t$ are the elements of the set:
:$\set {s \in T: \map \pi s = t}$
where $\map \pi s$ denotes the parent mapping of $s$.

That is, the children of $t$ are all the nodes of $T$ of which $t$ is the parent.


=== Grandchild Node ===
Let $T$ be a rooted tree with root $r_T$.

Let $t$ be a node of $T$.


A child of a child node of a node $t$ can be referred to as a grandchild node of $t$.

In terms of the parent mapping $\pi$ of $T$, a grandchild node of $t$ is a node $s$ such that:

:$\map \pi {\map \pi s} = t$


Category:Definitions/Descendant Nodes

=== Descendant ===
Let $T$ be a rooted tree with root $r_T$.

Let $t$ be a node of $T$.


A descendant node $s$ of a $t$ is a node such that $t$ is in the path from $s$ to $r_T$.

That is, the descendant nodes of $t$ are all the nodes of $T$ of which $t$ is an ancestor node.


=== Proper Descendant ===
Let $T$ be a rooted tree with root $r_T$.

Let $t$ be a node of $T$.


A proper descendant node of $t$ is a descendant of $t$ which is not $t$ itself.


Category:Definitions/Descendant Nodes

=== Sibling ===
Let $T$ be a rooted tree with root $r_T$.

Two children of the same node of $T$ are called siblings.

That is, siblings are nodes which both have the same parent.


Category:Definitions/Rooted Trees

=== Leaf Node ===
Let $v$ be a node of a tree $T$.

Then $v$ is a leaf node of a $T$  if and only if  $v$ is of degree $1$.


If $T$ is a rooted tree, this is equivalent to saying that $v$ has no child nodes.

=== Branch ===
Let $T$ be a rooted tree with root node $r_T$.

A subset $\Gamma$ of $T$ is a branch  if and only if  all the following conditions hold:
:$(1): \quad$ The root node $r_T$ belongs to $\Gamma$
:$(2): \quad$ The parent of each node in $\Gamma \setminus \set {r_T}$ is in $\Gamma$
:$(3): \quad$ Each node in $\Gamma$ either:
::$\text {(a)}: \quad$ is a leaf node of $T$
:or:
::$\text {(b)}: \quad$ has exactly one child node in $\Gamma$.


Informally, a branch of a rooted tree is the path from the root to a leaf.


=== Finite ===
Let $T$ be a rooted tree with root node $r_T$.

Let $\Gamma$ be a branch of $T$.


Then $\Gamma$ is finite  if and only if  it has exactly one leaf node.

=== Infinite ===
Let $T$ be a rooted tree with root node $r_T$.

Let $\Gamma$ be a branch of $T$.


Then $\Gamma$ is infinite  if and only if  it has no leaf node at the end.",Definition:Rooted Tree,"['Definitions/Rooted Trees', 'Definitions/Graph Theory', 'Definitions/Tree Theory']"
Definition:Tree,Tree,"Let $\struct {T, \preceq}$ be an ordered set.

Let $\struct {T, \preceq}$ be such that for every $t \in T$, the lower closure of $t$:
:$t^\preceq := \set {s \in T: s \preceq t}$ 

is well-ordered by $\preceq$.


Then $\struct {T, \preceq}$ is a tree.


=== Branch ===
Let $\struct {T, \preceq}$ be a tree.

A branch of $\struct {T, \preceq}$ is a maximal chain in $\struct {T, \preceq}$.


Category:Definitions/Set Theory

=== Subtree ===
Let $\struct {T, \preceq}$ be a tree.

A subtree of $\struct {T, \preceq}$ is an ordered subset $\struct {S, \preceq}$ with the property that:
:for every $\forall s \in S: \forall t \in T: t \preceq s \implies t \in S$

That is, such that $\struct {S, \preceq}$ is a lower closure of $\struct {T, \preceq}$.


Category:Definitions/Set Theory

Category:Definitions/Set Theory",Definition:Tree (Set Theory),['Definitions/Set Theory']
Definition:Triangle,Triangle,":

A triangle is a polygon with exactly three sides.


Thus a triangle is a $2$-simplex.


Because it is a polygon, it follows that it also has three vertices and three angles.",Definition:Triangle (Geometry),"['Definitions/Triangles', 'Definitions/Polygons', 'Definitions/Simplices']"
Definition:Triangle,Triangle,"The complete graph $K_3$ of order $3$ is called a triangle.

:

Category:Definitions/Complete Graphs
Category:Definitions/Examples of Graphs",Definition:Triangle (Graph Theory),"['Definitions/Complete Graphs', 'Definitions/Examples of Graphs']"
Definition:Trivial,Trivial,"Let $S$ be a set such that $S \ne \O$.

There are two partitions on $S$ which are referred to as the trivial partitions on $S$:


=== Singleton Partition ===
The singleton partition on $S$ is defined as:
:$\PP = \set S$

That is, it is a partition with only one component.


Category:Definitions/Set Partitions

=== Partition of Singletons ===
Let $S$ be a set such that $S \ne \O$.


The partition of singletons on $S$ is defined as:
:$\PP = \set {\set x: x \in S}$

That is, it is a partition such that every component is a singleton.


Category:Definitions/Set Partitions

Category:Definitions/Set Partitions",Definition:Trivial Partition,['Definitions/Set Partitions']
Definition:Trivial,Trivial,"Let $\Delta_S$ be the diagonal relation on a set $S$.

As $\Delta_S$ is an equivalence, we can form the quotient mapping:
:$q_{\Delta_S}: S \to S / \Delta_S$.


This quotient mapping is called the trivial quotient of $S$.",Definition:Trivial Quotient,['Definitions/Quotient Mappings']
Definition:Trivial,Trivial,"The trivial relation is the relation $\RR \subseteq S \times T$ in $S$ to $T$ such that every element of $S$ relates to every element in $T$:

:$\RR: S \times T: \forall \tuple {s, t} \in S \times T: \tuple {s, t} \in \RR$


That is:
:$\RR = S \times T$
the relation which equals the product of the sets on which it is defined.",Definition:Trivial Relation,"['Definitions/Examples of Relations', 'Definitions/Examples of Equivalence Relations']"
Definition:Trivial,Trivial,A trivial group is a group with only one element $e$.,Definition:Trivial Group,['Definitions/Examples of Groups']
Definition:Trivial,Trivial,"A ring $\struct {R, +, \circ}$ is a trivial ring  if and only if :

:$\forall x, y \in R: x \circ y = 0_R$",Definition:Trivial Ring,['Definitions/Ring Theory']
Definition:Trivial,Trivial,"Let $\struct {G, +}$ be a finite abelian group.

Let $\struct {\C_{\ne 0}, \times}$ be the multiplicative group of complex numbers.


A character of $G$ is a group homomorphism:

:$\chi: G \to \C_{\ne 0}$",Definition:Character (Number Theory),['Definitions/Analytic Number Theory']
Definition:Trivial,Trivial,"Let $\struct {D, +, \circ}$ be an integral domain.

Let $\struct {U_D, \circ}$ be the group of units of $\struct {D, +, \circ}$.


A factorization in $\struct {D, +, \circ}$ of the form $x = u \circ y$, where $u \in U_D$ (that is, where $x$ is an associate of $y$) is called a trivial factorization.


=== Non-Trivial Factorization ===
Let $\struct {D, +, \circ}$ be an integral domain.

Let $\struct {U_D, \circ}$ be the group of units of $\struct {D, +, \circ}$.


A factorization in $\struct {D, +, \circ}$ of the form $x = z \circ y$, where neither $y$ nor $z$ is a unit of $D$, is called a non-trivial factorization.",Definition:Trivial Factorization,['Definitions/Factorization']
Definition:Trivial,Trivial,"Let $S$ be a set.


A filter $\FF$ on $S$ by definition specifically does not include the empty set $\O$.

If a filter $\FF$ were to include $\O$, then from Empty Set is Subset of All Sets it would follow that every subset of $S$ would have to be in $\FF$, and so $\FF = \powerset S$.


Such a ""filter"" is called the trivial filter on $S$.


Category:Definitions/Filters on Sets",Definition:Filter on Set/Trivial Filter,['Definitions/Filters on Sets']
Definition:Trivial,Trivial,"Let $S \ne \O$ be a set.

Let $\tau = \set {S, \O}$.


Then $\tau$ is called the indiscrete topology on $S$.


A topological space $\struct {S, \set {S, \O} }$ is known as an indiscrete space.",Definition:Indiscrete Topology,"['Definitions/Indiscrete Topology', 'Definitions/Examples of Topologies']"
Definition:Trivial,Trivial,"A trivial topological space is a topological space with only one element.


The open sets of a trivial topological space $T = \struct {\set s, \tau}$ are $\O$ and $\set s$.",Definition:Trivial Topological Space,['Definitions/Examples of Topologies']
Definition:Trivial,Trivial,"Let $V$ be a vector space with zero vector $\mathbf 0$.


Then the set $(\mathbf 0) := \left\{{\mathbf 0}\right\}$ is called the zero subspace of $V$.


This name is appropriate as $(\mathbf 0)$ is in fact a subspace of $V$, as proved in Zero Subspace is Subspace.


 

Category:Definitions/Vector Spaces",Definition:Zero Subspace,['Definitions/Vector Spaces']
Definition:Trivial,Trivial," Division Ring 
Let $\struct {R, +, \circ}$ be a division ring, and denote its zero by $0_R$.


Then the map $\norm {\cdot}: R \to \R_{\ge 0}$ given by:

:$\norm x = \begin{cases}
  0 & : \text{if $x = 0_R$}\\
  1 & : \text{otherwise}
\end{cases}$

defines a norm on $R$, called the trivial norm.

 Vector Space 
Let $\struct {K, +, \circ}$ be a division ring endowed with the trivial norm.

Let $V$ be a vector space over $K$, with zero $0_V$.


Then the map $\norm {\cdot}: V \to \R_+ \cup \set 0$ given by:

:$\norm x = \begin{cases}
  0 & : \text {if $x = 0_V$} \\
  1 & : \text {otherwise}
\end{cases}$

defines a norm on $V$, called the trivial norm.",Definition:Trivial Norm,"['Definitions/Examples of Norms', 'Definitions/Trivial Norms']"
Definition:Trivial,Trivial,"The following categories can be seen described as trivial:


=== Zero Catgegory ===
The category $\mathbf 0$, zero, is the empty category:


:$\qquad$


with:
:no objects
and consequently:
:no morphisms.

=== One Category ===
The category one $\mathbf 1$, is the category with:

 

=== Discrete Category ===
Let $\CC$ be a metacategory.


Then $\CC$ is said to be discrete  if and only if  it comprises only identity morphisms.

If the collection $\CC$ constitutes the objects of $\mathbf C$, then $\mathbf C$ may also be denoted $\map {\mathbf {Dis} } \CC$.",Definition:Trivial Category,['Definitions/Category Theory']
Definition:Trivial,Trivial,"The trivial zeroes of the Riemann $\zeta$ function are the strictly negative even integers :

:$\set {n \in \Z: n = -2 \times k: k \in \N_{\ne 0} } = \set {-2, -4, -6, \ldots}$",Definition:Riemann Zeta Function/Zero/Trivial,['Definitions/Riemann Zeta Function']
Definition:Union,Union,"Let $S$ and $T$ be sets.


The (set) union of $S$ and $T$ is the set $S \cup T$, which consists of all the elements which are contained in either (or both) of $S$ and $T$:
:$x \in S \cup T \iff x \in S \lor x \in T$

or, slightly more formally:
:$A = S \cup T \iff \forall z: \paren {z \in A \iff z \in S \lor z \in T}$


We can write:
:$S \cup T := \set {x: x \in S \lor x \in T}$

and can voice it $S$ union $T$.


It can be seen that, in this form, $\cup$ is a binary operation which acts on sets.


=== Set of Sets ===
Let $\mathbb S$ be a set of sets.

The union of $\mathbb S$ is:
:$\bigcup \mathbb S := \set {x: \exists X \in \mathbb S: x \in X}$
That is, the set of all elements of all elements of $\mathbb S$.


Thus the general union of two sets can be defined as:
:$\bigcup \set {S, T} = S \cup T$

=== Family of Sets ===
Let $I$ be an indexing set.

Let $\family {S_i}_{i \mathop \in I}$ be a family of sets indexed by $I$.


Then the union of $\family {S_i}$ is defined as:

:$\ds \bigcup_{i \mathop \in I} S_i := \set {x: \exists i \in I: x \in S_i}$


=== In the context of the Universal Set ===

In treatments of set theory in which the concept of the universal set is recognised, this can be expressed as follows.

Let $\mathbb U$ be a universal set.

Let $I$ be an indexing set.

Let $\family {S_i}_{i \mathop \in I}$ be an indexed family of subsets of $\mathbb U$.


Then the union of $\family {S_i}$ is defined and denoted as:

:$\ds \bigcup_{i \mathop \in I} S_i := \set {x \in \mathbb U: \exists i \in I: x \in S_i}$

=== Subsets of General Set ===
This definition is the same when the universal set $\mathbb U$ is replaced by any set $X$, which may or may not be a universal set:

Let $I$ be an indexing set.

Let $\family {S_i}_{i \mathop \in I}$ be an indexed family of subsets of a set $X$.


Then the union of $\family {S_i}$ is defined as:

:$\ds \bigcup_{i \mathop \in I} S_i := \set {x \in X: \exists i \in I: x \in S_i}$
where $i$ is a bound variable.

=== Countable Union ===
Let $\mathbb S$ be a set of sets.

Let $\sequence {S_n}_{n \mathop \in \N}$ be a sequence in $\mathbb S$.

Let $S$ be the union of $\sequence {S_n}_{n \mathop \in \N}$:
:$\ds S = \bigcup_{n \mathop \in \N} S_n$


Then $S$ is a countable union of sets in $\mathbb S$.

=== Finite Union ===
Let $S = S_1 \cup S_2 \cup \ldots \cup S_n$.

Then:
:$\ds S = \bigcup_{i \mathop \in \N^*_n} S_i = \set {x: \exists i \in \N^*_n: x \in S_i}$
where $\N^*_n = \set {1, 2, 3, \ldots, n}$.


If it is clear from the context that $i \in \N^*_n$, we can also write $\ds \bigcup_{\N^*_n} S_i$.


 ",Definition:Set Union,"['Definitions/Set Theory', 'Definitions/Set Union']"
Definition:Union,Union,"Let $S$ and $T$ be sets.

Let $\RR_1$ and $\RR_2$ be relations on $S \times T$.


The union of $\RR_1$ and $\RR_2$ is the relation $\QQ$ defined by:

:$\QQ := \RR_1 \cup \RR_2$

where $\cup$ denotes set union.


Explicitly, for $s \in S$ and $t \in T$, we have:

:$s \mathrel \QQ t$  if and only if  $s \mathrel {\RR_1} t$ or $s \mathrel {\RR_2} t$


=== General Definition ===
Let $S$ and $T$ be sets.

Let $\mathscr R$ be a collection of relations on $S \times T$.


The union of $\mathscr R$ is the relation $\RR$ defined by:

:$\ds \RR = \bigcup \mathscr R$

where $\bigcup$ denotes set union.


Explicitly, for $s \in S$ and $t \in T$:

:$s \mathrel \RR t$  if and only if  for some $\QQ \in \mathscr R$, $s \mathrel \QQ t$",Definition:Union of Relations,"['Definitions/Relation Theory', 'Definitions/Set Union']"
Definition:Union,Union,"Let:

:$(1): \quad f_1: S_1 \to T_1$ be a mapping from $S_1$ to $T_1$

:$(2): \quad f_2: S_2 \to T_2$ be a mapping from $S_2$ to $T_2$

Let $f_1$ and $f_2$ be combinable, that is, that they agree on $S_1 \cap S_2$.


Then the union mapping $f = f_1 \cup f_2$ of $f_1$ and $f_2$ is:

:$f: S_1 \cup S_2 \to T_1 \cup T_2: \map f s = \begin{cases}
\map {f_1} s : & s \in S_1 \\
\map {f_2} s : & s \in S_2
\end{cases}$


=== Finite Set of Mappings ===
Let $S = \set {f_1, f_2, \ldots, f_n}$ denote a finite set of mappings.


The union mapping $f$ of $S$ is defined when:

:$\forall i, j \in \set {1, 2, \ldots, n}: f_i$ and $f_j$ are combinable

and is defined as:

:$\forall x \in \ds \bigcup \set {\Dom {f_i}: i \in \set {1, 2, \ldots, n} } x \in \Dom {f_i} \implies f = \map {f_i} x$

=== Family of Mappings ===
Let $I$ be an indexing set.

Let $F = \family {f_i}_{i \mathop \in I}$ be a family of mappings indexed by $I$

The union mapping $f$ of $F$ is defined when:

:$\forall i, j \in I: f_i$ and $f_j$ are combinable

and is defined as:

:$\forall x \in \ds \bigcup \set {\Dom {f_i}: i \in I} x \in \Dom {f_i} \implies f = \map {f_i} x$",Definition:Union Mapping,"['Definitions/Mapping Theory', 'Definitions/Set Union', 'Definitions/Union Mappings']"
Definition:Union,Union,"Let:

:$(1): \quad \RR_1 \subseteq S_1 \times T_1$ be a relation on $S_1 \times T_1$

:$(2): \quad \RR_2 \subseteq S_2 \times T_2$ be a relation on $S_2 \times T_2$

Let $\RR_1$ and $\RR_2$ be combinable, that is, that they agree on $S_1 \cap S_2$.


Then the union relation (or combined relation) $\RR$ of $\RR_1$ and $\RR_2$ is:

:$\RR \subseteq \paren {S_1 \cup S_2} \times \paren {T_1 \cup T_2}: \map \RR s =
\begin{cases}
 \map {\RR_1} s : & s \in S_1 \\
 \map {\RR_2} s : & s \in S_2
\end{cases}$",Definition:Union Relation,['Definitions/Relation Theory']
Definition:Union,Union,"Let $I$ be an indexing set.

Let $\family {S_i}_{i \mathop \in I}$ be a family of sets indexed by $I$.


Then the union of $\family {S_i}$ is defined as:

:$\ds \bigcup_{i \mathop \in I} S_i := \set {x: \exists i \in I: x \in S_i}$


=== In the context of the Universal Set ===

In treatments of set theory in which the concept of the universal set is recognised, this can be expressed as follows.

Let $\mathbb U$ be a universal set.

Let $I$ be an indexing set.

Let $\family {S_i}_{i \mathop \in I}$ be an indexed family of subsets of $\mathbb U$.


Then the union of $\family {S_i}$ is defined and denoted as:

:$\ds \bigcup_{i \mathop \in I} S_i := \set {x \in \mathbb U: \exists i \in I: x \in S_i}$

=== Subsets of General Set ===
This definition is the same when the universal set $\mathbb U$ is replaced by any set $X$, which may or may not be a universal set:

Let $I$ be an indexing set.

Let $\family {S_i}_{i \mathop \in I}$ be an indexed family of subsets of a set $X$.


Then the union of $\family {S_i}$ is defined as:

:$\ds \bigcup_{i \mathop \in I} S_i := \set {x \in X: \exists i \in I: x \in S_i}$
where $i$ is a bound variable.",Definition:Set Union/Family of Sets,"['Definitions/Set Union', 'Definitions/Indexed Families']"
Definition:Union,Union,"Let $\struct {\R, \tau_d}$ be the real number line $\R$ under the usual (Euclidean) topology $\tau_d$.

Let $a, b, c \in \R$ where $a < b < c$.

Let $A$ be the union of the two open intervals:
:$A := \openint a b \cup \openint b c$


Then $\struct {A, \tau_d}$ is the union of adjacent open intervals.",Definition:Union of Adjacent Open Intervals,"['Definitions/Examples of Topologies', 'Definitions/Real Intervals']"
Definition:Unit,Unit,"Let $\struct {R, +, \circ}$ be a ring with unity whose unity is $1_R$.


=== Definition 1 ===
Let $\struct {R, +, \circ}$ be a ring with unity whose unity is $1_R$.


An element $x \in R$ is a unit of $\struct {R, +, \circ}$  if and only if  $x$ is invertible under $\circ$.


That is, a unit of $R$ is an element of $R$ which has an inverse.
:$\exists y \in R: x \circ y = 1_R = y \circ x$

=== Definition 2 ===
Let $\struct {R, +, \circ}$ be a ring with unity whose unity is $1_R$.


An element $x \in R$ is a unit of $\struct {R, +, \circ}$  if and only if  $x$ is divisor of $1_R$.

=== Product Inverse ===
Let $\struct {R, +, \circ}$ be a ring with unity.

Let $U_R$ denotes the group of units of $R$.

The inverse of $x \in U_R$ by $\circ$ is called the (ring) product inverse of $x$.


The usual means of denoting the product inverse of an element $x$ is by $x^{-1}$.

Thus it is distinguished from the additive inverse of $x$, that is, the (ring) negative of $x$, which is usually denoted $-x$.

=== Group of Units ===
Let $\struct {R, +, \circ}$ be a ring with unity.


Then the set $U_R$ of units of $\struct {R, +, \circ}$ is called the group of units of $\struct {R, +, \circ}$.


This can be denoted explicitly as $\struct {U_R, \circ}$.",Definition:Unit of Ring,"['Definitions/Units of Rings', 'Definitions/Ring Theory', 'Definitions/Factorization']"
Definition:Unit,Unit,"Let $\SS$ be a system of sets.

Let $U \in \SS$ such that:
:$\forall A \in \SS: A \cap U = A$


Then $U$ is the unit of $\SS$.


Note that, for a given system of sets, if $U$ exists then it is unique.

",Definition:Unit of System of Sets,['Definitions/Set Systems']
Definition:Unit,Unit,"Let $R$ be a commutative ring.

Let $\struct {A_R, \oplus}$ be a unital algebra over $R$.


The unit of $\struct {A_R, \oplus}$, denoted $1_A$, is the identity element of the operation $\oplus$:
:$\forall a \in A_R: a \oplus 1_A = 1_A \oplus a = a$

It is usually denoted $1$ when there is no source of confusion with the identity elements of the underlying structures of the algebra.",Definition:Unit of Algebra,['Definitions/Unital Algebras']
Definition:Unit,Unit,"Let $R$ be a commutative ring.

Let $\struct {A, *}$ be an algebra over $R$. 


Then $\struct {A, *}$ is a unital algebra  if and only if  the algebraic structure $\struct {A, \oplus}$ has an identity element.

That is:
:$\exists 1_A \in A: \forall a \in A: a * 1_A = 1_A * a = a$",Definition:Unital Algebra,['Definitions/Algebras']
Definition:Unit,Unit,"Let $\struct {R, +, \circ}$ be a ring.

If the semigroup $\struct {R, \circ}$ has an identity, this identity is referred to as the unity of the ring $\struct {R, +, \circ}$.

It is (usually) denoted $1_R$, where the subscript denotes the particular ring to which $1_R$ belongs (or often $1$ if there is no danger of ambiguity).


The ring $R$ itself is then referred to as a ring with unity.",Definition:Unity (Abstract Algebra)/Ring,"['Definitions/Ring Theory', 'Definitions/Unity']"
Definition:Unit,Unit,"A unit of measurement is a specified magnitude of a given physical quantity, defined by convention.

It is used as a standard for measurement of that physical quantity.

Any other value of the physical quantity can be expressed as a multiple of that unit of measurement.",Definition:Unit of Measurement,"['Definitions/Physics', 'Definitions/Units of Measurement']"
Definition:Unity,Unity,"=== Unity of Ring ===
Let $\struct {R, +, \circ}$ be a ring.

If the semigroup $\struct {R, \circ}$ has an identity, this identity is referred to as the unity of the ring $\struct {R, +, \circ}$.

It is (usually) denoted $1_R$, where the subscript denotes the particular ring to which $1_R$ belongs (or often $1$ if there is no danger of ambiguity).


The ring $R$ itself is then referred to as a ring with unity.

=== Unity of Field ===
Let $\struct {F, +, \times}$ be a field.

The identity element of the multiplicative group $\struct {F^*, \times}$ of $F$ is called the multiplicative identity of $F$.

It is often denoted $e_F$ or $1_F$, or, if there is no danger of ambiguity, $e$ or $1$.

Category:Definitions/Unity
Category:Definitions/Identity Elements",Definition:Unity (Abstract Algebra),"['Definitions/Unity', 'Definitions/Identity Elements']"
Definition:Unity,Unity,"Let $\struct {R, +, \circ}$ be a ring.

If the semigroup $\struct {R, \circ}$ has an identity, this identity is referred to as the unity of the ring $\struct {R, +, \circ}$.

It is (usually) denoted $1_R$, where the subscript denotes the particular ring to which $1_R$ belongs (or often $1$ if there is no danger of ambiguity).


The ring $R$ itself is then referred to as a ring with unity.",Definition:Unity (Abstract Algebra)/Ring,"['Definitions/Ring Theory', 'Definitions/Unity']"
Definition:Unity,Unity,"Let $\struct {F, +, \times}$ be a field.

The identity element of the multiplicative group $\struct {F^*, \times}$ of $F$ is called the multiplicative identity of $F$.

It is often denoted $e_F$ or $1_F$, or, if there is no danger of ambiguity, $e$ or $1$.",Definition:Multiplicative Identity,"['Definitions/Field Theory', 'Definitions/Unity']"
Definition:Universal,Universal,"Sets are considered to be subsets of some large universal set, also called the universe.

Exactly what this universe is will vary depending on the subject and context.

When discussing particular sets, it should be made clear just what that universe is.

However, note that from There Exists No Universal Set, this universe cannot be everything that there is.


The traditional symbol used to signify the universe is $\mathfrak A$.

However, this is old-fashioned and inconvenient, so some newer texts have taken to using $\mathbb U$ or just $U$ instead.


With this notation, this definition can be put into symbols as:
:$\forall S: S \subseteq \mathbb U$


The use of $\mathbb U$ or a variant is not universal: some sources use $X$.",Definition:Universe (Set Theory),['Definitions/Set Theory']
Definition:Universal,Universal,"The universal class is the class of which all sets are elements.


The universal class is defined most commonly in literature as:

:$V = \set {x: x = x}$

where $x$ ranges over all sets.


It can be briefly defined as the class of all sets.",Definition:Universal Class,"['Definitions/Class Theory', 'Definitions/Universal Class']"
Definition:Universal,Universal,"Let $C$ be a category.


A universal object of $C$ is an object that is initial or terminal.",Definition:Universal Object,['Definitions/Category Theory']
Definition:Universal,Universal,"A universal cover is a covering space which is simply connected.

 

Category:Definitions/Topology",Definition:Universal Cover,['Definitions/Topology']
Definition:Universe,Universe,"The universe of discourse is the term used to mean everything we are talking about.


When introducing the symbols:
:$\forall$ (the universal quantifier)
or:
:$\exists$ (the existential quantifier)
it is understood that the objects referred to are those in the specified universe of discourse.

It is usual to define that universe.",Definition:Universe of Discourse,"['Definitions/Universe of Discourse', 'Definitions/Logic']"
Definition:Universe,Universe,"A Grothendieck universe is a set (not a class) which has the properties expected of the universe $\mathbb U$ of sets in the sense of the Zermelo-Fraenkel axioms with the following properties:

:$(1): \quad \mathbb U$ is a transitive set: If $u \in \mathbb U$ and $x \in u$ then $x \in \mathbb U$

:$(2): \quad$ If $ u, v \in \mathbb U$ then $\set {u, v} \in \mathbb U$

:$(3): \quad$ If $u \in \mathbb U$ then the power set $\powerset u \in \mathbb U$

:$(4): \quad$ If $A \in \mathbb U$, and $\set {u_\alpha: \alpha \in A}$ is a family of elements $u_\alpha \in \mathbb U$ indexed by $A$, then $\ds \bigcup_{\alpha \mathop \in A} u_\alpha \in \mathbb U$


 ",Definition:Grothendieck Universe,"['Definitions/Tarski-Grothendieck Set Theory', 'Definitions/Category Theory']"
Definition:Universe,Universe,"The population of a statistical study is everything in the universe of discourse that is relevant to the study.

It includes all objects, measures, and observations under discussion.


=== Finite Population ===
A finite population is a population which is finite.

=== Infinite Population ===
An infinite population is a population which is infinite.",Definition:Population,"['Definitions/Statistics', 'Definitions/Applied Mathematics']"
Definition:Universe,Universe,"The physical universe, or usually just universe, is commonly defined as, and understood to be,  .


=== Real-World ===
Used to describe a phenomenon or object that has a genuine existence in the physical universe.",Definition:Physical Universe,"['Definitions/Applied Mathematics', 'Definitions/Physics']"
Definition:Upper Bound,Upper Bound,"Let $\struct {S, \preceq}$ be an ordered set.

Let $T$ be a subset of $S$.


An upper bound for $T$ (in $S$) is an element $M \in S$ such that:
:$\forall t \in T: t \preceq M$

That is, $M$ succeeds every element of $T$.


=== Subset of Real Numbers ===

The concept is usually encountered where $\struct {S, \preceq}$ is the set of real numbers under the usual ordering $\struct {\R, \le}$:

Let $\R$ be the set of real numbers.

Let $T$ be a subset of $\R$.


An upper bound for $T$ (in $\R$) is an element $M \in \R$ such that:
:$\forall t \in T: t \le M$

That is, $M$ is greater than or equal to every element of $T$.


=== Upper Bound of Number ===
When considering the upper bound of a set of numbers, it is commonplace to ignore the set and instead refer just to the number itself.

Thus the construction:

:The set of numbers which fulfil the propositional function $\map P n$ is bounded above with the upper bound $N$

would be reported as:

:The number $n$ such that $\map P n$ has the upper bound $N$.


This construct obscures the details of what is actually being stated. Its use on   is considered an abuse of notation and so discouraged.


This also applies in the case where it is the upper bound of a mapping which is under discussion.


Category:Definitions/Numbers
Category:Definitions/Boundedness",Definition:Upper Bound of Set,['Definitions/Boundedness']
Definition:Upper Bound,Upper Bound,"Let $\R$ be the set of real numbers.

Let $T$ be a subset of $\R$.


An upper bound for $T$ (in $\R$) is an element $M \in \R$ such that:
:$\forall t \in T: t \le M$

That is, $M$ is greater than or equal to every element of $T$.


=== Upper Bound of Number ===
When considering the upper bound of a set of numbers, it is commonplace to ignore the set and instead refer just to the number itself.

Thus the construction:

:The set of numbers which fulfil the propositional function $\map P n$ is bounded above with the upper bound $N$

would be reported as:

:The number $n$ such that $\map P n$ has the upper bound $N$.


This construct obscures the details of what is actually being stated. Its use on   is considered an abuse of notation and so discouraged.


This also applies in the case where it is the upper bound of a mapping which is under discussion.


Category:Definitions/Numbers
Category:Definitions/Boundedness",Definition:Upper Bound of Set/Real Numbers,['Definitions/Boundedness']
Definition:Upper Bound,Upper Bound,"Let $f: S \to T$ be a mapping whose codomain is an ordered set $\struct {T, \preceq}$.


Let $f$ be bounded above in $T$ by $H \in T$.


Then $H$ is an upper bound of $f$.


=== Real-Valued Function ===

The concept is usually encountered where $\struct {T, \preceq}$ is the set of real numbers under the usual ordering $\struct {\R, \le}$:

Let $f: S \to \R$ be a real-valued function.


Let $f$ be bounded above in $\R$ by $H \in \R$.


Then $H$ is an upper bound of $f$.


=== Upper Bound of Number ===
When considering the upper bound of a set of numbers, it is commonplace to ignore the set and instead refer just to the number itself.

Thus the construction:

:The set of numbers which fulfil the propositional function $\map P n$ is bounded above with the upper bound $N$

would be reported as:

:The number $n$ such that $\map P n$ has the upper bound $N$.


This construct obscures the details of what is actually being stated. Its use on   is considered an abuse of notation and so discouraged.


This also applies in the case where it is the upper bound of a mapping which is under discussion.


Category:Definitions/Numbers
Category:Definitions/Boundedness",Definition:Upper Bound of Mapping,['Definitions/Boundedness']
Definition:Upper Bound,Upper Bound,"Let $f: S \to \R$ be a real-valued function.


Let $f$ be bounded above in $\R$ by $H \in \R$.


Then $H$ is an upper bound of $f$.


=== Upper Bound of Number ===
When considering the upper bound of a set of numbers, it is commonplace to ignore the set and instead refer just to the number itself.

Thus the construction:

:The set of numbers which fulfil the propositional function $\map P n$ is bounded above with the upper bound $N$

would be reported as:

:The number $n$ such that $\map P n$ has the upper bound $N$.


This construct obscures the details of what is actually being stated. Its use on   is considered an abuse of notation and so discouraged.


This also applies in the case where it is the upper bound of a mapping which is under discussion.


Category:Definitions/Numbers
Category:Definitions/Boundedness",Definition:Upper Bound of Mapping/Real-Valued,['Definitions/Boundedness']
Definition:Upper Bound,Upper Bound,"A special case of an upper bound of a mapping is an upper bound of a sequence, where the domain of the mapping is $\N$.

Let $\struct {T, \preceq}$ be an ordered set.

Let $\sequence {x_n}$ be a sequence in $T$.


Let $\sequence {x_n}$ be bounded above in $T$ by $H \in T$.


Then $H$ is an upper bound of $\sequence {x_n}$.


=== Real Sequence ===

The concept is usually encountered where $\struct {T, \preceq}$ is the set of real numbers under the usual ordering $\struct {\R, \le}$:

Let $\sequence {x_n}$ be a real sequence.


Let $\sequence {x_n}$ be bounded above by $H \in \R$.


Then $H$ is an upper bound of $\sequence {x_n}$.


=== Upper Bound of Number ===
When considering the upper bound of a set of numbers, it is commonplace to ignore the set and instead refer just to the number itself.

Thus the construction:

:The set of numbers which fulfil the propositional function $\map P n$ is bounded above with the upper bound $N$

would be reported as:

:The number $n$ such that $\map P n$ has the upper bound $N$.


This construct obscures the details of what is actually being stated. Its use on   is considered an abuse of notation and so discouraged.


This also applies in the case where it is the upper bound of a mapping which is under discussion.


Category:Definitions/Numbers
Category:Definitions/Boundedness",Definition:Upper Bound of Sequence,['Definitions/Boundedness']
Definition:Upper Bound,Upper Bound,"Let $\sequence {x_n}$ be a real sequence.


Let $\sequence {x_n}$ be bounded above by $H \in \R$.


Then $H$ is an upper bound of $\sequence {x_n}$.


=== Upper Bound of Number ===
When considering the upper bound of a set of numbers, it is commonplace to ignore the set and instead refer just to the number itself.

Thus the construction:

:The set of numbers which fulfil the propositional function $\map P n$ is bounded above with the upper bound $N$

would be reported as:

:The number $n$ such that $\map P n$ has the upper bound $N$.


This construct obscures the details of what is actually being stated. Its use on   is considered an abuse of notation and so discouraged.


This also applies in the case where it is the upper bound of a mapping which is under discussion.


Category:Definitions/Numbers
Category:Definitions/Boundedness",Definition:Upper Bound of Sequence/Real,"['Definitions/Boundedness', 'Definitions/Sequences']"
Definition:Vacuous,Vacuous,"Let $P \implies Q$ be a conditional statement.

Suppose that $P$ is false.

Then the statement $P \implies Q$ is a vacuous truth, or is vacuously true.


It is frequently encountered in the form:
:$\forall x: \map P x \implies \map Q x$
when the propositional function $\map P x$ is false for all $x$.

Such a statement is also a vacuous truth.


For example, the statement:
:All cats who are expert chess-players are also fluent in ancient Sanskrit
is (vacuously) true, because (as far as the author knows) there are no cats who are expert chess-players.",Definition:Vacuous Truth,['Definitions/Logic']
Definition:Vacuous,Vacuous,"The empty set is a set which has no elements.

That is, $x \in \O$ is false, whatever $x$ is.


It is usually denoted by some variant of a zero with a line through it, for example $\O$ or $\emptyset$, and can always be represented as $\set {}$.",Definition:Empty Set,"['Definitions/Empty Set', 'Definitions/Set Theory']"
Definition:Vacuous,Vacuous,"Take the summation:
:$\ds \sum_{\map \Phi j} a_j$
where $\map \Phi j$ is a propositional function of $j$.

Suppose that there are no values of $j$ for which $\map \Phi j$ is true.

Then $\ds \sum_{\map \Phi j} a_j$ is defined as being $0$.

This summation is called a vacuous summation.


This is because:
:$\forall a: a + 0 = a$
where $a$ is a number.

Hence for all $j$ for which $\map \Phi j$ is false, the sum is unaffected.


This is most frequently seen in the form:
:$\ds \sum_{j \mathop = m}^n a_j = 0$
where $m > n$.

In this case, $j$ can not at the same time be both greater than or equal to $m$ and less than or equal to $n$.


Some sources consider such a treatment as abuse of notation.",Definition:Summation/Vacuous Summation,['Definitions/Summations']
Definition:Vacuous,Vacuous,"Take the composite expressed as a continued product:
:$\ds \prod_{\map R j} a_j$
where $\map R j$ is a propositional function of $j$.

Suppose that there are no values of $j$ for which $\map R j$ is true.

Then $\ds \prod_{\map R j} a_j$ is defined to be $1$.

Beware: not zero.

This composite is called a vacuous product.


This is because:
:$\forall a: a \times 1 = a$
where $a$ is a number.

Hence for all $j$ for which $\map R j$ is false, the value of the product is unaffected.


This is most frequently seen in the form:
:$\ds \prod_{j \mathop = m}^n a_j = 1$
where $m > n$.

In this case, $j$ can not at the same time be both greater than or equal to $m$ and less than or equal to $n$.",Definition:Continued Product/Vacuous Product,['Definitions/Continued Products']
Definition:Valid,Valid,"A valid argument is a logical argument in which the premises provide conclusive reasons for the conclusion.


When a proof is valid, we may say one of the following:
* The conclusion follows from the premises;
* The premises entail the conclusion;
* The conclusion is true on the strength of the premises;
* The conclusion is drawn from the premises;
* The conclusion is deduced from the premises;
* The conclusion is derived from the premises.


=== Proof ===

If all the premises of a valid argument are true, then the conclusion must also therefore be true.

It is not possible for the premises of a valid argument to be true, but for the conclusion to be false.

A proof is a valid argument whose premises are all true.


Hence a valid argument that has one or more false premises is not a proof.


Suppose $P$ is a proposition whose truth or falsehood is to be determined.

Constructing a valid argument upon a set of premises, all of which have previously been established as being true, is called proving $P$.


=== Formal Proof ===

Let $\mathscr P$ be a proof system for a formal language $\LL$.

Let $\mathscr P$ be a proof system for a formal language $\LL$.


Let $\phi$ be a WFF of $\LL$.

A formal proof of $\phi$ in $\mathscr P$ is a collection of axioms and rules of inference of $\mathscr P$ that leads to the conclusion that $\phi$ is a theorem of $\mathscr P$.


The term formal proof is also used to refer to specific presentations of such collections.

For example, the term applies to tableau proofs in natural deduction.",Definition:Valid Argument,"['Definitions/Valid Arguments', 'Definitions/Logical Arguments']"
Definition:Valid,Valid,"Let $\LL$ be a formal language.

Part of specifying a formal semantics $\mathscr M$ for $\LL$ is to define a notion of validity.


Concretely, a precise meaning needs to be assigned to the phrase:

:""The $\LL$-WFF $\phi$ is valid in the $\mathscr M$-structure $\MM$.""

It can be expressed symbolically as:

:$\MM \models_{\mathscr M} \phi$",Definition:Formal Semantics/Valid,['Definitions/Formal Semantics']
Definition:Value,Value,"A variable $x$ may be (temporarily, conceptually) identified with a particular object.

If so, then that object is called the value of $x$.",Definition:Variable/Value,"['Definitions/Predicate Logic', 'Definitions/Algebra', 'Definitions/Variables']"
Definition:Value,Value,"=== Definition 1 ===
Let $x \in \R$ be a real number.


The absolute value of $x$ is denoted $\size x$, and is defined using the usual ordering on the real numbers as follows:
:$\size x = \begin{cases} x & : x > 0 \\ 0 & : x = 0 \\ -x & : x < 0 \end{cases}$

=== Definition 2 ===
Let $x \in \R$ be a real number.

The absolute value of $x$ is denoted $\size x$, and is defined as:

:$\size x = +\sqrt {x^2}$

where $+\sqrt {x^2}$ is the positive square root of $x^2$.",Definition:Absolute Value,"['Definitions/Absolute Value Function', 'Definitions/Field Theory', 'Definitions/Algebra', 'Definitions/Real Analysis']"
Definition:Value,Value,"Let $F$ be a field, such as the field of real numbers $\R$.


=== Finite Continued Fraction ===
Let $F$ be a field, such as the field of real numbers $\R$.

Let $n \ge 0$ be a natural number.

Let $\sequence {a_k}_{0 \mathop \le k \mathop \le n}$ be a finite continued fraction in $F$.

Let $\overline F = F \cup \set \infty$ be extended by infinity.


=== Definition 1 ===

The value $\sqbrk {a_0, a_1, \ldots, a_n} \in F \cup \set \infty$ is the right iteration of the binary operation:
:$\sqbrk {\cdot, \cdot}: F \times \overline F \to \overline F$:
:$\sqbrk {a, b} = a + \dfrac 1 b$.
That is, it is recursively defined as:
:$\sqbrk {a_0, \ldots, a_n} = \begin{cases} 
  a_0 & : n = 0 \\
  a_0 + \dfrac 1 {\sqbrk {a_1, \ldots, a_n} } & : n > 0 \\
\end{cases}$
or as:
:$\sqbrk {a_0, \ldots, a_n} = \begin{cases}
  a_0 & : n = 0 \\
  \sqbrk {a_0, \ldots, a_{n - 2}, a_{n - 1} + \dfrac 1 {a_n} } & : n > 0 \\
\end{cases}$


=== Definition 2 ===

Let the matrix product:
:$\begin{pmatrix} a_0 & 1 \\ 1 & 0 \end{pmatrix} \cdots \begin{pmatrix} a_n & 1 \\ 1 & 0 \end{pmatrix} = \begin{pmatrix} x_{11} & x_{12} \\ x_{21} & x_{22} \end{pmatrix}$


The value of the finite continued fraction is $\dfrac{x_{11} }{x_{21} }$

=== Infinite Continued Fraction ===
Let $\struct {F, \norm {\,\cdot\,} }$ be a valued field.

Let $C = \sequence {a_n}_{n \mathop \ge 0}$ be a infinite continued fraction in $F$.


Let $C$ converge to $x \in F$:

Then $x$ is the value of $C$.",Definition:Value of Continued Fraction,['Definitions/Continued Fractions']
Definition:Value,Value,"Let $f: S \to T$ be a mapping.

Let $s \in S$.

The image of $s$ (under $f$) is defined as:

:$\Img s = \map f s = \ds \bigcup \set {t \in T: \tuple {s, t} \in f}$

That is, $\map f s$ is the element of the codomain of $f$ related to $s$ by $f$.


By the nature of a mapping, $\map f s$ is guaranteed to exist and to be unique for any given $s$ in the domain of $f$.",Definition:Image (Relation Theory)/Mapping/Element,['Definitions/Images']
Definition:Value,Value,"Let $G$ be a game.


The value of $G$ is the payoff resulting from a solution of $G$.",Definition:Value of Game,"['Definitions/Values of Games', 'Definitions/Game Theory']"
Definition:Value,Value,"Let $K$ be a physical constant.

The value of $K$ is defined as the number of units of the specific physical quantity that go to make up $K$.


Category:Definitions/Physics",Definition:Value (Physics),"['Definitions/Physics', 'Definitions/Physics']"
Definition:Vanishing Ideal,Vanishing Ideal,"Let $A$ be a commutative ring with unity.

Let $V \subseteq \Spec A$ be a set of prime ideals of $A$.


Its vanishing ideal is its intersection, the set of elements of $A$ that are in each $\mathfrak p \in V$:
:$\map I V = \bigcap V$",Definition:Vanishing Ideal of Set of Prime Ideals,['Definitions/Commutative Algebra']
Definition:Vanishing Ideal,Vanishing Ideal,"Let $k$ be a field.

Let $n \ge 0$ be a natural number.

Let $k \sqbrk {X_1, \ldots, X_n}$ be the polynomial ring in $n$ variables over $k$.

Let $S \subseteq \mathbb A^n_k$ be a subset of the standard affine space over $k$.


Its vanishing ideal is the ideal:
:$\map I S = \set {f \in k \sqbrk {X_1, \ldots, X_n} : \forall x \in S : \map f x = 0}$",Definition:Vanishing Ideal of Subset of Affine Space,['Definitions/Algebraic Geometry']
Definition:Vertex,Vertex," 

Let $G = \struct {V, E}$ be a graph.

The vertices (singular: vertex) are the elements of $V$.

Informally, the vertices are the points that are connected by the edges.


In the above, the vertices are the points $A, B, C, D, E, F, G$ which are marked as dots.",Definition:Graph (Graph Theory)/Vertex,['Definitions/Vertices of Graphs']
Definition:Vertex,Vertex,"A vertex is a point on the boundary of a geometric figure at which $2$ or more line segments meet.

=== Vertex of Polygon ===

:

:

A corner of a polygon is known as a vertex.

Thus, in the polygon above, the vertices are $A, B, C, D$ and $E$.

=== Vertex of Polyhedron ===
The vertices of a polyhedron are the vertices of the polygons which constitute its faces.",Definition:Vertex (Geometry),['Definitions/Geometry']
Definition:Vertex,Vertex,":

A corner of a polygon is known as a vertex.

Thus, in the polygon above, the vertices are $A, B, C, D$ and $E$.",Definition:Polygon/Vertex,"['Definitions/Polygons', 'Definitions/Vertices (Geometry)']"
Definition:Vertex,Vertex,The vertices of a polyhedron are the vertices of the polygons which constitute its faces.,Definition:Polyhedron/Vertex,"['Definitions/Polyhedra', 'Definitions/Vertices (Geometry)']"
Definition:Vertex,Vertex,The point at which the arms of an angle meet is known as the vertex of that angle.,Definition:Angle/Vertex,['Definitions/Angles']
Definition:Vertex,Vertex,The common vertex of the angles containing a solid angle is known as the vertex of that solid angle.,Definition:Solid Angle/Vertex,['Definitions/Solid Angles']
Definition:Vertex,Vertex,"Consider a cone consisting of the set of all straight lines joining the boundary of a plane figure $PQR$ to a point $A$ not in the same plane of $PQR$:


:


In the above diagram, the point $A$ is known as the apex of the cone.",Definition:Cone (Geometry)/Apex,['Definitions/Cones']
Definition:Vertex,Vertex,"=== Vertex of Ellipse ===
:


Let $K$ be an ellipse.

A vertex of $K$ is either of the two endpoints of the major axis of $K$.


In the above diagram, $V_1$ and $V_2$ are the vertices of $K$.

=== Vertex of Parabola ===
:


Let $P$ be a parabola.

The vertex of $P$ is the point where the axis intersects $P$.


In the above diagram, $V$ is the vertex of $P$.

=== Vertex of Hyperbola ===
",Definition:Vertex of Conic Section,"['Definitions/Vertices of Conic Sections', 'Definitions/Conic Sections']"
Definition:Vertex,Vertex,":


Let $K$ be an ellipse.

A vertex of $K$ is either of the two endpoints of the major axis of $K$.


In the above diagram, $V_1$ and $V_2$ are the vertices of $K$.",Definition:Ellipse/Vertex,"['Definitions/Vertices of Ellipses', 'Definitions/Vertices of Conic Sections', 'Definitions/Ellipses']"
Definition:Vertex,Vertex,":


Let $P$ be a parabola.

The vertex of $P$ is the point where the axis intersects $P$.


In the above diagram, $V$ is the vertex of $P$.",Definition:Parabola/Vertex,"['Definitions/Vertices of Conic Sections', 'Definitions/Parabolas']"
Definition:Walk,Walk,"Let $G = \struct {V, E}$ be a graph.

A walk $W$ on $G$ is:
:an alternating sequence of vertices $v_1, v_2, \ldots$ and edges $e_1, e_2, \ldots$ of $G$
:beginning and ending with a vertex
:in which edge $e_j$ of $W$ is incident with the vertex $v_j$ and the vertex $v_{j + 1}$.


A walk between two vertices $u$ and $v$ is called a $u$-$v$ walk.


To describe a walk on a simple graph it is sufficient to list just the vertices in order, as the edges (being unique between vertices) are unambiguous.


=== Closed ===
A closed walk is a walk whose first vertex is the same as the last.

That is, it is a walk which ends where it starts.
=== Open ===
An open walk is a walk whose first vertex and last vertex are distinct.

That is, it is a walk which ends on a different vertex from the one where it starts.",Definition:Walk (Graph Theory),"['Definitions/Graph Theory', 'Definitions/Walks']"
Definition:Walk,Walk,"A closed walk is a walk whose first vertex is the same as the last.

That is, it is a walk which ends where it starts.",Definition:Walk (Graph Theory)/Closed,['Definitions/Walks']
Definition:Walk,Walk,"An open walk is a walk whose first vertex and last vertex are distinct.

That is, it is a walk which ends on a different vertex from the one where it starts.",Definition:Walk (Graph Theory)/Open,['Definitions/Walks']
Definition:Walk,Walk,"Let $G = \struct {V, A}$ be a digraph.


A directed walk in $G$ is a finite or infinite sequence $\sequence {x_k}$ such that:

:$\forall k \in \N: k + 1 \in \Dom {\sequence {x_k} }: \tuple {x_k, x_{k + 1} } \in A$",Definition:Directed Walk,"['Definitions/Digraphs', 'Definitions/Walks']"
Definition:Walk,Walk,"=== One-Dimensional Random Walk ===
Let $\sequence {X_n}_{n \mathop \ge 0}$ be a Markov chain whose state space is the set of integers $\Z$.

Let $\sequence {X_n}$ be such that $X_{n + 1}$ is an element of the set $\set {X_n + 1, X_n, X_n - 1}$.

Then $\sequence {X_n}$ is a one-dimensional random walk.

Category:Definitions/Stochastic Processes
Category:Definitions/Markov Chains
Category:Definitions/Random Walks",Definition:Random Walk,"['Definitions/Stochastic Processes', 'Definitions/Markov Chains', 'Definitions/Random Walks']"
Definition:Wavelength,Wavelength,"Let $\phi$ be a periodic wave expressed as:
:$\forall x, t \in \R: \map \phi {x, t} = \map f {x - c t}$


The wavelength $\lambda$ of $\phi$ is the period of the wave profile of $\phi$.",Definition:Periodic Wave/Wavelength,"['Definitions/Periodic Waves', 'Definitions/Wavelength']"
Definition:Wavelength,Wavelength,"The wavelength of a wave is is the distance over which the wave's shape repeats.


 ",Definition:Wavelength (Physics),['Definitions/Physics']
Definition:Weak,Weak,"In a conditional $p \implies q$, the statement $q$ is weaker than $p$.",Definition:Conditional/Language of Conditional/Weak,['Definitions/Conditional']
Definition:Weak,Weak,"Let $X$ be a set.

Let $I$ be an indexing set.


Let $\family {\struct {Y_i, \tau_i} }_{i \mathop \in I}$ be an indexed family of topological spaces indexed by $I$.

Let $\family {f_i: X \to Y_i}_{i \mathop \in I}$ be an indexed family of mappings indexed by $I$.


=== Definition 1 ===
Let $X$ be a set.

Let $I$ be an indexing set.


Let $\family {\struct {Y_i, \tau_i} }_{i \mathop \in I}$ be an indexed family of topological spaces indexed by $I$.

Let $\family {f_i: X \to Y_i}_{i \mathop \in I}$ be an indexed family of mappings indexed by $I$.


Let:
:$\SS = \set {f_i^{-1} \sqbrk U: i \in I, U \in \tau_i}$
where $f_i^{-1} \sqbrk U$ denotes the preimage of $U$ under $f_i$.

The topology $\tau$ on $X$ generated by $\SS$ is called the initial topology on $X$ with respect to $\family {f_i}_{i \mathop \in I}$.

=== Definition 2 ===
Let $X$ be a set.

Let $I$ be an indexing set.


Let $\family {\struct {Y_i, \tau_i} }_{i \mathop \in I}$ be an indexed family of topological spaces indexed by $I$.

Let $\family {f_i: X \to Y_i}_{i \mathop \in I}$ be an indexed family of mappings indexed by $I$.


Let $\tau$ be the coarsest topology on $X$ such that each $f_i: X \to Y_i$ is $\tuple {\tau, \tau_i}$-continuous.

Then $\tau$ is known as the initial topology on $X$ with respect to $\family {f_i}_{i \mathop \in I}$.",Definition:Initial Topology,"['Definitions/Examples of Topologies', 'Definitions/Initial Topology']"
Definition:Weak,Weak,"Let $T = \struct {S, \tau}$ be a topological space.


Then $T$ is weakly locally compact  if and only if  every point of $S$ has a compact neighborhood.",Definition:Weakly Locally Compact Space,['Definitions/Compact Spaces']
Definition:Weight,Weight,"The weight of a body is the magnitude of the force exerted on it by the influence of a gravitational field.


The context is that the gravitational field in question is usually that of the Earth.


=== Weigh ===
To weigh a body is to determine its weight, and thence its mass.

Similarly we can say that:
:body $B$ weigh $x$
to mean:
:the weight of the body $B$ is $x$
and both mean the same thing.

=== Dimension of Weight ===
The dimension of weight is $\mathsf M \mathsf L \mathsf T^{-2}$: mass times acceleration, that is, a force.

=== Units of Weight ===
The SI unit of weight is $\mathrm N$ (newton).

The CGS unit of weight is $\mathrm {dyn}$ (dyne).",Definition:Weight (Physics),"['Definitions/Weight (Physics)', 'Definitions/Physics', 'Definitions/Dimensions of Measurement']"
Definition:Weight,Weight,"Let $N = \struct {V, E, w}$ be a network with weight function $w: E \to \R$.


The values of the elements of $E$ under $w$ are known as the weights of the edges of $N$.


The weights of a network $N$ can be depicted by writing the appropriate numbers next to the edges of the underlying graph of $N$.",Definition:Network/Weight,['Definitions/Network Theory']
Definition:Weight,Weight,"Let $N = \left({V, E, w}\right)$ be a network.

The mapping $w: E \to \R$ is known as the weight function of $N$.",Definition:Network/Weight Function,['Definitions/Network Theory']
Definition:Weight,Weight,"A weight function on a set $S$ is a mapping from $S$ to the real numbers:
:$w: S \to \R$


It is common for the requirements of a specific application under discussion for the codomain of $w$ to be restricted to the positive reals:
:$w: S \to \R_{\ge 0}$


The thing that determines whether a given mapping is a weight function depends more on how it is used.",Definition:Weight Function,"['Definitions/Statistics', 'Definitions/Discrete Mathematics', 'Definitions/Analysis']"
Definition:Weight,Weight,"Let $C$ be a codeword of a linear code.

The weight of $C$ is the number of non-zero terms of $C$.",Definition:Weight of Linear Codeword,['Definitions/Linear Codes']
Definition:Weight,Weight,"A set of weights consists of a number of solid pieces of matter whose mass is measured and known.

They are used as reference masses for the purpose of determining to a certain limit of accuracy the weight of a body whose mass is unknown.


 ",Definition:Weights,['Definitions/Weight (Physics)']
Definition:Word,Word,"Let $\struct {M, \circ}$ be a magma.

Let $S \subseteq M$ be a subset.

A word in $S$ is the product of a finite number of elements of $S$.


The set of words in $S$ is denoted $\map W S$:
:$\map W S := \set {s_1 \circ s_2 \circ \cdots \circ s_n: n \in \N_{>0}: s_i \in S, 1 \le i \le n}$


Note that there is nothing in this definition preventing any of the elements of $S$ being repeated, neither is anything said about the order of these elements.


=== Monoid ===
Let $\struct {M, \circ}$ be a monoid whose identity element is $e$.

Let $S \subseteq M$ be a subset of $M$.

The set of words in $S$ is denoted and defined:
:$\map W S := \set {\ds \sum_{i \mathop = 1}^r n_i \cdot s_i : r \in \N, n_i \in \N, s_i \in S}$
where:
:$n_i \cdot s_i$ denotes the power of $s_i$:
::$n \cdot a = \begin {cases}
e & : n = 0 \\
\paren {\paren {n - 1} \cdot a} \circ a & : n > 0
\end {cases}$
:$\ds \sum_{i \mathop = 1}^r n_i \cdot s_i := \paren {n_1 \cdot s_1} \circ \paren {n_2 \cdot s_2} \circ \cdots \circ \paren {n_r \cdot s_r}$",Definition:Word (Abstract Algebra),"['Definitions/Words (Abstract Algebra)', 'Definitions/Group Theory', 'Definitions/Abstract Algebra']"
Definition:Word,Word,"Let $n \in \N$ be a natural number.

Let $\N^*_n$ be the first $n$ non-zero natural numbers:
:$\N^*_n := \set {1, 2, \ldots, n}$


=== Definition 1 ===
Let $n \in \N$ be a natural number.

Let $\N^*_n$ be the first $n$ non-zero natural numbers:
:$\N^*_n := \set {1, 2, \ldots, n}$


An ordered tuple (of length $n$) is a finite sequence whose domain is $\N^*_n$.

=== Definition 2 ===
Let $n \in \N$ be a natural number.

Let $\N_n$ denote the first $n$ non-zero natural numbers:
:$\N_n := \set {1, 2, \ldots, n}$


Let $\family {S_i}_{i \mathop \in \N_n}$ be a family of sets indexed by $\N_n$.

Let $\ds \prod_{i \mathop \in \N_n} S_i$ be the Cartesian product of $\family {S_i}_{i \mathop \in \N_n}$.

An ordered tuple of length $n$ of $\family {S_i}$ is an element of $\ds \prod_{i \mathop \in \N_n} S_i$.

=== Ordered Tuple on Set ===
Let $S$ be a set.

Let $s: \N^*_n \to S$ be an ordered tuple.


Then $s$ is called an ordered tuple on $S$, its codomain.


Category:Definitions/Ordered Tuples

=== Empty Ordered Tuple ===
Let $S$ be a set.

The empty ordered tuple on $S$ is the empty mapping:

:$\O \to S$

from the empty set $\O$ to $S$.

It is justified to call this an ordered tuple because the ""first $0$ non-zero natural numbers"" form the empty set:

:$\N^*_0 = \O$

=== Ordered Tuple Defined by Sequence ===
Let $\sequence {a_k}_{k \mathop \in A}$ be a finite sequence of $n$ terms.

Let $\sigma$ be a permutation of $A$.


Then the ordered $n$-tuple defined by the sequence $\sequence {a_{\map \sigma k} }_{k \mathop \in A}$ is the ordered $n$-tuple:
:$\sequence {a_{\map \sigma {\map \tau j} } }_{1 \mathop \le j \mathop \le n}$
where $\tau$ is the unique isomorphism from the totally ordered set $\closedint 1 n$ onto the totally ordered set $A$.


 ",Definition:Ordered Tuple,"['Definitions/Set Theory', 'Definitions/Mapping Theory', 'Definitions/Cartesian Product', 'Definitions/Sequences', 'Definitions/Ordered Tuples']"
Definition:Word,Word,"Let $S$ be a set.


A group word on $S$ is an ordered tuple on the set of literals $S^\pm$ of $S$.",Definition:Group Word on Set,['Definitions/Group Words']
Definition:Word,Word,"Let $\AA$ be an alphabet.


Then a word in $\AA$ is a juxtaposition of finitely many (primitive) symbols of $\AA$.

Words are the most ubiquitous of collations used for formal languages.",Definition:Word (Formal Systems),['Definitions/Collations']
Definition:Word,Word,"A word in natural language is intuitively understood as a sequence of sounds which expresses a concept.

When written down, it appears as a sequence of letters, each one of which either is, or contributes to, a phoneme.",Definition:Word (Natural Language),['Definitions/Language Definitions']
Definition:Word,Word,"Let $\struct {G, \circ}$ be a group.

Let $S$ be a generating set for $G$ which is closed under inverses (that is, $x^{-1} \in S \iff x \in S$).


The word metric on $G$ with respect to $S$ is the metric $d_S$ defined as follows:

:For any $g, h \in G$, let $\map {d_S} {g, h}$ be the minimum length among the finite sequences $\tuple {x_1, \dots, x_n}$ with each $x_i \in S$ such that $g \circ x_1 \circ \cdots \circ x_n = h$.


Informally, $\map {d_S} {g, h}$ is the smallest number of elements from $S$ that one needs to multiply by to get from $g$ to $h$.",Definition:Word Metric,"['Definitions/Group Theory', 'Definitions/Examples of Metric Spaces']"
Definition:Zero,Zero,"The zero ordinal, denoted $0$, is the empty set $\O$.",Definition:Zero (Ordinal),['Definitions/Ordinals']
Definition:Zero,Zero,"The cardinal associated with the empty set $\O$ is called zero, and is denoted $0$.


More informally, this means that zero is defined as being the number of elements in the empty set.",Definition:Zero (Cardinal),['Definitions/Cardinals']
Definition:Zero,Zero,"The number zero is defined as being the cardinal of the empty set.


=== Naturally Ordered Semigroup ===
Let $\struct {S, \circ, \preceq}$ be a naturally ordered semigroup.

Then from  , $\struct {S, \circ, \preceq}$ has a smallest element.


This smallest element of $\struct {S, \circ, \preceq}$ is called zero and has the symbol $0$.

That is:
:$\forall n \in S: 0 \preceq n$

=== Natural Numbers ===

=== Integers ===

=== Rational Numbers ===

=== Real Numbers ===

=== Complex Numbers ===
Let $\C$ denote the set of complex numbers.

The zero of $\C$ is the complex number:
:$0 + 0 i$

 ",Definition:Zero (Number),"['Definitions/Abstract Algebra', 'Definitions/Zero']"
Definition:Zero,Zero,"Let $\C$ denote the set of complex numbers.

The zero of $\C$ is the complex number:
:$0 + 0 i$",Definition:Zero (Number)/Complex,"['Definitions/Complex Numbers', 'Definitions/Zero']"
Definition:Zero,Zero,"Let $x \in \R$ be a number.

Let $b \in \Z$ such that $b > 1$ be a number base in which $x$ is represented.

By the Basis Representation Theorem, $x$ can be expressed uniquely in the form:

:$\ds x = \sum_{j \mathop \in \Z}^m r_j b^j$


Any instance of $r_j$ being equal to $0$ is known as a zero (digit) of $n$.",Definition:Zero Digit,"['Definitions/Zero Digit', 'Definitions/Zero', 'Definitions/Digits', 'Definitions/Numbers']"
Definition:Zero,Zero,"Let $\struct {S, \circ, \preceq}$ be a naturally ordered semigroup.

Then from  , $\struct {S, \circ, \preceq}$ has a smallest element.


This smallest element of $\struct {S, \circ, \preceq}$ is called zero and has the symbol $0$.

That is:
:$\forall n \in S: 0 \preceq n$",Definition:Zero (Number)/Naturally Ordered Semigroup,"['Definitions/Naturally Ordered Semigroup', 'Definitions/Zero']"
Definition:Zero,Zero,"Let $\struct {S, \circ}$ be an algebraic structure.


=== Left Zero ===
Let $\struct {S, \circ}$ be an algebraic structure.

An element $z_L \in S$ is called a left zero element (or just left zero)  if and only if :
:$\forall x \in S: z_L \circ x = z_L$

=== Right Zero ===
Let $\struct {S, \circ}$ be an algebraic structure.

An element $z_R \in S$ is called a right zero element (or just right zero)  if and only if :
:$\forall x \in S: x \circ z_R = z_R$

=== Zero ===

An element $z \in S$ is called a two-sided zero element (or simply zero element or zero)  if and only if  it is both a left zero and a right zero:
:$\forall x \in S: x \circ z = z = z \circ x$",Definition:Zero Element,"['Definitions/Abstract Algebra', 'Definitions/Zero Elements']"
Definition:Zero,Zero,"Let $\struct {R, +, \circ}$ be a ring.

The identity for ring addition is called the ring zero (of $\struct {R, +, \circ}$).


It is denoted $0_R$ (or just $0$ if there is no danger of ambiguity).",Definition:Ring Zero,['Definitions/Ring Theory']
Definition:Zero,Zero,"Let $\struct {F, +, \times}$ be a field.

The identity for field addition is called the field zero (of $\struct {F, +, \times}$).


It is denoted $0_F$ (or just $0$ if there is no danger of ambiguity).",Definition:Field Zero,['Definitions/Field Theory']
Definition:Zero,Zero,"Let $\mathbb A$ be one of the standard number systems $\N,\Z,\Q,\R,\C$.

Let $S$ be a set.


Let $f_0: S \to \mathbb A$ denote the constant mapping:

:$\forall x \in S: \map {f_0} x = 0$

Then $f_0$ is referred to as the zero mapping.


=== Vector Space ===
Let $Y$ be a vector space.

Let $S$ be a set.

Let $\mathbf 0_Y$ be the identity element of $Y$.

Suppose $\mathbf 0 : S \to Y$ is a mapping such that:

:$\forall x \in S: \map {\mathbf 0} x = \mathbf 0_Y$


Then $\mathbf 0$ is referred to as the zero mapping.

=== Distribution ===
Let $\map \DD \R$ be the test function space.

Let $\mathbf 0 \in \map {\DD'} \R$ be a distribution.

Suppose:

:$\forall \phi \in \map \DD \R : \map {\mathbf 0} \phi = 0$


Then $\mathbf 0$ is referred to as the zero distribution.


Category:Definitions/Distributions",Definition:Zero Mapping,['Definitions/Mapping Theory']
Definition:Zero,Zero,"Let $f: R \to R$ be a mapping on a ring $R$.

Let $x \in R$.


Then the values of $x$ for which $\map f x = 0_R$ are known as the roots of the mapping $f$.",Definition:Root of Mapping,"['Definitions/Roots of Mappings', 'Definitions/Ring Theory', 'Definitions/Field Theory', 'Definitions/Real Analysis', 'Definitions/Complex Analysis']"
Definition:Zero,Zero,"Let $\struct {R, +_R, \times_R}$ be a ring.

Let $\struct {G, +_G}$ be an abelian group.

Let $\struct {G, +_G, \circ}_R$ be an $R$-module.


The identity of $\struct {G, +_G}$ is usually denoted $\bszero$, or some variant of this, and called the zero vector:

:$\forall \mathbf a \in \struct {G, +_G, \circ}_R: \bszero +_G \mathbf a = \mathbf a = \mathbf a +_G \bszero$


Note that on occasion it is advantageous to denote the zero vector differently, for example by $e$, or $\bszero_V$ or $\bszero_G$, in order to highlight the fact that the zero vector is not the same object as the zero scalar.


=== Zero Vector in $\R^n$ ===
Let $\struct {\R^n, +, \times}_\R$ be a real vector space.

The zero vector in $\struct {\R^n, +, \times}_\R$ is:

:$\mathbf 0_{n \times 1} := \begin {bmatrix} 0 \\ 0 \\ \vdots \\ 0 \end {bmatrix}$

where $0 \in \R$.

=== Zero Vector Quantity ===
A vector quantity whose magnitude is zero is referred to as a zero vector.",Definition:Zero Vector,"['Definitions/Module Theory', 'Definitions/Vector Algebra', 'Definitions/Linear Algebra', 'Definitions/Vectors', 'Definitions/Zero Vectors']"
Definition:Zero Locus,Zero Locus,"Let $k$ be a field.

Let $n\geq1$ be a natural number.

Let $A = k \sqbrk {X_1, \ldots, X_n}$ be the polynomial ring in $n$ variables over $k$.

Let $I \subseteq A$ be a set.


Then the zero locus of $I$ is the set:

:$\map V I = \set {x \in k^n : \forall f \in I: \map f x = 0}$",Definition:Zero Locus of Set of Polynomials,['Definitions/Algebraic Geometry']
Definition:Zero Locus,Zero Locus,"Let $A$ be a commutative ring with unity.

Let $S \subseteq A$ be a subset.


The vanishing set of $S$ is the set of prime ideals of $A$ containing $S$:
:$\map V S = \set {\mathfrak p \in \Spec A: \mathfrak p \supseteq S}$",Definition:Vanishing Set of Subset of Ring,['Definitions/Zariski Topology']
