2025-03-08 00:50:36,314 : 60263510.py : <module> : INFO : fold 1: new term 412, new candi 451,  train 1121 
2025-03-08 00:50:36,348 : 60263510.py : <module> : INFO : fold 2: new term 362, new candi 478,  train 1144 
2025-03-08 00:50:36,378 : 60263510.py : <module> : INFO : fold 3: new term 342, new candi 501,  train 1141 
2025-03-08 00:50:36,408 : 60263510.py : <module> : INFO : fold 4: new term 445, new candi 417,  train 1122 
2025-03-08 00:50:36,438 : 60263510.py : <module> : INFO : fold 5: new term 423, new candi 429,  train 1132 
2025-03-08 00:52:35,189 : 60263510.py : <module> : INFO : fold 1: new term 412, new candi 451,  train 1121 
2025-03-08 00:52:35,219 : 60263510.py : <module> : INFO : fold 2: new term 362, new candi 478,  train 1144 
2025-03-08 00:52:35,246 : 60263510.py : <module> : INFO : fold 3: new term 342, new candi 501,  train 1141 
2025-03-08 00:52:35,272 : 60263510.py : <module> : INFO : fold 4: new term 445, new candi 417,  train 1122 
2025-03-08 00:52:35,299 : 60263510.py : <module> : INFO : fold 5: new term 423, new candi 429,  train 1132 
2025-03-08 00:53:41,916 : 1481625066.py : <module> : INFO : fold 1: new term 412, new candi 451,  train 1121 
2025-03-08 00:53:41,946 : 1481625066.py : <module> : INFO : fold 2: new term 362, new candi 478,  train 1144 
2025-03-08 00:53:41,972 : 1481625066.py : <module> : INFO : fold 3: new term 342, new candi 501,  train 1141 
2025-03-08 00:53:41,999 : 1481625066.py : <module> : INFO : fold 4: new term 445, new candi 417,  train 1122 
2025-03-08 00:53:42,024 : 1481625066.py : <module> : INFO : fold 5: new term 423, new candi 429,  train 1132 
2025-03-08 00:55:00,856 : 213369433.py : <module> : INFO : fold 1: new term 412, new candi 451,  train 1121 
2025-03-08 00:55:00,885 : 213369433.py : <module> : INFO : fold 2: new term 362, new candi 478,  train 1144 
2025-03-08 00:55:00,912 : 213369433.py : <module> : INFO : fold 3: new term 342, new candi 501,  train 1141 
2025-03-08 00:55:00,937 : 213369433.py : <module> : INFO : fold 4: new term 445, new candi 417,  train 1122 
2025-03-08 00:55:00,963 : 213369433.py : <module> : INFO : fold 5: new term 423, new candi 429,  train 1132 
2025-03-08 00:55:54,701 : 213369433.py : <module> : INFO : fold 1: new term 412, new candi 419,  train 1153 
2025-03-08 00:55:54,736 : 213369433.py : <module> : INFO : fold 2: new term 362, new candi 441,  train 1181 
2025-03-08 00:55:54,763 : 213369433.py : <module> : INFO : fold 3: new term 342, new candi 461,  train 1181 
2025-03-08 00:55:54,789 : 213369433.py : <module> : INFO : fold 4: new term 445, new candi 379,  train 1160 
2025-03-08 00:55:54,816 : 213369433.py : <module> : INFO : fold 5: new term 423, new candi 392,  train 1169 
2025-03-08 01:09:20,291 : 916236790.py : __call__ : INFO : loaded dataset, sample = 1169
2025-03-08 01:09:20,292 : 1341934384.py : train_scp : INFO : 109483778
2025-03-08 01:11:43,885 : 916236790.py : __call__ : INFO : loaded dataset, sample = 1153
2025-03-08 01:11:43,886 : 1341934384.py : train_scp : INFO : 109483778
2025-03-08 01:11:54,845 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.20052140399813653
2025-03-08 01:12:07,279 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.19964990548789502
2025-03-08 01:12:18,168 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.1992850886285305
2025-03-08 01:12:30,367 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.1993238827213645
2025-03-08 01:12:41,210 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.19944994911551475
2025-03-08 01:12:50,361 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_1_epoch.pt
2025-03-08 01:13:01,430 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.1979841920733452
2025-03-08 01:13:14,094 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.1984727030992508
2025-03-08 01:13:25,565 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.19563182386259237
2025-03-08 01:13:36,680 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.1953885433450341
2025-03-08 01:13:48,951 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.19266429615020753
2025-03-08 01:13:57,677 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_2_epoch.pt
2025-03-08 01:14:09,606 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.1808740333840251
2025-03-08 01:14:21,442 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.17899120477959513
2025-03-08 01:14:33,955 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.181985233190159
2025-03-08 01:14:45,379 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.1832689198013395
2025-03-08 01:14:56,819 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.18346094315499067
2025-03-08 01:15:04,770 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_3_epoch.pt
2025-03-08 01:15:16,597 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.1790999135375023
2025-03-08 01:15:27,984 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.18445047358050942
2025-03-08 01:15:39,069 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.1852877208714684
2025-03-08 01:15:50,937 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.18550494438968598
2025-03-08 01:16:03,075 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.18413033932447434
2025-03-08 01:16:11,490 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_4_epoch.pt
2025-03-08 01:16:23,271 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.18630790948867798
2025-03-08 01:16:35,208 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.18226463861763478
2025-03-08 01:16:46,237 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.18219695607821146
2025-03-08 01:16:58,509 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.18070606732740999
2025-03-08 01:17:10,072 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.17916241490095855
2025-03-08 01:17:18,513 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_5_epoch.pt
2025-03-08 01:17:30,702 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.17897746793925762
2025-03-08 01:17:41,544 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.17889817362651228
2025-03-08 01:17:52,294 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.1782309706757466
2025-03-08 01:18:03,343 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.1791160016413778
2025-03-08 01:18:15,415 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.17775861244648694
2025-03-08 01:18:25,128 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_6_epoch.pt
2025-03-08 01:18:37,052 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.17195955496281384
2025-03-08 01:18:48,200 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.172132058236748
2025-03-08 01:19:00,247 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.17240844039867323
2025-03-08 01:19:11,645 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.1714234904013574
2025-03-08 01:19:23,132 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.17054335727542638
2025-03-08 01:19:33,000 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_7_epoch.pt
2025-03-08 01:19:45,330 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.15939145550131797
2025-03-08 01:19:56,770 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.14927058236673474
2025-03-08 01:20:09,715 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.14231249378373226
2025-03-08 01:20:21,159 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.13336376254446805
2025-03-08 01:20:31,324 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.12681032685190438
2025-03-08 01:20:39,427 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_8_epoch.pt
2025-03-08 01:20:51,025 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.07404961723834276
2025-03-08 01:21:02,863 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.07803405974060297
2025-03-08 01:21:13,787 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.07645510005454223
2025-03-08 01:21:25,807 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.07594185495749116
2025-03-08 01:21:37,462 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.07534677045792341
2025-03-08 01:21:46,403 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_9_epoch.pt
2025-03-08 01:21:58,609 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.08007817026227712
2025-03-08 01:22:10,494 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.07536434803158044
2025-03-08 01:22:21,781 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.07284259849538406
2025-03-08 01:22:32,752 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.07197156353853643
2025-03-08 01:22:44,765 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.07593579307198524
2025-03-08 01:22:53,249 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_10_epoch.pt
2025-03-08 01:23:04,313 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.061332782469689845
2025-03-08 01:23:15,583 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.06371596708893776
2025-03-08 01:23:26,695 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.0655845233425498
2025-03-08 01:23:39,132 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.06547279140911996
2025-03-08 01:23:50,740 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.06617794012278319
2025-03-08 01:23:59,067 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_11_epoch.pt
2025-03-08 01:24:11,595 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.06473677061498165
2025-03-08 01:24:23,478 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.06198525071144104
2025-03-08 01:24:34,344 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.060378109874824684
2025-03-08 01:24:44,658 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.05960365684702992
2025-03-08 01:24:56,283 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.05959901252388954
2025-03-08 01:25:05,877 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_12_epoch.pt
2025-03-08 01:25:17,722 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.05612586859613657
2025-03-08 01:25:29,343 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.056757274623960255
2025-03-08 01:25:41,040 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.0561875680834055
2025-03-08 01:25:52,197 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.057936951545998455
2025-03-08 01:26:03,520 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.05905942330509424
2025-03-08 01:26:11,965 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_13_epoch.pt
2025-03-08 01:26:22,870 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.05293162897229195
2025-03-08 01:26:34,515 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.056703977044671776
2025-03-08 01:26:47,220 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.05735163091371457
2025-03-08 01:26:58,780 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.05714997662231326
2025-03-08 01:27:10,300 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.057517747431993484
2025-03-08 01:27:18,644 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_14_epoch.pt
2025-03-08 01:27:29,961 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.06414232950657606
2025-03-08 01:27:41,087 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.0581412765942514
2025-03-08 01:27:52,943 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.06043104915569226
2025-03-08 01:28:04,522 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.05844413003884256
2025-03-08 01:28:15,928 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.05892508980631828
2025-03-08 01:28:24,255 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_15_epoch.pt
2025-03-08 01:28:35,719 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.054127156138420104
2025-03-08 01:28:47,152 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.05632638987153769
2025-03-08 01:28:59,573 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.05432017356157303
2025-03-08 01:29:10,324 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.05513240084052086
2025-03-08 01:29:20,939 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.05441985458880663
2025-03-08 01:29:30,572 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_16_epoch.pt
2025-03-08 01:29:41,616 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.06184486325830221
2025-03-08 01:29:53,639 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.05826318170875311
2025-03-08 01:30:04,877 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.05687906747063001
2025-03-08 01:30:16,004 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.055359181193634865
2025-03-08 01:30:27,886 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.055240404069423675
2025-03-08 01:30:36,712 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_17_epoch.pt
2025-03-08 01:30:48,538 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.045087270736694336
2025-03-08 01:31:00,483 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04857364421710372
2025-03-08 01:31:12,438 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.050608059614896776
2025-03-08 01:31:23,547 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.052142149275168775
2025-03-08 01:31:34,261 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.05264774186164141
2025-03-08 01:31:42,624 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_18_epoch.pt
2025-03-08 01:31:54,257 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04891874201595783
2025-03-08 01:32:05,287 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04791815435513854
2025-03-08 01:32:17,394 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04897073441495498
2025-03-08 01:32:29,796 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.050142458798363806
2025-03-08 01:32:40,853 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04979066892713308
2025-03-08 01:32:49,083 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_19_epoch.pt
2025-03-08 01:33:00,484 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.054320663325488565
2025-03-08 01:33:12,071 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.05121453709900379
2025-03-08 01:33:24,220 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.05059084021796783
2025-03-08 01:33:34,281 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.05391413606703281
2025-03-08 01:33:45,995 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.052298679403960706
2025-03-08 01:33:55,138 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_20_epoch.pt
2025-03-08 01:34:06,614 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.05712525196373463
2025-03-08 01:34:18,285 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.05423212306573987
2025-03-08 01:34:29,989 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.052274656519293784
2025-03-08 01:34:41,402 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.05196092322468757
2025-03-08 01:34:53,431 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.052774180106818676
2025-03-08 01:35:01,673 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_21_epoch.pt
2025-03-08 01:35:12,931 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.05755700815469027
2025-03-08 01:35:25,327 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.055625300612300636
2025-03-08 01:35:36,560 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.05108814682811499
2025-03-08 01:35:47,022 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.05135086012072861
2025-03-08 01:35:58,418 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.05060859929770231
2025-03-08 01:36:07,789 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_22_epoch.pt
2025-03-08 01:36:19,545 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.059382094368338585
2025-03-08 01:36:31,037 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.054110599029809234
2025-03-08 01:36:42,656 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.052899338242908316
2025-03-08 01:36:53,806 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.05084994897246361
2025-03-08 01:37:06,639 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.05175614546239376
2025-03-08 01:37:13,914 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_23_epoch.pt
2025-03-08 01:37:25,843 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04810140267014504
2025-03-08 01:37:37,022 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.05679985016584396
2025-03-08 01:37:48,171 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.05733987522621949
2025-03-08 01:37:59,308 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.05639941178262234
2025-03-08 01:38:11,469 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.056098465099930765
2025-03-08 01:38:20,425 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_24_epoch.pt
2025-03-08 01:38:32,214 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04762043666094542
2025-03-08 01:38:43,709 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04676024397835135
2025-03-08 01:38:54,286 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04884805121769508
2025-03-08 01:39:05,557 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.048971719732508065
2025-03-08 01:39:17,522 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04769366151839495
2025-03-08 01:39:26,403 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_25_epoch.pt
2025-03-08 01:39:38,667 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04672322869300842
2025-03-08 01:39:49,880 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.045644078738987445
2025-03-08 01:40:01,314 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04813877080877622
2025-03-08 01:40:12,290 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.051989516373723746
2025-03-08 01:40:24,346 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.05250659891217947
2025-03-08 01:40:32,837 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_26_epoch.pt
2025-03-08 01:40:43,787 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.049134302884340286
2025-03-08 01:40:56,081 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.048765028323978186
2025-03-08 01:41:06,917 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.05102380933860938
2025-03-08 01:41:18,496 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04899274284951389
2025-03-08 01:41:30,040 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.05053360550105572
2025-03-08 01:41:38,715 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_27_epoch.pt
2025-03-08 01:41:50,512 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.0408833659440279
2025-03-08 01:42:02,697 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04508347904309631
2025-03-08 01:42:13,425 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04536130487918854
2025-03-08 01:42:24,962 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.046519657131284475
2025-03-08 01:42:37,045 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04684080234915018
2025-03-08 01:42:44,991 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_28_epoch.pt
2025-03-08 01:42:57,408 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04231542345136404
2025-03-08 01:43:10,048 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04643124669790268
2025-03-08 01:43:21,310 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04601072087883949
2025-03-08 01:43:32,555 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.047904981011524794
2025-03-08 01:43:43,420 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04735731174051762
2025-03-08 01:43:51,549 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_29_epoch.pt
2025-03-08 01:44:02,201 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.043167963698506354
2025-03-08 01:44:14,515 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04821664072573185
2025-03-08 01:44:25,338 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04944627327223619
2025-03-08 01:44:37,332 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04853445667773485
2025-03-08 01:44:49,339 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.046745152182877066
2025-03-08 01:44:57,350 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_30_epoch.pt
2025-03-08 01:45:08,983 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.055981329493224624
2025-03-08 01:45:20,860 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.049482246469706295
2025-03-08 01:45:32,565 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04573132948329051
2025-03-08 01:45:42,648 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04687504991889
2025-03-08 01:45:54,254 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04701243829727173
2025-03-08 01:46:02,679 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_31_epoch.pt
2025-03-08 01:46:15,086 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.05487357597798109
2025-03-08 01:46:26,947 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.05500046974048019
2025-03-08 01:46:38,057 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.052343955660859746
2025-03-08 01:46:49,121 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.05062009716406465
2025-03-08 01:46:59,530 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.05019465527683496
2025-03-08 01:47:08,402 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_32_epoch.pt
2025-03-08 01:47:21,013 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.043790225610136985
2025-03-08 01:47:32,979 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.0446039055287838
2025-03-08 01:47:43,551 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.046697688549757005
2025-03-08 01:47:55,530 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.045674707526341084
2025-03-08 01:48:06,092 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04870181113481522
2025-03-08 01:48:14,845 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_33_epoch.pt
2025-03-08 01:48:26,377 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.05248279679566622
2025-03-08 01:48:38,595 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04967143436893821
2025-03-08 01:48:49,439 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04876618965218465
2025-03-08 01:49:00,168 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04674229856580496
2025-03-08 01:49:12,275 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04959869887679815
2025-03-08 01:49:21,337 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_34_epoch.pt
2025-03-08 01:49:33,722 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.05480724472552538
2025-03-08 01:49:44,978 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.05128946609795094
2025-03-08 01:49:56,452 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.05012305644651254
2025-03-08 01:50:08,529 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.049546166518703104
2025-03-08 01:50:19,231 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.048145239472389224
2025-03-08 01:50:27,347 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_35_epoch.pt
2025-03-08 01:50:38,130 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04953824464231729
2025-03-08 01:50:50,133 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04513657085597515
2025-03-08 01:51:01,780 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04836391276369492
2025-03-08 01:51:13,977 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.048101930031552914
2025-03-08 01:51:25,155 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04701188498735428
2025-03-08 01:51:33,284 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_36_epoch.pt
2025-03-08 01:51:44,875 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.043694525733590124
2025-03-08 01:51:56,053 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04248655339702964
2025-03-08 01:52:07,380 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.045497856065630914
2025-03-08 01:52:18,652 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04543778222054243
2025-03-08 01:52:30,685 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.0460736101642251
2025-03-08 01:52:39,626 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_37_epoch.pt
2025-03-08 01:52:52,222 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.038131649866700174
2025-03-08 01:53:02,380 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.042566153965890405
2025-03-08 01:53:13,549 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.0418891853839159
2025-03-08 01:53:25,345 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04218767874874175
2025-03-08 01:53:36,845 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04197839789092541
2025-03-08 01:53:45,930 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_38_epoch.pt
2025-03-08 01:53:56,915 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04470956299453974
2025-03-08 01:54:08,512 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04616777878254652
2025-03-08 01:54:19,130 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04487966690212488
2025-03-08 01:54:30,648 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04682702548801899
2025-03-08 01:54:42,512 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04806439254432917
2025-03-08 01:54:51,727 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_39_epoch.pt
2025-03-08 01:55:03,121 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.05092189084738493
2025-03-08 01:55:14,549 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.0452276511117816
2025-03-08 01:55:25,841 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04529706506679455
2025-03-08 01:55:38,001 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.047871465282514694
2025-03-08 01:55:49,113 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04690802066773176
2025-03-08 01:55:57,495 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_40_epoch.pt
2025-03-08 01:56:09,801 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04564714878797531
2025-03-08 01:56:20,809 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04257982261478901
2025-03-08 01:56:32,110 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04047260961184899
2025-03-08 01:56:44,300 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.03988068975508213
2025-03-08 01:56:55,402 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04034944523870945
2025-03-08 01:57:03,506 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_41_epoch.pt
2025-03-08 01:57:14,708 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04258237585425377
2025-03-08 01:57:26,375 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.045825176686048505
2025-03-08 01:57:37,827 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.047668905841807524
2025-03-08 01:57:48,965 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.0474837741535157
2025-03-08 01:58:00,415 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04717537741363049
2025-03-08 01:58:09,890 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_42_epoch.pt
2025-03-08 01:58:22,293 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.050709905251860615
2025-03-08 01:58:32,998 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.05170692149549723
2025-03-08 01:58:44,727 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.05039398948351542
2025-03-08 01:58:56,124 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.0481690183095634
2025-03-08 01:59:07,116 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04752826387435198
2025-03-08 01:59:16,185 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_43_epoch.pt
2025-03-08 01:59:28,285 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04661577720195055
2025-03-08 01:59:40,281 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04761101981624961
2025-03-08 01:59:51,382 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04569525873909394
2025-03-08 02:00:03,109 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04669473207555711
2025-03-08 02:00:14,078 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04955954701453447
2025-03-08 02:00:22,445 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_44_epoch.pt
2025-03-08 02:00:33,996 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04873011369258165
2025-03-08 02:00:45,071 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.046941212844103575
2025-03-08 02:00:56,859 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.0464336010068655
2025-03-08 02:01:08,629 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.046457218546420334
2025-03-08 02:01:19,663 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.047448575183749196
2025-03-08 02:01:28,687 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_45_epoch.pt
2025-03-08 02:01:40,260 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.05432069044560194
2025-03-08 02:01:51,187 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.05064565485343337
2025-03-08 02:02:03,417 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.05187512718141079
2025-03-08 02:02:14,185 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04899192566983402
2025-03-08 02:02:26,236 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.0501938321813941
2025-03-08 02:02:35,118 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_46_epoch.pt
2025-03-08 02:02:46,681 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.05153484996408224
2025-03-08 02:02:58,041 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04470316864550114
2025-03-08 02:03:10,179 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.0440708656112353
2025-03-08 02:03:21,791 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04344911365769803
2025-03-08 02:03:32,618 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.042429880127310755
2025-03-08 02:03:40,910 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_47_epoch.pt
2025-03-08 02:03:52,175 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04797887016087771
2025-03-08 02:04:03,065 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.046200520303100345
2025-03-08 02:04:14,391 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04834409819295009
2025-03-08 02:04:25,918 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04921191070228815
2025-03-08 02:04:37,671 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.048237224869430065
2025-03-08 02:04:46,763 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_48_epoch.pt
2025-03-08 02:04:58,389 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04073727894574404
2025-03-08 02:05:09,479 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.043921094592660664
2025-03-08 02:05:20,866 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04426634440819422
2025-03-08 02:05:32,465 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04234840148128569
2025-03-08 02:05:43,210 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.0423514329791069
2025-03-08 02:05:52,590 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_49_epoch.pt
2025-03-08 02:06:04,072 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04661041367799044
2025-03-08 02:06:16,188 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.048586706835776566
2025-03-08 02:06:26,515 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.046698983572423455
2025-03-08 02:06:37,214 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.045375450449064374
2025-03-08 02:06:49,224 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04420468323677778
2025-03-08 02:06:58,358 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_50_epoch.pt
2025-03-08 02:07:11,125 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04925485257059336
2025-03-08 02:07:22,085 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.05027948495000601
2025-03-08 02:07:33,658 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04926350334038337
2025-03-08 02:07:44,657 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04700828932225704
2025-03-08 02:07:55,990 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.046207585722208025
2025-03-08 02:08:04,150 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_51_epoch.pt
2025-03-08 02:08:15,732 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.0515790706500411
2025-03-08 02:08:27,045 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.05533624218776822
2025-03-08 02:08:38,508 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.054234146401286126
2025-03-08 02:08:49,831 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.05193363697268069
2025-03-08 02:09:02,259 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.05173970618844032
2025-03-08 02:09:11,175 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_52_epoch.pt
2025-03-08 02:09:23,350 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04321443349123001
2025-03-08 02:09:35,316 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.03984129574149847
2025-03-08 02:09:46,814 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04079431917518377
2025-03-08 02:09:57,895 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04085805223323405
2025-03-08 02:10:09,273 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04094020515680313
2025-03-08 02:10:17,929 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_53_epoch.pt
2025-03-08 02:10:29,652 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04305563159286976
2025-03-08 02:10:41,273 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.041246739253401755
2025-03-08 02:10:53,682 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04051924431075653
2025-03-08 02:11:04,980 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.042268676180392505
2025-03-08 02:11:15,626 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04476366090774536
2025-03-08 02:11:23,951 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_54_epoch.pt
2025-03-08 02:11:35,531 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.05065688263624907
2025-03-08 02:11:47,655 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.048523946609348056
2025-03-08 02:11:58,814 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.045349411740899084
2025-03-08 02:12:10,362 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.045831757821142675
2025-03-08 02:12:20,707 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04574137657880783
2025-03-08 02:12:30,033 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_55_epoch.pt
2025-03-08 02:12:41,331 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.051984058171510694
2025-03-08 02:12:52,690 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.045240680426359176
2025-03-08 02:13:04,118 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04657950547834237
2025-03-08 02:13:16,713 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.047307605454698205
2025-03-08 02:13:27,728 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04662957252562046
2025-03-08 02:13:36,257 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_56_epoch.pt
2025-03-08 02:13:47,994 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04919103279709816
2025-03-08 02:13:58,978 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.045267080292105674
2025-03-08 02:14:11,596 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04598923069735368
2025-03-08 02:14:23,393 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04683232340961695
2025-03-08 02:14:34,376 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04622948068380356
2025-03-08 02:14:42,340 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_57_epoch.pt
2025-03-08 02:14:54,303 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04222213823348284
2025-03-08 02:15:05,354 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.03932929007336497
2025-03-08 02:15:16,937 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.040526471820970376
2025-03-08 02:15:27,974 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.043371996469795705
2025-03-08 02:15:40,543 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.043341748878359794
2025-03-08 02:15:48,519 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_58_epoch.pt
2025-03-08 02:16:00,145 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.049065173864364625
2025-03-08 02:16:10,867 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04468585399910808
2025-03-08 02:16:22,313 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.047803271301090716
2025-03-08 02:16:33,444 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.047520886305719615
2025-03-08 02:16:44,990 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04831453888118267
2025-03-08 02:16:54,349 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_59_epoch.pt
2025-03-08 02:17:05,915 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04651388395577669
2025-03-08 02:17:17,095 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04481924757361412
2025-03-08 02:17:29,162 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04674335385362307
2025-03-08 02:17:40,515 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.045889914780855176
2025-03-08 02:17:52,531 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.044673246987164024
2025-03-08 02:18:00,756 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_60_epoch.pt
2025-03-08 02:18:13,493 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.048669675290584566
2025-03-08 02:18:24,721 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04866917945444584
2025-03-08 02:18:36,465 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04825611916681131
2025-03-08 02:18:47,701 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04524622012861073
2025-03-08 02:18:58,979 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04410362143814564
2025-03-08 02:19:07,333 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_61_epoch.pt
2025-03-08 02:19:19,084 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.050628181248903274
2025-03-08 02:19:29,846 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.046076817102730275
2025-03-08 02:19:42,097 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04346420470625162
2025-03-08 02:19:53,822 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04425876662135124
2025-03-08 02:20:04,667 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.043727779738605026
2025-03-08 02:20:13,771 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_62_epoch.pt
2025-03-08 02:20:26,336 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04403365295380354
2025-03-08 02:20:37,376 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04382723683491349
2025-03-08 02:20:48,401 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04398975644260645
2025-03-08 02:20:59,914 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.044634921625256535
2025-03-08 02:21:11,087 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04516493684798479
2025-03-08 02:21:19,876 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_63_epoch.pt
2025-03-08 02:21:31,760 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04775729980319739
2025-03-08 02:21:42,726 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04617053499445319
2025-03-08 02:21:53,474 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.0454633155465126
2025-03-08 02:22:04,286 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.045690472889691594
2025-03-08 02:22:17,230 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04647722887247801
2025-03-08 02:22:25,916 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_64_epoch.pt
2025-03-08 02:22:37,597 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03834301378577948
2025-03-08 02:22:49,649 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04004914984107018
2025-03-08 02:23:00,613 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04479964000483354
2025-03-08 02:23:11,533 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04352449658326805
2025-03-08 02:23:22,566 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04479994691163301
2025-03-08 02:23:32,111 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_65_epoch.pt
2025-03-08 02:23:44,129 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.046580847911536695
2025-03-08 02:23:55,910 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04760879972949624
2025-03-08 02:24:07,153 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.050788500010967255
2025-03-08 02:24:18,755 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04874193213880062
2025-03-08 02:24:29,727 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.048032393790781495
2025-03-08 02:24:38,149 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_66_epoch.pt
2025-03-08 02:24:49,908 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.0434197086840868
2025-03-08 02:25:01,340 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04202906046062708
2025-03-08 02:25:12,517 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04113338066885869
2025-03-08 02:25:24,183 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04080046201124787
2025-03-08 02:25:36,083 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.038989589482545856
2025-03-08 02:25:43,811 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_67_epoch.pt
2025-03-08 02:25:55,657 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03792709145694971
2025-03-08 02:26:06,805 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04101538710296154
2025-03-08 02:26:18,463 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.043441656852761905
2025-03-08 02:26:29,703 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04344179088249803
2025-03-08 02:26:40,676 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04377996647357941
2025-03-08 02:26:49,317 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_68_epoch.pt
2025-03-08 02:27:00,953 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04735178910195827
2025-03-08 02:27:12,724 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04643712980672717
2025-03-08 02:27:24,455 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.046306801637013756
2025-03-08 02:27:35,280 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04644975997507572
2025-03-08 02:27:45,983 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04556686889380217
2025-03-08 02:27:55,239 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_69_epoch.pt
2025-03-08 02:28:06,280 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04490483727306128
2025-03-08 02:28:18,544 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04343575900420547
2025-03-08 02:28:30,383 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04314781257261833
2025-03-08 02:28:42,137 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04294630984775722
2025-03-08 02:28:53,190 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.043682133436203
2025-03-08 02:29:01,394 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_70_epoch.pt
2025-03-08 02:29:13,153 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04436898373067379
2025-03-08 02:29:24,115 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04418818423524499
2025-03-08 02:29:35,592 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.045156392008066176
2025-03-08 02:29:47,288 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04250479025766254
2025-03-08 02:29:58,912 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.043779097899794575
2025-03-08 02:30:07,277 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_71_epoch.pt
2025-03-08 02:30:19,226 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03563783958554268
2025-03-08 02:30:30,670 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04070950303226709
2025-03-08 02:30:42,255 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04009182933717966
2025-03-08 02:30:53,540 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.040556981060653925
2025-03-08 02:31:05,301 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04190754846483469
2025-03-08 02:31:13,375 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_72_epoch.pt
2025-03-08 02:31:25,853 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04072652716189623
2025-03-08 02:31:37,131 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.0429296007193625
2025-03-08 02:31:48,465 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04096228583405415
2025-03-08 02:32:00,212 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04177544360980392
2025-03-08 02:32:10,636 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04141312758624554
2025-03-08 02:32:19,915 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_73_epoch.pt
2025-03-08 02:32:31,506 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.046423090435564515
2025-03-08 02:32:43,423 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04752965144813061
2025-03-08 02:32:54,223 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04395672255506118
2025-03-08 02:33:06,649 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04338234225288033
2025-03-08 02:33:18,443 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.044171748608350755
2025-03-08 02:33:26,527 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_74_epoch.pt
2025-03-08 02:33:38,423 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.033622218891978264
2025-03-08 02:33:49,721 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.03962581310421229
2025-03-08 02:34:00,996 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.040639676675200465
2025-03-08 02:34:12,934 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.041155339358374475
2025-03-08 02:34:24,090 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04170141126960516
2025-03-08 02:34:32,201 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_75_epoch.pt
2025-03-08 02:34:45,199 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04128651309758425
2025-03-08 02:34:56,503 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04057126346975565
2025-03-08 02:35:07,600 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.043336058494945366
2025-03-08 02:35:17,704 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04279075462371111
2025-03-08 02:35:30,019 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04371302728354931
2025-03-08 02:35:39,040 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_76_epoch.pt
2025-03-08 02:35:49,948 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.028160765022039413
2025-03-08 02:36:01,225 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.03817784445360303
2025-03-08 02:36:13,184 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.040135144305725894
2025-03-08 02:36:23,929 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04021733521483838
2025-03-08 02:36:35,586 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04166042923927307
2025-03-08 02:36:45,166 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_77_epoch.pt
2025-03-08 02:36:56,804 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.041478628255426885
2025-03-08 02:37:08,093 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04265227118507028
2025-03-08 02:37:18,502 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.0404484678308169
2025-03-08 02:37:30,162 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.040620500463992355
2025-03-08 02:37:41,870 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04073126204311848
2025-03-08 02:37:51,539 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_78_epoch.pt
2025-03-08 02:38:02,924 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03998671978712082
2025-03-08 02:38:13,589 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04433340849354863
2025-03-08 02:38:25,926 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.041740800042947133
2025-03-08 02:38:37,107 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04178208653815091
2025-03-08 02:38:48,792 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.041972636483609674
2025-03-08 02:38:57,601 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_79_epoch.pt
2025-03-08 02:39:09,905 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04904335599392653
2025-03-08 02:39:22,491 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.046612158939242365
2025-03-08 02:39:33,410 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.0484473921234409
2025-03-08 02:39:44,014 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04752272474579513
2025-03-08 02:39:55,520 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.047423723801970484
2025-03-08 02:40:04,201 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_80_epoch.pt
2025-03-08 02:40:16,339 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04269930768758059
2025-03-08 02:40:26,616 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.03734676595777273
2025-03-08 02:40:39,143 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04046095388631026
2025-03-08 02:40:49,971 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.042445434546098115
2025-03-08 02:41:01,183 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.042388498090207574
2025-03-08 02:41:09,932 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_81_epoch.pt
2025-03-08 02:41:21,512 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04758257109671831
2025-03-08 02:41:33,012 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04828835939988494
2025-03-08 02:41:44,619 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04525359025845925
2025-03-08 02:41:55,596 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04552862526848912
2025-03-08 02:42:07,483 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.044994287833571435
2025-03-08 02:42:15,838 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_82_epoch.pt
2025-03-08 02:42:27,142 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03941242966800928
2025-03-08 02:42:38,662 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04576466403901577
2025-03-08 02:42:50,399 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04584493091950814
2025-03-08 02:43:01,435 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.042522934852167966
2025-03-08 02:43:13,049 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04270145776867867
2025-03-08 02:43:22,037 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_83_epoch.pt
2025-03-08 02:43:34,318 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.039820371046662334
2025-03-08 02:43:44,349 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04269074566662311
2025-03-08 02:43:56,328 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.044485096211234726
2025-03-08 02:44:07,651 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04517409057356417
2025-03-08 02:44:18,389 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.045542327925562856
2025-03-08 02:44:27,869 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_84_epoch.pt
2025-03-08 02:44:39,388 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03752677448093891
2025-03-08 02:44:50,507 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.03916343806311488
2025-03-08 02:45:01,550 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04199106823652983
2025-03-08 02:45:12,701 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04307378225028515
2025-03-08 02:45:25,218 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.041807051606476305
2025-03-08 02:45:33,902 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_85_epoch.pt
2025-03-08 02:45:45,601 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04300634615123272
2025-03-08 02:45:57,372 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04349551517516374
2025-03-08 02:46:08,336 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04027363169938326
2025-03-08 02:46:20,235 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.041008772375062105
2025-03-08 02:46:31,894 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.042713774912059306
2025-03-08 02:46:39,891 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_86_epoch.pt
2025-03-08 02:46:52,401 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03825739812105894
2025-03-08 02:47:03,757 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.0394408587552607
2025-03-08 02:47:14,689 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.042393198224405446
2025-03-08 02:47:25,719 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.0433412479609251
2025-03-08 02:47:36,711 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04228172179311514
2025-03-08 02:47:46,334 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_87_epoch.pt
2025-03-08 02:47:58,667 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.040318647548556324
2025-03-08 02:48:10,696 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04603404954075813
2025-03-08 02:48:21,935 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04665848496059577
2025-03-08 02:48:32,451 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04701754463836551
2025-03-08 02:48:44,178 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04585337309539318
2025-03-08 02:48:52,452 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_88_epoch.pt
2025-03-08 02:49:03,451 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04546771001070738
2025-03-08 02:49:15,210 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04607975082471967
2025-03-08 02:49:26,811 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04657120825101932
2025-03-08 02:49:38,336 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.046292284522205594
2025-03-08 02:49:49,113 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04710667635500431
2025-03-08 02:49:58,704 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_89_epoch.pt
2025-03-08 02:50:09,872 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.05498236011713743
2025-03-08 02:50:21,410 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04903469208627939
2025-03-08 02:50:33,384 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04597127694636583
2025-03-08 02:50:45,946 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04564029694534838
2025-03-08 02:50:56,932 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04459800995886326
2025-03-08 02:51:05,029 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_90_epoch.pt
2025-03-08 02:51:17,119 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.041281757541000844
2025-03-08 02:51:28,426 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.041331794783473014
2025-03-08 02:51:40,002 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04311653996507327
2025-03-08 02:51:51,495 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04268162796273828
2025-03-08 02:52:03,043 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04283765305578709
2025-03-08 02:52:11,098 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_91_epoch.pt
2025-03-08 02:52:23,838 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.046698320209980014
2025-03-08 02:52:35,312 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.044488361850380895
2025-03-08 02:52:46,514 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.046772603541612626
2025-03-08 02:52:57,556 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04495217781513929
2025-03-08 02:53:09,374 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04493921970576048
2025-03-08 02:53:17,425 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_92_epoch.pt
2025-03-08 02:53:30,358 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.037356306649744514
2025-03-08 02:53:42,663 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04093701209872961
2025-03-08 02:53:53,913 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04302401673048735
2025-03-08 02:54:03,735 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.044294445076957345
2025-03-08 02:54:15,050 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04368475335091353
2025-03-08 02:54:23,851 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_93_epoch.pt
2025-03-08 02:54:35,455 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.041982161588966845
2025-03-08 02:54:47,119 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04788966344669461
2025-03-08 02:54:57,835 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.047010731833676496
2025-03-08 02:55:09,197 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04557957214303315
2025-03-08 02:55:20,775 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04634062316268683
2025-03-08 02:55:29,583 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_94_epoch.pt
2025-03-08 02:55:42,547 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04899272717535496
2025-03-08 02:55:54,275 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.047653363775461914
2025-03-08 02:56:05,306 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.046121244045595325
2025-03-08 02:56:16,441 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04581766756251454
2025-03-08 02:56:28,035 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04398213572055101
2025-03-08 02:56:36,037 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_95_epoch.pt
2025-03-08 02:56:48,357 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.05068134646862745
2025-03-08 02:57:00,028 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.046864854004234076
2025-03-08 02:57:11,353 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04586193190266689
2025-03-08 02:57:22,324 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04455801722593605
2025-03-08 02:57:34,034 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.044445190973579886
2025-03-08 02:57:42,334 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_96_epoch.pt
2025-03-08 02:57:54,984 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.047966558374464514
2025-03-08 02:58:05,550 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04563723059371114
2025-03-08 02:58:17,680 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.044414469711482524
2025-03-08 02:58:28,685 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04484330528415739
2025-03-08 02:58:39,367 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04490166713297367
2025-03-08 02:58:48,074 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_97_epoch.pt
2025-03-08 02:58:59,711 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.046796085387468336
2025-03-08 02:59:11,299 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04359571486711502
2025-03-08 02:59:23,060 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04098676152527332
2025-03-08 02:59:34,215 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.040599261224269864
2025-03-08 02:59:46,386 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04231990281492472
2025-03-08 02:59:54,089 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_98_epoch.pt
2025-03-08 03:00:07,767 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04162546254694462
2025-03-08 03:00:19,312 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04195577027276158
2025-03-08 03:00:30,528 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04317512760559718
2025-03-08 03:00:41,464 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04416388848796487
2025-03-08 03:00:52,326 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04457626325637102
2025-03-08 03:01:00,632 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_99_epoch.pt
2025-03-08 03:01:12,784 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03873577810823917
2025-03-08 03:01:24,041 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04198024122044444
2025-03-08 03:01:35,184 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.03899534450223049
2025-03-08 03:01:47,114 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.039302487904205916
2025-03-08 03:01:58,706 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04083259052038193
2025-03-08 03:02:07,227 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_100_epoch.pt
2025-03-08 03:02:19,031 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04203929085284472
2025-03-08 03:02:30,158 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04390598565340042
2025-03-08 03:02:40,964 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.042433493882417676
2025-03-08 03:02:52,367 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04264260013587773
2025-03-08 03:03:03,683 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04346977339684963
2025-03-08 03:03:12,920 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_101_epoch.pt
2025-03-08 03:03:25,367 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04222967084497213
2025-03-08 03:03:35,976 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.03964971324428916
2025-03-08 03:03:47,320 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.043250575462977094
2025-03-08 03:03:59,312 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04468302128836513
2025-03-08 03:04:09,676 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04377380581200123
2025-03-08 03:04:18,912 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_102_epoch.pt
2025-03-08 03:04:29,531 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.0355388855189085
2025-03-08 03:04:41,096 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04001649748533964
2025-03-08 03:04:53,609 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04212819959968329
2025-03-08 03:05:04,442 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04458048579283059
2025-03-08 03:05:16,665 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04454139620065689
2025-03-08 03:05:24,764 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_103_epoch.pt
2025-03-08 03:05:36,802 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04079313345253468
2025-03-08 03:05:48,189 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.0384443923458457
2025-03-08 03:05:59,282 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.040109642955164115
2025-03-08 03:06:10,647 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.0421556680649519
2025-03-08 03:06:22,060 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.040775255419313906
2025-03-08 03:06:30,633 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_104_epoch.pt
2025-03-08 03:06:41,668 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04285112492740154
2025-03-08 03:06:53,959 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04754757430404424
2025-03-08 03:07:04,185 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04398750132570664
2025-03-08 03:07:15,669 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.043808818869292734
2025-03-08 03:07:27,700 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.042054198145866396
2025-03-08 03:07:36,963 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_105_epoch.pt
2025-03-08 03:07:48,632 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04679525524377823
2025-03-08 03:07:58,844 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04493570934981108
2025-03-08 03:08:10,976 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.044457832748691244
2025-03-08 03:08:22,588 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.043160413717851044
2025-03-08 03:08:34,603 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04380425323545933
2025-03-08 03:08:43,069 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_106_epoch.pt
2025-03-08 03:08:54,991 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04294148713350296
2025-03-08 03:09:06,364 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.03819837098941207
2025-03-08 03:09:17,779 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.040737316285570466
2025-03-08 03:09:29,272 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.042738660778850315
2025-03-08 03:09:40,110 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04139184785634279
2025-03-08 03:09:49,456 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_107_epoch.pt
2025-03-08 03:10:02,260 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.042955117225646974
2025-03-08 03:10:12,465 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.043956457488238815
2025-03-08 03:10:23,864 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04283852831770976
2025-03-08 03:10:34,789 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04506926969625056
2025-03-08 03:10:45,739 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04647749426215887
2025-03-08 03:10:55,213 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_108_epoch.pt
2025-03-08 03:11:07,076 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04302611902356148
2025-03-08 03:11:19,015 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.041796015929430726
2025-03-08 03:11:30,640 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04394304464260737
2025-03-08 03:11:41,813 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04360419979318977
2025-03-08 03:11:53,349 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04331502878665924
2025-03-08 03:12:01,844 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_109_epoch.pt
2025-03-08 03:12:14,196 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03624770008027554
2025-03-08 03:12:25,894 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.03794054388999939
2025-03-08 03:12:37,135 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.039708656556904316
2025-03-08 03:12:48,700 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.040915331551805136
2025-03-08 03:12:59,912 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04106833479553461
2025-03-08 03:13:07,365 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_110_epoch.pt
2025-03-08 03:13:18,966 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03526579856872559
2025-03-08 03:13:30,476 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04261010874062777
2025-03-08 03:13:41,578 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.0441510151575009
2025-03-08 03:13:52,921 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04345695417374373
2025-03-08 03:14:05,176 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04412343369424343
2025-03-08 03:14:13,826 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_111_epoch.pt
2025-03-08 03:14:26,440 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.05059658259153366
2025-03-08 03:14:37,218 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04321323074400425
2025-03-08 03:14:48,244 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04108772755910953
2025-03-08 03:14:59,755 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.039882990950718525
2025-03-08 03:15:10,927 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04046152512729168
2025-03-08 03:15:18,893 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_112_epoch.pt
2025-03-08 03:15:32,462 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04518594551831484
2025-03-08 03:15:43,457 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04675255568698049
2025-03-08 03:15:54,667 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04667925720413526
2025-03-08 03:16:06,332 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04596313835121691
2025-03-08 03:16:17,759 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04481860706210136
2025-03-08 03:16:26,315 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_113_epoch.pt
2025-03-08 03:16:38,305 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03497354552149773
2025-03-08 03:16:49,303 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04040588976815343
2025-03-08 03:17:00,842 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.03981102861464023
2025-03-08 03:17:13,262 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.040286571625620125
2025-03-08 03:17:24,623 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.03994030124694109
2025-03-08 03:17:32,826 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_114_epoch.pt
2025-03-08 03:17:44,914 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04770848251879215
2025-03-08 03:17:56,425 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04844303909689188
2025-03-08 03:18:07,471 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04476881129046281
2025-03-08 03:18:19,202 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04390833063051105
2025-03-08 03:18:30,250 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04389129277318716
2025-03-08 03:18:38,853 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_115_epoch.pt
2025-03-08 03:18:51,264 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03893543504178524
2025-03-08 03:19:03,127 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04406121343374252
2025-03-08 03:19:14,650 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04447294181833664
2025-03-08 03:19:26,478 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04409685521386564
2025-03-08 03:19:36,794 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.043881951428949836
2025-03-08 03:19:45,185 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_116_epoch.pt
2025-03-08 03:19:56,655 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04233325593173504
2025-03-08 03:20:08,643 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04175584178417921
2025-03-08 03:20:19,924 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.03904384762048721
2025-03-08 03:20:30,922 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.039578039264306426
2025-03-08 03:20:42,659 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.03937324809283018
2025-03-08 03:20:50,635 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_117_epoch.pt
2025-03-08 03:21:02,276 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04724818557500839
2025-03-08 03:21:13,817 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04391260491684079
2025-03-08 03:21:24,943 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.045543160972495875
2025-03-08 03:21:35,497 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04655515479855239
2025-03-08 03:21:47,554 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04652579621970653
2025-03-08 03:21:56,780 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_118_epoch.pt
2025-03-08 03:22:08,602 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.036691167242825035
2025-03-08 03:22:20,284 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04130795281380415
2025-03-08 03:22:31,793 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04175587080419064
2025-03-08 03:22:42,895 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.0416914480458945
2025-03-08 03:22:54,521 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.0408860534131527
2025-03-08 03:23:02,546 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_119_epoch.pt
2025-03-08 03:23:13,983 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04703670900315046
2025-03-08 03:23:25,206 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04830929052084684
2025-03-08 03:23:36,789 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04502218826363484
2025-03-08 03:23:47,803 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04389726330526173
2025-03-08 03:23:58,797 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04506753554940224
2025-03-08 03:24:08,834 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_120_epoch.pt
2025-03-08 03:24:20,908 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04045190151780844
2025-03-08 03:24:32,565 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.03563601726666093
2025-03-08 03:24:42,757 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.03597301156570514
2025-03-08 03:24:54,744 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.039062493061646816
2025-03-08 03:25:05,589 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.03843547581881285
2025-03-08 03:25:15,140 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_121_epoch.pt
2025-03-08 03:25:26,404 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04238061960786581
2025-03-08 03:25:37,198 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04136529268696904
2025-03-08 03:25:49,152 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.040070476792752746
2025-03-08 03:26:00,266 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.03996841003187001
2025-03-08 03:26:12,138 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.042013991832733154
2025-03-08 03:26:20,951 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_122_epoch.pt
2025-03-08 03:26:32,969 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04860309507697821
2025-03-08 03:26:43,814 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04431667787954211
2025-03-08 03:26:55,001 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04315761305391788
2025-03-08 03:27:06,367 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04328839200548828
2025-03-08 03:27:17,796 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.043361602425575255
2025-03-08 03:27:26,737 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_123_epoch.pt
2025-03-08 03:27:38,423 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04559838477522135
2025-03-08 03:27:49,060 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04243176084011793
2025-03-08 03:28:01,264 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.0438435256605347
2025-03-08 03:28:12,548 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04402477937750518
2025-03-08 03:28:23,492 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04363314189761877
2025-03-08 03:28:32,331 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_124_epoch.pt
2025-03-08 03:28:43,739 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04331640381366014
2025-03-08 03:28:55,543 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04044866908341646
2025-03-08 03:29:07,315 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04353393434236447
2025-03-08 03:29:17,688 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.044739307388663295
2025-03-08 03:29:29,002 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04307240803539753
2025-03-08 03:29:38,351 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_125_epoch.pt
2025-03-08 03:29:49,274 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03999540325254202
2025-03-08 03:30:02,442 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04036072166636586
2025-03-08 03:30:14,275 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04229622587561607
2025-03-08 03:30:24,787 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04216657355427742
2025-03-08 03:30:37,018 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.043951401956379414
2025-03-08 03:30:44,969 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_126_epoch.pt
2025-03-08 03:30:56,574 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.046498729772865774
2025-03-08 03:31:07,813 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04284052116796375
2025-03-08 03:31:17,799 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04325945573548476
2025-03-08 03:31:30,010 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04261288091540336
2025-03-08 03:31:41,499 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04148314853012562
2025-03-08 03:31:50,827 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_127_epoch.pt
2025-03-08 03:32:02,575 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03626020271331072
2025-03-08 03:32:13,486 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.03995098741725087
2025-03-08 03:32:24,641 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.0398703358694911
2025-03-08 03:32:36,677 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04160363857634366
2025-03-08 03:32:48,852 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04150779958814382
2025-03-08 03:32:57,227 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_128_epoch.pt
2025-03-08 03:33:09,073 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04336930245161057
2025-03-08 03:33:20,888 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04289675591513514
2025-03-08 03:33:32,060 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04503248785932859
2025-03-08 03:33:42,983 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04428985670208931
2025-03-08 03:33:55,006 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.043697360813617706
2025-03-08 03:34:03,245 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_129_epoch.pt
2025-03-08 03:34:15,412 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03759377621114254
2025-03-08 03:34:26,006 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04289414077997208
2025-03-08 03:34:37,262 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04080402525762717
2025-03-08 03:34:49,514 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.039009958701208235
2025-03-08 03:35:00,882 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.03897089503705502
2025-03-08 03:35:10,401 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_130_epoch.pt
2025-03-08 03:35:22,207 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03761617012321949
2025-03-08 03:35:33,930 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04217661639675498
2025-03-08 03:35:45,288 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04196232281625271
2025-03-08 03:35:56,492 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.041885059773921963
2025-03-08 03:36:08,175 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.043464542880654336
2025-03-08 03:36:16,895 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_131_epoch.pt
2025-03-08 03:36:28,523 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.041005915366113185
2025-03-08 03:36:39,372 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04489375537261367
2025-03-08 03:36:49,886 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04295050653318564
2025-03-08 03:37:02,148 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04319018680602312
2025-03-08 03:37:13,939 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.042598187543451786
2025-03-08 03:37:22,871 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_132_epoch.pt
2025-03-08 03:37:34,858 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03673101749271154
2025-03-08 03:37:46,564 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04008633121848106
2025-03-08 03:37:58,729 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.0402735403055946
2025-03-08 03:38:09,771 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04005453921854496
2025-03-08 03:38:21,289 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04073099357634783
2025-03-08 03:38:29,173 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_133_epoch.pt
2025-03-08 03:38:41,433 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.052256592474877836
2025-03-08 03:38:53,176 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.046330185420811174
2025-03-08 03:39:03,118 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04519456966469685
2025-03-08 03:39:15,386 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.0448463315051049
2025-03-08 03:39:26,726 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04480032301694155
2025-03-08 03:39:35,337 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_134_epoch.pt
2025-03-08 03:39:47,468 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.053012855872511866
2025-03-08 03:40:00,102 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04743632901459932
2025-03-08 03:40:11,700 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04520969259242217
2025-03-08 03:40:22,491 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.044456203989684584
2025-03-08 03:40:33,785 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04421580485254526
2025-03-08 03:40:41,978 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_135_epoch.pt
2025-03-08 03:40:53,678 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03791036881506443
2025-03-08 03:41:04,678 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.037696552593261004
2025-03-08 03:41:16,133 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.0388618541508913
2025-03-08 03:41:27,992 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04184825330041349
2025-03-08 03:41:39,641 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.041414951264858245
2025-03-08 03:41:47,803 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_136_epoch.pt
2025-03-08 03:41:59,491 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04562813963741064
2025-03-08 03:42:10,258 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.0451847842335701
2025-03-08 03:42:21,862 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.043332457939783735
2025-03-08 03:42:33,671 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04207405922003091
2025-03-08 03:42:44,950 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.041487504065036776
2025-03-08 03:42:53,841 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_137_epoch.pt
2025-03-08 03:43:06,439 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.037998936735093594
2025-03-08 03:43:17,360 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04150678059086203
2025-03-08 03:43:28,252 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04081325548390547
2025-03-08 03:43:39,293 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04104844626039267
2025-03-08 03:43:51,287 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04061171609908342
2025-03-08 03:44:00,141 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_138_epoch.pt
2025-03-08 03:44:11,392 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.0496927759051323
2025-03-08 03:44:23,194 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04976254116743803
2025-03-08 03:44:34,468 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.045504008196294306
2025-03-08 03:44:46,179 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04421651276759803
2025-03-08 03:44:56,688 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04310325288772583
2025-03-08 03:45:05,651 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_139_epoch.pt
2025-03-08 03:45:17,579 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03810685306787491
2025-03-08 03:45:28,023 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04193465599790216
2025-03-08 03:45:39,622 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04244104320804278
2025-03-08 03:45:50,751 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.042481056954711674
2025-03-08 03:46:02,200 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.040440979167819024
2025-03-08 03:46:11,480 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_140_epoch.pt
2025-03-08 03:46:23,431 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.047655606381595134
2025-03-08 03:46:34,341 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04725612282752991
2025-03-08 03:46:46,245 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04439474238703648
2025-03-08 03:46:58,711 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04280195816420019
2025-03-08 03:47:09,040 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.0417439978569746
2025-03-08 03:47:17,918 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_141_epoch.pt
2025-03-08 03:47:29,286 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04041929610073566
2025-03-08 03:47:39,552 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04321274515241384
2025-03-08 03:47:52,044 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04152871821075678
2025-03-08 03:48:03,623 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.041355987135320904
2025-03-08 03:48:14,995 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.0414943263605237
2025-03-08 03:48:23,344 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_142_epoch.pt
2025-03-08 03:48:35,068 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04849352151155472
2025-03-08 03:48:46,381 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04663113243877888
2025-03-08 03:48:57,251 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.047948919584353766
2025-03-08 03:49:09,014 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04627862576395273
2025-03-08 03:49:20,253 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04484588957577944
2025-03-08 03:49:29,205 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_143_epoch.pt
2025-03-08 03:49:41,171 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04149131137877703
2025-03-08 03:49:52,490 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.0388335788063705
2025-03-08 03:50:03,659 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.03916772068788608
2025-03-08 03:50:15,395 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04194949042983353
2025-03-08 03:50:27,701 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04241670835018158
2025-03-08 03:50:35,865 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_144_epoch.pt
2025-03-08 03:50:46,969 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03862154807895422
2025-03-08 03:50:57,605 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.044189059995114806
2025-03-08 03:51:09,668 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04540481310337782
2025-03-08 03:51:21,132 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.0436944377887994
2025-03-08 03:51:33,376 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04414510789513588
2025-03-08 03:51:41,939 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_145_epoch.pt
2025-03-08 03:51:53,089 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.044362489469349384
2025-03-08 03:52:04,305 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.0414592501334846
2025-03-08 03:52:16,607 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.042295664077003796
2025-03-08 03:52:27,427 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04353641931898892
2025-03-08 03:52:39,270 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.043654100179672244
2025-03-08 03:52:48,121 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_146_epoch.pt
2025-03-08 03:52:59,079 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03606595553457737
2025-03-08 03:53:10,877 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.038740609139204026
2025-03-08 03:53:23,119 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.039583715883394086
2025-03-08 03:53:33,538 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.03883190788328648
2025-03-08 03:53:44,614 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.03842930931597948
2025-03-08 03:53:53,859 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_147_epoch.pt
2025-03-08 03:54:05,953 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04432140350341797
2025-03-08 03:54:16,590 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04195842245593667
2025-03-08 03:54:27,906 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.039423510258396466
2025-03-08 03:54:40,010 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.041923003904521465
2025-03-08 03:54:50,560 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.041772621169686316
2025-03-08 03:54:59,177 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_148_epoch.pt
2025-03-08 03:55:12,282 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.044482250846922394
2025-03-08 03:55:23,260 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04759081466123462
2025-03-08 03:55:34,408 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.045142944492399695
2025-03-08 03:55:45,353 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04363946943543851
2025-03-08 03:55:57,246 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.0420697685778141
2025-03-08 03:56:06,553 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_149_epoch.pt
2025-03-08 03:56:19,309 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04180807959288359
2025-03-08 03:56:29,847 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04052466252818704
2025-03-08 03:56:41,102 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.040258533308903374
2025-03-08 03:56:52,814 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04114243160001933
2025-03-08 03:57:03,931 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04167937644571066
2025-03-08 03:57:12,872 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_150_epoch.pt
2025-03-08 03:57:25,265 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.040969812199473384
2025-03-08 03:57:36,448 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04089406872168183
2025-03-08 03:57:46,485 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04076578407237927
2025-03-08 03:57:58,371 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04124046138487756
2025-03-08 03:58:10,203 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.042758816950023176
2025-03-08 03:58:19,065 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_151_epoch.pt
2025-03-08 03:58:31,089 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04114517878741026
2025-03-08 03:58:41,736 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.0423813945055008
2025-03-08 03:58:53,353 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.044869285523891446
2025-03-08 03:59:05,097 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04293111930601299
2025-03-08 03:59:16,537 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04449867229908705
2025-03-08 03:59:25,786 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_152_epoch.pt
2025-03-08 03:59:37,456 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04382087405771017
2025-03-08 03:59:48,533 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04383098000660539
2025-03-08 04:00:00,446 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.041932745253046355
2025-03-08 04:00:11,850 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04120075429789722
2025-03-08 04:00:22,959 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04095968899130821
2025-03-08 04:00:31,592 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_153_epoch.pt
2025-03-08 04:00:43,473 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03851520232856274
2025-03-08 04:00:55,466 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.03763507975265384
2025-03-08 04:01:06,067 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.03781523829946915
2025-03-08 04:01:17,613 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.03959620085544884
2025-03-08 04:01:29,043 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.03956317299604416
2025-03-08 04:01:38,048 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_154_epoch.pt
2025-03-08 04:01:50,163 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04249896217137575
2025-03-08 04:02:01,368 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.042009928841143844
2025-03-08 04:02:12,950 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.042601120906571546
2025-03-08 04:02:24,636 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04130687055177987
2025-03-08 04:02:35,384 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04253177866339684
2025-03-08 04:02:44,614 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_155_epoch.pt
2025-03-08 04:02:56,829 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03494944375008344
2025-03-08 04:03:08,445 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.035029096640646455
2025-03-08 04:03:18,768 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.038137955678006014
2025-03-08 04:03:31,702 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.03865477969869971
2025-03-08 04:03:43,140 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.040651005774736404
2025-03-08 04:03:50,850 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_156_epoch.pt
2025-03-08 04:04:03,262 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.045398783348500726
2025-03-08 04:04:15,385 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.043246264532208444
2025-03-08 04:04:27,084 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04492563998947541
2025-03-08 04:04:37,404 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.045331380125135184
2025-03-08 04:04:49,167 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04489361122250557
2025-03-08 04:04:56,912 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_157_epoch.pt
2025-03-08 04:05:09,458 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.042779849730432036
2025-03-08 04:05:20,892 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04212336681783199
2025-03-08 04:05:32,343 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04156323393185934
2025-03-08 04:05:44,164 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.043140040887519715
2025-03-08 04:05:55,471 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04240424407273531
2025-03-08 04:06:04,366 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_158_epoch.pt
2025-03-08 04:06:16,062 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.040228472612798216
2025-03-08 04:06:26,175 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.03894271658733487
2025-03-08 04:06:37,382 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04092441439628601
2025-03-08 04:06:48,601 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.03895578671246767
2025-03-08 04:07:01,274 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.03956984893977642
2025-03-08 04:07:10,483 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_159_epoch.pt
2025-03-08 04:07:22,749 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03824560683220625
2025-03-08 04:07:34,700 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.03984044693410396
2025-03-08 04:07:46,574 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.03731792541841666
2025-03-08 04:07:57,299 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.03833538126200438
2025-03-08 04:08:08,271 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.039305069461464885
2025-03-08 04:08:16,112 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_160_epoch.pt
2025-03-08 04:08:28,463 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04886613752692938
2025-03-08 04:08:39,557 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.044110184777528046
2025-03-08 04:08:50,739 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04348888988296191
2025-03-08 04:09:02,024 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.043363977838307616
2025-03-08 04:09:13,710 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04348495210707188
2025-03-08 04:09:22,350 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_161_epoch.pt
2025-03-08 04:09:34,483 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03854524318128824
2025-03-08 04:09:45,284 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.038500920701771975
2025-03-08 04:09:56,544 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.03720566641539335
2025-03-08 04:10:07,871 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.03781588049605489
2025-03-08 04:10:19,009 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.0400353177562356
2025-03-08 04:10:28,022 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_162_epoch.pt
2025-03-08 04:10:40,674 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04978885885328054
2025-03-08 04:10:51,145 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04385633863508701
2025-03-08 04:11:02,479 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04480427290002505
2025-03-08 04:11:13,885 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04550564811564982
2025-03-08 04:11:25,051 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.045440964162349703
2025-03-08 04:11:33,806 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_163_epoch.pt
2025-03-08 04:11:46,077 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04362649321556091
2025-03-08 04:11:57,549 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.040890762209892274
2025-03-08 04:12:09,002 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04024159791568915
2025-03-08 04:12:20,014 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04055548582226038
2025-03-08 04:12:31,470 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.03987814638763666
2025-03-08 04:12:39,818 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_164_epoch.pt
2025-03-08 04:12:52,313 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04166657354682684
2025-03-08 04:13:03,901 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04545390654355288
2025-03-08 04:13:14,969 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.042763639874756336
2025-03-08 04:13:25,886 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.043210516711696983
2025-03-08 04:13:37,399 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04418022509664297
2025-03-08 04:13:46,450 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_165_epoch.pt
2025-03-08 04:13:57,382 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03925635691732168
2025-03-08 04:14:09,608 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.039810073487460615
2025-03-08 04:14:20,813 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04056096643209457
2025-03-08 04:14:31,957 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04247927935793996
2025-03-08 04:14:43,012 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.043903869666159154
2025-03-08 04:14:52,729 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_166_epoch.pt
2025-03-08 04:15:04,649 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03975066099315882
2025-03-08 04:15:16,082 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.042099723033607005
2025-03-08 04:15:27,721 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.045196396360794706
2025-03-08 04:15:39,493 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04437903059646487
2025-03-08 04:15:50,298 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.0437247007638216
2025-03-08 04:15:58,859 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_167_epoch.pt
2025-03-08 04:16:11,832 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04579408708959818
2025-03-08 04:16:22,481 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.044474779646843675
2025-03-08 04:16:33,754 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04367749931911628
2025-03-08 04:16:45,447 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.042197977332398294
2025-03-08 04:16:56,752 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04192584064602852
2025-03-08 04:17:05,462 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_168_epoch.pt
2025-03-08 04:17:16,488 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03686453234404326
2025-03-08 04:17:27,678 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.03691395981237292
2025-03-08 04:17:39,682 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.038603521138429644
2025-03-08 04:17:50,566 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.040239353934302925
2025-03-08 04:18:02,860 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.0388594159334898
2025-03-08 04:18:11,736 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_169_epoch.pt
2025-03-08 04:18:23,546 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04500859450548887
2025-03-08 04:18:33,624 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04243973402306438
2025-03-08 04:18:44,693 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04425220319380363
2025-03-08 04:18:57,630 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.043997905161231754
2025-03-08 04:19:09,024 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.0427983929887414
2025-03-08 04:19:17,745 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_170_epoch.pt
2025-03-08 04:19:29,185 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03509511958807707
2025-03-08 04:19:40,116 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.03913411561399698
2025-03-08 04:19:50,829 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.038946924743553005
2025-03-08 04:20:02,990 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.03886880703270435
2025-03-08 04:20:14,252 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.03844340122491121
2025-03-08 04:20:23,391 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_171_epoch.pt
2025-03-08 04:20:35,866 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.039675821736454966
2025-03-08 04:20:47,754 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04195388341322541
2025-03-08 04:20:57,991 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.03982289844503006
2025-03-08 04:21:08,952 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.03988511521369219
2025-03-08 04:21:20,324 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.041115821301937104
2025-03-08 04:21:29,824 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_172_epoch.pt
2025-03-08 04:21:41,479 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04404190853238106
2025-03-08 04:21:53,391 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.039488034565001724
2025-03-08 04:22:04,554 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04026715733110905
2025-03-08 04:22:15,769 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04168296419084072
2025-03-08 04:22:28,095 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.0443587893396616
2025-03-08 04:22:36,612 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_173_epoch.pt
2025-03-08 04:22:47,553 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03389780938625336
2025-03-08 04:22:58,415 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.03760684000328183
2025-03-08 04:23:09,607 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.040724258571863174
2025-03-08 04:23:21,481 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.039170152656733986
2025-03-08 04:23:33,328 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.03945374974608421
2025-03-08 04:23:42,518 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_174_epoch.pt
2025-03-08 04:23:54,639 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.036020873710513115
2025-03-08 04:24:05,726 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.041370109245181086
2025-03-08 04:24:16,890 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04243415491034587
2025-03-08 04:24:28,264 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04344809009693563
2025-03-08 04:24:39,692 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04391529665887356
2025-03-08 04:24:48,643 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_175_epoch.pt
2025-03-08 04:25:00,407 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04323022682219744
2025-03-08 04:25:12,131 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.0408853343501687
2025-03-08 04:25:23,008 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.040714848637580875
2025-03-08 04:25:33,695 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.039775200923904776
2025-03-08 04:25:45,298 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04017029548436403
2025-03-08 04:25:54,106 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_176_epoch.pt
2025-03-08 04:26:06,475 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.047495644837617874
2025-03-08 04:26:17,297 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04945513539016247
2025-03-08 04:26:28,777 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04727503349383672
2025-03-08 04:26:40,336 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.0464921070728451
2025-03-08 04:26:51,599 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04506109170615673
2025-03-08 04:26:59,890 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_177_epoch.pt
2025-03-08 04:27:11,810 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.037923429757356644
2025-03-08 04:27:23,317 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.045598124265670774
2025-03-08 04:27:35,478 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.042525977653761704
2025-03-08 04:27:46,790 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.043638499099761245
2025-03-08 04:27:56,720 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.043132654145359994
2025-03-08 04:28:05,202 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_178_epoch.pt
2025-03-08 04:28:17,270 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03964015386998653
2025-03-08 04:28:28,180 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.041904302816838025
2025-03-08 04:28:39,549 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.0434636885796984
2025-03-08 04:28:50,897 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04304127217270434
2025-03-08 04:29:03,762 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04209943027049303
2025-03-08 04:29:11,731 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_179_epoch.pt
2025-03-08 04:29:23,418 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.030054307244718075
2025-03-08 04:29:35,121 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.0385977472923696
2025-03-08 04:29:46,560 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.039743550966183344
2025-03-08 04:29:58,479 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04196584708988667
2025-03-08 04:30:09,366 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.042012383222579955
2025-03-08 04:30:17,269 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_180_epoch.pt
2025-03-08 04:30:29,067 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.042232638858258724
2025-03-08 04:30:40,237 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04241497909650207
2025-03-08 04:30:51,753 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04537239587555329
2025-03-08 04:31:03,413 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.043841553051024676
2025-03-08 04:31:14,872 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.042745523817837236
2025-03-08 04:31:23,747 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_181_epoch.pt
2025-03-08 04:31:36,189 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03482806514948607
2025-03-08 04:31:47,680 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.03732449447736144
2025-03-08 04:31:59,249 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04068791305025419
2025-03-08 04:32:10,796 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.03880112422630191
2025-03-08 04:32:22,271 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.040447801932692526
2025-03-08 04:32:30,416 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_182_epoch.pt
2025-03-08 04:32:42,281 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04203598264604807
2025-03-08 04:32:52,900 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04162275029346347
2025-03-08 04:33:04,689 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.043564394786953925
2025-03-08 04:33:15,644 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04192649998702109
2025-03-08 04:33:27,101 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04337081845104694
2025-03-08 04:33:36,196 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_183_epoch.pt
2025-03-08 04:33:48,624 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04174396593123674
2025-03-08 04:34:00,031 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04453247610479593
2025-03-08 04:34:10,961 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04518779796858629
2025-03-08 04:34:22,745 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.0427838131878525
2025-03-08 04:34:34,119 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04346859040111303
2025-03-08 04:34:42,308 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_184_epoch.pt
2025-03-08 04:34:54,458 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.046745446287095545
2025-03-08 04:35:06,015 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04299179719761014
2025-03-08 04:35:17,945 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04498426103343566
2025-03-08 04:35:27,772 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.044498396506533026
2025-03-08 04:35:39,277 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.045666712448000905
2025-03-08 04:35:48,289 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_185_epoch.pt
2025-03-08 04:35:59,295 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.034653211049735545
2025-03-08 04:36:11,495 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.03776892395690083
2025-03-08 04:36:22,801 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.038913161307573316
2025-03-08 04:36:34,829 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.03869730909354985
2025-03-08 04:36:45,901 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.03806341128051281
2025-03-08 04:36:54,334 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_186_epoch.pt
2025-03-08 04:37:06,199 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04148037180304527
2025-03-08 04:37:17,598 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.040441286247223616
2025-03-08 04:37:29,835 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.037233512587845324
2025-03-08 04:37:41,265 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.036800606595352295
2025-03-08 04:37:51,979 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.0388671505600214
2025-03-08 04:38:00,363 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_187_epoch.pt
2025-03-08 04:38:11,700 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04279589783400297
2025-03-08 04:38:23,489 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.044627191685140136
2025-03-08 04:38:34,906 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.043873582991460956
2025-03-08 04:38:47,407 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.043684641253203156
2025-03-08 04:38:58,518 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.040329318068921566
2025-03-08 04:39:06,778 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_188_epoch.pt
2025-03-08 04:39:18,886 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.043817094974219796
2025-03-08 04:39:30,544 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04350171284750104
2025-03-08 04:39:41,911 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.041112218524018924
2025-03-08 04:39:53,511 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04193917877040804
2025-03-08 04:40:04,950 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.0413612724468112
2025-03-08 04:40:13,235 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_189_epoch.pt
2025-03-08 04:40:25,675 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.032654828615486624
2025-03-08 04:40:36,860 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.03754506131634116
2025-03-08 04:40:48,059 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.038488416771094006
2025-03-08 04:40:59,241 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.038278461433947086
2025-03-08 04:41:10,816 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.03941653830558062
2025-03-08 04:41:19,316 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_190_epoch.pt
2025-03-08 04:41:31,006 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04943652726709843
2025-03-08 04:41:43,067 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04748699503019452
2025-03-08 04:41:54,006 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04433385996768872
2025-03-08 04:42:05,596 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.044179993029683826
2025-03-08 04:42:16,145 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04273200082778931
2025-03-08 04:42:25,176 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_191_epoch.pt
2025-03-08 04:42:36,803 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04331303022801876
2025-03-08 04:42:48,873 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.0436708870716393
2025-03-08 04:43:01,160 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04342925164848566
2025-03-08 04:43:12,859 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04368509245105088
2025-03-08 04:43:22,771 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04248865125328302
2025-03-08 04:43:31,201 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_192_epoch.pt
2025-03-08 04:43:43,809 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.05053924839943647
2025-03-08 04:43:54,988 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04617846570909023
2025-03-08 04:44:05,808 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04288933609922727
2025-03-08 04:44:17,023 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.0429295739158988
2025-03-08 04:44:29,173 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.042815709859132765
2025-03-08 04:44:38,059 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_193_epoch.pt
2025-03-08 04:44:49,491 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04524534661322832
2025-03-08 04:45:00,247 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.040838080160319805
2025-03-08 04:45:11,900 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.040205200004080933
2025-03-08 04:45:23,234 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.03901497365906834
2025-03-08 04:45:35,023 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.03957038114219904
2025-03-08 04:45:43,597 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_194_epoch.pt
2025-03-08 04:45:56,336 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03591076388955116
2025-03-08 04:46:07,896 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.035344412717968225
2025-03-08 04:46:19,391 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.03663833146293958
2025-03-08 04:46:31,523 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.03997697683982551
2025-03-08 04:46:42,219 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04114909379929304
2025-03-08 04:46:50,685 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_195_epoch.pt
2025-03-08 04:47:01,950 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.035000396817922594
2025-03-08 04:47:13,809 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.03668189994990825
2025-03-08 04:47:24,505 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.040282369243601956
2025-03-08 04:47:36,762 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.039976530969142915
2025-03-08 04:47:48,017 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.040238425552845
2025-03-08 04:47:56,970 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_196_epoch.pt
2025-03-08 04:48:09,290 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03713352147489786
2025-03-08 04:48:20,804 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.038435940723866226
2025-03-08 04:48:32,183 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.037691700235009194
2025-03-08 04:48:43,555 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.03980300604365766
2025-03-08 04:48:54,990 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.03889031258225441
2025-03-08 04:49:02,785 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_197_epoch.pt
2025-03-08 04:49:14,257 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04167092237621546
2025-03-08 04:49:27,365 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04147185187786818
2025-03-08 04:49:37,365 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04119957951207956
2025-03-08 04:49:49,174 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.042891571735963224
2025-03-08 04:49:59,710 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.043741582430899144
2025-03-08 04:50:08,741 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_198_epoch.pt
2025-03-08 04:50:20,829 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03626260440796614
2025-03-08 04:50:32,101 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.039808750953525304
2025-03-08 04:50:43,581 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04039628277222315
2025-03-08 04:50:55,147 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04181914630346
2025-03-08 04:51:06,587 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04139940334111452
2025-03-08 04:51:15,042 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_199_epoch.pt
2025-03-08 04:51:26,901 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04622646514326334
2025-03-08 04:51:37,987 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.041035508811473845
2025-03-08 04:51:48,681 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.039725547867516676
2025-03-08 04:51:59,827 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.039751595314592124
2025-03-08 04:52:11,613 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04035897281765938
2025-03-08 04:52:20,949 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_1/_bert-base-uncased_200_epoch.pt
2025-03-08 04:52:22,097 : 916236790.py : __call__ : INFO : loaded dataset, sample = 1181
2025-03-08 04:52:22,098 : 1341934384.py : train_scp : INFO : 109483778
2025-03-08 04:52:33,691 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.20099202305078506
2025-03-08 04:52:45,329 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.19998560652136801
2025-03-08 04:52:57,164 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.19963900725046793
2025-03-08 04:53:08,336 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.19933596439659595
2025-03-08 04:53:19,908 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.20021722531318664
2025-03-08 04:53:31,253 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_1_epoch.pt
2025-03-08 04:53:43,009 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.19436597615480422
2025-03-08 04:53:54,252 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.19534075133502482
2025-03-08 04:54:06,979 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.19567378928263982
2025-03-08 04:54:18,588 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.19548752296715974
2025-03-08 04:54:29,635 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.19393110564351082
2025-03-08 04:54:40,237 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_2_epoch.pt
2025-03-08 04:54:51,913 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.18268708199262618
2025-03-08 04:55:03,382 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.18534247798845171
2025-03-08 04:55:15,592 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.18617978528141976
2025-03-08 04:55:27,334 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.18500332351773977
2025-03-08 04:55:37,892 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.18381836345791816
2025-03-08 04:55:48,345 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_3_epoch.pt
2025-03-08 04:56:00,305 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.1731759337335825
2025-03-08 04:56:11,911 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.17610159549862148
2025-03-08 04:56:22,566 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.17705560778578122
2025-03-08 04:56:34,671 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.17576470631174743
2025-03-08 04:56:45,838 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.17317038882523775
2025-03-08 04:56:55,942 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_4_epoch.pt
2025-03-08 04:57:07,628 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.15820785865187645
2025-03-08 04:57:19,023 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.15496055502444506
2025-03-08 04:57:29,574 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.1517609971513351
2025-03-08 04:57:41,598 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.15019914470613002
2025-03-08 04:57:52,957 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.14795253337919712
2025-03-08 04:58:04,052 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_5_epoch.pt
2025-03-08 04:58:15,809 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.12656365044414997
2025-03-08 04:58:27,026 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.13145744930952788
2025-03-08 04:58:38,296 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.13211649025479952
2025-03-08 04:58:49,579 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.13442020172253252
2025-03-08 04:59:01,819 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.13384323816001414
2025-03-08 04:59:13,318 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_6_epoch.pt
2025-03-08 04:59:24,241 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.13530412185937166
2025-03-08 04:59:35,785 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.12271401025354862
2025-03-08 04:59:47,416 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.11231736350804568
2025-03-08 04:59:58,668 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.10755896694026887
2025-03-08 05:00:11,030 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.10254342345893383
2025-03-08 05:00:21,604 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_7_epoch.pt
2025-03-08 05:00:33,579 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.07167517255991697
2025-03-08 05:00:45,678 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.07158305458724498
2025-03-08 05:00:56,328 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.07197190493345261
2025-03-08 05:01:07,894 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.07015888982452452
2025-03-08 05:01:19,523 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.0703698780760169
2025-03-08 05:01:29,747 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_8_epoch.pt
2025-03-08 05:01:41,971 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.07316352739930153
2025-03-08 05:01:53,232 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.06943273292854428
2025-03-08 05:02:04,881 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.06893312913676103
2025-03-08 05:02:15,876 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.06727422738447786
2025-03-08 05:02:27,735 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.06451603445410728
2025-03-08 05:02:37,722 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_9_epoch.pt
2025-03-08 05:02:49,836 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.06041066061705351
2025-03-08 05:03:00,929 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.06239915192127228
2025-03-08 05:03:11,913 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.06294289885709684
2025-03-08 05:03:23,235 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.06148559007793665
2025-03-08 05:03:35,575 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.06169773215800524
2025-03-08 05:03:45,052 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_10_epoch.pt
2025-03-08 05:03:56,946 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.05837454706430435
2025-03-08 05:04:07,200 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.06020894706249237
2025-03-08 05:04:19,101 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.05772294450551271
2025-03-08 05:04:30,324 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.05483621634542942
2025-03-08 05:04:41,546 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.0583200054243207
2025-03-08 05:04:52,730 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_11_epoch.pt
2025-03-08 05:05:04,548 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.05666997488588095
2025-03-08 05:05:15,584 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.06248846158385277
2025-03-08 05:05:27,557 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.06036020508656899
2025-03-08 05:05:39,535 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.061017058100551365
2025-03-08 05:05:51,316 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.05975519749522209
2025-03-08 05:06:01,115 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_12_epoch.pt
2025-03-08 05:06:13,094 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.05878815729171038
2025-03-08 05:06:24,197 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.05493765439838171
2025-03-08 05:06:35,607 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.05715469778825839
2025-03-08 05:06:46,918 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.058991705905646084
2025-03-08 05:06:59,558 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.0588209877461195
2025-03-08 05:07:09,536 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_13_epoch.pt
2025-03-08 05:07:21,878 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04564353346824646
2025-03-08 05:07:34,597 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.051109229922294615
2025-03-08 05:07:46,730 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.05176900355766217
2025-03-08 05:07:56,435 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.051037363959476355
2025-03-08 05:08:07,084 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.053816079385578634
2025-03-08 05:08:17,693 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_14_epoch.pt
2025-03-08 05:08:29,937 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.05430447246879339
2025-03-08 05:08:41,004 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.052003962360322474
2025-03-08 05:08:51,748 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.05250545425961415
2025-03-08 05:09:04,455 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.05191525812260807
2025-03-08 05:09:15,629 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.052545741051435474
2025-03-08 05:09:25,769 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_15_epoch.pt
2025-03-08 05:09:38,474 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.055727525502443316
2025-03-08 05:09:49,794 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.05459163933992386
2025-03-08 05:10:01,230 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.05354913834482431
2025-03-08 05:10:12,804 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.05487394895404577
2025-03-08 05:10:24,062 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.05516535326093435
2025-03-08 05:10:33,857 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_16_epoch.pt
2025-03-08 05:10:45,008 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.052839806005358696
2025-03-08 05:10:55,807 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.054411612413823605
2025-03-08 05:11:07,238 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.050920780499776205
2025-03-08 05:11:19,071 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.05277718130499125
2025-03-08 05:11:30,797 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.05270922637730837
2025-03-08 05:11:41,593 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_17_epoch.pt
2025-03-08 05:11:54,315 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.05098281916230917
2025-03-08 05:12:05,839 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.048944819793105124
2025-03-08 05:12:17,076 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.046051860849062605
2025-03-08 05:12:27,995 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04679930553771555
2025-03-08 05:12:39,047 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.049628264389932154
2025-03-08 05:12:49,808 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_18_epoch.pt
2025-03-08 05:13:02,445 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.057797797322273255
2025-03-08 05:13:13,566 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.05257120868191123
2025-03-08 05:13:24,354 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.05067060843110085
2025-03-08 05:13:35,547 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04984633299522102
2025-03-08 05:13:47,193 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.05041352850198746
2025-03-08 05:13:57,870 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_19_epoch.pt
2025-03-08 05:14:10,051 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.05577523868530989
2025-03-08 05:14:20,732 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.052450345307588575
2025-03-08 05:14:32,006 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.054306557613114516
2025-03-08 05:14:43,246 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.05294913522899151
2025-03-08 05:14:55,217 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.05282695963233709
2025-03-08 05:15:05,872 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_20_epoch.pt
2025-03-08 05:15:17,300 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.0572939008846879
2025-03-08 05:15:29,303 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.05157504189759493
2025-03-08 05:15:41,225 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.05209247261285782
2025-03-08 05:15:52,233 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.05223562629893422
2025-03-08 05:16:02,730 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.0525837562456727
2025-03-08 05:16:13,870 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_21_epoch.pt
2025-03-08 05:16:26,508 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.05691251542419195
2025-03-08 05:16:37,975 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.05317541943863034
2025-03-08 05:16:49,667 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.05464282511423031
2025-03-08 05:17:00,615 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.052938572792336346
2025-03-08 05:17:11,107 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.05203075600415468
2025-03-08 05:17:21,746 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_22_epoch.pt
2025-03-08 05:17:33,254 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04785642188042402
2025-03-08 05:17:44,330 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04588236644864083
2025-03-08 05:17:55,363 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04859856840223074
2025-03-08 05:18:07,364 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04958325829356909
2025-03-08 05:18:18,987 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.050976599268615246
2025-03-08 05:18:29,560 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_23_epoch.pt
2025-03-08 05:18:41,692 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04967598598450422
2025-03-08 05:18:53,632 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.050803503319621086
2025-03-08 05:19:04,527 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04713834255933762
2025-03-08 05:19:16,213 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04880751398392022
2025-03-08 05:19:28,228 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04778036780655384
2025-03-08 05:19:37,660 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_24_epoch.pt
2025-03-08 05:19:49,773 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.05067102383822203
2025-03-08 05:20:01,698 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.047117618452757595
2025-03-08 05:20:12,601 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.046025424537559353
2025-03-08 05:20:23,866 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04687441254034638
2025-03-08 05:20:34,296 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04727530712634325
2025-03-08 05:20:45,303 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_25_epoch.pt
2025-03-08 05:20:56,681 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.0464038647338748
2025-03-08 05:21:08,721 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.05129583176225424
2025-03-08 05:21:20,233 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.046729708040754
2025-03-08 05:21:31,419 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04601381901651621
2025-03-08 05:21:43,103 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.046243329122662546
2025-03-08 05:21:53,160 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_26_epoch.pt
2025-03-08 05:22:05,296 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.047744144834578034
2025-03-08 05:22:16,926 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.05095353294163942
2025-03-08 05:22:27,778 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04911409272501866
2025-03-08 05:22:39,725 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04847797493450343
2025-03-08 05:22:50,681 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04860081965476275
2025-03-08 05:23:01,109 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_27_epoch.pt
2025-03-08 05:23:12,716 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.0404472803324461
2025-03-08 05:23:22,947 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.042890804708004
2025-03-08 05:23:34,429 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04538247394065062
2025-03-08 05:23:46,518 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04452212662436068
2025-03-08 05:23:58,201 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.043157226048409936
2025-03-08 05:24:08,652 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_28_epoch.pt
2025-03-08 05:24:20,689 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04522419642657041
2025-03-08 05:24:32,834 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.049805303905159234
2025-03-08 05:24:44,414 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04825599091748396
2025-03-08 05:24:55,685 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04961260481737554
2025-03-08 05:25:06,664 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.049269629172980785
2025-03-08 05:25:16,959 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_29_epoch.pt
2025-03-08 05:25:28,271 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04639600835740566
2025-03-08 05:25:39,823 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.046960461605340245
2025-03-08 05:25:50,721 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04633610157916943
2025-03-08 05:26:01,796 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.045053233914077284
2025-03-08 05:26:13,467 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04641629129648209
2025-03-08 05:26:24,512 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_30_epoch.pt
2025-03-08 05:26:37,510 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04979451224207878
2025-03-08 05:26:49,024 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04932670049369335
2025-03-08 05:27:00,253 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.049055164294938246
2025-03-08 05:27:11,384 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04762539097107947
2025-03-08 05:27:21,861 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04706305442005396
2025-03-08 05:27:32,085 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_31_epoch.pt
2025-03-08 05:27:44,700 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04634781882166863
2025-03-08 05:27:57,216 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04854245318099856
2025-03-08 05:28:08,198 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04550805550068617
2025-03-08 05:28:18,366 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04564992683939636
2025-03-08 05:28:30,452 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04380274935811758
2025-03-08 05:28:40,148 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_32_epoch.pt
2025-03-08 05:28:51,195 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04389565210789442
2025-03-08 05:29:03,101 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.045752012468874455
2025-03-08 05:29:15,009 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.046072244172294934
2025-03-08 05:29:26,011 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04738256158307195
2025-03-08 05:29:37,946 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.046461026720702646
2025-03-08 05:29:48,563 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_33_epoch.pt
2025-03-08 05:30:01,087 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03315612256526947
2025-03-08 05:30:11,684 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.03865667028352618
2025-03-08 05:30:23,013 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.042746522476275764
2025-03-08 05:30:34,800 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.041552968015894295
2025-03-08 05:30:45,788 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04157261408120394
2025-03-08 05:30:56,324 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_34_epoch.pt
2025-03-08 05:31:08,057 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03852164026349783
2025-03-08 05:31:19,125 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04378737147897482
2025-03-08 05:31:30,124 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04551110837608576
2025-03-08 05:31:42,332 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.043993508871644736
2025-03-08 05:31:54,138 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04366265908628702
2025-03-08 05:32:04,424 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_35_epoch.pt
2025-03-08 05:32:16,256 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04693734925240278
2025-03-08 05:32:27,497 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.0467925176024437
2025-03-08 05:32:38,474 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04752120907107989
2025-03-08 05:32:49,923 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.048029449684545394
2025-03-08 05:33:01,372 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.048389092430472375
2025-03-08 05:33:12,051 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_36_epoch.pt
2025-03-08 05:33:24,301 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04398254353553057
2025-03-08 05:33:35,657 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04535403475165367
2025-03-08 05:33:47,991 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.042016998666028184
2025-03-08 05:33:59,374 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04508589880540967
2025-03-08 05:34:09,710 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.045667948685586456
2025-03-08 05:34:19,883 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_37_epoch.pt
2025-03-08 05:34:31,714 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.051548081152141094
2025-03-08 05:34:44,499 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.053119872994720936
2025-03-08 05:34:55,450 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.051932161984344326
2025-03-08 05:35:06,024 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04821027659811079
2025-03-08 05:35:17,375 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04692578393220902
2025-03-08 05:35:27,321 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_38_epoch.pt
2025-03-08 05:35:39,526 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04941537216305733
2025-03-08 05:35:49,974 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04813508234918117
2025-03-08 05:36:02,917 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04559128920237223
2025-03-08 05:36:13,912 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04440456858836114
2025-03-08 05:36:24,842 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.045656110763549806
2025-03-08 05:36:35,622 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_39_epoch.pt
2025-03-08 05:36:47,495 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.05175632286816836
2025-03-08 05:36:58,607 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.048466320466250185
2025-03-08 05:37:10,134 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04718411333858967
2025-03-08 05:37:22,192 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.046845404356718065
2025-03-08 05:37:33,339 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04565691789984703
2025-03-08 05:37:43,388 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_40_epoch.pt
2025-03-08 05:37:55,706 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.049896827563643456
2025-03-08 05:38:06,811 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.047003669943660495
2025-03-08 05:38:17,989 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.044267384919027486
2025-03-08 05:38:29,290 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04507925366051495
2025-03-08 05:38:40,442 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04442850241810083
2025-03-08 05:38:51,316 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_41_epoch.pt
2025-03-08 05:39:02,718 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.043796133510768415
2025-03-08 05:39:14,235 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.043385867513716224
2025-03-08 05:39:25,280 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04225645231703917
2025-03-08 05:39:36,601 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04287972304970026
2025-03-08 05:39:48,896 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.044019277013838294
2025-03-08 05:39:59,392 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_42_epoch.pt
2025-03-08 05:40:11,113 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.05004175383597612
2025-03-08 05:40:22,376 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04621248384937644
2025-03-08 05:40:34,080 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.043857047483325
2025-03-08 05:40:44,501 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04672464449889958
2025-03-08 05:40:55,784 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.045752685762941836
2025-03-08 05:41:06,639 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_43_epoch.pt
2025-03-08 05:41:19,128 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04760578509420157
2025-03-08 05:41:31,036 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.042742083016783
2025-03-08 05:41:42,001 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04189734832694133
2025-03-08 05:41:52,964 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.0419357521366328
2025-03-08 05:42:04,787 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.042589855320751664
2025-03-08 05:42:14,845 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_44_epoch.pt
2025-03-08 05:42:26,680 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04031636487692594
2025-03-08 05:42:38,665 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.041649601720273495
2025-03-08 05:42:49,560 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.042257058173418044
2025-03-08 05:43:00,924 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04276495878584683
2025-03-08 05:43:11,686 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04441778210550547
2025-03-08 05:43:22,062 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_45_epoch.pt
2025-03-08 05:43:34,440 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.047461737468838694
2025-03-08 05:43:45,804 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04502914726734161
2025-03-08 05:43:56,901 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.045069263440867265
2025-03-08 05:44:08,108 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04289037643000484
2025-03-08 05:44:19,697 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.0439868880584836
2025-03-08 05:44:29,810 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_46_epoch.pt
2025-03-08 05:44:41,829 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04756804995238781
2025-03-08 05:44:52,939 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04466516120359301
2025-03-08 05:45:03,815 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.040771536367634933
2025-03-08 05:45:16,389 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.042208732245489954
2025-03-08 05:45:27,278 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.0417291561588645
2025-03-08 05:45:37,521 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_47_epoch.pt
2025-03-08 05:45:49,881 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04926055423915386
2025-03-08 05:46:00,582 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.047740918695926664
2025-03-08 05:46:12,606 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.048005621731281284
2025-03-08 05:46:24,122 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04592588906176388
2025-03-08 05:46:35,112 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04514572653919458
2025-03-08 05:46:45,804 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_48_epoch.pt
2025-03-08 05:46:57,412 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04229654788970947
2025-03-08 05:47:08,896 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.05046672437340021
2025-03-08 05:47:20,536 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04897393385569255
2025-03-08 05:47:32,074 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.047838051775470375
2025-03-08 05:47:43,353 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04667909701168537
2025-03-08 05:47:53,306 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_49_epoch.pt
2025-03-08 05:48:04,685 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03876686301082373
2025-03-08 05:48:15,813 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.03792814007028937
2025-03-08 05:48:26,723 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.03645620297640562
2025-03-08 05:48:38,061 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04036828176118434
2025-03-08 05:48:50,060 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.040409879118204116
2025-03-08 05:49:01,861 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_50_epoch.pt
2025-03-08 05:49:14,540 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.0371619176119566
2025-03-08 05:49:25,117 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.036852135993540286
2025-03-08 05:49:35,224 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.040680736092229684
2025-03-08 05:49:46,299 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.042435809299349786
2025-03-08 05:49:58,379 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.041494558796286586
2025-03-08 05:50:09,466 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_51_epoch.pt
2025-03-08 05:50:20,717 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04756572976708412
2025-03-08 05:50:32,214 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04722797509282827
2025-03-08 05:50:44,100 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.0482404334222277
2025-03-08 05:50:55,329 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04752362293191254
2025-03-08 05:51:07,478 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.047336986348032954
2025-03-08 05:51:17,367 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_52_epoch.pt
2025-03-08 05:51:29,180 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.048820459991693495
2025-03-08 05:51:40,278 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.047607892714440825
2025-03-08 05:51:52,335 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04701668964078029
2025-03-08 05:52:03,307 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.045892730923369526
2025-03-08 05:52:14,796 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04373651556670666
2025-03-08 05:52:25,244 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_53_epoch.pt
2025-03-08 05:52:36,537 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.044813948683440685
2025-03-08 05:52:48,981 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04513549569994211
2025-03-08 05:53:00,872 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04588887299100558
2025-03-08 05:53:11,284 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04479475671425462
2025-03-08 05:53:22,715 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04278104002773762
2025-03-08 05:53:32,885 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_54_epoch.pt
2025-03-08 05:53:44,550 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.043380946964025495
2025-03-08 05:53:56,572 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04306917868554592
2025-03-08 05:54:07,860 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04201525757710139
2025-03-08 05:54:19,113 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04330977093428373
2025-03-08 05:54:30,121 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.043470614776015284
2025-03-08 05:54:40,608 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_55_epoch.pt
2025-03-08 05:54:52,534 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04874776639044285
2025-03-08 05:55:03,155 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.045743564050644635
2025-03-08 05:55:14,993 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04629414357244968
2025-03-08 05:55:26,403 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04489682728424668
2025-03-08 05:55:37,826 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04432938531041145
2025-03-08 05:55:48,290 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_56_epoch.pt
2025-03-08 05:56:00,083 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04617576383054257
2025-03-08 05:56:12,130 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04492105482146144
2025-03-08 05:56:23,850 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.043735795095562933
2025-03-08 05:56:34,211 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.0440940753929317
2025-03-08 05:56:45,973 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04462928104400635
2025-03-08 05:56:56,171 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_57_epoch.pt
2025-03-08 05:57:07,796 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03999882806092501
2025-03-08 05:57:19,235 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04212581260129809
2025-03-08 05:57:30,750 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04169470850378275
2025-03-08 05:57:41,604 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04490234847180545
2025-03-08 05:57:52,296 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04610153949260712
2025-03-08 05:58:03,617 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_58_epoch.pt
2025-03-08 05:58:15,002 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03562260910868645
2025-03-08 05:58:26,042 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04084350135177374
2025-03-08 05:58:37,229 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04035874818762143
2025-03-08 05:58:49,343 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04063182727433741
2025-03-08 05:59:00,366 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04347201397269964
2025-03-08 05:59:11,324 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_59_epoch.pt
2025-03-08 05:59:22,797 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03899764936417341
2025-03-08 05:59:34,112 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.037668969351798295
2025-03-08 05:59:45,865 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.039594595233599345
2025-03-08 05:59:57,736 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.03912099397741258
2025-03-08 06:00:08,333 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04127955047041178
2025-03-08 06:00:18,561 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_60_epoch.pt
2025-03-08 06:00:29,949 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.042871464192867276
2025-03-08 06:00:41,868 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.041357812192291024
2025-03-08 06:00:52,476 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.041187466805179915
2025-03-08 06:01:04,195 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.041460205568000674
2025-03-08 06:01:15,497 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04066628481447697
2025-03-08 06:01:25,724 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_61_epoch.pt
2025-03-08 06:01:37,738 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03694341141730547
2025-03-08 06:01:48,965 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04271047791466117
2025-03-08 06:02:00,284 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04209067136049271
2025-03-08 06:02:11,338 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04330368034541607
2025-03-08 06:02:23,063 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04354454281181097
2025-03-08 06:02:33,405 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_62_epoch.pt
2025-03-08 06:02:44,822 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04508278474211693
2025-03-08 06:02:56,120 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.0435163945145905
2025-03-08 06:03:07,039 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.0422592784712712
2025-03-08 06:03:19,105 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.040626051183789966
2025-03-08 06:03:30,751 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.039475683473050596
2025-03-08 06:03:41,368 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_63_epoch.pt
2025-03-08 06:03:54,048 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.045813875272870064
2025-03-08 06:04:04,856 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04447873702272773
2025-03-08 06:04:16,334 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04488596711307764
2025-03-08 06:04:26,848 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.045102057019248604
2025-03-08 06:04:39,661 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04409701435267925
2025-03-08 06:04:49,996 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_64_epoch.pt
2025-03-08 06:05:03,199 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04064553935080767
2025-03-08 06:05:14,206 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04056121485307813
2025-03-08 06:05:26,399 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.044511753047506014
2025-03-08 06:05:36,393 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04462313096038997
2025-03-08 06:05:47,815 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.045293796338140964
2025-03-08 06:05:57,362 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_65_epoch.pt
2025-03-08 06:06:09,071 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03518072880804539
2025-03-08 06:06:20,289 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.03668305221945047
2025-03-08 06:06:31,907 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.03612051802376906
2025-03-08 06:06:43,204 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.039688900941982866
2025-03-08 06:06:55,341 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.040915483608841896
2025-03-08 06:07:05,569 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_66_epoch.pt
2025-03-08 06:07:16,852 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.0354880278557539
2025-03-08 06:07:28,188 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.039432072453200816
2025-03-08 06:07:39,686 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.03804916137208541
2025-03-08 06:07:50,743 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.037903283461928367
2025-03-08 06:08:03,222 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04073038542270661
2025-03-08 06:08:13,442 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_67_epoch.pt
2025-03-08 06:08:25,294 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04039785593748093
2025-03-08 06:08:36,531 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04136838080361485
2025-03-08 06:08:47,430 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04143970001488924
2025-03-08 06:08:59,390 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04151411294937134
2025-03-08 06:09:11,454 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.041614284723997114
2025-03-08 06:09:21,922 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_68_epoch.pt
2025-03-08 06:09:33,851 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.039417783245444295
2025-03-08 06:09:44,518 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.03896874977275729
2025-03-08 06:09:55,738 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.040197482059399284
2025-03-08 06:10:07,034 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.042135815005749463
2025-03-08 06:10:19,328 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04154912047088146
2025-03-08 06:10:29,872 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_69_epoch.pt
2025-03-08 06:10:42,281 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04077532712370157
2025-03-08 06:10:53,961 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04555317293852568
2025-03-08 06:11:04,829 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04372119764486949
2025-03-08 06:11:16,046 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04316814775578678
2025-03-08 06:11:27,201 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04400586239248514
2025-03-08 06:11:37,761 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_70_epoch.pt
2025-03-08 06:11:50,012 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.039722331762313844
2025-03-08 06:12:00,420 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.042954142428934576
2025-03-08 06:12:12,378 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.0445733150963982
2025-03-08 06:12:23,731 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.0457314345613122
2025-03-08 06:12:35,709 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04485500550270081
2025-03-08 06:12:45,863 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_71_epoch.pt
2025-03-08 06:12:58,206 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03737788196653127
2025-03-08 06:13:09,652 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.041692270394414664
2025-03-08 06:13:21,353 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.038216282799839976
2025-03-08 06:13:32,743 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.038498009601607916
2025-03-08 06:13:43,235 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04003773564100265
2025-03-08 06:13:54,034 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_72_epoch.pt
2025-03-08 06:14:05,329 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.039675924219191076
2025-03-08 06:14:16,737 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04180288843810558
2025-03-08 06:14:28,890 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04214044572164615
2025-03-08 06:14:41,076 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.0415953675378114
2025-03-08 06:14:52,480 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04143195262551308
2025-03-08 06:15:01,959 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_73_epoch.pt
2025-03-08 06:15:13,782 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03861025810241699
2025-03-08 06:15:25,116 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04525429198518396
2025-03-08 06:15:36,925 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.043251233833531535
2025-03-08 06:15:48,203 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04267344060353935
2025-03-08 06:15:59,378 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04320385834574699
2025-03-08 06:16:09,942 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_74_epoch.pt
2025-03-08 06:16:22,904 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03794222809374333
2025-03-08 06:16:33,956 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.040185143947601316
2025-03-08 06:16:45,554 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04261972675720851
2025-03-08 06:16:56,688 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04182279214262962
2025-03-08 06:17:08,070 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.042476231724023816
2025-03-08 06:17:17,788 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_75_epoch.pt
2025-03-08 06:17:30,257 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.041188711896538735
2025-03-08 06:17:42,441 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.039140287823975085
2025-03-08 06:17:53,638 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.03980902856836716
2025-03-08 06:18:04,764 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04152532551437616
2025-03-08 06:18:15,318 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.03977140959352255
2025-03-08 06:18:25,688 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_76_epoch.pt
2025-03-08 06:18:38,360 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.035058420412242414
2025-03-08 06:18:48,834 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.038509407583624124
2025-03-08 06:18:59,876 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.0372196211044987
2025-03-08 06:19:11,464 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.03716430234722793
2025-03-08 06:19:22,275 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.03911063123494387
2025-03-08 06:19:33,235 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_77_epoch.pt
2025-03-08 06:19:45,432 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04311624620109797
2025-03-08 06:19:57,365 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04130989694967866
2025-03-08 06:20:08,533 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04019043002277613
2025-03-08 06:20:19,278 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04018137991428375
2025-03-08 06:20:30,750 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04015749111026525
2025-03-08 06:20:40,889 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_78_epoch.pt
2025-03-08 06:20:52,761 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04663714796304703
2025-03-08 06:21:04,471 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04081300910562277
2025-03-08 06:21:15,668 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.0423610250155131
2025-03-08 06:21:26,718 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04296231762506068
2025-03-08 06:21:37,510 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.0418255133330822
2025-03-08 06:21:48,639 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_79_epoch.pt
2025-03-08 06:22:00,732 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.042533894889056685
2025-03-08 06:22:11,585 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.043812804035842416
2025-03-08 06:22:22,593 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.044854759921630225
2025-03-08 06:22:35,397 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04441370528191328
2025-03-08 06:22:47,433 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.0427840720936656
2025-03-08 06:22:56,914 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_80_epoch.pt
2025-03-08 06:23:09,072 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04227942142635584
2025-03-08 06:23:19,551 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04194467389956116
2025-03-08 06:23:31,077 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04017390480885903
2025-03-08 06:23:42,558 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.042166854050010444
2025-03-08 06:23:54,030 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.041666804172098636
2025-03-08 06:24:04,789 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_81_epoch.pt
2025-03-08 06:24:17,544 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04419011630117893
2025-03-08 06:24:28,695 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.045875554867088796
2025-03-08 06:24:39,594 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.044752801209688185
2025-03-08 06:24:51,619 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.043712040446698665
2025-03-08 06:25:02,469 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.043854260504245755
2025-03-08 06:25:12,642 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_82_epoch.pt
2025-03-08 06:25:24,984 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03159474715590477
2025-03-08 06:25:36,627 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.03308489197865128
2025-03-08 06:25:47,920 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.037701908412079016
2025-03-08 06:25:59,200 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.037476352974772455
2025-03-08 06:26:09,462 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.03945478531718254
2025-03-08 06:26:19,874 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_83_epoch.pt
2025-03-08 06:26:33,263 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.05071727994829416
2025-03-08 06:26:45,362 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04261080538854003
2025-03-08 06:26:55,684 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04181205241630475
2025-03-08 06:27:06,167 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04081483297981322
2025-03-08 06:27:17,784 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04112636077404022
2025-03-08 06:27:27,741 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_84_epoch.pt
2025-03-08 06:27:39,239 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.05290497370064259
2025-03-08 06:27:50,661 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.047203916031867264
2025-03-08 06:28:02,392 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04648595357934634
2025-03-08 06:28:13,015 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04281917098909616
2025-03-08 06:28:24,638 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04355873665958643
2025-03-08 06:28:35,554 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_85_epoch.pt
2025-03-08 06:28:47,250 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04450375311076641
2025-03-08 06:28:59,620 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04561606451869011
2025-03-08 06:29:09,585 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04424043420702219
2025-03-08 06:29:20,358 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04452401554211974
2025-03-08 06:29:32,354 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04272142043709755
2025-03-08 06:29:43,363 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_86_epoch.pt
2025-03-08 06:29:55,117 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04282763659954071
2025-03-08 06:30:07,626 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.040517782941460606
2025-03-08 06:30:18,569 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04098743236313263
2025-03-08 06:30:29,676 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.03963410725817084
2025-03-08 06:30:41,341 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04228153877705336
2025-03-08 06:30:51,707 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_87_epoch.pt
2025-03-08 06:31:04,034 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.05643531259149313
2025-03-08 06:31:15,458 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.049568050056695935
2025-03-08 06:31:25,868 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04628582768142223
2025-03-08 06:31:38,093 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04509397645480931
2025-03-08 06:31:48,795 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04407892368733883
2025-03-08 06:31:59,506 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_88_epoch.pt
2025-03-08 06:32:11,019 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.044536103941500184
2025-03-08 06:32:23,116 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04365053325891495
2025-03-08 06:32:34,656 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04122453600168228
2025-03-08 06:32:45,594 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04172256927937269
2025-03-08 06:32:56,665 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04129630269855261
2025-03-08 06:33:07,291 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_89_epoch.pt
2025-03-08 06:33:18,497 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04209315296262503
2025-03-08 06:33:30,959 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04096966946497559
2025-03-08 06:33:42,220 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04076446656137705
2025-03-08 06:33:53,710 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.041438961783424016
2025-03-08 06:34:05,481 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04203376714885235
2025-03-08 06:34:15,450 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_90_epoch.pt
2025-03-08 06:34:27,887 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.031654455475509165
2025-03-08 06:34:38,506 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.034139552898705
2025-03-08 06:34:50,026 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.03507487720499436
2025-03-08 06:35:01,966 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.035531675228849056
2025-03-08 06:35:12,712 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.03663383503258228
2025-03-08 06:35:22,955 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_91_epoch.pt
2025-03-08 06:35:35,577 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.0415564551204443
2025-03-08 06:35:46,075 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.03984309194609523
2025-03-08 06:35:56,464 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04016975513348977
2025-03-08 06:36:08,680 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04020153728313744
2025-03-08 06:36:19,983 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.038816376693546775
2025-03-08 06:36:31,437 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_92_epoch.pt
2025-03-08 06:36:42,928 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04293040238320828
2025-03-08 06:36:54,940 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.045570904556661844
2025-03-08 06:37:05,143 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.044060932832459605
2025-03-08 06:37:16,403 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04081903169862926
2025-03-08 06:37:28,733 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04086177599430084
2025-03-08 06:37:39,254 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_93_epoch.pt
2025-03-08 06:37:50,161 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04632252983748913
2025-03-08 06:38:02,356 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04405374402180314
2025-03-08 06:38:14,458 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04389079261571169
2025-03-08 06:38:26,033 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.044870341252535584
2025-03-08 06:38:37,257 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04449487712234259
2025-03-08 06:38:47,389 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_94_epoch.pt
2025-03-08 06:38:59,484 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03625065926462412
2025-03-08 06:39:10,932 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04000380231067538
2025-03-08 06:39:22,287 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.03805208839476108
2025-03-08 06:39:34,021 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.0396511394623667
2025-03-08 06:39:45,463 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04087319078296423
2025-03-08 06:39:55,555 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_95_epoch.pt
2025-03-08 06:40:07,624 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.038934181295335296
2025-03-08 06:40:18,118 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04017152391374111
2025-03-08 06:40:30,404 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04220468411842982
2025-03-08 06:40:41,331 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04318241799250245
2025-03-08 06:40:52,783 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.042063989475369455
2025-03-08 06:41:03,332 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_96_epoch.pt
2025-03-08 06:41:16,291 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.0353809729963541
2025-03-08 06:41:27,489 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.03863545496016741
2025-03-08 06:41:38,196 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.03893578718105952
2025-03-08 06:41:49,935 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.039073376851156354
2025-03-08 06:42:00,210 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.039540877021849156
2025-03-08 06:42:11,560 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_97_epoch.pt
2025-03-08 06:42:23,246 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.036518281027674675
2025-03-08 06:42:34,275 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.041118037104606625
2025-03-08 06:42:45,487 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04352996474752824
2025-03-08 06:42:56,450 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.043281859708949925
2025-03-08 06:43:08,149 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04324562443047762
2025-03-08 06:43:19,287 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_98_epoch.pt
2025-03-08 06:43:31,454 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.047634694166481494
2025-03-08 06:43:42,173 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.044460536781698465
2025-03-08 06:43:53,902 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04211465608328581
2025-03-08 06:44:05,210 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.042743921149522066
2025-03-08 06:44:17,157 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04246828542649746
2025-03-08 06:44:27,693 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_99_epoch.pt
2025-03-08 06:44:39,252 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04627154048532248
2025-03-08 06:44:50,553 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04603811187669635
2025-03-08 06:45:02,466 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04602227363735437
2025-03-08 06:45:14,811 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04606202204711735
2025-03-08 06:45:25,141 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.044322425052523616
2025-03-08 06:45:35,642 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_100_epoch.pt
2025-03-08 06:45:47,684 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03795123234391212
2025-03-08 06:45:59,681 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04322091145440936
2025-03-08 06:46:11,332 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04341313883662224
2025-03-08 06:46:22,956 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04284821419976652
2025-03-08 06:46:33,710 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.041348200127482415
2025-03-08 06:46:43,017 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_101_epoch.pt
2025-03-08 06:46:55,332 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04446735497564078
2025-03-08 06:47:06,683 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.042540284730494024
2025-03-08 06:47:18,171 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.040157234941919646
2025-03-08 06:47:29,693 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04034127360209823
2025-03-08 06:47:40,583 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04057659421861172
2025-03-08 06:47:50,811 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_102_epoch.pt
2025-03-08 06:48:03,180 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.043776851408183576
2025-03-08 06:48:13,887 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04403324568644166
2025-03-08 06:48:25,539 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.046019032796223956
2025-03-08 06:48:37,415 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04557384368963539
2025-03-08 06:48:48,243 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04565897888690233
2025-03-08 06:48:58,912 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_103_epoch.pt
2025-03-08 06:49:09,891 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.050288325510919094
2025-03-08 06:49:21,309 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.046249683685600756
2025-03-08 06:49:33,008 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04439309887588024
2025-03-08 06:49:45,291 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04353398115374148
2025-03-08 06:49:56,273 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.0430361390337348
2025-03-08 06:50:06,787 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_104_epoch.pt
2025-03-08 06:50:18,212 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.047997620552778245
2025-03-08 06:50:29,624 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.0443368412181735
2025-03-08 06:50:40,959 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04372199797381957
2025-03-08 06:50:53,112 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04067211078479886
2025-03-08 06:51:04,648 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04156768745929003
2025-03-08 06:51:14,824 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_105_epoch.pt
2025-03-08 06:51:26,883 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.046110088638961315
2025-03-08 06:51:37,348 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04180059002712369
2025-03-08 06:51:48,645 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.03995247347901265
2025-03-08 06:52:00,061 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04311348148621619
2025-03-08 06:52:11,518 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04456325253099203
2025-03-08 06:52:22,475 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_106_epoch.pt
2025-03-08 06:52:34,459 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03540532242506742
2025-03-08 06:52:46,380 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04085596097633243
2025-03-08 06:52:56,556 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.03734732531011105
2025-03-08 06:53:07,973 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04038322216831148
2025-03-08 06:53:19,952 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.040340693101286886
2025-03-08 06:53:30,900 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_107_epoch.pt
2025-03-08 06:53:42,481 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.039834620766341686
2025-03-08 06:53:54,547 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04306275464594364
2025-03-08 06:54:05,402 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04332916509360075
2025-03-08 06:54:16,753 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04258023247122764
2025-03-08 06:54:27,800 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04175861812382937
2025-03-08 06:54:38,208 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_108_epoch.pt
2025-03-08 06:54:49,966 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04560303784906864
2025-03-08 06:55:01,538 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.048852959200739864
2025-03-08 06:55:13,649 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.0456763381510973
2025-03-08 06:55:24,317 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04249090331606567
2025-03-08 06:55:36,426 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.0419597763940692
2025-03-08 06:55:45,969 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_109_epoch.pt
2025-03-08 06:55:58,285 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04132495976984501
2025-03-08 06:56:09,116 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04030961150303483
2025-03-08 06:56:21,092 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.042024649009108545
2025-03-08 06:56:32,737 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04242015475407243
2025-03-08 06:56:44,759 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.0434990298897028
2025-03-08 06:56:54,097 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_110_epoch.pt
2025-03-08 06:57:05,336 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.037201497480273245
2025-03-08 06:57:16,757 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04271211706101894
2025-03-08 06:57:29,153 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04474561414370934
2025-03-08 06:57:40,019 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04343850874342024
2025-03-08 06:57:51,876 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04136025842279196
2025-03-08 06:58:02,597 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_111_epoch.pt
2025-03-08 06:58:14,595 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03534228876233101
2025-03-08 06:58:25,424 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.034076019339263436
2025-03-08 06:58:37,518 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.03676568672060967
2025-03-08 06:58:48,509 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.03828598254360258
2025-03-08 06:58:59,685 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.038876152634620666
2025-03-08 06:59:09,817 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_112_epoch.pt
2025-03-08 06:59:22,081 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.036374380439519884
2025-03-08 06:59:33,339 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04138716731220484
2025-03-08 06:59:45,096 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.041944073277215166
2025-03-08 06:59:56,011 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.041871312372386456
2025-03-08 07:00:07,246 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04136188827455044
2025-03-08 07:00:18,080 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_113_epoch.pt
2025-03-08 07:00:29,498 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04357187986373901
2025-03-08 07:00:40,662 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.041521313562989236
2025-03-08 07:00:51,689 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.042693884645899134
2025-03-08 07:01:03,398 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04106082451529801
2025-03-08 07:01:14,963 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.042163119316101075
2025-03-08 07:01:25,526 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_114_epoch.pt
2025-03-08 07:01:38,063 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.0370054641738534
2025-03-08 07:01:48,768 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04049500968307257
2025-03-08 07:01:59,507 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.03748597462972005
2025-03-08 07:02:09,783 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.03896117925643921
2025-03-08 07:02:22,088 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.03968556621670723
2025-03-08 07:02:32,912 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_115_epoch.pt
2025-03-08 07:02:45,412 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04012954313308001
2025-03-08 07:02:57,133 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.041138340644538404
2025-03-08 07:03:07,211 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04228975851088762
2025-03-08 07:03:18,282 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04073488332331181
2025-03-08 07:03:29,730 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.03934320933371782
2025-03-08 07:03:40,390 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_116_epoch.pt
2025-03-08 07:03:51,964 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.0544930474832654
2025-03-08 07:04:03,973 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04657063068822026
2025-03-08 07:04:15,405 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04611780530462662
2025-03-08 07:04:27,077 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04465458869002759
2025-03-08 07:04:38,305 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04358356837928295
2025-03-08 07:04:48,340 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_117_epoch.pt
2025-03-08 07:04:59,962 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.043260515704751014
2025-03-08 07:05:10,224 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.03953377330675721
2025-03-08 07:05:22,340 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.03823226165026426
2025-03-08 07:05:34,296 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.03768557019531727
2025-03-08 07:05:45,725 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.03759770174324512
2025-03-08 07:05:55,473 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_118_epoch.pt
2025-03-08 07:06:06,677 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04366388697177172
2025-03-08 07:06:17,902 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04534814327955246
2025-03-08 07:06:29,562 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04691240308185418
2025-03-08 07:06:41,704 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04574535538442433
2025-03-08 07:06:52,922 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04558987306058407
2025-03-08 07:07:02,950 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_119_epoch.pt
2025-03-08 07:07:14,567 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03510228730738163
2025-03-08 07:07:26,618 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.039996232092380526
2025-03-08 07:07:38,539 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04082038473337889
2025-03-08 07:07:50,492 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04155114670284092
2025-03-08 07:08:01,350 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04060275438427925
2025-03-08 07:08:11,166 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_120_epoch.pt
2025-03-08 07:08:22,917 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.042560612037777903
2025-03-08 07:08:33,780 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.0378272139467299
2025-03-08 07:08:44,947 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04132708682368199
2025-03-08 07:08:56,639 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.039157872116193176
2025-03-08 07:09:08,226 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04003107213973999
2025-03-08 07:09:18,998 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_121_epoch.pt
2025-03-08 07:09:31,376 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04826660871505737
2025-03-08 07:09:41,881 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04465761987492442
2025-03-08 07:09:54,154 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04494105781118075
2025-03-08 07:10:05,545 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.043322754316031935
2025-03-08 07:10:17,035 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.0430784907117486
2025-03-08 07:10:27,502 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_122_epoch.pt
2025-03-08 07:10:38,721 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04412331566214561
2025-03-08 07:10:50,432 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04027319682762027
2025-03-08 07:11:02,371 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.03946550698330005
2025-03-08 07:11:14,191 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04459715147502721
2025-03-08 07:11:25,145 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.043556772127747535
2025-03-08 07:11:35,608 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_123_epoch.pt
2025-03-08 07:11:47,673 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04127516984939575
2025-03-08 07:11:59,072 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.039031135495752094
2025-03-08 07:12:10,192 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.038674435131251815
2025-03-08 07:12:20,818 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.03888515714555979
2025-03-08 07:12:32,490 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.040171040654182436
2025-03-08 07:12:44,034 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_124_epoch.pt
2025-03-08 07:12:56,322 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.040231071189045904
2025-03-08 07:13:06,675 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04332872478291392
2025-03-08 07:13:18,279 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04473260780175527
2025-03-08 07:13:29,711 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04547384255565703
2025-03-08 07:13:40,888 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04390033659338951
2025-03-08 07:13:52,002 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_125_epoch.pt
2025-03-08 07:14:04,567 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04212894842028618
2025-03-08 07:14:15,794 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.037191360387951136
2025-03-08 07:14:26,166 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.03822386040041844
2025-03-08 07:14:37,844 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.039339578365907076
2025-03-08 07:14:49,569 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04065004275739193
2025-03-08 07:15:00,139 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_126_epoch.pt
2025-03-08 07:15:12,086 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04140646375715733
2025-03-08 07:15:23,036 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04038068864494562
2025-03-08 07:15:35,197 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.03899692508081595
2025-03-08 07:15:46,382 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.03988171180710196
2025-03-08 07:15:57,998 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04135345728695393
2025-03-08 07:16:08,436 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_127_epoch.pt
2025-03-08 07:16:19,677 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03766999904066324
2025-03-08 07:16:30,775 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.038000140879303215
2025-03-08 07:16:42,809 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.03865889590233564
2025-03-08 07:16:53,439 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04045880387537181
2025-03-08 07:17:05,008 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04149096514284611
2025-03-08 07:17:15,676 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_128_epoch.pt
2025-03-08 07:17:28,344 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.042882624082267284
2025-03-08 07:17:40,350 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04026381209492683
2025-03-08 07:17:51,309 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.037678485612074535
2025-03-08 07:18:02,059 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.03675446639768779
2025-03-08 07:18:14,043 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.038342110469937324
2025-03-08 07:18:24,269 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_129_epoch.pt
2025-03-08 07:18:36,608 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04555017989128828
2025-03-08 07:18:47,481 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04313921622931957
2025-03-08 07:18:59,210 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.0391809165601929
2025-03-08 07:19:09,849 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.038296187967061995
2025-03-08 07:19:20,991 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.03771835605055094
2025-03-08 07:19:31,546 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_130_epoch.pt
2025-03-08 07:19:43,542 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04337773475795984
2025-03-08 07:19:55,015 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04341695478186011
2025-03-08 07:20:06,654 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.0453346474096179
2025-03-08 07:20:18,273 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.044671223759651185
2025-03-08 07:20:29,212 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04455679980665445
2025-03-08 07:20:39,438 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_131_epoch.pt
2025-03-08 07:20:50,804 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.05447573870420456
2025-03-08 07:21:02,912 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.044949990883469584
2025-03-08 07:21:13,828 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.042747033971051375
2025-03-08 07:21:26,113 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04303825418464839
2025-03-08 07:21:37,702 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04122232554852962
2025-03-08 07:21:47,672 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_132_epoch.pt
2025-03-08 07:21:59,267 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03797198411077261
2025-03-08 07:22:10,647 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04026881851255894
2025-03-08 07:22:21,662 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04088191989809275
2025-03-08 07:22:32,752 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04027424419298768
2025-03-08 07:22:45,217 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04204278595000505
2025-03-08 07:22:55,865 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_133_epoch.pt
2025-03-08 07:23:07,511 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04326218105852604
2025-03-08 07:23:19,007 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.045752973929047584
2025-03-08 07:23:31,261 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04136471490065257
2025-03-08 07:23:42,494 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.03888512719422579
2025-03-08 07:23:53,795 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.03991233345866203
2025-03-08 07:24:04,114 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_134_epoch.pt
2025-03-08 07:24:16,532 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04576416414231062
2025-03-08 07:24:27,891 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04744277838617563
2025-03-08 07:24:40,200 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.0456087426841259
2025-03-08 07:24:51,088 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04432673082686961
2025-03-08 07:25:02,442 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.044563097164034844
2025-03-08 07:25:12,377 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_135_epoch.pt
2025-03-08 07:25:24,405 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04274798020720482
2025-03-08 07:25:34,598 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04402021506801248
2025-03-08 07:25:46,094 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04317688790460428
2025-03-08 07:25:56,518 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.040036765569821
2025-03-08 07:26:08,785 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.040512754261493686
2025-03-08 07:26:20,260 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_136_epoch.pt
2025-03-08 07:26:32,437 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03663080848753452
2025-03-08 07:26:42,613 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.037814270295202734
2025-03-08 07:26:54,741 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.03892108287662268
2025-03-08 07:27:06,536 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.03875912455841899
2025-03-08 07:27:17,861 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.03814402636140585
2025-03-08 07:27:28,240 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_137_epoch.pt
2025-03-08 07:27:39,976 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03870163317769766
2025-03-08 07:27:51,550 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.039828653056174515
2025-03-08 07:28:03,565 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04100760322064161
2025-03-08 07:28:15,095 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.0420477008074522
2025-03-08 07:28:26,502 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.042054365709424016
2025-03-08 07:28:36,342 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_138_epoch.pt
2025-03-08 07:28:47,375 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03560417827218771
2025-03-08 07:28:58,654 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.03670563695952296
2025-03-08 07:29:10,738 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04040734831243754
2025-03-08 07:29:21,483 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.03907371507957578
2025-03-08 07:29:33,536 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.040210719577968124
2025-03-08 07:29:43,833 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_139_epoch.pt
2025-03-08 07:29:54,777 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03538796115666628
2025-03-08 07:30:05,823 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.03757390592247248
2025-03-08 07:30:16,762 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04121784873306751
2025-03-08 07:30:28,557 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04328130338340998
2025-03-08 07:30:40,442 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04296634396165609
2025-03-08 07:30:51,603 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_140_epoch.pt
2025-03-08 07:31:03,605 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04534316208213568
2025-03-08 07:31:15,290 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04051633758470416
2025-03-08 07:31:26,504 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04051267256339391
2025-03-08 07:31:37,957 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.039507645498961214
2025-03-08 07:31:48,387 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.03887216385453939
2025-03-08 07:31:59,262 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_141_epoch.pt
2025-03-08 07:32:11,710 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.037976816073060034
2025-03-08 07:32:22,901 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.0420334492623806
2025-03-08 07:32:34,238 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.040939045039316016
2025-03-08 07:32:44,949 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04170750099234283
2025-03-08 07:32:56,520 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.041592355325818064
2025-03-08 07:33:06,308 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_142_epoch.pt
2025-03-08 07:33:18,378 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.040020360797643664
2025-03-08 07:33:29,845 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.043875800874084235
2025-03-08 07:33:41,806 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04711681773265203
2025-03-08 07:33:53,420 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.044922844506800175
2025-03-08 07:34:04,613 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04415505799651146
2025-03-08 07:34:13,998 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_143_epoch.pt
2025-03-08 07:34:25,675 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.041611069813370705
2025-03-08 07:34:37,809 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04046354068443179
2025-03-08 07:34:49,707 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.042089251254995665
2025-03-08 07:34:59,631 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.041448286157101395
2025-03-08 07:35:10,947 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04101332414150238
2025-03-08 07:35:21,337 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_144_epoch.pt
2025-03-08 07:35:32,629 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03970151763409376
2025-03-08 07:35:43,273 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.03926044600084424
2025-03-08 07:35:56,116 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04150537179162105
2025-03-08 07:36:07,495 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04165527729317546
2025-03-08 07:36:19,180 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.040869234144687655
2025-03-08 07:36:28,962 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_145_epoch.pt
2025-03-08 07:36:40,383 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03768613673746586
2025-03-08 07:36:52,018 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04386303327977657
2025-03-08 07:37:03,052 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.045087352183957895
2025-03-08 07:37:14,146 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04276734451763332
2025-03-08 07:37:26,499 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04321621000021696
2025-03-08 07:37:37,288 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_146_epoch.pt
2025-03-08 07:37:48,773 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03672855775803328
2025-03-08 07:38:00,416 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.036956896781921385
2025-03-08 07:38:11,462 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.0399114802852273
2025-03-08 07:38:23,244 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04164131564088166
2025-03-08 07:38:34,632 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.042327081233263016
2025-03-08 07:38:45,805 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_147_epoch.pt
2025-03-08 07:38:57,478 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04198374532163143
2025-03-08 07:39:08,606 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04271382344886661
2025-03-08 07:39:19,645 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04262343890964985
2025-03-08 07:39:31,239 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04179911651648581
2025-03-08 07:39:43,074 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04235992780327797
2025-03-08 07:39:53,477 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_148_epoch.pt
2025-03-08 07:40:05,190 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.048158724419772626
2025-03-08 07:40:16,396 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.045755476672202346
2025-03-08 07:40:27,035 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.0434893037378788
2025-03-08 07:40:39,440 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04206255280412734
2025-03-08 07:40:50,844 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04154166083782911
2025-03-08 07:41:01,684 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_149_epoch.pt
2025-03-08 07:41:13,707 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04103591285645962
2025-03-08 07:41:25,909 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.041080373506993055
2025-03-08 07:41:37,401 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.040988729260861875
2025-03-08 07:41:48,293 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04087587811984122
2025-03-08 07:41:59,410 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.03952280059456825
2025-03-08 07:42:10,232 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_150_epoch.pt
2025-03-08 07:42:21,293 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03818815305829048
2025-03-08 07:42:32,120 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.0430376660451293
2025-03-08 07:42:43,862 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04129580872754256
2025-03-08 07:42:55,488 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04157763414084911
2025-03-08 07:43:07,205 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04209608956426382
2025-03-08 07:43:17,545 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_151_epoch.pt
2025-03-08 07:43:28,689 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04762453731149435
2025-03-08 07:43:40,404 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04350837755948305
2025-03-08 07:43:51,754 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04281220073501269
2025-03-08 07:44:03,012 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.042821645261719825
2025-03-08 07:44:14,805 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04169524099677801
2025-03-08 07:44:25,297 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_152_epoch.pt
2025-03-08 07:44:36,956 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04151983883231878
2025-03-08 07:44:48,542 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04871716927736998
2025-03-08 07:44:59,652 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04738337186475595
2025-03-08 07:45:12,767 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.047870088694617154
2025-03-08 07:45:24,147 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04599189675599337
2025-03-08 07:45:33,513 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_153_epoch.pt
2025-03-08 07:45:46,016 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04369592856615782
2025-03-08 07:45:56,475 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04262454830110073
2025-03-08 07:46:07,070 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04274385002752145
2025-03-08 07:46:18,817 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04251235813833773
2025-03-08 07:46:30,516 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.041640155762434
2025-03-08 07:46:40,980 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_154_epoch.pt
2025-03-08 07:46:52,527 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.0425624730437994
2025-03-08 07:47:04,295 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04069679709151387
2025-03-08 07:47:16,194 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04081464828302463
2025-03-08 07:47:27,973 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04116006295196712
2025-03-08 07:47:39,368 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04184716386348009
2025-03-08 07:47:48,656 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_155_epoch.pt
2025-03-08 07:48:00,291 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04059764947742224
2025-03-08 07:48:11,199 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.038657399136573074
2025-03-08 07:48:23,474 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.040217639890809856
2025-03-08 07:48:34,876 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.03882450986653566
2025-03-08 07:48:45,704 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.03979767102003098
2025-03-08 07:48:56,393 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_156_epoch.pt
2025-03-08 07:49:09,152 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.044304180108010766
2025-03-08 07:49:20,364 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04462194262072444
2025-03-08 07:49:32,384 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04443990845233202
2025-03-08 07:49:42,469 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.041151972580701114
2025-03-08 07:49:53,821 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.041437494665384295
2025-03-08 07:50:04,160 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_157_epoch.pt
2025-03-08 07:50:16,129 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.032012363635003566
2025-03-08 07:50:28,170 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.03979403818026185
2025-03-08 07:50:39,484 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.040697619517644244
2025-03-08 07:50:51,369 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04130114584229887
2025-03-08 07:51:03,303 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.041963501915335655
2025-03-08 07:51:12,469 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_158_epoch.pt
2025-03-08 07:51:24,999 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03652253497391939
2025-03-08 07:51:36,340 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.0433250586874783
2025-03-08 07:51:47,954 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04390274246533712
2025-03-08 07:51:59,346 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04178073455579579
2025-03-08 07:52:10,863 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04186688069999218
2025-03-08 07:52:19,951 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_159_epoch.pt
2025-03-08 07:52:32,022 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03871106922626495
2025-03-08 07:52:43,420 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.0398364670202136
2025-03-08 07:52:54,475 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04072055708616972
2025-03-08 07:53:05,990 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.03954435615800321
2025-03-08 07:53:17,154 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.040262542873620986
2025-03-08 07:53:27,578 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_160_epoch.pt
2025-03-08 07:53:39,517 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04552388906478882
2025-03-08 07:53:50,094 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04656601952388883
2025-03-08 07:54:01,000 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04533747840672731
2025-03-08 07:54:11,660 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04429164618253708
2025-03-08 07:54:24,193 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04353343185037375
2025-03-08 07:54:35,368 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_161_epoch.pt
2025-03-08 07:54:47,146 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03828682988882065
2025-03-08 07:54:58,924 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.03563424836844206
2025-03-08 07:55:11,116 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.037492184154689315
2025-03-08 07:55:22,159 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.03757808043621481
2025-03-08 07:55:32,784 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.03900635895878077
2025-03-08 07:55:43,355 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_162_epoch.pt
2025-03-08 07:55:54,933 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03823094427585602
2025-03-08 07:56:06,770 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04000603137537837
2025-03-08 07:56:18,380 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.039685704621175925
2025-03-08 07:56:29,101 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04276991072110832
2025-03-08 07:56:40,851 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.042548178113996984
2025-03-08 07:56:51,233 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_163_epoch.pt
2025-03-08 07:57:02,364 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04556781433522701
2025-03-08 07:57:13,824 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.0456813901476562
2025-03-08 07:57:25,992 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04572472664217154
2025-03-08 07:57:37,195 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.044442999698221686
2025-03-08 07:57:48,522 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04269899623095989
2025-03-08 07:57:58,767 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_164_epoch.pt
2025-03-08 07:58:10,463 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.052017164900898936
2025-03-08 07:58:21,548 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04661879548802972
2025-03-08 07:58:33,479 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.044137423162659006
2025-03-08 07:58:44,822 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04381535558961332
2025-03-08 07:58:55,721 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.043937316961586474
2025-03-08 07:59:06,561 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_165_epoch.pt
2025-03-08 07:59:19,071 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.05123635396361351
2025-03-08 07:59:29,623 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04084453880786896
2025-03-08 07:59:41,335 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04177140064537525
2025-03-08 07:59:52,947 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.041292860759422186
2025-03-08 08:00:04,876 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04114463629573584
2025-03-08 08:00:14,852 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_166_epoch.pt
2025-03-08 08:00:26,988 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03420957211405039
2025-03-08 08:00:38,537 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.03979685300961137
2025-03-08 08:00:49,849 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.0393897644802928
2025-03-08 08:01:01,346 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04107722546905279
2025-03-08 08:01:13,252 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04021202539652586
2025-03-08 08:01:23,232 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_167_epoch.pt
2025-03-08 08:01:34,944 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.0423437462374568
2025-03-08 08:01:47,110 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04573272794485092
2025-03-08 08:01:57,877 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.043577046630283195
2025-03-08 08:02:09,757 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04205558239482343
2025-03-08 08:02:20,585 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04361537119746208
2025-03-08 08:02:30,812 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_168_epoch.pt
2025-03-08 08:02:42,525 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.034438821114599706
2025-03-08 08:02:53,154 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.03536411741748452
2025-03-08 08:03:04,881 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.036903829375902815
2025-03-08 08:03:17,200 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.036226295977830884
2025-03-08 08:03:27,360 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.03593970191478729
2025-03-08 08:03:37,619 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_169_epoch.pt
2025-03-08 08:03:49,981 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04388346925377846
2025-03-08 08:04:01,446 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04052308237180114
2025-03-08 08:04:13,768 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04251189449181159
2025-03-08 08:04:23,821 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.043013443564996125
2025-03-08 08:04:34,606 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.041197653710842136
2025-03-08 08:04:45,360 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_170_epoch.pt
2025-03-08 08:04:57,071 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03901747278869152
2025-03-08 08:05:08,239 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.03703306594863534
2025-03-08 08:05:20,266 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.03787303236623605
2025-03-08 08:05:32,393 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.03813773724250495
2025-03-08 08:05:43,636 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.03980065356194973
2025-03-08 08:05:53,798 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_171_epoch.pt
2025-03-08 08:06:05,337 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04462232433259487
2025-03-08 08:06:16,221 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04317781737074256
2025-03-08 08:06:27,442 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04253836338718732
2025-03-08 08:06:39,023 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04149983813986182
2025-03-08 08:06:50,852 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04238811266422272
2025-03-08 08:07:01,713 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_172_epoch.pt
2025-03-08 08:07:13,662 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03765839472413063
2025-03-08 08:07:24,235 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.03426687739789486
2025-03-08 08:07:35,387 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.03921120579044024
2025-03-08 08:07:47,671 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.038402250930666924
2025-03-08 08:07:59,334 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.03862810734659433
2025-03-08 08:08:09,633 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_173_epoch.pt
2025-03-08 08:08:21,755 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04879426248371601
2025-03-08 08:08:33,043 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.045919778477400544
2025-03-08 08:08:45,209 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.042549096184472245
2025-03-08 08:08:56,016 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.044364303089678286
2025-03-08 08:09:06,868 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04333684483170509
2025-03-08 08:09:18,092 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_174_epoch.pt
2025-03-08 08:09:29,693 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.038276760131120684
2025-03-08 08:09:41,283 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04604203401133418
2025-03-08 08:09:51,256 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04285693208376567
2025-03-08 08:10:03,145 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04129391317255795
2025-03-08 08:10:14,905 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.03773285247385502
2025-03-08 08:10:25,485 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_175_epoch.pt
2025-03-08 08:10:37,368 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04288079414516687
2025-03-08 08:10:48,419 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04152050916105509
2025-03-08 08:10:59,148 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.044744875207543375
2025-03-08 08:11:11,268 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04541321763768792
2025-03-08 08:11:22,952 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04524395731836557
2025-03-08 08:11:33,337 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_176_epoch.pt
2025-03-08 08:11:45,856 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04123612020164728
2025-03-08 08:11:57,658 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.0443744576536119
2025-03-08 08:12:08,617 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04198609340935946
2025-03-08 08:12:19,942 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04103441867977381
2025-03-08 08:12:31,317 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.03886874017864466
2025-03-08 08:12:41,962 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_177_epoch.pt
2025-03-08 08:12:54,133 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03941979806870222
2025-03-08 08:13:05,906 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.039668952114880085
2025-03-08 08:13:17,408 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04150074020028114
2025-03-08 08:13:29,326 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04177641749382019
2025-03-08 08:13:40,617 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.03917609811574221
2025-03-08 08:13:49,829 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_178_epoch.pt
2025-03-08 08:14:02,167 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04923644319176674
2025-03-08 08:14:13,938 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04531889976933599
2025-03-08 08:14:24,723 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04274053420871496
2025-03-08 08:14:36,114 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.0397066499106586
2025-03-08 08:14:47,758 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.03857723590731621
2025-03-08 08:14:57,561 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_179_epoch.pt
2025-03-08 08:15:09,503 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04722459744662046
2025-03-08 08:15:20,949 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04121824501082301
2025-03-08 08:15:31,978 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.03984493688990672
2025-03-08 08:15:43,323 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.041419828347861766
2025-03-08 08:15:55,217 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04141061293333769
2025-03-08 08:16:04,870 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_180_epoch.pt
2025-03-08 08:16:16,830 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.0387108988314867
2025-03-08 08:16:29,257 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.03659629784524441
2025-03-08 08:16:40,109 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.03894432810445626
2025-03-08 08:16:50,264 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.03846398752182722
2025-03-08 08:17:02,178 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.03926426852494478
2025-03-08 08:17:12,969 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_181_epoch.pt
2025-03-08 08:17:25,654 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04201930675655603
2025-03-08 08:17:36,931 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.043902239054441454
2025-03-08 08:17:49,207 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.043760523473223054
2025-03-08 08:18:00,292 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.041878465283662084
2025-03-08 08:18:11,008 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04284516727924347
2025-03-08 08:18:21,313 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_182_epoch.pt
2025-03-08 08:18:33,197 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03686830546706915
2025-03-08 08:18:46,357 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.041801804043352604
2025-03-08 08:18:56,968 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04144382974753777
2025-03-08 08:19:07,535 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04000439397059381
2025-03-08 08:19:19,592 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.03994482038915157
2025-03-08 08:19:29,467 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_183_epoch.pt
2025-03-08 08:19:41,640 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03919688142836094
2025-03-08 08:19:52,681 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.037802733313292265
2025-03-08 08:20:04,750 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.03978126258899768
2025-03-08 08:20:16,390 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.042700337795540694
2025-03-08 08:20:28,308 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.041524634629487994
2025-03-08 08:20:37,654 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_184_epoch.pt
2025-03-08 08:20:48,179 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04095651488751173
2025-03-08 08:21:00,252 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04535075107589364
2025-03-08 08:21:11,758 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04630129389464855
2025-03-08 08:21:23,933 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.044995692046359184
2025-03-08 08:21:35,527 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04462366673350334
2025-03-08 08:21:45,173 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_185_epoch.pt
2025-03-08 08:21:57,351 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04135540124028921
2025-03-08 08:22:09,613 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.0411465678922832
2025-03-08 08:22:21,453 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04084669387588898
2025-03-08 08:22:31,659 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04106354300864041
2025-03-08 08:22:42,706 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04068767385184765
2025-03-08 08:22:52,629 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_186_epoch.pt
2025-03-08 08:23:04,820 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04135148584842682
2025-03-08 08:23:15,087 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.0395242079347372
2025-03-08 08:23:26,227 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.039430354498326776
2025-03-08 08:23:39,514 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.038940127585083244
2025-03-08 08:23:50,916 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.038161622151732445
2025-03-08 08:24:00,556 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_187_epoch.pt
2025-03-08 08:24:12,657 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.041265374273061754
2025-03-08 08:24:24,157 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04008847365155816
2025-03-08 08:24:35,924 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.041978275403380394
2025-03-08 08:24:46,851 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04341985834762454
2025-03-08 08:24:57,887 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04222933794558048
2025-03-08 08:25:08,896 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_188_epoch.pt
2025-03-08 08:25:21,752 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04564436506479978
2025-03-08 08:25:33,125 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04318572411313653
2025-03-08 08:25:45,203 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.043557786382734776
2025-03-08 08:25:56,539 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.043386813336983326
2025-03-08 08:26:07,300 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.043569769524037835
2025-03-08 08:26:16,918 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_189_epoch.pt
2025-03-08 08:26:28,168 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03259264145046473
2025-03-08 08:26:39,462 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.036499851867556575
2025-03-08 08:26:50,193 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.036424582501252495
2025-03-08 08:27:01,532 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.03849765333347022
2025-03-08 08:27:13,390 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.040303739964962004
2025-03-08 08:27:24,504 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_190_epoch.pt
2025-03-08 08:27:36,478 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03854486633092165
2025-03-08 08:27:48,107 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.03962030045688152
2025-03-08 08:27:58,746 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.03936680495738983
2025-03-08 08:28:11,052 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.03851778823882342
2025-03-08 08:28:22,435 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.03903362017869949
2025-03-08 08:28:32,404 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_191_epoch.pt
2025-03-08 08:28:43,334 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03880977764725685
2025-03-08 08:28:55,154 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.039004942663013936
2025-03-08 08:29:06,092 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.03741511460393667
2025-03-08 08:29:17,642 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.03810208409093321
2025-03-08 08:29:28,622 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.038927758663892746
2025-03-08 08:29:39,957 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_192_epoch.pt
2025-03-08 08:29:51,687 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03703815147280693
2025-03-08 08:30:03,138 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.03551015267148614
2025-03-08 08:30:14,357 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.03802445052812497
2025-03-08 08:30:25,141 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.03784101347438991
2025-03-08 08:30:36,532 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.03878448229283094
2025-03-08 08:30:47,436 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_193_epoch.pt
2025-03-08 08:30:59,129 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.037833399660885336
2025-03-08 08:31:10,805 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.043840448390692476
2025-03-08 08:31:23,490 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.043186347000300884
2025-03-08 08:31:35,056 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04294437693431973
2025-03-08 08:31:45,552 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.043724699065089224
2025-03-08 08:31:55,771 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_194_epoch.pt
2025-03-08 08:32:07,115 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.037735689356923105
2025-03-08 08:32:18,367 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.03860873397439718
2025-03-08 08:32:29,541 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.039370221855739754
2025-03-08 08:32:41,006 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.03999979641288519
2025-03-08 08:32:52,760 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.03885652379691601
2025-03-08 08:33:03,275 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_195_epoch.pt
2025-03-08 08:33:16,085 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04212857790291309
2025-03-08 08:33:27,681 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.036872241888195274
2025-03-08 08:33:38,911 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.03893378395587206
2025-03-08 08:33:49,765 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.03797943225130439
2025-03-08 08:34:00,369 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.0415483740568161
2025-03-08 08:34:10,769 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_196_epoch.pt
2025-03-08 08:34:22,644 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03752229992300272
2025-03-08 08:34:34,599 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.038320678379386665
2025-03-08 08:34:46,697 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04045837675531705
2025-03-08 08:34:57,482 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04163077598437667
2025-03-08 08:35:08,389 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04110156578570604
2025-03-08 08:35:17,971 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_197_epoch.pt
2025-03-08 08:35:29,999 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03514239873737097
2025-03-08 08:35:41,957 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.036713303681463
2025-03-08 08:35:53,817 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.037156976585586865
2025-03-08 08:36:04,669 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.039082592576742174
2025-03-08 08:36:15,687 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.039619917646050454
2025-03-08 08:36:26,187 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_198_epoch.pt
2025-03-08 08:36:38,547 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.02933949664235115
2025-03-08 08:36:49,662 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.031240304596722127
2025-03-08 08:37:01,539 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.033214667352537315
2025-03-08 08:37:13,511 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.03593056928366423
2025-03-08 08:37:24,093 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.03754923684895039
2025-03-08 08:37:34,549 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_199_epoch.pt
2025-03-08 08:37:45,424 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.041517376862466336
2025-03-08 08:37:56,657 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.044693620447069406
2025-03-08 08:38:09,427 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04411841722826163
2025-03-08 08:38:20,241 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.042453385964035986
2025-03-08 08:38:32,025 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.042378454193472866
2025-03-08 08:38:42,026 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_2/_bert-base-uncased_200_epoch.pt
2025-03-08 08:38:43,144 : 916236790.py : __call__ : INFO : loaded dataset, sample = 1181
2025-03-08 08:38:43,145 : 1341934384.py : train_scp : INFO : 109483778
2025-03-08 08:38:54,652 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.20330380484461785
2025-03-08 08:39:06,723 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.20431197967380285
2025-03-08 08:39:18,530 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.20283112791677316
2025-03-08 08:39:30,143 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.20322425659745932
2025-03-08 08:39:40,884 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.2034571922570467
2025-03-08 08:39:51,281 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_1_epoch.pt
2025-03-08 08:40:03,174 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.19987479597330093
2025-03-08 08:40:15,521 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.199946952983737
2025-03-08 08:40:26,990 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.19998740583658217
2025-03-08 08:40:38,902 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.1999705071747303
2025-03-08 08:40:50,774 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.1999673253595829
2025-03-08 08:41:01,026 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_2_epoch.pt
2025-03-08 08:41:13,147 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.19929878681898117
2025-03-08 08:41:23,483 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.1991344840824604
2025-03-08 08:41:35,801 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.1993708974123001
2025-03-08 08:41:47,283 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.19968748107552528
2025-03-08 08:41:59,054 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.19973531848192214
2025-03-08 08:42:10,139 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_3_epoch.pt
2025-03-08 08:42:22,333 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.2000097931921482
2025-03-08 08:42:34,132 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.19993990905582903
2025-03-08 08:42:44,692 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.19996876974900563
2025-03-08 08:42:56,120 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.19996810737997295
2025-03-08 08:43:08,338 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.1999816276729107
2025-03-08 08:43:18,693 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_4_epoch.pt
2025-03-08 08:43:30,535 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.19872716151177883
2025-03-08 08:43:41,886 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.1998416193202138
2025-03-08 08:43:53,513 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.19989043282965818
2025-03-08 08:44:05,456 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.19991598458960652
2025-03-08 08:44:17,189 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.19993252132833003
2025-03-08 08:44:27,266 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_5_epoch.pt
2025-03-08 08:44:39,900 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.2000019383430481
2025-03-08 08:44:51,602 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.19999718300998212
2025-03-08 08:45:03,256 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.19999788378675779
2025-03-08 08:45:14,833 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.19999660443514586
2025-03-08 08:45:26,539 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.19999623167514802
2025-03-08 08:45:36,633 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_6_epoch.pt
2025-03-08 08:45:48,270 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.1999886167049408
2025-03-08 08:46:00,399 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.19999271348118783
2025-03-08 08:46:11,249 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.19999179994066557
2025-03-08 08:46:23,481 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.1999936554208398
2025-03-08 08:46:35,753 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.19999188378453256
2025-03-08 08:46:45,362 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_7_epoch.pt
2025-03-08 08:46:56,826 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.19998639404773713
2025-03-08 08:47:07,417 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.19999776363372804
2025-03-08 08:47:19,949 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.199993949731191
2025-03-08 08:47:31,424 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.19999403886497022
2025-03-08 08:47:44,370 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.19999052700400352
2025-03-08 08:47:54,596 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_8_epoch.pt
2025-03-08 08:48:06,468 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.19996846586465836
2025-03-08 08:48:17,500 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.19997659228742123
2025-03-08 08:48:29,207 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.19998030617833137
2025-03-08 08:48:40,630 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.19998451333492995
2025-03-08 08:48:52,324 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.19998185768723487
2025-03-08 08:49:02,996 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_9_epoch.pt
2025-03-08 08:49:14,718 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.1999952821433544
2025-03-08 08:49:27,128 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.19998402886092662
2025-03-08 08:49:39,162 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.19992834051450092
2025-03-08 08:49:49,813 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.19755233254283666
2025-03-08 08:50:02,038 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.1926407756432891
2025-03-08 08:50:11,808 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_10_epoch.pt
2025-03-08 08:50:24,829 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.15091947186738253
2025-03-08 08:50:36,008 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.14948058251291513
2025-03-08 08:50:48,316 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.14348184317350388
2025-03-08 08:50:59,621 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.13405633958987892
2025-03-08 08:51:09,977 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.12403687002509832
2025-03-08 08:51:20,070 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_11_epoch.pt
2025-03-08 08:51:32,170 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.08271489698439836
2025-03-08 08:51:42,105 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.0791501491330564
2025-03-08 08:51:54,463 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.0804567177221179
2025-03-08 08:52:05,901 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.07969442108646035
2025-03-08 08:52:17,313 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.0766273431777954
2025-03-08 08:52:27,678 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_12_epoch.pt
2025-03-08 08:52:39,396 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.06628328897058963
2025-03-08 08:52:51,646 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.0691245874017477
2025-03-08 08:53:03,114 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.06868647493422031
2025-03-08 08:53:15,063 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.06673040525056421
2025-03-08 08:53:26,234 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.06641194474697112
2025-03-08 08:53:35,650 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_13_epoch.pt
2025-03-08 08:53:48,371 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.052384626977145674
2025-03-08 08:54:00,629 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.06113254128023982
2025-03-08 08:54:11,274 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.0641676677018404
2025-03-08 08:54:22,032 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.06554138101637363
2025-03-08 08:54:32,575 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.06715720065683127
2025-03-08 08:54:43,504 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_14_epoch.pt
2025-03-08 08:54:55,442 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.0628676486760378
2025-03-08 08:55:06,500 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.06129863040521741
2025-03-08 08:55:18,158 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.06086611824731032
2025-03-08 08:55:29,343 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.060940310126170516
2025-03-08 08:55:41,100 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.060612577192485334
2025-03-08 08:55:51,519 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_15_epoch.pt
2025-03-08 08:56:04,224 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.06464068006724119
2025-03-08 08:56:14,479 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.05468830632045865
2025-03-08 08:56:26,428 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.055107689797878265
2025-03-08 08:56:38,179 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.05587102497927845
2025-03-08 08:56:49,882 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.05628988108038902
2025-03-08 08:56:59,755 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_16_epoch.pt
2025-03-08 08:57:11,997 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.05593714110553265
2025-03-08 08:57:23,482 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.05815274115651846
2025-03-08 08:57:34,272 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.0585529645656546
2025-03-08 08:57:45,693 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.058663569930940865
2025-03-08 08:57:57,920 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.05907863248884678
2025-03-08 08:58:07,897 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_17_epoch.pt
2025-03-08 08:58:19,508 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.054018921814858914
2025-03-08 08:58:31,211 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.05687192337587476
2025-03-08 08:58:42,868 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.055795933455228805
2025-03-08 08:58:54,901 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.052800945714116095
2025-03-08 08:59:06,729 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.05363134358078241
2025-03-08 08:59:16,743 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_18_epoch.pt
2025-03-08 08:59:28,656 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.058263327553868295
2025-03-08 08:59:39,168 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.05563663089647889
2025-03-08 08:59:50,410 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.0543822480738163
2025-03-08 09:00:02,089 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.054276415379717946
2025-03-08 09:00:13,099 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.05423668042570352
2025-03-08 09:00:24,244 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_19_epoch.pt
2025-03-08 09:00:36,137 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04916549373418093
2025-03-08 09:00:47,309 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.048232913669198754
2025-03-08 09:00:58,580 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.048941129855811595
2025-03-08 09:01:09,491 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.049899564292281866
2025-03-08 09:01:21,083 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04996759048849344
2025-03-08 09:01:31,920 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_20_epoch.pt
2025-03-08 09:01:42,700 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.05877592951059341
2025-03-08 09:01:54,352 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.05480590486899018
2025-03-08 09:02:05,280 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.05394477998216947
2025-03-08 09:02:16,707 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.053788249380886555
2025-03-08 09:02:28,548 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.05215786172449589
2025-03-08 09:02:39,805 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_21_epoch.pt
2025-03-08 09:02:51,501 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.06303578063845634
2025-03-08 09:03:02,535 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.06166409119963646
2025-03-08 09:03:14,050 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.058934875838458536
2025-03-08 09:03:26,320 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.05767764354124665
2025-03-08 09:03:37,109 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.059315037421882155
2025-03-08 09:03:47,634 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_22_epoch.pt
2025-03-08 09:03:59,809 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.05707898940891028
2025-03-08 09:04:09,503 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.05342909464612603
2025-03-08 09:04:21,051 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.05359620568652948
2025-03-08 09:04:32,507 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.051893157241865995
2025-03-08 09:04:44,376 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.05152398865669966
2025-03-08 09:04:55,463 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_23_epoch.pt
2025-03-08 09:05:06,567 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.05018133252859116
2025-03-08 09:05:18,398 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.052194899413734674
2025-03-08 09:05:29,704 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.052836436616877716
2025-03-08 09:05:40,097 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.0505831309966743
2025-03-08 09:05:52,081 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.05374399296194315
2025-03-08 09:06:03,252 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_24_epoch.pt
2025-03-08 09:06:15,723 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.05714162040501833
2025-03-08 09:06:26,616 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.05289614945650101
2025-03-08 09:06:37,012 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.05022666501502196
2025-03-08 09:06:48,755 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.05102358073927462
2025-03-08 09:07:00,638 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.0493764987885952
2025-03-08 09:07:11,308 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_25_epoch.pt
2025-03-08 09:07:23,955 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.05050341840833426
2025-03-08 09:07:34,394 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.051258889231830834
2025-03-08 09:07:45,786 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.0535348225881656
2025-03-08 09:07:57,373 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.05023255217820406
2025-03-08 09:08:08,747 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.05006064357608557
2025-03-08 09:08:19,444 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_26_epoch.pt
2025-03-08 09:08:30,931 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.05580958921462297
2025-03-08 09:08:42,576 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.051471142619848254
2025-03-08 09:08:54,406 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.05399600041409334
2025-03-08 09:09:05,994 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.05210666789673269
2025-03-08 09:09:17,707 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.0520362031981349
2025-03-08 09:09:27,765 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_27_epoch.pt
2025-03-08 09:09:40,226 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.058156730979681014
2025-03-08 09:09:51,576 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.05469869744032621
2025-03-08 09:10:02,842 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.049801501606901485
2025-03-08 09:10:14,190 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.049277854105457666
2025-03-08 09:10:25,716 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.0486800438016653
2025-03-08 09:10:35,287 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_28_epoch.pt
2025-03-08 09:10:47,702 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.046173540614545344
2025-03-08 09:10:58,809 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04101364947855472
2025-03-08 09:11:09,662 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04179527396957079
2025-03-08 09:11:21,657 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04237808983772993
2025-03-08 09:11:32,554 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04487309914827347
2025-03-08 09:11:43,082 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_29_epoch.pt
2025-03-08 09:11:55,769 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.0589263154938817
2025-03-08 09:12:05,734 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.051764534562826155
2025-03-08 09:12:18,249 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.05082623974730571
2025-03-08 09:12:29,951 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.05109631640836596
2025-03-08 09:12:40,863 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.05111475292593241
2025-03-08 09:12:50,772 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_30_epoch.pt
2025-03-08 09:13:01,943 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.039536245614290234
2025-03-08 09:13:13,101 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.042665857337415215
2025-03-08 09:13:24,549 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04173954011251529
2025-03-08 09:13:36,717 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04517759776674211
2025-03-08 09:13:47,927 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.045780184127390385
2025-03-08 09:13:58,619 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_31_epoch.pt
2025-03-08 09:14:10,280 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04760041449218989
2025-03-08 09:14:20,546 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.047215265911072495
2025-03-08 09:14:32,691 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04592002358287573
2025-03-08 09:14:43,878 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04627427971921861
2025-03-08 09:14:56,460 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04760414159297943
2025-03-08 09:15:06,743 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_32_epoch.pt
2025-03-08 09:15:18,774 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.045612930096685884
2025-03-08 09:15:30,568 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.0413097245618701
2025-03-08 09:15:41,802 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.043393552315731845
2025-03-08 09:15:53,920 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.045812097331509
2025-03-08 09:16:03,568 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04496798833459616
2025-03-08 09:16:13,906 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_33_epoch.pt
2025-03-08 09:16:26,092 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.041787089072167875
2025-03-08 09:16:37,266 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.05155551554635167
2025-03-08 09:16:48,604 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.049926106048127014
2025-03-08 09:16:59,557 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.047572317933663726
2025-03-08 09:17:10,576 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04649255739152432
2025-03-08 09:17:22,017 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_34_epoch.pt
2025-03-08 09:17:33,917 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.05938808128237724
2025-03-08 09:17:44,411 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.05285555360838771
2025-03-08 09:17:56,076 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.05303815934807062
2025-03-08 09:18:08,365 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.05115656329318881
2025-03-08 09:18:18,901 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.049995730563998225
2025-03-08 09:18:28,805 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_35_epoch.pt
2025-03-08 09:18:40,986 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.049383957050740716
2025-03-08 09:18:51,797 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04741510121151805
2025-03-08 09:19:03,478 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.051106689907610414
2025-03-08 09:19:14,113 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.05070024952292442
2025-03-08 09:19:26,463 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.05087219228595495
2025-03-08 09:19:36,898 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_36_epoch.pt
2025-03-08 09:19:50,119 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.044967481531202794
2025-03-08 09:20:01,312 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.0432638399861753
2025-03-08 09:20:12,873 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04057926006615162
2025-03-08 09:20:23,782 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.043872648039832714
2025-03-08 09:20:35,031 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04578959376364947
2025-03-08 09:20:45,091 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_37_epoch.pt
2025-03-08 09:20:57,069 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04970600176602602
2025-03-08 09:21:08,668 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.0482094001211226
2025-03-08 09:21:19,846 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.043259895431498684
2025-03-08 09:21:31,813 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.044215976046398285
2025-03-08 09:21:43,483 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04496642891317606
2025-03-08 09:21:53,101 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_38_epoch.pt
2025-03-08 09:22:05,060 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.040496638193726536
2025-03-08 09:22:16,992 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.043074270877987146
2025-03-08 09:22:28,973 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04491422609736522
2025-03-08 09:22:41,199 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04608320763334632
2025-03-08 09:22:51,387 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04499634309113026
2025-03-08 09:23:00,969 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_39_epoch.pt
2025-03-08 09:23:12,614 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.05122448354959488
2025-03-08 09:23:23,721 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04862112168222666
2025-03-08 09:23:35,692 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04616484017421802
2025-03-08 09:23:47,376 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.0477159757912159
2025-03-08 09:23:57,688 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04712991219758987
2025-03-08 09:24:08,325 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_40_epoch.pt
2025-03-08 09:24:20,633 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.054524114467203615
2025-03-08 09:24:31,491 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04836269034072757
2025-03-08 09:24:42,833 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04812088758995136
2025-03-08 09:24:54,040 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.047238378217443824
2025-03-08 09:25:05,935 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04670915503054857
2025-03-08 09:25:16,095 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_41_epoch.pt
2025-03-08 09:25:27,650 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.05109423849731684
2025-03-08 09:25:39,041 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.047468815185129645
2025-03-08 09:25:50,938 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04578669855991999
2025-03-08 09:26:02,112 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04629990573041141
2025-03-08 09:26:13,817 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.044990778774023056
2025-03-08 09:26:24,061 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_42_epoch.pt
2025-03-08 09:26:36,381 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03906217601150274
2025-03-08 09:26:47,522 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.03898454360663891
2025-03-08 09:26:58,917 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04258121692885955
2025-03-08 09:27:10,243 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.043482543993741275
2025-03-08 09:27:21,892 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04496009279042482
2025-03-08 09:27:32,014 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_43_epoch.pt
2025-03-08 09:27:44,589 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04059134766459465
2025-03-08 09:27:56,809 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.046521269530057904
2025-03-08 09:28:08,370 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04481850137313207
2025-03-08 09:28:19,597 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.0445096524246037
2025-03-08 09:28:29,853 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04333732546120882
2025-03-08 09:28:40,181 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_44_epoch.pt
2025-03-08 09:28:51,718 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03851020112633705
2025-03-08 09:29:03,530 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04026565231382847
2025-03-08 09:29:14,733 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04336841080337763
2025-03-08 09:29:26,160 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.044657282531261444
2025-03-08 09:29:37,991 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.045315872684121135
2025-03-08 09:29:47,970 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_45_epoch.pt
2025-03-08 09:30:01,515 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.042067285664379596
2025-03-08 09:30:12,135 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04441331459209323
2025-03-08 09:30:23,586 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04605341845502456
2025-03-08 09:30:35,153 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.045759560149163006
2025-03-08 09:30:46,054 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04657688188552857
2025-03-08 09:30:56,246 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_46_epoch.pt
2025-03-08 09:31:08,474 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03825331073254347
2025-03-08 09:31:18,693 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.044616953898221255
2025-03-08 09:31:29,866 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.0451714201644063
2025-03-08 09:31:41,526 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04471724108792841
2025-03-08 09:31:53,455 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04484665852785111
2025-03-08 09:32:03,731 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_47_epoch.pt
2025-03-08 09:32:16,981 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.0380533767491579
2025-03-08 09:32:27,901 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04072012789547443
2025-03-08 09:32:38,559 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.041899108154078324
2025-03-08 09:32:49,638 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04272048948332667
2025-03-08 09:33:00,937 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04382871031016111
2025-03-08 09:33:11,484 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_48_epoch.pt
2025-03-08 09:33:22,652 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03410935454070568
2025-03-08 09:33:34,103 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.03943685093894601
2025-03-08 09:33:45,537 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.043128539708753424
2025-03-08 09:33:58,090 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.044239846486598254
2025-03-08 09:34:09,663 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04461677321791649
2025-03-08 09:34:19,659 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_49_epoch.pt
2025-03-08 09:34:30,564 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.045956208184361455
2025-03-08 09:34:42,407 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04522854762151837
2025-03-08 09:34:54,052 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.047262984961271286
2025-03-08 09:35:05,874 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04642916617915034
2025-03-08 09:35:16,428 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04886359822005033
2025-03-08 09:35:27,170 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_50_epoch.pt
2025-03-08 09:35:39,689 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04001430634409189
2025-03-08 09:35:50,843 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.043331300634890796
2025-03-08 09:36:02,490 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04679574817419052
2025-03-08 09:36:13,515 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.0470276736933738
2025-03-08 09:36:24,371 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04624797745794058
2025-03-08 09:36:35,409 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_51_epoch.pt
2025-03-08 09:36:47,488 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04942065272480249
2025-03-08 09:36:59,628 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04641303105279803
2025-03-08 09:37:11,300 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.046631585794190564
2025-03-08 09:37:22,453 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04546473482623696
2025-03-08 09:37:33,336 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.047220663152635095
2025-03-08 09:37:43,631 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_52_epoch.pt
2025-03-08 09:37:54,978 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04966874297708273
2025-03-08 09:38:06,940 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04395998081192374
2025-03-08 09:38:18,667 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04327697843313217
2025-03-08 09:38:31,135 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.042349149165675046
2025-03-08 09:38:42,180 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.041245804466307164
2025-03-08 09:38:51,938 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_53_epoch.pt
2025-03-08 09:39:04,666 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03191882081329823
2025-03-08 09:39:15,541 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.039851772487163546
2025-03-08 09:39:27,108 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04289374180138111
2025-03-08 09:39:38,382 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.042900195941329
2025-03-08 09:39:49,744 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.045563502676784995
2025-03-08 09:40:00,180 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_54_epoch.pt
2025-03-08 09:40:12,074 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.047470385916531084
2025-03-08 09:40:23,969 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.043622668460011484
2025-03-08 09:40:34,738 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.042622744366526605
2025-03-08 09:40:46,239 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04281419664621353
2025-03-08 09:40:57,848 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04303734307736158
2025-03-08 09:41:08,002 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_55_epoch.pt
2025-03-08 09:41:19,229 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04668429452925921
2025-03-08 09:41:30,607 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04751310588791966
2025-03-08 09:41:42,122 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04659502691278855
2025-03-08 09:41:53,303 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.044589533386752006
2025-03-08 09:42:05,125 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.046320260152220724
2025-03-08 09:42:15,991 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_56_epoch.pt
2025-03-08 09:42:27,353 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.0403571480512619
2025-03-08 09:42:39,143 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.042951215840876106
2025-03-08 09:42:49,388 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04275193056712548
2025-03-08 09:43:00,876 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04133236011490226
2025-03-08 09:43:13,564 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04081310922652483
2025-03-08 09:43:23,626 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_57_epoch.pt
2025-03-08 09:43:34,801 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.05078834991902113
2025-03-08 09:43:46,855 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04786524925380945
2025-03-08 09:43:58,140 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04465702434380849
2025-03-08 09:44:09,156 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.043781131785362956
2025-03-08 09:44:21,242 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.042711930349469185
2025-03-08 09:44:31,500 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_58_epoch.pt
2025-03-08 09:44:43,028 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03964190423488617
2025-03-08 09:44:55,432 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.03818257702514529
2025-03-08 09:45:07,047 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.03722276077916225
2025-03-08 09:45:17,609 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.039132340420037506
2025-03-08 09:45:28,608 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.039549434140324594
2025-03-08 09:45:38,702 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_59_epoch.pt
2025-03-08 09:45:49,994 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04168441269546747
2025-03-08 09:46:01,853 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.040853440016508105
2025-03-08 09:46:13,290 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04120564022411903
2025-03-08 09:46:24,413 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.038237857269123196
2025-03-08 09:46:35,555 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04129579054564238
2025-03-08 09:46:46,086 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_60_epoch.pt
2025-03-08 09:46:57,306 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04503590196371079
2025-03-08 09:47:08,622 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.051977541632950305
2025-03-08 09:47:20,796 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04996997726460298
2025-03-08 09:47:31,648 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04873063744045794
2025-03-08 09:47:43,253 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04781454150378704
2025-03-08 09:47:53,311 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_61_epoch.pt
2025-03-08 09:48:04,924 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04377081748098135
2025-03-08 09:48:16,311 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.044737935569137334
2025-03-08 09:48:28,917 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.0425099704042077
2025-03-08 09:48:39,298 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04190801015123725
2025-03-08 09:48:50,730 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04151892732828855
2025-03-08 09:49:00,937 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_62_epoch.pt
2025-03-08 09:49:12,883 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04490810073912144
2025-03-08 09:49:23,885 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04984516019001603
2025-03-08 09:49:35,487 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.047217380131284396
2025-03-08 09:49:46,705 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.0472429964877665
2025-03-08 09:49:58,504 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.044844131268560884
2025-03-08 09:50:08,889 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_63_epoch.pt
2025-03-08 09:50:20,989 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04527638122439384
2025-03-08 09:50:33,294 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.03963027602061629
2025-03-08 09:50:44,284 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.041247872114181516
2025-03-08 09:50:55,444 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04106823183596134
2025-03-08 09:51:06,276 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04204372972995043
2025-03-08 09:51:16,785 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_64_epoch.pt
2025-03-08 09:51:28,212 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.043996769078075884
2025-03-08 09:51:39,793 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04253077510744333
2025-03-08 09:51:51,200 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04607846152037382
2025-03-08 09:52:03,221 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04502698064781725
2025-03-08 09:52:15,196 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.044333679772913456
2025-03-08 09:52:25,485 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_65_epoch.pt
2025-03-08 09:52:38,424 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04883273847401142
2025-03-08 09:52:50,302 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.05007728345692158
2025-03-08 09:53:01,222 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04707582766811053
2025-03-08 09:53:11,406 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04507562738843262
2025-03-08 09:53:23,577 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.045117117844522
2025-03-08 09:53:33,148 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_66_epoch.pt
2025-03-08 09:53:45,196 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.035929818600416184
2025-03-08 09:53:56,935 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.037215694952756166
2025-03-08 09:54:07,445 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04090750665714343
2025-03-08 09:54:19,668 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.039867708990350366
2025-03-08 09:54:30,705 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.040891973719000815
2025-03-08 09:54:40,318 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_67_epoch.pt
2025-03-08 09:54:52,174 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04573275584727526
2025-03-08 09:55:03,850 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04514714553952217
2025-03-08 09:55:14,649 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04266006740430991
2025-03-08 09:55:26,458 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04283148054033518
2025-03-08 09:55:37,439 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.042694377988576886
2025-03-08 09:55:48,178 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_68_epoch.pt
2025-03-08 09:56:00,650 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.043551012836396695
2025-03-08 09:56:11,799 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04447348143905401
2025-03-08 09:56:23,199 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04290110925833384
2025-03-08 09:56:34,177 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04221114378422499
2025-03-08 09:56:45,801 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.040800586119294165
2025-03-08 09:56:56,318 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_69_epoch.pt
2025-03-08 09:57:08,654 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.043090074136853215
2025-03-08 09:57:20,111 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04303625240921974
2025-03-08 09:57:32,082 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.040996164220074814
2025-03-08 09:57:43,017 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04219168913550675
2025-03-08 09:57:53,750 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.044268633469939235
2025-03-08 09:58:03,627 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_70_epoch.pt
2025-03-08 09:58:15,746 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.043923946134746074
2025-03-08 09:58:27,562 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.046088877283036706
2025-03-08 09:58:39,295 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04418870009481907
2025-03-08 09:58:49,897 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04445220498368144
2025-03-08 09:59:00,921 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04301318345963955
2025-03-08 09:59:11,198 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_71_epoch.pt
2025-03-08 09:59:23,206 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04698402501642704
2025-03-08 09:59:33,814 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04700061047449708
2025-03-08 09:59:45,876 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04855158184965452
2025-03-08 09:59:57,608 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04645268601365388
2025-03-08 10:00:09,116 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04438749849051237
2025-03-08 10:00:19,186 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_72_epoch.pt
2025-03-08 10:00:30,640 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.045509341210126876
2025-03-08 10:00:42,741 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04588153829798102
2025-03-08 10:00:53,071 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04296355986346801
2025-03-08 10:01:04,729 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.040832167863845824
2025-03-08 10:01:17,397 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04104993586987257
2025-03-08 10:01:27,103 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_73_epoch.pt
2025-03-08 10:01:38,535 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04631581202149391
2025-03-08 10:01:49,943 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04171733412891626
2025-03-08 10:02:00,731 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.041641444464524585
2025-03-08 10:02:13,137 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04333119933493435
2025-03-08 10:02:24,172 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04475406166166067
2025-03-08 10:02:34,783 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_74_epoch.pt
2025-03-08 10:02:47,005 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.030074092261493206
2025-03-08 10:02:58,382 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.03293613251298666
2025-03-08 10:03:08,785 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.03632471515486638
2025-03-08 10:03:20,929 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.03898559276945889
2025-03-08 10:03:31,709 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.039346225686371326
2025-03-08 10:03:42,065 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_75_epoch.pt
2025-03-08 10:03:53,487 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.0406055823713541
2025-03-08 10:04:05,305 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04722733523696661
2025-03-08 10:04:15,918 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04688450651864211
2025-03-08 10:04:27,146 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.047626396855339405
2025-03-08 10:04:39,084 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.046692047640681265
2025-03-08 10:04:49,443 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_76_epoch.pt
2025-03-08 10:05:03,082 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04140631690621376
2025-03-08 10:05:14,875 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.042770153004676105
2025-03-08 10:05:25,257 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.041914276890456674
2025-03-08 10:05:37,056 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04216966796666384
2025-03-08 10:05:46,978 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.042387590654194354
2025-03-08 10:05:57,737 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_77_epoch.pt
2025-03-08 10:06:10,309 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.053695650845766066
2025-03-08 10:06:20,923 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.050676792468875645
2025-03-08 10:06:32,452 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.05095832330485185
2025-03-08 10:06:45,365 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04844916612841189
2025-03-08 10:06:56,270 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.046881827272474766
2025-03-08 10:07:05,400 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_78_epoch.pt
2025-03-08 10:07:16,942 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03745538052171469
2025-03-08 10:07:29,130 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.044017892330884933
2025-03-08 10:07:40,071 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04125275964538256
2025-03-08 10:07:51,177 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.042703114384785294
2025-03-08 10:08:03,099 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04258844847977161
2025-03-08 10:08:13,336 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_79_epoch.pt
2025-03-08 10:08:25,052 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.0470120370388031
2025-03-08 10:08:36,779 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04835102019831538
2025-03-08 10:08:47,965 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04457715379695098
2025-03-08 10:08:59,532 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.0432699778303504
2025-03-08 10:09:10,708 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.0426585887670517
2025-03-08 10:09:20,461 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_80_epoch.pt
2025-03-08 10:09:32,309 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04885745152831077
2025-03-08 10:09:44,195 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.048554418571293355
2025-03-08 10:09:55,457 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.047293669680754345
2025-03-08 10:10:06,620 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04558934049680829
2025-03-08 10:10:17,936 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04367869506031275
2025-03-08 10:10:28,288 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_81_epoch.pt
2025-03-08 10:10:41,704 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.043963351659476756
2025-03-08 10:10:53,194 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04083606684580445
2025-03-08 10:11:04,492 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04198097379257282
2025-03-08 10:11:15,843 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.043288469528779384
2025-03-08 10:11:26,789 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04328617732971907
2025-03-08 10:11:36,839 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_82_epoch.pt
2025-03-08 10:11:48,736 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04866590209305286
2025-03-08 10:12:00,139 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.045522321946918964
2025-03-08 10:12:11,021 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.044994868226349356
2025-03-08 10:12:21,372 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04296273465268314
2025-03-08 10:12:32,529 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04170744890719652
2025-03-08 10:12:44,210 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_83_epoch.pt
2025-03-08 10:12:55,808 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.0494593595713377
2025-03-08 10:13:07,903 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.05176178263500333
2025-03-08 10:13:19,795 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04819727023442586
2025-03-08 10:13:31,100 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04749954292550683
2025-03-08 10:13:42,418 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04652091098576784
2025-03-08 10:13:52,308 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_84_epoch.pt
2025-03-08 10:14:04,338 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03795915018767118
2025-03-08 10:14:15,568 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.03952790753915906
2025-03-08 10:14:26,802 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04165151178836823
2025-03-08 10:14:38,971 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.040676288874819874
2025-03-08 10:14:49,898 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04152017406374216
2025-03-08 10:14:59,918 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_85_epoch.pt
2025-03-08 10:15:12,309 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04206326134502888
2025-03-08 10:15:24,182 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.042167494520545
2025-03-08 10:15:35,352 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04219111277411381
2025-03-08 10:15:46,909 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.041167605621740225
2025-03-08 10:15:58,415 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04101089563965797
2025-03-08 10:16:08,563 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_86_epoch.pt
2025-03-08 10:16:19,801 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03420711651444435
2025-03-08 10:16:32,012 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.037263770829886196
2025-03-08 10:16:43,781 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.037997395458320773
2025-03-08 10:16:54,616 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.036503634843975306
2025-03-08 10:17:05,577 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.03707014347612858
2025-03-08 10:17:16,351 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_87_epoch.pt
2025-03-08 10:17:27,711 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04006092514842749
2025-03-08 10:17:39,731 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04304267583414912
2025-03-08 10:17:50,710 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04353072967380285
2025-03-08 10:18:01,265 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04269446461461485
2025-03-08 10:18:12,568 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04439982747286558
2025-03-08 10:18:23,901 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_88_epoch.pt
2025-03-08 10:18:35,926 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03961432430893183
2025-03-08 10:18:47,978 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.041376973632723094
2025-03-08 10:18:59,621 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.042214538405338925
2025-03-08 10:19:10,420 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04172414408996701
2025-03-08 10:19:21,905 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04214334317296743
2025-03-08 10:19:31,633 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_89_epoch.pt
2025-03-08 10:19:43,321 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.0451634119078517
2025-03-08 10:19:54,830 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.045649372711777686
2025-03-08 10:20:05,680 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04651437729597092
2025-03-08 10:20:17,176 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04558782008476556
2025-03-08 10:20:28,742 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04372339271754026
2025-03-08 10:20:39,889 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_90_epoch.pt
2025-03-08 10:20:52,009 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.051759234368801116
2025-03-08 10:21:03,407 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04447653017938137
2025-03-08 10:21:14,856 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.044256959532697995
2025-03-08 10:21:26,747 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04404733878560364
2025-03-08 10:21:37,318 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04426004619151354
2025-03-08 10:21:47,791 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_91_epoch.pt
2025-03-08 10:22:00,655 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04205716848373413
2025-03-08 10:22:12,069 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04115879181772471
2025-03-08 10:22:23,694 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.038486864057679974
2025-03-08 10:22:34,710 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.036894404878839854
2025-03-08 10:22:45,844 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.038610098600387575
2025-03-08 10:22:55,830 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_92_epoch.pt
2025-03-08 10:23:08,006 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04313705675303936
2025-03-08 10:23:19,321 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04048056153580546
2025-03-08 10:23:30,797 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04199241250753403
2025-03-08 10:23:41,568 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.044584270613268015
2025-03-08 10:23:52,894 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.043304889857769015
2025-03-08 10:24:03,639 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_93_epoch.pt
2025-03-08 10:24:15,130 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04007811293005943
2025-03-08 10:24:25,925 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.03674771936610341
2025-03-08 10:24:38,442 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.03731691633661588
2025-03-08 10:24:50,277 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.039931374629959465
2025-03-08 10:25:01,234 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.039913965910673144
2025-03-08 10:25:11,804 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_94_epoch.pt
2025-03-08 10:25:23,896 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.036812325343489646
2025-03-08 10:25:36,205 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.03819623464718461
2025-03-08 10:25:46,788 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.036340856192012626
2025-03-08 10:25:57,802 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.0371418141014874
2025-03-08 10:26:08,605 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.0393402763903141
2025-03-08 10:26:19,705 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_95_epoch.pt
2025-03-08 10:26:31,528 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04385515127331018
2025-03-08 10:26:43,460 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.03865194508805871
2025-03-08 10:26:54,463 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04158169254660606
2025-03-08 10:27:05,778 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.042684560138732196
2025-03-08 10:27:17,286 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.0426546301022172
2025-03-08 10:27:27,730 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_96_epoch.pt
2025-03-08 10:27:39,478 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04265046048909426
2025-03-08 10:27:51,259 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.037462210543453695
2025-03-08 10:28:02,240 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.03749164874354998
2025-03-08 10:28:13,405 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.03773423614911735
2025-03-08 10:28:24,742 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.03905134274065494
2025-03-08 10:28:35,901 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_97_epoch.pt
2025-03-08 10:28:48,886 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.036977635584771636
2025-03-08 10:29:00,295 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.038807981349527836
2025-03-08 10:29:12,049 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.0418697897096475
2025-03-08 10:29:23,216 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04161742799915373
2025-03-08 10:29:33,949 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04147757424414158
2025-03-08 10:29:43,615 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_98_epoch.pt
2025-03-08 10:29:55,077 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.044120515212416646
2025-03-08 10:30:06,306 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.03895374588668346
2025-03-08 10:30:17,428 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.0370090027526021
2025-03-08 10:30:28,310 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04040514984168112
2025-03-08 10:30:41,344 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.040359358370304105
2025-03-08 10:30:51,452 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_99_epoch.pt
2025-03-08 10:31:03,902 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03858576171100139
2025-03-08 10:31:15,090 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.0416894088126719
2025-03-08 10:31:26,586 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04556544113904238
2025-03-08 10:31:37,722 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.045858239056542516
2025-03-08 10:31:48,883 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.045671663872897623
2025-03-08 10:31:59,657 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_100_epoch.pt
2025-03-08 10:32:11,466 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03279185611754656
2025-03-08 10:32:22,783 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.040356991402804854
2025-03-08 10:32:33,909 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04052927124003569
2025-03-08 10:32:45,172 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04145055403932929
2025-03-08 10:32:56,405 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.03964611198753119
2025-03-08 10:33:06,982 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_101_epoch.pt
2025-03-08 10:33:19,706 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04642975762486458
2025-03-08 10:33:31,328 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.042435351908206936
2025-03-08 10:33:42,705 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04022822527835766
2025-03-08 10:33:54,605 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.03932072293944657
2025-03-08 10:34:05,526 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.03840656041353941
2025-03-08 10:34:14,644 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_102_epoch.pt
2025-03-08 10:34:26,212 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.045741212889552116
2025-03-08 10:34:37,774 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.036793847046792505
2025-03-08 10:34:49,877 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.03762383211404085
2025-03-08 10:35:01,296 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.038074457785114646
2025-03-08 10:35:12,769 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.037951328136026856
2025-03-08 10:35:22,938 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_103_epoch.pt
2025-03-08 10:35:35,002 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04618703734129667
2025-03-08 10:35:46,288 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.038753746189177034
2025-03-08 10:35:57,037 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04149824255456527
2025-03-08 10:36:09,740 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04001938872970641
2025-03-08 10:36:20,893 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.040702033817768095
2025-03-08 10:36:30,903 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_104_epoch.pt
2025-03-08 10:36:42,725 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03140106592327356
2025-03-08 10:36:54,646 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.039515027310699224
2025-03-08 10:37:05,944 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.038271489950517816
2025-03-08 10:37:16,906 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.03956127362325788
2025-03-08 10:37:28,587 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.039232940763235094
2025-03-08 10:37:38,798 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_105_epoch.pt
2025-03-08 10:37:51,019 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.032639209963381294
2025-03-08 10:38:03,193 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.03252630613744259
2025-03-08 10:38:13,966 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.03639134009679158
2025-03-08 10:38:25,285 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.039567062435671686
2025-03-08 10:38:36,676 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.039892406418919564
2025-03-08 10:38:46,762 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_106_epoch.pt
2025-03-08 10:38:59,147 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.047370655685663225
2025-03-08 10:39:10,138 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.050646502878516915
2025-03-08 10:39:21,472 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.046526097878813746
2025-03-08 10:39:33,558 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04350406844168901
2025-03-08 10:39:45,088 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04226817437261343
2025-03-08 10:39:55,075 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_107_epoch.pt
2025-03-08 10:40:06,622 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.047532778568565845
2025-03-08 10:40:17,654 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.041483378633856774
2025-03-08 10:40:28,388 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.042481845828394094
2025-03-08 10:40:41,252 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04200604122132063
2025-03-08 10:40:52,888 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04173414760828018
2025-03-08 10:41:02,609 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_108_epoch.pt
2025-03-08 10:41:14,334 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04220400031656027
2025-03-08 10:41:24,606 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.03849869236350059
2025-03-08 10:41:36,560 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.03930703952908516
2025-03-08 10:41:49,008 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04178784952498973
2025-03-08 10:42:00,021 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.041601955346763135
2025-03-08 10:42:11,022 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_109_epoch.pt
2025-03-08 10:42:21,675 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03649850483983755
2025-03-08 10:42:32,500 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04133423741906881
2025-03-08 10:42:44,377 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.040162379294633864
2025-03-08 10:42:57,072 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.038375914916396144
2025-03-08 10:43:08,496 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.03865376222878694
2025-03-08 10:43:18,131 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_110_epoch.pt
2025-03-08 10:43:30,549 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04472718089818954
2025-03-08 10:43:41,211 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04004333285614848
2025-03-08 10:43:52,892 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.037349699301024275
2025-03-08 10:44:04,061 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.03838052792474628
2025-03-08 10:44:14,973 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.037179014287889
2025-03-08 10:44:25,585 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_111_epoch.pt
2025-03-08 10:44:36,808 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03747352164238691
2025-03-08 10:44:47,903 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.03433112423866987
2025-03-08 10:44:58,565 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.035953043525417644
2025-03-08 10:45:10,117 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.03337057179771364
2025-03-08 10:45:22,581 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.03462867308408022
2025-03-08 10:45:33,186 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_112_epoch.pt
2025-03-08 10:45:46,433 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.031784475035965445
2025-03-08 10:45:57,387 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.03883618786931038
2025-03-08 10:46:09,376 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.03852398633956909
2025-03-08 10:46:20,210 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.03973214279860258
2025-03-08 10:46:31,901 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04216270178556442
2025-03-08 10:46:41,259 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_113_epoch.pt
2025-03-08 10:46:53,055 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.049361273646354675
2025-03-08 10:47:03,893 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04597118170931935
2025-03-08 10:47:15,853 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04470614707718293
2025-03-08 10:47:26,808 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04067904245108366
2025-03-08 10:47:39,028 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04217583390325308
2025-03-08 10:47:49,050 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_114_epoch.pt
2025-03-08 10:48:00,589 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.05108328580856323
2025-03-08 10:48:12,226 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04032977825030684
2025-03-08 10:48:24,104 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.043236991725862026
2025-03-08 10:48:34,579 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04447008950635791
2025-03-08 10:48:46,527 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.043288478136062625
2025-03-08 10:48:57,091 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_115_epoch.pt
2025-03-08 10:49:10,106 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.037028677277266976
2025-03-08 10:49:21,264 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.03929683720692992
2025-03-08 10:49:33,348 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.038384504082302255
2025-03-08 10:49:44,829 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04133260959759354
2025-03-08 10:49:55,730 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04230638314783573
2025-03-08 10:50:05,595 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_116_epoch.pt
2025-03-08 10:50:17,694 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03937649339437485
2025-03-08 10:50:28,360 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.038741319067776205
2025-03-08 10:50:39,811 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.03907039973884821
2025-03-08 10:50:51,092 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.03931825248524547
2025-03-08 10:51:01,803 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.041190747074782846
2025-03-08 10:51:12,981 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_117_epoch.pt
2025-03-08 10:51:25,090 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.038635566122829916
2025-03-08 10:51:36,694 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.03675627620890737
2025-03-08 10:51:47,668 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04038932554423809
2025-03-08 10:51:59,754 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04128062766045332
2025-03-08 10:52:10,364 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.042406214587390426
2025-03-08 10:52:20,899 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_118_epoch.pt
2025-03-08 10:52:32,537 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.044099167101085184
2025-03-08 10:52:44,374 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04052900020033121
2025-03-08 10:52:56,140 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.03913124692936738
2025-03-08 10:53:07,367 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.03883360505104065
2025-03-08 10:53:18,705 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.037451800033450125
2025-03-08 10:53:28,924 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_119_epoch.pt
2025-03-08 10:53:40,607 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03802282679826021
2025-03-08 10:53:52,385 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04553355345502496
2025-03-08 10:54:03,540 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.043312112440665566
2025-03-08 10:54:15,576 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04663525444455445
2025-03-08 10:54:26,985 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.0451000676676631
2025-03-08 10:54:36,846 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_120_epoch.pt
2025-03-08 10:54:48,469 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03718432024121285
2025-03-08 10:54:59,763 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04004578996449709
2025-03-08 10:55:12,218 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.03982361372560263
2025-03-08 10:55:23,183 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.03940180162899196
2025-03-08 10:55:34,903 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.03808606480807066
2025-03-08 10:55:44,805 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_121_epoch.pt
2025-03-08 10:55:56,174 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.042632730267941954
2025-03-08 10:56:07,538 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04002529172226787
2025-03-08 10:56:18,489 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04027609433978796
2025-03-08 10:56:30,509 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04165229476056993
2025-03-08 10:56:41,388 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.042432123050093654
2025-03-08 10:56:52,091 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_122_epoch.pt
2025-03-08 10:57:04,440 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03748239893466234
2025-03-08 10:57:15,886 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.03941153097897768
2025-03-08 10:57:27,373 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.041128620157639184
2025-03-08 10:57:38,552 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.041924153454601765
2025-03-08 10:57:49,529 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04145246322453022
2025-03-08 10:57:59,813 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_123_epoch.pt
2025-03-08 10:58:11,051 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03842118065804243
2025-03-08 10:58:22,635 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04502440545707941
2025-03-08 10:58:35,002 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.044903727062046525
2025-03-08 10:58:46,688 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04506131264381111
2025-03-08 10:58:58,154 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04598971467465162
2025-03-08 10:59:07,655 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_124_epoch.pt
2025-03-08 10:59:20,183 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04167071271687746
2025-03-08 10:59:30,563 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04338141903281212
2025-03-08 10:59:41,996 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.044341280224422616
2025-03-08 10:59:54,205 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04294361581094563
2025-03-08 11:00:05,215 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.041904231257736686
2025-03-08 11:00:15,307 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_125_epoch.pt
2025-03-08 11:00:26,793 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.0434770929813385
2025-03-08 11:00:38,886 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04098184585571289
2025-03-08 11:00:50,580 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04086960837244988
2025-03-08 11:01:01,169 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04083181199617684
2025-03-08 11:01:12,039 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.040707408718764784
2025-03-08 11:01:22,664 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_126_epoch.pt
2025-03-08 11:01:34,103 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04495820727199316
2025-03-08 11:01:45,684 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.048311875332146884
2025-03-08 11:01:56,985 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04755004322777192
2025-03-08 11:02:08,131 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04608514460735023
2025-03-08 11:02:19,817 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.0437288768813014
2025-03-08 11:02:30,165 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_127_epoch.pt
2025-03-08 11:02:42,102 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04211017161607742
2025-03-08 11:02:53,363 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04474568113684654
2025-03-08 11:03:05,263 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04654837689052026
2025-03-08 11:03:16,668 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04482252437621355
2025-03-08 11:03:27,940 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04239124285429716
2025-03-08 11:03:38,254 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_128_epoch.pt
2025-03-08 11:03:49,197 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04061373647302389
2025-03-08 11:04:00,830 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04268448730930686
2025-03-08 11:04:12,533 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.043112970292568206
2025-03-08 11:04:24,382 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04435694850981235
2025-03-08 11:04:36,388 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04225228242576122
2025-03-08 11:04:45,875 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_129_epoch.pt
2025-03-08 11:04:57,313 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03956842090934515
2025-03-08 11:05:09,710 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.0392933395318687
2025-03-08 11:05:20,899 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04062810452034076
2025-03-08 11:05:33,190 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.038867799686267974
2025-03-08 11:05:44,233 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.03871559026837349
2025-03-08 11:05:53,657 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_130_epoch.pt
2025-03-08 11:06:05,459 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.05112834103405475
2025-03-08 11:06:15,682 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04270161975175142
2025-03-08 11:06:27,496 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.0425709522763888
2025-03-08 11:06:38,741 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.042134072715416554
2025-03-08 11:06:50,834 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04104036889970303
2025-03-08 11:07:01,625 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_131_epoch.pt
2025-03-08 11:07:13,398 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04192595660686493
2025-03-08 11:07:23,736 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04619565723463893
2025-03-08 11:07:35,618 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.042560596118370694
2025-03-08 11:07:47,293 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.043289873814210296
2025-03-08 11:07:58,709 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.043612865149974825
2025-03-08 11:08:09,494 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_132_epoch.pt
2025-03-08 11:08:21,200 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03416627336293459
2025-03-08 11:08:32,692 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.03775294002145529
2025-03-08 11:08:44,610 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.038280538469553
2025-03-08 11:08:55,964 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.03926268933340907
2025-03-08 11:09:06,809 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.040324112102389335
2025-03-08 11:09:17,074 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_133_epoch.pt
2025-03-08 11:09:28,891 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04000408850610256
2025-03-08 11:09:39,252 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.042196949869394304
2025-03-08 11:09:50,780 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04111259096612533
2025-03-08 11:10:03,151 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04146032014861703
2025-03-08 11:10:14,817 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04016826821118593
2025-03-08 11:10:25,386 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_134_epoch.pt
2025-03-08 11:10:37,678 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.050943890511989595
2025-03-08 11:10:49,203 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04200733976438641
2025-03-08 11:11:00,153 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04274364447842042
2025-03-08 11:11:11,089 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04182962647639215
2025-03-08 11:11:22,865 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04164557503163815
2025-03-08 11:11:33,344 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_135_epoch.pt
2025-03-08 11:11:46,137 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03438568845391274
2025-03-08 11:11:57,406 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04214977946132421
2025-03-08 11:12:08,498 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04184423816700777
2025-03-08 11:12:20,621 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04076007856987417
2025-03-08 11:12:31,100 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04268714201450348
2025-03-08 11:12:42,080 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_136_epoch.pt
2025-03-08 11:12:55,348 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.033795122802257535
2025-03-08 11:13:06,471 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04596931299194693
2025-03-08 11:13:18,358 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04598670264085134
2025-03-08 11:13:28,662 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04699378727003932
2025-03-08 11:13:40,175 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04594627859443426
2025-03-08 11:13:50,213 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_137_epoch.pt
2025-03-08 11:14:02,599 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.05347592607140541
2025-03-08 11:14:13,304 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04621884290128946
2025-03-08 11:14:24,793 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.044949797851343946
2025-03-08 11:14:35,705 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04490264130756259
2025-03-08 11:14:47,141 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04400487876683474
2025-03-08 11:14:58,262 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_138_epoch.pt
2025-03-08 11:15:10,294 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04538039434701204
2025-03-08 11:15:21,660 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04100072925910354
2025-03-08 11:15:32,175 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.041176362956563635
2025-03-08 11:15:43,418 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.040935125388205054
2025-03-08 11:15:54,894 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04097124420106411
2025-03-08 11:16:05,752 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_139_epoch.pt
2025-03-08 11:16:19,140 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.041966289728879926
2025-03-08 11:16:30,706 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04213288372382522
2025-03-08 11:16:41,907 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04394405952344338
2025-03-08 11:16:52,942 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04394134949892759
2025-03-08 11:17:03,470 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04221185079962015
2025-03-08 11:17:14,514 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_140_epoch.pt
2025-03-08 11:17:25,700 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.041594080962240695
2025-03-08 11:17:37,008 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.039885354693979025
2025-03-08 11:17:49,488 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.040545486211776734
2025-03-08 11:18:00,415 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.040642230696976187
2025-03-08 11:18:12,000 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.041680351331830026
2025-03-08 11:18:21,618 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_141_epoch.pt
2025-03-08 11:18:33,095 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04069301526993513
2025-03-08 11:18:44,638 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04567430693656206
2025-03-08 11:18:55,344 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04365619784841935
2025-03-08 11:19:07,226 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.042409225283190605
2025-03-08 11:19:18,651 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04141738034039736
2025-03-08 11:19:29,445 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_142_epoch.pt
2025-03-08 11:19:41,769 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03982948642224073
2025-03-08 11:19:53,740 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04458018904551864
2025-03-08 11:20:04,946 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.041919506279130776
2025-03-08 11:20:17,086 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04233953836373985
2025-03-08 11:20:28,414 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04058522047847509
2025-03-08 11:20:37,988 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_143_epoch.pt
2025-03-08 11:20:50,536 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04120733488351107
2025-03-08 11:21:01,981 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04339433567598462
2025-03-08 11:21:12,752 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04334593914449215
2025-03-08 11:21:24,112 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04349311610683799
2025-03-08 11:21:35,884 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.044232126876711846
2025-03-08 11:21:45,570 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_144_epoch.pt
2025-03-08 11:21:57,314 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.040049327984452245
2025-03-08 11:22:08,261 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04143275754526257
2025-03-08 11:22:19,660 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.040376486169795195
2025-03-08 11:22:31,095 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.03861921285279095
2025-03-08 11:22:41,692 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.039344447411596775
2025-03-08 11:22:53,345 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_145_epoch.pt
2025-03-08 11:23:05,230 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04357918277382851
2025-03-08 11:23:17,209 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.03902949050068855
2025-03-08 11:23:28,346 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04074017534653346
2025-03-08 11:23:40,193 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04285478848032653
2025-03-08 11:23:50,782 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04236672300100326
2025-03-08 11:24:00,878 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_146_epoch.pt
2025-03-08 11:24:13,205 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03941118776798248
2025-03-08 11:24:24,813 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.03948524253442884
2025-03-08 11:24:36,380 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04040739248196284
2025-03-08 11:24:48,275 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.040916794762015346
2025-03-08 11:24:59,334 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04030748508870602
2025-03-08 11:25:08,675 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_147_epoch.pt
2025-03-08 11:25:20,337 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04426296904683113
2025-03-08 11:25:31,628 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04281606802716851
2025-03-08 11:25:43,137 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.0389386094858249
2025-03-08 11:25:56,060 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.03959545839577913
2025-03-08 11:26:07,275 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.03967292872071266
2025-03-08 11:26:16,813 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_148_epoch.pt
2025-03-08 11:26:28,226 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.044689142554998396
2025-03-08 11:26:39,014 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.047774505224078896
2025-03-08 11:26:50,577 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04510882973670959
2025-03-08 11:27:02,533 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.043767685564234854
2025-03-08 11:27:13,972 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.044655755765736105
2025-03-08 11:27:24,418 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_149_epoch.pt
2025-03-08 11:27:36,020 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.040964827984571454
2025-03-08 11:27:47,312 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.042628461960703135
2025-03-08 11:27:59,286 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04104585104932388
2025-03-08 11:28:10,633 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.040138414781540635
2025-03-08 11:28:21,431 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.03857289668917656
2025-03-08 11:28:32,071 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_150_epoch.pt
2025-03-08 11:28:43,251 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.041929186061024665
2025-03-08 11:28:54,920 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.043669013418257234
2025-03-08 11:29:05,788 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04060980144888163
2025-03-08 11:29:17,475 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.039618634283542634
2025-03-08 11:29:28,533 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.03949106165766716
2025-03-08 11:29:39,198 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_151_epoch.pt
2025-03-08 11:29:51,194 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.038896060809493066
2025-03-08 11:30:02,395 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.03959403863176703
2025-03-08 11:30:13,860 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.040191750712692736
2025-03-08 11:30:25,106 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.03815417481586337
2025-03-08 11:30:36,506 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04016265320032835
2025-03-08 11:30:47,094 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_152_epoch.pt
2025-03-08 11:30:59,345 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03977249063551426
2025-03-08 11:31:10,407 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04725856501609087
2025-03-08 11:31:22,274 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04646787873158852
2025-03-08 11:31:33,634 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.046826346404850484
2025-03-08 11:31:44,459 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04631715626269579
2025-03-08 11:31:54,691 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_153_epoch.pt
2025-03-08 11:32:06,900 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.046068965829908846
2025-03-08 11:32:18,661 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.0424415472894907
2025-03-08 11:32:29,974 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04011612142125765
2025-03-08 11:32:41,345 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.03703650748357177
2025-03-08 11:32:51,966 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.038982476390898226
2025-03-08 11:33:03,060 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_154_epoch.pt
2025-03-08 11:33:14,814 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03538398083299398
2025-03-08 11:33:26,365 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.03874742239713669
2025-03-08 11:33:37,969 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.039054310160378615
2025-03-08 11:33:49,226 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.03930276135914028
2025-03-08 11:34:00,328 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.0396928371861577
2025-03-08 11:34:10,942 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_155_epoch.pt
2025-03-08 11:34:22,192 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.037608376257121566
2025-03-08 11:34:33,504 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.03930130638182163
2025-03-08 11:34:45,034 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.040183435020347436
2025-03-08 11:34:55,893 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04020122024230659
2025-03-08 11:35:08,402 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04125665439665317
2025-03-08 11:35:19,125 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_156_epoch.pt
2025-03-08 11:35:31,174 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.044462077841162684
2025-03-08 11:35:42,000 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04086838375777006
2025-03-08 11:35:53,171 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.03815822631120682
2025-03-08 11:36:04,975 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.038546302299946544
2025-03-08 11:36:17,124 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.03918121077120304
2025-03-08 11:36:27,364 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_157_epoch.pt
2025-03-08 11:36:38,870 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03414163187146187
2025-03-08 11:36:50,651 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.037646249309182166
2025-03-08 11:37:01,722 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.03841804613669713
2025-03-08 11:37:13,059 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.03848926811479032
2025-03-08 11:37:24,562 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.03636720184236765
2025-03-08 11:37:34,765 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_158_epoch.pt
2025-03-08 11:37:46,463 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04356946427375078
2025-03-08 11:37:58,198 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04245397176593542
2025-03-08 11:38:10,092 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.042994070624311764
2025-03-08 11:38:20,746 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04180605165660381
2025-03-08 11:38:32,229 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04091812215745449
2025-03-08 11:38:42,251 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_159_epoch.pt
2025-03-08 11:38:53,927 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.046140543185174465
2025-03-08 11:39:06,178 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.03894128730520606
2025-03-08 11:39:16,818 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.0399198201050361
2025-03-08 11:39:27,775 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.03898761412128806
2025-03-08 11:39:40,412 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.03992086626589298
2025-03-08 11:39:50,425 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_160_epoch.pt
2025-03-08 11:40:02,044 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.033505235016345974
2025-03-08 11:40:14,080 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.03802716435864568
2025-03-08 11:40:25,822 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.037752431705594065
2025-03-08 11:40:36,697 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.03902686310000718
2025-03-08 11:40:48,721 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.03992012990266085
2025-03-08 11:40:58,888 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_161_epoch.pt
2025-03-08 11:41:10,070 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03302816636860371
2025-03-08 11:41:22,218 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.03698295522481203
2025-03-08 11:41:34,431 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.03814339390645424
2025-03-08 11:41:44,892 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.037663308968767524
2025-03-08 11:41:56,709 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.03866907440125942
2025-03-08 11:42:06,951 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_162_epoch.pt
2025-03-08 11:42:19,315 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03775019720196724
2025-03-08 11:42:30,957 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.03515214383602142
2025-03-08 11:42:42,091 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.03382609662910303
2025-03-08 11:42:53,412 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.035645006764680146
2025-03-08 11:43:04,343 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.03721111488342285
2025-03-08 11:43:14,118 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_163_epoch.pt
2025-03-08 11:43:26,599 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04174592461436987
2025-03-08 11:43:37,497 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.044401806890964506
2025-03-08 11:43:49,074 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04312774131695429
2025-03-08 11:43:59,954 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04425774513743818
2025-03-08 11:44:11,441 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04324207777529955
2025-03-08 11:44:22,398 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_164_epoch.pt
2025-03-08 11:44:35,340 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03484356265515089
2025-03-08 11:44:46,312 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.03686542926356196
2025-03-08 11:44:57,870 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.038251160110036535
2025-03-08 11:45:09,139 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.040421650148928165
2025-03-08 11:45:20,441 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04046817720681429
2025-03-08 11:45:30,558 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_165_epoch.pt
2025-03-08 11:45:42,857 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.042471438758075235
2025-03-08 11:45:54,741 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04025127476081252
2025-03-08 11:46:06,117 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04208441251267989
2025-03-08 11:46:17,485 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04097795478068292
2025-03-08 11:46:28,863 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.040215806394815444
2025-03-08 11:46:38,844 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_166_epoch.pt
2025-03-08 11:46:50,175 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.0378563292697072
2025-03-08 11:47:01,514 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.042649641390889885
2025-03-08 11:47:12,510 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04001926970978578
2025-03-08 11:47:25,551 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.040326529927551744
2025-03-08 11:47:36,541 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04186162056773901
2025-03-08 11:47:47,207 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_167_epoch.pt
2025-03-08 11:47:59,614 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03584037933498621
2025-03-08 11:48:10,215 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04337620481848717
2025-03-08 11:48:21,571 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.0406509085247914
2025-03-08 11:48:33,551 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.040184204513207075
2025-03-08 11:48:45,014 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04267451829463244
2025-03-08 11:48:55,145 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_168_epoch.pt
2025-03-08 11:49:07,370 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04380185931921005
2025-03-08 11:49:19,000 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04393639415502548
2025-03-08 11:49:30,184 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04198720884819825
2025-03-08 11:49:41,413 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04317155778408051
2025-03-08 11:49:53,143 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04471596546471119
2025-03-08 11:50:03,220 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_169_epoch.pt
2025-03-08 11:50:15,553 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04340616710484028
2025-03-08 11:50:26,615 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.042417136952281
2025-03-08 11:50:38,060 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04370616439729929
2025-03-08 11:50:49,313 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04234807980246842
2025-03-08 11:50:59,856 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.041983557268977165
2025-03-08 11:51:10,228 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_170_epoch.pt
2025-03-08 11:51:21,299 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04711535293608904
2025-03-08 11:51:32,658 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04228895386680961
2025-03-08 11:51:45,445 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04268099044760068
2025-03-08 11:51:56,649 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04182435378432274
2025-03-08 11:52:07,997 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04282560925185681
2025-03-08 11:52:18,680 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_171_epoch.pt
2025-03-08 11:52:30,587 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.040875914432108404
2025-03-08 11:52:41,444 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.0426156566105783
2025-03-08 11:52:53,268 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.040354037607709566
2025-03-08 11:53:04,886 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04147642548196018
2025-03-08 11:53:15,968 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.0406029729694128
2025-03-08 11:53:26,137 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_172_epoch.pt
2025-03-08 11:53:38,944 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04758940957486629
2025-03-08 11:53:49,930 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04678671471774578
2025-03-08 11:54:01,758 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.043502230892578764
2025-03-08 11:54:13,685 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04268347182311118
2025-03-08 11:54:24,737 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04274587990343571
2025-03-08 11:54:34,166 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_173_epoch.pt
2025-03-08 11:54:46,283 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.038664155080914496
2025-03-08 11:54:57,016 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.039045743066817525
2025-03-08 11:55:09,018 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.040836393361290295
2025-03-08 11:55:20,209 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.039297985266894105
2025-03-08 11:55:31,636 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.03807892385870218
2025-03-08 11:55:42,377 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_174_epoch.pt
2025-03-08 11:55:54,365 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04076090853661299
2025-03-08 11:56:06,240 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.03927040796726942
2025-03-08 11:56:18,021 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.037101526347299414
2025-03-08 11:56:29,567 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.03913595121353865
2025-03-08 11:56:40,316 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.03778962668776512
2025-03-08 11:56:50,275 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_175_epoch.pt
2025-03-08 11:57:02,621 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.05161779548972845
2025-03-08 11:57:14,721 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04716931603848934
2025-03-08 11:57:25,676 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.044970538566509885
2025-03-08 11:57:36,614 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04359312585555017
2025-03-08 11:57:47,978 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04219305501133203
2025-03-08 11:57:58,072 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_176_epoch.pt
2025-03-08 11:58:10,215 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04268874567002058
2025-03-08 11:58:22,091 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.041489542052149775
2025-03-08 11:58:33,010 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04064062848687172
2025-03-08 11:58:44,912 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.03938758986070752
2025-03-08 11:58:55,954 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.040362119987607005
2025-03-08 11:59:05,828 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_177_epoch.pt
2025-03-08 11:59:17,226 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03887443140149117
2025-03-08 11:59:28,560 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.038927700370550156
2025-03-08 11:59:41,006 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.0408496368303895
2025-03-08 11:59:51,964 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04051455764099956
2025-03-08 12:00:03,124 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.03924144683033228
2025-03-08 12:00:13,516 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_178_epoch.pt
2025-03-08 12:00:25,554 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04157514553517103
2025-03-08 12:00:36,881 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04561273921281099
2025-03-08 12:00:47,782 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.0438186044121782
2025-03-08 12:00:59,622 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.045854686945676806
2025-03-08 12:01:11,641 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04531226892024279
2025-03-08 12:01:21,081 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_179_epoch.pt
2025-03-08 12:01:32,887 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03756231751292944
2025-03-08 12:01:44,227 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.040045473761856555
2025-03-08 12:01:55,435 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04206864422808091
2025-03-08 12:02:07,153 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.03913249759934843
2025-03-08 12:02:18,438 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.03954168340563774
2025-03-08 12:02:28,788 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_180_epoch.pt
2025-03-08 12:02:40,911 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03681275770068169
2025-03-08 12:02:51,381 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.03594227125868201
2025-03-08 12:03:02,916 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.03899835620075464
2025-03-08 12:03:12,977 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.03812808101065457
2025-03-08 12:03:25,085 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.03959219478815794
2025-03-08 12:03:35,929 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_181_epoch.pt
2025-03-08 12:03:48,521 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04715125408023596
2025-03-08 12:04:00,492 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04398887751623988
2025-03-08 12:04:11,126 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04454750712960959
2025-03-08 12:04:22,511 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04361242519691586
2025-03-08 12:04:33,767 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04358627200126648
2025-03-08 12:04:44,001 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_182_epoch.pt
2025-03-08 12:04:55,489 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.039037504009902475
2025-03-08 12:05:07,388 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.0426373490318656
2025-03-08 12:05:19,378 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.044111915888885654
2025-03-08 12:05:30,953 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04479814140126109
2025-03-08 12:05:42,562 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.043256645165383814
2025-03-08 12:05:52,301 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_183_epoch.pt
2025-03-08 12:06:04,475 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.036527515836060044
2025-03-08 12:06:16,425 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.045394295156002046
2025-03-08 12:06:27,593 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04550735964129368
2025-03-08 12:06:38,960 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04472817445173859
2025-03-08 12:06:49,532 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04512017227709293
2025-03-08 12:06:59,494 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_184_epoch.pt
2025-03-08 12:07:11,800 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04056168548762798
2025-03-08 12:07:23,113 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.042790037654340266
2025-03-08 12:07:34,298 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04183577850461006
2025-03-08 12:07:45,968 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.041074889488518236
2025-03-08 12:07:57,851 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.040180992506444455
2025-03-08 12:08:07,393 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_185_epoch.pt
2025-03-08 12:08:19,757 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03400885261595249
2025-03-08 12:08:30,420 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.036126144137233494
2025-03-08 12:08:41,616 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.03826889163504044
2025-03-08 12:08:52,591 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04052519688382745
2025-03-08 12:09:03,991 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.0405273526981473
2025-03-08 12:09:15,434 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_186_epoch.pt
2025-03-08 12:09:27,393 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.02952274978160858
2025-03-08 12:09:38,576 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.03417675409466028
2025-03-08 12:09:49,969 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.034522060652573905
2025-03-08 12:10:01,762 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.0368266867659986
2025-03-08 12:10:13,002 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.03909633621573448
2025-03-08 12:10:22,923 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_187_epoch.pt
2025-03-08 12:10:34,543 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03289417695254088
2025-03-08 12:10:46,186 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.038386655040085316
2025-03-08 12:10:58,424 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.039077858763436474
2025-03-08 12:11:10,066 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.03938434436917305
2025-03-08 12:11:20,847 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.03906160172075033
2025-03-08 12:11:31,278 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_188_epoch.pt
2025-03-08 12:11:43,501 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.038813165463507174
2025-03-08 12:11:54,742 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.03732098650187254
2025-03-08 12:12:05,857 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.038742306555310885
2025-03-08 12:12:17,336 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04137054045684636
2025-03-08 12:12:29,044 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04124687500298023
2025-03-08 12:12:39,043 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_189_epoch.pt
2025-03-08 12:12:50,745 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04177444972097874
2025-03-08 12:13:02,391 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04166592825204134
2025-03-08 12:13:14,717 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04023187042524417
2025-03-08 12:13:26,112 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.040600698320195076
2025-03-08 12:13:36,794 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04223286724835634
2025-03-08 12:13:46,761 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_190_epoch.pt
2025-03-08 12:13:58,332 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04955723535269499
2025-03-08 12:14:10,145 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04063073683530092
2025-03-08 12:14:21,707 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.045237949676811696
2025-03-08 12:14:32,226 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04394933332689106
2025-03-08 12:14:44,731 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04286072136461735
2025-03-08 12:14:54,429 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_191_epoch.pt
2025-03-08 12:15:04,891 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04364205587655306
2025-03-08 12:15:17,075 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04113785246387124
2025-03-08 12:15:27,960 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.042515648901462554
2025-03-08 12:15:39,278 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04293522243387997
2025-03-08 12:15:51,994 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04254797377437353
2025-03-08 12:16:02,140 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_192_epoch.pt
2025-03-08 12:16:13,085 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.036916885748505594
2025-03-08 12:16:24,400 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.03758901396766305
2025-03-08 12:16:35,440 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.03665499149511258
2025-03-08 12:16:46,725 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.036849947245791555
2025-03-08 12:16:59,205 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.03881126639991999
2025-03-08 12:17:09,818 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_193_epoch.pt
2025-03-08 12:17:22,131 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03741752192378044
2025-03-08 12:17:33,408 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.037989067286252974
2025-03-08 12:17:44,864 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.03805114880204201
2025-03-08 12:17:55,621 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.03789911014959216
2025-03-08 12:18:07,122 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.03836513169854879
2025-03-08 12:18:17,847 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_194_epoch.pt
2025-03-08 12:18:29,534 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.029619955532252787
2025-03-08 12:18:40,930 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.03785431331023574
2025-03-08 12:18:51,681 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.039717784350117045
2025-03-08 12:19:02,720 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04155797398649156
2025-03-08 12:19:13,991 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04116548765450716
2025-03-08 12:19:25,375 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_195_epoch.pt
2025-03-08 12:19:36,561 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04426835983991623
2025-03-08 12:19:46,765 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.03955280967056751
2025-03-08 12:19:58,786 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.03590118291477362
2025-03-08 12:20:10,320 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.03918237069621682
2025-03-08 12:20:21,847 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.038882837004959585
2025-03-08 12:20:32,975 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_196_epoch.pt
2025-03-08 12:20:45,024 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04467474792152643
2025-03-08 12:20:57,032 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04062656387686729
2025-03-08 12:21:08,764 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.03951803413530191
2025-03-08 12:21:20,155 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04114650219678879
2025-03-08 12:21:31,331 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.03973018189519644
2025-03-08 12:21:40,752 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_197_epoch.pt
2025-03-08 12:21:51,992 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03266133464872837
2025-03-08 12:22:02,514 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.0372281800955534
2025-03-08 12:22:13,914 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.03740539108713468
2025-03-08 12:22:25,496 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.037328433617949484
2025-03-08 12:22:37,179 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.0371152486205101
2025-03-08 12:22:47,633 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_198_epoch.pt
2025-03-08 12:22:59,770 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03560609769076109
2025-03-08 12:23:11,886 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.038780787810683254
2025-03-08 12:23:23,381 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04106015750517448
2025-03-08 12:23:34,384 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.03895237357355654
2025-03-08 12:23:45,812 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.038773203879594806
2025-03-08 12:23:55,283 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_199_epoch.pt
2025-03-08 12:24:07,212 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04130634855479002
2025-03-08 12:24:18,128 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.039634631648659706
2025-03-08 12:24:29,423 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.03896018305172523
2025-03-08 12:24:41,526 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.03757513811811805
2025-03-08 12:24:53,071 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.03848408766835928
2025-03-08 12:25:03,516 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_3/_bert-base-uncased_200_epoch.pt
2025-03-08 12:25:04,666 : 916236790.py : __call__ : INFO : loaded dataset, sample = 1160
2025-03-08 12:25:04,667 : 1341934384.py : train_scp : INFO : 109483778
2025-03-08 12:25:15,905 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.20102366238832473
2025-03-08 12:25:27,954 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.19984956987202168
2025-03-08 12:25:38,787 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.19903755376736323
2025-03-08 12:25:50,475 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.19460877139121294
2025-03-08 12:26:02,691 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.19368786507844926
2025-03-08 12:26:12,849 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_1_epoch.pt
2025-03-08 12:26:25,399 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.18230872377753257
2025-03-08 12:26:36,590 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.1844072525203228
2025-03-08 12:26:48,501 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.18436137462655702
2025-03-08 12:26:59,972 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.18571346363984048
2025-03-08 12:27:11,844 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.18469439543038607
2025-03-08 12:27:21,777 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_2_epoch.pt
2025-03-08 12:27:34,073 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.1881682562828064
2025-03-08 12:27:46,490 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.1938085211068392
2025-03-08 12:27:58,266 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.19248434940973919
2025-03-08 12:28:09,571 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.1899992349743843
2025-03-08 12:28:22,049 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.18862259328365327
2025-03-08 12:28:30,758 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_3_epoch.pt
2025-03-08 12:28:43,124 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.19846839372068645
2025-03-08 12:28:55,588 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.1913556550629437
2025-03-08 12:29:06,675 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.18834610455979903
2025-03-08 12:29:18,698 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.18490711878053845
2025-03-08 12:29:30,843 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.18626531886309386
2025-03-08 12:29:40,010 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_4_epoch.pt
2025-03-08 12:29:52,796 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.18944804817438127
2025-03-08 12:30:04,601 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.18845939364284278
2025-03-08 12:30:15,582 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.19074120052158833
2025-03-08 12:30:27,898 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.18986084413714707
2025-03-08 12:30:39,607 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.19093004009872674
2025-03-08 12:30:49,111 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_5_epoch.pt
2025-03-08 12:31:01,721 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.182463493719697
2025-03-08 12:31:12,833 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.1817143380269408
2025-03-08 12:31:25,039 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.17776064285387597
2025-03-08 12:31:38,181 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.17912270896136762
2025-03-08 12:31:49,975 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.17851482117176057
2025-03-08 12:31:58,739 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_6_epoch.pt
2025-03-08 12:32:11,790 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.17395126663148403
2025-03-08 12:32:23,383 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.17182098232209683
2025-03-08 12:32:34,843 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.17371730680267017
2025-03-08 12:32:47,263 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.17288560347631574
2025-03-08 12:33:00,174 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.1745882005393505
2025-03-08 12:33:09,120 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_7_epoch.pt
2025-03-08 12:33:19,558 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.17253534987568855
2025-03-08 12:33:31,494 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.1747670677304268
2025-03-08 12:33:43,590 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.17482135586440564
2025-03-08 12:33:56,406 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.17541731791570783
2025-03-08 12:34:08,335 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.17435717259347439
2025-03-08 12:34:18,012 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_8_epoch.pt
2025-03-08 12:34:30,573 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.17205720756202936
2025-03-08 12:34:43,041 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.17525072606280445
2025-03-08 12:34:54,944 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.17687506870677075
2025-03-08 12:35:07,323 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.17827651311643422
2025-03-08 12:35:18,517 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.17784583271294832
2025-03-08 12:35:27,344 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_9_epoch.pt
2025-03-08 12:35:38,980 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.1789829123020172
2025-03-08 12:35:50,123 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.17770570093765856
2025-03-08 12:36:02,390 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.1769664792343974
2025-03-08 12:36:14,878 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.1757800483610481
2025-03-08 12:36:26,592 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.17562152609229087
2025-03-08 12:36:36,438 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_10_epoch.pt
2025-03-08 12:36:48,079 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.18099211975932122
2025-03-08 12:37:01,207 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.17896264612674714
2025-03-08 12:37:11,876 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.1766374431177974
2025-03-08 12:37:24,689 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.17547483229078353
2025-03-08 12:37:36,071 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.17487873304635287
2025-03-08 12:37:45,131 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_11_epoch.pt
2025-03-08 12:37:57,673 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.17297664735466242
2025-03-08 12:38:09,002 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.1749368272908032
2025-03-08 12:38:20,685 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.17712080561866364
2025-03-08 12:38:32,565 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.17571215947158636
2025-03-08 12:38:43,665 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.17496679643541574
2025-03-08 12:38:53,510 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_12_epoch.pt
2025-03-08 12:39:04,962 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.17597192779183388
2025-03-08 12:39:17,613 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.17620724365115165
2025-03-08 12:39:28,801 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.1762999333937963
2025-03-08 12:39:40,577 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.17722448706626892
2025-03-08 12:39:51,782 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.17607225775718688
2025-03-08 12:40:02,572 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_13_epoch.pt
2025-03-08 12:40:14,340 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.17449648901820183
2025-03-08 12:40:26,463 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.17809161603450774
2025-03-08 12:40:39,107 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.1770893630385399
2025-03-08 12:40:51,003 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.17631211426109075
2025-03-08 12:41:02,627 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.1751437165439129
2025-03-08 12:41:11,730 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_14_epoch.pt
2025-03-08 12:41:24,086 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.17897773623466492
2025-03-08 12:41:35,938 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.17771217174828052
2025-03-08 12:41:47,607 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.1757900343462825
2025-03-08 12:41:59,042 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.17566507608629764
2025-03-08 12:42:10,908 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.1755303243249655
2025-03-08 12:42:20,422 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_15_epoch.pt
2025-03-08 12:42:33,214 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.17647399500012398
2025-03-08 12:42:45,321 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.179734373614192
2025-03-08 12:42:56,960 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.17878154113888742
2025-03-08 12:43:08,583 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.176582422722131
2025-03-08 12:43:20,068 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.17636405141651632
2025-03-08 12:43:29,483 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_16_epoch.pt
2025-03-08 12:43:42,729 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.1774816432595253
2025-03-08 12:43:53,946 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.17833650257438421
2025-03-08 12:44:05,787 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.17811447915931541
2025-03-08 12:44:17,885 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.17782327607274057
2025-03-08 12:44:29,096 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.178128548681736
2025-03-08 12:44:39,011 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_17_epoch.pt
2025-03-08 12:44:51,386 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.20057881228625773
2025-03-08 12:45:03,797 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.2022051950916648
2025-03-08 12:45:16,155 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.2013114986071984
2025-03-08 12:45:27,930 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.20219375731423497
2025-03-08 12:45:39,524 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.20163532830774783
2025-03-08 12:45:48,483 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_18_epoch.pt
2025-03-08 12:46:00,752 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.19998332172632216
2025-03-08 12:46:13,331 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.19998130202293396
2025-03-08 12:46:24,317 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.199986149619023
2025-03-08 12:46:36,754 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.19999048262834548
2025-03-08 12:46:48,155 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.19999220815300942
2025-03-08 12:46:57,982 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_19_epoch.pt
2025-03-08 12:47:10,190 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.1999731567502022
2025-03-08 12:47:22,610 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.19998217917978764
2025-03-08 12:47:34,046 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.20001989483833313
2025-03-08 12:47:45,861 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.20001437477767467
2025-03-08 12:47:57,245 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.20001079168915747
2025-03-08 12:48:07,054 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_20_epoch.pt
2025-03-08 12:48:19,635 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.20006173327565194
2025-03-08 12:48:31,797 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.20002957120537757
2025-03-08 12:48:44,203 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.20002252891659736
2025-03-08 12:48:55,329 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.20001928504556418
2025-03-08 12:49:06,587 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.2000141426026821
2025-03-08 12:49:16,230 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_21_epoch.pt
2025-03-08 12:49:28,507 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.1999846149981022
2025-03-08 12:49:39,942 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.19999178878962995
2025-03-08 12:49:52,377 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.1999933660030365
2025-03-08 12:50:04,485 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.20002043612301348
2025-03-08 12:50:15,733 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.20001655864715576
2025-03-08 12:50:24,937 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_22_epoch.pt
2025-03-08 12:50:36,384 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.19999767631292342
2025-03-08 12:50:47,175 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.1999983198195696
2025-03-08 12:50:59,171 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.19999866927663484
2025-03-08 12:51:11,903 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.19999795630574227
2025-03-08 12:51:24,214 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.19999819663167
2025-03-08 12:51:34,554 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_23_epoch.pt
2025-03-08 12:51:46,472 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.1999971029162407
2025-03-08 12:51:58,431 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.19999695487320424
2025-03-08 12:52:09,921 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.19999839479724565
2025-03-08 12:52:21,933 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.1999987233802676
2025-03-08 12:52:34,411 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.19999914062023164
2025-03-08 12:52:44,105 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_24_epoch.pt
2025-03-08 12:52:57,101 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.19999873265624046
2025-03-08 12:53:09,318 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.19999752707779409
2025-03-08 12:53:20,333 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.19998962566256523
2025-03-08 12:53:31,301 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.1999863039329648
2025-03-08 12:53:43,355 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.1999990681409836
2025-03-08 12:53:53,268 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_25_epoch.pt
2025-03-08 12:54:05,801 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.19999343976378442
2025-03-08 12:54:17,448 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.19999201700091362
2025-03-08 12:54:29,061 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.1999976645410061
2025-03-08 12:54:42,097 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.20000162653625012
2025-03-08 12:54:53,693 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.1999994631409645
2025-03-08 12:55:02,970 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_26_epoch.pt
2025-03-08 12:55:14,473 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.19999437004327775
2025-03-08 12:55:27,387 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.20000323578715323
2025-03-08 12:55:38,877 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.20000273471077284
2025-03-08 12:55:50,161 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.20000449117273092
2025-03-08 12:56:02,361 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.1999914734363556
2025-03-08 12:56:12,223 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_27_epoch.pt
2025-03-08 12:56:24,745 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.1999986283481121
2025-03-08 12:56:38,200 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.19999437354505062
2025-03-08 12:56:49,849 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.19999522204200426
2025-03-08 12:57:01,287 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.19999820474535226
2025-03-08 12:57:12,823 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.19999818992614746
2025-03-08 12:57:21,884 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_28_epoch.pt
2025-03-08 12:57:34,146 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.20000359073281287
2025-03-08 12:57:46,041 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.20003389373421668
2025-03-08 12:57:57,750 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.200022512425979
2025-03-08 12:58:10,452 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.2000160076469183
2025-03-08 12:58:22,578 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.20001264104247093
2025-03-08 12:58:31,481 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_29_epoch.pt
2025-03-08 12:58:43,859 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.19999914824962617
2025-03-08 12:58:57,205 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.19999971993267537
2025-03-08 12:59:08,994 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.1999985794723034
2025-03-08 12:59:19,852 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.19999775681644677
2025-03-08 12:59:31,985 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.1999970487654209
2025-03-08 12:59:40,657 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_30_epoch.pt
2025-03-08 12:59:52,730 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.2000005380809307
2025-03-08 13:00:04,805 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.1999991664290428
2025-03-08 13:00:16,698 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.19999967510501543
2025-03-08 13:00:28,076 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.2000004544481635
2025-03-08 13:00:40,219 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.19999949368834496
2025-03-08 13:00:50,194 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_31_epoch.pt
2025-03-08 13:01:01,800 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.19999637082219124
2025-03-08 13:01:13,937 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.1999991074204445
2025-03-08 13:01:25,870 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.1999984008570512
2025-03-08 13:01:37,428 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.19999772641807795
2025-03-08 13:01:48,433 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.1999966411292553
2025-03-08 13:01:59,559 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_32_epoch.pt
2025-03-08 13:02:11,746 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.20000192344188691
2025-03-08 13:02:22,837 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.1999980802088976
2025-03-08 13:02:35,226 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.19999418556690216
2025-03-08 13:02:47,267 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.19999545603990554
2025-03-08 13:02:58,840 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.19999346494674683
2025-03-08 13:03:08,554 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_33_epoch.pt
2025-03-08 13:03:20,844 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.1999807773530483
2025-03-08 13:03:31,257 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.19996789999306203
2025-03-08 13:03:43,215 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.19745510148505369
2025-03-08 13:03:55,630 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.19027804147452115
2025-03-08 13:04:07,973 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.18717431625723838
2025-03-08 13:04:17,242 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_34_epoch.pt
2025-03-08 13:04:29,856 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.17859870929270982
2025-03-08 13:04:41,566 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.17541952315717935
2025-03-08 13:04:53,711 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.17729617916047574
2025-03-08 13:05:05,789 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.1797177191078663
2025-03-08 13:05:17,587 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.17914189429581165
2025-03-08 13:05:25,736 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_35_epoch.pt
2025-03-08 13:05:38,723 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.1669216162338853
2025-03-08 13:05:50,003 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.16705843394622208
2025-03-08 13:06:01,398 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.16503085610767207
2025-03-08 13:06:13,072 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.16436316925100983
2025-03-08 13:06:25,175 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.16472387375682593
2025-03-08 13:06:35,093 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_36_epoch.pt
2025-03-08 13:06:47,290 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.16092138469219208
2025-03-08 13:06:58,974 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.16712209969758987
2025-03-08 13:07:10,906 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.16730112213641404
2025-03-08 13:07:23,405 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.16039968927390874
2025-03-08 13:07:34,411 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.15859962924569845
2025-03-08 13:07:44,062 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_37_epoch.pt
2025-03-08 13:07:56,690 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.1457812014594674
2025-03-08 13:08:07,659 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.13419944511726498
2025-03-08 13:08:20,058 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.1301624934623639
2025-03-08 13:08:32,117 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.12416774525307119
2025-03-08 13:08:43,328 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.1189381025955081
2025-03-08 13:08:53,104 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_38_epoch.pt
2025-03-08 13:09:06,302 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.10091866854578256
2025-03-08 13:09:17,647 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.09933338528499007
2025-03-08 13:09:29,592 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.09567356017728647
2025-03-08 13:09:40,598 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.09387083837762475
2025-03-08 13:09:51,600 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.09054131564497948
2025-03-08 13:10:01,026 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_39_epoch.pt
2025-03-08 13:10:13,833 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.08277341902256012
2025-03-08 13:10:26,034 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.08375824455171824
2025-03-08 13:10:38,236 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.08305518622199694
2025-03-08 13:10:49,027 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.08203421561978758
2025-03-08 13:11:00,104 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.08118659070879221
2025-03-08 13:11:09,682 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_40_epoch.pt
2025-03-08 13:11:21,332 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.0757766042649746
2025-03-08 13:11:33,495 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.0815300047583878
2025-03-08 13:11:44,787 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.0774193108951052
2025-03-08 13:11:57,428 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.07482124799862504
2025-03-08 13:12:09,771 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.07513045064359904
2025-03-08 13:12:18,731 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_41_epoch.pt
2025-03-08 13:12:31,055 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.06296615768224001
2025-03-08 13:12:43,272 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.06673588749021292
2025-03-08 13:12:54,488 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.06599644309530656
2025-03-08 13:13:06,162 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.06916397940367461
2025-03-08 13:13:18,201 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.06841238435357809
2025-03-08 13:13:27,478 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_42_epoch.pt
2025-03-08 13:13:40,219 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.06845022279769182
2025-03-08 13:13:52,623 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.06634298648685216
2025-03-08 13:14:04,754 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.06851206846535206
2025-03-08 13:14:15,643 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.06690160054713487
2025-03-08 13:14:26,379 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.06653920454531909
2025-03-08 13:14:35,884 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_43_epoch.pt
2025-03-08 13:14:47,483 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.05936978656798601
2025-03-08 13:14:59,924 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.06353734135627746
2025-03-08 13:15:12,289 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.06648574297626814
2025-03-08 13:15:24,051 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.06663369687274098
2025-03-08 13:15:35,093 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.06341714284569025
2025-03-08 13:15:44,208 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_44_epoch.pt
2025-03-08 13:15:56,213 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.0652725813910365
2025-03-08 13:16:08,538 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.07193812137469649
2025-03-08 13:16:20,535 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.0704669360195597
2025-03-08 13:16:31,461 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.06750977681949735
2025-03-08 13:16:43,384 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.06710075282305479
2025-03-08 13:16:52,241 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_45_epoch.pt
2025-03-08 13:17:04,505 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.07432554330676794
2025-03-08 13:17:16,235 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.06699821231886745
2025-03-08 13:17:28,751 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.059945265675584475
2025-03-08 13:17:40,530 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.05882832758128643
2025-03-08 13:17:51,524 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.05799397896975279
2025-03-08 13:18:00,037 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_46_epoch.pt
2025-03-08 13:18:10,859 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.07262255530804396
2025-03-08 13:18:23,507 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.0643587427958846
2025-03-08 13:18:36,322 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.059922975674271586
2025-03-08 13:18:48,292 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.061544547490775585
2025-03-08 13:19:00,281 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.06041452344506979
2025-03-08 13:19:08,806 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_47_epoch.pt
2025-03-08 13:19:20,761 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.06506149280816316
2025-03-08 13:19:31,887 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.057304760292172435
2025-03-08 13:19:43,779 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.05671580540637175
2025-03-08 13:19:54,864 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.05750735063105822
2025-03-08 13:20:06,238 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.05837482640892267
2025-03-08 13:20:17,046 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_48_epoch.pt
2025-03-08 13:20:29,342 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.05847895409911871
2025-03-08 13:20:41,018 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.059140337146818635
2025-03-08 13:20:52,739 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.057517171762883665
2025-03-08 13:21:04,238 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.05610508954152465
2025-03-08 13:21:15,600 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.055203779198229316
2025-03-08 13:21:25,266 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_49_epoch.pt
2025-03-08 13:21:37,010 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.061843847967684266
2025-03-08 13:21:48,961 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.05765458509325981
2025-03-08 13:22:01,188 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.05941978151599566
2025-03-08 13:22:12,359 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.05894041586667299
2025-03-08 13:22:24,906 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.0588093406111002
2025-03-08 13:22:34,428 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_50_epoch.pt
2025-03-08 13:22:46,848 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.058499231114983556
2025-03-08 13:22:59,020 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.05797608474269509
2025-03-08 13:23:11,678 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.05696797632922729
2025-03-08 13:23:24,114 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.058251351239159706
2025-03-08 13:23:34,438 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.05985947836190462
2025-03-08 13:23:43,457 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_51_epoch.pt
2025-03-08 13:23:54,912 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.052646584361791614
2025-03-08 13:24:06,160 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.05386090066283941
2025-03-08 13:24:17,779 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.056421663972238696
2025-03-08 13:24:30,172 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.05559782832860947
2025-03-08 13:24:42,599 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.05791342771053314
2025-03-08 13:24:51,028 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_52_epoch.pt
2025-03-08 13:25:02,869 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.05592338338494301
2025-03-08 13:25:14,824 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.052045359630137684
2025-03-08 13:25:26,674 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.05285727934290965
2025-03-08 13:25:38,328 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.054669282622635366
2025-03-08 13:25:49,558 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.05663310281187296
2025-03-08 13:25:59,243 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_53_epoch.pt
2025-03-08 13:26:10,980 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.0508027359098196
2025-03-08 13:26:22,840 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.0544421162083745
2025-03-08 13:26:34,412 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.0540398662040631
2025-03-08 13:26:45,974 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.055029155081138016
2025-03-08 13:26:57,895 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.055104018472135065
2025-03-08 13:27:07,190 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_54_epoch.pt
2025-03-08 13:27:20,491 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.052105224058032036
2025-03-08 13:27:31,625 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.050442953668534754
2025-03-08 13:27:43,176 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.05313466815898816
2025-03-08 13:27:54,462 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.05439501653425396
2025-03-08 13:28:05,890 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.055005815103650095
2025-03-08 13:28:15,360 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_55_epoch.pt
2025-03-08 13:28:28,203 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.05164545252919197
2025-03-08 13:28:39,975 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04467018587514758
2025-03-08 13:28:52,070 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04864304338892301
2025-03-08 13:29:03,726 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.050486096553504466
2025-03-08 13:29:14,849 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.05111800636351108
2025-03-08 13:29:23,943 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_56_epoch.pt
2025-03-08 13:29:36,442 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.05817564144730568
2025-03-08 13:29:48,796 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.05462498709559441
2025-03-08 13:29:59,441 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.05383031984170278
2025-03-08 13:30:10,950 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.05199647257104516
2025-03-08 13:30:22,915 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.05306675636023283
2025-03-08 13:30:32,489 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_57_epoch.pt
2025-03-08 13:30:45,000 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04680733609944582
2025-03-08 13:30:57,327 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04750828927382827
2025-03-08 13:31:08,599 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.048123573511838914
2025-03-08 13:31:20,280 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.048528446182608605
2025-03-08 13:31:32,461 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04855611279606819
2025-03-08 13:31:41,181 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_58_epoch.pt
2025-03-08 13:31:53,262 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.053010842949151996
2025-03-08 13:32:05,555 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.05269364370033145
2025-03-08 13:32:17,425 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.05143822025507688
2025-03-08 13:32:29,145 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.05254350619390607
2025-03-08 13:32:40,812 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.052085386507213116
2025-03-08 13:32:49,354 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_59_epoch.pt
2025-03-08 13:33:03,339 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.047210138626396655
2025-03-08 13:33:14,262 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.05160544028505683
2025-03-08 13:33:26,028 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.052953906022012234
2025-03-08 13:33:37,273 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.05207719292491674
2025-03-08 13:33:48,454 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.051570006236433985
2025-03-08 13:33:57,644 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_60_epoch.pt
2025-03-08 13:34:09,725 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.056633954271674156
2025-03-08 13:34:21,736 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.054643059205263855
2025-03-08 13:34:33,678 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.053693288763364155
2025-03-08 13:34:44,581 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.05185804384760559
2025-03-08 13:34:56,744 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.052939954943954946
2025-03-08 13:35:06,492 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_61_epoch.pt
2025-03-08 13:35:19,667 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.061843129470944405
2025-03-08 13:35:31,251 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04815875373780727
2025-03-08 13:35:42,766 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04809598453342914
2025-03-08 13:35:54,760 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.049047314273193476
2025-03-08 13:36:05,750 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04915441508591175
2025-03-08 13:36:15,270 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_62_epoch.pt
2025-03-08 13:36:27,243 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.05759056344628334
2025-03-08 13:36:39,324 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.05831254370510578
2025-03-08 13:36:50,482 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.05495291084051132
2025-03-08 13:37:02,260 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.05603164294734597
2025-03-08 13:37:14,514 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.05380255719274282
2025-03-08 13:37:23,889 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_63_epoch.pt
2025-03-08 13:37:36,902 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.06128958072513342
2025-03-08 13:37:48,071 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.05324945842847228
2025-03-08 13:38:00,035 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.05213573017468055
2025-03-08 13:38:11,253 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04924183242022991
2025-03-08 13:38:23,125 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.049434654369950294
2025-03-08 13:38:32,654 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_64_epoch.pt
2025-03-08 13:38:44,988 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.046115502268075946
2025-03-08 13:38:57,341 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.045401430744677784
2025-03-08 13:39:08,887 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.045876947902143
2025-03-08 13:39:19,945 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04567935698665679
2025-03-08 13:39:31,475 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04916618043929338
2025-03-08 13:39:40,722 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_65_epoch.pt
2025-03-08 13:39:52,703 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.048468668721616266
2025-03-08 13:40:03,859 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04800039365887642
2025-03-08 13:40:16,195 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04900435986618201
2025-03-08 13:40:27,833 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04795727669261396
2025-03-08 13:40:38,698 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.049002372100949286
2025-03-08 13:40:48,154 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_66_epoch.pt
2025-03-08 13:41:00,730 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.05540349543094635
2025-03-08 13:41:13,112 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.0499368884973228
2025-03-08 13:41:24,551 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.054988846803704895
2025-03-08 13:41:35,688 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.055076641496270895
2025-03-08 13:41:46,423 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.05428915698826313
2025-03-08 13:41:56,631 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_67_epoch.pt
2025-03-08 13:42:09,049 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03394402489066124
2025-03-08 13:42:21,901 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.040238727014511824
2025-03-08 13:42:34,384 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04536959388603767
2025-03-08 13:42:45,042 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.050123182879760864
2025-03-08 13:42:56,016 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.05026561073958874
2025-03-08 13:43:04,770 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_68_epoch.pt
2025-03-08 13:43:15,636 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04925481796264648
2025-03-08 13:43:26,803 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04955155067145824
2025-03-08 13:43:38,948 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.05107885982841253
2025-03-08 13:43:49,944 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.05045856880955398
2025-03-08 13:44:03,012 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04954759054630995
2025-03-08 13:44:12,744 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_69_epoch.pt
2025-03-08 13:44:25,058 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.05664220754057169
2025-03-08 13:44:36,572 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.057427802719175816
2025-03-08 13:44:48,235 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.058017739777763686
2025-03-08 13:45:00,350 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.05707831296138465
2025-03-08 13:45:12,404 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.053949771493673325
2025-03-08 13:45:21,036 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_70_epoch.pt
2025-03-08 13:45:33,442 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.05158188287168741
2025-03-08 13:45:46,128 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04706505838781595
2025-03-08 13:45:57,471 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04717364011953274
2025-03-08 13:46:08,501 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.046756603736430404
2025-03-08 13:46:20,069 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04631558259576559
2025-03-08 13:46:29,766 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_71_epoch.pt
2025-03-08 13:46:41,287 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04762358073145151
2025-03-08 13:46:52,933 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.047757774218916894
2025-03-08 13:47:04,739 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.05033507646371921
2025-03-08 13:47:17,714 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.05017818790860474
2025-03-08 13:47:28,833 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.05008087886124849
2025-03-08 13:47:37,953 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_72_epoch.pt
2025-03-08 13:47:50,300 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.039678826332092285
2025-03-08 13:48:02,132 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.044911731593310834
2025-03-08 13:48:13,869 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.045777878252168495
2025-03-08 13:48:25,576 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04394882198423147
2025-03-08 13:48:36,708 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04418994615972042
2025-03-08 13:48:46,098 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_73_epoch.pt
2025-03-08 13:48:57,788 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.05190733395516872
2025-03-08 13:49:08,957 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.053794990368187426
2025-03-08 13:49:21,660 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.052475045025348666
2025-03-08 13:49:33,303 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.05208838219754398
2025-03-08 13:49:45,286 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.0523757186755538
2025-03-08 13:49:54,890 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_74_epoch.pt
2025-03-08 13:50:06,708 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.053211811482906345
2025-03-08 13:50:18,577 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.051002528071403506
2025-03-08 13:50:31,020 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04938186629364888
2025-03-08 13:50:42,393 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.049896796448156235
2025-03-08 13:50:53,926 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.05076796143501997
2025-03-08 13:51:03,304 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_75_epoch.pt
2025-03-08 13:51:15,010 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.0469572027400136
2025-03-08 13:51:26,533 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.050417831130325795
2025-03-08 13:51:38,510 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.051047561429440974
2025-03-08 13:51:50,742 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04960979080758989
2025-03-08 13:52:01,909 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04961949299275875
2025-03-08 13:52:11,372 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_76_epoch.pt
2025-03-08 13:52:23,537 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.037385656908154485
2025-03-08 13:52:34,605 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.03833511484786868
2025-03-08 13:52:46,735 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.03898441337049007
2025-03-08 13:52:57,964 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04400241690687835
2025-03-08 13:53:10,874 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04422459394484758
2025-03-08 13:53:19,956 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_77_epoch.pt
2025-03-08 13:53:30,994 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04869833886623383
2025-03-08 13:53:43,536 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.047826030887663365
2025-03-08 13:53:55,567 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.049437750751773514
2025-03-08 13:54:06,750 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.047990754544734955
2025-03-08 13:54:17,752 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04682549764961004
2025-03-08 13:54:27,194 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_78_epoch.pt
2025-03-08 13:54:39,163 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04209996066987515
2025-03-08 13:54:50,562 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.044746231716126204
2025-03-08 13:55:02,128 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.046322764282425245
2025-03-08 13:55:14,101 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.0470754293166101
2025-03-08 13:55:25,574 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.046754608117043975
2025-03-08 13:55:34,921 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_79_epoch.pt
2025-03-08 13:55:47,371 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.05526788048446178
2025-03-08 13:55:58,488 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.05069258971139789
2025-03-08 13:56:10,031 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.049776540510356425
2025-03-08 13:56:21,676 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.05104561197571456
2025-03-08 13:56:34,114 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.051062685623764995
2025-03-08 13:56:42,985 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_80_epoch.pt
2025-03-08 13:56:55,160 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.043990264646708965
2025-03-08 13:57:07,758 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.046955227311700584
2025-03-08 13:57:19,934 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04629543794939915
2025-03-08 13:57:31,411 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04901219468563795
2025-03-08 13:57:41,850 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.05117280191928148
2025-03-08 13:57:50,899 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_81_epoch.pt
2025-03-08 13:58:03,452 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04957100085914135
2025-03-08 13:58:15,048 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.049541051760315896
2025-03-08 13:58:26,964 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04843202162533999
2025-03-08 13:58:39,161 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.049470026679337024
2025-03-08 13:58:50,455 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.049375683799386026
2025-03-08 13:58:59,576 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_82_epoch.pt
2025-03-08 13:59:12,675 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.059701844900846485
2025-03-08 13:59:24,271 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.05402983736246824
2025-03-08 13:59:36,180 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.052056503693262736
2025-03-08 13:59:47,561 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.050755133824422954
2025-03-08 13:59:58,476 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.05108079345524311
2025-03-08 14:00:08,356 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_83_epoch.pt
2025-03-08 14:00:20,424 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.042506469041109086
2025-03-08 14:00:32,405 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04308267964050174
2025-03-08 14:00:44,133 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04407871883362532
2025-03-08 14:00:55,367 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04646804123185575
2025-03-08 14:01:07,117 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04715434036403894
2025-03-08 14:01:17,647 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_84_epoch.pt
2025-03-08 14:01:29,869 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04612770803272724
2025-03-08 14:01:40,709 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04493777824565768
2025-03-08 14:01:52,258 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04668632240345081
2025-03-08 14:02:05,303 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.046880002273246645
2025-03-08 14:02:16,723 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.047399484917521474
2025-03-08 14:02:25,939 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_85_epoch.pt
2025-03-08 14:02:38,262 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.038613713532686236
2025-03-08 14:02:50,821 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04469838695600629
2025-03-08 14:03:02,787 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04633384719491005
2025-03-08 14:03:13,920 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.046454958766698834
2025-03-08 14:03:24,909 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.047642737209796907
2025-03-08 14:03:34,146 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_86_epoch.pt
2025-03-08 14:03:45,994 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04315248776227236
2025-03-08 14:03:58,734 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.044160620495677
2025-03-08 14:04:09,618 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.046143160151938596
2025-03-08 14:04:21,481 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04568774307146668
2025-03-08 14:04:33,015 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.0471179533302784
2025-03-08 14:04:42,049 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_87_epoch.pt
2025-03-08 14:04:54,313 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.058877080790698526
2025-03-08 14:05:06,179 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04918030351400375
2025-03-08 14:05:17,573 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.05197711014499267
2025-03-08 14:05:29,246 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.05016951248981059
2025-03-08 14:05:41,418 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04878133535385132
2025-03-08 14:05:50,600 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_88_epoch.pt
2025-03-08 14:06:03,227 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.0485940895229578
2025-03-08 14:06:15,273 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04675234301015735
2025-03-08 14:06:27,026 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.045677097973724205
2025-03-08 14:06:39,157 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.046188650391995906
2025-03-08 14:06:50,723 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04859809902310371
2025-03-08 14:06:59,127 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_89_epoch.pt
2025-03-08 14:07:11,868 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.0453183164447546
2025-03-08 14:07:23,195 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.0447412483766675
2025-03-08 14:07:34,748 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04190161953369776
2025-03-08 14:07:46,321 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04361999395303428
2025-03-08 14:07:59,013 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04513532593101263
2025-03-08 14:08:07,963 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_90_epoch.pt
2025-03-08 14:08:20,247 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.0493787194788456
2025-03-08 14:08:31,680 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04820903066545725
2025-03-08 14:08:43,315 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04651593601951996
2025-03-08 14:08:55,298 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.047348707979545
2025-03-08 14:09:06,907 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.048160875260829926
2025-03-08 14:09:15,795 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_91_epoch.pt
2025-03-08 14:09:27,478 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.055536733232438566
2025-03-08 14:09:39,647 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.05095552586019039
2025-03-08 14:09:51,566 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.050137939850489296
2025-03-08 14:10:03,832 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04939218414016068
2025-03-08 14:10:15,091 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04641342367231846
2025-03-08 14:10:23,759 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_92_epoch.pt
2025-03-08 14:10:35,774 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.047820157669484616
2025-03-08 14:10:47,462 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.045122647527605295
2025-03-08 14:10:58,345 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.046120732227961224
2025-03-08 14:11:10,457 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04632838824763894
2025-03-08 14:11:22,153 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.045693560130894186
2025-03-08 14:11:30,843 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_93_epoch.pt
2025-03-08 14:11:43,774 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04548580754548311
2025-03-08 14:11:54,804 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.05350341131910682
2025-03-08 14:12:06,003 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.05113226044923067
2025-03-08 14:12:17,915 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04879679873585701
2025-03-08 14:12:29,615 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04919519940763712
2025-03-08 14:12:39,643 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_94_epoch.pt
2025-03-08 14:12:51,537 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04962043769657612
2025-03-08 14:13:03,412 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.051508269589394334
2025-03-08 14:13:15,614 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.05136569708585739
2025-03-08 14:13:27,566 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.0507304282207042
2025-03-08 14:13:39,315 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04919490011036396
2025-03-08 14:13:47,922 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_95_epoch.pt
2025-03-08 14:13:59,786 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.0461939438059926
2025-03-08 14:14:11,709 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04555694207549095
2025-03-08 14:14:23,071 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04707086559385061
2025-03-08 14:14:34,563 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.047209847550839186
2025-03-08 14:14:47,225 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.048025467306375504
2025-03-08 14:14:56,653 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_96_epoch.pt
2025-03-08 14:15:09,191 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04648659352213144
2025-03-08 14:15:21,170 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.0451849278062582
2025-03-08 14:15:33,494 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04413275721172492
2025-03-08 14:15:44,683 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.044130947794765236
2025-03-08 14:15:56,179 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.043601333603262904
2025-03-08 14:16:04,868 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_97_epoch.pt
2025-03-08 14:16:17,767 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03840189140290022
2025-03-08 14:16:30,306 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04932174272835255
2025-03-08 14:16:41,817 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.05036636638144652
2025-03-08 14:16:53,064 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04970466146245599
2025-03-08 14:17:04,058 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04898091273009777
2025-03-08 14:17:13,251 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_98_epoch.pt
2025-03-08 14:17:26,193 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04641728118062019
2025-03-08 14:17:38,908 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04526559276506305
2025-03-08 14:17:50,230 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04602930327256521
2025-03-08 14:18:00,410 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.046163489436730744
2025-03-08 14:18:12,570 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04620753943175077
2025-03-08 14:18:21,760 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_99_epoch.pt
2025-03-08 14:18:34,083 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04796110387891531
2025-03-08 14:18:46,203 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.05458429442718625
2025-03-08 14:18:58,473 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.0533107553049922
2025-03-08 14:19:09,611 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.05270642866380513
2025-03-08 14:19:21,734 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.05140500355511904
2025-03-08 14:19:29,749 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_100_epoch.pt
2025-03-08 14:19:41,759 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.0466331397369504
2025-03-08 14:19:53,199 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.049331706464290616
2025-03-08 14:20:05,282 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.046822073968748254
2025-03-08 14:20:16,447 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.048357975846156476
2025-03-08 14:20:27,765 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04861113078892231
2025-03-08 14:20:37,209 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_101_epoch.pt
2025-03-08 14:20:49,275 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.05769366282969713
2025-03-08 14:21:01,380 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.05092486958950758
2025-03-08 14:21:12,536 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04900265645235777
2025-03-08 14:21:24,198 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.048533950718119744
2025-03-08 14:21:35,475 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04758439824730158
2025-03-08 14:21:45,218 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_102_epoch.pt
2025-03-08 14:21:58,462 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04982699748128652
2025-03-08 14:22:09,988 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04792464232072234
2025-03-08 14:22:21,299 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.049133963336547216
2025-03-08 14:22:33,187 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.049983914885669944
2025-03-08 14:22:44,116 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.049102251648902896
2025-03-08 14:22:53,276 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_103_epoch.pt
2025-03-08 14:23:05,718 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.05133881989866495
2025-03-08 14:23:17,256 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04749102499336004
2025-03-08 14:23:29,520 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04648615856965383
2025-03-08 14:23:40,225 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.045227331509813666
2025-03-08 14:23:51,514 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04457513769716025
2025-03-08 14:24:01,232 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_104_epoch.pt
2025-03-08 14:24:13,612 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.0417360620200634
2025-03-08 14:24:24,792 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.0524698743596673
2025-03-08 14:24:35,973 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.05194429539144039
2025-03-08 14:24:48,782 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04825321081094444
2025-03-08 14:24:59,981 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04657854422926903
2025-03-08 14:25:09,524 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_105_epoch.pt
2025-03-08 14:25:21,170 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03585491504520178
2025-03-08 14:25:32,460 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.03973905541002751
2025-03-08 14:25:44,034 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04464833514144023
2025-03-08 14:25:56,387 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04583529212512076
2025-03-08 14:26:08,932 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04444372475147247
2025-03-08 14:26:17,799 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_106_epoch.pt
2025-03-08 14:26:29,898 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.043507876023650166
2025-03-08 14:26:41,598 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.050013213939964773
2025-03-08 14:26:54,396 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.049931726045906545
2025-03-08 14:27:05,119 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.050927996737882494
2025-03-08 14:27:16,928 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04891389869153499
2025-03-08 14:27:25,527 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_107_epoch.pt
2025-03-08 14:27:38,396 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.041580916084349155
2025-03-08 14:27:49,694 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.0439136646874249
2025-03-08 14:28:00,882 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04485199436545372
2025-03-08 14:28:11,977 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04618083896115422
2025-03-08 14:28:23,851 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.0465422860532999
2025-03-08 14:28:33,106 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_108_epoch.pt
2025-03-08 14:28:45,383 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03902970150113106
2025-03-08 14:28:56,928 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.038476901426911354
2025-03-08 14:29:08,541 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.041128049977123736
2025-03-08 14:29:20,625 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.042050144886597994
2025-03-08 14:29:32,392 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.0414848028793931
2025-03-08 14:29:41,351 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_109_epoch.pt
2025-03-08 14:29:53,201 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.046927275955677035
2025-03-08 14:30:05,039 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.0445570963062346
2025-03-08 14:30:17,159 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04694767797986666
2025-03-08 14:30:28,216 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04537338606081903
2025-03-08 14:30:39,650 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.045648644357919696
2025-03-08 14:30:50,173 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_110_epoch.pt
2025-03-08 14:31:02,176 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04537516359239817
2025-03-08 14:31:14,012 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04822638252750039
2025-03-08 14:31:26,242 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04990306147684653
2025-03-08 14:31:37,691 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04757651649415493
2025-03-08 14:31:49,590 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.0468970850110054
2025-03-08 14:31:58,813 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_111_epoch.pt
2025-03-08 14:32:10,586 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04845907174050808
2025-03-08 14:32:22,902 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.045066644605249166
2025-03-08 14:32:33,804 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04528111898650726
2025-03-08 14:32:45,365 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04434838907793164
2025-03-08 14:32:57,102 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04514348413795233
2025-03-08 14:33:06,614 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_112_epoch.pt
2025-03-08 14:33:19,852 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04695448122918606
2025-03-08 14:33:30,837 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.040264194160699845
2025-03-08 14:33:42,378 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04130331493914127
2025-03-08 14:33:53,678 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04259687868878245
2025-03-08 14:34:05,536 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04386853014677763
2025-03-08 14:34:15,074 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_113_epoch.pt
2025-03-08 14:34:27,516 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.039071062989532945
2025-03-08 14:34:39,333 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04014979343861341
2025-03-08 14:34:50,902 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04269926572839419
2025-03-08 14:35:03,053 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.043813758194446564
2025-03-08 14:35:14,619 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04495116040855646
2025-03-08 14:35:23,267 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_114_epoch.pt
2025-03-08 14:35:34,204 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.0344970840215683
2025-03-08 14:35:45,176 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04068574633449316
2025-03-08 14:35:57,171 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.0419667038321495
2025-03-08 14:36:09,019 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04421158087439835
2025-03-08 14:36:20,526 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04477790106087923
2025-03-08 14:36:31,011 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_115_epoch.pt
2025-03-08 14:36:43,343 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04964574508368969
2025-03-08 14:36:55,220 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04946357175707817
2025-03-08 14:37:06,369 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04859620997061332
2025-03-08 14:37:19,565 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.046875600228086116
2025-03-08 14:37:31,236 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04644572021067143
2025-03-08 14:37:39,506 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_116_epoch.pt
2025-03-08 14:37:52,010 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.0401267359405756
2025-03-08 14:38:04,838 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04228313883766532
2025-03-08 14:38:15,582 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.042261571983496345
2025-03-08 14:38:27,204 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04300657507032156
2025-03-08 14:38:38,462 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.044760808773338795
2025-03-08 14:38:47,711 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_117_epoch.pt
2025-03-08 14:39:00,120 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.040268036276102065
2025-03-08 14:39:11,159 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04253775205463171
2025-03-08 14:39:23,514 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.0417234377314647
2025-03-08 14:39:34,894 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04140099790878594
2025-03-08 14:39:46,385 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.041704567886888984
2025-03-08 14:39:55,876 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_118_epoch.pt
2025-03-08 14:40:07,844 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04672655444592237
2025-03-08 14:40:19,891 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.05079497952014208
2025-03-08 14:40:32,547 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.0503724596525232
2025-03-08 14:40:44,946 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04948853014968336
2025-03-08 14:40:54,993 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04804348312318325
2025-03-08 14:41:04,463 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_119_epoch.pt
2025-03-08 14:41:17,664 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04363152496516705
2025-03-08 14:41:30,261 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04483389897271991
2025-03-08 14:41:41,216 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.044774235809842744
2025-03-08 14:41:52,951 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.0453149666916579
2025-03-08 14:42:04,272 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04569742424041033
2025-03-08 14:42:12,930 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_120_epoch.pt
2025-03-08 14:42:25,647 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.0370187982916832
2025-03-08 14:42:37,760 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04333852322772145
2025-03-08 14:42:49,458 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.0455314489454031
2025-03-08 14:43:01,225 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04399338230490685
2025-03-08 14:43:12,767 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04178465715795755
2025-03-08 14:43:21,501 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_121_epoch.pt
2025-03-08 14:43:34,854 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.053313989490270615
2025-03-08 14:43:44,977 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04611118165776133
2025-03-08 14:43:57,196 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04383996018519004
2025-03-08 14:44:08,307 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04484602964483202
2025-03-08 14:44:20,703 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04378345849364996
2025-03-08 14:44:29,956 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_122_epoch.pt
2025-03-08 14:44:41,952 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04285366278141737
2025-03-08 14:44:54,174 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.039259491991251706
2025-03-08 14:45:05,959 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.0417111162468791
2025-03-08 14:45:17,190 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.0417765002977103
2025-03-08 14:45:28,994 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04238327792286873
2025-03-08 14:45:38,383 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_123_epoch.pt
2025-03-08 14:45:49,865 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04952388308942318
2025-03-08 14:46:01,797 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.048298900220543145
2025-03-08 14:46:13,791 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04859731605897347
2025-03-08 14:46:25,729 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.047031581792980434
2025-03-08 14:46:37,425 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.046976518042385576
2025-03-08 14:46:46,750 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_124_epoch.pt
2025-03-08 14:46:58,720 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.0503315070271492
2025-03-08 14:47:11,776 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04560224676504731
2025-03-08 14:47:23,653 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.047305350539584956
2025-03-08 14:47:34,841 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.044646868165582415
2025-03-08 14:47:45,623 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04413778550922871
2025-03-08 14:47:55,065 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_125_epoch.pt
2025-03-08 14:48:07,345 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.05295623380690813
2025-03-08 14:48:18,988 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04744605507701635
2025-03-08 14:48:30,661 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04964609755824009
2025-03-08 14:48:42,233 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04880436877720058
2025-03-08 14:48:53,486 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.0489387382492423
2025-03-08 14:49:03,091 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_126_epoch.pt
2025-03-08 14:49:15,284 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04126834560185671
2025-03-08 14:49:27,141 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04385522363707423
2025-03-08 14:49:39,181 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.042557577242453896
2025-03-08 14:49:50,455 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04461541800759733
2025-03-08 14:50:01,500 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04490968438237906
2025-03-08 14:50:10,961 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_127_epoch.pt
2025-03-08 14:50:23,193 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04358866430819035
2025-03-08 14:50:34,046 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04309705536812544
2025-03-08 14:50:46,067 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.045848688669502737
2025-03-08 14:50:57,130 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04344797858037055
2025-03-08 14:51:10,272 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04417757903784513
2025-03-08 14:51:19,767 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_128_epoch.pt
2025-03-08 14:51:32,517 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03953425750136375
2025-03-08 14:51:44,678 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04094409577548504
2025-03-08 14:51:55,689 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04213665644327799
2025-03-08 14:52:06,964 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.0418828056473285
2025-03-08 14:52:19,073 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.043355821639299394
2025-03-08 14:52:29,079 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_129_epoch.pt
2025-03-08 14:52:42,452 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04257856722921133
2025-03-08 14:52:53,611 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.045561614390462635
2025-03-08 14:53:05,557 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04339027289301157
2025-03-08 14:53:17,002 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04369259084574878
2025-03-08 14:53:27,658 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.044669008769094944
2025-03-08 14:53:37,342 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_130_epoch.pt
2025-03-08 14:53:49,430 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03888822123408318
2025-03-08 14:54:00,710 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.03958253579214215
2025-03-08 14:54:11,444 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.041368792926271755
2025-03-08 14:54:23,758 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.042436545491218565
2025-03-08 14:54:35,598 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04500160519778729
2025-03-08 14:54:45,090 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_131_epoch.pt
2025-03-08 14:54:56,868 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04092047836631536
2025-03-08 14:55:09,738 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04110749935731292
2025-03-08 14:55:21,412 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04232180513441563
2025-03-08 14:55:33,184 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.042769917054101825
2025-03-08 14:55:44,859 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04251565736532211
2025-03-08 14:55:53,818 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_132_epoch.pt
2025-03-08 14:56:05,383 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03928904324769974
2025-03-08 14:56:16,885 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.0401325342617929
2025-03-08 14:56:28,711 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.0410392190143466
2025-03-08 14:56:40,732 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.042502669999375936
2025-03-08 14:56:53,448 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04363880822062492
2025-03-08 14:57:02,409 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_133_epoch.pt
2025-03-08 14:57:15,143 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03937921367585659
2025-03-08 14:57:26,558 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.046128582675009966
2025-03-08 14:57:38,304 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04706842456012964
2025-03-08 14:57:49,565 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.048396104900166395
2025-03-08 14:58:01,142 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04863640046864748
2025-03-08 14:58:10,933 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_134_epoch.pt
2025-03-08 14:58:22,317 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03801937572658062
2025-03-08 14:58:34,365 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.03928400931879878
2025-03-08 14:58:46,014 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.040432465535899006
2025-03-08 14:58:58,104 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.0431968688685447
2025-03-08 14:59:09,962 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.042664812080562116
2025-03-08 14:59:19,771 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_135_epoch.pt
2025-03-08 14:59:31,888 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04064104203134775
2025-03-08 14:59:43,186 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04066682361066341
2025-03-08 14:59:55,390 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.040116576167444386
2025-03-08 15:00:07,404 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.03895961559377611
2025-03-08 15:00:18,947 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.039559216469526294
2025-03-08 15:00:28,687 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_136_epoch.pt
2025-03-08 15:00:41,447 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04358052350580692
2025-03-08 15:00:52,761 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04691668743267655
2025-03-08 15:01:04,695 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.047752805550893146
2025-03-08 15:01:16,300 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04638491825200617
2025-03-08 15:01:27,353 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04606738972663879
2025-03-08 15:01:37,310 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_137_epoch.pt
2025-03-08 15:01:49,779 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03872761618345976
2025-03-08 15:02:01,149 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04004588253796101
2025-03-08 15:02:12,464 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04381652678052584
2025-03-08 15:02:24,799 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04477428128011525
2025-03-08 15:02:36,028 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04473492127656937
2025-03-08 15:02:45,673 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_138_epoch.pt
2025-03-08 15:02:57,788 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.037245636023581025
2025-03-08 15:03:08,164 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.03910766121000051
2025-03-08 15:03:19,116 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.0398345093925794
2025-03-08 15:03:31,709 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04191978398710489
2025-03-08 15:03:43,782 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04271039547026157
2025-03-08 15:03:53,710 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_139_epoch.pt
2025-03-08 15:04:05,667 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.05662826351821423
2025-03-08 15:04:16,721 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04524298220872879
2025-03-08 15:04:29,229 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04640699494630098
2025-03-08 15:04:40,790 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.045005548438057305
2025-03-08 15:04:52,947 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.045028355315327645
2025-03-08 15:05:02,123 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_140_epoch.pt
2025-03-08 15:05:13,299 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04606800377368927
2025-03-08 15:05:25,541 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.045098415073007346
2025-03-08 15:05:38,296 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04646515024205049
2025-03-08 15:05:49,824 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04531698054634035
2025-03-08 15:06:01,521 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04488767558336258
2025-03-08 15:06:10,608 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_141_epoch.pt
2025-03-08 15:06:22,108 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04650904688984156
2025-03-08 15:06:34,180 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.046456351764500144
2025-03-08 15:06:45,406 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.045671533880134424
2025-03-08 15:06:56,954 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04705064022913575
2025-03-08 15:07:09,510 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04618653464317322
2025-03-08 15:07:18,512 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_142_epoch.pt
2025-03-08 15:07:31,134 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04578192710876465
2025-03-08 15:07:42,639 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.048575387336313725
2025-03-08 15:07:54,407 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.049214462575813134
2025-03-08 15:08:06,244 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04904007377102971
2025-03-08 15:08:18,303 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.0486122153699398
2025-03-08 15:08:27,600 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_143_epoch.pt
2025-03-08 15:08:40,827 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03917152669280768
2025-03-08 15:08:52,901 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.044102181289345024
2025-03-08 15:09:03,859 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.047053503580391405
2025-03-08 15:09:15,397 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04798204592429101
2025-03-08 15:09:26,724 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.047999938525259495
2025-03-08 15:09:35,684 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_144_epoch.pt
2025-03-08 15:09:47,221 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.040599946156144144
2025-03-08 15:09:58,150 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.03676796745508909
2025-03-08 15:10:10,428 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.039392310604453085
2025-03-08 15:10:22,423 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.040361420139670375
2025-03-08 15:10:34,531 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04291992347687483
2025-03-08 15:10:44,463 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_145_epoch.pt
2025-03-08 15:10:56,366 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.039769036807119844
2025-03-08 15:11:07,547 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04454023141413927
2025-03-08 15:11:18,980 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04662162714948257
2025-03-08 15:11:31,375 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04645255065523088
2025-03-08 15:11:42,658 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.0477826658859849
2025-03-08 15:11:52,027 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_146_epoch.pt
2025-03-08 15:12:04,214 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03617035523056984
2025-03-08 15:12:16,589 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.03868978183716536
2025-03-08 15:12:28,025 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.03684281434863806
2025-03-08 15:12:39,646 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.03756438496522605
2025-03-08 15:12:51,215 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.037987682305276396
2025-03-08 15:13:00,657 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_147_epoch.pt
2025-03-08 15:13:13,615 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03893025543540716
2025-03-08 15:13:24,889 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.0455494543351233
2025-03-08 15:13:36,392 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04396159240355094
2025-03-08 15:13:47,443 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.0454021324776113
2025-03-08 15:13:58,809 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04525069360435009
2025-03-08 15:14:08,911 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_148_epoch.pt
2025-03-08 15:14:21,171 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04452504709362984
2025-03-08 15:14:33,182 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.0415923972427845
2025-03-08 15:14:45,341 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04558132426192363
2025-03-08 15:14:56,440 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04398326424881816
2025-03-08 15:15:08,026 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.042324691824615
2025-03-08 15:15:16,858 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_149_epoch.pt
2025-03-08 15:15:28,195 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04517427321523428
2025-03-08 15:15:39,697 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04138543853536248
2025-03-08 15:15:52,891 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.040254669648905596
2025-03-08 15:16:04,167 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.0408485621213913
2025-03-08 15:16:15,235 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.040038545675575736
2025-03-08 15:16:25,104 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_150_epoch.pt
2025-03-08 15:16:37,320 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03742494650185108
2025-03-08 15:16:49,464 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04092757601290941
2025-03-08 15:17:00,100 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04238424268861612
2025-03-08 15:17:11,931 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.044372633593156935
2025-03-08 15:17:23,207 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04525935016572476
2025-03-08 15:17:33,669 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_151_epoch.pt
2025-03-08 15:17:46,261 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04982364751398563
2025-03-08 15:17:57,837 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04658156791701913
2025-03-08 15:18:09,095 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04432639071096977
2025-03-08 15:18:20,991 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.044745784830302
2025-03-08 15:18:32,239 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04457508648186922
2025-03-08 15:18:41,652 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_152_epoch.pt
2025-03-08 15:18:54,276 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.045164900831878185
2025-03-08 15:19:06,143 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04557895367965102
2025-03-08 15:19:17,530 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.048999614144364996
2025-03-08 15:19:29,096 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.048200820256024596
2025-03-08 15:19:40,278 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04780368871986866
2025-03-08 15:19:49,505 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_153_epoch.pt
2025-03-08 15:20:01,152 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04870687637478113
2025-03-08 15:20:12,331 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.046075216047465804
2025-03-08 15:20:24,143 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.045936881465216475
2025-03-08 15:20:37,117 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.044746230635792014
2025-03-08 15:20:49,024 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.043993527427315715
2025-03-08 15:20:57,729 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_154_epoch.pt
2025-03-08 15:21:10,600 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.040049071870744225
2025-03-08 15:21:21,654 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.03704573903232813
2025-03-08 15:21:32,335 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.038528833004335565
2025-03-08 15:21:44,242 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.03851633932441473
2025-03-08 15:21:56,770 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04059210282564163
2025-03-08 15:22:06,425 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_155_epoch.pt
2025-03-08 15:22:18,591 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.047911293283104894
2025-03-08 15:22:29,680 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04536878550425172
2025-03-08 15:22:41,666 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04464480063567559
2025-03-08 15:22:53,332 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04349250151775777
2025-03-08 15:23:05,179 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04336465734243393
2025-03-08 15:23:15,018 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_156_epoch.pt
2025-03-08 15:23:26,936 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04887689609080553
2025-03-08 15:23:38,645 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04695551482960582
2025-03-08 15:23:50,452 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04587111578633388
2025-03-08 15:24:02,666 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04541658652946353
2025-03-08 15:24:14,391 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04554530688375234
2025-03-08 15:24:23,208 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_157_epoch.pt
2025-03-08 15:24:35,795 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.043223119378089904
2025-03-08 15:24:46,900 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04306033270433545
2025-03-08 15:24:58,251 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.043128496619562307
2025-03-08 15:25:09,672 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04327042101882398
2025-03-08 15:25:21,825 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04400539112836122
2025-03-08 15:25:31,424 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_158_epoch.pt
2025-03-08 15:25:41,775 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.042603477872908115
2025-03-08 15:25:54,096 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04692678140476346
2025-03-08 15:26:05,980 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04841035137573878
2025-03-08 15:26:17,593 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.0481067016068846
2025-03-08 15:26:30,197 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04797798125445843
2025-03-08 15:26:39,822 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_159_epoch.pt
2025-03-08 15:26:51,948 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04011489555239677
2025-03-08 15:27:03,287 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.041030000746250156
2025-03-08 15:27:15,166 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04026009937127431
2025-03-08 15:27:26,526 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04218825054354966
2025-03-08 15:27:37,851 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.043238391138613226
2025-03-08 15:27:47,882 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_160_epoch.pt
2025-03-08 15:28:00,234 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04564396370202303
2025-03-08 15:28:11,797 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.046660449244081977
2025-03-08 15:28:23,438 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04796981995304426
2025-03-08 15:28:34,752 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04836047249846161
2025-03-08 15:28:45,996 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04739976952970028
2025-03-08 15:28:56,342 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_161_epoch.pt
2025-03-08 15:29:08,609 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.05564698681235313
2025-03-08 15:29:20,316 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.05228720005601645
2025-03-08 15:29:31,145 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.048556021476785345
2025-03-08 15:29:42,278 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.048102520117536185
2025-03-08 15:29:54,148 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.047631537571549414
2025-03-08 15:30:04,857 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_162_epoch.pt
2025-03-08 15:30:16,500 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.05258289862424135
2025-03-08 15:30:28,369 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.05031307853758335
2025-03-08 15:30:40,486 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04800430429478486
2025-03-08 15:30:51,924 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04566390425898135
2025-03-08 15:31:03,216 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04579495497047901
2025-03-08 15:31:12,595 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_163_epoch.pt
2025-03-08 15:31:24,017 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.0479256272315979
2025-03-08 15:31:35,314 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04703421428799629
2025-03-08 15:31:46,299 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04805565184603135
2025-03-08 15:31:58,868 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04601594425737858
2025-03-08 15:32:10,169 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04516880578547716
2025-03-08 15:32:20,577 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_164_epoch.pt
2025-03-08 15:32:32,160 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.051404612250626085
2025-03-08 15:32:44,845 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.050677229594439266
2025-03-08 15:32:56,477 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04877256552378337
2025-03-08 15:33:07,922 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.045892412215471266
2025-03-08 15:33:19,929 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04569786301255226
2025-03-08 15:33:28,942 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_165_epoch.pt
2025-03-08 15:33:41,153 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04958638112992048
2025-03-08 15:33:52,415 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04534324692562222
2025-03-08 15:34:04,190 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04395174360523621
2025-03-08 15:34:16,430 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04267051532864571
2025-03-08 15:34:27,470 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04238422148674727
2025-03-08 15:34:36,872 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_166_epoch.pt
2025-03-08 15:34:49,762 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04840514559298754
2025-03-08 15:35:00,897 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04920296791940928
2025-03-08 15:35:12,823 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04606122779349486
2025-03-08 15:35:24,709 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.043010963071137664
2025-03-08 15:35:36,045 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04296132975071669
2025-03-08 15:35:44,745 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_167_epoch.pt
2025-03-08 15:35:58,172 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04701452180743217
2025-03-08 15:36:09,817 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.048340246845036745
2025-03-08 15:36:20,512 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.0460385192806522
2025-03-08 15:36:33,167 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.0459113297238946
2025-03-08 15:36:45,209 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04631730127334595
2025-03-08 15:36:53,820 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_168_epoch.pt
2025-03-08 15:37:06,308 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04599467758089304
2025-03-08 15:37:17,163 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04388790588825941
2025-03-08 15:37:29,665 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.044836962086459
2025-03-08 15:37:40,502 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.043791790157556534
2025-03-08 15:37:52,105 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.045253848046064374
2025-03-08 15:38:01,809 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_169_epoch.pt
2025-03-08 15:38:14,680 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04811688259243965
2025-03-08 15:38:27,233 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04431277660652995
2025-03-08 15:38:38,671 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04289654122044643
2025-03-08 15:38:49,948 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.0444430144969374
2025-03-08 15:39:00,823 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.044020702786743644
2025-03-08 15:39:10,798 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_170_epoch.pt
2025-03-08 15:39:22,742 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.049062637984752654
2025-03-08 15:39:33,979 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04765468841418624
2025-03-08 15:39:46,567 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04427644208073616
2025-03-08 15:39:58,605 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.043840682841837404
2025-03-08 15:40:09,510 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04340244033187628
2025-03-08 15:40:19,175 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_171_epoch.pt
2025-03-08 15:40:31,304 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.048616242967545985
2025-03-08 15:40:42,976 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04523284723982215
2025-03-08 15:40:55,782 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04381807532161474
2025-03-08 15:41:06,878 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.043644279576838016
2025-03-08 15:41:17,749 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04259643796086311
2025-03-08 15:41:27,012 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_172_epoch.pt
2025-03-08 15:41:39,658 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04614756982773542
2025-03-08 15:41:51,116 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.041988157778978345
2025-03-08 15:42:03,087 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.03995463551332553
2025-03-08 15:42:14,568 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04020939751528203
2025-03-08 15:42:26,017 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04081703396141529
2025-03-08 15:42:35,841 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_173_epoch.pt
2025-03-08 15:42:47,063 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04153683692216873
2025-03-08 15:42:57,796 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04337236260995269
2025-03-08 15:43:09,684 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04454650741070509
2025-03-08 15:43:22,389 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04254889969713986
2025-03-08 15:43:33,815 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.041093050718307496
2025-03-08 15:43:43,697 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_174_epoch.pt
2025-03-08 15:43:56,364 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.043888875469565394
2025-03-08 15:44:08,394 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04450016941875219
2025-03-08 15:44:19,227 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04444656599313021
2025-03-08 15:44:30,898 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.044600863726809624
2025-03-08 15:44:43,437 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04536620280891657
2025-03-08 15:44:52,796 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_175_epoch.pt
2025-03-08 15:45:05,738 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04875123407691717
2025-03-08 15:45:17,691 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04497279090806842
2025-03-08 15:45:30,098 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.043166920654475686
2025-03-08 15:45:41,364 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04298990638926625
2025-03-08 15:45:52,624 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04233312779664993
2025-03-08 15:46:00,999 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_176_epoch.pt
2025-03-08 15:46:12,924 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04647634744644165
2025-03-08 15:46:25,048 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.0433105593919754
2025-03-08 15:46:37,214 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04215974652518829
2025-03-08 15:46:48,267 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04243212584406138
2025-03-08 15:46:59,364 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04201265898346901
2025-03-08 15:47:08,820 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_177_epoch.pt
2025-03-08 15:47:20,716 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04540849208831787
2025-03-08 15:47:32,455 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04277574265375733
2025-03-08 15:47:45,025 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04399948128809532
2025-03-08 15:47:55,977 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04298225084319711
2025-03-08 15:48:07,639 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04427161150425672
2025-03-08 15:48:16,728 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_178_epoch.pt
2025-03-08 15:48:28,521 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03658734556287527
2025-03-08 15:48:40,747 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.038860116712749006
2025-03-08 15:48:53,665 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04300209070245425
2025-03-08 15:49:05,917 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04462118282914162
2025-03-08 15:49:16,107 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04625259551405907
2025-03-08 15:49:24,575 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_179_epoch.pt
2025-03-08 15:49:36,586 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03733651719987392
2025-03-08 15:49:47,849 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04272350309416652
2025-03-08 15:49:59,589 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.03993297128627698
2025-03-08 15:50:11,391 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.042184713613241914
2025-03-08 15:50:23,216 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04286412067711353
2025-03-08 15:50:32,928 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_180_epoch.pt
2025-03-08 15:50:45,068 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04118026353418827
2025-03-08 15:50:57,112 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04293297713622451
2025-03-08 15:51:08,084 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04218248744805654
2025-03-08 15:51:20,253 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.041128751905635
2025-03-08 15:51:32,322 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.03992936885356903
2025-03-08 15:51:41,609 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_181_epoch.pt
2025-03-08 15:51:53,020 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.05115016788244248
2025-03-08 15:52:05,728 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.046185814626514915
2025-03-08 15:52:17,367 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04503235559910536
2025-03-08 15:52:28,732 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.045226109335199
2025-03-08 15:52:40,560 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.044562081448733804
2025-03-08 15:52:50,274 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_182_epoch.pt
2025-03-08 15:53:02,214 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.048246467448771
2025-03-08 15:53:14,039 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.05080884285271168
2025-03-08 15:53:25,596 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04881860371679068
2025-03-08 15:53:37,501 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.048268870757892726
2025-03-08 15:53:49,327 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.046544550709426404
2025-03-08 15:53:58,421 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_183_epoch.pt
2025-03-08 15:54:10,396 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.0383367283642292
2025-03-08 15:54:21,620 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.03560097647830844
2025-03-08 15:54:33,470 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.03965885152419408
2025-03-08 15:54:44,794 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04101450880989432
2025-03-08 15:54:57,622 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04046071361750364
2025-03-08 15:55:06,885 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_184_epoch.pt
2025-03-08 15:55:19,796 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.0399986258521676
2025-03-08 15:55:31,190 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04130235765129328
2025-03-08 15:55:42,810 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04202233945329984
2025-03-08 15:55:53,823 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.043753439001739024
2025-03-08 15:56:06,265 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.0425586639419198
2025-03-08 15:56:15,641 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_185_epoch.pt
2025-03-08 15:56:28,516 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04862269435077906
2025-03-08 15:56:39,739 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.044315878599882126
2025-03-08 15:56:50,610 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04256215003629526
2025-03-08 15:57:03,003 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04183632934466004
2025-03-08 15:57:14,307 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.041562872484326364
2025-03-08 15:57:24,041 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_186_epoch.pt
2025-03-08 15:57:36,379 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.036265814676880836
2025-03-08 15:57:47,236 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.03717754205688834
2025-03-08 15:57:57,780 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.03738507358978192
2025-03-08 15:58:08,783 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.0383545046672225
2025-03-08 15:58:20,992 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.03852420174330473
2025-03-08 15:58:30,931 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_187_epoch.pt
2025-03-08 15:58:42,368 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04579557038843632
2025-03-08 15:58:53,886 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04603791292756796
2025-03-08 15:59:05,440 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.045568471364676955
2025-03-08 15:59:16,846 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04487092005088925
2025-03-08 15:59:29,418 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04562544970214367
2025-03-08 15:59:38,757 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_188_epoch.pt
2025-03-08 15:59:51,118 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.0451912809163332
2025-03-08 16:00:02,692 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04635055378079414
2025-03-08 16:00:13,743 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04564070674280325
2025-03-08 16:00:26,699 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.0448102667555213
2025-03-08 16:00:38,242 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04456241939216852
2025-03-08 16:00:47,056 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_189_epoch.pt
2025-03-08 16:00:59,965 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04687087129801512
2025-03-08 16:01:10,832 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.045537721402943133
2025-03-08 16:01:22,043 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.043038947929938634
2025-03-08 16:01:34,904 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04304562484845519
2025-03-08 16:01:46,261 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04421319580078125
2025-03-08 16:01:55,304 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_190_epoch.pt
2025-03-08 16:02:06,904 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.0460006869584322
2025-03-08 16:02:19,389 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.0453613606095314
2025-03-08 16:02:31,619 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04515433721244335
2025-03-08 16:02:42,754 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.045019561573863026
2025-03-08 16:02:54,565 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04477950599044561
2025-03-08 16:03:03,632 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_191_epoch.pt
2025-03-08 16:03:14,742 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03703654117882252
2025-03-08 16:03:26,575 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.044866731949150564
2025-03-08 16:03:38,122 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04453205369412899
2025-03-08 16:03:50,362 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04599493170157075
2025-03-08 16:04:01,618 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.0455808015242219
2025-03-08 16:04:11,438 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_192_epoch.pt
2025-03-08 16:04:23,217 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03825929522514343
2025-03-08 16:04:35,507 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.041104966308921576
2025-03-08 16:04:46,366 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.0417678427323699
2025-03-08 16:04:57,986 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.03931357005611062
2025-03-08 16:05:09,763 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.040570417769253256
2025-03-08 16:05:19,918 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_193_epoch.pt
2025-03-08 16:05:31,646 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.037490420639514924
2025-03-08 16:05:43,735 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.038221498336642984
2025-03-08 16:05:56,189 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04031792974720399
2025-03-08 16:06:07,967 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04201333463191986
2025-03-08 16:06:19,556 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04246787720918655
2025-03-08 16:06:28,034 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_194_epoch.pt
2025-03-08 16:06:40,327 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.043718984127044676
2025-03-08 16:06:51,888 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04226280920207501
2025-03-08 16:07:03,908 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04291669475535552
2025-03-08 16:07:16,257 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.0404079211037606
2025-03-08 16:07:27,050 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.040788637965917585
2025-03-08 16:07:36,039 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_195_epoch.pt
2025-03-08 16:07:47,487 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03880074504762888
2025-03-08 16:07:58,982 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04245538236573339
2025-03-08 16:08:11,594 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04283541178951661
2025-03-08 16:08:23,489 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04374447948299348
2025-03-08 16:08:35,017 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.046496332675218584
2025-03-08 16:08:44,269 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_196_epoch.pt
2025-03-08 16:08:56,409 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.043429837003350256
2025-03-08 16:09:08,765 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04699699671939015
2025-03-08 16:09:20,402 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.046096543048818904
2025-03-08 16:09:31,933 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.045683592334389685
2025-03-08 16:09:43,551 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04492066393047571
2025-03-08 16:09:52,587 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_197_epoch.pt
2025-03-08 16:10:04,278 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03687508676201105
2025-03-08 16:10:15,512 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.03647721841931343
2025-03-08 16:10:26,983 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04167370898028215
2025-03-08 16:10:39,678 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04208264989778399
2025-03-08 16:10:51,107 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04328116577863693
2025-03-08 16:11:00,678 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_198_epoch.pt
2025-03-08 16:11:13,176 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04779006872326136
2025-03-08 16:11:24,846 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04545339083299041
2025-03-08 16:11:35,808 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04317121112098297
2025-03-08 16:11:47,531 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.043087000967934726
2025-03-08 16:11:59,451 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04166443658620119
2025-03-08 16:12:09,008 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_199_epoch.pt
2025-03-08 16:12:21,185 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03953270301222801
2025-03-08 16:12:32,432 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.0394971227273345
2025-03-08 16:12:43,978 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04266148742288351
2025-03-08 16:12:56,437 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.043331902576610445
2025-03-08 16:13:07,603 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04212386511266231
2025-03-08 16:13:17,603 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_4/_bert-base-uncased_200_epoch.pt
2025-03-08 16:13:18,856 : 916236790.py : __call__ : INFO : loaded dataset, sample = 1169
2025-03-08 16:13:18,857 : 1341934384.py : train_scp : INFO : 109483778
2025-03-08 16:13:30,892 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.1991146783530712
2025-03-08 16:13:42,485 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.20321589406579732
2025-03-08 16:13:54,239 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.1993569193035364
2025-03-08 16:14:05,362 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.19746965961530805
2025-03-08 16:14:16,534 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.196343823030591
2025-03-08 16:14:26,438 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_1_epoch.pt
2025-03-08 16:14:39,311 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.19225098699331283
2025-03-08 16:14:51,078 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.18909913012757898
2025-03-08 16:15:01,955 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.19015430697550376
2025-03-08 16:15:13,401 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.19060675387270748
2025-03-08 16:15:24,989 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.19119723864644766
2025-03-08 16:15:34,276 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_2_epoch.pt
2025-03-08 16:15:46,174 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.18915896654129027
2025-03-08 16:15:56,135 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.18351702839136125
2025-03-08 16:16:07,944 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.18012025420864422
2025-03-08 16:16:20,633 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.176641431087628
2025-03-08 16:16:31,914 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.17510396917164325
2025-03-08 16:16:41,989 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_3_epoch.pt
2025-03-08 16:16:54,890 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.14653641521930694
2025-03-08 16:17:06,611 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.13820527149364353
2025-03-08 16:17:17,972 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.1311022346590956
2025-03-08 16:17:29,116 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.12520301503129302
2025-03-08 16:17:40,481 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.11735198853164912
2025-03-08 16:17:49,424 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_4_epoch.pt
2025-03-08 16:18:00,559 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.08282803785055876
2025-03-08 16:18:11,735 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.0806326961889863
2025-03-08 16:18:24,383 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.0787388097991546
2025-03-08 16:18:36,871 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.07632282482460141
2025-03-08 16:18:47,966 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.07769244624674321
2025-03-08 16:18:56,654 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_5_epoch.pt
2025-03-08 16:19:09,144 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.07090597681701183
2025-03-08 16:19:19,861 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.07135523412376642
2025-03-08 16:19:31,694 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.07254726198812325
2025-03-08 16:19:42,936 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.07333826495334506
2025-03-08 16:19:54,246 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.07379923188686371
2025-03-08 16:20:03,637 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_6_epoch.pt
2025-03-08 16:20:15,282 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.07174954812973738
2025-03-08 16:20:26,075 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.0673536504432559
2025-03-08 16:20:37,834 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.06999742909024159
2025-03-08 16:20:49,536 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.06896395955234766
2025-03-08 16:21:00,863 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.07043407464772462
2025-03-08 16:21:10,524 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_7_epoch.pt
2025-03-08 16:21:23,442 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.06861542262136937
2025-03-08 16:21:35,775 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.06463714499026536
2025-03-08 16:21:46,846 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.0676091929897666
2025-03-08 16:21:58,273 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.06589633261784911
2025-03-08 16:22:09,312 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.06534003444761038
2025-03-08 16:22:18,228 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_8_epoch.pt
2025-03-08 16:22:29,409 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.06015412364155054
2025-03-08 16:22:41,265 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.05936509542167187
2025-03-08 16:22:53,782 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.06112039578457673
2025-03-08 16:23:04,970 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.0633772255666554
2025-03-08 16:23:16,589 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.06097672607004642
2025-03-08 16:23:25,471 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_9_epoch.pt
2025-03-08 16:23:37,242 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.06305046647787094
2025-03-08 16:23:48,722 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.05918647825717926
2025-03-08 16:24:00,194 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.060294162829717
2025-03-08 16:24:12,380 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.06406329780817031
2025-03-08 16:24:23,969 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.06420155350118875
2025-03-08 16:24:32,623 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_10_epoch.pt
2025-03-08 16:24:44,593 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.06293710634112358
2025-03-08 16:24:55,951 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.059675513748079535
2025-03-08 16:25:08,128 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.061726388819515705
2025-03-08 16:25:19,444 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.06107662542723119
2025-03-08 16:25:30,544 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.06090318968892097
2025-03-08 16:25:39,603 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_11_epoch.pt
2025-03-08 16:25:50,977 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.06446750443428755
2025-03-08 16:26:02,672 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.06056738473474979
2025-03-08 16:26:14,314 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.06081589010854562
2025-03-08 16:26:25,425 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.06217521136626601
2025-03-08 16:26:36,844 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.061950301483273504
2025-03-08 16:26:46,728 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_12_epoch.pt
2025-03-08 16:26:58,989 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.05774494878947735
2025-03-08 16:27:09,553 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.060021023135632276
2025-03-08 16:27:21,262 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.05654251328359047
2025-03-08 16:27:32,314 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.056194525565952065
2025-03-08 16:27:43,841 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.05484367050975561
2025-03-08 16:27:53,839 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_13_epoch.pt
2025-03-08 16:28:05,890 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.05469015214592218
2025-03-08 16:28:17,329 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.05561747934669256
2025-03-08 16:28:28,791 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.05472221723447243
2025-03-08 16:28:40,328 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.05585933427326381
2025-03-08 16:28:51,972 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.05530866947770119
2025-03-08 16:29:01,506 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_14_epoch.pt
2025-03-08 16:29:13,915 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.06373614493757486
2025-03-08 16:29:25,233 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.05719958210363984
2025-03-08 16:29:36,541 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.05458853349089623
2025-03-08 16:29:47,183 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.051538499090820554
2025-03-08 16:29:58,875 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.051473523430526254
2025-03-08 16:30:08,839 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_15_epoch.pt
2025-03-08 16:30:20,979 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.0637518148124218
2025-03-08 16:30:32,024 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.05685161782428622
2025-03-08 16:30:43,581 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.052900045303006966
2025-03-08 16:30:55,378 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.05352977520786226
2025-03-08 16:31:06,709 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.05391549046337604
2025-03-08 16:31:16,515 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_16_epoch.pt
2025-03-08 16:31:27,456 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.05380777038633824
2025-03-08 16:31:40,238 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.050714468620717525
2025-03-08 16:31:51,724 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.056611732207238676
2025-03-08 16:32:03,417 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.05709697092883289
2025-03-08 16:32:14,566 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.05666755272448063
2025-03-08 16:32:23,597 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_17_epoch.pt
2025-03-08 16:32:35,014 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.048203361853957176
2025-03-08 16:32:46,161 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.05180926160886884
2025-03-08 16:32:58,462 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.05044927731156349
2025-03-08 16:33:09,773 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.05052048915065825
2025-03-08 16:33:20,709 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.048814913012087346
2025-03-08 16:33:29,789 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_18_epoch.pt
2025-03-08 16:33:42,309 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.0468142070248723
2025-03-08 16:33:53,679 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04961325569078326
2025-03-08 16:34:05,143 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04980390659223
2025-03-08 16:34:16,849 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.0495640299282968
2025-03-08 16:34:28,211 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04917071586102247
2025-03-08 16:34:37,527 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_19_epoch.pt
2025-03-08 16:34:49,069 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.05109190169721842
2025-03-08 16:35:00,911 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.05212886355817318
2025-03-08 16:35:11,765 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.05025820774336656
2025-03-08 16:35:23,218 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.0509579224139452
2025-03-08 16:35:35,549 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.050358474157750605
2025-03-08 16:35:44,866 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_20_epoch.pt
2025-03-08 16:35:57,169 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.06007001161575318
2025-03-08 16:36:08,189 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.05358320174738765
2025-03-08 16:36:20,502 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.05077724053213994
2025-03-08 16:36:31,685 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.052542711785063145
2025-03-08 16:36:41,959 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.05273955478519201
2025-03-08 16:36:51,669 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_21_epoch.pt
2025-03-08 16:37:03,164 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.046656244285404685
2025-03-08 16:37:13,952 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.0457674334384501
2025-03-08 16:37:25,322 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04853856064379215
2025-03-08 16:37:37,061 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.049800931885838506
2025-03-08 16:37:49,314 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.050918402001261714
2025-03-08 16:37:58,219 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_22_epoch.pt
2025-03-08 16:38:11,840 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.050037585720419886
2025-03-08 16:38:21,843 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.05095989529043436
2025-03-08 16:38:33,667 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04814412665863832
2025-03-08 16:38:45,018 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04849430038593709
2025-03-08 16:38:55,300 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04866676614433527
2025-03-08 16:39:05,496 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_23_epoch.pt
2025-03-08 16:39:17,036 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.05361767042428255
2025-03-08 16:39:29,237 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.05388427907600999
2025-03-08 16:39:39,918 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.05281402206669251
2025-03-08 16:39:51,155 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.050990586960688235
2025-03-08 16:40:02,723 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.05207164914906025
2025-03-08 16:40:12,551 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_24_epoch.pt
2025-03-08 16:40:24,740 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.05660183530300856
2025-03-08 16:40:36,008 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.050228623412549495
2025-03-08 16:40:46,648 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.05176177132874727
2025-03-08 16:40:58,601 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.05121783873066306
2025-03-08 16:41:10,208 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.05219504995644093
2025-03-08 16:41:19,368 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_25_epoch.pt
2025-03-08 16:41:32,395 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.05255667146295309
2025-03-08 16:41:42,773 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.05222966037690639
2025-03-08 16:41:54,075 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.0514883383984367
2025-03-08 16:42:05,673 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.0523277682531625
2025-03-08 16:42:16,669 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.051878239944577215
2025-03-08 16:42:26,478 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_26_epoch.pt
2025-03-08 16:42:38,235 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04295441929250956
2025-03-08 16:42:50,192 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04235174836590886
2025-03-08 16:43:01,394 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04482747095326583
2025-03-08 16:43:11,939 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.047174212960526346
2025-03-08 16:43:23,744 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04818908724188804
2025-03-08 16:43:33,070 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_27_epoch.pt
2025-03-08 16:43:44,646 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04884168229997158
2025-03-08 16:43:55,774 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.047781188134104015
2025-03-08 16:44:07,368 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.047325972281396386
2025-03-08 16:44:18,721 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.046508253272622826
2025-03-08 16:44:29,972 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.046516433715820316
2025-03-08 16:44:40,267 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_28_epoch.pt
2025-03-08 16:44:51,875 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.053357026763260366
2025-03-08 16:45:03,857 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04701705552637577
2025-03-08 16:45:15,271 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04805962098141511
2025-03-08 16:45:26,562 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04868924242444336
2025-03-08 16:45:37,227 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04968132321536541
2025-03-08 16:45:46,565 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_29_epoch.pt
2025-03-08 16:46:00,501 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04967706691473722
2025-03-08 16:46:11,686 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.05110634349286556
2025-03-08 16:46:23,685 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.05098432509849469
2025-03-08 16:46:34,644 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.05079153897240758
2025-03-08 16:46:44,853 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.051563328564167024
2025-03-08 16:46:53,920 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_30_epoch.pt
2025-03-08 16:47:05,735 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04602833155542612
2025-03-08 16:47:16,152 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04730660654604435
2025-03-08 16:47:28,222 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04783740878105164
2025-03-08 16:47:38,870 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04554371912963688
2025-03-08 16:47:50,356 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04774896560609341
2025-03-08 16:48:01,071 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_31_epoch.pt
2025-03-08 16:48:12,930 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.047150947339832786
2025-03-08 16:48:24,231 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04944841239601374
2025-03-08 16:48:35,659 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04660092658052842
2025-03-08 16:48:46,516 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.0459988321736455
2025-03-08 16:48:57,487 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04607102405279875
2025-03-08 16:49:07,694 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_32_epoch.pt
2025-03-08 16:49:19,302 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.040790865980088714
2025-03-08 16:49:30,678 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04687607122585177
2025-03-08 16:49:42,028 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04636589949329694
2025-03-08 16:49:53,878 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.046793165653944015
2025-03-08 16:50:04,685 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04715937892347574
2025-03-08 16:50:14,537 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_33_epoch.pt
2025-03-08 16:50:25,637 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.054356583431363105
2025-03-08 16:50:37,920 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.05262602368369698
2025-03-08 16:50:49,275 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.052600825317203996
2025-03-08 16:51:00,731 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.05128718088380992
2025-03-08 16:51:11,725 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.05189025431871414
2025-03-08 16:51:21,252 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_34_epoch.pt
2025-03-08 16:51:33,170 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03829405594617128
2025-03-08 16:51:44,235 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.03771803863346577
2025-03-08 16:51:56,211 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.039848597732683025
2025-03-08 16:52:07,899 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.043446524599567056
2025-03-08 16:52:18,564 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04526848012953997
2025-03-08 16:52:27,895 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_35_epoch.pt
2025-03-08 16:52:39,917 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.050711119696497914
2025-03-08 16:52:49,801 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04860539592802524
2025-03-08 16:53:01,724 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.0490102057904005
2025-03-08 16:53:13,009 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04707739247009158
2025-03-08 16:53:24,482 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.047341428488492966
2025-03-08 16:53:34,915 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_36_epoch.pt
2025-03-08 16:53:46,572 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04746229618787765
2025-03-08 16:53:57,138 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04669527389109135
2025-03-08 16:54:09,576 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04790286583205064
2025-03-08 16:54:20,687 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04712261999025941
2025-03-08 16:54:31,539 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04719282178580761
2025-03-08 16:54:42,045 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_37_epoch.pt
2025-03-08 16:54:54,460 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.0496756012365222
2025-03-08 16:55:05,394 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04604903120547533
2025-03-08 16:55:16,228 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04313401444504658
2025-03-08 16:55:27,632 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.043923973320052025
2025-03-08 16:55:39,739 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04647224199026823
2025-03-08 16:55:48,755 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_38_epoch.pt
2025-03-08 16:56:00,580 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.039463645070791246
2025-03-08 16:56:12,546 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04095265094190836
2025-03-08 16:56:22,950 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.041230920453866324
2025-03-08 16:56:33,954 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.043285103803500535
2025-03-08 16:56:45,646 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04450510054826737
2025-03-08 16:56:55,833 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_39_epoch.pt
2025-03-08 16:57:07,579 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04386333879083395
2025-03-08 16:57:18,830 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04915538921952248
2025-03-08 16:57:30,140 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.047566244987150036
2025-03-08 16:57:42,179 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.046957146152853964
2025-03-08 16:57:53,058 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04887115716934204
2025-03-08 16:58:02,877 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_40_epoch.pt
2025-03-08 16:58:13,801 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04353988930583
2025-03-08 16:58:24,605 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.0460183759406209
2025-03-08 16:58:36,673 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04606362355252107
2025-03-08 16:58:48,236 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.0439579941984266
2025-03-08 16:58:59,487 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.044180721960961816
2025-03-08 16:59:09,627 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_41_epoch.pt
2025-03-08 16:59:21,690 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04447418827563524
2025-03-08 16:59:32,508 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04164729256182909
2025-03-08 16:59:43,820 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04429074592888355
2025-03-08 16:59:54,643 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.042836309121921656
2025-03-08 17:00:06,274 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.045652490131556984
2025-03-08 17:00:16,925 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_42_epoch.pt
2025-03-08 17:00:28,695 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.053137837536633016
2025-03-08 17:00:40,302 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.048147611562162636
2025-03-08 17:00:52,082 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.0467594788223505
2025-03-08 17:01:03,245 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04606567136012018
2025-03-08 17:01:14,126 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04681589671969414
2025-03-08 17:01:23,422 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_43_epoch.pt
2025-03-08 17:01:34,704 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.05076973643153906
2025-03-08 17:01:46,754 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04574948692694306
2025-03-08 17:01:57,522 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.044781546145677566
2025-03-08 17:02:09,073 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.043353211795911194
2025-03-08 17:02:20,509 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04437335982173681
2025-03-08 17:02:30,987 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_44_epoch.pt
2025-03-08 17:02:43,979 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04820412140339613
2025-03-08 17:02:55,733 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04828724045306444
2025-03-08 17:03:06,498 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04663836287955443
2025-03-08 17:03:17,106 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04457909944467246
2025-03-08 17:03:28,785 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04474590028077364
2025-03-08 17:03:37,464 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_45_epoch.pt
2025-03-08 17:03:49,231 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04206127058714628
2025-03-08 17:04:00,842 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.042865767646580934
2025-03-08 17:04:12,635 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.043114503833154835
2025-03-08 17:04:23,767 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04412643210962415
2025-03-08 17:04:34,412 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.043834485299885276
2025-03-08 17:04:44,165 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_46_epoch.pt
2025-03-08 17:04:56,538 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.05380953185260296
2025-03-08 17:05:09,179 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04515450164675713
2025-03-08 17:05:19,248 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.042518008475502335
2025-03-08 17:05:29,704 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.042287338795140385
2025-03-08 17:05:41,621 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04192381784319878
2025-03-08 17:05:51,154 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_47_epoch.pt
2025-03-08 17:06:03,242 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.0526355766877532
2025-03-08 17:06:15,321 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.05099796319380403
2025-03-08 17:06:26,407 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04769843300183614
2025-03-08 17:06:37,723 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04736382683739066
2025-03-08 17:06:48,758 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04584311097115278
2025-03-08 17:06:57,878 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_48_epoch.pt
2025-03-08 17:07:09,852 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.035607558563351634
2025-03-08 17:07:21,025 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.03863514667376876
2025-03-08 17:07:32,740 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04333955618242423
2025-03-08 17:07:44,446 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04360384481027722
2025-03-08 17:07:56,339 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.043151335313916205
2025-03-08 17:08:05,879 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_49_epoch.pt
2025-03-08 17:08:17,361 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03887132316827774
2025-03-08 17:08:29,360 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.03882427191361785
2025-03-08 17:08:41,029 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04061205220719179
2025-03-08 17:08:51,914 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.041386379562318325
2025-03-08 17:09:03,149 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04183829560130835
2025-03-08 17:09:12,494 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_50_epoch.pt
2025-03-08 17:09:24,332 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04700728911906481
2025-03-08 17:09:36,431 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04472808580845594
2025-03-08 17:09:47,261 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04786746114492416
2025-03-08 17:09:59,089 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.048100611045956615
2025-03-08 17:10:10,874 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04818554834276438
2025-03-08 17:10:19,710 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_51_epoch.pt
2025-03-08 17:10:31,993 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04338314477354288
2025-03-08 17:10:43,347 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.046645586602389814
2025-03-08 17:10:54,947 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04687665355702241
2025-03-08 17:11:05,282 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.0484015990421176
2025-03-08 17:11:16,903 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04927080269902945
2025-03-08 17:11:26,230 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_52_epoch.pt
2025-03-08 17:11:38,584 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.053929915204644206
2025-03-08 17:11:49,985 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.0471881665289402
2025-03-08 17:12:01,274 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.046655004235605396
2025-03-08 17:12:12,220 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04744219858199358
2025-03-08 17:12:23,167 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04607323752343655
2025-03-08 17:12:33,035 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_53_epoch.pt
2025-03-08 17:12:45,626 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.0439559892937541
2025-03-08 17:12:56,395 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04178087875247002
2025-03-08 17:13:07,966 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04338069857408603
2025-03-08 17:13:19,075 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.041366069484502076
2025-03-08 17:13:30,833 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.0415296067148447
2025-03-08 17:13:40,757 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_54_epoch.pt
2025-03-08 17:13:52,486 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.042738749384880065
2025-03-08 17:14:04,318 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04214883167296648
2025-03-08 17:14:16,000 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.042374223843216895
2025-03-08 17:14:27,314 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.040557010769844054
2025-03-08 17:14:38,906 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04189479406923056
2025-03-08 17:14:48,195 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_55_epoch.pt
2025-03-08 17:15:00,601 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.037965946979820725
2025-03-08 17:15:10,672 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04375717766582966
2025-03-08 17:15:22,768 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04152558250973622
2025-03-08 17:15:34,521 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.041117809377610684
2025-03-08 17:15:45,926 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.043756648175418376
2025-03-08 17:15:55,227 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_56_epoch.pt
2025-03-08 17:16:06,734 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04689434606581926
2025-03-08 17:16:17,832 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04101164834573865
2025-03-08 17:16:29,081 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04229893870651722
2025-03-08 17:16:40,808 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04230557031929493
2025-03-08 17:16:52,294 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.043490896813571454
2025-03-08 17:17:02,236 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_57_epoch.pt
2025-03-08 17:17:14,003 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.046144632697105406
2025-03-08 17:17:25,094 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.047023810390383
2025-03-08 17:17:36,971 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04650716261317333
2025-03-08 17:17:48,227 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04693304661661386
2025-03-08 17:17:58,996 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04592017566412687
2025-03-08 17:18:09,093 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_58_epoch.pt
2025-03-08 17:18:21,430 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04513214312493801
2025-03-08 17:18:32,865 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.046330120153725146
2025-03-08 17:18:43,881 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04585988296816746
2025-03-08 17:18:54,850 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04324384402483702
2025-03-08 17:19:06,328 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04440499862283468
2025-03-08 17:19:15,684 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_59_epoch.pt
2025-03-08 17:19:27,037 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.036050511598587034
2025-03-08 17:19:37,370 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.03697319310158491
2025-03-08 17:19:49,350 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.03716476035614808
2025-03-08 17:20:00,589 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.03975918755866587
2025-03-08 17:20:12,583 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.03895128735154867
2025-03-08 17:20:22,841 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_60_epoch.pt
2025-03-08 17:20:34,379 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.036921190246939656
2025-03-08 17:20:45,809 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.0373624262958765
2025-03-08 17:20:56,852 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04093350077668826
2025-03-08 17:21:08,896 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.041836688676849004
2025-03-08 17:21:20,347 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04266311019659042
2025-03-08 17:21:29,715 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_61_epoch.pt
2025-03-08 17:21:41,441 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04312752071768045
2025-03-08 17:21:53,412 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04528559425845742
2025-03-08 17:22:04,303 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04433869698395332
2025-03-08 17:22:15,625 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04462453872896731
2025-03-08 17:22:27,182 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.0439041672796011
2025-03-08 17:22:36,375 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_62_epoch.pt
2025-03-08 17:22:48,225 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04048805981874466
2025-03-08 17:22:58,838 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04517667124047876
2025-03-08 17:23:10,538 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04313807292530934
2025-03-08 17:23:22,231 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04417801587842405
2025-03-08 17:23:33,863 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04198129668086767
2025-03-08 17:23:43,166 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_63_epoch.pt
2025-03-08 17:23:55,160 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04474903594702482
2025-03-08 17:24:06,077 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04391114935278893
2025-03-08 17:24:17,263 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.044563618463774525
2025-03-08 17:24:29,255 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04550378461368382
2025-03-08 17:24:40,637 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04468668186664581
2025-03-08 17:24:49,833 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_64_epoch.pt
2025-03-08 17:25:01,877 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04664508774876595
2025-03-08 17:25:12,326 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04415353463962674
2025-03-08 17:25:23,479 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04280471722284953
2025-03-08 17:25:35,636 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.042738126358017325
2025-03-08 17:25:46,405 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04257682041823864
2025-03-08 17:25:56,559 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_65_epoch.pt
2025-03-08 17:26:08,654 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.036160130761563776
2025-03-08 17:26:19,610 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.038841425385326145
2025-03-08 17:26:30,567 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04196378263334433
2025-03-08 17:26:42,218 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04446707199327648
2025-03-08 17:26:53,968 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04441075569391251
2025-03-08 17:27:03,168 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_66_epoch.pt
2025-03-08 17:27:14,121 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.034124501049518585
2025-03-08 17:27:24,780 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.03945558251813054
2025-03-08 17:27:36,280 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04206173174083233
2025-03-08 17:27:48,266 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04260854948312044
2025-03-08 17:27:59,905 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.043999995410442354
2025-03-08 17:28:09,597 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_67_epoch.pt
2025-03-08 17:28:21,102 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04270366065204143
2025-03-08 17:28:33,180 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04558886030688882
2025-03-08 17:28:44,496 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04287704740961393
2025-03-08 17:28:55,155 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04412133526988327
2025-03-08 17:29:05,931 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04515096314251423
2025-03-08 17:29:16,358 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_68_epoch.pt
2025-03-08 17:29:27,811 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.038072336837649344
2025-03-08 17:29:39,651 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04020127011463046
2025-03-08 17:29:51,081 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04331323721756538
2025-03-08 17:30:01,992 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.044002151498571035
2025-03-08 17:30:13,312 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.042694000713527205
2025-03-08 17:30:23,038 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_69_epoch.pt
2025-03-08 17:30:34,314 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04262135028839111
2025-03-08 17:30:44,310 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04198074361309409
2025-03-08 17:30:56,003 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04184647884219885
2025-03-08 17:31:07,799 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04078766501508653
2025-03-08 17:31:19,475 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04175885945558548
2025-03-08 17:31:29,271 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_70_epoch.pt
2025-03-08 17:31:40,612 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.039705119542777535
2025-03-08 17:31:52,693 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04168337102979422
2025-03-08 17:32:03,796 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.042893842148284116
2025-03-08 17:32:14,718 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.0423928605671972
2025-03-08 17:32:26,355 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.041531879499554634
2025-03-08 17:32:35,840 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_71_epoch.pt
2025-03-08 17:32:48,263 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.05049836505204439
2025-03-08 17:32:59,807 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04176228230819106
2025-03-08 17:33:10,469 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.045624931280811624
2025-03-08 17:33:22,162 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.047575638936832546
2025-03-08 17:33:33,133 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.047064143277704716
2025-03-08 17:33:42,965 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_72_epoch.pt
2025-03-08 17:33:54,803 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04826960101723671
2025-03-08 17:34:06,363 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04381374780088663
2025-03-08 17:34:17,063 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04245084347824256
2025-03-08 17:34:27,994 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.043219660902395846
2025-03-08 17:34:39,414 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.0416133818551898
2025-03-08 17:34:49,607 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_73_epoch.pt
2025-03-08 17:35:01,955 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04418297972530127
2025-03-08 17:35:12,343 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.043755723349750045
2025-03-08 17:35:24,978 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.043798633938034376
2025-03-08 17:35:36,431 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04522036735899746
2025-03-08 17:35:47,745 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.0443941522538662
2025-03-08 17:35:57,177 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_74_epoch.pt
2025-03-08 17:36:10,070 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.053007830940186976
2025-03-08 17:36:21,307 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04513291684910655
2025-03-08 17:36:32,095 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04252632309993108
2025-03-08 17:36:42,624 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04225221869535744
2025-03-08 17:36:54,879 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04200095029175282
2025-03-08 17:37:04,353 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_75_epoch.pt
2025-03-08 17:37:16,379 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.043298336863517764
2025-03-08 17:37:28,524 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04810673337429762
2025-03-08 17:37:41,112 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04537175141274929
2025-03-08 17:37:52,021 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04533086009323597
2025-03-08 17:38:03,226 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04618572252988815
2025-03-08 17:38:11,595 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_76_epoch.pt
2025-03-08 17:38:22,754 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.038744224794209005
2025-03-08 17:38:33,816 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04051783164963126
2025-03-08 17:38:44,658 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04033716226617495
2025-03-08 17:38:56,075 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.041514409519731996
2025-03-08 17:39:08,279 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.042992931105196475
2025-03-08 17:39:18,727 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_77_epoch.pt
2025-03-08 17:39:30,590 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.0365141125395894
2025-03-08 17:39:41,764 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.040339495595544575
2025-03-08 17:39:53,047 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04260064367204905
2025-03-08 17:40:05,079 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04336338868364692
2025-03-08 17:40:16,773 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.042307568393647674
2025-03-08 17:40:25,537 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_78_epoch.pt
2025-03-08 17:40:37,173 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04584341984242201
2025-03-08 17:40:48,427 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04333397228270769
2025-03-08 17:40:59,676 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04314195498824119
2025-03-08 17:41:11,531 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.041557073835283516
2025-03-08 17:41:22,671 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04178971160948276
2025-03-08 17:41:32,421 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_79_epoch.pt
2025-03-08 17:41:44,767 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03626844570040703
2025-03-08 17:41:56,242 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.0414312095195055
2025-03-08 17:42:07,541 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.03941611432780822
2025-03-08 17:42:19,448 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04245848241262138
2025-03-08 17:42:31,017 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.042676040023565294
2025-03-08 17:42:39,466 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_80_epoch.pt
2025-03-08 17:42:51,685 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03638661101460457
2025-03-08 17:43:02,253 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.03670698266476393
2025-03-08 17:43:13,456 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.0401193696508805
2025-03-08 17:43:25,794 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.041225443873554465
2025-03-08 17:43:37,140 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04020353330671787
2025-03-08 17:43:46,617 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_81_epoch.pt
2025-03-08 17:43:58,889 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03951262131333351
2025-03-08 17:44:10,030 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.03847870940342545
2025-03-08 17:44:21,923 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04168253949532906
2025-03-08 17:44:33,733 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.041713789673522114
2025-03-08 17:44:44,136 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04070622768253088
2025-03-08 17:44:53,559 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_82_epoch.pt
2025-03-08 17:45:06,368 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04494006332010031
2025-03-08 17:45:18,216 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.043908866122365
2025-03-08 17:45:29,385 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04689865620185932
2025-03-08 17:45:40,499 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04657886524684727
2025-03-08 17:45:52,014 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04648774796724319
2025-03-08 17:46:00,847 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_83_epoch.pt
2025-03-08 17:46:13,057 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04441058851778507
2025-03-08 17:46:25,255 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04213899254798889
2025-03-08 17:46:37,182 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04231364767998457
2025-03-08 17:46:48,695 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04537372448481619
2025-03-08 17:46:58,903 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.045350489661097525
2025-03-08 17:47:08,018 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_84_epoch.pt
2025-03-08 17:47:20,527 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03907779388129711
2025-03-08 17:47:31,888 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.0414650185033679
2025-03-08 17:47:43,575 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04427821687112252
2025-03-08 17:47:55,248 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04560420387424528
2025-03-08 17:48:05,743 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04552842514961958
2025-03-08 17:48:14,741 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_85_epoch.pt
2025-03-08 17:48:26,603 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04458759028464556
2025-03-08 17:48:38,126 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04212640065699816
2025-03-08 17:48:49,602 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.0412220382442077
2025-03-08 17:49:00,641 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04043619001284242
2025-03-08 17:49:12,236 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04173953919112682
2025-03-08 17:49:21,852 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_86_epoch.pt
2025-03-08 17:49:34,331 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04877149187028408
2025-03-08 17:49:45,471 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.05063031492754817
2025-03-08 17:49:57,116 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04813685327768326
2025-03-08 17:50:08,343 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.047198277981951836
2025-03-08 17:50:20,485 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04672610452771187
2025-03-08 17:50:29,606 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_87_epoch.pt
2025-03-08 17:50:41,590 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03917761221528053
2025-03-08 17:50:53,580 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04422125587239861
2025-03-08 17:51:04,383 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.041998973215619725
2025-03-08 17:51:15,727 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.03984322816133499
2025-03-08 17:51:27,129 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.0419407462105155
2025-03-08 17:51:36,677 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_88_epoch.pt
2025-03-08 17:51:48,333 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03891664396971464
2025-03-08 17:51:59,850 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.041445947885513305
2025-03-08 17:52:11,354 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04455745464811722
2025-03-08 17:52:21,856 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.0442376614920795
2025-03-08 17:52:33,968 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.043554518416523934
2025-03-08 17:52:43,745 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_89_epoch.pt
2025-03-08 17:52:56,626 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04648789588361978
2025-03-08 17:53:08,143 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04964633997529745
2025-03-08 17:53:19,402 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04892351701855659
2025-03-08 17:53:30,488 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.047265560114756223
2025-03-08 17:53:41,722 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04709081067889929
2025-03-08 17:53:50,831 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_90_epoch.pt
2025-03-08 17:54:03,718 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.047284059152007105
2025-03-08 17:54:13,911 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04162426646798849
2025-03-08 17:54:24,615 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04213091934720675
2025-03-08 17:54:35,399 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.041894912412390115
2025-03-08 17:54:47,414 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.043162667989730834
2025-03-08 17:54:57,170 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_91_epoch.pt
2025-03-08 17:55:08,514 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03759073365479708
2025-03-08 17:55:20,044 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04217818886041641
2025-03-08 17:55:31,422 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.03914056313534578
2025-03-08 17:55:43,253 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.03800166254863143
2025-03-08 17:55:55,218 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.03648495886474848
2025-03-08 17:56:04,572 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_92_epoch.pt
2025-03-08 17:56:16,092 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.041637973450124266
2025-03-08 17:56:27,759 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04216332184150815
2025-03-08 17:56:39,413 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04428076785057783
2025-03-08 17:56:50,265 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.0427041406929493
2025-03-08 17:57:01,816 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04294187814742327
2025-03-08 17:57:11,453 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_93_epoch.pt
2025-03-08 17:57:22,616 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.051174861639738084
2025-03-08 17:57:34,085 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04853503715246916
2025-03-08 17:57:45,595 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04665277307232221
2025-03-08 17:57:56,543 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.045180082553997636
2025-03-08 17:58:08,747 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04251634863764048
2025-03-08 17:58:18,161 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_94_epoch.pt
2025-03-08 17:58:30,604 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04887115966528654
2025-03-08 17:58:41,372 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04707452164962888
2025-03-08 17:58:52,654 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04320627647141616
2025-03-08 17:59:03,905 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04300228789448738
2025-03-08 17:59:15,231 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04235265833884477
2025-03-08 17:59:24,722 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_95_epoch.pt
2025-03-08 17:59:35,968 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.044649971500039104
2025-03-08 17:59:47,654 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04164412347599864
2025-03-08 17:59:59,461 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.044405537818868954
2025-03-08 18:00:10,124 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04212263529188931
2025-03-08 18:00:22,301 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.041903865538537505
2025-03-08 18:00:32,311 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_96_epoch.pt
2025-03-08 18:00:44,293 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03884177945554256
2025-03-08 18:00:54,817 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04124153260141611
2025-03-08 18:01:06,477 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.042295458701749646
2025-03-08 18:01:18,402 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.045297328233718875
2025-03-08 18:01:30,303 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.0461326244622469
2025-03-08 18:01:39,650 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_97_epoch.pt
2025-03-08 18:01:51,233 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04465859211981296
2025-03-08 18:02:01,808 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04242899687960744
2025-03-08 18:02:12,925 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04203199217716853
2025-03-08 18:02:24,344 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.042456004470586776
2025-03-08 18:02:36,682 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04168086641281843
2025-03-08 18:02:47,179 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_98_epoch.pt
2025-03-08 18:02:58,837 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03656743984669447
2025-03-08 18:03:09,534 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04217301478609443
2025-03-08 18:03:20,701 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04093476535131534
2025-03-08 18:03:32,779 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04348871409893036
2025-03-08 18:03:44,491 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.043763569824397566
2025-03-08 18:03:54,212 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_99_epoch.pt
2025-03-08 18:04:06,625 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03772333659231663
2025-03-08 18:04:17,880 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04036839636042714
2025-03-08 18:04:28,800 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04138887902100881
2025-03-08 18:04:40,707 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04210550831630826
2025-03-08 18:04:51,604 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04183265972137451
2025-03-08 18:05:01,422 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_100_epoch.pt
2025-03-08 18:05:13,217 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04685934811830521
2025-03-08 18:05:24,827 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04154243534430861
2025-03-08 18:05:36,188 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.039849018901586535
2025-03-08 18:05:47,652 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04124022951349616
2025-03-08 18:05:58,482 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04369425911456346
2025-03-08 18:06:08,719 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_101_epoch.pt
2025-03-08 18:06:20,447 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04723418604582548
2025-03-08 18:06:32,698 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.0468302109092474
2025-03-08 18:06:43,347 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.045193732380867005
2025-03-08 18:06:53,488 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.044211108032613994
2025-03-08 18:07:05,440 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04307062424719334
2025-03-08 18:07:15,579 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_102_epoch.pt
2025-03-08 18:07:26,999 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.0462549601867795
2025-03-08 18:07:37,965 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.044178985450416805
2025-03-08 18:07:49,363 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04213530115783214
2025-03-08 18:08:00,894 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04158364006318152
2025-03-08 18:08:11,581 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04099034835398197
2025-03-08 18:08:22,618 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_103_epoch.pt
2025-03-08 18:08:33,322 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03522957626730203
2025-03-08 18:08:44,783 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.03885182362049818
2025-03-08 18:08:57,265 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.03946497858812412
2025-03-08 18:09:08,521 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.03980877397581935
2025-03-08 18:09:19,330 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.041236466728150845
2025-03-08 18:09:28,976 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_104_epoch.pt
2025-03-08 18:09:40,046 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03447221051901579
2025-03-08 18:09:51,390 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.03713779676705599
2025-03-08 18:10:03,057 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.03999238960444927
2025-03-08 18:10:14,933 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.041855024779215454
2025-03-08 18:10:26,137 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.042082557320594786
2025-03-08 18:10:36,108 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_105_epoch.pt
2025-03-08 18:10:47,102 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.05039944000542164
2025-03-08 18:10:58,507 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.043137053046375516
2025-03-08 18:11:09,295 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04527270939201117
2025-03-08 18:11:21,700 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04372452849522233
2025-03-08 18:11:33,438 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.043150360442698
2025-03-08 18:11:43,226 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_106_epoch.pt
2025-03-08 18:11:55,338 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.0397427437081933
2025-03-08 18:12:06,221 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.043864376991987225
2025-03-08 18:12:17,768 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04491936011860768
2025-03-08 18:12:28,501 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04368865923024714
2025-03-08 18:12:39,667 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04270886154472828
2025-03-08 18:12:49,470 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_107_epoch.pt
2025-03-08 18:13:01,049 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03700182411819696
2025-03-08 18:13:13,143 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.03456303102895617
2025-03-08 18:13:23,978 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.034977855558196706
2025-03-08 18:13:34,974 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.03642963217571378
2025-03-08 18:13:46,647 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.037358200363814834
2025-03-08 18:13:56,106 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_108_epoch.pt
2025-03-08 18:14:08,944 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04160532902926207
2025-03-08 18:14:20,968 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04062477517873049
2025-03-08 18:14:31,638 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.041185620613396166
2025-03-08 18:14:42,425 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.0412974408082664
2025-03-08 18:14:53,166 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.041302306227385996
2025-03-08 18:15:02,882 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_109_epoch.pt
2025-03-08 18:15:15,253 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04284572444856167
2025-03-08 18:15:27,057 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04197013696655631
2025-03-08 18:15:38,723 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.040094656062622865
2025-03-08 18:15:49,485 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04191790771670639
2025-03-08 18:16:00,117 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04125582972168922
2025-03-08 18:16:10,097 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_110_epoch.pt
2025-03-08 18:16:21,477 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.039113610498607156
2025-03-08 18:16:32,785 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.03757416656240821
2025-03-08 18:16:45,172 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.037333903970817726
2025-03-08 18:16:56,623 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.03937542292289436
2025-03-08 18:17:07,517 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.039426238685846327
2025-03-08 18:17:17,319 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_111_epoch.pt
2025-03-08 18:17:29,213 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.043086590245366096
2025-03-08 18:17:41,595 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.0401136863976717
2025-03-08 18:17:51,734 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.03985228680074215
2025-03-08 18:18:03,143 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04036910021677613
2025-03-08 18:18:13,701 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.041390306651592254
2025-03-08 18:18:23,454 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_112_epoch.pt
2025-03-08 18:18:35,006 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.0350639246404171
2025-03-08 18:18:47,097 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.03657620007172227
2025-03-08 18:18:58,733 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.03789799836774667
2025-03-08 18:19:10,192 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.038196941753849384
2025-03-08 18:19:21,068 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.03978137773275375
2025-03-08 18:19:30,687 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_113_epoch.pt
2025-03-08 18:19:42,524 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03404334183782339
2025-03-08 18:19:53,883 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.03795948015525937
2025-03-08 18:20:04,774 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.03763861111054818
2025-03-08 18:20:16,107 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.03683493886142969
2025-03-08 18:20:27,493 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.03799751541763544
2025-03-08 18:20:37,527 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_114_epoch.pt
2025-03-08 18:20:49,394 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04618795409798622
2025-03-08 18:21:01,298 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04619638146832585
2025-03-08 18:21:12,771 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04432511695971091
2025-03-08 18:21:24,212 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04260522180236876
2025-03-08 18:21:35,386 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.042590792573988435
2025-03-08 18:21:44,629 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_115_epoch.pt
2025-03-08 18:21:56,906 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03834437873214483
2025-03-08 18:22:08,393 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04283263763412833
2025-03-08 18:22:19,172 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04256313104182482
2025-03-08 18:22:30,798 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04281754676252603
2025-03-08 18:22:42,411 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04325096867978573
2025-03-08 18:22:52,163 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_116_epoch.pt
2025-03-08 18:23:05,100 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.044895129129290584
2025-03-08 18:23:16,500 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.044055687058717014
2025-03-08 18:23:28,270 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.042810927281777066
2025-03-08 18:23:38,921 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.045150372013449666
2025-03-08 18:23:50,990 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04489402498304844
2025-03-08 18:23:59,508 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_117_epoch.pt
2025-03-08 18:24:11,266 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04372077133506536
2025-03-08 18:24:22,547 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.045606542248278856
2025-03-08 18:24:33,954 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04555641507109006
2025-03-08 18:24:45,977 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.045036680130288005
2025-03-08 18:24:56,138 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.0444389309361577
2025-03-08 18:25:05,765 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_118_epoch.pt
2025-03-08 18:25:17,726 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03893378764390945
2025-03-08 18:25:29,070 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04128299575299024
2025-03-08 18:25:40,382 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04050305674473444
2025-03-08 18:25:52,057 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.0406638340651989
2025-03-08 18:26:03,632 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04143434990942478
2025-03-08 18:26:12,839 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_119_epoch.pt
2025-03-08 18:26:24,906 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04519163105636835
2025-03-08 18:26:36,233 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04438705593347549
2025-03-08 18:26:47,218 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04275653250515461
2025-03-08 18:26:58,838 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.043141237385571005
2025-03-08 18:27:10,594 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04242237514257431
2025-03-08 18:27:20,036 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_120_epoch.pt
2025-03-08 18:27:32,622 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04258142791688442
2025-03-08 18:27:43,348 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04008308304473758
2025-03-08 18:27:53,223 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.03878842542568842
2025-03-08 18:28:04,044 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.0373211198206991
2025-03-08 18:28:15,998 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.03981154201924801
2025-03-08 18:28:26,849 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_121_epoch.pt
2025-03-08 18:28:37,885 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04715815581381321
2025-03-08 18:28:48,615 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04411554526537657
2025-03-08 18:29:00,105 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04436197352906068
2025-03-08 18:29:11,369 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.042290993304923176
2025-03-08 18:29:23,567 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.042008140906691555
2025-03-08 18:29:33,348 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_122_epoch.pt
2025-03-08 18:29:44,833 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.05124160841107368
2025-03-08 18:29:55,481 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.0460118031129241
2025-03-08 18:30:06,680 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.047603266599277654
2025-03-08 18:30:19,256 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.044279642878100274
2025-03-08 18:30:30,938 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04523151510208845
2025-03-08 18:30:40,253 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_123_epoch.pt
2025-03-08 18:30:53,081 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04768411222845316
2025-03-08 18:31:03,474 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04421413380652666
2025-03-08 18:31:14,940 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.044802794009447096
2025-03-08 18:31:26,886 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04442810332402587
2025-03-08 18:31:38,457 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04354022079706192
2025-03-08 18:31:47,566 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_124_epoch.pt
2025-03-08 18:32:00,004 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04005896236747503
2025-03-08 18:32:11,009 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.03746585175395012
2025-03-08 18:32:22,018 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.03804804893831412
2025-03-08 18:32:33,585 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.03913450892083347
2025-03-08 18:32:44,383 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.041324648164212704
2025-03-08 18:32:54,057 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_125_epoch.pt
2025-03-08 18:33:05,968 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04018365819007158
2025-03-08 18:33:16,686 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.03950016245245933
2025-03-08 18:33:28,761 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.0384942368293802
2025-03-08 18:33:40,093 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.03866691691800952
2025-03-08 18:33:50,727 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.038488048136234285
2025-03-08 18:34:00,601 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_126_epoch.pt
2025-03-08 18:34:11,726 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.0379039254412055
2025-03-08 18:34:23,517 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04158623661845923
2025-03-08 18:34:35,344 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.0408536379908522
2025-03-08 18:34:45,832 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.03968589821830392
2025-03-08 18:34:57,772 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.03940884920954704
2025-03-08 18:35:07,155 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_127_epoch.pt
2025-03-08 18:35:19,279 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04114965181797743
2025-03-08 18:35:31,094 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04285512588918209
2025-03-08 18:35:42,265 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04215921502560377
2025-03-08 18:35:54,031 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04236438780091703
2025-03-08 18:36:04,599 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.041677711963653566
2025-03-08 18:36:14,084 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_128_epoch.pt
2025-03-08 18:36:27,185 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04153063215315342
2025-03-08 18:36:38,213 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.039313311260193586
2025-03-08 18:36:49,635 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04089291983594497
2025-03-08 18:37:00,291 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.040985041027888656
2025-03-08 18:37:11,741 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04172324296832085
2025-03-08 18:37:20,454 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_129_epoch.pt
2025-03-08 18:37:32,174 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04585669778287411
2025-03-08 18:37:43,645 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.044351045303046704
2025-03-08 18:37:54,762 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04379453501353661
2025-03-08 18:38:05,697 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04332392294891178
2025-03-08 18:38:16,654 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04116080790758133
2025-03-08 18:38:27,295 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_130_epoch.pt
2025-03-08 18:38:39,140 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.05070311561226845
2025-03-08 18:38:49,794 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04821929477155209
2025-03-08 18:39:00,978 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.046583341596027214
2025-03-08 18:39:12,762 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.048197828168049454
2025-03-08 18:39:24,438 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.0456301426589489
2025-03-08 18:39:34,572 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_131_epoch.pt
2025-03-08 18:39:46,559 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03350331641733646
2025-03-08 18:39:57,741 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.0342439828068018
2025-03-08 18:40:08,853 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.03648290110131105
2025-03-08 18:40:21,084 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.03778387987986207
2025-03-08 18:40:31,692 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.038013969480991366
2025-03-08 18:40:40,912 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_132_epoch.pt
2025-03-08 18:40:52,969 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04287641424685717
2025-03-08 18:41:04,059 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.039562966879457234
2025-03-08 18:41:15,622 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.03894006188958883
2025-03-08 18:41:26,447 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04071133215911686
2025-03-08 18:41:38,245 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.043431709587574
2025-03-08 18:41:48,010 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_133_epoch.pt
2025-03-08 18:42:00,801 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04085212826728821
2025-03-08 18:42:12,053 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.045331523921340704
2025-03-08 18:42:22,813 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.0460313863803943
2025-03-08 18:42:34,021 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04469774420373142
2025-03-08 18:42:44,784 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04420659849047661
2025-03-08 18:42:54,676 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_134_epoch.pt
2025-03-08 18:43:06,516 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04529147110879421
2025-03-08 18:43:17,849 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04366123996675014
2025-03-08 18:43:29,989 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.045120194293558596
2025-03-08 18:43:41,748 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04573717917315662
2025-03-08 18:43:53,110 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.044171414569020274
2025-03-08 18:44:01,498 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_135_epoch.pt
2025-03-08 18:44:12,662 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.034009628444910046
2025-03-08 18:44:24,492 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04179385747760534
2025-03-08 18:44:35,343 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.042136885958413285
2025-03-08 18:44:47,321 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.042757495045661925
2025-03-08 18:44:59,434 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04212319502979517
2025-03-08 18:45:08,222 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_136_epoch.pt
2025-03-08 18:45:20,321 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04515340629965067
2025-03-08 18:45:31,569 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04320722635835409
2025-03-08 18:45:42,461 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.043127805379529795
2025-03-08 18:45:54,197 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04409586208872497
2025-03-08 18:46:06,188 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04373765816539526
2025-03-08 18:46:15,697 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_137_epoch.pt
2025-03-08 18:46:28,178 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04766515336930752
2025-03-08 18:46:40,082 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04092306777834892
2025-03-08 18:46:51,349 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04212526881446441
2025-03-08 18:47:02,098 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04214814407750964
2025-03-08 18:47:12,161 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04034785646945238
2025-03-08 18:47:22,259 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_138_epoch.pt
2025-03-08 18:47:33,591 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.039588831700384616
2025-03-08 18:47:44,684 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04072722593322396
2025-03-08 18:47:56,233 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.03864006372789542
2025-03-08 18:48:08,239 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04030885358341038
2025-03-08 18:48:20,142 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04081519062817097
2025-03-08 18:48:29,007 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_139_epoch.pt
2025-03-08 18:48:39,741 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04376921284943819
2025-03-08 18:48:51,187 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.045955396499484776
2025-03-08 18:49:03,238 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04525743305683136
2025-03-08 18:49:14,075 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04314893055707216
2025-03-08 18:49:25,057 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.045063143976032734
2025-03-08 18:49:35,583 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_140_epoch.pt
2025-03-08 18:49:47,537 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04858836200088262
2025-03-08 18:49:59,669 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04826751997694373
2025-03-08 18:50:11,243 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.045850104677180446
2025-03-08 18:50:22,028 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04254507620818913
2025-03-08 18:50:33,635 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04279507258534431
2025-03-08 18:50:42,286 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_141_epoch.pt
2025-03-08 18:50:54,463 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03579969793558121
2025-03-08 18:51:05,631 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04017043003812432
2025-03-08 18:51:16,386 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.040120844654738905
2025-03-08 18:51:27,249 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04061395573429763
2025-03-08 18:51:39,664 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.0422108816280961
2025-03-08 18:51:49,325 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_142_epoch.pt
2025-03-08 18:52:01,433 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04372558455914259
2025-03-08 18:52:12,732 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.03883142838254571
2025-03-08 18:52:24,070 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04043889980763197
2025-03-08 18:52:35,034 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04135877884924412
2025-03-08 18:52:46,321 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.0404622508212924
2025-03-08 18:52:56,411 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_143_epoch.pt
2025-03-08 18:53:09,214 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.042004909552633765
2025-03-08 18:53:20,279 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.037975625842809675
2025-03-08 18:53:32,073 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.038373641533156234
2025-03-08 18:53:42,786 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04128871169872582
2025-03-08 18:53:53,816 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04176466391980648
2025-03-08 18:54:03,299 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_144_epoch.pt
2025-03-08 18:54:15,780 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03981888715177775
2025-03-08 18:54:27,027 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.03832093706354499
2025-03-08 18:54:38,191 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.037921543021996816
2025-03-08 18:54:49,090 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.03780876623466611
2025-03-08 18:55:00,608 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.03847888339310884
2025-03-08 18:55:09,804 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_145_epoch.pt
2025-03-08 18:55:21,310 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.045713878460228445
2025-03-08 18:55:31,944 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04809655480086803
2025-03-08 18:55:43,586 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04741796121001243
2025-03-08 18:55:56,163 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04676066074520349
2025-03-08 18:56:07,106 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.044858303159475324
2025-03-08 18:56:17,094 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_146_epoch.pt
2025-03-08 18:56:29,594 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03852422639727592
2025-03-08 18:56:41,560 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04240290684625506
2025-03-08 18:56:52,892 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.042813199001053966
2025-03-08 18:57:03,377 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.042630120553076266
2025-03-08 18:57:14,049 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04352090789377689
2025-03-08 18:57:23,880 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_147_epoch.pt
2025-03-08 18:57:36,074 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04410060964524746
2025-03-08 18:57:47,023 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.0395108800008893
2025-03-08 18:57:59,249 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.040566793208320934
2025-03-08 18:58:10,454 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.039996977699920534
2025-03-08 18:58:22,397 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04069503303617239
2025-03-08 18:58:30,744 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_148_epoch.pt
2025-03-08 18:58:42,288 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.036765605211257935
2025-03-08 18:58:53,263 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04004897341132164
2025-03-08 18:59:05,035 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04079216808080673
2025-03-08 18:59:16,706 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04133297050371766
2025-03-08 18:59:27,328 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.040506858624517916
2025-03-08 18:59:37,457 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_149_epoch.pt
2025-03-08 18:59:49,184 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04855203103274107
2025-03-08 18:59:59,501 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.047240203395485875
2025-03-08 19:00:10,662 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.0457229266067346
2025-03-08 19:00:23,061 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.042954278672114016
2025-03-08 19:00:34,873 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04246695464104414
2025-03-08 19:00:44,429 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_150_epoch.pt
2025-03-08 19:00:55,310 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.039973924569785595
2025-03-08 19:01:07,377 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.043088418021798136
2025-03-08 19:01:18,594 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04054798752069473
2025-03-08 19:01:30,570 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04283723228611052
2025-03-08 19:01:41,245 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.0426017514243722
2025-03-08 19:01:51,271 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_151_epoch.pt
2025-03-08 19:02:03,141 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.045438476651906964
2025-03-08 19:02:14,109 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04188740834593773
2025-03-08 19:02:25,635 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04495971946666638
2025-03-08 19:02:37,242 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04344503912143409
2025-03-08 19:02:48,781 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04531442505121231
2025-03-08 19:02:58,604 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_152_epoch.pt
2025-03-08 19:03:10,157 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04366995416581631
2025-03-08 19:03:20,546 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04677918791770935
2025-03-08 19:03:32,799 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04210093716780345
2025-03-08 19:03:44,827 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04232063238508999
2025-03-08 19:03:55,585 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.044305843383073806
2025-03-08 19:04:05,535 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_153_epoch.pt
2025-03-08 19:04:16,646 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.040769380293786524
2025-03-08 19:04:28,347 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.041406008824706075
2025-03-08 19:04:39,367 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04349648926407099
2025-03-08 19:04:51,386 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.043057845495641234
2025-03-08 19:05:03,340 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04219258996099234
2025-03-08 19:05:12,779 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_154_epoch.pt
2025-03-08 19:05:23,826 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03568913035094738
2025-03-08 19:05:35,841 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04340426865965128
2025-03-08 19:05:47,141 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04680502487967412
2025-03-08 19:05:58,988 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04718301403336227
2025-03-08 19:06:09,989 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04570914516597986
2025-03-08 19:06:19,266 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_155_epoch.pt
2025-03-08 19:06:31,453 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04216666352003813
2025-03-08 19:06:42,427 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04462193606421352
2025-03-08 19:06:53,252 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.0436020132775108
2025-03-08 19:07:04,933 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04375721412710845
2025-03-08 19:07:17,098 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04141341409087181
2025-03-08 19:07:25,947 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_156_epoch.pt
2025-03-08 19:07:37,427 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.040723313614726066
2025-03-08 19:07:48,825 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.03879357615485787
2025-03-08 19:08:00,540 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.03929235752671957
2025-03-08 19:08:11,773 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.038464471073821185
2025-03-08 19:08:22,839 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04060204053670168
2025-03-08 19:08:32,747 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_157_epoch.pt
2025-03-08 19:08:44,186 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04154048822820187
2025-03-08 19:08:54,542 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.0451765089482069
2025-03-08 19:09:06,395 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04349076313277086
2025-03-08 19:09:17,887 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04277249116450548
2025-03-08 19:09:30,469 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.043090528555214405
2025-03-08 19:09:39,735 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_158_epoch.pt
2025-03-08 19:09:52,016 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04844934493303299
2025-03-08 19:10:04,017 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04418947234749794
2025-03-08 19:10:15,327 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.0431055515135328
2025-03-08 19:10:26,636 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04126797705888748
2025-03-08 19:10:37,874 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.03983715874701738
2025-03-08 19:10:46,916 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_159_epoch.pt
2025-03-08 19:10:58,471 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04554566539824009
2025-03-08 19:11:09,735 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.043950681183487175
2025-03-08 19:11:20,554 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04408002857118845
2025-03-08 19:11:32,168 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.044821137730032205
2025-03-08 19:11:44,085 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04396876611560583
2025-03-08 19:11:53,738 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_160_epoch.pt
2025-03-08 19:12:05,477 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04526048369705677
2025-03-08 19:12:17,455 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04451635308563709
2025-03-08 19:12:28,340 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04354934877405564
2025-03-08 19:12:39,149 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04301890417002142
2025-03-08 19:12:50,678 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.043112300664186476
2025-03-08 19:12:59,856 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_161_epoch.pt
2025-03-08 19:13:11,912 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.034579278416931626
2025-03-08 19:13:23,432 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.03627863347530365
2025-03-08 19:13:34,682 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.03752851424117883
2025-03-08 19:13:46,022 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.0398049654904753
2025-03-08 19:13:57,470 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.03918121057748795
2025-03-08 19:14:06,646 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_162_epoch.pt
2025-03-08 19:14:19,098 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.041617329455912115
2025-03-08 19:14:30,376 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.042345703318715094
2025-03-08 19:14:41,060 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04136580586433411
2025-03-08 19:14:52,343 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04213425456546247
2025-03-08 19:15:03,765 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.042160708844661714
2025-03-08 19:15:12,957 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_163_epoch.pt
2025-03-08 19:15:24,634 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04526763781905174
2025-03-08 19:15:36,013 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04189049150794744
2025-03-08 19:15:47,917 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.0398382451881965
2025-03-08 19:15:58,698 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.040738385766744614
2025-03-08 19:16:10,438 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.040556924410164355
2025-03-08 19:16:19,462 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_164_epoch.pt
2025-03-08 19:16:30,771 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03903606865555048
2025-03-08 19:16:40,885 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.03920130243524909
2025-03-08 19:16:51,982 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.03973073484996954
2025-03-08 19:17:04,227 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04136638431809843
2025-03-08 19:17:15,975 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04190489475429058
2025-03-08 19:17:26,218 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_165_epoch.pt
2025-03-08 19:17:37,871 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04589188169687986
2025-03-08 19:17:49,066 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04403610061854124
2025-03-08 19:18:00,915 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04032882450769345
2025-03-08 19:18:12,331 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.040812391014769675
2025-03-08 19:18:23,605 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.03948963735252619
2025-03-08 19:18:33,257 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_166_epoch.pt
2025-03-08 19:18:46,110 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03540086433291435
2025-03-08 19:18:58,316 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.03652155647054314
2025-03-08 19:19:08,428 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04066623693952958
2025-03-08 19:19:19,805 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.03998520744033158
2025-03-08 19:19:30,809 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.0394740806594491
2025-03-08 19:19:40,005 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_167_epoch.pt
2025-03-08 19:19:52,221 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04043426483869553
2025-03-08 19:20:04,081 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04482076203450561
2025-03-08 19:20:15,222 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.040515044815838334
2025-03-08 19:20:26,264 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04149478898383677
2025-03-08 19:20:37,543 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04094095879048109
2025-03-08 19:20:47,406 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_168_epoch.pt
2025-03-08 19:20:58,869 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04703616537153721
2025-03-08 19:21:10,839 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04672768948599696
2025-03-08 19:21:22,808 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.0437999290600419
2025-03-08 19:21:34,082 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.042623112164437774
2025-03-08 19:21:45,708 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04334092666208744
2025-03-08 19:21:54,391 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_169_epoch.pt
2025-03-08 19:22:06,099 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.047131741605699065
2025-03-08 19:22:16,913 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04542957307770848
2025-03-08 19:22:28,767 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04360725406557322
2025-03-08 19:22:39,932 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.042863037586212155
2025-03-08 19:22:50,374 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04268480109423399
2025-03-08 19:23:01,011 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_170_epoch.pt
2025-03-08 19:23:12,344 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.041740453280508516
2025-03-08 19:23:23,431 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04197430616244674
2025-03-08 19:23:35,235 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.044167406571408115
2025-03-08 19:23:47,253 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.043110475512221455
2025-03-08 19:23:57,896 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.042345948055386544
2025-03-08 19:24:07,528 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_171_epoch.pt
2025-03-08 19:24:18,533 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.05442801877856254
2025-03-08 19:24:29,814 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.045388007182627914
2025-03-08 19:24:40,792 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.043330149948596956
2025-03-08 19:24:53,286 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.0441487236507237
2025-03-08 19:25:04,784 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04251130436360836
2025-03-08 19:25:14,628 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_172_epoch.pt
2025-03-08 19:25:25,906 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04053756382316351
2025-03-08 19:25:38,304 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.042181229963898656
2025-03-08 19:25:50,647 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.040560120046138765
2025-03-08 19:26:02,156 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04157915711402893
2025-03-08 19:26:12,908 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04151367558538914
2025-03-08 19:26:21,943 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_173_epoch.pt
2025-03-08 19:26:33,704 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04866711553186178
2025-03-08 19:26:44,842 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04876779051497579
2025-03-08 19:26:56,381 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.045695293011764684
2025-03-08 19:27:08,235 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.0433573423884809
2025-03-08 19:27:18,893 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04453145196288824
2025-03-08 19:27:29,062 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_174_epoch.pt
2025-03-08 19:27:40,271 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04346292559057474
2025-03-08 19:27:52,300 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04047245243564248
2025-03-08 19:28:03,250 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04160987861454487
2025-03-08 19:28:14,554 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.040975406328216196
2025-03-08 19:28:25,876 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04301644799858332
2025-03-08 19:28:35,410 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_175_epoch.pt
2025-03-08 19:28:47,567 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04860805671662092
2025-03-08 19:28:59,248 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.0426795500703156
2025-03-08 19:29:10,289 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.041120452905694646
2025-03-08 19:29:21,637 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.041596992341801524
2025-03-08 19:29:33,211 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.042261460989713666
2025-03-08 19:29:43,150 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_176_epoch.pt
2025-03-08 19:29:55,282 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.042176859900355336
2025-03-08 19:30:06,560 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04384722709655762
2025-03-08 19:30:18,261 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04240965264538924
2025-03-08 19:30:29,466 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04223894950933754
2025-03-08 19:30:40,834 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04279159258306026
2025-03-08 19:30:50,099 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_177_epoch.pt
2025-03-08 19:31:01,879 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04589396271854639
2025-03-08 19:31:13,092 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.039391147755086425
2025-03-08 19:31:23,720 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.040225891880691055
2025-03-08 19:31:35,318 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.0410543167591095
2025-03-08 19:31:47,424 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.040986668065190315
2025-03-08 19:31:56,971 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_178_epoch.pt
2025-03-08 19:32:08,132 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04167454082518816
2025-03-08 19:32:19,709 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.03827713320031762
2025-03-08 19:32:31,220 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.03582929857075214
2025-03-08 19:32:42,889 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.03875185091979802
2025-03-08 19:32:53,600 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.038487617529928686
2025-03-08 19:33:03,500 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_179_epoch.pt
2025-03-08 19:33:15,865 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03935941495001316
2025-03-08 19:33:27,246 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.037229183185845616
2025-03-08 19:33:38,124 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.0394599595417579
2025-03-08 19:33:49,337 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.040797409480437634
2025-03-08 19:34:00,243 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04189870561659336
2025-03-08 19:34:10,056 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_180_epoch.pt
2025-03-08 19:34:22,642 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03194573402404785
2025-03-08 19:34:33,152 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.03497560564428568
2025-03-08 19:34:44,869 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.040587096475064754
2025-03-08 19:34:55,823 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.042617205400019884
2025-03-08 19:35:07,125 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.042668762907385825
2025-03-08 19:35:16,394 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_181_epoch.pt
2025-03-08 19:35:28,038 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03870464134961367
2025-03-08 19:35:38,467 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.041679002176970244
2025-03-08 19:35:50,253 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04137245857467254
2025-03-08 19:36:02,007 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.03998149774037302
2025-03-08 19:36:13,617 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.039984823405742644
2025-03-08 19:36:22,970 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_182_epoch.pt
2025-03-08 19:36:34,475 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04137910824269056
2025-03-08 19:36:45,710 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.045894547440111635
2025-03-08 19:36:57,273 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04452922439823548
2025-03-08 19:37:10,163 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.0440060271974653
2025-03-08 19:37:20,654 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04474759629368782
2025-03-08 19:37:29,662 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_183_epoch.pt
2025-03-08 19:37:41,804 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04593201939016581
2025-03-08 19:37:53,079 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.045319002121686935
2025-03-08 19:38:03,990 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04676310795048873
2025-03-08 19:38:15,730 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.0447275191731751
2025-03-08 19:38:26,731 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04363980760425329
2025-03-08 19:38:36,761 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_184_epoch.pt
2025-03-08 19:38:49,086 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03802309293299913
2025-03-08 19:39:00,952 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.03660183411091566
2025-03-08 19:39:12,041 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.034787397719919684
2025-03-08 19:39:23,868 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.03878747976385057
2025-03-08 19:39:34,067 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.03829451120644808
2025-03-08 19:39:43,515 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_185_epoch.pt
2025-03-08 19:39:55,134 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04363826304674148
2025-03-08 19:40:06,788 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.045175778716802596
2025-03-08 19:40:17,858 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.0481236249456803
2025-03-08 19:40:28,951 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.0473337325360626
2025-03-08 19:40:40,533 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.046083616323769096
2025-03-08 19:40:50,419 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_186_epoch.pt
2025-03-08 19:41:02,299 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03926516812294722
2025-03-08 19:41:13,717 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04013054426759481
2025-03-08 19:41:25,335 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.040425729776422184
2025-03-08 19:41:37,272 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04209296024404466
2025-03-08 19:41:48,481 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04458866057544947
2025-03-08 19:41:56,905 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_187_epoch.pt
2025-03-08 19:42:08,527 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.039046932160854336
2025-03-08 19:42:18,561 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04042694337666035
2025-03-08 19:42:29,976 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.042243170303603014
2025-03-08 19:42:42,052 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04210807653144002
2025-03-08 19:42:53,130 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.041993065714836124
2025-03-08 19:43:03,693 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_188_epoch.pt
2025-03-08 19:43:15,133 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.038463146425783634
2025-03-08 19:43:26,389 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.037176108583807946
2025-03-08 19:43:38,011 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.03682133390257756
2025-03-08 19:43:48,963 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.039684936981648204
2025-03-08 19:44:00,595 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.03948377703130245
2025-03-08 19:44:10,541 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_189_epoch.pt
2025-03-08 19:44:22,737 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03805845931172371
2025-03-08 19:44:33,527 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04216655122116208
2025-03-08 19:44:45,660 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.042991403229534625
2025-03-08 19:44:56,708 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.043467479674145576
2025-03-08 19:45:08,087 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.045429440274834636
2025-03-08 19:45:17,055 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_190_epoch.pt
2025-03-08 19:45:28,524 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04636220216751099
2025-03-08 19:45:39,849 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04211651764810085
2025-03-08 19:45:51,288 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.040894527733325955
2025-03-08 19:46:02,654 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04048305350355804
2025-03-08 19:46:13,477 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04049066396802664
2025-03-08 19:46:23,752 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_191_epoch.pt
2025-03-08 19:46:35,938 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03976257544010878
2025-03-08 19:46:47,401 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.043438345044851306
2025-03-08 19:46:58,353 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.041853954270482066
2025-03-08 19:47:09,009 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.041290498301386834
2025-03-08 19:47:21,057 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.040780112840235236
2025-03-08 19:47:30,678 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_192_epoch.pt
2025-03-08 19:47:41,944 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.05275186635553837
2025-03-08 19:47:52,861 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.049691282156854866
2025-03-08 19:48:04,977 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04774425003677606
2025-03-08 19:48:16,668 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.045614260947331786
2025-03-08 19:48:27,202 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04451736336946487
2025-03-08 19:48:36,972 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_193_epoch.pt
2025-03-08 19:48:48,570 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.046303605251014235
2025-03-08 19:48:59,083 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.0453162551112473
2025-03-08 19:49:10,490 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04284088085095088
2025-03-08 19:49:22,464 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04150061751715839
2025-03-08 19:49:33,312 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04010752049833536
2025-03-08 19:49:42,855 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_194_epoch.pt
2025-03-08 19:49:55,171 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.044035553745925424
2025-03-08 19:50:05,306 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04079219803214073
2025-03-08 19:50:16,732 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04046491081515948
2025-03-08 19:50:28,696 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.039787579430267214
2025-03-08 19:50:39,977 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04104345727711916
2025-03-08 19:50:49,466 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_195_epoch.pt
2025-03-08 19:51:01,111 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.037939650416374204
2025-03-08 19:51:11,872 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.03820138745009899
2025-03-08 19:51:22,858 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.040527167059481145
2025-03-08 19:51:33,510 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.0399290067050606
2025-03-08 19:51:45,438 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.041068543829023835
2025-03-08 19:51:55,598 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_196_epoch.pt
2025-03-08 19:52:06,656 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03420564357191324
2025-03-08 19:52:17,572 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.03711428198963404
2025-03-08 19:52:29,672 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.03792610081533591
2025-03-08 19:52:39,798 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04067418714985251
2025-03-08 19:52:51,214 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04195827557146549
2025-03-08 19:53:02,318 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_197_epoch.pt
2025-03-08 19:53:13,265 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.0389417539909482
2025-03-08 19:53:24,354 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.039505277164280414
2025-03-08 19:53:36,164 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.03998544113089641
2025-03-08 19:53:48,029 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.03943112642504275
2025-03-08 19:53:59,367 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.03938441657274962
2025-03-08 19:54:09,448 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_198_epoch.pt
2025-03-08 19:54:22,212 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.036647354066371915
2025-03-08 19:54:33,922 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.040300203114748
2025-03-08 19:54:43,957 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04054137631009022
2025-03-08 19:54:54,991 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04057006620801985
2025-03-08 19:55:06,435 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04008876991271973
2025-03-08 19:55:16,496 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_199_epoch.pt
2025-03-08 19:55:27,928 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04286414030939341
2025-03-08 19:55:39,851 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.03993805082514882
2025-03-08 19:55:50,340 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.03852234501391649
2025-03-08 19:56:02,099 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04132966729812324
2025-03-08 19:56:13,353 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04154134897142649
2025-03-08 19:56:23,475 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_5/_bert-base-uncased_200_epoch.pt
2025-03-08 20:54:43,566 : 1196508602.py : eval_pred : INFO : processing 0 lines 
2025-03-08 20:55:39,773 : 1196508602.py : eval_pred : INFO : processing 100 lines 
2025-03-08 20:56:39,345 : 1196508602.py : eval_pred : INFO : processing 200 lines 
2025-03-08 20:57:24,924 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-08 20:58:20,980 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-08 20:59:19,515 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-08 21:00:59,163 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-08 21:01:56,741 : 2266857300.py : eval_pred : INFO : processing 400 lines 
2025-03-08 21:02:00,957 : 1683485883.py : run_eval : INFO : Epoch 130: F1: 0.8741830065359479, ACC: 0.9004854368932039
2025-03-08 21:02:01,751 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-08 21:03:30,940 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-08 21:05:17,232 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-08 21:06:09,264 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-08 21:06:32,329 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-08 21:07:25,053 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-08 21:08:21,944 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-08 21:09:19,862 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-08 21:10:57,425 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-08 21:11:55,885 : 2266857300.py : eval_pred : INFO : processing 400 lines 
2025-03-08 21:12:00,005 : 4240814748.py : run_eval : INFO : Epoch 130: F1: 0.8741830065359479, ACC: 0.9004854368932039
2025-03-08 21:12:00,793 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-08 21:13:28,799 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-08 21:15:18,558 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-08 21:17:21,841 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-08 21:20:08,958 : 2266857300.py : eval_pred : INFO : processing 400 lines 
2025-03-08 21:20:57,967 : 4240814748.py : run_eval : INFO : Epoch 130: F1: 0.9016260162601625, ACC: 0.918854415274463
2025-03-08 21:20:59,707 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-08 21:21:53,477 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-08 21:22:49,088 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-08 21:24:25,854 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-08 21:25:22,560 : 2266857300.py : eval_pred : INFO : processing 400 lines 
2025-03-08 21:25:26,660 : 4240814748.py : run_eval : INFO : Epoch 154: F1: 0.8786764705882354, ACC: 0.9029126213592233
2025-03-08 21:25:27,494 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-08 21:26:54,770 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-08 21:28:42,523 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-08 21:30:45,157 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-08 21:33:30,520 : 2266857300.py : eval_pred : INFO : processing 400 lines 
2025-03-08 21:34:19,274 : 4240814748.py : run_eval : INFO : Epoch 154: F1: 0.8983739837398373, ACC: 0.9164677804295943
2025-03-08 21:34:21,133 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-08 21:35:14,125 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-08 21:36:09,430 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-08 21:37:46,794 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-08 21:38:43,596 : 2266857300.py : eval_pred : INFO : processing 400 lines 
2025-03-08 21:38:47,902 : 4240814748.py : run_eval : INFO : Epoch 155: F1: 0.877450980392157, ACC: 0.9029126213592233
2025-03-08 21:38:49,275 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-08 21:40:17,915 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-08 21:42:06,086 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-08 21:44:08,171 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-08 21:46:53,553 : 2266857300.py : eval_pred : INFO : processing 400 lines 
2025-03-08 21:47:42,243 : 4240814748.py : run_eval : INFO : Epoch 155: F1: 0.9073170731707317, ACC: 0.9236276849642004
2025-03-08 21:47:43,995 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-08 21:48:36,923 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-08 21:49:33,254 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-08 21:51:09,662 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-08 21:52:07,025 : 2266857300.py : eval_pred : INFO : processing 400 lines 
2025-03-08 21:52:11,209 : 4240814748.py : run_eval : INFO : Epoch 171: F1: 0.8770424836601309, ACC: 0.9029126213592233
2025-03-08 21:52:12,141 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-08 21:53:40,481 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-08 21:55:28,205 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-08 21:57:30,581 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-08 22:00:15,943 : 2266857300.py : eval_pred : INFO : processing 400 lines 
2025-03-08 22:01:04,699 : 4240814748.py : run_eval : INFO : Epoch 171: F1: 0.9113821138211382, ACC: 0.9260143198090692
2025-03-08 22:01:06,623 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-08 22:01:58,864 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-08 22:02:54,921 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-08 22:04:31,258 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-08 22:05:28,773 : 2266857300.py : eval_pred : INFO : processing 400 lines 
2025-03-08 22:05:32,939 : 4240814748.py : run_eval : INFO : Epoch 172: F1: 0.8721405228758172, ACC: 0.8980582524271845
2025-03-08 22:05:33,705 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-08 22:07:02,654 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-08 22:08:51,240 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-08 22:10:54,690 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-08 22:13:40,443 : 2266857300.py : eval_pred : INFO : processing 400 lines 
2025-03-08 22:14:29,201 : 4240814748.py : run_eval : INFO : Epoch 172: F1: 0.9113821138211382, ACC: 0.9260143198090692
2025-03-08 22:14:30,875 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-08 22:15:23,772 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-08 22:16:19,402 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-08 22:17:55,649 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-08 22:18:52,805 : 2266857300.py : eval_pred : INFO : processing 400 lines 
2025-03-08 22:18:56,875 : 4240814748.py : run_eval : INFO : Epoch 195: F1: 0.8709150326797389, ACC: 0.8980582524271845
2025-03-08 22:18:57,811 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-08 22:20:26,180 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-08 22:22:14,334 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-08 22:24:16,509 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-08 22:27:02,082 : 2266857300.py : eval_pred : INFO : processing 400 lines 
2025-03-08 22:27:51,056 : 4240814748.py : run_eval : INFO : Epoch 195: F1: 0.9146341463414634, ACC: 0.9284009546539379
2025-03-08 22:27:51,890 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-08 22:28:44,187 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-08 22:29:45,484 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-08 22:31:18,193 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-08 22:31:41,770 : 4240814748.py : run_eval : INFO : Epoch 51: F1: 0.9466666666666669, ACC: 0.9585635359116023
2025-03-08 22:31:42,531 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-08 22:33:12,328 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-08 22:34:53,493 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-08 22:36:36,880 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-08 22:39:24,461 : 2266857300.py : eval_pred : INFO : processing 400 lines 
2025-03-08 22:40:47,690 : 4240814748.py : run_eval : INFO : Epoch 51: F1: 0.8752886836027715, ACC: 0.891156462585034
2025-03-08 22:40:49,389 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-08 22:41:41,449 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-08 22:42:42,486 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-08 22:44:16,187 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-08 22:44:39,522 : 4240814748.py : run_eval : INFO : Epoch 66: F1: 0.9540740740740743, ACC: 0.9640883977900553
2025-03-08 22:44:40,299 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-08 22:46:11,433 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-08 22:47:53,281 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-08 22:49:38,200 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-08 22:52:26,466 : 2266857300.py : eval_pred : INFO : processing 400 lines 
2025-03-08 22:53:49,911 : 4240814748.py : run_eval : INFO : Epoch 66: F1: 0.8868360277136261, ACC: 0.9002267573696145
2025-03-08 22:53:51,706 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-08 22:54:43,703 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-08 22:55:44,323 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-08 22:57:17,514 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-08 22:57:40,927 : 4240814748.py : run_eval : INFO : Epoch 77: F1: 0.9651851851851853, ACC: 0.9723756906077348
2025-03-08 22:57:41,781 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-08 22:59:12,488 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-08 23:00:54,534 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-08 23:02:37,922 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-08 23:05:25,341 : 2266857300.py : eval_pred : INFO : processing 400 lines 
2025-03-08 23:06:48,307 : 4240814748.py : run_eval : INFO : Epoch 77: F1: 0.8926096997690532, ACC: 0.9047619047619048
2025-03-08 23:06:50,299 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-08 23:07:42,422 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-08 23:08:43,707 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-08 23:10:18,020 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-08 23:10:41,638 : 4240814748.py : run_eval : INFO : Epoch 83: F1: 0.9796296296296296, ACC: 0.9834254143646409
2025-03-08 23:10:42,385 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-08 23:12:12,841 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-08 23:13:54,830 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-08 23:15:38,836 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-08 23:17:08,328 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-08 23:17:54,370 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-08 23:18:05,421 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-08 23:19:29,590 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-08 23:20:24,911 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-08 23:21:23,641 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-08 23:23:04,140 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-08 23:24:02,309 : 2266857300.py : eval_pred : INFO : processing 400 lines 
2025-03-08 23:24:06,641 : 1816976590.py : run_eval : INFO : Epoch 130: F1: 0.8741830065359479, ACC: 0.9004854368932039
2025-03-08 23:24:07,403 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-08 23:25:40,837 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-08 23:27:35,852 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-08 23:29:46,295 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-08 23:32:42,831 : 2266857300.py : eval_pred : INFO : processing 400 lines 
2025-03-08 23:33:34,011 : 1816976590.py : run_eval : INFO : Epoch 130: F1: 0.9052845528455283, ACC: 0.9212410501193318
2025-03-08 23:33:35,670 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-08 23:34:31,226 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-08 23:35:28,579 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-08 23:37:09,059 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-08 23:38:05,370 : 2266857300.py : eval_pred : INFO : processing 400 lines 
2025-03-08 23:38:09,549 : 1816976590.py : run_eval : INFO : Epoch 154: F1: 0.8774509803921572, ACC: 0.9029126213592233
2025-03-08 23:38:10,289 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-08 23:39:40,139 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-08 23:41:34,484 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-08 23:43:45,183 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-08 23:46:40,688 : 2266857300.py : eval_pred : INFO : processing 400 lines 
2025-03-08 23:47:31,351 : 1816976590.py : run_eval : INFO : Epoch 154: F1: 0.9016260162601625, ACC: 0.918854415274463
2025-03-08 23:47:33,153 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-08 23:48:26,277 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-08 23:49:22,070 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-08 23:50:58,057 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-08 23:51:55,059 : 2266857300.py : eval_pred : INFO : processing 400 lines 
2025-03-08 23:51:59,221 : 1816976590.py : run_eval : INFO : Epoch 171: F1: 0.8786764705882354, ACC: 0.9029126213592233
2025-03-08 23:52:00,180 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-08 23:53:33,345 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-08 23:55:29,136 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-08 23:57:38,822 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-09 00:00:32,188 : 2266857300.py : eval_pred : INFO : processing 400 lines 
2025-03-09 00:01:23,101 : 1816976590.py : run_eval : INFO : Epoch 171: F1: 0.9170731707317074, ACC: 0.9307875894988067
2025-03-09 00:01:24,908 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-09 00:02:17,784 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-09 00:03:14,047 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-09 00:04:49,931 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-09 00:05:46,205 : 2266857300.py : eval_pred : INFO : processing 400 lines 
2025-03-09 00:05:50,245 : 1816976590.py : run_eval : INFO : Epoch 195: F1: 0.8709150326797389, ACC: 0.8980582524271845
2025-03-09 00:05:51,100 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-09 00:07:23,948 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-09 00:09:18,952 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-09 00:11:28,678 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-09 00:14:23,743 : 2266857300.py : eval_pred : INFO : processing 400 lines 
2025-03-09 00:15:13,806 : 1816976590.py : run_eval : INFO : Epoch 195: F1: 0.9138211382113821, ACC: 0.9284009546539379
2025-03-09 00:15:15,412 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-09 00:16:08,502 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-09 00:17:11,419 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-09 00:18:48,433 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-09 00:19:11,892 : 1816976590.py : run_eval : INFO : Epoch 51: F1: 0.944814814814815, ACC: 0.9558011049723757
2025-03-09 00:19:13,285 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-09 00:20:46,111 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-09 00:22:29,917 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-09 00:24:16,865 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-09 00:27:06,999 : 2266857300.py : eval_pred : INFO : processing 400 lines 
2025-03-09 00:28:31,550 : 1816976590.py : run_eval : INFO : Epoch 51: F1: 0.8702848344880678, ACC: 0.8866213151927438
2025-03-09 00:28:33,586 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-09 00:29:25,433 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-09 00:30:26,540 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-09 00:32:01,725 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-09 00:32:25,636 : 1816976590.py : run_eval : INFO : Epoch 66: F1: 0.9466666666666669, ACC: 0.9585635359116023
2025-03-09 00:32:26,356 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-09 00:34:00,425 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-09 00:35:46,256 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-09 00:37:33,861 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-09 00:40:26,656 : 2266857300.py : eval_pred : INFO : processing 400 lines 
2025-03-09 00:41:52,321 : 1816976590.py : run_eval : INFO : Epoch 66: F1: 0.8818321785989225, ACC: 0.8956916099773242
2025-03-09 00:41:54,077 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-09 00:42:46,740 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-09 00:43:48,360 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-09 00:45:24,499 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-09 00:45:47,806 : 1816976590.py : run_eval : INFO : Epoch 77: F1: 0.9648148148148149, ACC: 0.9723756906077348
2025-03-09 00:45:48,734 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-09 00:47:22,109 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-09 00:49:06,490 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-09 00:50:53,664 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-09 00:53:47,204 : 2266857300.py : eval_pred : INFO : processing 400 lines 
2025-03-09 00:55:13,504 : 1816976590.py : run_eval : INFO : Epoch 77: F1: 0.8849114703618169, ACC: 0.8979591836734694
2025-03-09 00:55:15,294 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-09 00:56:07,994 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-09 00:57:10,724 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-09 00:58:48,084 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-09 00:59:11,574 : 1816976590.py : run_eval : INFO : Epoch 83: F1: 0.9725925925925927, ACC: 0.9779005524861878
2025-03-09 00:59:12,324 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-09 01:00:47,062 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-09 01:02:31,515 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-09 01:04:18,615 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-09 01:07:12,055 : 2266857300.py : eval_pred : INFO : processing 400 lines 
2025-03-09 01:08:38,398 : 1816976590.py : run_eval : INFO : Epoch 83: F1: 0.8772132409545806, ACC: 0.8934240362811792
2025-03-09 01:08:40,113 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-09 01:09:32,800 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-09 01:10:35,086 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-09 01:12:11,944 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-09 01:12:35,426 : 1816976590.py : run_eval : INFO : Epoch 91: F1: 0.9648148148148149, ACC: 0.9723756906077348
2025-03-09 01:12:36,210 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-09 01:14:10,046 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-09 01:15:55,466 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-09 01:17:42,741 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-09 01:20:35,840 : 2266857300.py : eval_pred : INFO : processing 400 lines 
2025-03-09 01:22:03,230 : 1816976590.py : run_eval : INFO : Epoch 91: F1: 0.8722093918398771, ACC: 0.8888888888888888
2025-03-09 01:22:04,715 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-09 01:22:57,648 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-09 01:24:00,478 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-09 01:25:37,330 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-09 01:26:00,786 : 1816976590.py : run_eval : INFO : Epoch 139: F1: 0.9648148148148151, ACC: 0.9723756906077348
2025-03-09 01:26:01,532 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-09 01:27:36,236 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-09 01:29:21,175 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-09 01:31:06,591 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-09 01:33:59,321 : 2266857300.py : eval_pred : INFO : processing 400 lines 
2025-03-09 01:35:26,214 : 1816976590.py : run_eval : INFO : Epoch 139: F1: 0.8922247882986913, ACC: 0.9047619047619048
2025-03-09 01:35:27,432 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-09 01:36:20,697 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-09 01:37:23,616 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-09 01:39:01,517 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-09 01:39:25,459 : 1816976590.py : run_eval : INFO : Epoch 142: F1: 0.9648148148148151, ACC: 0.9723756906077348
2025-03-09 01:39:26,206 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-09 01:41:01,131 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-09 01:42:47,640 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-09 01:44:35,598 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-09 01:47:28,873 : 2266857300.py : eval_pred : INFO : processing 400 lines 
2025-03-09 01:48:55,066 : 1816976590.py : run_eval : INFO : Epoch 142: F1: 0.8968437259430333, ACC: 0.909297052154195
2025-03-09 01:48:56,367 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-09 01:49:50,040 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-09 01:50:53,897 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-09 01:52:31,958 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-09 01:52:56,496 : 1816976590.py : run_eval : INFO : Epoch 169: F1: 0.9648148148148151, ACC: 0.9723756906077348
2025-03-09 01:52:57,298 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-09 01:54:34,365 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-09 01:56:23,562 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-09 01:58:10,315 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-09 02:01:05,001 : 2266857300.py : eval_pred : INFO : processing 400 lines 
2025-03-09 02:02:31,954 : 1816976590.py : run_eval : INFO : Epoch 169: F1: 0.9214780600461894, ACC: 0.927437641723356
2025-03-09 02:02:33,633 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-09 02:03:27,398 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-09 02:04:31,354 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-09 02:06:09,508 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-09 02:06:32,990 : 1816976590.py : run_eval : INFO : Epoch 170: F1: 0.9648148148148151, ACC: 0.9723756906077348
2025-03-09 02:06:33,752 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-09 02:08:09,429 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-09 02:09:55,765 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-09 02:11:44,288 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-09 02:14:39,000 : 2266857300.py : eval_pred : INFO : processing 400 lines 
2025-03-09 02:16:07,023 : 1816976590.py : run_eval : INFO : Epoch 170: F1: 0.9160892994611242, ACC: 0.9229024943310657
2025-03-09 02:16:08,904 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-09 02:17:02,001 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-09 02:18:06,458 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-09 02:19:45,348 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-09 02:20:09,351 : 1816976590.py : run_eval : INFO : Epoch 199: F1: 0.9722222222222224, ACC: 0.9779005524861878
2025-03-09 02:20:10,192 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-09 02:21:48,241 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-09 02:23:36,544 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-09 02:25:23,149 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-09 02:28:16,131 : 2266857300.py : eval_pred : INFO : processing 400 lines 
2025-03-09 02:29:42,326 : 1816976590.py : run_eval : INFO : Epoch 199: F1: 0.9083910700538876, ACC: 0.9183673469387755
2025-03-09 02:29:43,385 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-09 02:30:31,397 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-09 02:31:36,886 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-09 02:32:29,578 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-09 02:32:47,150 : 1816976590.py : run_eval : INFO : Epoch 74: F1: 0.8838235294117647, ACC: 0.9122807017543859
2025-03-09 02:32:47,931 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-09 02:34:19,041 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-09 02:35:56,845 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-09 02:38:14,937 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-09 02:40:20,594 : 2266857300.py : eval_pred : INFO : processing 400 lines 
2025-03-09 02:42:55,725 : 1816976590.py : run_eval : INFO : Epoch 74: F1: 0.889434523809524, ACC: 0.9002169197396963
2025-03-09 02:42:56,548 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-09 02:43:45,277 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-09 02:44:51,084 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-09 02:45:43,877 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-09 02:46:02,175 : 1816976590.py : run_eval : INFO : Epoch 75: F1: 0.8882352941176469, ACC: 0.9152046783625731
2025-03-09 02:46:02,914 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-09 02:47:34,511 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-09 02:49:13,773 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-09 02:51:30,689 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-09 02:53:35,576 : 2266857300.py : eval_pred : INFO : processing 400 lines 
2025-03-09 02:56:05,850 : 1816976590.py : run_eval : INFO : Epoch 75: F1: 0.8868303571428573, ACC: 0.8980477223427332
2025-03-09 02:56:07,012 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-09 02:56:55,391 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-09 02:58:00,642 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-09 02:58:53,566 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-09 02:59:11,199 : 1816976590.py : run_eval : INFO : Epoch 87: F1: 0.8975490196078431, ACC: 0.9210526315789473
2025-03-09 02:59:11,924 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-09 03:00:43,472 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-09 03:02:22,569 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-09 03:04:39,451 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-09 03:06:44,690 : 2266857300.py : eval_pred : INFO : processing 400 lines 
2025-03-09 03:09:17,353 : 1816976590.py : run_eval : INFO : Epoch 87: F1: 0.8920386904761906, ACC: 0.9045553145336226
2025-03-09 03:09:18,747 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-09 03:10:06,810 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-09 03:11:11,868 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-09 03:12:04,694 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-09 03:12:22,448 : 1816976590.py : run_eval : INFO : Epoch 88: F1: 0.8921568627450979, ACC: 0.9181286549707602
2025-03-09 03:12:23,197 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-09 03:13:54,081 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-09 03:15:32,154 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-09 03:17:47,917 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-09 03:19:52,433 : 2266857300.py : eval_pred : INFO : processing 400 lines 
2025-03-09 03:22:25,201 : 1816976590.py : run_eval : INFO : Epoch 88: F1: 0.8849702380952383, ACC: 0.89587852494577
2025-03-09 03:22:26,982 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-09 03:23:15,266 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-09 03:24:21,164 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-09 03:25:13,912 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-09 03:25:31,625 : 1816976590.py : run_eval : INFO : Epoch 106: F1: 0.892156862745098, ACC: 0.9181286549707602
2025-03-09 03:25:32,386 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-09 03:27:04,826 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-09 03:28:43,384 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-09 03:31:02,317 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-09 03:33:08,061 : 2266857300.py : eval_pred : INFO : processing 400 lines 
2025-03-09 03:35:40,477 : 1816976590.py : run_eval : INFO : Epoch 106: F1: 0.9009672619047618, ACC: 0.9088937093275488
2025-03-09 03:35:41,550 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-09 03:36:29,910 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-09 03:37:36,074 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-09 03:38:28,693 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-09 03:38:46,222 : 1816976590.py : run_eval : INFO : Epoch 113: F1: 0.8887254901960782, ACC: 0.9152046783625731
2025-03-09 03:38:46,954 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-09 03:40:18,744 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-09 03:41:57,902 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-09 03:44:16,967 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-09 03:46:22,177 : 2266857300.py : eval_pred : INFO : processing 400 lines 
2025-03-09 03:48:52,794 : 1816976590.py : run_eval : INFO : Epoch 113: F1: 0.9009672619047621, ACC: 0.9088937093275488
2025-03-09 03:48:53,555 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-09 03:49:41,766 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-09 03:50:46,919 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-09 03:51:39,607 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-09 03:51:57,327 : 1816976590.py : run_eval : INFO : Epoch 162: F1: 0.9053921568627449, ACC: 0.9269005847953217
2025-03-09 03:51:58,113 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-09 03:53:29,761 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-09 03:55:08,528 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-09 03:57:26,493 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-09 03:59:31,507 : 2266857300.py : eval_pred : INFO : processing 400 lines 
2025-03-09 04:02:05,113 : 1816976590.py : run_eval : INFO : Epoch 162: F1: 0.9013392857142858, ACC: 0.911062906724512
2025-03-09 04:02:06,116 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-09 04:02:55,115 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-09 04:04:00,999 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-09 04:04:53,788 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-09 04:05:11,372 : 1816976590.py : run_eval : INFO : Epoch 187: F1: 0.9053921568627449, ACC: 0.9269005847953217
2025-03-09 04:05:12,119 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-09 04:06:44,033 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-09 04:08:23,425 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-09 04:10:41,221 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-09 04:12:47,261 : 2266857300.py : eval_pred : INFO : processing 400 lines 
2025-03-09 04:15:20,837 : 1816976590.py : run_eval : INFO : Epoch 187: F1: 0.8983630952380953, ACC: 0.9088937093275488
2025-03-09 04:15:21,567 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-09 04:16:10,178 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-09 04:17:16,538 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-09 04:18:09,101 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-09 04:18:26,710 : 1816976590.py : run_eval : INFO : Epoch 188: F1: 0.8936274509803921, ACC: 0.9181286549707602
2025-03-09 04:18:27,443 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-09 04:19:59,651 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-09 04:21:38,220 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-09 04:23:56,912 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-09 04:26:02,214 : 2266857300.py : eval_pred : INFO : processing 400 lines 
2025-03-09 04:28:34,975 : 1816976590.py : run_eval : INFO : Epoch 188: F1: 0.8961309523809524, ACC: 0.9067245119305857
2025-03-09 04:28:35,745 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-09 04:29:30,969 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-09 04:30:28,823 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-09 04:31:35,976 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-09 04:33:47,910 : 2266857300.py : eval_pred : INFO : processing 400 lines 
2025-03-09 04:35:11,210 : 1816976590.py : run_eval : INFO : Epoch 131: F1: 0.8848006019563583, ACC: 0.9101123595505618
2025-03-09 04:35:11,990 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-09 04:36:38,268 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-09 04:38:38,409 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-09 04:40:31,591 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-09 04:42:00,646 : 1816976590.py : run_eval : INFO : Epoch 131: F1: 0.8657754010695188, ACC: 0.8812664907651715
2025-03-09 04:42:01,452 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-09 04:42:55,895 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-09 04:43:52,866 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-09 04:44:59,263 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-09 04:47:08,818 : 2266857300.py : eval_pred : INFO : processing 400 lines 
2025-03-09 04:48:30,568 : 1816976590.py : run_eval : INFO : Epoch 147: F1: 0.8805116629044395, ACC: 0.9078651685393259
2025-03-09 04:48:31,307 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-09 04:49:58,847 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-09 04:51:59,110 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-09 04:53:51,601 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-09 04:55:19,733 : 1816976590.py : run_eval : INFO : Epoch 147: F1: 0.876470588235294, ACC: 0.8891820580474934
2025-03-09 04:55:20,464 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-09 04:56:15,200 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-09 04:57:12,635 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-09 04:58:18,376 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-09 05:00:27,556 : 2266857300.py : eval_pred : INFO : processing 400 lines 
2025-03-09 05:01:49,142 : 1816976590.py : run_eval : INFO : Epoch 148: F1: 0.8759969902182093, ACC: 0.9056179775280899
2025-03-09 05:01:49,925 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-09 05:03:16,909 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-09 05:05:16,871 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-09 05:07:10,093 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-09 05:08:38,507 : 1816976590.py : run_eval : INFO : Epoch 148: F1: 0.8782531194295901, ACC: 0.8918205804749341
2025-03-09 05:08:39,247 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-09 05:09:34,287 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-09 05:10:31,606 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-09 05:11:37,290 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-09 05:13:46,526 : 2266857300.py : eval_pred : INFO : processing 400 lines 
2025-03-09 05:15:08,849 : 1816976590.py : run_eval : INFO : Epoch 187: F1: 0.8823927765237022, ACC: 0.9101123595505618
2025-03-09 05:15:09,596 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-09 05:16:35,951 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-09 05:18:36,141 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-09 05:20:29,233 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-09 05:21:57,899 : 1816976590.py : run_eval : INFO : Epoch 187: F1: 0.8898395721925134, ACC: 0.899736147757256
2025-03-09 05:21:59,053 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-09 05:23:12,339 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-09 05:24:24,763 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-09 05:26:03,752 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-09 05:27:09,089 : 2266857300.py : eval_pred : INFO : processing 400 lines 
2025-03-09 05:27:17,517 : 1816976590.py : run_eval : INFO : Epoch 49: F1: 0.8584564860426933, ACC: 0.8888888888888888
2025-03-09 05:27:18,271 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-09 05:28:40,156 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-09 05:30:25,898 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-09 05:32:16,993 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-09 05:35:30,817 : 1816976590.py : run_eval : INFO : Epoch 49: F1: 0.9129870129870131, ACC: 0.923469387755102
2025-03-09 05:35:31,558 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-09 05:36:45,132 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-09 05:37:58,471 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-09 05:39:39,345 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-09 05:40:44,321 : 2266857300.py : eval_pred : INFO : processing 400 lines 
2025-03-09 05:40:52,921 : 1816976590.py : run_eval : INFO : Epoch 61: F1: 0.8587848932676521, ACC: 0.8888888888888888
2025-03-09 05:40:53,851 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-09 05:42:14,896 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-09 05:44:00,488 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-09 05:45:52,664 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-09 05:49:07,662 : 1816976590.py : run_eval : INFO : Epoch 61: F1: 0.9064935064935065, ACC: 0.9183673469387755
2025-03-09 05:49:08,431 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-09 05:50:21,426 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-09 05:51:34,283 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-09 05:53:14,007 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-09 05:54:19,769 : 2266857300.py : eval_pred : INFO : processing 400 lines 
2025-03-09 05:54:28,309 : 1816976590.py : run_eval : INFO : Epoch 104: F1: 0.8706075533661742, ACC: 0.8983451536643026
2025-03-09 05:54:29,114 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-09 05:55:50,387 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-09 05:57:35,913 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-09 05:59:27,737 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-09 06:02:42,783 : 1816976590.py : run_eval : INFO : Epoch 104: F1: 0.9047619047619049, ACC: 0.9183673469387755
2025-03-09 06:02:43,544 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-09 06:03:57,489 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-09 06:05:10,648 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-09 06:06:51,126 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-09 06:07:56,373 : 2266857300.py : eval_pred : INFO : processing 400 lines 
2025-03-09 06:08:04,699 : 1816976590.py : run_eval : INFO : Epoch 105: F1: 0.8754515599343187, ACC: 0.9030732860520094
2025-03-09 06:08:05,466 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-09 06:09:26,256 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-09 06:11:11,536 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-09 06:13:03,512 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-09 06:16:19,399 : 1816976590.py : run_eval : INFO : Epoch 105: F1: 0.9038961038961041, ACC: 0.9183673469387755
2025-03-09 06:16:20,302 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-09 06:17:33,697 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-09 06:18:47,401 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-09 06:20:27,723 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-09 06:21:32,580 : 2266857300.py : eval_pred : INFO : processing 400 lines 
2025-03-09 06:21:40,924 : 1816976590.py : run_eval : INFO : Epoch 114: F1: 0.8643678160919541, ACC: 0.8959810874704491
2025-03-09 06:21:41,781 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-09 06:23:02,988 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-09 06:24:48,516 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-09 06:26:40,285 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-09 06:29:55,942 : 1816976590.py : run_eval : INFO : Epoch 114: F1: 0.8930735930735931, ACC: 0.9081632653061225
2025-03-09 06:29:56,816 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-09 06:31:09,323 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-09 06:32:22,180 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-09 06:34:01,521 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-09 06:35:06,469 : 2266857300.py : eval_pred : INFO : processing 400 lines 
2025-03-09 06:35:15,130 : 1816976590.py : run_eval : INFO : Epoch 132: F1: 0.8761904761904765, ACC: 0.9030732860520094
2025-03-09 06:35:16,186 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-09 06:36:36,769 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-09 06:38:21,919 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-09 06:40:13,381 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-09 06:43:27,935 : 1816976590.py : run_eval : INFO : Epoch 132: F1: 0.9034632034632035, ACC: 0.9158163265306123
2025-03-09 06:43:28,778 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-09 06:44:42,515 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-09 06:45:55,373 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-09 06:47:35,192 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-09 06:48:40,605 : 2266857300.py : eval_pred : INFO : processing 400 lines 
2025-03-09 06:48:48,980 : 1816976590.py : run_eval : INFO : Epoch 162: F1: 0.8636406286652595, ACC: 0.8936170212765957
2025-03-09 06:48:49,794 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-09 06:50:10,868 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-09 06:51:56,246 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-09 06:53:48,546 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-09 06:57:03,307 : 1816976590.py : run_eval : INFO : Epoch 162: F1: 0.8974025974025974, ACC: 0.9107142857142857
2025-03-09 13:07:27,799 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-09 13:08:54,909 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-09 13:10:16,573 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-09 13:11:42,242 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-09 13:15:13,221 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-09 13:15:13,554 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-09 13:15:13,854 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-09 13:15:14,290 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-09 13:15:14,819 : 2266857300.py : eval_pred : INFO : processing 400 lines 
2025-03-09 13:15:14,969 : <timed exec> : <module> : INFO : STS with InriaValda/cc_math_bert_ep10 on new candi fold 1: F1: 0.26098208701867254, ACC: 0.32935560859188545
2025-03-09 13:15:14,970 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-09 13:15:15,103 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-09 13:15:15,235 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-09 13:15:15,605 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-09 13:15:15,758 : 2266857300.py : eval_pred : INFO : processing 400 lines 
2025-03-09 13:15:15,774 : <timed exec> : <module> : INFO : STS with InriaValda/cc_math_bert_ep10 on new term fold 1: F1: 0.3225898692810459, ACC: 0.42718446601941745
2025-03-09 13:15:17,016 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-09 13:15:17,238 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-09 13:15:17,530 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-09 13:15:17,867 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-09 13:15:18,487 : 2266857300.py : eval_pred : INFO : processing 400 lines 
2025-03-09 13:15:18,727 : <timed exec> : <module> : INFO : STS with InriaValda/cc_math_bert_ep10 on new candi fold 2: F1: 0.26450629816680643, ACC: 0.3356009070294785
2025-03-09 13:15:18,728 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-09 13:15:18,840 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-09 13:15:18,966 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-09 13:15:19,221 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-09 13:15:19,271 : <timed exec> : <module> : INFO : STS with InriaValda/cc_math_bert_ep10 on new term fold 2: F1: 0.35535734327401014, ACC: 0.44751381215469616
2025-03-09 13:15:20,562 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-09 13:15:20,801 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-09 13:15:21,057 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-09 13:15:21,534 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-09 13:15:22,038 : 2266857300.py : eval_pred : INFO : processing 400 lines 
2025-03-09 13:15:22,515 : <timed exec> : <module> : INFO : STS with InriaValda/cc_math_bert_ep10 on new candi fold 3: F1: 0.24424841651404167, ACC: 0.31453362255965295
2025-03-09 13:15:22,515 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-09 13:15:22,635 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-09 13:15:22,769 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-09 13:15:22,877 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-09 13:15:22,920 : <timed exec> : <module> : INFO : STS with InriaValda/cc_math_bert_ep10 on new term fold 3: F1: 0.3593258212375862, ACC: 0.4619883040935672
2025-03-09 13:15:24,132 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-09 13:15:24,369 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-09 13:15:24,808 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-09 13:15:25,142 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-09 13:15:25,518 : <timed exec> : <module> : INFO : STS with InriaValda/cc_math_bert_ep10 on new candi fold 4: F1: 0.24877557083439444, ACC: 0.31926121372031663
2025-03-09 13:15:25,519 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-09 13:15:25,666 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-09 13:15:25,812 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-09 13:15:25,983 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-09 13:15:26,369 : 2266857300.py : eval_pred : INFO : processing 400 lines 
2025-03-09 13:15:26,622 : <timed exec> : <module> : INFO : STS with InriaValda/cc_math_bert_ep10 on new term fold 4: F1: 0.33804880405597454, ACC: 0.4337078651685393
2025-03-09 13:15:28,012 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-09 13:15:28,225 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-09 13:15:28,498 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-09 13:15:28,934 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-09 13:15:29,562 : <timed exec> : <module> : INFO : STS with InriaValda/cc_math_bert_ep10 on new candi fold 5: F1: 0.26759914688486125, ACC: 0.336734693877551
2025-03-09 13:15:29,563 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-09 13:15:29,746 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-09 13:15:29,942 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-09 13:15:30,307 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-09 13:15:30,463 : 2266857300.py : eval_pred : INFO : processing 400 lines 
2025-03-09 13:15:30,492 : <timed exec> : <module> : INFO : STS with InriaValda/cc_math_bert_ep10 on new term fold 5: F1: 0.3395995356586491, ACC: 0.4302600472813239
2025-03-09 13:18:22,300 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-09 13:18:22,451 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-09 13:18:22,652 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-09 13:18:22,933 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-09 13:18:23,272 : 2266857300.py : eval_pred : INFO : processing 400 lines 
2025-03-09 13:18:23,369 : <timed exec> : <module> : INFO : STS with sentence-transformers/all-MiniLM-L6-v2 on new candi fold 1: F1: 0.9027642276422764, ACC: 0.9236276849642004
2025-03-09 13:18:23,369 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-09 13:18:23,453 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-09 13:18:23,537 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-09 13:18:23,766 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-09 13:18:23,863 : 2266857300.py : eval_pred : INFO : processing 400 lines 
2025-03-09 13:18:23,877 : <timed exec> : <module> : INFO : STS with sentence-transformers/all-MiniLM-L6-v2 on new term fold 1: F1: 0.8387254901960787, ACC: 0.8713592233009708
2025-03-09 13:18:24,295 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-09 13:18:24,446 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-09 13:18:24,641 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-09 13:18:24,867 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-09 13:18:25,269 : 2266857300.py : eval_pred : INFO : processing 400 lines 
2025-03-09 13:18:25,423 : <timed exec> : <module> : INFO : STS with sentence-transformers/all-MiniLM-L6-v2 on new candi fold 2: F1: 0.8891454965357969, ACC: 0.909297052154195
2025-03-09 13:18:25,424 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-09 13:18:25,500 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-09 13:18:25,586 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-09 13:18:25,759 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-09 13:18:25,796 : <timed exec> : <module> : INFO : STS with sentence-transformers/all-MiniLM-L6-v2 on new term fold 2: F1: 0.8995370370370372, ACC: 0.9226519337016574
2025-03-09 13:18:26,183 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-09 13:18:26,333 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-09 13:18:26,492 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-09 13:18:26,788 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-09 13:18:27,100 : 2266857300.py : eval_pred : INFO : processing 400 lines 
2025-03-09 13:18:27,397 : <timed exec> : <module> : INFO : STS with sentence-transformers/all-MiniLM-L6-v2 on new candi fold 3: F1: 0.8890625, ACC: 0.911062906724512
2025-03-09 13:18:27,398 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-09 13:18:27,478 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-09 13:18:27,568 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-09 13:18:27,640 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-09 13:18:27,671 : <timed exec> : <module> : INFO : STS with sentence-transformers/all-MiniLM-L6-v2 on new term fold 3: F1: 0.9034313725490195, ACC: 0.9269005847953217
2025-03-09 13:18:28,057 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-09 13:18:28,207 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-09 13:18:28,479 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-09 13:18:28,684 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-09 13:18:28,925 : <timed exec> : <module> : INFO : STS with sentence-transformers/all-MiniLM-L6-v2 on new candi fold 4: F1: 0.89349376114082, ACC: 0.9102902374670184
2025-03-09 13:18:28,925 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-09 13:18:29,018 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-09 13:18:29,112 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-09 13:18:29,222 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-09 13:18:29,468 : 2266857300.py : eval_pred : INFO : processing 400 lines 
2025-03-09 13:18:29,632 : <timed exec> : <module> : INFO : STS with sentence-transformers/all-MiniLM-L6-v2 on new term fold 4: F1: 0.9153498871331829, ACC: 0.9348314606741573
2025-03-09 13:18:30,084 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-09 13:18:30,218 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-09 13:18:30,388 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-09 13:18:30,656 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-09 13:18:31,049 : <timed exec> : <module> : INFO : STS with sentence-transformers/all-MiniLM-L6-v2 on new candi fold 5: F1: 0.899047619047619, ACC: 0.9158163265306123
2025-03-09 13:18:31,050 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-09 13:18:31,160 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-09 13:18:31,280 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-09 13:18:31,503 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-09 13:18:31,601 : 2266857300.py : eval_pred : INFO : processing 400 lines 
2025-03-09 13:18:31,624 : <timed exec> : <module> : INFO : STS with sentence-transformers/all-MiniLM-L6-v2 on new term fold 5: F1: 0.8968801313628902, ACC: 0.9219858156028369
2025-03-09 13:20:32,734 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-09 13:20:32,946 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-09 13:20:33,229 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-09 13:20:33,634 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-09 13:20:34,172 : 2266857300.py : eval_pred : INFO : processing 400 lines 
2025-03-09 13:20:34,326 : <timed exec> : <module> : INFO : STS with bert-base-uncased on new candi fold 1: F1: 0.18894431199951103, ACC: 0.2577565632458234
2025-03-09 13:20:34,327 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-09 13:20:34,462 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-09 13:20:34,599 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-09 13:20:34,979 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-09 13:20:35,137 : 2266857300.py : eval_pred : INFO : processing 400 lines 
2025-03-09 13:20:35,154 : <timed exec> : <module> : INFO : STS with bert-base-uncased on new term fold 1: F1: 0.2759187934433034, ACC: 0.38106796116504854
2025-03-09 13:20:35,695 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-09 13:20:35,937 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-09 13:20:36,250 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-09 13:20:36,628 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-09 13:20:37,292 : 2266857300.py : eval_pred : INFO : processing 400 lines 
2025-03-09 13:20:37,540 : <timed exec> : <module> : INFO : STS with bert-base-uncased on new candi fold 2: F1: 0.21143143977469983, ACC: 0.2743764172335601
2025-03-09 13:20:37,541 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-09 13:20:37,666 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-09 13:20:37,809 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-09 13:20:38,085 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-09 13:20:38,136 : <timed exec> : <module> : INFO : STS with bert-base-uncased on new term fold 2: F1: 0.3333778258778261, ACC: 0.43370165745856354
2025-03-09 13:20:39,033 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-09 13:20:39,263 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-09 13:20:39,510 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-09 13:20:39,971 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-09 13:20:40,451 : 2266857300.py : eval_pred : INFO : processing 400 lines 
2025-03-09 13:20:40,934 : <timed exec> : <module> : INFO : STS with bert-base-uncased on new candi fold 3: F1: 0.19296242839240962, ACC: 0.2603036876355748
2025-03-09 13:20:40,934 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-09 13:20:41,067 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-09 13:20:41,217 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-09 13:20:41,339 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-09 13:20:41,385 : <timed exec> : <module> : INFO : STS with bert-base-uncased on new term fold 3: F1: 0.3515476190476193, ACC: 0.45321637426900585
2025-03-09 13:20:41,889 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-09 13:20:42,130 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-09 13:20:42,569 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-09 13:20:42,901 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-09 13:20:43,285 : <timed exec> : <module> : INFO : STS with bert-base-uncased on new candi fold 4: F1: 0.21658694418756338, ACC: 0.29023746701846964
2025-03-09 13:20:43,286 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-09 13:20:43,438 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-09 13:20:43,593 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-09 13:20:43,772 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-09 13:20:44,169 : 2266857300.py : eval_pred : INFO : processing 400 lines 
2025-03-09 13:20:44,430 : <timed exec> : <module> : INFO : STS with bert-base-uncased on new term fold 4: F1: 0.2801981045535276, ACC: 0.3685393258426966
2025-03-09 13:20:44,982 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-09 13:20:45,200 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-09 13:20:45,476 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-09 13:20:45,919 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-09 13:20:46,549 : <timed exec> : <module> : INFO : STS with bert-base-uncased on new candi fold 5: F1: 0.2145780251494538, ACC: 0.2780612244897959
2025-03-09 13:20:46,550 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-09 13:20:46,734 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-09 13:20:46,932 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-09 13:20:47,304 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-09 13:20:47,464 : 2266857300.py : eval_pred : INFO : processing 400 lines 
2025-03-09 13:20:47,493 : <timed exec> : <module> : INFO : STS with bert-base-uncased on new term fold 5: F1: 0.262217585370295, ACC: 0.35933806146572106
2025-03-09 13:21:17,886 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-09 13:21:18,119 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-09 13:21:18,428 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-09 13:21:18,865 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-09 13:21:19,407 : 2266857300.py : eval_pred : INFO : processing 400 lines 
2025-03-09 13:21:19,560 : <timed exec> : <module> : INFO : STS with math-similarity/Bert-MLM_arXiv on new candi fold 1: F1: 0.21406645405804367, ACC: 0.27684964200477324
2025-03-09 13:21:19,561 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-09 13:21:19,696 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-09 13:21:19,832 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-09 13:21:20,215 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-09 13:21:20,374 : 2266857300.py : eval_pred : INFO : processing 400 lines 
2025-03-09 13:21:20,391 : <timed exec> : <module> : INFO : STS with math-similarity/Bert-MLM_arXiv on new term fold 1: F1: 0.3068218954248368, ACC: 0.41262135922330095
2025-03-09 13:21:21,113 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-09 13:21:21,358 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-09 13:21:21,681 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-09 13:21:22,059 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-09 13:21:22,731 : 2266857300.py : eval_pred : INFO : processing 400 lines 
2025-03-09 13:21:22,982 : <timed exec> : <module> : INFO : STS with math-similarity/Bert-MLM_arXiv on new candi fold 2: F1: 0.24296056351483616, ACC: 0.30839002267573695
2025-03-09 13:21:22,983 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-09 13:21:23,110 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-09 13:21:23,255 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-09 13:21:23,535 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-09 13:21:23,588 : <timed exec> : <module> : INFO : STS with math-similarity/Bert-MLM_arXiv on new term fold 2: F1: 0.3272220834720837, ACC: 0.4281767955801105
2025-03-09 13:21:24,391 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-09 13:21:24,632 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-09 13:21:24,891 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-09 13:21:25,380 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-09 13:21:25,885 : 2266857300.py : eval_pred : INFO : processing 400 lines 
2025-03-09 13:21:26,371 : <timed exec> : <module> : INFO : STS with math-similarity/Bert-MLM_arXiv on new candi fold 3: F1: 0.21556578854916295, ACC: 0.28199566160520606
2025-03-09 13:21:26,372 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-09 13:21:26,502 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-09 13:21:26,649 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-09 13:21:26,766 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-09 13:21:26,811 : <timed exec> : <module> : INFO : STS with math-similarity/Bert-MLM_arXiv on new term fold 3: F1: 0.3759502588914356, ACC: 0.47368421052631576
2025-03-09 13:21:27,537 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-09 13:21:27,785 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-09 13:21:28,223 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-09 13:21:28,566 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-09 13:21:28,958 : <timed exec> : <module> : INFO : STS with math-similarity/Bert-MLM_arXiv on new candi fold 4: F1: 0.24168318312703346, ACC: 0.3034300791556728
2025-03-09 13:21:28,959 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-09 13:21:29,110 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-09 13:21:29,264 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-09 13:21:29,444 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-09 13:21:29,844 : 2266857300.py : eval_pred : INFO : processing 400 lines 
2025-03-09 13:21:30,107 : <timed exec> : <module> : INFO : STS with math-similarity/Bert-MLM_arXiv on new term fold 4: F1: 0.30914968160628653, ACC: 0.4044943820224719
2025-03-09 13:21:30,849 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-09 13:21:31,066 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-09 13:21:31,350 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-09 13:21:31,795 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-09 13:21:32,439 : <timed exec> : <module> : INFO : STS with math-similarity/Bert-MLM_arXiv on new candi fold 5: F1: 0.24511985004595846, ACC: 0.30357142857142855
2025-03-09 13:21:32,440 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-09 13:21:32,628 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-09 13:21:32,829 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-09 13:21:33,199 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-09 13:21:33,362 : 2266857300.py : eval_pred : INFO : processing 400 lines 
2025-03-09 13:21:33,391 : <timed exec> : <module> : INFO : STS with math-similarity/Bert-MLM_arXiv on new term fold 5: F1: 0.28254813942295864, ACC: 0.3806146572104019
2025-03-09 13:21:52,786 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-09 13:21:53,022 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-09 13:21:53,342 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-09 13:21:53,792 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-09 13:21:54,348 : 2266857300.py : eval_pred : INFO : processing 400 lines 
2025-03-09 13:21:54,505 : <timed exec> : <module> : INFO : STS with math-similarity/Bert-MLM_arXiv-MP-class_zbMath on new candi fold 1: F1: 0.4193263646922183, ACC: 0.49403341288782815
2025-03-09 13:21:54,506 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-09 13:21:54,643 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-09 13:21:54,782 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-09 13:21:55,163 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-09 13:21:55,321 : 2266857300.py : eval_pred : INFO : processing 400 lines 
2025-03-09 13:21:55,339 : <timed exec> : <module> : INFO : STS with math-similarity/Bert-MLM_arXiv-MP-class_zbMath on new term fold 1: F1: 0.48611111111111055, ACC: 0.5679611650485437
2025-03-09 13:21:56,064 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-09 13:21:56,312 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-09 13:21:56,633 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-09 13:21:57,013 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-09 13:21:57,690 : 2266857300.py : eval_pred : INFO : processing 400 lines 
2025-03-09 13:21:57,941 : <timed exec> : <module> : INFO : STS with math-similarity/Bert-MLM_arXiv-MP-class_zbMath on new candi fold 2: F1: 0.4082481029363246, ACC: 0.4852607709750567
2025-03-09 13:21:57,942 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-09 13:21:58,068 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-09 13:21:58,210 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-09 13:21:58,498 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-09 13:21:58,553 : <timed exec> : <module> : INFO : STS with math-similarity/Bert-MLM_arXiv-MP-class_zbMath on new term fold 2: F1: 0.5814814814814812, ACC: 0.6546961325966851
2025-03-09 13:21:59,283 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-09 13:21:59,514 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-09 13:21:59,762 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-09 13:22:00,251 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-09 13:22:00,757 : 2266857300.py : eval_pred : INFO : processing 400 lines 
2025-03-09 13:22:01,239 : <timed exec> : <module> : INFO : STS with math-similarity/Bert-MLM_arXiv-MP-class_zbMath on new candi fold 3: F1: 0.40746173469387736, ACC: 0.4793926247288503
2025-03-09 13:22:01,240 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-09 13:22:01,373 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-09 13:22:01,520 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-09 13:22:01,637 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-09 13:22:01,683 : <timed exec> : <module> : INFO : STS with math-similarity/Bert-MLM_arXiv-MP-class_zbMath on new term fold 3: F1: 0.5268767507002798, ACC: 0.6111111111111112
2025-03-09 13:22:02,563 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-09 13:22:02,809 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-09 13:22:03,258 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-09 13:22:03,594 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-09 13:22:03,981 : <timed exec> : <module> : INFO : STS with math-similarity/Bert-MLM_arXiv-MP-class_zbMath on new candi fold 4: F1: 0.40477463712757833, ACC: 0.47229551451187335
2025-03-09 13:22:03,982 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-09 13:22:04,132 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-09 13:22:04,285 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-09 13:22:04,460 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-09 13:22:04,854 : 2266857300.py : eval_pred : INFO : processing 400 lines 
2025-03-09 13:22:05,116 : <timed exec> : <module> : INFO : STS with math-similarity/Bert-MLM_arXiv-MP-class_zbMath on new term fold 4: F1: 0.49300763194668346, ACC: 0.5797752808988764
2025-03-09 13:22:05,961 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-09 13:22:06,176 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-09 13:22:06,448 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-09 13:22:06,882 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-09 13:22:07,511 : <timed exec> : <module> : INFO : STS with math-similarity/Bert-MLM_arXiv-MP-class_zbMath on new candi fold 5: F1: 0.41823129251700675, ACC: 0.4872448979591837
2025-03-09 13:22:07,512 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-09 13:22:07,695 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-09 13:22:07,893 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-09 13:22:08,263 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-09 13:22:08,421 : 2266857300.py : eval_pred : INFO : processing 400 lines 
2025-03-09 13:22:08,451 : <timed exec> : <module> : INFO : STS with math-similarity/Bert-MLM_arXiv-MP-class_zbMath on new term fold 5: F1: 0.5033075299085149, ACC: 0.5768321513002365
2025-03-09 13:22:55,928 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-09 13:22:56,076 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-09 13:22:56,268 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-09 13:22:56,541 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-09 13:22:56,879 : 2266857300.py : eval_pred : INFO : processing 400 lines 
2025-03-09 13:22:56,978 : <timed exec> : <module> : INFO : STS with sentence-transformers/all-MiniLM-L12-v2 on new candi fold 1: F1: 0.908780487804878, ACC: 0.9284009546539379
2025-03-09 13:22:56,979 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-09 13:22:57,064 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-09 13:22:57,147 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-09 13:22:57,375 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-09 13:22:57,469 : 2266857300.py : eval_pred : INFO : processing 400 lines 
2025-03-09 13:22:57,483 : <timed exec> : <module> : INFO : STS with sentence-transformers/all-MiniLM-L12-v2 on new term fold 1: F1: 0.8629084967320262, ACC: 0.8932038834951457
2025-03-09 13:22:57,920 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-09 13:22:58,070 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-09 13:22:58,264 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-09 13:22:58,492 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-09 13:22:58,894 : 2266857300.py : eval_pred : INFO : processing 400 lines 
2025-03-09 13:22:59,051 : <timed exec> : <module> : INFO : STS with sentence-transformers/all-MiniLM-L12-v2 on new candi fold 2: F1: 0.8806774441878369, ACC: 0.9024943310657596
2025-03-09 13:22:59,052 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-09 13:22:59,129 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-09 13:22:59,219 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-09 13:22:59,393 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-09 13:22:59,430 : <timed exec> : <module> : INFO : STS with sentence-transformers/all-MiniLM-L12-v2 on new term fold 2: F1: 0.9194444444444447, ACC: 0.9392265193370166
2025-03-09 13:22:59,865 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-09 13:23:00,014 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-09 13:23:00,174 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-09 13:23:00,477 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-09 13:23:00,791 : 2266857300.py : eval_pred : INFO : processing 400 lines 
2025-03-09 13:23:01,095 : <timed exec> : <module> : INFO : STS with sentence-transformers/all-MiniLM-L12-v2 on new candi fold 3: F1: 0.8949404761904762, ACC: 0.9154013015184381
2025-03-09 13:23:01,097 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-09 13:23:01,179 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-09 13:23:01,270 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-09 13:23:01,344 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-09 13:23:01,377 : <timed exec> : <module> : INFO : STS with sentence-transformers/all-MiniLM-L12-v2 on new term fold 3: F1: 0.89656862745098, ACC: 0.9210526315789473
2025-03-09 13:23:01,800 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-09 13:23:01,950 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-09 13:23:02,223 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-09 13:23:02,426 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-09 13:23:02,663 : <timed exec> : <module> : INFO : STS with sentence-transformers/all-MiniLM-L12-v2 on new candi fold 4: F1: 0.8957219251336899, ACC: 0.9129287598944591
2025-03-09 13:23:02,663 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-09 13:23:02,756 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-09 13:23:02,851 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-09 13:23:02,961 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-09 13:23:03,207 : 2266857300.py : eval_pred : INFO : processing 400 lines 
2025-03-09 13:23:03,373 : <timed exec> : <module> : INFO : STS with sentence-transformers/all-MiniLM-L12-v2 on new term fold 4: F1: 0.9228743416102332, ACC: 0.9415730337078652
2025-03-09 13:23:03,802 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-09 13:23:03,936 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-09 13:23:04,107 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-09 13:23:04,379 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-09 13:23:04,778 : <timed exec> : <module> : INFO : STS with sentence-transformers/all-MiniLM-L12-v2 on new candi fold 5: F1: 0.8924675324675324, ACC: 0.9107142857142857
2025-03-09 13:23:04,779 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-09 13:23:04,892 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-09 13:23:05,015 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-09 13:23:05,240 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-09 13:23:05,338 : 2266857300.py : eval_pred : INFO : processing 400 lines 
2025-03-09 13:23:05,360 : <timed exec> : <module> : INFO : STS with sentence-transformers/all-MiniLM-L12-v2 on new term fold 5: F1: 0.91247947454844, ACC: 0.933806146572104
2025-03-09 13:24:04,378 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-09 13:24:04,605 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-09 13:24:04,908 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-09 13:24:05,343 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-09 13:24:05,900 : 2266857300.py : eval_pred : INFO : processing 400 lines 
2025-03-09 13:24:06,056 : <timed exec> : <module> : INFO : STS with sentence-transformers/all-mpnet-base-v2 on new candi fold 1: F1: 0.918211382113821, ACC: 0.9355608591885441
2025-03-09 13:24:06,057 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-09 13:24:06,189 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-09 13:24:06,322 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-09 13:24:06,689 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-09 13:24:06,844 : 2266857300.py : eval_pred : INFO : processing 400 lines 
2025-03-09 13:24:06,862 : <timed exec> : <module> : INFO : STS with sentence-transformers/all-mpnet-base-v2 on new term fold 1: F1: 0.863562091503268, ACC: 0.8956310679611651
2025-03-09 13:24:07,363 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-09 13:24:07,614 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-09 13:24:07,946 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-09 13:24:08,330 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-09 13:24:09,007 : 2266857300.py : eval_pred : INFO : processing 400 lines 
2025-03-09 13:24:09,259 : <timed exec> : <module> : INFO : STS with sentence-transformers/all-mpnet-base-v2 on new candi fold 2: F1: 0.9049268668206314, ACC: 0.9229024943310657
2025-03-09 13:24:09,259 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-09 13:24:09,384 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-09 13:24:09,523 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-09 13:24:09,811 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-09 13:24:09,868 : <timed exec> : <module> : INFO : STS with sentence-transformers/all-mpnet-base-v2 on new term fold 2: F1: 0.9162037037037036, ACC: 0.93646408839779
2025-03-09 13:24:10,711 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-09 13:24:10,957 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-09 13:24:11,223 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-09 13:24:11,716 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-09 13:24:12,228 : 2266857300.py : eval_pred : INFO : processing 400 lines 
2025-03-09 13:24:12,717 : <timed exec> : <module> : INFO : STS with sentence-transformers/all-mpnet-base-v2 on new candi fold 3: F1: 0.9113095238095239, ACC: 0.928416485900217
2025-03-09 13:24:12,717 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-09 13:24:12,848 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-09 13:24:12,993 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-09 13:24:13,112 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-09 13:24:13,158 : <timed exec> : <module> : INFO : STS with sentence-transformers/all-mpnet-base-v2 on new term fold 3: F1: 0.8906862745098036, ACC: 0.9181286549707602
2025-03-09 13:24:13,826 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-09 13:24:14,059 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-09 13:24:14,486 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-09 13:24:14,808 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-09 13:24:15,176 : <timed exec> : <module> : INFO : STS with sentence-transformers/all-mpnet-base-v2 on new candi fold 4: F1: 0.9205882352941177, ACC: 0.9340369393139841
2025-03-09 13:24:15,177 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-09 13:24:15,328 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-09 13:24:15,480 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-09 13:24:15,659 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-09 13:24:16,065 : 2266857300.py : eval_pred : INFO : processing 400 lines 
2025-03-09 13:24:16,333 : <timed exec> : <module> : INFO : STS with sentence-transformers/all-mpnet-base-v2 on new term fold 4: F1: 0.9051918735891646, ACC: 0.9280898876404494
2025-03-09 13:24:16,899 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-09 13:24:17,111 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-09 13:24:17,377 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-09 13:24:17,812 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-09 13:24:18,455 : <timed exec> : <module> : INFO : STS with sentence-transformers/all-mpnet-base-v2 on new candi fold 5: F1: 0.8985281385281385, ACC: 0.9158163265306123
2025-03-09 13:24:18,456 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-09 13:24:18,638 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-09 13:24:18,837 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-09 13:24:19,208 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-09 13:24:19,370 : 2266857300.py : eval_pred : INFO : processing 400 lines 
2025-03-09 13:24:19,400 : <timed exec> : <module> : INFO : STS with sentence-transformers/all-mpnet-base-v2 on new term fold 5: F1: 0.9171592775041052, ACC: 0.933806146572104
2025-03-09 13:25:35,845 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-09 13:25:36,085 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-09 13:25:36,409 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-09 13:25:36,866 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-09 13:25:37,429 : 2266857300.py : eval_pred : INFO : processing 400 lines 
2025-03-09 13:25:37,588 : <timed exec> : <module> : INFO : STS with Lihuchen/pearl_base on new candi fold 1: F1: 0.9099186991869919, ACC: 0.9236276849642004
2025-03-09 13:25:37,589 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-09 13:25:37,727 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-09 13:25:37,869 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-09 13:25:38,257 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-09 13:25:38,419 : 2266857300.py : eval_pred : INFO : processing 400 lines 
2025-03-09 13:25:38,437 : <timed exec> : <module> : INFO : STS with Lihuchen/pearl_base on new term fold 1: F1: 0.8386437908496732, ACC: 0.8737864077669902
2025-03-09 13:25:38,941 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-09 13:25:39,180 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-09 13:25:39,495 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-09 13:25:39,869 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-09 13:25:40,533 : 2266857300.py : eval_pred : INFO : processing 400 lines 
2025-03-09 13:25:40,781 : <timed exec> : <module> : INFO : STS with Lihuchen/pearl_base on new candi fold 2: F1: 0.8791377983063897, ACC: 0.8956916099773242
2025-03-09 13:25:40,782 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-09 13:25:40,900 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-09 13:25:41,033 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-09 13:25:41,314 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-09 13:25:41,367 : <timed exec> : <module> : INFO : STS with Lihuchen/pearl_base on new term fold 2: F1: 0.8959259259259258, ACC: 0.9171270718232044
2025-03-09 13:25:41,874 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-09 13:25:42,118 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-09 13:25:42,380 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-09 13:25:42,877 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-09 13:25:43,393 : 2266857300.py : eval_pred : INFO : processing 400 lines 
2025-03-09 13:25:43,885 : <timed exec> : <module> : INFO : STS with Lihuchen/pearl_base on new candi fold 3: F1: 0.8803571428571431, ACC: 0.8980477223427332
2025-03-09 13:25:43,886 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-09 13:25:44,017 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-09 13:25:44,163 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-09 13:25:44,279 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-09 13:25:44,325 : <timed exec> : <module> : INFO : STS with Lihuchen/pearl_base on new term fold 3: F1: 0.8955882352941175, ACC: 0.9210526315789473
2025-03-09 13:25:44,877 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-09 13:25:45,126 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-09 13:25:45,577 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-09 13:25:45,918 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-09 13:25:46,308 : <timed exec> : <module> : INFO : STS with Lihuchen/pearl_base on new candi fold 4: F1: 0.8754010695187167, ACC: 0.8918205804749341
2025-03-09 13:25:46,309 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-09 13:25:46,462 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-09 13:25:46,616 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-09 13:25:46,795 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-09 13:25:47,206 : 2266857300.py : eval_pred : INFO : processing 400 lines 
2025-03-09 13:25:47,479 : <timed exec> : <module> : INFO : STS with Lihuchen/pearl_base on new term fold 4: F1: 0.8954100827689996, ACC: 0.9191011235955057
2025-03-09 13:25:47,990 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-09 13:25:48,216 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-09 13:25:48,500 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-09 13:25:48,949 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-09 13:25:49,591 : <timed exec> : <module> : INFO : STS with Lihuchen/pearl_base on new candi fold 5: F1: 0.8902164502164504, ACC: 0.9030612244897959
2025-03-09 13:25:49,592 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-09 13:25:49,777 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-09 13:25:49,976 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-09 13:25:50,354 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-09 13:25:50,516 : 2266857300.py : eval_pred : INFO : processing 400 lines 
2025-03-09 13:25:50,546 : <timed exec> : <module> : INFO : STS with Lihuchen/pearl_base on new term fold 5: F1: 0.873563218390805, ACC: 0.900709219858156
2025-03-09 13:26:38,625 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-09 13:28:06,066 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-09 13:29:55,535 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-09 13:32:00,538 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-09 13:34:49,195 : 2266857300.py : eval_pred : INFO : processing 400 lines 
2025-03-09 13:35:38,614 : <timed exec> : <module> : INFO : NSP with bert-base-uncased on new candi fold 1: F1: 0.8197677119628338, ACC: 0.847255369928401
2025-03-09 13:35:38,615 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-09 13:36:31,530 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-09 13:37:27,736 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-09 13:39:04,275 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-09 13:40:02,170 : 2266857300.py : eval_pred : INFO : processing 400 lines 
2025-03-09 13:40:06,374 : <timed exec> : <module> : INFO : NSP with bert-base-uncased on new term fold 1: F1: 0.7504084967320264, ACC: 0.7985436893203883
2025-03-09 13:40:06,955 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-09 13:41:39,435 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-09 13:43:22,806 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-09 13:45:08,583 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-09 13:48:00,508 : 2266857300.py : eval_pred : INFO : processing 400 lines 
2025-03-09 13:49:26,531 : <timed exec> : <module> : INFO : NSP with bert-base-uncased on new candi fold 2: F1: 0.7931485758275597, ACC: 0.8231292517006803
2025-03-09 13:49:26,531 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-09 13:50:18,580 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-09 13:51:21,404 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-09 13:52:57,890 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-09 13:53:21,607 : <timed exec> : <module> : INFO : NSP with bert-base-uncased on new term fold 2: F1: 0.8346296296296297, ACC: 0.8729281767955801
2025-03-09 13:53:22,263 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-09 13:54:54,426 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-09 13:56:33,714 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-09 13:58:51,995 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-09 14:00:57,875 : 2266857300.py : eval_pred : INFO : processing 400 lines 
2025-03-09 14:03:31,040 : <timed exec> : <module> : INFO : NSP with bert-base-uncased on new candi fold 3: F1: 0.8067814625850342, ACC: 0.8329718004338394
2025-03-09 14:03:31,041 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-09 14:04:19,539 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-09 14:05:25,909 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-09 14:06:18,925 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-09 14:06:37,120 : <timed exec> : <module> : INFO : NSP with bert-base-uncased on new term fold 3: F1: 0.8318627450980388, ACC: 0.868421052631579
2025-03-09 14:06:37,645 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-09 14:08:07,583 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-09 14:10:11,573 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-09 14:12:06,853 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-09 14:13:36,190 : <timed exec> : <module> : INFO : NSP with bert-base-uncased on new candi fold 4: F1: 0.8244206773618538, ACC: 0.8496042216358839
2025-03-09 14:13:36,191 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-09 14:14:32,218 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-09 14:15:30,624 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-09 14:16:37,533 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-09 14:18:49,875 : 2266857300.py : eval_pred : INFO : processing 400 lines 
2025-03-09 14:20:14,270 : <timed exec> : <module> : INFO : NSP with bert-base-uncased on new term fold 4: F1: 0.803085026335591, ACC: 0.8449438202247191
2025-03-09 14:20:14,796 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-09 14:21:37,110 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-09 14:23:23,011 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-09 14:25:14,948 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-09 14:28:30,652 : <timed exec> : <module> : INFO : NSP with bert-base-uncased on new candi fold 5: F1: 0.8227829313543598, ACC: 0.8469387755102041
2025-03-09 14:28:30,653 : 2266857300.py : eval_pred : INFO : processing 0 lines 
2025-03-09 14:29:44,857 : 2266857300.py : eval_pred : INFO : processing 100 lines 
2025-03-09 14:30:57,263 : 2266857300.py : eval_pred : INFO : processing 200 lines 
2025-03-09 14:32:36,570 : 2266857300.py : eval_pred : INFO : processing 300 lines 
2025-03-09 14:33:41,664 : 2266857300.py : eval_pred : INFO : processing 400 lines 
2025-03-09 14:33:50,003 : <timed exec> : <module> : INFO : NSP with bert-base-uncased on new term fold 5: F1: 0.8051724137931037, ACC: 0.8439716312056738
