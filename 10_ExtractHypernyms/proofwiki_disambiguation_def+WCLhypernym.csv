CATEGORIES,DD_PAGE_TITLE,DEFINITION,DEF_PAGE_TITLE,HYPERNYM,IS_DEFINITION,PLAIN_TEXT_DEFINITION,TERM
['Definitions/Language Definitions'],Definition:Above,"In the context of numbers, above means greater than.

Note that this applies to:
:the natural numbers $mathbb N$
:the integers $mathbb Z$
:the rational numbers $mathbb Q$
:the real numbers $mathbb R$

but specifically not the complex numbers $mathbb C$ because the complex numbers do not have a usual ordering.",Definition:Above (Number),,false,"In the context of numbers, above means greater than.

Note that this applies to:
:the natural numbers ℕ
:the integers ℤ
:the rational numbers ℚ
:the real numbers ℝ

but specifically not the complex numbers ℂ because the complex numbers do not have a usual ordering.",Above
"['Definitions/Language Definitions', 'Definitions/Solid Geometry']",Definition:Above,"Let $a$ and $b$ be points in $3$-dimensional Euclidean space $mathbb R^3$.

Let $P$ be an arbitrary plane embedded in $S$ be distinguished and defined as horizontal.

Then:
:$a$ is above $b$
 if and only if :
:the height of $a$   $P$ is greater than the height of $b$   $P$.",Definition:Above (Solid Geometry),,false,"Let a and b be points in 3-dimensional Euclidean space ℝ^3.

Let P be an arbitrary plane embedded in S be distinguished and defined as horizontal.

Then:
:a is above b
 if and only if :
:the height of a   P is greater than the height of b   P.",Above
"['Definitions/Language Definitions', 'Definitions/Plane Geometry']",Definition:Above,"Let $a$ and $b$ be points in the cartesian plane $mathbb R^2$.

Then:
:$a$ is above $b$
 if and only if :
:the $y$ coordinate of $a$ is greater than the $y$ coordinate of $b$.",Definition:Above (Plane Geometry),,false,"Let a and b be points in the cartesian plane ℝ^2.

Then:
:a is above b
 if and only if :
:the y coordinate of a is greater than the y coordinate of b.",Above
"['Definitions/Algebra', 'Definitions/Numbers']",Definition:Absolute,"An absolute number is a number in an expression which has a single value.

It is either expressed using actual figures, in an agreed number system, or by a symbol which is understood to represent that specific number.",Definition:Absolute Number,,false,"An absolute number is a number in an expression which has a single value.

It is either expressed using actual figures, in an agreed number system, or by a symbol which is understood to represent that specific number.",Absolute
"['Definitions/Constants', 'Definitions/Algebra']",Definition:Absolute,"A constant is a name for an object (usually a number, but the concept has wider applications) which does not change during the context of a logical or mathematical argument.


A constant can be considered as an operator which takes no operands.

A constant can also be considered as a variable whose domain is a singleton.",Definition:Constant,,false,"A constant is a name for an object (usually a number, but the concept has wider applications) which does not change during the context of a logical or mathematical argument.


A constant can be considered as an operator which takes no operands.

A constant can also be considered as a variable whose domain is a singleton.",Absolute
"['Definitions/Absolute Value Function', 'Definitions/Field Theory', 'Definitions/Algebra', 'Definitions/Real Analysis']",Definition:Absolute,"=== Definition 1 ===
Let $x in mathbb R$ be a real number.


The absolute value of $x$ is denoted $leftlvert x rightrvert$, and is defined using the usual ordering on the real numbers as follows:
:$leftlvert x rightrvert = begin{cases} x & : x > 0 \ 0 & : x = 0 \ -x & : x < 0 end{cases}$

=== Definition 2 ===
Let $x in mathbb R$ be a real number.

The absolute value of $x$ is denoted $leftlvert x rightrvert$, and is defined as:

:$leftlvert x rightrvert = +sqrt {x^2}$

where $+sqrt {x^2}$ is the positive square root of $x^2$.",Definition:Absolute Value,,false,"=== Definition 1 ===
Let x ∈ℝ be a real number.


The absolute value of x is denoted | x |, and is defined using the usual ordering on the real numbers as follows:
:| x | =  x     : x > 0 
 0     : x = 0 
 -x     : x < 0

=== Definition 2 ===
Let x ∈ℝ be a real number.

The absolute value of x is denoted | x |, and is defined as:

:| x | = +√(x^2)

where +√(x^2) is the positive square root of x^2.",Absolute
"['Definitions/Absolute Continuity', 'Definitions/Continuity']",Definition:Absolute,"=== Real Function ===
Let $I subseteq mathbb R$ be a real interval.


A real function $f: I to mathbb R$ is said to be absolutely continuous  if and only if  it satisfies the following property:

:For every $epsilon > 0$ there exists $delta > 0$ such that the following property holds:

::For every finite set of pairwise disjoint closed real intervals $left[ a_1 ,.,.,   right]{b_1}, dotsc, left[ a_n ,.,.,   right]{b_n} subseteq I$ such that:
:::$ds sum_{i mathop = 1}^n leftlvert b_i - a_i rightrvert < delta$
::it holds that:
:::$ds sum_{i mathop = 1}^n leftlvert f left(   right){b_i} - f left(   right){a_i}  rightrvert < epsilon$

=== Measure ===
Let $left( X, unicode{x3a3} right)$ be a measurable space.

Let $mu$ and $nu$ be measures on $left( X, unicode{x3a3} right)$.


We say that $nu$ is absolutely continuous with respect to $mu$ and write: 

:$nu ll mu$

 if and only if :

:$ds forall A in unicode{x3a3} : mu left(   right)A = 0 implies nu left(   right)A = 0$

=== Signed Measure ===
Let $left( X, unicode{x3a3} right)$ be a measurable space.

Let $mu$ be a measure on $left( X, unicode{x3a3} right)$.

Let $nu$ be a signed measure on $left( X, unicode{x3a3} right)$.

Let $leftlvert nu rightrvert$ be the variation of $nu$.


We say that $nu$ is absolutely continuous with respect to $mu$  if and only if :

:$leftlvert nu rightrvert$ is absolutely continuous with respect to $mu$.

We write:

:$nu ll mu$

=== Complex Measure ===
Let $left( X, unicode{x3a3} right)$ be a measurable space.

Let $mu$ be a measure on $left( X, unicode{x3a3} right)$.

Let $nu$ be a complex measure on $left( X, unicode{x3a3} right)$.

Let $leftlvert nu rightrvert$ be the variation of $nu$.


We say that $nu$ is absolutely continuous with respect to $mu$  if and only if :

:$leftlvert nu rightrvert$ is absolutely continuous with respect to $mu$.

We write:

:$nu ll mu$",Definition:Absolute Continuity,,false,"=== Real Function ===
Let I ⊆ℝ be a real interval.


A real function f: I →ℝ is said to be absolutely continuous  if and only if  it satisfies the following property:

:For every ϵ > 0 there exists δ > 0 such that the following property holds:

::For every finite set of pairwise disjoint closed real intervals [ a_1  . . ]b_1, …, [ a_n  . . ]b_n⊆ I such that:
:::∑_i  = 1^n | b_i - a_i | < δ
::it holds that:
:::∑_i  = 1^n | f (   )b_i - f (   )a_i| < ϵ

=== Measure ===
Let ( X, x3a3) be a measurable space.

Let μ and ν be measures on ( X, x3a3).


We say that ν is absolutely continuous with respect to μ and write: 

:ν≪μ

 if and only if :

:∀ A ∈x3a3 : μ(   )A = 0 ν(   )A = 0

=== Signed Measure ===
Let ( X, x3a3) be a measurable space.

Let μ be a measure on ( X, x3a3).

Let ν be a signed measure on ( X, x3a3).

Let |ν| be the variation of ν.


We say that ν is absolutely continuous with respect to μ  if and only if :

:|ν| is absolutely continuous with respect to μ.

We write:

:ν≪μ

=== Complex Measure ===
Let ( X, x3a3) be a measurable space.

Let μ be a measure on ( X, x3a3).

Let ν be a complex measure on ( X, x3a3).

Let |ν| be the variation of ν.


We say that ν is absolutely continuous with respect to μ  if and only if :

:|ν| is absolutely continuous with respect to μ.

We write:

:ν≪μ",Absolute
['Definitions/Symmetric Functions'],Definition:Absolute,"Let $f: mathbb R^n to mathbb R$ be a real-valued function.

Let $f$ be such that, for all $mathbf x := left( x_1, x_2, ldots, x_n right) in mathbb R^n$:
:$f left(   right){mathbf x} = f left(   right){mathbf y}$
where $mathbf y$ is a permutation of $left( x_1, x_2, ldots, x_n right)$.

Then $f$ is an absolutely symmetric function.",Definition:Symmetric Function/Absolute,,false,"Let f: ℝ^n →ℝ be a real-valued function.

Let f be such that, for all 𝐱 := ( x_1, x_2, …, x_n ) ∈ℝ^n:
:f (   )𝐱 = f (   )𝐲
where 𝐲 is a permutation of ( x_1, x_2, …, x_n ).

Then f is an absolutely symmetric function.",Absolute
"['Definitions/Absolute Galois Groups', 'Definitions/Galois Groups of Field Extensions', 'Definitions/Galois Theory']",Definition:Absolute,"Let $K$ be a field.


=== Definition 1 ===
Let $K$ be a field.


The absolute Galois group of $K$ is the Galois group $mathrm {Gal} left( K^{operatorname{sep} } mid K right)$ of its separable closure.

=== Definition 2 ===
Let $K$ be a field.


The absolute Galois group of $K$ is the automorphism group $mathrm {Aut} left( overline K mid K right)$ of its algebraic closure.",Definition:Absolute Galois Group,,false,"Let K be a field.


=== Definition 1 ===
Let K be a field.


The absolute Galois group of K is the Galois group Gal( K^sep| K ) of its separable closure.

=== Definition 2 ===
Let K be a field.


The absolute Galois group of K is the automorphism group Aut( K| K ) of its algebraic closure.",Absolute
"['Definitions/Group Theory', 'Definitions/Generalized Sums']",Definition:Absolute,"Let $V$ be a Banach space.

Let $leftlangle v_i rightrangle_{i mathop in I}$ be an indexed family of elements of $V$.


Then $ds sum leftlbrace v_i: i in I rightrbrace$ converges absolutely  if and only if  $ds sum leftlbrace leftlVert v_i rightrVert: i mathop in I rightrbrace$ converges.

This nomenclature is appropriate as we have Absolutely Convergent Generalized Sum Converges.",Definition:Generalized Sum/Absolute Net Convergence,,false,"Let V be a Banach space.

Let ⟨ v_i ⟩_i ∈ I be an indexed family of elements of V.


Then ∑{ v_i: i ∈ I } converges absolutely  if and only if  ∑{‖ v_i ‖: i ∈ I } converges.

This nomenclature is appropriate as we have Absolutely Convergent Generalized Sum Converges.",Absolute
['Definitions/Topology'],Definition:Absolute,"Let $T_1 = left( S_1, tau_1 right)$ be a topological space.

Let $T_2 = left( S_2, tau_2 right)$ be a topological subspace of $T_1$.


Let $T_2$ be a retract of $T_1$.


$T_2$ is an absolute retract of $T_1$  if and only if :
:for every closed subspace $B$ of a $T_4$ space $T$ such that $B$ is homeomorphic to $A$, then $B$ is a retract of $T$.",Definition:Retract (Topology)/Absolute,,false,"Let T_1 = ( S_1, τ_1 ) be a topological space.

Let T_2 = ( S_2, τ_2 ) be a topological subspace of T_1.


Let T_2 be a retract of T_1.


T_2 is an absolute retract of T_1  if and only if :
:for every closed subspace B of a T_4 space T such that B is homeomorphic to A, then B is a retract of T.",Absolute
['Definitions/Errors'],Definition:Absolute,"Let $x_0$ be an approximation to a (true) value $x$.


The absolute error of $x_0$ in $x$ is defined as:

:$Delta x := x_0 - x$


=== Correction ===
Let $x_0$ be an approximation to a (true) value $x$.

Let $Delta x$ denote the absolute error of $x_0$.


The correction to $x$ is the quantity that needs to be added to $x_0$ to change it to $x$.

That is, the correction to $x$ is defined as $-Delta x$.",Definition:Error/Absolute,,false,"Let x_0 be an approximation to a (true) value x.


The absolute error of x_0 in x is defined as:

:Δ x := x_0 - x


=== Correction ===
Let x_0 be an approximation to a (true) value x.

Let Δ x denote the absolute error of x_0.


The correction to x is the quantity that needs to be added to x_0 to change it to x.

That is, the correction to x is defined as -Δ x.",Absolute
"['Definitions/Frequency (Descriptive Statistics)', 'Definitions/Class Intervals', 'Definitions/Qualitative Variables', 'Definitions/Descriptive Statistics']",Definition:Absolute,"Let $S$ be a sample or a population.

Let $omega$ be a qualitative variable, or a class interval of a quantitative variable.


The frequency of $omega$ is the number of individuals in $S$ satisfying $omega$.",Definition:Frequency (Descriptive Statistics),,false,"Let S be a sample or a population.

Let ω be a qualitative variable, or a class interval of a quantitative variable.


The frequency of ω is the number of individuals in S satisfying ω.",Absolute
['Definitions/Dispersion (Statistics)'],Definition:Absolute,An absolute measure of dispersion is a measure of dispersion that indicates how spread out or scattered a set of observations with respect to the actual values of those observations.,Definition:Absolute Measure of Dispersion,,false,An absolute measure of dispersion is a measure of dispersion that indicates how spread out or scattered a set of observations with respect to the actual values of those observations.,Absolute
"['Definitions/Absolute Geometry', 'Definitions/Geometry', 'Definitions/Branches of Mathematics']",Definition:Absolute,Absolute geometry is the study of Euclidean geometry without the parallel postulate.,Definition:Absolute Geometry,geometry,true,Absolute geometry is the study of Euclidean geometry without the parallel postulate.,Absolute
"['Definitions/Absolutely Normal Numbers', 'Definitions/Normal Numbers', 'Definitions/Numbers']",Definition:Absolute,"A real number $r$ is absolutely normal if it is normal   every number base $b$.

That is,  if and only if  its basis expansion in every number base $b$ is such that:
:no finite sequence of digits of $r$ of length $n$ occurs more frequently than any other such finite sequence of length $n$.


In particular, for every number base $b$, all digits of $r$ have the same natural density in the basis expansion of $r$.",Definition:Absolutely Normal Number,,false,"A real number r is absolutely normal if it is normal   every number base b.

That is,  if and only if  its basis expansion in every number base b is such that:
:no finite sequence of digits of r of length n occurs more frequently than any other such finite sequence of length n.


In particular, for every number base b, all digits of r have the same natural density in the basis expansion of r.",Absolute
['Definitions/Temperature'],Definition:Absolute,"Absolute temperature is a measure of the amount of heat energy in a body.

It is defined as:
:$T = dfrac 1 k left( dfrac {partial U} {partial ln g}  right)$
where:
:$k$ is a constant that relates the mean kinetic energy and absolute temperature of the body $B$
:$U$ is the total energy of $B$
:$g$ is the number of possible states in which $B$ can be.


=== Dimension ===
Temperature is frequently, at elementary levels at least, considered as one of the fundamental dimensions of physics.

In dimensional analysis it is usually assigned the symbol $Theta$.


Category:Definitions/Dimensions of Measurement
Category:Definitions/Temperature",Definition:Temperature/Absolute,measure,true,"Absolute temperature is a measure of the amount of heat energy in a body.

It is defined as:
:T =  1 k ( ∂ U∂ln g)
where:
:k is a constant that relates the mean kinetic energy and absolute temperature of the body B
:U is the total energy of B
:g is the number of possible states in which B can be.


=== Dimension ===
Temperature is frequently, at elementary levels at least, considered as one of the fundamental dimensions of physics.

In dimensional analysis it is usually assigned the symbol Θ.


Category:Definitions/Dimensions of Measurement
Category:Definitions/Temperature",Absolute
['Definitions/Temperature'],Definition:Absolute,"Absolute zero is the lowest temperature which can theoretically be achieved.

It is the temperature where all motion due to thermal effects stops.

Before that temperature can be reached, quantum effects come into play.",Definition:Absolute Zero,,false,"Absolute zero is the lowest temperature which can theoretically be achieved.

It is the temperature where all motion due to thermal effects stops.

Before that temperature can be reached, quantum effects come into play.",Absolute
['Definitions/Arrays'],Definition:Absolute,"Let $mathbf A$ be an array.


When referring to a specific element of $mathbf A$ directly by its indices $left( i, j right)$, those indices can be referred to as the absolute address of that element.

This terminology is most often seen in the context of computer spreadsheet programs.",Definition:Array/Element/Absolute Address,,false,"Let 𝐀 be an array.


When referring to a specific element of 𝐀 directly by its indices ( i, j ), those indices can be referred to as the absolute address of that element.

This terminology is most often seen in the context of computer spreadsheet programs.",Absolute
['Definitions/Force'],Definition:Action,"Let $B_1$ and $B_2$ be bodies in space.

Let $B_1$ apply a force $mathbf F$ on $B_2$.

The force $mathbf F$ is known as the action that $B_1$ applies to $B_2$.",Definition:Action (Physics),,false,"Let B_1 and B_2 be bodies in space.

Let B_1 apply a force 𝐅 on B_2.

The force 𝐅 is known as the action that B_1 applies to B_2.",Action
['Definitions/Dynamics'],Definition:Action,"The action for a given segment from point $A$ to point $B$ on the trajectory of a dynamical system is defined by the line integral:

:$ds sum_i int_A^B p_i ,mathrm d q_i$

where:
:$q_i$ are the generalized coordinates
:$p_i$ are the generalized momenta.",Definition:Action (Dynamics),,false,"The action for a given segment from point A to point B on the trajectory of a dynamical system is defined by the line integral:

:∑_i ∫_A^B p_i  d q_i

where:
:q_i are the generalized coordinates
:p_i are the generalized momenta.",Action
"['Definitions/Hamiltonian Mechanics', 'Definitions/Dimensions of Measurement']",Definition:Action,"The action applied by a system from state $1$ to state $2$ is defined as the definite integral of the Lagrangian over time from state $1$ to state $2$:

:$ds S_{12} = int_{t_1}^{t_2} mathcal L ,mathrm d t$
where:
:$S_{12}$ is the action from $1$ to $2$
:$t$ is time
:$mathcal L$ is the Lagrangian.",Definition:Action Applied by System,,false,"The action applied by a system from state 1 to state 2 is defined as the definite integral of the Lagrangian over time from state 1 to state 2:

:S_12 = ∫_t_1^t_2ℒ d t
where:
:S_12 is the action from 1 to 2
:t is time
:ℒ is the Lagrangian.",Action
['Definitions/Group Actions'],Definition:Action,"Let $X$ be a set.

Let $left( G, circ right)$ be a group whose identity is $e$.


=== Left Group Action ===
Let $X$ be a set.

Let $left( G, circ right)$ be a group whose identity is $e$.


A (left) group action is an operation $phi: G times X to X$ such that:

:$forall left( g, x right) in G times X: g * x := phi left(   right){g, x} in X$

in such a way that the group action axioms are satisfied:
 

=== Right Group Action ===
Let $X$ be a set.

Let $left( G, circ right)$ be a group whose identity is $e$.


A right group action is a mapping $phi: X times G to X$ such that:

:$forall left( x, g right) in X times G : x * g := phi left(   right){x, g} in X$

in such a way that the right group action axioms are satisfied:
 

The group $G$ thus acts on the set $X$.

The group $G$ can be referred to as the group of transformations, or a transformation group.


=== From Permutation Representation ===
Let $G$ be a group.

Let $X$ be a set.

Let $left( Gamma left(   right)X, circ right)$ be the symmetric group on $X$.

Let $rho: G to left( Gamma left(   right)X, circ right)$ be a permutation representation.


The group action of $G$ associated to the permutation representation $rho$ is the group action $phi: G times X to X$ defined by:
:$phi left(   right){g, x} = rho_g left(   right)x$

where $rho_g : X to X$ is the permutation representation associated to $rho$ for $g in G$ by $rho_g left(   right)x = phi left(   right){g, x}$.",Definition:Group Action,,false,"Let X be a set.

Let ( G, ∘) be a group whose identity is e.


=== Left Group Action ===
Let X be a set.

Let ( G, ∘) be a group whose identity is e.


A (left) group action is an operation ϕ: G × X → X such that:

:∀( g, x ) ∈ G × X: g * x := ϕ(   )g, x∈ X

in such a way that the group action axioms are satisfied:
 

=== Right Group Action ===
Let X be a set.

Let ( G, ∘) be a group whose identity is e.


A right group action is a mapping ϕ: X × G → X such that:

:∀( x, g ) ∈ X × G : x * g := ϕ(   )x, g∈ X

in such a way that the right group action axioms are satisfied:
 

The group G thus acts on the set X.

The group G can be referred to as the group of transformations, or a transformation group.


=== From Permutation Representation ===
Let G be a group.

Let X be a set.

Let ( Γ(   )X, ∘) be the symmetric group on X.

Let ρ: G →( Γ(   )X, ∘) be a permutation representation.


The group action of G associated to the permutation representation ρ is the group action ϕ: G × X → X defined by:
:ϕ(   )g, x = ρ_g (   )x

where ρ_g : X → X is the permutation representation associated to ρ for g ∈ G by ρ_g (   )x = ϕ(   )g, x.",Action
"['Definitions/Module Theory', 'Definitions/Linear Ring Actions']",Definition:Action,"Let $R$ be a ring.

Let $M$ be an abelian group.


=== Left Ring Action ===
Let $R$ be a ring.

Let $M$ be an abelian group.

Let $circ : R times M to M$ be a mapping from the cartesian product $R times M$.


$circ$ is a left linear ring action of $R$ on $M$  if and only if  $circ$ satisfies the left ring action axioms:
 

=== Right Ring Action ===
Let $R$ be a ring.

Let $M$ be an abelian group.

Let $circ : M times R to M$ be a mapping from the cartesian product $M times R$.


$circ$ is a right linear ring action of $R$ on $M$  if and only if  $circ$ satisfies the right ring action axioms:
 ",Definition:Linear Ring Action,,false,"Let R be a ring.

Let M be an abelian group.


=== Left Ring Action ===
Let R be a ring.

Let M be an abelian group.

Let ∘ : R × M → M be a mapping from the cartesian product R × M.


∘ is a left linear ring action of R on M  if and only if  ∘ satisfies the left ring action axioms:
 

=== Right Ring Action ===
Let R be a ring.

Let M be an abelian group.

Let ∘ : M × R → M be a mapping from the cartesian product M × R.


∘ is a right linear ring action of R on M  if and only if  ∘ satisfies the right ring action axioms:
 ",Action
['Definitions/Homological Algebra'],Definition:Acyclic,"Let $mathbf A$ be an abelian category with enough injectives.

Let $mathbf B$ be an abelian category.

Let $F : mathbf A to mathbf B$ be a left exact functor.

Let $X$ be an object of $mathbf A$.


Then $X$ is $F$-acyclic  if and only if  $mathrm R^i F left(   right)X = 0$ for all positive integers $i in mathbb Z_{i mathop ge 1}$.

In the above $mathrm R^i F$ denotes the $i$-th right derived functor of $F$.


Category:Definitions/Homological Algebra",Definition:Acyclic Object,,false,"Let 𝐀 be an abelian category with enough injectives.

Let 𝐁 be an abelian category.

Let F : 𝐀→𝐁 be a left exact functor.

Let X be an object of 𝐀.


Then X is F-acyclic  if and only if  R^i F (   )X = 0 for all positive integers i ∈ℤ_i ≥ 1.

In the above R^i F denotes the i-th right derived functor of F.


Category:Definitions/Homological Algebra",Acyclic
['Definitions/Homological Algebra'],Definition:Acyclic,"Let $mathbf A$ be an abelian category with enough injectives.

Let $mathbf B$ be an arbitrary abelian category.

Let $F : mathbf A to mathbf B$ be a left-exact functor.

Let $X$ be an object in $mathbf A$.


An $F$-acyclic resolution of $X$ is a cochain complex $P := leftlangle d^i : I^i to I^{i + 1}  rightrangle_{i mathop in mathbb Z}$ in $mathbf A$, such that:
:$(1): quad forall i < 0 : I^i = 0$
:$(2): quad I^i$ is $F$-acyclic for all $i ge 0$
together with a morphism $varepsilon : X to I^0$, such that the cochain complex:
:$begin{xy}
xymatrix{
dots ar[r] & 0 ar[r] & 0 ar[r] & X ar[r]^{varepsilon} & I^0 ar[r]^{d^0} & I^1 ar[r]^{d^1} & I^2 ar[r]^{d^2} & dots
}
end{xy}$
is exact.",Definition:Acyclic Resolution,,false,"Let 𝐀 be an abelian category with enough injectives.

Let 𝐁 be an arbitrary abelian category.

Let F : 𝐀→𝐁 be a left-exact functor.

Let X be an object in 𝐀.


An F-acyclic resolution of X is a cochain complex P := ⟨ d^i : I^i → I^i + 1⟩_i ∈ℤ in 𝐀, such that:
:(1):   ∀ i < 0 : I^i = 0
:(2):    I^i is F-acyclic for all i ≥ 0
together with a morphism ε : X → I^0, such that the cochain complex:
:…[r]     0 [r]     0 [r]     X [r]^ε    I^0 [r]^d^0    I^1 [r]^d^1    I^2 [r]^d^2   …
is exact.",Acyclic
['Definitions/Graph Theory'],Definition:Acyclic,An acyclic graph is a graph or digraph with no cycles.,Definition:Acyclic Graph,,false,An acyclic graph is a graph or digraph with no cycles.,Acyclic
"['Definitions/Homological Algebra', 'Definitions/Sheaf Theory']",Definition:Acyclic,"Let $X$ be a topological space.

Let $mathcal F$ be an abelian sheaf on $X$.

Let $Gamma left(   right){X, -} : mathbf {Ab}  left(   right)X to mathbf {Ab}$ be the global sections functor on $X$.


Then $mathcal F$ is acyclic  if and only if  $mathcal F$ is $Gamma left(   right){X, -}$-acyclic in the category of abelian sheaves $mathbf {Ab}  left(   right)X$ on $X$.",Definition:Acyclic Sheaf,,false,"Let X be a topological space.

Let ℱ be an abelian sheaf on X.

Let Γ(   )X, - : 𝐀𝐛(   )X →𝐀𝐛 be the global sections functor on X.


Then ℱ is acyclic  if and only if  ℱ is Γ(   )X, --acyclic in the category of abelian sheaves 𝐀𝐛(   )X on X.",Acyclic
"['Definitions/Analysis', 'Definitions/Abstract Algebra', 'Definitions/Linear Algebra']",Definition:Additive Function,"Let $f: S to S$ be a mapping on an algebraic structure $left( S, + right)$.


Then $f$ is an additive function  if and only if  it preserves the addition operation:
:$forall x, y in S: f left(   right){x + y} = f left(   right)x + f left(   right)y$",Definition:Additive Function (Conventional),,false,"Let f: S → S be a mapping on an algebraic structure ( S, + ).


Then f is an additive function  if and only if  it preserves the addition operation:
:∀ x, y ∈ S: f (   )x + y = f (   )x + f (   )y",Additive Function
['Definitions/Ring Theory'],Definition:Additive Function,"Let $R$ be a unique factorization domain.

Let $f : R to mathbb C$ be a complex-valued function.


Then $f$ is additive  if and only if :
:For all coprime $x, y in R$: $f left(   right){x y} = f left(   right)x + f left(   right)y$


=== Arithmetic Function ===
Let $f : mathbb N to mathbb C$ be an arithmetic function.


Then $f$ is additive  if and only if :
:$m perp n implies f left(   right){m n} = f left(   right)m + f left(   right)n$",Definition:Additive Function on UFD,,false,"Let R be a unique factorization domain.

Let f : R →ℂ be a complex-valued function.


Then f is additive  if and only if :
:For all coprime x, y ∈ R: f (   )x y = f (   )x + f (   )y


=== Arithmetic Function ===
Let f : ℕ→ℂ be an arithmetic function.


Then f is additive  if and only if :
:m ⊥ n  f (   )m n = f (   )m + f (   )n",Additive Function
"['Definitions/Ring Theory', 'Definitions/Number Theory']",Definition:Additive Function,"Let $left( R, +, times right)$ be a ring.

Let $f: R to R$ be a mapping on $R$.


Then $f$ is described as completely additive  if and only if :

:$forall m, n in R: f left(   right){m times n} = f left(   right)m + f left(   right)n$


That is, a completely additive function is one where the value of a product of two numbers equals the sum of the value of each one individually.",Definition:Completely Additive Function,,false,"Let ( R, +, ×) be a ring.

Let f: R → R be a mapping on R.


Then f is described as completely additive  if and only if :

:∀ m, n ∈ R: f (   )m × n = f (   )m + f (   )n


That is, a completely additive function is one where the value of a product of two numbers equals the sum of the value of each one individually.",Additive Function
['Definitions/Measure Theory'],Definition:Additive Function,"Let $mathcal S$ be an algebra of sets.

Let $f: mathcal S to overline mathbb R$ be a function, where $overline mathbb R$ denotes the set of extended real numbers.


Then $f$ is defined to be additive  if and only if :

:$forall S, T in mathcal S: S cap T = varnothing implies f left(   right){S cup T} = f left(   right)S + f left(   right)T$

That is, for any two disjoint elements of $mathcal S$, $f$ of their union equals the sum of $f$ of the individual elements.


Note from Finite Union of Sets in Additive Function that:

:$ds f left(   right){bigcup_{i mathop = 1}^n S_i} = sum_{i mathop = 1}^n f left(   right){S_i}$

where $S_1, S_2, ldots, S_n$ is any finite collection of pairwise disjoint elements of $mathcal S$.",Definition:Additive Function (Measure Theory),,false,"Let 𝒮 be an algebra of sets.

Let f: 𝒮→R be a function, where R denotes the set of extended real numbers.


Then f is defined to be additive  if and only if :

:∀ S, T ∈𝒮: S ∩ T = ∅ f (   )S ∪ T = f (   )S + f (   )T

That is, for any two disjoint elements of 𝒮, f of their union equals the sum of f of the individual elements.


Note from Finite Union of Sets in Additive Function that:

:f (   )⋃_i  = 1^n S_i = ∑_i  = 1^n f (   )S_i

where S_1, S_2, …, S_n is any finite collection of pairwise disjoint elements of 𝒮.",Additive Function
"['Definitions/Set Systems', 'Definitions/Measure Theory']",Definition:Additive Function,"Let $unicode{x3a3}$ be a $sigma$-algebra.

Let $f: unicode{x3a3} to overline mathbb R$ be a function, where $overline mathbb R$ denotes the set of extended real numbers.


Then $f$ is defined as countably additive  if and only if :
:$ds f left(   right){bigcup_{n mathop in mathbb N} E_n} = sum_{n mathop in mathbb N} f left(   right){E_n}$

where $leftlangle E_n rightrangle$ is any sequence of pairwise disjoint elements of $unicode{x3a3}$.


That is, for any countably infinite set of pairwise disjoint elements of $unicode{x3a3}$, $f$ of their union equals the sum of $f$ of the individual elements.",Definition:Countably Additive Function,,false,"Let x3a3 be a σ-algebra.

Let f: x3a3→R be a function, where R denotes the set of extended real numbers.


Then f is defined as countably additive  if and only if :
:f (   )⋃_n ∈ℕ E_n = ∑_n ∈ℕ f (   )E_n

where ⟨ E_n ⟩ is any sequence of pairwise disjoint elements of x3a3.


That is, for any countably infinite set of pairwise disjoint elements of x3a3, f of their union equals the sum of f of the individual elements.",Additive Function
"['Definitions/Adjacency (Graph Theory)', 'Definitions/Graph Theory']",Definition:Adjacent,"=== Vertices ===
=== Undirected Graph ===
Let $G = left( V, E right)$ be an undirected graph.

Two vertices $u, v in V$ of $G$ are adjacent  if and only if  there exists an edge $e = leftlbrace u, v rightrbrace in E$ of $G$ to which they are both incident.


Otherwise they are non-adjacent.

=== Digraph ===
Let $G = left( V, E right)$ be a digraph.

Two vertices $u, v in V$ of $G$ are adjacent  if and only if  there exists an arc $e = left( u, v right) in E$ of $G$ to which they are both incident.


Otherwise they are non-adjacent.

=== Non-Adjacent ===
Let $G = left( V, E right)$ be a graph.

Two vertices $u, v in V$ of $G$ are non-adjacent  if and only if  they are not adjacent.

=== Edges ===
=== Undirected Graph ===
Let $G = left( V, E right)$ be an undirected graph.

Two edges $e_1, e_2 in E$ of $G$ adjacent  if and only if  there exists a vertex $v in V$ to which they are both incident.


Otherwise they are non-adjacent.

=== Digraph ===
Let $G = left( V, E right)$ be a digraph.

Two arcs $e_1, e_2 in E$ of $G$ adjacent  if and only if  there exists a vertex $v in V$ to which they are both incident.


Otherwise they are non-adjacent.

=== Non-Adjacent ===
Let $G = left( V, E right)$ be a graph.

Two edges $u, v in V$ of $G$ are non-adjacent  if and only if  they are not adjacent.

=== Faces ===
:

Let $G = left( V, E right)$ be a planar graph.

Two faces of $G$ are adjacent  if and only if  they are both incident to the same edge (or edges).

In the above diagram, $BCEF$ and $ABF$ are adjacent, but $BCEF$ and $AFG$ are not adjacent.


Note that faces which are both incident to the same vertex are not considered adjacent unless they are also both incident to the same edge.",Definition:Adjacent (Graph Theory),,false,"=== Vertices ===
=== Undirected Graph ===
Let G = ( V, E ) be an undirected graph.

Two vertices u, v ∈ V of G are adjacent  if and only if  there exists an edge e = { u, v }∈ E of G to which they are both incident.


Otherwise they are non-adjacent.

=== Digraph ===
Let G = ( V, E ) be a digraph.

Two vertices u, v ∈ V of G are adjacent  if and only if  there exists an arc e = ( u, v ) ∈ E of G to which they are both incident.


Otherwise they are non-adjacent.

=== Non-Adjacent ===
Let G = ( V, E ) be a graph.

Two vertices u, v ∈ V of G are non-adjacent  if and only if  they are not adjacent.

=== Edges ===
=== Undirected Graph ===
Let G = ( V, E ) be an undirected graph.

Two edges e_1, e_2 ∈ E of G adjacent  if and only if  there exists a vertex v ∈ V to which they are both incident.


Otherwise they are non-adjacent.

=== Digraph ===
Let G = ( V, E ) be a digraph.

Two arcs e_1, e_2 ∈ E of G adjacent  if and only if  there exists a vertex v ∈ V to which they are both incident.


Otherwise they are non-adjacent.

=== Non-Adjacent ===
Let G = ( V, E ) be a graph.

Two edges u, v ∈ V of G are non-adjacent  if and only if  they are not adjacent.

=== Faces ===
:

Let G = ( V, E ) be a planar graph.

Two faces of G are adjacent  if and only if  they are both incident to the same edge (or edges).

In the above diagram, BCEF and ABF are adjacent, but BCEF and AFG are not adjacent.


Note that faces which are both incident to the same vertex are not considered adjacent unless they are also both incident to the same edge.",Adjacent
"['Definitions/Adjacency (Graph Theory)', 'Definitions/Vertices of Graphs']",Definition:Adjacent,"=== Undirected Graph ===
Let $G = left( V, E right)$ be an undirected graph.

Two vertices $u, v in V$ of $G$ are adjacent  if and only if  there exists an edge $e = leftlbrace u, v rightrbrace in E$ of $G$ to which they are both incident.


Otherwise they are non-adjacent.

=== Digraph ===
Let $G = left( V, E right)$ be a digraph.

Two vertices $u, v in V$ of $G$ are adjacent  if and only if  there exists an arc $e = left( u, v right) in E$ of $G$ to which they are both incident.


Otherwise they are non-adjacent.

=== Non-Adjacent ===
Let $G = left( V, E right)$ be a graph.

Two vertices $u, v in V$ of $G$ are non-adjacent  if and only if  they are not adjacent.",Definition:Adjacent (Graph Theory)/Vertices,,false,"=== Undirected Graph ===
Let G = ( V, E ) be an undirected graph.

Two vertices u, v ∈ V of G are adjacent  if and only if  there exists an edge e = { u, v }∈ E of G to which they are both incident.


Otherwise they are non-adjacent.

=== Digraph ===
Let G = ( V, E ) be a digraph.

Two vertices u, v ∈ V of G are adjacent  if and only if  there exists an arc e = ( u, v ) ∈ E of G to which they are both incident.


Otherwise they are non-adjacent.

=== Non-Adjacent ===
Let G = ( V, E ) be a graph.

Two vertices u, v ∈ V of G are non-adjacent  if and only if  they are not adjacent.",Adjacent
"['Definitions/Edges of Graphs', 'Definitions/Adjacency (Graph Theory)']",Definition:Adjacent,"=== Undirected Graph ===
Let $G = left( V, E right)$ be an undirected graph.

Two edges $e_1, e_2 in E$ of $G$ adjacent  if and only if  there exists a vertex $v in V$ to which they are both incident.


Otherwise they are non-adjacent.

=== Digraph ===
Let $G = left( V, E right)$ be a digraph.

Two arcs $e_1, e_2 in E$ of $G$ adjacent  if and only if  there exists a vertex $v in V$ to which they are both incident.


Otherwise they are non-adjacent.

=== Non-Adjacent ===
Let $G = left( V, E right)$ be a graph.

Two edges $u, v in V$ of $G$ are non-adjacent  if and only if  they are not adjacent.",Definition:Adjacent (Graph Theory)/Edges,,false,"=== Undirected Graph ===
Let G = ( V, E ) be an undirected graph.

Two edges e_1, e_2 ∈ E of G adjacent  if and only if  there exists a vertex v ∈ V to which they are both incident.


Otherwise they are non-adjacent.

=== Digraph ===
Let G = ( V, E ) be a digraph.

Two arcs e_1, e_2 ∈ E of G adjacent  if and only if  there exists a vertex v ∈ V to which they are both incident.


Otherwise they are non-adjacent.

=== Non-Adjacent ===
Let G = ( V, E ) be a graph.

Two edges u, v ∈ V of G are non-adjacent  if and only if  they are not adjacent.",Adjacent
"['Definitions/Adjacency (Graph Theory)', 'Definitions/Faces of Graphs']",Definition:Adjacent,":

Let $G = left( V, E right)$ be a planar graph.

Two faces of $G$ are adjacent  if and only if  they are both incident to the same edge (or edges).

In the above diagram, $BCEF$ and $ABF$ are adjacent, but $BCEF$ and $AFG$ are not adjacent.


Note that faces which are both incident to the same vertex are not considered adjacent unless they are also both incident to the same edge.",Definition:Adjacent (Graph Theory)/Faces,,false,":

Let G = ( V, E ) be a planar graph.

Two faces of G are adjacent  if and only if  they are both incident to the same edge (or edges).

In the above diagram, BCEF and ABF are adjacent, but BCEF and AFG are not adjacent.


Note that faces which are both incident to the same vertex are not considered adjacent unless they are also both incident to the same edge.",Adjacent
['Definitions/Angles'],Definition:Adjacent,"Two angles are adjacent if they have an intersecting line in common:

:",Definition:Angle/Adjacent,,false,"Two angles are adjacent if they have an intersecting line in common:

:",Adjacent
"['Definitions/Adjacent (Polygons)', 'Definitions/Polygons']",Definition:Adjacent,"=== Adjacent Side to Vertex ===
Each vertex of a polygon is formed by the intersection of two sides.

The two sides that form a particular vertex are referred to as the adjacents of that vertex, or described as adjacent to that vertex.

=== Adjacent Sides ===
Two sides of a polygon that meet at the same vertex are adjacent to each other.

=== Adjacent Vertex to Side ===
Each side of a polygon intersects two other sides, and so is terminated at either endpoint by two vertices.

The two vertices that terminate a particular side are referred to as the adjacents of that side, or described as adjacent to that side.

=== Adjacent Vertices ===
Each side of a polygon intersects two other sides, and so is terminated at either endpoint by two vertices.


Those two vertices are described as adjacent to each other.",Definition:Polygon/Adjacent,,false,"=== Adjacent Side to Vertex ===
Each vertex of a polygon is formed by the intersection of two sides.

The two sides that form a particular vertex are referred to as the adjacents of that vertex, or described as adjacent to that vertex.

=== Adjacent Sides ===
Two sides of a polygon that meet at the same vertex are adjacent to each other.

=== Adjacent Vertex to Side ===
Each side of a polygon intersects two other sides, and so is terminated at either endpoint by two vertices.

The two vertices that terminate a particular side are referred to as the adjacents of that side, or described as adjacent to that side.

=== Adjacent Vertices ===
Each side of a polygon intersects two other sides, and so is terminated at either endpoint by two vertices.


Those two vertices are described as adjacent to each other.",Adjacent
['Definitions/Adjacent (Polygons)'],Definition:Adjacent,"Each vertex of a polygon is formed by the intersection of two sides.

The two sides that form a particular vertex are referred to as the adjacents of that vertex, or described as adjacent to that vertex.",Definition:Polygon/Adjacent/Side to Vertex,,false,"Each vertex of a polygon is formed by the intersection of two sides.

The two sides that form a particular vertex are referred to as the adjacents of that vertex, or described as adjacent to that vertex.",Adjacent
['Definitions/Adjacent (Polygons)'],Definition:Adjacent,"Each side of a polygon intersects two other sides, and so is terminated at either endpoint by two vertices.

The two vertices that terminate a particular side are referred to as the adjacents of that side, or described as adjacent to that side.",Definition:Polygon/Adjacent/Vertex to Side,,false,"Each side of a polygon intersects two other sides, and so is terminated at either endpoint by two vertices.

The two vertices that terminate a particular side are referred to as the adjacents of that side, or described as adjacent to that side.",Adjacent
['Definitions/Adjacent (Polygons)'],Definition:Adjacent,Two sides of a polygon that meet at the same vertex are adjacent to each other.,Definition:Polygon/Adjacent/Sides,,false,Two sides of a polygon that meet at the same vertex are adjacent to each other.,Adjacent
['Definitions/Adjacent (Polygons)'],Definition:Adjacent,"Each side of a polygon intersects two other sides, and so is terminated at either endpoint by two vertices.


Those two vertices are described as adjacent to each other.",Definition:Polygon/Adjacent/Vertices,,false,"Each side of a polygon intersects two other sides, and so is terminated at either endpoint by two vertices.


Those two vertices are described as adjacent to each other.",Adjacent
"['Definitions/Adjacent (Polyhedra)', 'Definitions/Polyhedra']",Definition:Adjacent,"=== Adjacent Face to Vertex ===
Each vertex of a polyhedron is formed by the intersection of a number of faces.

The faces that form a particular vertex are referred to as the adjacents of that vertex, or described as adjacent to that vertex.

=== Adjacent Face to Edge ===
Each edge of a polyhedron is formed by the intersection of two faces.

The faces that form a particular edge are referred to as the adjacents of that edge, or described as adjacent to that edge.

=== Adjacent Faces ===
Two faces of a polyhedron that meet at the same vertex are adjacent to each other.

=== Adjacent Edge to Face ===
The edges that intersect at a particular face are referred to as the adjacents of that face, or described as adjacent to that face.

=== Adjacent Edge to Edge ===
Two edges of a polyhedron that intersect at a particular vertex are referred to as adjacent to each other.

=== Adjacent Vertex to Face ===
The vertices that intersect a particular face are referred to as the adjacents of that face, or described as adjacent to that face.",Definition:Polyhedron/Adjacent,,false,"=== Adjacent Face to Vertex ===
Each vertex of a polyhedron is formed by the intersection of a number of faces.

The faces that form a particular vertex are referred to as the adjacents of that vertex, or described as adjacent to that vertex.

=== Adjacent Face to Edge ===
Each edge of a polyhedron is formed by the intersection of two faces.

The faces that form a particular edge are referred to as the adjacents of that edge, or described as adjacent to that edge.

=== Adjacent Faces ===
Two faces of a polyhedron that meet at the same vertex are adjacent to each other.

=== Adjacent Edge to Face ===
The edges that intersect at a particular face are referred to as the adjacents of that face, or described as adjacent to that face.

=== Adjacent Edge to Edge ===
Two edges of a polyhedron that intersect at a particular vertex are referred to as adjacent to each other.

=== Adjacent Vertex to Face ===
The vertices that intersect a particular face are referred to as the adjacents of that face, or described as adjacent to that face.",Adjacent
['Definitions/Adjacent (Polyhedra)'],Definition:Adjacent,"Each vertex of a polyhedron is formed by the intersection of a number of faces.

The faces that form a particular vertex are referred to as the adjacents of that vertex, or described as adjacent to that vertex.",Definition:Polyhedron/Adjacent/Face to Vertex,,false,"Each vertex of a polyhedron is formed by the intersection of a number of faces.

The faces that form a particular vertex are referred to as the adjacents of that vertex, or described as adjacent to that vertex.",Adjacent
['Definitions/Adjacent (Polyhedra)'],Definition:Adjacent,"The vertices that intersect a particular face are referred to as the adjacents of that face, or described as adjacent to that face.",Definition:Polyhedron/Adjacent/Vertex to Face,,false,"The vertices that intersect a particular face are referred to as the adjacents of that face, or described as adjacent to that face.",Adjacent
['Definitions/Adjacent (Polyhedra)'],Definition:Adjacent,"Each edge of a polyhedron is formed by the intersection of two faces.

The faces that form a particular edge are referred to as the adjacents of that edge, or described as adjacent to that edge.",Definition:Polyhedron/Adjacent/Face to Edge,,false,"Each edge of a polyhedron is formed by the intersection of two faces.

The faces that form a particular edge are referred to as the adjacents of that edge, or described as adjacent to that edge.",Adjacent
['Definitions/Adjacent (Polyhedra)'],Definition:Adjacent,"The edges that intersect at a particular face are referred to as the adjacents of that face, or described as adjacent to that face.",Definition:Polyhedron/Adjacent/Edge to Face,,false,"The edges that intersect at a particular face are referred to as the adjacents of that face, or described as adjacent to that face.",Adjacent
['Definitions/Adjacent (Polyhedra)'],Definition:Adjacent,Two faces of a polyhedron that meet at the same vertex are adjacent to each other.,Definition:Polyhedron/Adjacent/Faces,,false,Two faces of a polyhedron that meet at the same vertex are adjacent to each other.,Adjacent
['Definitions/Triangles'],Definition:Adjacent,"The two sides of a triangle which form a particular vertex are referred to as adjacent to that angle.

Similarly, the two vertices of a triangle to which a particular side contributes are referred to as adjacent to that side.


Category:Definitions/Triangles",Definition:Triangle (Geometry)/Adjacent,,false,"The two sides of a triangle which form a particular vertex are referred to as adjacent to that angle.

Similarly, the two vertices of a triangle to which a particular side contributes are referred to as adjacent to that side.


Category:Definitions/Triangles",Adjacent
['Definitions/Right Triangles'],Definition:Adjacent,":


In a right-angled triangle, for a given non-right angled vertex, the adjacent side which is not the hypotenuse is referred to as the adjacent.

In the above figure:
: the adjacent to vertex $A$ is side $c$
: the adjacent to vertex $C$ is side $a$.",Definition:Triangle (Geometry)/Right-Angled/Adjacent,,false,":


In a right-angled triangle, for a given non-right angled vertex, the adjacent side which is not the hypotenuse is referred to as the adjacent.

In the above figure:
: the adjacent to vertex A is side c
: the adjacent to vertex C is side a.",Adjacent
"['Definitions/Adjacency (Graph Theory)', 'Definitions/Faces of Graphs']",Definition:Adjacent Faces,":

Let $G = left( V, E right)$ be a planar graph.

Two faces of $G$ are adjacent  if and only if  they are both incident to the same edge (or edges).

In the above diagram, $BCEF$ and $ABF$ are adjacent, but $BCEF$ and $AFG$ are not adjacent.


Note that faces which are both incident to the same vertex are not considered adjacent unless they are also both incident to the same edge.",Definition:Adjacent (Graph Theory)/Faces,,false,":

Let G = ( V, E ) be a planar graph.

Two faces of G are adjacent  if and only if  they are both incident to the same edge (or edges).

In the above diagram, BCEF and ABF are adjacent, but BCEF and AFG are not adjacent.


Note that faces which are both incident to the same vertex are not considered adjacent unless they are also both incident to the same edge.",Adjacent Faces
['Definitions/Adjacent (Polyhedra)'],Definition:Adjacent Faces,Two faces of a polyhedron that meet at the same vertex are adjacent to each other.,Definition:Polyhedron/Adjacent/Faces,,false,Two faces of a polyhedron that meet at the same vertex are adjacent to each other.,Adjacent Faces
"['Definitions/Adjoints', 'Definitions/Linear Transformations on Hilbert Spaces']",Definition:Adjoint,"Let $mathcal H$ and $mathcal K$ be Hilbert spaces.

Let $mathcal B left(   right){mathcal H, mathcal K}$ be the set of bounded linear transformations from $mathcal H$ to $mathcal K$.

Let $A in mathcal B left(   right){mathcal H, mathcal K}$ be a bounded linear transformation.


By Existence and Uniqueness of Adjoint, there exists a unique bounded linear transformation $A^* in mathcal B left(   right){mathcal K, mathcal H}$ such that:

:$forall h in mathcal H, k in mathcal K: {leftlangle A left(   right)h,   rightrangle k}_mathcal K = {leftlangle h,   rightrangle{A^* left(   right)k} }_mathcal H$

where $leftlangle cdot,   rightranglecdot_mathcal H$ and $leftlangle cdot,   rightranglecdot_mathcal K$ are inner products on $mathcal H$ and $mathcal K$ respectively.


$A^*$ is called the adjoint of $A$.


The operation of assigning $A^*$ to $A$ may be referred to as adjoining.",Definition:Adjoint Linear Transformation,,false,"Let ℋ and 𝒦 be Hilbert spaces.

Let ℬ(   )ℋ, 𝒦 be the set of bounded linear transformations from ℋ to 𝒦.

Let A ∈ℬ(   )ℋ, 𝒦 be a bounded linear transformation.


By Existence and Uniqueness of Adjoint, there exists a unique bounded linear transformation A^* ∈ℬ(   )𝒦, ℋ such that:

:∀ h ∈ℋ, k ∈𝒦: ⟨ A (   )h,   ⟩ k_𝒦 = ⟨ h,   ⟩A^* (   )k_ℋ

where ⟨·,   ⟩·_ℋ and ⟨·,   ⟩·_𝒦 are inner products on ℋ and 𝒦 respectively.


A^* is called the adjoint of A.


The operation of assigning A^* to A may be referred to as adjoining.",Adjoint
['Definitions/Galois Connections'],Definition:Adjoint,"Let $left( S, preceq right)$ and $left( T, precsim right)$ be ordered sets.

Let $g: S to T$, $d: T to S$ be mappings.

Let $left( g, d right)$ be a Galois connection.


Then:

:$g$ is called the upper adjoint of the Galois connection.",Definition:Galois Connection/Upper Adjoint,,false,"Let ( S, ≼) and ( T, ≾) be ordered sets.

Let g: S → T, d: T → S be mappings.

Let ( g, d ) be a Galois connection.


Then:

:g is called the upper adjoint of the Galois connection.",Adjoint
['Definitions/Galois Connections'],Definition:Adjoint,"Let $left( S, preceq right)$ and $left( T, precsim right)$ be ordered sets.

Let $g: S to T$, $d: T to S$ be mappings.

Let $left( g, d right)$ be a Galois connection.


Then:

:$d$ is called the lower adjoint of the Galois connection.",Definition:Galois Connection/Lower Adjoint,,false,"Let ( S, ≼) and ( T, ≾) be ordered sets.

Let g: S → T, d: T → S be mappings.

Let ( g, d ) be a Galois connection.


Then:

:d is called the lower adjoint of the Galois connection.",Adjoint
['Definitions/Category Theory'],Definition:Adjoint," 
 ",Definition:Adjoint Functor,,," 
 ",Adjoint
['Definitions/Category Theory'],Definition:Adjoint,"Let $mathbf C$, $mathbf D$ be locally small categories.

Let $F : mathbf D to mathbf C$ and $G : mathbf C to mathbf D$ be functors.

$F$ is a left adjoint functor of $G$  if and only if  there exists an adjunction $left( F, G, alpha right)$.",Definition:Left Adjoint Functor,,false,"Let 𝐂, 𝐃 be locally small categories.

Let F : 𝐃→𝐂 and G : 𝐂→𝐃 be functors.

F is a left adjoint functor of G  if and only if  there exists an adjunction ( F, G, α).",Adjoint
['Definitions/Category Theory'],Definition:Adjoint,"Let $mathbf C$, $mathbf D$ be locally small categories.

Let $F : mathbf D to mathbf C$ and $G : mathbf C to mathbf D$ be functors.

$G$ is a right adjoint functor of $F$  if and only if  there exists an adjunction $left( F, G, alpha right)$.",Definition:Right Adjoint Functor,,false,"Let 𝐂, 𝐃 be locally small categories.

Let F : 𝐃→𝐂 and G : 𝐂→𝐃 be functors.

G is a right adjoint functor of F  if and only if  there exists an adjunction ( F, G, α).",Adjoint
"['Definitions/Affine Geometry', 'Definitions/Geometry', 'Definitions/Branches of Mathematics']",Definition:Affine,"Affine geometry is the study of the geometry of affine spaces.

Hence it is the study of properties and types of geometric figures which are invariant under an affine transformation.

It provides a modern axiomatic approach to the study of configurations of lines, planes and hypersurfaces.

In particular an affine space can be thought of as a finite dimensional vector space with no distinguished origin, and its affine transformations are those that preserve collinearity.",Definition:Affine Geometry,geometry,true,"Affine geometry is the study of the geometry of affine spaces.

Hence it is the study of properties and types of geometric figures which are invariant under an affine transformation.

It provides a modern axiomatic approach to the study of configurations of lines, planes and hypersurfaces.

In particular an affine space can be thought of as a finite dimensional vector space with no distinguished origin, and its affine transformations are those that preserve collinearity.",Affine
"['Definitions/Algebraic Geometry', 'Definitions/Affine Geometry']",Definition:Affine,"Let $K$ be a field.

Let $A = K left[ X_1, ldots, X_n right]$ be the ring of polynomial functions in $n$ variables over $K$.


Then a subset $X subseteq K^n$ is an affine algebraic set  if and only if  it is the zero locus of some set $T subseteq A$.",Definition:Affine Algebraic Set,,false,"Let K be a field.

Let A = K [ X_1, …, X_n ] be the ring of polynomial functions in n variables over K.


Then a subset X ⊆ K^n is an affine algebraic set  if and only if  it is the zero locus of some set T ⊆ A.",Affine
['Definitions/Algebraic Geometry'],Definition:Affine,"Let $k$ be a field.

Let $n ge 1$ be an Integer.

Then a subset $X subseteq k^n$ is an affine algebraic variety if the following hold:

:$(1): quad X$ is an affine algebraic set 

:$(2): quad X$ is irreducible   the Zariski topology.",Definition:Affine Algebraic Variety,,false,"Let k be a field.

Let n ≥ 1 be an Integer.

Then a subset X ⊆ k^n is an affine algebraic variety if the following hold:

:(1):    X is an affine algebraic set 

:(2):    X is irreducible   the Zariski topology.",Affine
['Definitions/Schemes'],Definition:Affine,An affine scheme is a ringed space which is isomorphic to the spectrum of a commutative ring with unity.,Definition:Affine Scheme,,false,An affine scheme is a ringed space which is isomorphic to the spectrum of a commutative ring with unity.,Affine
['Definitions/Affine Geometry'],Definition:Affine,"Let $mathcal E$ be an affine space with difference space $V$.

=== Definition 1 ===
Let $mathcal E$ be an affine space with difference space $V$.


An affine frame in $mathcal E$ is an ordered tuple $left( p_0, e_1, ldots, e_n right)$, where:
:$p_0$ is an element of $mathcal E$ called the origin
:$left( e_1, ldots, e_n right)$ is an ordered basis for $V$.

=== Definition 2 ===
Let $mathcal E$ be an affine space with difference space $V$.


An affine frame may be given by the set of $n + 1$ points:
:$left( q_0, ldots, q_n right) = left( p_0, p_0 + e_1, ldots, p_0 + e_n right)$

The frame $left( p_0, e_1, ldots, e_n right)$ is then recovered by:
:$left( q_0, q_1 - q_0, ldots, q_n - q_0 right)$

Category:Definitions/Affine Geometry",Definition:Affine Frame,,false,"Let ℰ be an affine space with difference space V.

=== Definition 1 ===
Let ℰ be an affine space with difference space V.


An affine frame in ℰ is an ordered tuple ( p_0, e_1, …, e_n ), where:
:p_0 is an element of ℰ called the origin
:( e_1, …, e_n ) is an ordered basis for V.

=== Definition 2 ===
Let ℰ be an affine space with difference space V.


An affine frame may be given by the set of n + 1 points:
:( q_0, …, q_n ) = ( p_0, p_0 + e_1, …, p_0 + e_n )

The frame ( p_0, e_1, …, e_n ) is then recovered by:
:( q_0, q_1 - q_0, …, q_n - q_0 )

Category:Definitions/Affine Geometry",Affine
['Definitions/Monoids'],Definition:Affine,"An affine monoid is a monoid that is:
: finitely generated
and:
: isomorphic to a submonoid of a free abelian group $mathbb Z^d$, for some $d in mathbb Z_{ge 0}$.",Definition:Affine Monoid,,false,"An affine monoid is a monoid that is:
: finitely generated
and:
: isomorphic to a submonoid of a free abelian group ℤ^d, for some d ∈ℤ_≥ 0.",Affine
['Definitions/Affine Geometry'],Definition:Affine,"=== Associativity Axioms ===
Let $K$ be a field.

Let $left( V, +_V, circ right)$ be a vector space over $K$.

Let $mathcal E$ be a set on which two mappings are defined:

:$+ : mathcal E times V to mathcal E$
:$- : mathcal E times mathcal E to V$

satisfying the following associativity conditions:

 
 
 
 
 


Then the ordered triple $left( mathcal E, +, - right)$ is an affine space.

=== Group Action ===
Let $K$ be a field.

Let $left( V, +_V, circ right)$ be a vector space over $K$.

Let $mathcal E$ be a set.

Let $phi: mathcal E times V to mathcal E$ be a free and transitive group action of $left( V, +_V right)$ on $mathcal E$.


Then the ordered pair $left( mathcal E, phi right)$ is an affine space.

=== Weyl's Axioms ===
Let $K$ be a field.

Let $left( V, +_V, circ right)$ be a vector space over $K$.

Let $mathcal E$ be a set on which a mapping is defined:

:$- : mathcal E times mathcal E to V$

satisfying the following associativity conditions:

 
 
 
 


Then the ordered pair $left( mathcal E, - right)$ is an affine space.

=== Addition ===
Let $left( mathcal E, +, - right)$ be an affine space.


Then the mapping $+$ is called affine addition.

=== Subtraction ===
Let $left( mathcal E, +, - right)$ be an affine space.


Then the mapping $-$ is called affine subtraction.

=== Tangent Space ===
Let $left( mathcal E, +, - right)$ be an affine space.

Let $V$ be the vector space that is the codomain of $-$.


Then $V$ is the tangent space of $mathcal E$.

=== Vector ===
Let $mathcal E$ be an affine space.

Let $V$ be the tangent space of $mathcal E$.


An element $v$ of $V$ is called a vector.

=== Point ===
Let $mathcal E$ be an affine space.


Any element $p$ of $mathcal E$ is called a point.",Definition:Affine Space,,false,"=== Associativity Axioms ===
Let K be a field.

Let ( V, +_V, ∘) be a vector space over K.

Let ℰ be a set on which two mappings are defined:

:+ : ℰ× V →ℰ
:- : ℰ×ℰ→ V

satisfying the following associativity conditions:

 
 
 
 
 


Then the ordered triple ( ℰ, +, - ) is an affine space.

=== Group Action ===
Let K be a field.

Let ( V, +_V, ∘) be a vector space over K.

Let ℰ be a set.

Let ϕ: ℰ× V →ℰ be a free and transitive group action of ( V, +_V ) on ℰ.


Then the ordered pair ( ℰ, ϕ) is an affine space.

=== Weyl's Axioms ===
Let K be a field.

Let ( V, +_V, ∘) be a vector space over K.

Let ℰ be a set on which a mapping is defined:

:- : ℰ×ℰ→ V

satisfying the following associativity conditions:

 
 
 
 


Then the ordered pair ( ℰ, - ) is an affine space.

=== Addition ===
Let ( ℰ, +, - ) be an affine space.


Then the mapping + is called affine addition.

=== Subtraction ===
Let ( ℰ, +, - ) be an affine space.


Then the mapping - is called affine subtraction.

=== Tangent Space ===
Let ( ℰ, +, - ) be an affine space.

Let V be the vector space that is the codomain of -.


Then V is the tangent space of ℰ.

=== Vector ===
Let ℰ be an affine space.

Let V be the tangent space of ℰ.


An element v of V is called a vector.

=== Point ===
Let ℰ be an affine space.


Any element p of ℰ is called a point.",Affine
['Definitions/Affine Geometry'],Definition:Affine,"Let $mathcal E$ be an affine space with tangent space $E$.

Let $mathcal F subseteq mathcal E$ be a subset of $mathcal E$.


Then $mathcal F$ is an affine subspace of $mathcal E$  if and only if  there exists a point $p in mathcal E$ such that:

:$F_p := leftlbrace q - p: q in mathcal F rightrbrace$

is a vector subspace of the vector space $E$.",Definition:Affine Subspace,,false,"Let ℰ be an affine space with tangent space E.

Let ℱ⊆ℰ be a subset of ℰ.


Then ℱ is an affine subspace of ℰ  if and only if  there exists a point p ∈ℰ such that:

:F_p := { q - p: q ∈ℱ}

is a vector subspace of the vector space E.",Affine
"['Definitions/Affine Transformations', 'Definitions/Affine Geometry']",Definition:Affine,"Let $mathcal E$ and $mathcal F$ be affine spaces with difference spaces $E$ and $F$ respectively.

Let $mathcal L: mathcal E to mathcal F$ be a mapping.

=== Definition 1 ===
Let $mathcal E$ and $mathcal F$ be affine spaces with difference spaces $E$ and $F$ respectively.

Let $mathcal L: mathcal E to mathcal F$ be a mapping.


$mathcal L$ is an affine transformation  if and only if  there exists a linear transformation $L: E to F$ such that for every pair of points $p, q in mathcal E$:
:$mathcal L left(   right)q = mathcal L left(   right)p + L left(   right){vec {p q} }$

=== Definition 2 ===
Let $mathcal E$ and $mathcal F$ be affine spaces with difference spaces $E$ and $F$ respectively.

Let $mathcal L: mathcal E to mathcal F$ be a mapping.


$mathcal L$ is an affine transformation  if and only if :
:$forall v_1, v_2 in mathcal E: mathcal L left(   right){s v_1 + t v_2} = s mathcal L left(   right){v_1} + t mathcal L left(   right){v_2}$
for some $s, t in mathbb R$ such that $s + t = 1$.",Definition:Affine Transformation,,false,"Let ℰ and ℱ be affine spaces with difference spaces E and F respectively.

Let ℒ: ℰ→ℱ be a mapping.

=== Definition 1 ===
Let ℰ and ℱ be affine spaces with difference spaces E and F respectively.

Let ℒ: ℰ→ℱ be a mapping.


ℒ is an affine transformation  if and only if  there exists a linear transformation L: E → F such that for every pair of points p, q ∈ℰ:
:ℒ(   )q = ℒ(   )p + L (   )p⃗ ⃗q⃗

=== Definition 2 ===
Let ℰ and ℱ be affine spaces with difference spaces E and F respectively.

Let ℒ: ℰ→ℱ be a mapping.


ℒ is an affine transformation  if and only if :
:∀ v_1, v_2 ∈ℰ: ℒ(   )s v_1 + t v_2 = s ℒ(   )v_1 + t ℒ(   )v_2
for some s, t ∈ℝ such that s + t = 1.",Affine
['Definitions/Applied Mathematics'],Definition:Age,"The age of a physical object is defined as the period of time over which it has been in existence.


Category:Definitions/Applied Mathematics",Definition:Age (Time),,false,"The age of a physical object is defined as the period of time over which it has been in existence.


Category:Definitions/Applied Mathematics",Age
['Definitions/Model Theory for Predicate Logic'],Definition:Age,"Let $mathcal M$ be an $mathcal L$-structure.


An age of $mathcal M$ is a class $K$ of $mathcal L$-structures such that:
* if $mathcal A$ is a finitely generated $mathcal L$-structure such that there is an $mathcal L$-embedding $mathcal A to mathcal M$, then $mathcal A$ is isomorphic to some structure in $K$,
* no two structures in $K$ are isomorphic, and
* $K$ does not contain any structures which are not finitely generated or do not embed into $mathcal M$.

That is, $K$ is an age of $mathcal M$  if and only if  it contains exactly one representative from each isomorphism type of the finitely-generated structures that embed into $mathcal M$.

 

Category:Definitions/Model Theory for Predicate Logic",Definition:Age (Model Theory),,false,"Let ℳ be an ℒ-structure.


An age of ℳ is a class K of ℒ-structures such that:
* if 𝒜 is a finitely generated ℒ-structure such that there is an ℒ-embedding 𝒜→ℳ, then 𝒜 is isomorphic to some structure in K,
* no two structures in K are isomorphic, and
* K does not contain any structures which are not finitely generated or do not embed into ℳ.

That is, K is an age of ℳ  if and only if  it contains exactly one representative from each isomorphism type of the finitely-generated structures that embed into ℳ.

 

Category:Definitions/Model Theory for Predicate Logic",Age
"['Definitions/Parenthesis', 'Definitions/Symbolic Logic', 'Definitions/Algebra', 'Definitions/Arithmetic']",Definition:Aggregation,"Parenthesis is a syntactical technique to disambiguate the meaning of a logical formula.

It allows one to specify that a logical formula should (temporarily) be regarded as being a single entity, being on the same level as a statement variable.

Such a formula is referred to as being in parenthesis.

Typically, a formal language, in defining its formal grammar, ensures by means of parenthesis that all of its well-formed words are uniquely readable.


Generally, brackets are used to indicate that certain formulas are in parenthesis.

The brackets that are mostly used are round ones, the left (round) bracket $($ and the right (round) bracket $)$.",Definition:Parenthesis,,false,"Parenthesis is a syntactical technique to disambiguate the meaning of a logical formula.

It allows one to specify that a logical formula should (temporarily) be regarded as being a single entity, being on the same level as a statement variable.

Such a formula is referred to as being in parenthesis.

Typically, a formal language, in defining its formal grammar, ensures by means of parenthesis that all of its well-formed words are uniquely readable.


Generally, brackets are used to indicate that certain formulas are in parenthesis.

The brackets that are mostly used are round ones, the left (round) bracket ( and the right (round) bracket ).",Aggregation
"['Definitions/Set Theory', 'Definitions/Sets']",Definition:Aggregation,"A set is intuitively defined as any aggregation of objects, called elements, which can be precisely defined in some way or other.

We can think of each set as a single entity in itself, and we can denote it (and usually do) by means of a single symbol.


That is, anything you care to think of can be a set. This concept is known as the  .


However, there are problems with the  . If we allow it to be used without any restrictions at all, paradoxes arise, the most famous example probably being Russell's Paradox.


Hence some sources define a set as a  'well-defined' collection of objects, leaving the concept of what constitutes well-definition to later in the exposition.",Definition:Set,,false,"A set is intuitively defined as any aggregation of objects, called elements, which can be precisely defined in some way or other.

We can think of each set as a single entity in itself, and we can denote it (and usually do) by means of a single symbol.


That is, anything you care to think of can be a set. This concept is known as the  .


However, there are problems with the  . If we allow it to be used without any restrictions at all, paradoxes arise, the most famous example probably being Russell's Paradox.


Hence some sources define a set as a  'well-defined' collection of objects, leaving the concept of what constitutes well-definition to later in the exposition.",Aggregation
"['Definitions/Aggregations (Physics)', 'Definitions/Physics', 'Definitions/Applied Mathematics']",Definition:Aggregation,"An aggregation, in the context of physics, is a set of bodies (but usually particles) all of which are under the same or similar conditions, and which are assumed to behave (in certain aspects) as one body.

The concept is deliberately left vague.",Definition:Aggregation (Physics),,false,"An aggregation, in the context of physics, is a set of bodies (but usually particles) all of which are under the same or similar conditions, and which are assumed to behave (in certain aspects) as one body.

The concept is deliberately left vague.",Aggregation
['Definitions/Branches of Mathematics'],Definition:Algebra,Algebra is the branch of mathematics which studies the techniques of manipulation of objects and expressions.,Definition:Algebra (Mathematical Branch),mathematics,true,Algebra is the branch of mathematics which studies the techniques of manipulation of objects and expressions.,Algebra
"['Definitions/Branches of Mathematics', 'Definitions/Algebra', 'Definitions/Abstract Algebra']",Definition:Algebra,"Abstract algebra is a branch of mathematics which studies algebraic structures and algebraic systems.

It can be roughly described as the study of sets equipped with operations.",Definition:Abstract Algebra,,false,"Abstract algebra is a branch of mathematics which studies algebraic structures and algebraic systems.

It can be roughly described as the study of sets equipped with operations.",Algebra
"['Definitions/Abstract Algebra', 'Definitions/Algebraic Structures']",Definition:Algebra,"An algebraic structure with $n$ operations is an ordered tuple:
:$left( S, circ_1, circ_2, ldots, circ_n right)$
where:
:$S$ is a set
:$circ_1, circ_2, ldots, circ_n$ are $n$ binary operations which are defined on all the elements of $S times S$.


=== One Operation ===
An algebraic structure with $1$ operation is an ordered pair:
:$left( S, circ right)$
where:
:$S$ is a set
:$circ$ is a binary operation defined on all the elements of $S times S$.

=== Two Operations ===
An algebraic structure with $2$ operations is an ordered triple:
:$left( S, circ, * right)$
where:
:$S$ is a set
:$circ$ and $*$ are binary operations defined on all the elements of $S times S$.",Definition:Algebraic Structure,,false,"An algebraic structure with n operations is an ordered tuple:
:( S, ∘_1, ∘_2, …, ∘_n )
where:
:S is a set
:∘_1, ∘_2, …, ∘_n are n binary operations which are defined on all the elements of S × S.


=== One Operation ===
An algebraic structure with 1 operation is an ordered pair:
:( S, ∘)
where:
:S is a set
:∘ is a binary operation defined on all the elements of S × S.

=== Two Operations ===
An algebraic structure with 2 operations is an ordered triple:
:( S, ∘, * )
where:
:S is a set
:∘ and * are binary operations defined on all the elements of S × S.",Algebra
"['Definitions/Linear Algebra', 'Definitions/Algebra', 'Definitions/Linearity', 'Definitions/Branches of Mathematics']",Definition:Algebra,Linear algebra is the branch of algebra which studies vector spaces and linear transformations between them.,Definition:Linear Algebra (Mathematical Branch),,false,Linear algebra is the branch of algebra which studies vector spaces and linear transformations between them.,Algebra
"['Definitions/Branches of Mathematics', 'Definitions/Linear Algebra', 'Definitions/Vector Algebra']",Definition:Algebra,Vector Algebra is the branch of mathematics which studies the algebra of vector spaces.,Definition:Vector Algebra,,false,Vector Algebra is the branch of mathematics which studies the algebra of vector spaces.,Algebra
['Definitions/Abstract Algebra'],Definition:Algebra,"An algebra loop $left( S, circ right)$ is a quasigroup with an identity element.
:$exists e in S: forall x in S: x circ e = x = e circ x$",Definition:Algebra Loop,,false,"An algebra loop ( S, ∘) is a quasigroup with an identity element.
:∃ e ∈ S: ∀ x ∈ S: x ∘ e = x = e ∘ x",Algebra
"['Definitions/Set Systems', 'Definitions/Algebras of Sets', 'Definitions/Rings of Sets']",Definition:Algebra,"=== Definition 1 ===

Let $S$ be a set.

Let $mathcal P left( S right)$ be the power set of $S$.

Let $mathcal R subseteq mathcal P left( S right)$ be a set of subsets of $S$.


$mathcal R$ is an algebra of sets over $S$  if and only if  $mathcal R$ satisfies the algebra of sets axioms:
 

=== Definition 2 ===
An algebra of sets is a ring of sets with a unit.",Definition:Algebra of Sets,,false,"=== Definition 1 ===

Let S be a set.

Let 𝒫( S ) be the power set of S.

Let ℛ⊆𝒫( S ) be a set of subsets of S.


ℛ is an algebra of sets over S  if and only if  ℛ satisfies the algebra of sets axioms:
 

=== Definition 2 ===
An algebra of sets is a ring of sets with a unit.",Algebra
"['Definitions/Sigma-Algebras', 'Definitions/Set Systems', 'Definitions/Sigma-Rings', 'Definitions/Algebras of Sets', 'Definitions/Measure Theory']",Definition:Algebra,"=== Definition 1 ===
Let $X$ be a set.

Let $unicode{x3a3}$ be a system of subsets of $X$.


$unicode{x3a3}$ is a $sigma$-algebra over $X$  if and only if  $unicode{x3a3}$ satisfies the sigma-algebra axioms:
 

=== Definition 2 ===
Let $X$ be a set.

Let $unicode{x3a3}$ be a system of subsets of $X$.


$unicode{x3a3}$ is a $sigma$-algebra over $X$  if and only if  $unicode{x3a3}$ satisfies the sigma-algebra axioms:
 

=== Definition 3 ===
A $sigma$-algebra $unicode{x3a3}$ is a $sigma$-ring with a unit.

=== Definition 4 ===
Let $X$ be a set.

A $sigma$-algebra $unicode{x3a3}$ over $X$ is an algebra of sets which is closed under countable unions.",Definition:Sigma-Algebra,,false,"=== Definition 1 ===
Let X be a set.

Let x3a3 be a system of subsets of X.


x3a3 is a σ-algebra over X  if and only if  x3a3 satisfies the sigma-algebra axioms:
 

=== Definition 2 ===
Let X be a set.

Let x3a3 be a system of subsets of X.


x3a3 is a σ-algebra over X  if and only if  x3a3 satisfies the sigma-algebra axioms:
 

=== Definition 3 ===
A σ-algebra x3a3 is a σ-ring with a unit.

=== Definition 4 ===
Let X be a set.

A σ-algebra x3a3 over X is an algebra of sets which is closed under countable unions.",Algebra
"['Definitions/Topology', 'Definitions/Sigma-Algebras', 'Definitions/Measure Theory', 'Definitions/Borel Sigma-Algebras']",Definition:Algebra,"=== Topological Space ===
Let $left( S, tau right)$ be a topological space

The Borel sigma-algebra $mathcal B left(   right){S, tau}$ of $left( S, tau right)$ is the $sigma$-algebra generated by $tau$.

That is, it is the $sigma$-algebra generated by the set of open sets in $S$.


=== Borel Set ===
Let $left( S, tau right)$ be a topological space.

Let $mathcal B left(   right){S, tau}$ be the Borel $sigma$-algebra of $left( S, tau right)$.


The elements of $mathcal B left(   right){S, tau}$ are called the Borel (measurable) sets of $left( S, tau right)$.

=== Metric Space ===
Let $left( S, d right)$ be a metric space.

The Borel sigma-algebra (or $sigma$-algebra) on $left( S, d right)$ is the $sigma$-algebra generated by the open sets in $mathcal P left( S right)$.

By the definition of a topology induced by a metric, this definition is a particular instance of the definition of a Borel $sigma$-algebra on a topological space.


=== Borel Set ===
Let $left( S, tau right)$ be a topological space.

Let $mathcal B left(   right){S, tau}$ be the Borel $sigma$-algebra of $left( S, tau right)$.


The elements of $mathcal B left(   right){S, tau}$ are called the Borel (measurable) sets of $left( S, tau right)$.

=== Borel Set ===
Let $left( S, tau right)$ be a topological space.

Let $mathcal B left(   right){S, tau}$ be the Borel $sigma$-algebra of $left( S, tau right)$.


The elements of $mathcal B left(   right){S, tau}$ are called the Borel (measurable) sets of $left( S, tau right)$.",Definition:Borel Sigma-Algebra,,false,"=== Topological Space ===
Let ( S, τ) be a topological space

The Borel sigma-algebra ℬ(   )S, τ of ( S, τ) is the σ-algebra generated by τ.

That is, it is the σ-algebra generated by the set of open sets in S.


=== Borel Set ===
Let ( S, τ) be a topological space.

Let ℬ(   )S, τ be the Borel σ-algebra of ( S, τ).


The elements of ℬ(   )S, τ are called the Borel (measurable) sets of ( S, τ).

=== Metric Space ===
Let ( S, d ) be a metric space.

The Borel sigma-algebra (or σ-algebra) on ( S, d ) is the σ-algebra generated by the open sets in 𝒫( S ).

By the definition of a topology induced by a metric, this definition is a particular instance of the definition of a Borel σ-algebra on a topological space.


=== Borel Set ===
Let ( S, τ) be a topological space.

Let ℬ(   )S, τ be the Borel σ-algebra of ( S, τ).


The elements of ℬ(   )S, τ are called the Borel (measurable) sets of ( S, τ).

=== Borel Set ===
Let ( S, τ) be a topological space.

Let ℬ(   )S, τ be the Borel σ-algebra of ( S, τ).


The elements of ℬ(   )S, τ are called the Borel (measurable) sets of ( S, τ).",Algebra
"['Definitions/Algebraic Structures', 'Definitions/B-Algebras']",Definition:Algebra,"Let $left( X, circ right)$ be an algebraic structure.


$left( X, circ right)$ is a $B$-algebra  if and only if  $left( X, circ right)$ satisfies the $B$-algebra axioms:
 ",Definition:B-Algebra,,false,"Let ( X, ∘) be an algebraic structure.


( X, ∘) is a B-algebra  if and only if  ( X, ∘) satisfies the B-algebra axioms:
 ",Algebra
['Definitions/Abstract Algebra'],Definition:Algebra,"In the context of abstract algebra, in particular ring theory and linear algebra, the following varieties of algebra exist:

* Definition:Boolean Algebra

* Definition:Algebra over Ring: an $R$-module $G_R$ over a commutative ring $R$ with a bilinear mapping $oplus: G^2 to G$.

* Definition:Algebra over Field: a vector space $G_F$ over a field $F$ with a bilinear mapping $oplus: G^2 to G$.

* Definition:Real Algebra: an algebra over a field where the field in question is the field of real numbers $mathbb R$.

* Definition:Division Algebra: an algebra over a field $left( A_F, oplus right)$ such that $forall a, b in A_F, b ne mathbf 0_A: exists_1 x in A_F, y in A_F: a = b oplus x, a = y oplus b$.

* Definition:Associative Algebra: an algebra over a ring in which the bilinear mapping $oplus$ is associative. 

* Definition:Unitary Algebra, also known as a Unital Algebra: an algebra over a ring $left( A_R, oplus right)$ in which there exists an identity element, that is, a unit, usually denoted $1$, for $oplus$.

* Definition:Unitary Division Algebra: a division algebra $left( A_F, oplus right)$ in which there exists an identity element, that is, a unit, usually denoted $1$, for $oplus$.

* Definition:Graded Algebra: an algebra over a ring where the ring has a gradation, that is, is a graded ring.

* Definition:Filtered Algebra: an algebra over a field which has a sequence of subalgebras which constitute a gradation.

* Definition:Quadratic Algebra: a filtered algebra whose generator consists of degree one elements, with defining relations of degree 2.

* Definition:Lie Algebra",Definition:Algebra (Abstract Algebra),,false,"In the context of abstract algebra, in particular ring theory and linear algebra, the following varieties of algebra exist:

* Definition:Boolean Algebra

* Definition:Algebra over Ring: an R-module G_R over a commutative ring R with a bilinear mapping ⊕: G^2 → G.

* Definition:Algebra over Field: a vector space G_F over a field F with a bilinear mapping ⊕: G^2 → G.

* Definition:Real Algebra: an algebra over a field where the field in question is the field of real numbers ℝ.

* Definition:Division Algebra: an algebra over a field ( A_F, ⊕) such that ∀ a, b ∈ A_F, b 0_A: ∃_1 x ∈ A_F, y ∈ A_F: a = b ⊕ x, a = y ⊕ b.

* Definition:Associative Algebra: an algebra over a ring in which the bilinear mapping ⊕ is associative. 

* Definition:Unitary Algebra, also known as a Unital Algebra: an algebra over a ring ( A_R, ⊕) in which there exists an identity element, that is, a unit, usually denoted 1, for ⊕.

* Definition:Unitary Division Algebra: a division algebra ( A_F, ⊕) in which there exists an identity element, that is, a unit, usually denoted 1, for ⊕.

* Definition:Graded Algebra: an algebra over a ring where the ring has a gradation, that is, is a graded ring.

* Definition:Filtered Algebra: an algebra over a field which has a sequence of subalgebras which constitute a gradation.

* Definition:Quadratic Algebra: a filtered algebra whose generator consists of degree one elements, with defining relations of degree 2.

* Definition:Lie Algebra",Algebra
"['Definitions/Branches of Mathematics', 'Definitions/Abstract Algebra', 'Definitions/Algebraic Topology', 'Definitions/Topology']",Definition:Algebraic,Algebraic topology is a branch of topology which uses tools from abstract algebra to study topological spaces.,Definition:Algebraic Topology,topology,true,Algebraic topology is a branch of topology which uses tools from abstract algebra to study topological spaces.,Algebraic
"['Definitions/Algebraic Geometry', 'Definitions/Geometry', 'Definitions/Branches of Mathematics']",Definition:Algebraic,"Algebraic geometry is the branch of geometry which studies objects in multi-dimensional space using the techniques of abstract algebra.

In particular, techniques from commutative algebra are mainly used.

It also encompasses the study of algebraic varieties.",Definition:Algebraic Geometry,geometry,true,"Algebraic geometry is the branch of geometry which studies objects in multi-dimensional space using the techniques of abstract algebra.

In particular, techniques from commutative algebra are mainly used.

It also encompasses the study of algebraic varieties.",Algebraic
"['Definitions/Algebraic Varieties', 'Definitions/Algebraic Geometry']",Definition:Algebraic,"An algebraic variety is the solution set of a system of simultaneous polynomial equations:

 
 
 
 
 
 ",Definition:Algebraic Variety,,false,"An algebraic variety is the solution set of a system of simultaneous polynomial equations:

 
 
 
 
 
 ",Algebraic
['Definitions/Field Extensions'],Definition:Algebraic,"Let $L / K$ be a field extension.

Let $A subseteq L$ be a subset of $L$.

Let $K left(   right){leftlbrace X_alpha: alpha in A rightrbrace }$ be the field of rational functions in the indeterminates $leftlangle X_alpha rightrangle_{alpha mathop in A}$.


Then $A$ is algebraically independent over $K$  if and only if  there exists a homomorphism:
:$phi: K left(   right){leftlbrace X_alpha: alpha in A rightrbrace } to L$
such that, for all $alpha in A$:
:$phi left(   right){X_alpha} = alpha$",Definition:Algebraically Independent,,false,"Let L / K be a field extension.

Let A ⊆ L be a subset of L.

Let K (   ){ X_α: α∈ A } be the field of rational functions in the indeterminates ⟨ X_α⟩_α∈ A.


Then A is algebraically independent over K  if and only if  there exists a homomorphism:
:ϕ: K (   ){ X_α: α∈ A }→ L
such that, for all α∈ A:
:ϕ(   )X_α = α",Algebraic
"['Definitions/Branches of Mathematics', 'Definitions/Algebraic Number Theory', 'Definitions/Number Theory']",Definition:Algebraic,"Algebraic number theory is the branch of abstract algebra which studies structures in which the usual number fields are embedded.

As such it can also be considered to be a branch of number theory.",Definition:Algebraic Number Theory,,false,"Algebraic number theory is the branch of abstract algebra which studies structures in which the usual number fields are embedded.

As such it can also be considered to be a branch of number theory.",Algebraic
['Definitions/Algebraic Number Theory'],Definition:Algebraic,"Let $K / mathbb Q$ be an algebraic number field.

 


Then $alpha in K$ is an algebraic integer  if and only if  it satisfies a monic polynomial $f in mathbb Z left[ X right]$.

The set of all algebraic integers in $K$ is denoted $mathcal O_K$.


By Ring of Algebraic Integers it is a ring, hence usually referred to as the ring of algebraic integers of $K$.


=== Quadratic Integer ===
Let $K / mathbb Q$ be an algebraic number field.

Let $K / mathbb Q$ have degree two.


Then an algebraic integer in $K$ is a quadratic integer.",Definition:Algebraic Integer,,false,"Let K / ℚ be an algebraic number field.

 


Then α∈ K is an algebraic integer  if and only if  it satisfies a monic polynomial f ∈ℤ[ X ].

The set of all algebraic integers in K is denoted 𝒪_K.


By Ring of Algebraic Integers it is a ring, hence usually referred to as the ring of algebraic integers of K.


=== Quadratic Integer ===
Let K / ℚ be an algebraic number field.

Let K / ℚ have degree two.


Then an algebraic integer in K is a quadratic integer.",Algebraic
"['Definitions/Numbers', 'Definitions/Algebraic Numbers']",Definition:Algebraic,"An algebraic number is an algebraic element of the field extension $mathbb C / mathbb Q$.

That is, it is a complex number that is a root of a polynomial with rational coefficients.


The set of algebraic numbers can often be seen denoted as $mathbb A$.


=== Degree ===
Let $alpha$ be an algebraic number.

By definition, $alpha$ is the root of at least one polynomial $P_n$ with rational coefficients.


The degree of $alpha$ is the degree of the minimal polynomial $P_n$ whose coefficients are all in $mathbb Q$.


=== Algebraic Number over Field ===

Sources which define an algebraic number over a more general field define degree in the following terms:

Let $F$ be a field.

Let $z in mathbb C$ be algebraic over $F$.


The degree of $alpha$ is the degree of the minimal polynomial $m left(   right)x$ whose coefficients are all in $F$.",Definition:Algebraic Number,,false,"An algebraic number is an algebraic element of the field extension ℂ / ℚ.

That is, it is a complex number that is a root of a polynomial with rational coefficients.


The set of algebraic numbers can often be seen denoted as 𝔸.


=== Degree ===
Let α be an algebraic number.

By definition, α is the root of at least one polynomial P_n with rational coefficients.


The degree of α is the degree of the minimal polynomial P_n whose coefficients are all in ℚ.


=== Algebraic Number over Field ===

Sources which define an algebraic number over a more general field define degree in the following terms:

Let F be a field.

Let z ∈ℂ be algebraic over F.


The degree of α is the degree of the minimal polynomial m (   )x whose coefficients are all in F.",Algebraic
"['Definitions/Field Extensions', 'Definitions/Polynomial Theory']",Definition:Algebraic,"Let $E / F$ be a field extension.

Let $alpha in E$.


=== Definition 1 ===
Let $E / F$ be a field extension.

Let $alpha in E$.


$alpha$ is algebraic over $F$  if and only if  it is a root of some nonzero polynomial over $F$:
:$exists f in F left[ X right] setminus leftlbrace 0 rightrbrace: f left(   right)alpha = 0$
where $F left[ X right]$ denotes the ring of polynomial forms in $X$.

=== Definition 2 ===
Let $E / F$ be a field extension.

Let $alpha in E$.


$alpha$ is algebraic over $F$  if and only if  the evaluation homomorphism $F left[ X right] to E$ at $alpha$ is not injective.",Definition:Algebraic Element of Field Extension,,false,"Let E / F be a field extension.

Let α∈ E.


=== Definition 1 ===
Let E / F be a field extension.

Let α∈ E.


α is algebraic over F  if and only if  it is a root of some nonzero polynomial over F:
:∃ f ∈ F [ X ] ∖{ 0 }: f (   )α = 0
where F [ X ] denotes the ring of polynomial forms in X.

=== Definition 2 ===
Let E / F be a field extension.

Let α∈ E.


α is algebraic over F  if and only if  the evaluation homomorphism F [ X ] → E at α is not injective.",Algebraic
"['Definitions/Field Extensions', 'Definitions/Polynomial Theory']",Definition:Algebraic,"A field extension $E / F$ is said to be algebraic  if and only if :
: $forall alpha in E: alpha$ is algebraic over $F$",Definition:Algebraic Field Extension,,false,"A field extension E / F is said to be algebraic  if and only if :
: ∀α∈ E: α is algebraic over F",Algebraic
['Definitions/Field Theory'],Definition:Algebraic,"Let $K$ be a field.


An algebraic closure of $K$ is an algebraically closed algebraic field extension of $K$.


An algebraic closure of $K$ can be denoted $overline K$.",Definition:Algebraic Closure,,false,"Let K be a field.


An algebraic closure of K is an algebraically closed algebraic field extension of K.


An algebraic closure of K can be denoted K.",Algebraic
['Definitions/Field Extensions'],Definition:Algebraic,"Let $K$ be a field.


Then $K$ is algebraically closed  if and only if :


=== Definition 1 ===
Let $K$ be a field.


$K$ is algebraically closed  if and only if :

The only algebraic field extension of $K$ is $K$ itself.

=== Definition 2 ===
Let $K$ be a field.


$K$ is algebraically closed  if and only if :

Every irreducible polynomial $f$ over $K$ has degree $1$.

=== Definition 3 ===
Let $K$ be a field.


$K$ is algebraically closed  if and only if :

Every polynomial $f$ over $K$ of strictly positive degree has a root in $K$.",Definition:Algebraically Closed Field,,false,"Let K be a field.


Then K is algebraically closed  if and only if :


=== Definition 1 ===
Let K be a field.


K is algebraically closed  if and only if :

The only algebraic field extension of K is K itself.

=== Definition 2 ===
Let K be a field.


K is algebraically closed  if and only if :

Every irreducible polynomial f over K has degree 1.

=== Definition 3 ===
Let K be a field.


K is algebraically closed  if and only if :

Every polynomial f over K of strictly positive degree has a root in K.",Algebraic
"['Definitions/Analysis', 'Definitions/Algebraic Functions']",Definition:Algebraic,"=== Real Algebraic Function ===
Let $y$ be a solution to the polynomial equation:

:$p_0 left(   right)x + p_1 left(   right)x y + dotsb + p_{n - 1}  left(   right)x y^{n - 1} + p_n left(   right)x y^n = 0$

where $p_0 left(   right)x ne 0, p_1 left(   right)x, dotsc, p_n left(   right)x$ are real polynomial functions in $x$.


Then $y = f left(   right)x$ is a (real) algebraic function:

=== Complex Algebraic Function ===
Let $w$ be a solution to the polynomial equation:

:$p_0 left(   right)z + p_1 left(   right)z w + dotsb + p_{n - 1}  left(   right)z w^{n - 1} + p_n left(   right)z w^n = 0$

where $p_0 left(   right)z ne 0, p_1 left(   right)z, dotsc, p_n left(   right)z$ are complex polynomial functions in $z$.


Then $w = f left(   right)z$ is a (complex) algebraic function:",Definition:Algebraic Function,,false,"=== Real Algebraic Function ===
Let y be a solution to the polynomial equation:

:p_0 (   )x + p_1 (   )x y + … + p_n - 1(   )x y^n - 1 + p_n (   )x y^n = 0

where p_0 (   )x  0, p_1 (   )x, …, p_n (   )x are real polynomial functions in x.


Then y = f (   )x is a (real) algebraic function:

=== Complex Algebraic Function ===
Let w be a solution to the polynomial equation:

:p_0 (   )z + p_1 (   )z w + … + p_n - 1(   )z w^n - 1 + p_n (   )z w^n = 0

where p_0 (   )z  0, p_1 (   )z, …, p_n (   )z are complex polynomial functions in z.


Then w = f (   )z is a (complex) algebraic function:",Algebraic
"['Definitions/Algebraic Number Theory', 'Definitions/Number Fields']",Definition:Algebraic,An algebraic number field is a finite extension of the field of rational numbers $mathbb Q$.,Definition:Algebraic Number Field,,false,An algebraic number field is a finite extension of the field of rational numbers ℚ.,Algebraic
"['Definitions/Abstract Algebra', 'Definitions/Algebraic Structures']",Definition:Algebraic,"An algebraic structure with $n$ operations is an ordered tuple:
:$left( S, circ_1, circ_2, ldots, circ_n right)$
where:
:$S$ is a set
:$circ_1, circ_2, ldots, circ_n$ are $n$ binary operations which are defined on all the elements of $S times S$.


=== One Operation ===
An algebraic structure with $1$ operation is an ordered pair:
:$left( S, circ right)$
where:
:$S$ is a set
:$circ$ is a binary operation defined on all the elements of $S times S$.

=== Two Operations ===
An algebraic structure with $2$ operations is an ordered triple:
:$left( S, circ, * right)$
where:
:$S$ is a set
:$circ$ and $*$ are binary operations defined on all the elements of $S times S$.",Definition:Algebraic Structure,,false,"An algebraic structure with n operations is an ordered tuple:
:( S, ∘_1, ∘_2, …, ∘_n )
where:
:S is a set
:∘_1, ∘_2, …, ∘_n are n binary operations which are defined on all the elements of S × S.


=== One Operation ===
An algebraic structure with 1 operation is an ordered pair:
:( S, ∘)
where:
:S is a set
:∘ is a binary operation defined on all the elements of S × S.

=== Two Operations ===
An algebraic structure with 2 operations is an ordered triple:
:( S, ∘, * )
where:
:S is a set
:∘ and * are binary operations defined on all the elements of S × S.",Algebraic
['Definitions/Abstract Algebra'],Definition:Algebraic,"An algebraic system is a mathematical system $mathcal S = left( E, O right)$ where:

:$E$ is a non-empty set of elements

:$O$ is a set of finitary operations on $E$.",Definition:Algebraic System,,false,"An algebraic system is a mathematical system 𝒮 = ( E, O ) where:

:E is a non-empty set of elements

:O is a set of finitary operations on E.",Algebraic
['Definitions/Order Theory'],Definition:Algebraic,"Let $left( S, preceq right)$ be an ordered set.


Then $left( S, preceq right)$ is algebraic  if and only if 
:(for all elements $x$ of $S$: $x^{mathrm{compact} }$ is directed)
and:
:$left( S, preceq right)$ is up-complete and satisfies the axiom of $K$-approximation:
where $x^{mathrm{compact} }$ denotes the compact closure of $x$.",Definition:Algebraic Ordered Set,,false,"Let ( S, ≼) be an ordered set.


Then ( S, ≼) is algebraic  if and only if 
:(for all elements x of S: x^compact is directed)
and:
:( S, ≼) is up-complete and satisfies the axiom of K-approximation:
where x^compact denotes the compact closure of x.",Algebraic
"['Definitions/Alphabets (Formal Language)', 'Definitions/Formal Languages']",Definition:Alphabet,"Let $mathcal L$ be a formal language.


The alphabet $mathcal A$ of $mathcal L$ is a set of symbols from which collations in $mathcal L$ may be constructed.

An alphabet consists of the following parts:

:The letters
:The signs.

Depending on the specific nature of any particular formal language, these too may be subcategorized.


 

 


 ",Definition:Formal Language/Alphabet,,false,"Let ℒ be a formal language.


The alphabet 𝒜 of ℒ is a set of symbols from which collations in ℒ may be constructed.

An alphabet consists of the following parts:

:The letters
:The signs.

Depending on the specific nature of any particular formal language, these too may be subcategorized.


 

 


 ",Alphabet
"['Definitions/Natural Language', 'Definitions/Language Definitions']",Definition:Alphabet,"The alphabet of a natural language $mathcal L$ is the set of symbols, called letters, which are used to represent the sounds of $mathcal L$.


=== English ===
The English alphabet consists of the following letters:

:$texttt {A B C D E F G H I J K L M N O P Q R S T U V W X Y Z}$
:$texttt {a b c d e f g h i j k l m n o p q r s t u v w x y z}$",Definition:Alphabet of Natural Language,,false,"The alphabet of a natural language ℒ is the set of symbols, called letters, which are used to represent the sounds of ℒ.


=== English ===
The English alphabet consists of the following letters:

:
:",Alphabet
['Definitions/Disjunction'],Definition:Alternant,"Let $p lor q$ be a compound statement whose main connective is the disjunction:
:$p lor q$  if and only if  $p$ is true or $q$ is true or both are true.


The substatements $p$ and $q$ are known as the disjuncts.",Definition:Disjunction/Disjunct,,false,"Let p  q be a compound statement whose main connective is the disjunction:
:p  q  if and only if  p is true or q is true or both are true.


The substatements p and q are known as the disjuncts.",Alternant
"['Definitions/Alternants', 'Definitions/Linear Algebra']",Definition:Alternant,"An alternant is a determinant of order $n$ such that the element in the $i$th row and $j$th column is defined as:
:$f_i left(   right){r_j}$
where:
:the $f_i$ are $n$ mappings
:the $r_j$ are $n$ elements.",Definition:Alternant (Linear Algebra),determinant,true,"An alternant is a determinant of order n such that the element in the ith row and jth column is defined as:
:f_i (   )r_j
where:
:the f_i are n mappings
:the r_j are n elements.",Alternant
['Definitions/Disjunction'],Definition:Alternative,"Let $p lor q$ be a compound statement whose main connective is the disjunction:
:$p lor q$  if and only if  $p$ is true or $q$ is true or both are true.


The substatements $p$ and $q$ are known as the disjuncts.",Definition:Disjunction/Disjunct,,false,"Let p  q be a compound statement whose main connective is the disjunction:
:p  q  if and only if  p is true or q is true or both are true.


The substatements p and q are known as the disjuncts.",Alternative
['Definitions/Abstract Algebra'],Definition:Alternative,"Let $circ$ be a binary operation.


Then $circ$ is defined as being alternative on $S$  if and only if :

:$forall T := leftlbrace x, y rightrbrace subseteq S: forall x, y, z in T: left( x circ y right) circ z = x circ left( y circ z right)$

That is, $circ$ is associative over any two elements of $S$.


For example, for any $x, y in S$:
:$left( x circ y right) circ x = x circ left( y circ x right)$
:$left( x circ x right) circ y = x circ left( x circ y right)$
and so on.",Definition:Alternative Operation,,false,"Let ∘ be a binary operation.


Then ∘ is defined as being alternative on S  if and only if :

:∀ T := { x, y }⊆ S: ∀ x, y, z ∈ T: ( x ∘ y ) ∘ z = x ∘( y ∘ z )

That is, ∘ is associative over any two elements of S.


For example, for any x, y ∈ S:
:( x ∘ y ) ∘ x = x ∘( y ∘ x )
:( x ∘ x ) ∘ y = x ∘( x ∘ y )
and so on.",Alternative
"['Definitions/Altitudes (Geometry)', 'Definitions/Polygons']",Definition:Altitude,An altitude of a polygon is the longest perpendicular from the base to a vertex most distant from the base.,Definition:Altitude of Polygon,,false,An altitude of a polygon is the longest perpendicular from the base to a vertex most distant from the base.,Altitude
"['Definitions/Altitudes (Geometry)', 'Definitions/Triangles']",Definition:Altitude,"Let $triangle ABC$ be a triangle.

Let a perpendicular be dropped from $angle A$ to its opposite side $a$ or its production:

:

The line $h_a$ so constructed is called the altitude of $angle A$.


=== Foot of Altitude ===
Let $triangle ABC$ be a triangle.

Let $h_a$ be the altitude of $A$:

:


The point at which $h_a$ meets $BC$ (or its production) is the foot of the altitude $h_a$.


Category:Definitions/Triangles",Definition:Altitude of Triangle,,false,"Let ABC be a triangle.

Let a perpendicular be dropped from ∠ A to its opposite side a or its production:

:

The line h_a so constructed is called the altitude of ∠ A.


=== Foot of Altitude ===
Let ABC be a triangle.

Let h_a be the altitude of A:

:


The point at which h_a meets BC (or its production) is the foot of the altitude h_a.


Category:Definitions/Triangles",Altitude
['Definitions/Parallelograms'],Definition:Altitude,":

An altitude of a parallelogram is a line drawn perpendicular to its base, through one of its vertices to the side opposite to the base (which is extended if necessary).

In the parallelogram above, line $DE$ is an altitude of the parallelogram $ABCD$.


The term is also used for the length of such a line.",Definition:Quadrilateral/Parallelogram/Altitude,,false,":

An altitude of a parallelogram is a line drawn perpendicular to its base, through one of its vertices to the side opposite to the base (which is extended if necessary).

In the parallelogram above, line DE is an altitude of the parallelogram ABCD.


The term is also used for the length of such a line.",Altitude
"['Definitions/Altitudes (Geometry)', 'Definitions/Geometric Figures']",Definition:Altitude,"The altitude of a geometric figure $mathcal F$ is the length of a line segment giving the height of $mathcal F$.


=== Altitude of Polygon ===
An altitude of a polygon is the longest perpendicular from the base to a vertex most distant from the base.

=== Altitude of Polyhedron ===
An altitude of a polyhedron is the longest perpendicular from the base to a vertex most distant from the base.

=== Altitude of Cone ===
:

Let a perpendicular $AE$ be dropped from the apex of a cone to the plane containing its base.

The line segment $AE$ is an altitude of the cone.

=== Altitude of Cylinder ===

:


:

An altitude of a cylinder is a line segment drawn perpendicular to the base and its opposite plane.


In the above diagram, $HJ$ is an altitude of the cylinder $ACBDFE$.

=== Altitude of Prism ===

:
:

An altitude of a prism is a line which is perpendicular to the bases of the prism.

In the above diagram, a line of length $h$ is an altitude of the prism $AJ$.

=== Altitude of Pyramid ===

:
:

An altitude of a pyramid is a straight line perpendicular to the plane of the base to its apex.

In the above diagram, an altitude is a straight line length is $h$.",Definition:Altitude of Geometric Figure,,false,"The altitude of a geometric figure ℱ is the length of a line segment giving the height of ℱ.


=== Altitude of Polygon ===
An altitude of a polygon is the longest perpendicular from the base to a vertex most distant from the base.

=== Altitude of Polyhedron ===
An altitude of a polyhedron is the longest perpendicular from the base to a vertex most distant from the base.

=== Altitude of Cone ===
:

Let a perpendicular AE be dropped from the apex of a cone to the plane containing its base.

The line segment AE is an altitude of the cone.

=== Altitude of Cylinder ===

:


:

An altitude of a cylinder is a line segment drawn perpendicular to the base and its opposite plane.


In the above diagram, HJ is an altitude of the cylinder ACBDFE.

=== Altitude of Prism ===

:
:

An altitude of a prism is a line which is perpendicular to the bases of the prism.

In the above diagram, a line of length h is an altitude of the prism AJ.

=== Altitude of Pyramid ===

:
:

An altitude of a pyramid is a straight line perpendicular to the plane of the base to its apex.

In the above diagram, an altitude is a straight line length is h.",Altitude
"['Definitions/Altitudes (Geometry)', 'Definitions/Polygons']",Definition:Altitude,An altitude of a polyhedron is the longest perpendicular from the base to a vertex most distant from the base.,Definition:Altitude of Polyhedron,,false,An altitude of a polyhedron is the longest perpendicular from the base to a vertex most distant from the base.,Altitude
"['Definitions/Altitudes (Geometry)', 'Definitions/Cones']",Definition:Altitude,":

Let a perpendicular $AE$ be dropped from the apex of a cone to the plane containing its base.

The line segment $AE$ is an altitude of the cone.",Definition:Cone (Geometry)/Altitude,,false,":

Let a perpendicular AE be dropped from the apex of a cone to the plane containing its base.

The line segment AE is an altitude of the cone.",Altitude
"['Definitions/Altitudes (Geometry)', 'Definitions/Cylinders']",Definition:Altitude,":

An altitude of a cylinder is a line segment drawn perpendicular to the base and its opposite plane.


In the above diagram, $HJ$ is an altitude of the cylinder $ACBDFE$.",Definition:Altitude of Cylinder,,false,":

An altitude of a cylinder is a line segment drawn perpendicular to the base and its opposite plane.


In the above diagram, HJ is an altitude of the cylinder ACBDFE.",Altitude
"['Definitions/Altitudes (Geometry)', 'Definitions/Prisms']",Definition:Altitude,":

An altitude of a prism is a line which is perpendicular to the bases of the prism.

In the above diagram, a line of length $h$ is an altitude of the prism $AJ$.",Definition:Altitude of Prism,,false,":

An altitude of a prism is a line which is perpendicular to the bases of the prism.

In the above diagram, a line of length h is an altitude of the prism AJ.",Altitude
"['Definitions/Altitudes (Geometry)', 'Definitions/Pyramids']",Definition:Altitude,":

An altitude of a pyramid is a straight line perpendicular to the plane of the base to its apex.

In the above diagram, an altitude is a straight line length is $h$.",Definition:Altitude of Pyramid,,false,":

An altitude of a pyramid is a straight line perpendicular to the plane of the base to its apex.

In the above diagram, an altitude is a straight line length is h.",Altitude
"['Definitions/Celestial Altitude', 'Definitions/Celestial Sphere']",Definition:Altitude,"Let $X$ be a point on the celestial sphere.

The (celestial) altitude of $X$ is defined as the angle subtended by the the arc of the vertical circle through $X$ between the celestial horizon and $X$ itself.


=== Symbol ===
",Definition:Celestial Altitude,,false,"Let X be a point on the celestial sphere.

The (celestial) altitude of X is defined as the angle subtended by the the arc of the vertical circle through X between the celestial horizon and X itself.


=== Symbol ===
",Altitude
['Definitions/Periodic Functions'],Definition:Amplitude,"Let $f: mathbb R to mathbb R$ be a periodic real function.


The amplitude of $f$ is the maximum absolute difference of the value of $f$ from a reference level.",Definition:Periodic Real Function/Amplitude,,false,"Let f: ℝ→ℝ be a periodic real function.


The amplitude of f is the maximum absolute difference of the value of f from a reference level.",Amplitude
['Definitions/Incomplete Elliptic Integral of the First Kind'],Definition:Amplitude,"Let $u = F left(   right){k, phi}$ denote the incomplete elliptic integral of the first kind.

The parameter $phi$ of $u = F left(   right){k, phi}$ is called the amplitude of $u$.


=== Symbol ===
",Definition:Incomplete Elliptic Integral of the First Kind/Amplitude,,false,"Let u = F (   )k, ϕ denote the incomplete elliptic integral of the first kind.

The parameter ϕ of u = F (   )k, ϕ is called the amplitude of u.


=== Symbol ===
",Amplitude
['Definitions/Simple Harmonic Motion'],Definition:Amplitude,"Consider a physical system $S$ in a state of simple harmonic motion:
:$x = A sin left(   right){omega t + phi}$


The parameter $A$ is known as the amplitude of the motion.",Definition:Simple Harmonic Motion/Amplitude,,false,"Consider a physical system S in a state of simple harmonic motion:
:x = A sin(   )ω t + ϕ


The parameter A is known as the amplitude of the motion.",Amplitude
"['Definitions/Annuli', 'Definitions/Concentric Circles', 'Definitions/Circles']",Definition:Annulus,"An annulus is a plane figure whose boundary consists of $2$ concentric circles:

:

In the above diagram, the annulus is the colored area.


=== Center of Annulus ===
:

The center of an annulus is the common center of the $2$ concentric circles that form its boundary.

In the above diagram, the center is the point $O$.


Category:Definitions/Annuli

=== Inner Radius of Annulus ===
:

The inner radius of an annulus is the radius of the smaller of the $2$ concentric circles that form its boundary.

In the above diagram, the inner radius is denoted $r$.

=== Outer Radius of Annulus ===
:

The outer radius of an annulus is the radius of the larger of the $2$ concentric circles that form its boundary.

In the above diagram, the outer radius is denoted $R$.

=== Width of Annulus ===
:

The width of an annulus is the difference between its outer radius and the inner radius

In the above diagram, the width of the annulus is $R - r$.",Definition:Annulus (Geometry),plane figure,true,"An annulus is a plane figure whose boundary consists of 2 concentric circles:

:

In the above diagram, the annulus is the colored area.


=== Center of Annulus ===
:

The center of an annulus is the common center of the 2 concentric circles that form its boundary.

In the above diagram, the center is the point O.


Category:Definitions/Annuli

=== Inner Radius of Annulus ===
:

The inner radius of an annulus is the radius of the smaller of the 2 concentric circles that form its boundary.

In the above diagram, the inner radius is denoted r.

=== Outer Radius of Annulus ===
:

The outer radius of an annulus is the radius of the larger of the 2 concentric circles that form its boundary.

In the above diagram, the outer radius is denoted R.

=== Width of Annulus ===
:

The width of an annulus is the difference between its outer radius and the inner radius

In the above diagram, the width of the annulus is R - r.",Annulus
"['Definitions/Examples of Topological Spaces', 'Definitions/Annuli']",Definition:Annulus,"An annulus in the context of topology is a topological space which is homeomorphic with an annulus in the context of geometry.

:",Definition:Annulus (Topology),,false,"An annulus in the context of topology is a topological space which is homeomorphic with an annulus in the context of geometry.

:",Annulus
"['Definitions/Convex Polyhedra', 'Definitions/Archimedean Polyhedra']",Definition:Archimedean,"An Archimedean polyhedron is a convex polyhedron with the following properties:
:$(1): quad$ Each of its faces is a regular polygon
:$(2): quad$ It is isogonal
:$(3): quad$ The faces are not all congruent.
:$(4): quad$ It is not a regular prism or a regular antiprism.",Definition:Archimedean Polyhedron,,false,"An Archimedean polyhedron is a convex polyhedron with the following properties:
:(1): Each of its faces is a regular polygon
:(2): It is isogonal
:(3): The faces are not all congruent.
:(4): It is not a regular prism or a regular antiprism.",Archimedean
"['Definitions/Archimedean Spiral', 'Definitions/Spirals']",Definition:Archimedean,"The Archimedean spiral is the locus of the equation expressed in polar coordinates as:
:$r = a theta$


:


=== Archimedes' Definition ===
 ' definition of his Archimedean spiral is as follows:

:If a straight line of which one extremity remains fixed be made to revolve at a uniform rate in a plane until it returns to the position from which it started, and if, at the same time as the straight line revolves, a point moves at a uniform rate along the straight line, starting from the fixed extremity, the point will describe a spiral in the plane.",Definition:Archimedean Spiral,,false,"The Archimedean spiral is the locus of the equation expressed in polar coordinates as:
:r = a θ


:


=== Archimedes' Definition ===
 ' definition of his Archimedean spiral is as follows:

:If a straight line of which one extremity remains fixed be made to revolve at a uniform rate in a plane until it returns to the position from which it started, and if, at the same time as the straight line revolves, a point moves at a uniform rate along the straight line, starting from the fixed extremity, the point will describe a spiral in the plane.",Archimedean
['Definitions/Norm Theory'],Definition:Archimedean,"=== Non-Archimedean Norm (Vector Space) ===
Let $left( R, +, circ right)$ be a division ring with norm $leftlVert ,cdot, rightrVert_R$.

Let $X$ be a vector space over $R$, with zero $0_X$.


=== Definition 1 ===
Let $X$ be a vector space.

A norm $leftlVert ,cdot, rightrVert $ on $X$ is non-Archimedean  if and only if  $leftlVert , cdot , rightrVert$ satisfies the axiom:
 
 
 

=== Definition 2 ===
Let $left( R, +, circ right)$ be a division ring with norm $leftlVert ,cdot, rightrVert_R$.

Let $X$ be a vector space over $R$, with zero $0_X$.


A non-Archimedean norm on $X$ is a mapping from $X$ to the non-negative reals:
:$leftlVert , cdot , rightrVert: X to mathbb R_{ge 0}$
satisfying the non-Archimedean norm axioms:
 

The pair $left( X, leftlVert , cdot ,  rightrVert  right)$ is a non-Archimedean normed vector space.


=== Archimedean ===
A norm $leftlVert ,cdot, rightrVert $ on a vector space $X$ is Archimedean  if and only if  it is not non-Archimedean.


That is,  if and only if :
:$exists x, y in X: leftlVert x + y rightrVert > max leftlbrace  {leftlVert x rightrVert, leftlVert y rightrVert}  rightrbrace$

Category:Definitions/Norm Theory

=== Non-Archimedean Norm (Division Ring) ===
Let $left( R, +, circ right)$ be a division ring whose zero is denoted $0_R$.


=== Definition 1 ===
Let $left( R, +, circ right)$ be a division ring whose zero is denoted $0_R$.


A norm $leftlVert , cdot , rightrVert$ on $R$ is non-Archimedean  if and only if  $leftlVert , cdot , rightrVert$ satisfies the axiom:
 
 
 

=== Definition 2 ===
Let $left( R, +, circ right)$ be a division ring whose zero is denoted $0_R$.


A non-Archimedean norm on $R$ is a mapping from $R$ to the non-negative reals:
:$leftlVert , cdot , rightrVert: R to mathbb R_{ge 0}$
satisfying the non-Archimedean norm axioms:
 


The pair $left( R, leftlVert , cdot ,  rightrVert  right)$ is a non-Archimedean Normed Division Ring.


If $R$ is also a commutative ring, that is, $left( R, leftlVert ,cdot, rightrVert  right)$ is a valued field, then $left( R, leftlVert ,cdot, rightrVert  right)$ is a non-Archimedean Valued Field.


=== Archimedean ===
Let $leftlVert , cdot , rightrVert$ be a norm on a division ring $R$ satisfying the norm axioms:
: 


A norm $leftlVert , cdot , rightrVert$ is said to be Archimedean  if and only if  it does not satisfy the  .


That is, $leftlVert , cdot , rightrVert$ is Archimedean  if and only if :
 
 
 

=== Non-Archimedean Metric ===
A metric $d$ on a metric space $X$ is non-Archimedean  if and only if :
:$d left(   right){x, y} le max leftlbrace d left(   right){x, z}, d left(   right){y, z}  rightrbrace$
for all $x, y, z in X$.


=== Archimedean ===
A metric is Archimedean  if and only if  it is not non-Archimedean.


That is,  if and only if :
:$exists x, y, z, in X: d left(   right){x, y} > max leftlbrace d left(   right){x, z}, d left(   right){y, z}  rightrbrace$

Category:Definitions/Norm Theory

Category:Definitions/Norm Theory",Definition:Non-Archimedean,,false,"=== Non-Archimedean Norm (Vector Space) ===
Let ( R, +, ∘) be a division ring with norm ‖ · ‖_R.

Let X be a vector space over R, with zero 0_X.


=== Definition 1 ===
Let X be a vector space.

A norm ‖ · ‖ on X is non-Archimedean  if and only if  ‖ · ‖ satisfies the axiom:
 
 
 

=== Definition 2 ===
Let ( R, +, ∘) be a division ring with norm ‖ · ‖_R.

Let X be a vector space over R, with zero 0_X.


A non-Archimedean norm on X is a mapping from X to the non-negative reals:
:‖ · ‖: X →ℝ_≥ 0
satisfying the non-Archimedean norm axioms:
 

The pair ( X, ‖ · ‖) is a non-Archimedean normed vector space.


=== Archimedean ===
A norm ‖ · ‖ on a vector space X is Archimedean  if and only if  it is not non-Archimedean.


That is,  if and only if :
:∃ x, y ∈ X: ‖ x + y ‖ > max{‖ x ‖, ‖ y ‖}

Category:Definitions/Norm Theory

=== Non-Archimedean Norm (Division Ring) ===
Let ( R, +, ∘) be a division ring whose zero is denoted 0_R.


=== Definition 1 ===
Let ( R, +, ∘) be a division ring whose zero is denoted 0_R.


A norm ‖ · ‖ on R is non-Archimedean  if and only if  ‖ · ‖ satisfies the axiom:
 
 
 

=== Definition 2 ===
Let ( R, +, ∘) be a division ring whose zero is denoted 0_R.


A non-Archimedean norm on R is a mapping from R to the non-negative reals:
:‖ · ‖: R →ℝ_≥ 0
satisfying the non-Archimedean norm axioms:
 


The pair ( R, ‖ · ‖) is a non-Archimedean Normed Division Ring.


If R is also a commutative ring, that is, ( R, ‖ · ‖) is a valued field, then ( R, ‖ · ‖) is a non-Archimedean Valued Field.


=== Archimedean ===
Let ‖ · ‖ be a norm on a division ring R satisfying the norm axioms:
: 


A norm ‖ · ‖ is said to be Archimedean  if and only if  it does not satisfy the  .


That is, ‖ · ‖ is Archimedean  if and only if :
 
 
 

=== Non-Archimedean Metric ===
A metric d on a metric space X is non-Archimedean  if and only if :
:d (   )x, y≤max{ d (   )x, z, d (   )y, z}
for all x, y, z ∈ X.


=== Archimedean ===
A metric is Archimedean  if and only if  it is not non-Archimedean.


That is,  if and only if :
:∃ x, y, z, ∈ X: d (   )x, y > max{ d (   )x, z, d (   )y, z}

Category:Definitions/Norm Theory

Category:Definitions/Norm Theory",Archimedean
"['Definitions/Abstract Algebra', 'Definitions/Norm Theory']",Definition:Archimedean,"Let $left( S, circ right)$ be a closed algebraic structure on which there exists either an ordering or a norm.


Let $cdot: mathbb Z_{>0} times S to S$ be the operation defined as:
:$m cdot a = begin{cases}
a & : m = 1 \
a circ left( left( m - 1 right) cdot a right) & : m > 1 end {cases}$


=== Archimedean Property on Norm ===
Let $left( S, circ right)$ be a closed algebraic structure.

Let $cdot: mathbb Z_{>0} times S to S$ be the operation defined as:
:$m cdot a = begin{cases}
a & : m = 1 \
a circ left( left( m - 1 right) cdot a right) & : m > 1 end {cases}$


Let $n: S to mathbb R$ be a norm on $S$.
 
 

Then $n$ satisfies the Archimedean property on $S$  if and only if :
:$forall a, b in S: n left( a right) < n left( b right) implies exists m in mathbb N: n left( m cdot a right) > n left( b right)$


Using the more common symbology for a norm:
:$forall a, b in S: leftlVert a rightrVert < leftlVert b rightrVert implies exists m in mathbb Z_{>0}: leftlVert m cdot a rightrVert > leftlVert b rightrVert$


Category:Definitions/Abstract Algebra
Category:Definitions/Norm Theory

=== Archimedean Property on Ordering ===
Let $left( S, circ right)$ be a closed algebraic structure.

Let $cdot: mathbb Z_{>0} times S to S$ be the operation defined as:
:$m cdot a = begin{cases}
a & : m = 1 \
a circ left( left( m - 1 right) cdot a right) & : m > 1 end {cases}$


Let $preceq$ be an ordering on $S$.


Then $preceq$ satisfies the Archimedean property on $S$  if and only if :

:$forall a, b in S: a prec b implies exists m in mathbb Z_{>0}: b prec m cdot a$


Category:Definitions/Abstract Algebra
Category:Definitions/Order Theory",Definition:Archimedean Property,,false,"Let ( S, ∘) be a closed algebraic structure on which there exists either an ordering or a norm.


Let ·: ℤ_>0× S → S be the operation defined as:
:m · a = 
a     : m = 1 

a ∘( ( m - 1 ) · a )     : m > 1


=== Archimedean Property on Norm ===
Let ( S, ∘) be a closed algebraic structure.

Let ·: ℤ_>0× S → S be the operation defined as:
:m · a = 
a     : m = 1 

a ∘( ( m - 1 ) · a )     : m > 1


Let n: S →ℝ be a norm on S.
 
 

Then n satisfies the Archimedean property on S  if and only if :
:∀ a, b ∈ S: n ( a ) < n ( b ) ∃ m ∈ℕ: n ( m · a ) > n ( b )


Using the more common symbology for a norm:
:∀ a, b ∈ S: ‖ a ‖ < ‖ b ‖∃ m ∈ℤ_>0: ‖ m · a ‖ > ‖ b ‖


Category:Definitions/Abstract Algebra
Category:Definitions/Norm Theory

=== Archimedean Property on Ordering ===
Let ( S, ∘) be a closed algebraic structure.

Let ·: ℤ_>0× S → S be the operation defined as:
:m · a = 
a     : m = 1 

a ∘( ( m - 1 ) · a )     : m > 1


Let ≼ be an ordering on S.


Then ≼ satisfies the Archimedean property on S  if and only if :

:∀ a, b ∈ S: a ≺ b ∃ m ∈ℤ_>0: b ≺ m · a


Category:Definitions/Abstract Algebra
Category:Definitions/Order Theory",Archimedean
"['Definitions/Argument of Complex Number', 'Definitions/Complex Numbers', 'Definitions/Complex Analysis', 'Definitions/Polar Form of Complex Number']",Definition:Argument,"Let $z = x + i y$ be a complex number.

An argument of $z$, or $arg z$, is formally defined as a solution to the pair of equations:
:$(1): quad dfrac x {leftlvert z rightrvert} = cos left(   right){arg z}$
:$(2): quad dfrac y {leftlvert z rightrvert} = sin left(   right){arg z}$
where $leftlvert z rightrvert$ is the modulus of $z$.

From Sine and Cosine are Periodic on Reals, it follows that if $theta$ is an argument of $z$, then so is $theta + 2 k pi$ where $k in mathbb Z$ is any integer.


Thus, the argument of a complex number $z$ is a continuous multifunction.

 


=== Principal Range ===
It is understood that the argument of a complex number $z$ is unique only up to multiples of $2 k pi$.

With this understanding, we can limit the choice of what $theta$ can be for any given $z$ by requiring that $theta$ lie in some half open interval of length $2 pi$.

The most usual of these are:
:$left[ 0 ,.,.,   right){2 pi}$
:$left( -pi ,.,.,   right]pi$

but in theory any such interval may be used.

This interval is known as the principal range.

=== Principal Argument ===
Let $R$ be the principal range of the complex numbers $mathbb C$.

The unique value of $theta$ in $R$ is known as the principal argument, of $z$.

This is denoted $mathrm {Arg} left( z right)$.

Note the capital $A$.

The standard practice is for $R$ to be $left( -pi ,.,.,   right]pi$.

This ensures that the principal argument is continuous on the real axis for positive numbers.

Thus, if $z$ is represented in the complex plane, the principal argument $mathrm {Arg} left( z right)$ is intuitively defined as the angle which $z$ yields with the real ($y = 0$) axis.


 ",Definition:Argument of Complex Number,,false,"Let z = x + i y be a complex number.

An argument of z, or z, is formally defined as a solution to the pair of equations:
:(1):    x | z | = cos(   ) z
:(2):    y | z | = sin(   ) z
where | z | is the modulus of z.

From Sine and Cosine are Periodic on Reals, it follows that if θ is an argument of z, then so is θ + 2 k π where k ∈ℤ is any integer.


Thus, the argument of a complex number z is a continuous multifunction.

 


=== Principal Range ===
It is understood that the argument of a complex number z is unique only up to multiples of 2 k π.

With this understanding, we can limit the choice of what θ can be for any given z by requiring that θ lie in some half open interval of length 2 π.

The most usual of these are:
:[ 0  . . )2 π
:( -π . . ]π

but in theory any such interval may be used.

This interval is known as the principal range.

=== Principal Argument ===
Let R be the principal range of the complex numbers ℂ.

The unique value of θ in R is known as the principal argument, of z.

This is denoted Arg( z ).

Note the capital A.

The standard practice is for R to be ( -π . . ]π.

This ensures that the principal argument is continuous on the real axis for positive numbers.

Thus, if z is represented in the complex plane, the principal argument Arg( z ) is intuitively defined as the angle which z yields with the real (y = 0) axis.


 ",Argument
"['Definitions/Logical Arguments', 'Definitions/Logic']",Definition:Argument,"A logical argument (or just argument) is a process of creating a new statement from one or more existing statements.

An argument proceeds from a set of premises to a conclusion, by means of logical implication, via a procedure called logical inference.


An argument may have more than one premise, but only one conclusion.


While statements may be classified as either true or false, an argument may be classified as either valid or invalid.


Loosely speaking, a valid argument is one that leads unshakeably from true statements to other true statements, whereas an invalid argument is one that can lead you to, for example, a false statement from one which is true.


Thus:
:An argument may be valid, even though its premises are false.
:An argument may be invalid, even though its premises are true.
:An argument may be invalid and its premises false.

It is even possible for the conclusion of an argument to be true, even though the argument is invalid and its premises are false.


To be sure of the truth of a conclusion, it is necessary to make sure both that the premises are true and that the argument is valid.


However, while you may not actually know whether a statement is true or not, you can investigate the consequences of it being either true or false, and see what effect that has on the truth value of the proposition(s) of which it is a part. That, in short, is what the process of logical argument consists of.


An argument may be described symbolically by means of sequents, which specify the flow of an argument.


=== Finitary Argument ===
A finitary argument is a logical argument which starts with a finite number of axioms, and can be translated into a finite number of statements.",Definition:Logical Argument,,false,"A logical argument (or just argument) is a process of creating a new statement from one or more existing statements.

An argument proceeds from a set of premises to a conclusion, by means of logical implication, via a procedure called logical inference.


An argument may have more than one premise, but only one conclusion.


While statements may be classified as either true or false, an argument may be classified as either valid or invalid.


Loosely speaking, a valid argument is one that leads unshakeably from true statements to other true statements, whereas an invalid argument is one that can lead you to, for example, a false statement from one which is true.


Thus:
:An argument may be valid, even though its premises are false.
:An argument may be invalid, even though its premises are true.
:An argument may be invalid and its premises false.

It is even possible for the conclusion of an argument to be true, even though the argument is invalid and its premises are false.


To be sure of the truth of a conclusion, it is necessary to make sure both that the premises are true and that the argument is valid.


However, while you may not actually know whether a statement is true or not, you can investigate the consequences of it being either true or false, and see what effect that has on the truth value of the proposition(s) of which it is a part. That, in short, is what the process of logical argument consists of.


An argument may be described symbolically by means of sequents, which specify the flow of an argument.


=== Finitary Argument ===
A finitary argument is a logical argument which starts with a finite number of axioms, and can be translated into a finite number of statements.",Argument
['Definitions/Preimages'],Definition:Argument,"Let $f: S to T$ be a mapping.

Let $f^{-1} subseteq T times S$ be the inverse of $f$, defined as:

:$f^{-1} = leftlbrace left( t, s right): f left(   right)s = t rightrbrace$


Every $s in S$ such that $f left(   right)s = t$ is called a preimage of $t$.


The preimage of an element $t in T$ is defined as:

:$f^{-1}  left(   right)t := leftlbrace s in S: f left(   right)s = t rightrbrace$


This can also be expressed as:
:$f^{-1}  left(   right)t := leftlbrace s in mathrm {Img} left( f^{-1}  right): left( t, s right) in f^{-1}  rightrbrace$


That is, the preimage of $t$ under $f$ is the image of $t$ under $f^{-1}$.",Definition:Preimage/Mapping/Element,,false,"Let f: S → T be a mapping.

Let f^-1⊆ T × S be the inverse of f, defined as:

:f^-1 = {( t, s ): f (   )s = t }


Every s ∈ S such that f (   )s = t is called a preimage of t.


The preimage of an element t ∈ T is defined as:

:f^-1(   )t := { s ∈ S: f (   )s = t }


This can also be expressed as:
:f^-1(   )t := { s ∈Img( f^-1): ( t, s ) ∈ f^-1}


That is, the preimage of t under f is the image of t under f^-1.",Argument
"['Definitions/Independent Variables', 'Definitions/Analysis', 'Definitions/Mapping Theory']",Definition:Argument,"=== Real Function ===
Let $f: mathbb R to mathbb R$ be a real function.

Let $f left(   right)x = y$.

Then $x$ is referred to as an independent variable.

=== Complex Function ===
Let $f: mathbb C to mathbb C$ be a complex function.

Let $f left(   right)z = w$.


Then $z$ is referred to as an independent variable (of $f$).",Definition:Independent Variable,,false,"=== Real Function ===
Let f: ℝ→ℝ be a real function.

Let f (   )x = y.

Then x is referred to as an independent variable.

=== Complex Function ===
Let f: ℂ→ℂ be a complex function.

Let f (   )z = w.


Then z is referred to as an independent variable (of f).",Argument
['Definitions/Polar Coordinates'],Definition:Argument,"Let $P$ be a point in a system of polar coordinates where $O$ is the pole.


The angle measured anticlockwise from the polar axis to $OP$ is called the angular coordinate of $P$, and usually labelled $theta$.

If the angle is measured clockwise from the polar axis to $OP$, its value is considered negative.",Definition:Polar Coordinates/Angular Coordinate,,false,"Let P be a point in a system of polar coordinates where O is the pole.


The angle measured anticlockwise from the polar axis to OP is called the angular coordinate of P, and usually labelled θ.

If the angle is measured clockwise from the polar axis to OP, its value is considered negative.",Argument
"['Definitions/Morphisms', 'Definitions/Category Theory']",Definition:Arrow,"Let $mathbf C$ be a metacategory.


A morphism of $mathbf C$ is an object $f$, together with:

* A domain $operatorname {dom} f$, which is an object of $mathbf C$
* A codomain $operatorname {cod} f$, also an object of $mathbf C$


The collection of all morphisms of $mathbf C$ is denoted $mathbf C_1$.


If $A$ is the domain of $f$ and $B$ is its codomain, this is mostly represented by writing:

:$f: A to B$ or $A stackrel f longrightarrow B$",Definition:Morphism,,false,"Let 𝐂 be a metacategory.


A morphism of 𝐂 is an object f, together with:

* A domain dom f, which is an object of 𝐂
* A codomain cod f, also an object of 𝐂


The collection of all morphisms of 𝐂 is denoted 𝐂_1.


If A is the domain of f and B is its codomain, this is mostly represented by writing:

:f: A → B or A  f ⟶ B",Arrow
"['Definitions/Category Theory', 'Definitions/Morphisms', 'Definitions/Functors']",Definition:Arrow,"Informally, a functor is a morphism of categories.

It may be described as what one must define in order to define a natural transformation.

This is formalized by defining the category of categories.


=== Covariant Functor ===
Let $mathbf C$ and $mathbf D$ be metacategories.


A covariant functor $F: mathbf C to mathbf D$ consists of:

* An object functor $F_0$ that assigns to each object $X$ of $mathbf C$ an object $FX$ of $mathbf D$.

* An arrow functor $F_1$ that assigns to each arrow $f: X to Y$ of $mathbf C$ an arrow $Ff : FX to FY$ of $mathbf D$.


These functors must satisfy, for any morphisms $X stackrel f longrightarrow Y stackrel g longrightarrow Z$ in $mathbf C$:

:$F left(   right){g circ f} = F g circ F f$
and:
:$F left(   right){operatorname {id}_X} = operatorname{id}_{F X}$

where $operatorname {id}_W$ denotes the identity arrow on an object $W$, and $circ$ is the composition of morphisms.


The behaviour of a covariant functor can be pictured as follows:

::$begin{xy}
<4em,4em>*{mathbf C} = ""C"",
<0em,0em>*+{X} = ""a"",
<4em,0em>*+{Y} = ""b"",
<4em,-4em>*+{Z}= ""c"",

""a"";""b"" **@{-} ?>*@{>} ?<>(.5)*!/_1em/{f},
""b"";""c"" **@{-} ?>*@{>} ?<>(.5)*!/_1em/{g},
""a"";""c"" **@{-} ?>*@{>} ?<>(.5)*!/^1em/{g circ f},

""C""+/r9em/*{mathbf D},
""C""+/r2em/;""C""+/r6em/ **@{-} ?>*@{>} ?*!/_1em/{F},

""b""+/r2em/+/_2em/;""b""+/r6em/+/_2em/ **@{~} ?>*@2{>} ?<>(.5)*!/_.6em/{F},

""a""+/r13em/*+{FX}=""Fa"", 
""b""+/r13em/*+{FY}=""Fb"", 
""c""+/r13em/*+{FZ}=""Fc"",

""Fa"";""Fb"" **@{-} ?>*@{>} ?<>(.5)*!/_1em/{Ff},
""Fb"";""Fc"" **@{-} ?>*@{>} ?<>(.5)*!/_1em/{Fg},
""Fa"";""Fc"" **@{-} ?>*@{>} ?<>(.7)*!/r3em/{F left({g circ f}right) = \ Fg circ Ff},
end{xy}$

=== Contravariant Functor ===
=== Definition 1 ===
Let $mathbf C$ and $mathbf D$ be metacategories.


A contravariant functor $F : mathbf C to mathbf D$ consists of:

* An object functor $F_0$ that assigns to each object $X$ of $mathbf C$ an object $FX$ of $mathbf D$.

* An arrow functor $F_1$ that assigns to each arrow $f : X to Y$ of $mathbf C$ an arrow $Ff : FY to FX$ of $mathbf D$.


These functors must satisfy, for any morphisms $X stackrel f longrightarrow Y stackrel g longrightarrow Z$ in $mathbf C$:

:$F left(   right){g circ f} = F f circ F g$
and:
:$F left(   right){operatorname {id}_X} = operatorname {id}_{F X}$

where:
:$operatorname {id}_W$ denotes the identity arrow on an object $W$
and:
:$circ$ is the composition of morphisms.

=== Definition 2 ===
Let $mathbf C$ and $mathbf D$ be metacategories.

A contravariant functor $F : mathbf C to mathbf D$ is a covariant functor:
:$F: mathbf C^{text{op}} to mathbf D$
where $mathbf C^{text{op}}$ is the dual category of $mathbf C$.",Definition:Functor,,false,"Informally, a functor is a morphism of categories.

It may be described as what one must define in order to define a natural transformation.

This is formalized by defining the category of categories.


=== Covariant Functor ===
Let 𝐂 and 𝐃 be metacategories.


A covariant functor F: 𝐂→𝐃 consists of:

* An object functor F_0 that assigns to each object X of 𝐂 an object FX of 𝐃.

* An arrow functor F_1 that assigns to each arrow f: X → Y of 𝐂 an arrow Ff : FX → FY of 𝐃.


These functors must satisfy, for any morphisms X  f ⟶ Y  g ⟶ Z in 𝐂:

:F (   )g ∘ f = F g ∘ F f
and:
:F (   )id_X = id_F X

where id_W denotes the identity arrow on an object W, and ∘ is the composition of morphisms.


The behaviour of a covariant functor can be pictured as follows:

::<4em,4em>*𝐂 = ""C"",
<0em,0em>*+X = ""a"",
<4em,0em>*+Y = ""b"",
<4em,-4em>*+Z= ""c"",

""a"";""b"" **@- ?>*@> ?<>(.5)*!/_1em/f,
""b"";""c"" **@- ?>*@> ?<>(.5)*!/_1em/g,
""a"";""c"" **@- ?>*@> ?<>(.5)*!/^1em/g ∘ f,

""C""+/r9em/*𝐃,
""C""+/r2em/;""C""+/r6em/ **@- ?>*@> ?*!/_1em/F,

""b""+/r2em/+/_2em/;""b""+/r6em/+/_2em/ **@  ?>*@2> ?<>(.5)*!/_.6em/F,

""a""+/r13em/*+FX=""Fa"", 
""b""+/r13em/*+FY=""Fb"", 
""c""+/r13em/*+FZ=""Fc"",

""Fa"";""Fb"" **@- ?>*@> ?<>(.5)*!/_1em/Ff,
""Fb"";""Fc"" **@- ?>*@> ?<>(.5)*!/_1em/Fg,
""Fa"";""Fc"" **@- ?>*@> ?<>(.7)*!/r3em/F (g ∘ f) = 
 Fg ∘ Ff,

=== Contravariant Functor ===
=== Definition 1 ===
Let 𝐂 and 𝐃 be metacategories.


A contravariant functor F : 𝐂→𝐃 consists of:

* An object functor F_0 that assigns to each object X of 𝐂 an object FX of 𝐃.

* An arrow functor F_1 that assigns to each arrow f : X → Y of 𝐂 an arrow Ff : FY → FX of 𝐃.


These functors must satisfy, for any morphisms X  f ⟶ Y  g ⟶ Z in 𝐂:

:F (   )g ∘ f = F f ∘ F g
and:
:F (   )id_X = id_F X

where:
:id_W denotes the identity arrow on an object W
and:
:∘ is the composition of morphisms.

=== Definition 2 ===
Let 𝐂 and 𝐃 be metacategories.

A contravariant functor F : 𝐂→𝐃 is a covariant functor:
:F: 𝐂^op→𝐃
where 𝐂^op is the dual category of 𝐂.",Arrow
"['Definitions/Examples of Categories', 'Definitions/Morphisms']",Definition:Arrow,"Let $mathbf C$ be a metacategory.


Its morphism category, denoted $mathbf C^to$, is defined as follows:

 


The morphisms of $mathbf C^to$ can be made more intuitive by the following diagram:

::$begin{xy}
<-2em,0em>*+{f} = ""f"",
<2em,0em>*+{f'} = ""f2"",

""f"";""f2"" **@{-} ?>*@{>} ?*!/_1em/{scriptstyle left( g_1, g_2 right) },

<3em,0em>*{:},

<7em,2em>*+{A} = ""A"",
<7em,-2em>*+{B} = ""B"",
<11em,2em>*+{A'} = ""A2"",
<11em,-2em>*+{B'} = ""B2"",

""A"";""B"" **@{-} ?>*@{>} ?*!/^1em/{f},
""A"";""A2"" **@{-} ?>*@{>} ?*!/_1em/{g_1},
""A2"";""B2"" **@{-} ?>*@{>} ?*!/_1em/{f'},
""B"";""B2"" **@{-} ?>*@{>} ?*!/^1em/{g_2}
end{xy}$

The composition likewise benefits from a diagrammatic representation:

::$begin{xy}
<4em,5em>*{left( h_1, h_2 right) circ left( g_1, g_2 right) },

<0em,2em>*+{A} = ""A"",
<0em,-2em>*+{B} = ""B"",
<4em,2em>*+{A'} = ""A2"",
<4em,-2em>*+{B'} = ""B2"",

""A"";""B"" **@{-} ?>*@{>} ?*!/^1em/{f},
""A"";""A2"" **@{-} ?>*@{>} ?*!/_1em/{g_1},
""A2"";""B2"" **@{-} ?>*@{>} ?*!/_1em/{f'},
""B"";""B2"" **@{-} ?>*@{>} ?*!/^1em/{g_2},

<8em,2em>*+{A} = ""A3"",
<8em,-2em>*+{B} = ""B3"",

""A2"";""A3"" **@{-} ?>*@{>} ?*!/_1em/{h_1},
""B2"";""B3"" **@{-} ?>*@{>} ?*!/^1em/{h_2},
""A3"";""B3"" **@{-} ?>*@{>} ?*!/_1em/{f},

<12em,5em>*{=},
<10em,0em>;<14em,0em> **@{~} ?>*@2{>},

<20em,5em>*+{left( h_1 circ g_1, h_2 circ g_2 right) },

<16em,2em>*+{A} = ""AA"",
<16em,-2em>*+{B} = ""BB"",
<24em,2em>*+{A} = ""AA3"",
<24em,-2em>*+{B} = ""BB3"",

""AA"";""BB"" **@{-} ?>*@{>} ?*!/^1em/{f},
""AA"";""AA3"" **@{-} ?>*@{>} ?*!/_1em/{h_1 circ g_1},
""AA3"";""BB3"" **@{-} ?>*@{>} ?*!/_1em/{f},
""BB"";""BB3"" **@{-} ?>*@{>} ?*!/^1em/{h_2 circ g_2},
end{xy}$",Definition:Morphism Category,,false,"Let 𝐂 be a metacategory.


Its morphism category, denoted 𝐂^→, is defined as follows:

 


The morphisms of 𝐂^→ can be made more intuitive by the following diagram:

::<-2em,0em>*+f = ""f"",
<2em,0em>*+f' = ""f2"",

""f"";""f2"" **@- ?>*@> ?*!/_1em/( g_1, g_2 ) ,

<3em,0em>*:,

<7em,2em>*+A = ""A"",
<7em,-2em>*+B = ""B"",
<11em,2em>*+A' = ""A2"",
<11em,-2em>*+B' = ""B2"",

""A"";""B"" **@- ?>*@> ?*!/^1em/f,
""A"";""A2"" **@- ?>*@> ?*!/_1em/g_1,
""A2"";""B2"" **@- ?>*@> ?*!/_1em/f',
""B"";""B2"" **@- ?>*@> ?*!/^1em/g_2

The composition likewise benefits from a diagrammatic representation:

::<4em,5em>*( h_1, h_2 ) ∘( g_1, g_2 ) ,

<0em,2em>*+A = ""A"",
<0em,-2em>*+B = ""B"",
<4em,2em>*+A' = ""A2"",
<4em,-2em>*+B' = ""B2"",

""A"";""B"" **@- ?>*@> ?*!/^1em/f,
""A"";""A2"" **@- ?>*@> ?*!/_1em/g_1,
""A2"";""B2"" **@- ?>*@> ?*!/_1em/f',
""B"";""B2"" **@- ?>*@> ?*!/^1em/g_2,

<8em,2em>*+A = ""A3"",
<8em,-2em>*+B = ""B3"",

""A2"";""A3"" **@- ?>*@> ?*!/_1em/h_1,
""B2"";""B3"" **@- ?>*@> ?*!/^1em/h_2,
""A3"";""B3"" **@- ?>*@> ?*!/_1em/f,

<12em,5em>*=,
<10em,0em>;<14em,0em> **@  ?>*@2>,

<20em,5em>*+( h_1 ∘ g_1, h_2 ∘ g_2 ) ,

<16em,2em>*+A = ""AA"",
<16em,-2em>*+B = ""BB"",
<24em,2em>*+A = ""AA3"",
<24em,-2em>*+B = ""BB3"",

""AA"";""BB"" **@- ?>*@> ?*!/^1em/f,
""AA"";""AA3"" **@- ?>*@> ?*!/_1em/h_1 ∘ g_1,
""AA3"";""BB3"" **@- ?>*@> ?*!/_1em/f,
""BB"";""BB3"" **@- ?>*@> ?*!/^1em/h_2 ∘ g_2,",Arrow
['Definitions/Ring Theory'],Definition:Artinian,"Let $A$ be a commutative ring with unity.

Then $A$ is Artinian  if and only if  it is Artinian as an $A$-module.

 ",Definition:Artinian Ring,,false,"Let A be a commutative ring with unity.

Then A is Artinian  if and only if  it is Artinian as an A-module.

 ",Artinian
"['Definitions/Examples of Modules', 'Definitions/Artinian Modules']",Definition:Artinian,"Let $A$ be a commutative ring with unity.

Let $M$ be an $A$-module.


=== Definition 1 ===
Let $A$ be a commutative ring with unity.

Let $M$ be an $A$-module.


$M$ is a Artinian module  if and only if :
:$M$ satisfies the descending chain condition.

=== Definition 2 ===
Let $A$ be a commutative ring with unity.

Let $M$ be an $A$-module


$M$ is a Artinian module  if and only if :
:$M$ satisfies the minimal condition.",Definition:Artinian Module,,false,"Let A be a commutative ring with unity.

Let M be an A-module.


=== Definition 1 ===
Let A be a commutative ring with unity.

Let M be an A-module.


M is a Artinian module  if and only if :
:M satisfies the descending chain condition.

=== Definition 2 ===
Let A be a commutative ring with unity.

Let M be an A-module


M is a Artinian module  if and only if :
:M satisfies the minimal condition.",Artinian
"['Definitions/Abstract Algebra', 'Definitions/Operations', 'Definitions/Associativity']",Definition:Associative,"Let $S$ be a set.

Let $circ : S times S to S$ be a binary operation.


Then $circ$ is associative  if and only if :

:$forall x, y, z in S: left( x circ y right) circ z = x circ left( y circ z right)$",Definition:Associative Operation,,false,"Let S be a set.

Let ∘ : S × S → S be a binary operation.


Then ∘ is associative  if and only if :

:∀ x, y, z ∈ S: ( x ∘ y ) ∘ z = x ∘( y ∘ z )",Associative
"['Definitions/Abstract Algebra', 'Definitions/Operations']",Definition:Associative,"Let $circ$ be a binary operation.

Then $circ$ is defined as being power-associative on $S$  if and only if :

:$forall x in S: left( x circ x right) circ x = x circ left( x circ x right)$",Definition:Power-Associative Operation,,false,"Let ∘ be a binary operation.

Then ∘ is defined as being power-associative on S  if and only if :

:∀ x ∈ S: ( x ∘ x ) ∘ x = x ∘( x ∘ x )",Associative
"['Definitions/Semigroups', 'Definitions/Algebraic Structures']",Definition:Associative,"Let $left( S, circ right)$ be a magma.


Then $left( S, circ right)$ is a semigroup  if and only if  $circ$ is associative on $S$.


That is:

:A semigroup is an algebraic structure which is closed and whose operation is associative.


=== Semigroup Axioms ===

The properties that define a semigroup can be gathered together as follows:

 

=== Multiplicative Semigroup ===
Let $left( S, circ right)$ be a semigroup whose operation is multiplication.


Then $left( S, circ right)$ is a multiplicative semigroup.


It is implicit in this definition that the elements of $S$ are numbers, or objects derived from numbers upon which the concept of multiplication is applicable.

=== Additive Semigroup ===
Let $left( S, circ right)$ be a semigroup whose operation is addition.


Then $left( S, circ right)$ is an additive semigroup.


It is implicit in this definition that the elements of $S$ are numbers, or objects derived from numbers upon which the concept of addition is applicable.",Definition:Semigroup,,false,"Let ( S, ∘) be a magma.


Then ( S, ∘) is a semigroup  if and only if  ∘ is associative on S.


That is:

:A semigroup is an algebraic structure which is closed and whose operation is associative.


=== Semigroup Axioms ===

The properties that define a semigroup can be gathered together as follows:

 

=== Multiplicative Semigroup ===
Let ( S, ∘) be a semigroup whose operation is multiplication.


Then ( S, ∘) is a multiplicative semigroup.


It is implicit in this definition that the elements of S are numbers, or objects derived from numbers upon which the concept of multiplication is applicable.

=== Additive Semigroup ===
Let ( S, ∘) be a semigroup whose operation is addition.


Then ( S, ∘) is an additive semigroup.


It is implicit in this definition that the elements of S are numbers, or objects derived from numbers upon which the concept of addition is applicable.",Associative
"['Definitions/Associative Algebras', 'Definitions/Algebras', 'Definitions/Associativity']",Definition:Associative,"Let $R$ be a commutative ring.

Let $left( A_R, * right)$ be an algebra over $R$.


Then $left( A_R, * right)$ is an associative algebra  if and only if  $*$ is an associative operation.

That is:

:$forall a, b, c in A_R: left( a * b right) * c = a * left( b * c right)$",Definition:Associative Algebra,,false,"Let R be a commutative ring.

Let ( A_R, * ) be an algebra over R.


Then ( A_R, * ) is an associative algebra  if and only if  * is an associative operation.

That is:

:∀ a, b, c ∈ A_R: ( a * b ) * c = a * ( b * c )",Associative
['Definitions/Logic'],Definition:Atom,"In a particular branch of logic, certain concepts are at such a basic level of simplicity they can not be broken down into anything simpler.

Those concepts are called atoms or described as atomic.


Different branches of logic admit different atoms.


=== Propositional Logic ===
In propositional logic, the atoms are statements.",Definition:Atom (Logic),,false,"In a particular branch of logic, certain concepts are at such a basic level of simplicity they can not be broken down into anything simpler.

Those concepts are called atoms or described as atomic.


Different branches of logic admit different atoms.


=== Propositional Logic ===
In propositional logic, the atoms are statements.",Atom
['Definitions/Measures'],Definition:Atom,"Let $left( X, unicode{x3a3}, mu right)$ be a measure space.


An element $x in X$ is said to be an atom (of $mu$)  if and only if :

:$(1): quad leftlbrace x rightrbrace in unicode{x3a3}$
:$(2): quad mu left(   right){leftlbrace x rightrbrace} > 0$

 ",Definition:Atom of Measure,,false,"Let ( X, x3a3, μ) be a measure space.


An element x ∈ X is said to be an atom (of μ)  if and only if :

:(1):   { x }∈x3a3
:(2):   μ(   ){ x } > 0

 ",Atom
['Definitions/Sigma-Algebras'],Definition:Atom,"Let $left( X, unicode{x3a3} right)$ be a measurable space.

Let $E in unicode{x3a3}$ be non-empty.


$E$ is said to be an atom (of $unicode{x3a3}$)  if and only if  it satisfies:

:$forall F in unicode{x3a3}: F subsetneq E implies F = varnothing$


Thus, atoms are the minimal non-empty sets in $unicode{x3a3}$ with respect to the subset ordering.",Definition:Atom of Sigma-Algebra,,false,"Let ( X, x3a3) be a measurable space.

Let E ∈x3a3 be non-empty.


E is said to be an atom (of x3a3)  if and only if  it satisfies:

:∀ F ∈x3a3: F ⊊ E  F = ∅


Thus, atoms are the minimal non-empty sets in x3a3 with respect to the subset ordering.",Atom
"['Definitions/Atoms of Lattices', 'Definitions/Lattice Theory']",Definition:Atom,"Let $left( S, vee, wedge, preceq right)$ be a lattice.


An atom of $left( S, vee, wedge, preceq right)$ is an element $A in S$ such that:
:$forall B in S: B preceq A, B ne A implies B = bot$
:$A ne bot$
where $bot$ denotes the bottom of $left( S, vee, wedge, preceq right)$.",Definition:Atom of Lattice,,false,"Let ( S, ∨, ∧, ≼) be a lattice.


An atom of ( S, ∨, ∧, ≼) is an element A ∈ S such that:
:∀ B ∈ S: B ≼ A, B  A  B =
:A
where  denotes the bottom of ( S, ∨, ∧, ≼).",Atom
"['Definitions/Atoms', 'Definitions/Atomic Physics', 'Definitions/Chemistry']",Definition:Atom,"An atom (in the context of physics and chemistry) is the smallest piece of matter that can exist of a particular type of substance.


Atoms can be subdivided into smaller particles, but then it ceases to be that substance.


=== Diameter ===
The diameter of an atom (considered approximately spherical) is between about $1$ and $5$ angstroms.",Definition:Atom (Physics),piece of matter,true,"An atom (in the context of physics and chemistry) is the smallest piece of matter that can exist of a particular type of substance.


Atoms can be subdivided into smaller particles, but then it ceases to be that substance.


=== Diameter ===
The diameter of an atom (considered approximately spherical) is between about 1 and 5 angstroms.",Atom
"['Definitions/Automorphisms (Abstract Algebra)', 'Definitions/Isomorphisms (Abstract Algebra)', 'Definitions/Homomorphisms (Abstract Algebra)', 'Definitions/Automorphisms']",Definition:Automorphism,"An automorphism is an isomorphism from an algebraic structure to itself.

This applies to the term isomorphism as used both in the sense of bijective homomorphism as well as that of an order isomorphism.


Hence an automorphism is a permutation which is either a homomorphism or an order isomorphism, depending on context.


=== Semigroup Automorphism ===
Let $left( S, circ right)$ be a semigroup.

Let $phi: S to S$ be a (semigroup) isomorphism from $S$ to itself.


Then $phi$ is a semigroup automorphism.

=== Group Automorphism ===
Let $left( G, circ right)$ be a group.

Let $phi: G to G$ be a (group) isomorphism from $G$ to itself.


Then $phi$ is a group automorphism.

=== Ring Automorphism ===
Let $left( R, +, circ right)$ be a ring.

Let $phi: R to R$ be a (ring) isomorphism.


Then $phi$ is a ring automorphism.

That is, a ring automorphism is a (ring) isomorphism from a ring to itself.

=== Field Automorphism ===
Let $left( F, +, circ right)$ be a field.

Let $phi: F to F$ be a (field) isomorphism from $F$ to itself.


Then $phi$ is a field automorphism.

=== $R$-Algebraic Structure Automorphism ===
Let $left( S, ast_1, ast_2, ldots, ast_n, circ right)_R$ be an $R$-algebraic structure.

Let $phi: S to S$ be an $R$-algebraic structure isomorphism from $S$ to itself.


Then $phi$ is an $R$-algebraic structure automorphism.


This definition continues to apply when $S$ is a module, and also when it is a vector space.


=== Module Automorphism ===
Let $left( G, +_G, circ right)_R$ be an $R$-module.

Let $phi: G to G$ be a module isomorphism to itself.


Then $phi$ is a module automorphism.

=== Vector Space Automorphism ===
Let $V$ be a $K$-vector space.

Let $phi: V to V$ be a vector space isomorphism to itself.


Then $phi$ is a vector space automorphism.

=== Ordered Structure Automorphism ===
Let $left( S, circ, preceq right)$ be an ordered structures.

Let $phi: S to S$ be an ordered structure isomorphism from $S$ to itself.


Then $phi$ is an ordered structure automorphism.


=== Ordered Semigroup Automorphism ===
Let $left( S, circ, preceq right)$ be an ordered semigroup.

An ordered semigroup automorphism from $left( S, circ, preceq right)$ to itself is a mapping $phi: S to S$ that is both:

:$(1): quad$ A semigroup automorphism, that is, a semigroup isomorphism from the semigroup $left( S, circ right)$ to itself

:$(2): quad$ An order isomorphism from the ordered set $left( S, preceq right)$ to itself.

=== Ordered Group Automorphism ===
Let $left( G, circ, preceq right)$ be an ordered group.

An ordered group automorphism from $left( G, circ, preceq right)$ to itself is a mapping $phi: G to G$ that is both:

:$(1): quad$ A group automorphism, that is, a group isomorphism from the group $left( G, circ right)$ to itself

:$(2): quad$ An order isomorphism from the ordered set $left( G, preceq right)$ to itself.

=== Ordered Ring Automorphism ===
Let $left( R, +, circ, preceq right)$ be an ordered ring.


An ordered ring automorphism from $left( R, +, circ, preceq right)$ to itself is a mapping $phi: R to R$ that is both:

:$(1): quad$ An ordered group automorphism from the ordered group $left( R, +, preceq right)$ to itself

:$(2): quad$ A semigroup automorphism from the semigroup $left( R, circ right)$ to itself.",Definition:Automorphism (Abstract Algebra),isomorphism,true,"An automorphism is an isomorphism from an algebraic structure to itself.

This applies to the term isomorphism as used both in the sense of bijective homomorphism as well as that of an order isomorphism.


Hence an automorphism is a permutation which is either a homomorphism or an order isomorphism, depending on context.


=== Semigroup Automorphism ===
Let ( S, ∘) be a semigroup.

Let ϕ: S → S be a (semigroup) isomorphism from S to itself.


Then ϕ is a semigroup automorphism.

=== Group Automorphism ===
Let ( G, ∘) be a group.

Let ϕ: G → G be a (group) isomorphism from G to itself.


Then ϕ is a group automorphism.

=== Ring Automorphism ===
Let ( R, +, ∘) be a ring.

Let ϕ: R → R be a (ring) isomorphism.


Then ϕ is a ring automorphism.

That is, a ring automorphism is a (ring) isomorphism from a ring to itself.

=== Field Automorphism ===
Let ( F, +, ∘) be a field.

Let ϕ: F → F be a (field) isomorphism from F to itself.


Then ϕ is a field automorphism.

=== R-Algebraic Structure Automorphism ===
Let ( S, ∗_1, ∗_2, …, ∗_n, ∘)_R be an R-algebraic structure.

Let ϕ: S → S be an R-algebraic structure isomorphism from S to itself.


Then ϕ is an R-algebraic structure automorphism.


This definition continues to apply when S is a module, and also when it is a vector space.


=== Module Automorphism ===
Let ( G, +_G, ∘)_R be an R-module.

Let ϕ: G → G be a module isomorphism to itself.


Then ϕ is a module automorphism.

=== Vector Space Automorphism ===
Let V be a K-vector space.

Let ϕ: V → V be a vector space isomorphism to itself.


Then ϕ is a vector space automorphism.

=== Ordered Structure Automorphism ===
Let ( S, ∘, ≼) be an ordered structures.

Let ϕ: S → S be an ordered structure isomorphism from S to itself.


Then ϕ is an ordered structure automorphism.


=== Ordered Semigroup Automorphism ===
Let ( S, ∘, ≼) be an ordered semigroup.

An ordered semigroup automorphism from ( S, ∘, ≼) to itself is a mapping ϕ: S → S that is both:

:(1): A semigroup automorphism, that is, a semigroup isomorphism from the semigroup ( S, ∘) to itself

:(2): An order isomorphism from the ordered set ( S, ≼) to itself.

=== Ordered Group Automorphism ===
Let ( G, ∘, ≼) be an ordered group.

An ordered group automorphism from ( G, ∘, ≼) to itself is a mapping ϕ: G → G that is both:

:(1): A group automorphism, that is, a group isomorphism from the group ( G, ∘) to itself

:(2): An order isomorphism from the ordered set ( G, ≼) to itself.

=== Ordered Ring Automorphism ===
Let ( R, +, ∘, ≼) be an ordered ring.


An ordered ring automorphism from ( R, +, ∘, ≼) to itself is a mapping ϕ: R → R that is both:

:(1): An ordered group automorphism from the ordered group ( R, +, ≼) to itself

:(2): A semigroup automorphism from the semigroup ( R, ∘) to itself.",Automorphism
"['Definitions/Model Theory for Predicate Logic', 'Definitions/Automorphisms']",Definition:Automorphism,"Let $mathcal M$ and $mathcal N$ be $mathcal L$-structures with universes $M$ and $N$ respectively.


$j: mathcal M to mathcal N$ is an $mathcal L$-embedding  if and only if  it is an injective map $M to N$ which preserves interpretations of all symbols in $mathcal L$; that is, such that:
:$j left(   right){f^mathcal M left(   right){a_1, dots, a_{n_f} } } = f^mathcal N left(   right){j left(   right){a_1}, ldots, j left(   right){a_{n_f} } }$ for all function symbols $f$ in $mathcal L$ and $a_1, dots, a_{n_f}$ in $M$
:$left( a_1, ldots, a_{n_R}  right) in R^mathcal M iff left( j left(   right){a_1}, dots, j left(   right){a_{n_R} }  right) in R^mathcal N$ for all relation symbols $R$ in $mathcal L$ and $a_1, dots, a_{n_R}$ in $M$
:$j left(   right){c^mathcal M} = c^mathcal N$ for all constant symbols $c$ in $mathcal L$.


=== Partial Embedding ===

A common method of constructing isomorphisms and elementary embeddings in proofs is to recursively define them a finite number of elements at a time.  For this  purpose, it is useful to have a definition of embeddings using functions which are only defined on a subset of $M$:


Let $A subseteq M$ be a subset of $M$.


$j: A to mathcal N$ is a partial $mathcal L$-embedding  if and only if  it is an injective map $A to N$ which preserves interpretations of all symbols in $mathcal L$ applied to elements of $A$; that is, such that:
:$j left(   right){f^mathcal M left(   right){a_1, dots, a_{n_f} } } = f^mathcal N left(   right){j left(   right){a_1}, ldots, j left(   right){a_{n_f} } }$ for  all function symbols $f$ in $mathcal L$ and $a_1, dots, a_{n_f}$ in $A$
:$left( a_1, ldots, a_{n_R}  right) in R^mathcal M iff left( j left(   right){a_1}, dots, j left(   right){a_{n_R} }  right) in R^mathcal N$ for all relation symbols $R$ in $mathcal L$ and $a_1, dots, a_{n_R}$ in $A$
:$j left(   right){c^mathcal M} = c^mathcal N$ for all constant symbols $c$ in $mathcal L$.


=== Isomorphism ===

$j: mathcal M to mathcal N$ is an $mathcal L$-isomorphism  if and only if  it is a bijective $mathcal L$-embedding.


=== Automorphism ===

$j: mathcal M to mathcal N$ is an $mathcal L$-automorphism  if and only if  it is an $mathcal L$-isomorphism and $mathcal M = mathcal N$.


It is often useful to talk about automorphisms which are constant on subsets of $M$.  So, there is a definition and a notation for doing so:

Let $A subseteq M$ be a subset of $M$, and let $b in M$.


An $mathcal L$-automorphism $j$ is an $A$-automorphism  if and only if  $j left(   right)a = a$ for all $ain A$.

An $mathcal L$-automorphism $j$ is an $A, b$-automorphism  if and only if  it is an $left( A cup leftlbrace b rightrbrace right)$-automorphism; that is: $j left(   right)a = a$ for all $a in A$ and also $j left(   right)b = b$.",Definition:Embedding (Model Theory),,false,"Let ℳ and 𝒩 be ℒ-structures with universes M and N respectively.


j: ℳ→𝒩 is an ℒ-embedding  if and only if  it is an injective map M → N which preserves interpretations of all symbols in ℒ; that is, such that:
:j (   )f^ℳ(   )a_1, …, a_n_f = f^𝒩(   )j (   )a_1, …, j (   )a_n_f for all function symbols f in ℒ and a_1, …, a_n_f in M
:( a_1, …, a_n_R) ∈ R^ℳ( j (   )a_1, …, j (   )a_n_R) ∈ R^𝒩 for all relation symbols R in ℒ and a_1, …, a_n_R in M
:j (   )c^ℳ = c^𝒩 for all constant symbols c in ℒ.


=== Partial Embedding ===

A common method of constructing isomorphisms and elementary embeddings in proofs is to recursively define them a finite number of elements at a time.  For this  purpose, it is useful to have a definition of embeddings using functions which are only defined on a subset of M:


Let A ⊆ M be a subset of M.


j: A →𝒩 is a partial ℒ-embedding  if and only if  it is an injective map A → N which preserves interpretations of all symbols in ℒ applied to elements of A; that is, such that:
:j (   )f^ℳ(   )a_1, …, a_n_f = f^𝒩(   )j (   )a_1, …, j (   )a_n_f for  all function symbols f in ℒ and a_1, …, a_n_f in A
:( a_1, …, a_n_R) ∈ R^ℳ( j (   )a_1, …, j (   )a_n_R) ∈ R^𝒩 for all relation symbols R in ℒ and a_1, …, a_n_R in A
:j (   )c^ℳ = c^𝒩 for all constant symbols c in ℒ.


=== Isomorphism ===

j: ℳ→𝒩 is an ℒ-isomorphism  if and only if  it is a bijective ℒ-embedding.


=== Automorphism ===

j: ℳ→𝒩 is an ℒ-automorphism  if and only if  it is an ℒ-isomorphism and ℳ = 𝒩.


It is often useful to talk about automorphisms which are constant on subsets of M.  So, there is a definition and a notation for doing so:

Let A ⊆ M be a subset of M, and let b ∈ M.


An ℒ-automorphism j is an A-automorphism  if and only if  j (   )a = a for all a∈ A.

An ℒ-automorphism j is an A, b-automorphism  if and only if  it is an ( A ∪{ b })-automorphism; that is: j (   )a = a for all a ∈ A and also j (   )b = b.",Automorphism
"['Definitions/Auxiliary Equations', 'Definitions/Second Order ODEs']",Definition:Auxiliary,"Let:
:$(1): quad y + p y' + q y = 0$
be a constant coefficient homogeneous linear second order ODE.


The auxiliary equation of $(1)$ is the quadratic equation:
:$m^2 + p m + q = 0$",Definition:Auxiliary Equation,,false,"Let:
:(1):    y + p y' + q y = 0
be a constant coefficient homogeneous linear second order ODE.


The auxiliary equation of (1) is the quadratic equation:
:m^2 + p m + q = 0",Auxiliary
['Definitions/Order Theory'],Definition:Auxiliary,"Let $L = left( S, vee, preceq right)$ be a bounded below join semilattice.

Let $mathcal R subseteq S times S$ be a relation on $S$.


Then $mathcal R$ is an auxiliary relation  if and only if  $mathcal R$ satisfies the auxiliary relation axioms:
 ",Definition:Auxiliary Relation,,false,"Let L = ( S, ∨, ≼) be a bounded below join semilattice.

Let ℛ⊆ S × S be a relation on S.


Then ℛ is an auxiliary relation  if and only if  ℛ satisfies the auxiliary relation axioms:
 ",Auxiliary
"['Definitions/Auxiliary Circles', 'Definitions/Central Conics', 'Definitions/Conic Sections']",Definition:Auxiliary,"Let $mathcal K$ be a central conic, that is, an ellipse or a hyperbola.

The auxiliary circle of $mathcal K$ is the eccentric circle whose diameter coincides with the major axis of $mathcal K$.


=== Auxiliary Circle of Ellipse ===
Let $E$ be an ellipse.

The auxiliary circle of $E$ is the eccentric circle whose diameter coincides with the major axis of $E$:

:

In the above diagram, the auxiliary circle of $E$ is the circle $C$.

=== Auxiliary Circle of Hyperbola ===
Let $H$ be a hyperbola.

The auxiliary circle of $H$ is the eccentric circle whose diameter coincides with the major axis of $H$:

:

In the above diagram, the auxiliary circle of $H$ is the circle $C$.",Definition:Auxiliary Circle,,false,"Let 𝒦 be a central conic, that is, an ellipse or a hyperbola.

The auxiliary circle of 𝒦 is the eccentric circle whose diameter coincides with the major axis of 𝒦.


=== Auxiliary Circle of Ellipse ===
Let E be an ellipse.

The auxiliary circle of E is the eccentric circle whose diameter coincides with the major axis of E:

:

In the above diagram, the auxiliary circle of E is the circle C.

=== Auxiliary Circle of Hyperbola ===
Let H be a hyperbola.

The auxiliary circle of H is the eccentric circle whose diameter coincides with the major axis of H:

:

In the above diagram, the auxiliary circle of H is the circle C.",Auxiliary
"['Definitions/Angles', 'Definitions/Sine Function', 'Definitions/Cosine Function', 'Definitions/Trigonometry']",Definition:Auxiliary,"Consider the expression:

:$(1): quad p sin x + q cos x$

where $x in mathbb R$.

Let $(1)$ be expressed in the form:
:$(2): quad R cos left(   right){x + alpha}$

or:
:$(3): quad R sin left(   right){x + alpha}$


The angle $alpha$ is known as the auxiliary angle of either $(2)$ or $(3)$ as appropriate.",Definition:Auxiliary Angle,,false,"Consider the expression:

:(1):    p sin x + q cos x

where x ∈ℝ.

Let (1) be expressed in the form:
:(2):    R cos(   )x + α

or:
:(3):    R sin(   )x + α


The angle α is known as the auxiliary angle of either (2) or (3) as appropriate.",Auxiliary
"['Definitions/Topology', 'Definitions/Baire Spaces']",Definition:Baire Space,"Let $T = left( S, tau right)$ be a topological space.


=== Definition 1 ===
Let $T = left( S, tau right)$ be a topological space.


$T$ is a Baire space  if and only if  the union of any countable set of closed sets of $T$ whose interiors are empty also has an empty interior.

=== Definition 2 ===
Let $T = left( S, tau right)$ be a topological space.


$T$ is a Baire space  if and only if  the intersection of any countable set of open sets of $T$ which are everywhere dense is everywhere dense.

=== Definition 3 ===
Let $T = left( S, tau right)$ be a topological space.


$T$ is a Baire space  if and only if  the interior of the union of any countable set of closed sets of $T$ which are nowhere dense is empty.

=== Definition 4 ===
Let $T = left( S, tau right)$ be a topological space.


$T$ is a Baire space  if and only if , whenever the union of any countable set of closed sets of $T$ has an interior point, then one of those closed sets must have an interior point.",Definition:Baire Space (Topology),,false,"Let T = ( S, τ) be a topological space.


=== Definition 1 ===
Let T = ( S, τ) be a topological space.


T is a Baire space  if and only if  the union of any countable set of closed sets of T whose interiors are empty also has an empty interior.

=== Definition 2 ===
Let T = ( S, τ) be a topological space.


T is a Baire space  if and only if  the intersection of any countable set of open sets of T which are everywhere dense is everywhere dense.

=== Definition 3 ===
Let T = ( S, τ) be a topological space.


T is a Baire space  if and only if  the interior of the union of any countable set of closed sets of T which are nowhere dense is empty.

=== Definition 4 ===
Let T = ( S, τ) be a topological space.


T is a Baire space  if and only if , whenever the union of any countable set of closed sets of T has an interior point, then one of those closed sets must have an interior point.",Baire Space
['Definitions/Set Theory'],Definition:Baire Space,"The Baire space $mathbf B$ is defined as the set of all infinite sequences of natural numbers.

It can also be defined as the Cartesian product of a countably infinite number of copies of the set of natural numbers.",Definition:Baire Space (Set Theory),defined,true,"The Baire space 𝐁 is defined as the set of all infinite sequences of natural numbers.

It can also be defined as the Cartesian product of a countably infinite number of copies of the set of natural numbers.",Baire Space
"['Definitions/Bar Charts', 'Definitions/Graphs (Statistics)']",Definition:Bar,"A bar chart is a form of graph which consists of a finite set of (usually) vertical bars whose length determines the statistic being communicated.


:",Definition:Bar Chart,,false,"A bar chart is a form of graph which consists of a finite set of (usually) vertical bars whose length determines the statistic being communicated.


:",Bar
"['Definitions/Bar', 'Definitions/Units of Measurement', 'Definitions/CGS', 'Definitions/Pressure']",Definition:Bar,"The bar is a CGS unit of pressure.


It is defined as being:

:The amount of pressure equal to exactly $100 , 000$ pascals.


=== Conversion Factors ===
",Definition:Bar (Unit),CGS unit,true,"The bar is a CGS unit of pressure.


It is defined as being:

:The amount of pressure equal to exactly 100   000 pascals.


=== Conversion Factors ===
",Bar
['Definitions/Ideals in Physics'],Definition:Bar,A bar is a straight rigid body whose length in one dimension is such that its width in the other two dimensions is negligible.,Definition:Bar (Mechanics),body,true,A bar is a straight rigid body whose length in one dimension is such that its width in the other two dimensions is negligible.,Bar
['Definitions/Geometric Figures'],Definition:Base,"The base of a geometric figure is a specific part of that figure which is distinguished from the remainder of that figure and placed (actually or figuratively) at the bottom of a depiction or visualisation.

In some cases the base is truly qualitiatively different from the rest of the figure.

In other cases the base is selected arbitrarily as one of several parts of the figure which may equally well be so chosen.",Definition:Base of Geometric Figure,,false,"The base of a geometric figure is a specific part of that figure which is distinguished from the remainder of that figure and placed (actually or figuratively) at the bottom of a depiction or visualisation.

In some cases the base is truly qualitiatively different from the rest of the figure.

In other cases the base is selected arbitrarily as one of several parts of the figure which may equally well be so chosen.",Base
['Definitions/Triangles'],Definition:Base,":


For a given triangle, one of the sides can be distinguished as being the base.

It is immaterial which is so chosen.

The usual practice is that the triangle is drawn so that the base is made horizontal, and at the bottom.

In the above diagram, it would be conventional for the side $AC$ to be identified as the base.",Definition:Triangle (Geometry)/Base,,false,":


For a given triangle, one of the sides can be distinguished as being the base.

It is immaterial which is so chosen.

The usual practice is that the triangle is drawn so that the base is made horizontal, and at the bottom.

In the above diagram, it would be conventional for the side AC to be identified as the base.",Base
['Definitions/Isosceles Triangles'],Definition:Base,":


The base of an isosceles triangle is specifically defined to be the side which is a different length from the other two.

In the above diagram, $BC$ is the base.",Definition:Triangle (Geometry)/Isosceles/Base,,false,":


The base of an isosceles triangle is specifically defined to be the side which is a different length from the other two.

In the above diagram, BC is the base.",Base
['Definitions/Parallelograms'],Definition:Base,":

In a given parallelogram, one of the sides is distinguished as being the base.

It is immaterial which is so chosen, but usual practice is that it is one of the two longer sides.

In the parallelogram above, line $AB$ is considered to be the base.


Category:Definitions/Parallelograms",Definition:Quadrilateral/Parallelogram/Base,,false,":

In a given parallelogram, one of the sides is distinguished as being the base.

It is immaterial which is so chosen, but usual practice is that it is one of the two longer sides.

In the parallelogram above, line AB is considered to be the base.


Category:Definitions/Parallelograms",Base
['Definitions/Segments of Circles'],Definition:Base,":

The base of a segment of a circle is the straight line forming one of the boundaries of the seqment.

In the above diagram, $AB$ is the base of the highlighted segment.


Category:Definitions/Segments of Circles",Definition:Segment of Circle/Base,,false,":

The base of a segment of a circle is the straight line forming one of the boundaries of the seqment.

In the above diagram, AB is the base of the highlighted segment.


Category:Definitions/Segments of Circles",Base
['Definitions/Cones'],Definition:Base,"Consider a cone consisting of the set of all straight lines joining the boundary of a plane figure $PQR$ to a point $A$ not in the same plane of $PQR$:


:


The plane figure $PQR$ is called the base of the cone.",Definition:Cone (Geometry)/Base,,false,"Consider a cone consisting of the set of all straight lines joining the boundary of a plane figure PQR to a point A not in the same plane of PQR:


:


The plane figure PQR is called the base of the cone.",Base
['Definitions/Right Circular Cones'],Definition:Base,"

Let $triangle AOB$ be a right-angled triangle such that $angle AOB$ is the right angle.

Let $K$ be the right circular cone formed by the rotation of $triangle AOB$ around $OB$.

Let $BC$ be the circle described by $B$.

The base of $K$ is the plane surface enclosed by the circle $BC$.


 
: 
:And the base is the circle described by the straight line which is carried round.
 ''
 


Category:Definitions/Right Circular Cones",Definition:Right Circular Cone/Base,,false,"

Let AOB be a right-angled triangle such that ∠ AOB is the right angle.

Let K be the right circular cone formed by the rotation of AOB around OB.

Let BC be the circle described by B.

The base of K is the plane surface enclosed by the circle BC.


 
: 
:And the base is the circle described by the straight line which is carried round.
 ”
 


Category:Definitions/Right Circular Cones",Base
['Definitions/Pyramids'],Definition:Base,":


The polygon of a pyramid to whose vertices the apex is joined is called the base of the pyramid.

In the above diagram, $ABCDE$ is the base of the pyramid $ABCDEQ$.",Definition:Pyramid/Base,,false,":


The polygon of a pyramid to whose vertices the apex is joined is called the base of the pyramid.

In the above diagram, ABCDE is the base of the pyramid ABCDEQ.",Base
['Definitions/Isosceles Triangles'],Definition:Base,":


The two (equal) vertices adjacent to the base of an isosceles triangle are called the base angles.

In the above diagram, $angle ABC$ and $angle ACB$ are the base angles.",Definition:Triangle (Geometry)/Isosceles/Base Angles,,false,":


The two (equal) vertices adjacent to the base of an isosceles triangle are called the base angles.

In the above diagram, ∠ ABC and ∠ ACB are the base angles.",Base
['Definitions/Number Bases'],Definition:Base,"=== Integers ===
Let $n in mathbb Z$ be an integer.

Let $b in mathbb Z$ be an integer such that $b > 1$.

By the Basis Representation Theorem, $n$ can be expressed uniquely in the form:

:$ds n = sum_{j mathop = 0}^m r_j b^j$

where:
:$m$ is such that $b^m le n < b^{m + 1}$
:all the $r_j$ are such that $0 le r_j < b$.

  
 

The number $b$ is known as the number base to which $n$ is represented.

$n$ is thus described as being (written) in base $b$.


Thus we can write $ds n = sum_{j mathop = 0}^m {r_j b^j}$ as:
:$left[ r_m r_{m - 1} ldots r_2 r_1 r_0 right]_b$
or, if the context is clear:
:${r_m r_{m - 1} ldots r_2 r_1 r_0}_b$

=== Real Numbers ===
Let $x in mathbb R$ be a real number such that $x ge 0$.

Let $b in mathbb N: b ge 2$.


See the definition of Basis Expansion for how we can express $x$ in the form:

:$x = left[ s cdotp d_1 d_2 d_3 ldots right]_b$

Then we express $m$ as for integers, and arrive at:
:$x = left[ r_m r_{m - 1} ldots r_2 r_1 r_0 cdotp d_1 d_2 d_3 ldots right]_b$

or, if the context is clear:
:$r_m r_{m - 1} ldots r_2 r_1 r_0 cdotp d_1 d_2 d_3 ldots_b$

=== Integer Part ===
Let $x in mathbb R$ be a real number such that $x ge 0$.

Let $b in mathbb N: b ge 2$.


In the basis expansion:
:$x = left[{r_m r_{m-1} ldots r_2 r_1 r_0 . d_1 d_2 d_3 ldots}right]_b$
the part $r_m r_{m-1} ldots r_2 r_1 r_0$ is known as the integer part.

=== Fractional Part ===
Let $x in mathbb R$ be a real number such that $x ge 0$.

Let $b in mathbb N: b ge 2$.


In the basis expansion:
:$x = left[ r_m r_{m - 1} ldots r_2 r_1 r_0 . d_1 d_2 d_3 ldots right]_b$
the part $.d_1 d_2 d_3 ldots$ is known as the fractional part.

=== Radix Point ===
Let $x in mathbb R$ be a real number such that $x ge 0$.

Let $b in mathbb N: b ge 2$.


In the basis expansion:
:$x = left[ r_m r_{m - 1} ldots r_2 r_1 r_0 cdotp d_1 d_2 d_3 ldots right]_b$
the dot that separates the integer part from the fractional part is called the radix point.",Definition:Number Base,,false,"=== Integers ===
Let n ∈ℤ be an integer.

Let b ∈ℤ be an integer such that b > 1.

By the Basis Representation Theorem, n can be expressed uniquely in the form:

:n = ∑_j  = 0^m r_j b^j

where:
:m is such that b^m ≤ n < b^m + 1
:all the r_j are such that 0 ≤ r_j < b.

  
 

The number b is known as the number base to which n is represented.

n is thus described as being (written) in base b.


Thus we can write n = ∑_j  = 0^m r_j b^j as:
:[ r_m r_m - 1… r_2 r_1 r_0 ]_b
or, if the context is clear:
:r_m r_m - 1… r_2 r_1 r_0_b

=== Real Numbers ===
Let x ∈ℝ be a real number such that x ≥ 0.

Let b ∈ℕ: b ≥ 2.


See the definition of Basis Expansion for how we can express x in the form:

:x = [ s  d_1 d_2 d_3 …]_b

Then we express m as for integers, and arrive at:
:x = [ r_m r_m - 1… r_2 r_1 r_0  d_1 d_2 d_3 …]_b

or, if the context is clear:
:r_m r_m - 1… r_2 r_1 r_0  d_1 d_2 d_3 …_b

=== Integer Part ===
Let x ∈ℝ be a real number such that x ≥ 0.

Let b ∈ℕ: b ≥ 2.


In the basis expansion:
:x = [r_m r_m-1… r_2 r_1 r_0 . d_1 d_2 d_3 …]_b
the part r_m r_m-1… r_2 r_1 r_0 is known as the integer part.

=== Fractional Part ===
Let x ∈ℝ be a real number such that x ≥ 0.

Let b ∈ℕ: b ≥ 2.


In the basis expansion:
:x = [ r_m r_m - 1… r_2 r_1 r_0 . d_1 d_2 d_3 …]_b
the part .d_1 d_2 d_3 … is known as the fractional part.

=== Radix Point ===
Let x ∈ℝ be a real number such that x ≥ 0.

Let b ∈ℕ: b ≥ 2.


In the basis expansion:
:x = [ r_m r_m - 1… r_2 r_1 r_0  d_1 d_2 d_3 …]_b
the dot that separates the integer part from the fractional part is called the radix point.",Base
['Definitions/Logarithms'],Definition:Base,"Let $log_a$ denote the logarithm function on whatever domain: $mathbb R$ or $mathbb C$.

The constant $a$ is known as the base of the logarithm.",Definition:Logarithm/Base,,false,"Let log_a denote the logarithm function on whatever domain: ℝ or ℂ.

The constant a is known as the base of the logarithm.",Base
"['Definitions/Topology', 'Definitions/Topological Bases']",Definition:Basis,"=== Analytic Basis ===
=== Definition 1 ===
Let $left( S, tau right)$ be a topological space.


An analytic basis for $tau$ is a subset $mathcal B subseteq tau$ such that:
:$ds forall U in tau: exists mathcal A subseteq mathcal B: U = bigcup mathcal A$


That is, such that for all $U in tau$, $U$ is a union of sets from $mathcal B$.

=== Definition 2 ===
Let $left( S, tau right)$ be a topological space.

Let $mathcal B subseteq tau$.


Then $mathcal B$ is an analytic basis for $tau$  if and only if :
:$forall U in tau: forall x in U: exists V in mathcal B: x in V subseteq U$

=== Synthetic Basis ===
Let $S$ be a set.


A synthetic basis on $S$ is a subset $mathcal B subseteq mathcal P left( S right)$ of the power set of $S$ such that:

 
 
 
 

That is, the intersection of any pair of elements of $mathcal B$ is a union of sets of $mathcal B$.",Definition:Basis (Topology),,false,"=== Analytic Basis ===
=== Definition 1 ===
Let ( S, τ) be a topological space.


An analytic basis for τ is a subset ℬ⊆τ such that:
:∀ U ∈τ: ∃𝒜⊆ℬ: U = ⋃𝒜


That is, such that for all U ∈τ, U is a union of sets from ℬ.

=== Definition 2 ===
Let ( S, τ) be a topological space.

Let ℬ⊆τ.


Then ℬ is an analytic basis for τ  if and only if :
:∀ U ∈τ: ∀ x ∈ U: ∃ V ∈ℬ: x ∈ V ⊆ U

=== Synthetic Basis ===
Let S be a set.


A synthetic basis on S is a subset ℬ⊆𝒫( S ) of the power set of S such that:

 
 
 
 

That is, the intersection of any pair of elements of ℬ is a union of sets of ℬ.",Basis
"['Definitions/Vector Spaces', 'Definitions/Linear Algebra', 'Definitions/Bases of Vector Spaces']",Definition:Basis,"Let $K$ be a division ring.

Let $left( G, +_G, circ right)_R$ be a vector space over $K$.


=== Definition 1 ===
Let $R$ be a division ring.

Let $left( G, +_G, circ right)_R$ be an vector space over $R$.


A basis of $G$ is a linearly independent subset of $G$ which is a generator for $G$.

=== Definition 2 ===
Let $R$ be a division ring.

Let $left( G, +_G, circ right)_R$ be an vector space over $R$.


A basis is a maximal linearly independent subset of $G$.",Definition:Basis of Vector Space,,false,"Let K be a division ring.

Let ( G, +_G, ∘)_R be a vector space over K.


=== Definition 1 ===
Let R be a division ring.

Let ( G, +_G, ∘)_R be an vector space over R.


A basis of G is a linearly independent subset of G which is a generator for G.

=== Definition 2 ===
Let R be a division ring.

Let ( G, +_G, ∘)_R be an vector space over R.


A basis is a maximal linearly independent subset of G.",Basis
"['Definitions/Linear Algebra', 'Definitions/Module Theory', 'Definitions/Bases of Modules']",Definition:Basis,"Let $R$ be a ring with unity.

Let $left( G, +_G, circ right)_R$ be a unitary $R$-module.


=== Definition 1 ===
Let $R$ be a ring with unity.

Let $left( G, +_G, circ right)_R$ be a unitary $R$-module.


A basis of $G$ is a linearly independent subset of $G$ which is a generator for $G$.

=== Definition 2 ===
Let $R$ be a ring with unity.

Let $left( G, +_G, circ right)_R$ be a unitary $R$-module.

Let $mathcal B = leftlangle b_i rightrangle_{i mathop in I}$ be a family of elements of $M$.

Let $Psi: R^{left( I right)} to M$ be the homomorphism given by Universal Property of Free Module on Set.


Then $mathcal B$ is a basis of $G$   $Psi$ is an isomorphism.",Definition:Basis of Module,,false,"Let R be a ring with unity.

Let ( G, +_G, ∘)_R be a unitary R-module.


=== Definition 1 ===
Let R be a ring with unity.

Let ( G, +_G, ∘)_R be a unitary R-module.


A basis of G is a linearly independent subset of G which is a generator for G.

=== Definition 2 ===
Let R be a ring with unity.

Let ( G, +_G, ∘)_R be a unitary R-module.

Let ℬ = ⟨ b_i ⟩_i ∈ I be a family of elements of M.

Let Ψ: R^( I )→ M be the homomorphism given by Universal Property of Free Module on Set.


Then ℬ is a basis of G   Ψ is an isomorphism.",Basis
['Definitions/Hilbert Spaces'],Definition:Basis,"Let $H$ be a Hilbert space.


A basis for $H$ is a maximal orthonormal subset of $H$.

Thus, $B$ is a basis for $H$  if and only if  for all orthonormal subsets $B'$ of $H$:

:$B subseteq B' implies B = B'$",Definition:Basis (Hilbert Space),,false,"Let H be a Hilbert space.


A basis for H is a maximal orthonormal subset of H.

Thus, B is a basis for H  if and only if  for all orthonormal subsets B' of H:

:B ⊆ B'  B = B'",Basis
['Definitions/Language Definitions'],Definition:Below,"In the context of numbers, below means less than.

Note that this applies to:
:the natural numbers $mathbb N$
:the integers $mathbb Z$
:the rational numbers $mathbb Q$
:the real numbers $mathbb R$

but specifically not the complex numbers $mathbb C$ because the complex numbers do not have a usual ordering.",Definition:Below (Number),,false,"In the context of numbers, below means less than.

Note that this applies to:
:the natural numbers ℕ
:the integers ℤ
:the rational numbers ℚ
:the real numbers ℝ

but specifically not the complex numbers ℂ because the complex numbers do not have a usual ordering.",Below
"['Definitions/Language Definitions', 'Definitions/Solid Geometry']",Definition:Below,"Let $a$ and $b$ be points in $3$-dimensional Euclidean space $mathbb R^3$.

Let $P$ be an arbitrary plane embedded in $S$ be distinguished and defined as horizontal.

Then:
:$a$ is below $b$
 if and only if :
:the height of $a$   $P$ is less than the height of $b$   $P$.",Definition:Below (Solid Geometry),,false,"Let a and b be points in 3-dimensional Euclidean space ℝ^3.

Let P be an arbitrary plane embedded in S be distinguished and defined as horizontal.

Then:
:a is below b
 if and only if :
:the height of a   P is less than the height of b   P.",Below
"['Definitions/Language Definitions', 'Definitions/Plane Geometry']",Definition:Below,"Let $a$ and $b$ be points in the cartesian plane $mathbb R^2$.

Then:
:$a$ is below $b$
 if and only if :
:the $y$ coordinate of $a$ is less than the $y$ coordinate of $b$.",Definition:Below (Plane Geometry),,false,"Let a and b be points in the cartesian plane ℝ^2.

Then:
:a is below b
 if and only if :
:the y coordinate of a is less than the y coordinate of b.",Below
"['Definitions/Bilinear Forms (Linear Algebra)', 'Definitions/Linear Forms (Linear Algebra)', 'Definitions/Linear Algebra', 'Definitions/Module Theory', 'Definitions/Vector Spaces', 'Definitions/Bilinear Forms']",Definition:Bilinear Form,"Let $R$ be a ring.

Let $R_R$ denote the $R$-module $R$.

Let $M_R$ be an $R$-module.


A bilinear form on $M_R$ is a bilinear mapping $B : M_R times M_R to R_R$.",Definition:Bilinear Form (Linear Algebra),,false,"Let R be a ring.

Let R_R denote the R-module R.

Let M_R be an R-module.


A bilinear form on M_R is a bilinear mapping B : M_R × M_R → R_R.",Bilinear Form
"['Definitions/Bilinear Forms (Polynomial Theory)', 'Definitions/Linear Forms (Polynomial Theory)', 'Definitions/Bilinear Forms']",Definition:Bilinear Form,A bilinear form is a linear form of order $2$.,Definition:Bilinear Form (Polynomial Theory),,false,A bilinear form is a linear form of order 2.,Bilinear Form
['Definitions/Euclidean Number Theory'],Definition:Binomial,"Let $a$ and $b$ be two (strictly) positive real numbers such that:
: $(1): quad dfrac a b notin mathbb Q$
: $(2): quad left({dfrac a b}right)^2 in mathbb Q$
where $mathbb Q$ denotes the set of rational numbers.


Then $a + b$ is a binomial.


 

=== First Binomial ===
Let $a$ and $b$ be two (strictly) positive real numbers such that $a + b$ is a binomial.


Then $a + b$ is a first binomial  if and only if :
: $(1): quad a in mathbb Q$
: $(2): quad dfrac {sqrt {a^2 - b^2} } a in mathbb Q$
where $mathbb Q$ denotes the set of rational numbers.


 
: 
:Given a rational straight line and a binomial, divided into its terms, such that the square on the greater term is greater than the square on the lesser by the square on a straight line commensurable in length with the greater, then, if the greater term be commensurable in length with the rational straight line set out, let the whole be called a first binomial straight line;
 ''
 

=== Second Binomial ===
Let $a$ and $b$ be two (strictly) positive real numbers such that $a + b$ is a binomial.


Then $a + b$ is a second binomial  if and only if :
: $(1): quad b in mathbb Q$
: $(2): quad dfrac {sqrt {a^2 - b^2}} a in mathbb Q$
where $mathbb Q$ denotes the set of rational numbers.


 
: 
:but if the lesser term be commensurable in length with the rational straight line set out, let the whole be called a second binomial;
 ''
 

=== Third Binomial ===
Let $a$ and $b$ be two (strictly) positive real numbers such that $a + b$ is a binomial.


Then $a + b$ is a third binomial  if and only if :
: $(1): quad a notin mathbb Q$
: $(2): quad b notin mathbb Q$
: $(3): quad dfrac {sqrt {a^2 - b^2}} a in mathbb Q$
where $mathbb Q$ denotes the set of rational numbers.


 
: 
:and if neither of the terms be commensurable in length with the rational straight line set out, let the whole be called a third binomial.
 ''
 

=== Fourth Binomial ===
Let $a$ and $b$ be two (strictly) positive real numbers such that $a + b$ is a binomial.


Then $a + b$ is a fourth binomial  if and only if :
: $(1): quad a in mathbb Q$
: $(2): quad dfrac {sqrt {a^2 - b^2}} a notin mathbb Q$
where $mathbb Q$ denotes the set of rational numbers.


 
: 
:Again, if the square on the greater term be greater than the square on the lesser by the square on a straight line incommensurable in length with the greater, then, if the greater term be commensurable in length with the rational straight line set out, let the whole be called a fourth binomial;
 ''
 

=== Fifth Binomial ===
Let $a$ and $b$ be two (strictly) positive real numbers such that $a + b$ is a binomial.


Then $a + b$ is a fifth binomial  if and only if :
: $(1): quad b in mathbb Q$
: $(2): quad dfrac {sqrt {a^2 - b^2}} a notin mathbb Q$

where $mathbb Q$ denotes the set of rational numbers.


 
: 
:if the lesser, a fifth binomial;
 ''
 

=== Sixth Binomial ===
Let $a$ and $b$ be two (strictly) positive real numbers such that $a + b$ is a binomial.


Then $a + b$ is a sixth binomial  if and only if :
: $(1): quad: a notin mathbb Q$
: $(2): quad: b notin mathbb Q$
: $(3): quad: dfrac {sqrt {a^2 - b^2}} a notin mathbb Q$
where $mathbb Q$ denotes the set of rational numbers.


 
: 
:and if neither, a sixth binomial.
 ''
 ",Definition:Binomial (Euclidean),,false,"Let a and b be two (strictly) positive real numbers such that:
: (1):    a b ∉ℚ
: (2):   ( a b)^2 ∈ℚ
where ℚ denotes the set of rational numbers.


Then a + b is a binomial.


 

=== First Binomial ===
Let a and b be two (strictly) positive real numbers such that a + b is a binomial.


Then a + b is a first binomial  if and only if :
: (1):    a ∈ℚ
: (2):   √(a^2 - b^2) a ∈ℚ
where ℚ denotes the set of rational numbers.


 
: 
:Given a rational straight line and a binomial, divided into its terms, such that the square on the greater term is greater than the square on the lesser by the square on a straight line commensurable in length with the greater, then, if the greater term be commensurable in length with the rational straight line set out, let the whole be called a first binomial straight line;
 ”
 

=== Second Binomial ===
Let a and b be two (strictly) positive real numbers such that a + b is a binomial.


Then a + b is a second binomial  if and only if :
: (1):    b ∈ℚ
: (2):   √(a^2 - b^2) a ∈ℚ
where ℚ denotes the set of rational numbers.


 
: 
:but if the lesser term be commensurable in length with the rational straight line set out, let the whole be called a second binomial;
 ”
 

=== Third Binomial ===
Let a and b be two (strictly) positive real numbers such that a + b is a binomial.


Then a + b is a third binomial  if and only if :
: (1):    a ∉ℚ
: (2):    b ∉ℚ
: (3):   √(a^2 - b^2) a ∈ℚ
where ℚ denotes the set of rational numbers.


 
: 
:and if neither of the terms be commensurable in length with the rational straight line set out, let the whole be called a third binomial.
 ”
 

=== Fourth Binomial ===
Let a and b be two (strictly) positive real numbers such that a + b is a binomial.


Then a + b is a fourth binomial  if and only if :
: (1):    a ∈ℚ
: (2):   √(a^2 - b^2) a ∉ℚ
where ℚ denotes the set of rational numbers.


 
: 
:Again, if the square on the greater term be greater than the square on the lesser by the square on a straight line incommensurable in length with the greater, then, if the greater term be commensurable in length with the rational straight line set out, let the whole be called a fourth binomial;
 ”
 

=== Fifth Binomial ===
Let a and b be two (strictly) positive real numbers such that a + b is a binomial.


Then a + b is a fifth binomial  if and only if :
: (1):    b ∈ℚ
: (2):   √(a^2 - b^2) a ∉ℚ

where ℚ denotes the set of rational numbers.


 
: 
:if the lesser, a fifth binomial;
 ”
 

=== Sixth Binomial ===
Let a and b be two (strictly) positive real numbers such that a + b is a binomial.


Then a + b is a sixth binomial  if and only if :
: (1):   : a ∉ℚ
: (2):   : b ∉ℚ
: (3):   : √(a^2 - b^2) a ∉ℚ
where ℚ denotes the set of rational numbers.


 
: 
:and if neither, a sixth binomial.
 ”
 ",Binomial
"['Definitions/Binomials (Algebra)', 'Definitions/Algebra']",Definition:Binomial,A binomial is an expression which has $2$ terms.,Definition:Binomial (Algebra),expression,true,A binomial is an expression which has 2 terms.,Binomial
['Definitions/Binomial Coefficients'],Definition:Binomial,"=== Definition 1 ===
Let $n in mathbb Z_{ge 0}$ and $k in mathbb Z$.

Then the binomial coefficient $dbinom n k$ is defined as:

:$dbinom n k = begin {cases} dfrac {n!} {k! left( n - k right)!} & : 0 le k le n \ & \ 0 & : text { otherwise } end{cases}$

where $n!$ denotes the factorial of $n$.

=== Definition 2 ===
Let $n in mathbb Z_{ge 0}$ and $k in mathbb Z$.

The number of different ways $k$ objects can be chosen (irrespective of order) from a set of $n$ objects is denoted:
:$dbinom n k$

This number $dbinom n k$ is known as a binomial coefficient.

=== Definition 3 ===
Let $n in mathbb Z_{ge 0}$ and $k in mathbb Z$.

Then the binomial coefficient $dbinom n k$ is defined as the coefficient of the term $a^k b^{n - k}$ in the expansion of $left( a + b right)^n$.",Definition:Binomial Coefficient,,false,"=== Definition 1 ===
Let n ∈ℤ_≥ 0 and k ∈ℤ.

Then the binomial coefficient n k is defined as:

:n k = n!k! ( n - k )!    : 0 ≤ k ≤ n 
   
 0     :  otherwise

where n! denotes the factorial of n.

=== Definition 2 ===
Let n ∈ℤ_≥ 0 and k ∈ℤ.

The number of different ways k objects can be chosen (irrespective of order) from a set of n objects is denoted:
:n k

This number n k is known as a binomial coefficient.

=== Definition 3 ===
Let n ∈ℤ_≥ 0 and k ∈ℤ.

Then the binomial coefficient n k is defined as the coefficient of the term a^k b^n - k in the expansion of ( a + b )^n.",Binomial
"['Definitions/Binomial Distribution', 'Definitions/Examples of Probability Distributions']",Definition:Binomial,"Let $X$ be a discrete random variable on a probability space $left( Omega, unicode{x3a3}, Pr right)$.


Then $X$ has the binomial distribution with parameters $n$ and $p$  if and only if :

:$mathrm {Img} left( X right) = leftlbrace 0, 1, ldots, n rightrbrace$

:$Pr left(   right){X = k} = dbinom n k p^k left( 1 - p right)^{n - k}$

where $0 le p le 1$.


Note that the binomial distribution gives rise to a probability mass function satisfying $Pr left(   right)Omega = 1$, because:
:$ds sum_{k mathop in mathbb Z} dbinom n k p^k left( 1 - p right)^{n - k} = left( p + left( 1 - p right)  right)^n = 1$

This is apparent from the Binomial Theorem.


It is written:
:$X sim mathrm B left( n,   right)p$",Definition:Binomial Distribution,,false,"Let X be a discrete random variable on a probability space ( Ω, x3a3, ).


Then X has the binomial distribution with parameters n and p  if and only if :

:Img( X ) = { 0, 1, …, n }

:(   )X = k =  n k p^k ( 1 - p )^n - k

where 0 ≤ p ≤ 1.


Note that the binomial distribution gives rise to a probability mass function satisfying (   )Ω = 1, because:
:∑_k ∈ℤ n k p^k ( 1 - p )^n - k = ( p + ( 1 - p )  )^n = 1

This is apparent from the Binomial Theorem.


It is written:
:X ∼B( n,   )p",Binomial
"['Definitions/Bottom of Lattice', 'Definitions/Lattice Theory']",Definition:Bottom,"Let $left( S, vee, wedge, preceq right)$ be a lattice.


=== Definition 1 ===
Let $left( S, vee, wedge, preceq right)$ be a lattice.

Let $S$ admit a smallest element $bot$.


Then $bot$ is called the bottom of $S$.

=== Definition 2 ===
Let $left( S, vee, wedge, preceq right)$ be a lattice.

Let $vee$ have an identity element $bot$.


Then $bot$ is called the bottom of $S$.",Definition:Bottom of Lattice,,false,"Let ( S, ∨, ∧, ≼) be a lattice.


=== Definition 1 ===
Let ( S, ∨, ∧, ≼) be a lattice.

Let S admit a smallest element .


Then  is called the bottom of S.

=== Definition 2 ===
Let ( S, ∨, ∧, ≼) be a lattice.

Let ∨ have an identity element .


Then  is called the bottom of S.",Bottom
['Definitions/Bottom'],Definition:Bottom,"Bottom is a constant of propositional logic interpreted to mean the canonical, undoubted contradiction whose falsehood nobody could possibly ever question.

The symbol used is $bot$.",Definition:Bottom (Logic),,false,"Bottom is a constant of propositional logic interpreted to mean the canonical, undoubted contradiction whose falsehood nobody could possibly ever question.

The symbol used is .",Bottom
['Definitions/Geometry'],Definition:Boundary," 

For example, the endpoints of a line segment are its boundaries.


=== Containment ===
A geometric figure is said to be contained by its boundary or boundaries.

 ",Definition:Boundary (Geometry),,false," 

For example, the endpoints of a line segment are its boundaries.


=== Containment ===
A geometric figure is said to be contained by its boundary or boundaries.

 ",Boundary
"['Definitions/Set Boundaries', 'Definitions/Topology']",Definition:Boundary,"Let $T = left( S, tau right)$ be a topological space.

Let $H subseteq S$.


=== Definition from Closure and Interior ===
Let $T = left( S, tau right)$ be a topological space.

Let $H subseteq S$.


The boundary of $H$ consists of all the points in the closure of $H$ which are not in the interior of $H$.

Thus, the boundary of $H$ is defined as:
:$partial H := H^- setminus H^circ$
where $H^-$ denotes the closure and $H^circ$ the interior of $H$.

=== Definition from Neighborhood ===
Let $T = left( S, tau right)$ be a topological space.

Let $H subseteq S$.


 

$x in S$ is a boundary point of $H$  if and only if  every neighborhood $N$ of $x$ satisfies:
:$H cap N ne varnothing$
and
:$overline H cap N ne varnothing$
where $overline H$ is the complement of $H$ in $S$.

The boundary of $H$ consists of all the boundary point of $H$.

=== Definition from Intersection of Closure with Closure of Complement ===
Let $T = left( S, tau right)$ be a topological space.

Let $H subseteq S$.


The boundary of $H$ is the intersection of the closure of $H$ with the closure of the complement of $H$ in $T$:

:$partial H = H^- cap left( overline H right)^-$

=== Definition from Closure and Exterior ===
Let $T = left( S, tau right)$ be a topological space.

Let $H subseteq S$.


The boundary of $H$ consists of all the points in $H$ which are not in either the interior or exterior of $H$.

Thus, the boundary of $H$ is defined as:
:$partial H := H setminus left( H^circ cup H^e right)$
where:
:$H^circ$ denotes the interior of $H$
:$H^e$ denotes the exterior of $H$.",Definition:Boundary (Topology),,false,"Let T = ( S, τ) be a topological space.

Let H ⊆ S.


=== Definition from Closure and Interior ===
Let T = ( S, τ) be a topological space.

Let H ⊆ S.


The boundary of H consists of all the points in the closure of H which are not in the interior of H.

Thus, the boundary of H is defined as:
:∂ H := H^- ∖ H^∘
where H^- denotes the closure and H^∘ the interior of H.

=== Definition from Neighborhood ===
Let T = ( S, τ) be a topological space.

Let H ⊆ S.


 

x ∈ S is a boundary point of H  if and only if  every neighborhood N of x satisfies:
:H ∩ N ∅
and
:H∩ N ∅
where H is the complement of H in S.

The boundary of H consists of all the boundary point of H.

=== Definition from Intersection of Closure with Closure of Complement ===
Let T = ( S, τ) be a topological space.

Let H ⊆ S.


The boundary of H is the intersection of the closure of H with the closure of the complement of H in T:

:∂ H = H^- ∩( H)^-

=== Definition from Closure and Exterior ===
Let T = ( S, τ) be a topological space.

Let H ⊆ S.


The boundary of H consists of all the points in H which are not in either the interior or exterior of H.

Thus, the boundary of H is defined as:
:∂ H := H ∖( H^∘∪ H^e )
where:
:H^∘ denotes the interior of H
:H^e denotes the exterior of H.",Boundary
"['Definitions/Graph Theory', 'Definitions/Boundaries (Graph Theory)']",Definition:Boundary,"=== Simple Graph ===
Let $G = left( V, E right)$ be a simple graph.

Let $v in V$ be a vertex of $G$.


Then the boundary of $v$ is the set of all vertices of $G$ which are adjacent to $v$:
:$B left(   right)v = leftlbrace u in V: leftlbrace u, v rightrbrace in E rightrbrace$",Definition:Boundary (Graph Theory),,false,"=== Simple Graph ===
Let G = ( V, E ) be a simple graph.

Let v ∈ V be a vertex of G.


Then the boundary of v is the set of all vertices of G which are adjacent to v:
:B (   )v = { u ∈ V: { u, v }∈ E }",Boundary
"['Definitions/Bounded Sets', 'Definitions/Ordered Sets', 'Definitions/Boundedness']",Definition:Bounded,"Let $left( S, preceq right)$ be an ordered set.

Let $T subseteq S$ be both bounded below and bounded above in $S$.


Then $T$ is bounded in $S$.


=== Subset of Real Numbers ===

The concept is usually encountered where $left( S, preceq right)$ is the set of real numbers under the usual ordering $left( mathbb R, le right)$:

=== Definition 1 ===
Let $mathbb R$ be the set of real numbers.


Let $T subseteq mathbb R$ be both bounded below and bounded above in $mathbb R$.


Then $T$ is bounded in $mathbb R$.

=== Definition 2 ===
Let $mathbb R$ be the set of real numbers.


Let $T subseteq mathbb R$ be a subset of $mathbb R$ such that:
:$exists K in mathbb R: forall x in T: leftlvert x rightrvert le K$
where $leftlvert x rightrvert$ denotes the absolute value of $x$.


Then $T$ is bounded in $mathbb R$.",Definition:Bounded Set,,false,"Let ( S, ≼) be an ordered set.

Let T ⊆ S be both bounded below and bounded above in S.


Then T is bounded in S.


=== Subset of Real Numbers ===

The concept is usually encountered where ( S, ≼) is the set of real numbers under the usual ordering ( ℝ, ≤):

=== Definition 1 ===
Let ℝ be the set of real numbers.


Let T ⊆ℝ be both bounded below and bounded above in ℝ.


Then T is bounded in ℝ.

=== Definition 2 ===
Let ℝ be the set of real numbers.


Let T ⊆ℝ be a subset of ℝ such that:
:∃ K ∈ℝ: ∀ x ∈ T: | x |≤ K
where | x | denotes the absolute value of x.


Then T is bounded in ℝ.",Bounded
"['Definitions/Bounded Sets of Real Numbers', 'Definitions/Bounded Sets', 'Definitions/Real Numbers']",Definition:Bounded,"=== Definition 1 ===
Let $mathbb R$ be the set of real numbers.


Let $T subseteq mathbb R$ be both bounded below and bounded above in $mathbb R$.


Then $T$ is bounded in $mathbb R$.

=== Definition 2 ===
Let $mathbb R$ be the set of real numbers.


Let $T subseteq mathbb R$ be a subset of $mathbb R$ such that:
:$exists K in mathbb R: forall x in T: leftlvert x rightrvert le K$
where $leftlvert x rightrvert$ denotes the absolute value of $x$.


Then $T$ is bounded in $mathbb R$.",Definition:Bounded Set/Real Numbers,,false,"=== Definition 1 ===
Let ℝ be the set of real numbers.


Let T ⊆ℝ be both bounded below and bounded above in ℝ.


Then T is bounded in ℝ.

=== Definition 2 ===
Let ℝ be the set of real numbers.


Let T ⊆ℝ be a subset of ℝ such that:
:∃ K ∈ℝ: ∀ x ∈ T: | x |≤ K
where | x | denotes the absolute value of x.


Then T is bounded in ℝ.",Bounded
['Definitions/Bounded Classes'],Definition:Bounded,"Let $B$ be a class.


=== Bounded by Set ===
Let $B$ be a class.

Let $x$ be a set.

$B$ is bounded by $x$  if and only if :
:every element of $B$ is a subset of $x$.

=== Bounded Subset of Class ===
Let $B$ be a class.

Let $B$ be a subclass of a class $A$.

Then $B$ is a bounded subset of $A$  if and only if :
:there exists a set $x in A$ such that $B$ is bounded by $x$ 


That is,  if and only if  every element of $B$ is a subset of $x$.",Definition:Bounded Class,,false,"Let B be a class.


=== Bounded by Set ===
Let B be a class.

Let x be a set.

B is bounded by x  if and only if :
:every element of B is a subset of x.

=== Bounded Subset of Class ===
Let B be a class.

Let B be a subclass of a class A.

Then B is a bounded subset of A  if and only if :
:there exists a set x ∈ A such that B is bounded by x 


That is,  if and only if  every element of B is a subset of x.",Bounded
"['Definitions/Bounded Mappings', 'Definitions/Mappings', 'Definitions/Boundedness']",Definition:Bounded,"Let $S$ be a set.

Let $left( T, preceq right)$ be an ordered set.

Let $f: S to T$ be a mapping.

Let the image of $f$ be bounded.


Then $f$ is bounded.


That is, $f$ is bounded  if and only if  it is both bounded above and bounded below.


=== Real-Valued Function ===
Let $f: S to mathbb R$ be a real-valued function.


=== Definition 1 ===
Let $f: S to mathbb R$ be a real-valued function.


$f$ is bounded on $S$  if and only if :
:$f$ is bounded above on $S$
and also:
:$f$ is bounded below on $S$.

=== Definition 2 ===
Let $f: S to mathbb R$ be a real-valued function.


$f$ is bounded on $S$  if and only if :
:$exists K in mathbb R_{ge 0}: forall x in S: leftlvert f left(   right)x rightrvert le K$
where $leftlvert f left(   right)x rightrvert$ denotes the absolute value of $f left(   right)x$.

=== Definition 3 ===
Let $f: S to mathbb R$ be a real-valued function.


$f$ is bounded on $S$  if and only if :
:$exists a, b in mathbb R_{ge 0}: forall x in S: f left(   right)x in left[ a ,.,.,   right]b$
where $left[ a ,.,.,   right]b$ denotes the (closed) real interval from $a$ to $b$.

=== Function Attaining its Bounds ===
Let $f: S to mathbb R$ be a bounded real-valued function.

Let $T$ be a subset of $S$.

Suppose that:
:$exists a, b in T: forall x in S: f left(   right)a le f left(   right)x le f left(   right)b$


Then $f$ attains its bounds on $T$.

=== Complex-Valued Function ===
Let $f: S to mathbb C$ be a complex-valued function.


Then $f$ is bounded  if and only if  the real-valued function $leftlvert f rightrvert: S to mathbb R$ is bounded, where $leftlvert f rightrvert$ is the modulus of $f$.


That is, $f$ is bounded if there is a constant $K ge 0$ such that $leftlvert f left(   right)z rightrvert le K$ for all $z in S$.


=== Unbounded ===
Let $f: S to mathbb C$ be a complex-valued function.


Then $f$ is unbounded  if and only if  $f$ is not bounded.


That is, $f$ is unbounded if there does not exist a constant $K ge 0$ such that $leftlvert f left( z right) rightrvert le K$ for all $z in S$.

=== Normed Division Ring ===
Let $left( R, leftlVert ,cdot, rightrVert right)$ be a normed division ring.

Let $f: S to R$ be a mapping from $S$ into $R$.

Then $f$ is bounded  if and only if  the real-valued function $leftlVert ,cdot, rightrVert circ f: S to mathbb R$ is bounded, where $leftlVert ,cdot, rightrVert circ f$ is the composite of $leftlVert ,cdot, rightrVert$ and $f$.


That is, $f$ is bounded if there is a constant $K in mathbb R_{ge 0}$ such that $leftlVert f left( s right) rightrVert le K$ for all $s in S$.

=== Metric Space ===
Let $M$ be a metric space.

Let $f: X to M$ be a mapping from any set $X$ into $M$.


Then $f$ is a bounded mapping  if and only if  $f left[ X right]$ is bounded in $M$.

=== Normed Vector Space ===
Let $left( R, leftlVert , cdot , rightrVert  right)$ be a normed division ring.

Let $left( X, leftlVert , cdot , rightrVert  right)$ be a normed vector space over $R$. 

Let $S$ be a set.

Let $f : S to X$ be a mapping.


We say that $f$ is bounded  if and only if  there exists a real number $M > 0$ such that:
:$leftlVert f left(   right)x rightrVert le M$ for each $x in S$.",Definition:Bounded Mapping,,false,"Let S be a set.

Let ( T, ≼) be an ordered set.

Let f: S → T be a mapping.

Let the image of f be bounded.


Then f is bounded.


That is, f is bounded  if and only if  it is both bounded above and bounded below.


=== Real-Valued Function ===
Let f: S →ℝ be a real-valued function.


=== Definition 1 ===
Let f: S →ℝ be a real-valued function.


f is bounded on S  if and only if :
:f is bounded above on S
and also:
:f is bounded below on S.

=== Definition 2 ===
Let f: S →ℝ be a real-valued function.


f is bounded on S  if and only if :
:∃ K ∈ℝ_≥ 0: ∀ x ∈ S: | f (   )x |≤ K
where | f (   )x | denotes the absolute value of f (   )x.

=== Definition 3 ===
Let f: S →ℝ be a real-valued function.


f is bounded on S  if and only if :
:∃ a, b ∈ℝ_≥ 0: ∀ x ∈ S: f (   )x ∈[ a  . . ]b
where [ a  . . ]b denotes the (closed) real interval from a to b.

=== Function Attaining its Bounds ===
Let f: S →ℝ be a bounded real-valued function.

Let T be a subset of S.

Suppose that:
:∃ a, b ∈ T: ∀ x ∈ S: f (   )a ≤ f (   )x ≤ f (   )b


Then f attains its bounds on T.

=== Complex-Valued Function ===
Let f: S →ℂ be a complex-valued function.


Then f is bounded  if and only if  the real-valued function | f |: S →ℝ is bounded, where | f | is the modulus of f.


That is, f is bounded if there is a constant K ≥ 0 such that | f (   )z |≤ K for all z ∈ S.


=== Unbounded ===
Let f: S →ℂ be a complex-valued function.


Then f is unbounded  if and only if  f is not bounded.


That is, f is unbounded if there does not exist a constant K ≥ 0 such that | f ( z ) |≤ K for all z ∈ S.

=== Normed Division Ring ===
Let ( R, ‖ · ‖) be a normed division ring.

Let f: S → R be a mapping from S into R.

Then f is bounded  if and only if  the real-valued function ‖ · ‖∘ f: S →ℝ is bounded, where ‖ · ‖∘ f is the composite of ‖ · ‖ and f.


That is, f is bounded if there is a constant K ∈ℝ_≥ 0 such that ‖ f ( s ) ‖≤ K for all s ∈ S.

=== Metric Space ===
Let M be a metric space.

Let f: X → M be a mapping from any set X into M.


Then f is a bounded mapping  if and only if  f [ X ] is bounded in M.

=== Normed Vector Space ===
Let ( R, ‖ · ‖) be a normed division ring.

Let ( X, ‖ · ‖) be a normed vector space over R. 

Let S be a set.

Let f : S → X be a mapping.


We say that f is bounded  if and only if  there exists a real number M > 0 such that:
:‖ f (   )x ‖≤ M for each x ∈ S.",Bounded
"['Definitions/Bounded Real-Valued Functions', 'Definitions/Real-Valued Functions', 'Definitions/Bounded Mappings']",Definition:Bounded,"Let $f: S to mathbb R$ be a real-valued function.


=== Definition 1 ===
Let $f: S to mathbb R$ be a real-valued function.


$f$ is bounded on $S$  if and only if :
:$f$ is bounded above on $S$
and also:
:$f$ is bounded below on $S$.

=== Definition 2 ===
Let $f: S to mathbb R$ be a real-valued function.


$f$ is bounded on $S$  if and only if :
:$exists K in mathbb R_{ge 0}: forall x in S: leftlvert f left(   right)x rightrvert le K$
where $leftlvert f left(   right)x rightrvert$ denotes the absolute value of $f left(   right)x$.

=== Definition 3 ===
Let $f: S to mathbb R$ be a real-valued function.


$f$ is bounded on $S$  if and only if :
:$exists a, b in mathbb R_{ge 0}: forall x in S: f left(   right)x in left[ a ,.,.,   right]b$
where $left[ a ,.,.,   right]b$ denotes the (closed) real interval from $a$ to $b$.

=== Function Attaining its Bounds ===
Let $f: S to mathbb R$ be a bounded real-valued function.

Let $T$ be a subset of $S$.

Suppose that:
:$exists a, b in T: forall x in S: f left(   right)a le f left(   right)x le f left(   right)b$


Then $f$ attains its bounds on $T$.",Definition:Bounded Mapping/Real-Valued,,false,"Let f: S →ℝ be a real-valued function.


=== Definition 1 ===
Let f: S →ℝ be a real-valued function.


f is bounded on S  if and only if :
:f is bounded above on S
and also:
:f is bounded below on S.

=== Definition 2 ===
Let f: S →ℝ be a real-valued function.


f is bounded on S  if and only if :
:∃ K ∈ℝ_≥ 0: ∀ x ∈ S: | f (   )x |≤ K
where | f (   )x | denotes the absolute value of f (   )x.

=== Definition 3 ===
Let f: S →ℝ be a real-valued function.


f is bounded on S  if and only if :
:∃ a, b ∈ℝ_≥ 0: ∀ x ∈ S: f (   )x ∈[ a  . . ]b
where [ a  . . ]b denotes the (closed) real interval from a to b.

=== Function Attaining its Bounds ===
Let f: S →ℝ be a bounded real-valued function.

Let T be a subset of S.

Suppose that:
:∃ a, b ∈ T: ∀ x ∈ S: f (   )a ≤ f (   )x ≤ f (   )b


Then f attains its bounds on T.",Bounded
"['Definitions/Bounded Sequences', 'Definitions/Boundedness', 'Definitions/Sequences']",Definition:Bounded,"A special case of a bounded mapping is a bounded sequence, where the domain of the mapping is $mathbb N$.


Let $left( T, preceq right)$ be an ordered set.

Let $leftlangle x_n rightrangle$ be a sequence in $T$.


Then $leftlangle x_n rightrangle$ is bounded  if and only if  $exists m, M in T$ such that $forall i in mathbb N$:
:$(1): quad m preceq x_i$
:$(2): quad x_i preceq M$


That is,  if and only if  it is bounded above and bounded below.


=== Real Sequence ===

The concept is usually encountered where $left( T, preceq right)$ is the set of real numbers under the usual ordering: $left( mathbb R, le right)$:

Let $leftlangle x_n rightrangle$ be a real sequence.


Then $leftlangle x_n rightrangle$ is bounded  if and only if  $exists m, M in mathbb R$ such that $forall i in mathbb N$:
:$m le x_i$
:$x_i le M$


That is,  if and only if  it is bounded above and bounded below.


=== Unbounded ===
Let $leftlangle x_n rightrangle$ be a real sequence.


$leftlangle x_n rightrangle$ is unbounded  if and only if  it is not bounded.

=== Complex Sequence ===
Let $leftlangle z_n rightrangle$ be a complex sequence.


Then $leftlangle z_n rightrangle$ is bounded  if and only if :
:$exists M in mathbb R$ such that $forall i in mathbb N: leftlvert z_i rightrvert le M$
where $leftlvert z_i rightrvert$ denotes the complex modulus of $z_i$.

=== Normed Division Ring ===
Let $left( R, leftlVert , cdot , rightrVert  right)$ be a normed division ring.

Let $leftlangle x_n rightrangle$ be a sequence in $R$.

Then $leftlangle x_n rightrangle$ is bounded  if and only if :
:$exists K in mathbb R$ such that $forall n in mathbb N: leftlVert x_n rightrVert le K$


=== Unbounded ===
Let $left( R, leftlVert , cdot , rightrVert  right)$ be a normed division ring.

Let $leftlangle x_n rightrangle$ be a sequence in $R$.


$leftlangle x_n rightrangle$ is unbounded  if and only if  it is not bounded.

=== Normed Vector Space ===
Let $left( X, leftlVert , cdot , rightrVert  right)$ be a normed vector space.

Let $leftlangle x_n rightrangle$ be a sequence in $X$.


Then $leftlangle x_n rightrangle$ is bounded  if and only if :
:$exists K in mathbb R$ such that $forall n in mathbb N: leftlVert x_n rightrVert le K$


=== Unbounded ===
Let $left( X, leftlVert , cdot , rightrVert  right)$ be a normed vector space.

Let $leftlangle x_n rightrangle$ be a sequence in $R$.


$leftlangle x_n rightrangle$ is unbounded  if and only if  it is not bounded.

=== Metric Space ===
Let $M$ be a metric space.

Let $leftlangle x_n rightrangle$ be a sequence in $M$.

Then $leftlangle x_n rightrangle$ is a bounded sequence  if and only if  $leftlangle x_n rightrangle$ is bounded in $M$.

That is:
:$exists K in mathbb R: forall n, m in mathbb N: d left(   right){x_n, x_m} le K$


=== Unbounded ===
Let $M$ be a metric space.

Let $leftlangle x_n rightrangle$ be a sequence in $M$.


$leftlangle x_n rightrangle$ is unbounded  if and only if  it is not bounded.

Category:Definitions/Bounded Sequences
Category:Definitions/Metric Spaces",Definition:Bounded Sequence,,false,"A special case of a bounded mapping is a bounded sequence, where the domain of the mapping is ℕ.


Let ( T, ≼) be an ordered set.

Let ⟨ x_n ⟩ be a sequence in T.


Then ⟨ x_n ⟩ is bounded  if and only if  ∃ m, M ∈ T such that ∀ i ∈ℕ:
:(1):    m ≼ x_i
:(2):    x_i ≼ M


That is,  if and only if  it is bounded above and bounded below.


=== Real Sequence ===

The concept is usually encountered where ( T, ≼) is the set of real numbers under the usual ordering: ( ℝ, ≤):

Let ⟨ x_n ⟩ be a real sequence.


Then ⟨ x_n ⟩ is bounded  if and only if  ∃ m, M ∈ℝ such that ∀ i ∈ℕ:
:m ≤ x_i
:x_i ≤ M


That is,  if and only if  it is bounded above and bounded below.


=== Unbounded ===
Let ⟨ x_n ⟩ be a real sequence.


⟨ x_n ⟩ is unbounded  if and only if  it is not bounded.

=== Complex Sequence ===
Let ⟨ z_n ⟩ be a complex sequence.


Then ⟨ z_n ⟩ is bounded  if and only if :
:∃ M ∈ℝ such that ∀ i ∈ℕ: | z_i |≤ M
where | z_i | denotes the complex modulus of z_i.

=== Normed Division Ring ===
Let ( R, ‖ · ‖) be a normed division ring.

Let ⟨ x_n ⟩ be a sequence in R.

Then ⟨ x_n ⟩ is bounded  if and only if :
:∃ K ∈ℝ such that ∀ n ∈ℕ: ‖ x_n ‖≤ K


=== Unbounded ===
Let ( R, ‖ · ‖) be a normed division ring.

Let ⟨ x_n ⟩ be a sequence in R.


⟨ x_n ⟩ is unbounded  if and only if  it is not bounded.

=== Normed Vector Space ===
Let ( X, ‖ · ‖) be a normed vector space.

Let ⟨ x_n ⟩ be a sequence in X.


Then ⟨ x_n ⟩ is bounded  if and only if :
:∃ K ∈ℝ such that ∀ n ∈ℕ: ‖ x_n ‖≤ K


=== Unbounded ===
Let ( X, ‖ · ‖) be a normed vector space.

Let ⟨ x_n ⟩ be a sequence in R.


⟨ x_n ⟩ is unbounded  if and only if  it is not bounded.

=== Metric Space ===
Let M be a metric space.

Let ⟨ x_n ⟩ be a sequence in M.

Then ⟨ x_n ⟩ is a bounded sequence  if and only if  ⟨ x_n ⟩ is bounded in M.

That is:
:∃ K ∈ℝ: ∀ n, m ∈ℕ: d (   )x_n, x_m≤ K


=== Unbounded ===
Let M be a metric space.

Let ⟨ x_n ⟩ be a sequence in M.


⟨ x_n ⟩ is unbounded  if and only if  it is not bounded.

Category:Definitions/Bounded Sequences
Category:Definitions/Metric Spaces",Bounded
"['Definitions/Bounded Real Sequences', 'Definitions/Bounded Sequences', 'Definitions/Real Sequences']",Definition:Bounded,"Let $leftlangle x_n rightrangle$ be a real sequence.


Then $leftlangle x_n rightrangle$ is bounded  if and only if  $exists m, M in mathbb R$ such that $forall i in mathbb N$:
:$m le x_i$
:$x_i le M$


That is,  if and only if  it is bounded above and bounded below.


=== Unbounded ===
Let $leftlangle x_n rightrangle$ be a real sequence.


$leftlangle x_n rightrangle$ is unbounded  if and only if  it is not bounded.",Definition:Bounded Sequence/Real,,false,"Let ⟨ x_n ⟩ be a real sequence.


Then ⟨ x_n ⟩ is bounded  if and only if  ∃ m, M ∈ℝ such that ∀ i ∈ℕ:
:m ≤ x_i
:x_i ≤ M


That is,  if and only if  it is bounded above and bounded below.


=== Unbounded ===
Let ⟨ x_n ⟩ be a real sequence.


⟨ x_n ⟩ is unbounded  if and only if  it is not bounded.",Bounded
['Definitions/Geometry'],Definition:Bounded,"Let $D subseteq mathbb R^2$ be a subset of the plane.

$D$ is bounded  if and only if  there exists a circle in the plane which completely encloses $D$.",Definition:Bounded Region of Plane,,false,"Let D ⊆ℝ^2 be a subset of the plane.

D is bounded  if and only if  there exists a circle in the plane which completely encloses D.",Bounded
"['Definitions/Bounded Metric Spaces', 'Definitions/Metric Spaces']",Definition:Bounded,"Let $M = left( A, d right)$ be a metric space.

Let $M' = left( B, d_B right)$ be a subspace of $M$.


=== Definition 1 ===
Let $M = left( A, d right)$ be a metric space.

Let $M' = left( B, d_B right)$ be a subspace of $M$.


$M'$ is bounded (in $M$)  if and only if :
:$exists a in A, K in mathbb R: forall x in B: d left(   right){x, a} le K$

That is, there exists an element of $A$ within a finite distance of all elements of $B$.

=== Definition 2 ===
Let $M = left( A, d right)$ be a metric space.

Let $M' = left( B, d_B right)$ be a subspace of $M$.


$M'$ is bounded (in $M$)  if and only if :
:$exists K in mathbb R: forall x, y in M': d_B left(   right){x, y} le K$

That is, there exists a finite distance such that all pairs of elements of $B$ are within that distance.

=== Definition 3 ===
Let $M = left( A, d right)$ be a metric space.

Let $M' = left( B, d_B right)$ be a subspace of $M$.


$M'$ is bounded (in $M$)  if and only if :
:$exists x in A, epsilon in mathbb R_{>0}: B subseteq B_epsilon left(   right)x$

where $B_epsilon left(   right)x$ is the open $epsilon$-ball of $x$.


That is, $M'$ can be fitted inside an open ball.

=== Definition 4 ===
Let $M = left( A, d right)$ be a metric space.

Let $M' = left( B, d_B right)$ be a subspace of $M$.

Let $a' in A$.


$M'$ is bounded (in $M$)  if and only if :
:$exists K in mathbb R: forall x in B: d left(   right){x, a'} le K$",Definition:Bounded Metric Space,,false,"Let M = ( A, d ) be a metric space.

Let M' = ( B, d_B ) be a subspace of M.


=== Definition 1 ===
Let M = ( A, d ) be a metric space.

Let M' = ( B, d_B ) be a subspace of M.


M' is bounded (in M)  if and only if :
:∃ a ∈ A, K ∈ℝ: ∀ x ∈ B: d (   )x, a≤ K

That is, there exists an element of A within a finite distance of all elements of B.

=== Definition 2 ===
Let M = ( A, d ) be a metric space.

Let M' = ( B, d_B ) be a subspace of M.


M' is bounded (in M)  if and only if :
:∃ K ∈ℝ: ∀ x, y ∈ M': d_B (   )x, y≤ K

That is, there exists a finite distance such that all pairs of elements of B are within that distance.

=== Definition 3 ===
Let M = ( A, d ) be a metric space.

Let M' = ( B, d_B ) be a subspace of M.


M' is bounded (in M)  if and only if :
:∃ x ∈ A, ϵ∈ℝ_>0: B ⊆ B_ϵ(   )x

where B_ϵ(   )x is the open ϵ-ball of x.


That is, M' can be fitted inside an open ball.

=== Definition 4 ===
Let M = ( A, d ) be a metric space.

Let M' = ( B, d_B ) be a subspace of M.

Let a' ∈ A.


M' is bounded (in M)  if and only if :
:∃ K ∈ℝ: ∀ x ∈ B: d (   )x, a'≤ K",Bounded
"['Definitions/Bounded Metric Spaces', 'Definitions/Complex Plane']",Definition:Bounded,"Let $D$ be a subset of the complex plane $mathbb C$.


Then $D$ is bounded (in $mathbb C$)  if and only if  there exists $M in mathbb R$ such that:
: $forall z in D: leftlvert z rightrvert le M$


=== Unbounded ===
Let $D$ be a subset of the complex plane $mathbb C$.


Then $D$ is unbounded (in $mathbb C$)  if and only if :
: $nexists M in mathbb R: forall z in D: leftlvert z rightrvert le M$

That is, if $D$ is not bounded in $mathbb C$.",Definition:Bounded Metric Space/Complex,,false,"Let D be a subset of the complex plane ℂ.


Then D is bounded (in ℂ)  if and only if  there exists M ∈ℝ such that:
: ∀ z ∈ D: | z |≤ M


=== Unbounded ===
Let D be a subset of the complex plane ℂ.


Then D is unbounded (in ℂ)  if and only if :
: ∄ M ∈ℝ: ∀ z ∈ D: | z |≤ M

That is, if D is not bounded in ℂ.",Bounded
['Definitions/Topological Vector Spaces'],Definition:Bounded,"Let $mathbb F in leftlbrace mathbb R, mathbb C rightrbrace$.

Let $left( V, tau right)$ be a topological vector space over $mathbb F$.


A subset $B subseteq V$ is bounded  if and only if :
:for each $U in tau$ such that $mathbf 0_V in U$ there is an $epsilon in mathbb R_{>0}$ such that:
::$epsilon B subseteq U$

where:
:$bf 0_V$ denotes the zero vector of $V$
:$epsilon B$ denotes the dilation of $B$ by $epsilon$",Definition:Bounded Subset of Topological Vector Space,,false,"Let 𝔽∈{ℝ, ℂ}.

Let ( V, τ) be a topological vector space over 𝔽.


A subset B ⊆ V is bounded  if and only if :
:for each U ∈τ such that 0_V ∈ U there is an ϵ∈ℝ_>0 such that:
::ϵ B ⊆ U

where:
:0_V denotes the zero vector of V
:ϵ B denotes the dilation of B by ϵ",Bounded
"['Definitions/Bounded Subset of Normed Vector Space', 'Definitions/Bounded Subsets of Normed Vector Spaces', 'Definitions/Normed Vector Spaces', 'Definitions/Bounded Subsets of Normed Vector Spaces']",Definition:Bounded,"Let $M = left( X, leftlVert , cdot , rightrVert right)$ be a normed vector space.

Let $M' subseteq X$. 


=== Definition 1 ===
Let $M = left( X, leftlVert , cdot , rightrVert right)$ be a normed vector space.

Let $M' = left( Y, leftlVert , cdot , rightrVert_Y right)$ be a subspace of $M$.


$M'$ is bounded (in $M$)  if and only if :
:$exists x in X, C in mathbb R_{> 0}: forall y in Y: leftlVert x - y rightrVert le C$

=== Definition 2 ===
Let $M = left( X, leftlVert , cdot , rightrVert right)$ be a normed vector space.

Let $M' = left( Y, leftlVert , cdot , rightrVert_Y right)$ be a subspace of $M$.


$M'$ is bounded (in $M$)  if and only if :
:$exists epsilon in mathbb R_{>0} : exists x in X : Y subseteq B_epsilon^- left(   right)x$

where $B_epsilon^- left(   right)x$ is a closed ball in $M$.",Definition:Bounded Subset of Normed Vector Space,,false,"Let M = ( X, ‖ · ‖) be a normed vector space.

Let M' ⊆ X. 


=== Definition 1 ===
Let M = ( X, ‖ · ‖) be a normed vector space.

Let M' = ( Y, ‖ · ‖_Y ) be a subspace of M.


M' is bounded (in M)  if and only if :
:∃ x ∈ X, C ∈ℝ_> 0: ∀ y ∈ Y: ‖ x - y ‖≤ C

=== Definition 2 ===
Let M = ( X, ‖ · ‖) be a normed vector space.

Let M' = ( Y, ‖ · ‖_Y ) be a subspace of M.


M' is bounded (in M)  if and only if :
:∃ϵ∈ℝ_>0 : ∃ x ∈ X : Y ⊆ B_ϵ^- (   )x

where B_ϵ^- (   )x is a closed ball in M.",Bounded
"['Definitions/Bounded Above Sets', 'Definitions/Boundedness']",Definition:Bounded Above,"Let $left( S, preceq right)$ be an ordered set.


A subset $T subseteq S$ is bounded above (in $S$)  if and only if  $T$ admits an upper bound (in $S$).


=== Subset of Real Numbers ===

The concept is usually encountered where $left( S, preceq right)$ is the set of real numbers under the usual ordering $left( mathbb R, le right)$:

Let $mathbb R$ be the set of real numbers.

A subset $T subseteq mathbb R$ is bounded above (in $mathbb R$)  if and only if  $T$ admits an upper bound (in $mathbb R$).


=== Unbounded Above ===
Let $mathbb R$ be the set of real numbers.

Let $T subseteq mathbb R$ be a subset of $mathbb R$ .


$T subseteq mathbb R$ is unbounded above (in $mathbb R$)  if and only if  it is not bounded above.

=== Unbounded Above ===
Let $left( S, preceq right)$ be an ordered set.


A subset $T subseteq S$ is unbounded above (in $S$)  if and only if  it is not bounded above.",Definition:Bounded Above Set,,false,"Let ( S, ≼) be an ordered set.


A subset T ⊆ S is bounded above (in S)  if and only if  T admits an upper bound (in S).


=== Subset of Real Numbers ===

The concept is usually encountered where ( S, ≼) is the set of real numbers under the usual ordering ( ℝ, ≤):

Let ℝ be the set of real numbers.

A subset T ⊆ℝ is bounded above (in ℝ)  if and only if  T admits an upper bound (in ℝ).


=== Unbounded Above ===
Let ℝ be the set of real numbers.

Let T ⊆ℝ be a subset of ℝ .


T ⊆ℝ is unbounded above (in ℝ)  if and only if  it is not bounded above.

=== Unbounded Above ===
Let ( S, ≼) be an ordered set.


A subset T ⊆ S is unbounded above (in S)  if and only if  it is not bounded above.",Bounded Above
"['Definitions/Bounded Above Sets of Real Numbers', 'Definitions/Bounded Above Sets', 'Definitions/Real Numbers']",Definition:Bounded Above,"Let $mathbb R$ be the set of real numbers.

A subset $T subseteq mathbb R$ is bounded above (in $mathbb R$)  if and only if  $T$ admits an upper bound (in $mathbb R$).


=== Unbounded Above ===
Let $mathbb R$ be the set of real numbers.

Let $T subseteq mathbb R$ be a subset of $mathbb R$ .


$T subseteq mathbb R$ is unbounded above (in $mathbb R$)  if and only if  it is not bounded above.",Definition:Bounded Above Set/Real Numbers,,false,"Let ℝ be the set of real numbers.

A subset T ⊆ℝ is bounded above (in ℝ)  if and only if  T admits an upper bound (in ℝ).


=== Unbounded Above ===
Let ℝ be the set of real numbers.

Let T ⊆ℝ be a subset of ℝ .


T ⊆ℝ is unbounded above (in ℝ)  if and only if  it is not bounded above.",Bounded Above
"['Definitions/Bounded Above Mappings', 'Definitions/Mappings', 'Definitions/Boundedness']",Definition:Bounded Above,"Let $f: S to T$ be a mapping whose codomain is an ordered set $left( T, preceq right)$.


Then $f$ is bounded above on $S$ by the upper bound $H$  if and only if :
:$forall x in S: f left(   right)x preceq H$


That is,  if and only if  $f left[ S right] = leftlbrace f left(   right)x: x in S rightrbrace$ is bounded above by $H$.


=== Real-Valued Function ===

The concept is usually encountered where $left( T, preceq right)$ is the set of real numbers under the usual ordering $left( mathbb R, le right)$:

Let $f: S to mathbb R$ be a real-valued function.


$f$ is bounded above on $S$ by the upper bound $H$  if and only if :
:$forall x in S: f left(   right)x le H$


That is,  if and only if  the set $leftlbrace f left(   right)x: x in S rightrbrace$ is bounded above in $mathbb R$ by $H$.",Definition:Bounded Above Mapping,,false,"Let f: S → T be a mapping whose codomain is an ordered set ( T, ≼).


Then f is bounded above on S by the upper bound H  if and only if :
:∀ x ∈ S: f (   )x ≼ H


That is,  if and only if  f [ S ] = { f (   )x: x ∈ S } is bounded above by H.


=== Real-Valued Function ===

The concept is usually encountered where ( T, ≼) is the set of real numbers under the usual ordering ( ℝ, ≤):

Let f: S →ℝ be a real-valued function.


f is bounded above on S by the upper bound H  if and only if :
:∀ x ∈ S: f (   )x ≤ H


That is,  if and only if  the set { f (   )x: x ∈ S } is bounded above in ℝ by H.",Bounded Above
"['Definitions/Bounded Above Real-Valued Functions', 'Definitions/Real-Valued Functions', 'Definitions/Bounded Above Mappings']",Definition:Bounded Above,"Let $f: S to mathbb R$ be a real-valued function.


$f$ is bounded above on $S$ by the upper bound $H$  if and only if :
:$forall x in S: f left(   right)x le H$


That is,  if and only if  the set $leftlbrace f left(   right)x: x in S rightrbrace$ is bounded above in $mathbb R$ by $H$.",Definition:Bounded Above Mapping/Real-Valued,,false,"Let f: S →ℝ be a real-valued function.


f is bounded above on S by the upper bound H  if and only if :
:∀ x ∈ S: f (   )x ≤ H


That is,  if and only if  the set { f (   )x: x ∈ S } is bounded above in ℝ by H.",Bounded Above
"['Definitions/Bounded Above Sequences', 'Definitions/Boundedness', 'Definitions/Sequences']",Definition:Bounded Above,"Let $left( T, preceq right)$ be an ordered set.

Let $leftlangle x_n rightrangle$ be a sequence in $T$.


Then $leftlangle x_n rightrangle$ is bounded above  if and only if :
:$exists M in T: forall i in mathbb N: x_i preceq M$


=== Real Sequence ===

The concept is usually encountered where $left( T, preceq right)$ is the set of real numbers under the usual ordering $left( mathbb R, le right)$:

Let $leftlangle x_n rightrangle$ be a real sequence.


Then $leftlangle x_n rightrangle$ is bounded above  if and only if :
:$exists M in mathbb R: forall i in mathbb N: x_i le M$",Definition:Bounded Above Sequence,,false,"Let ( T, ≼) be an ordered set.

Let ⟨ x_n ⟩ be a sequence in T.


Then ⟨ x_n ⟩ is bounded above  if and only if :
:∃ M ∈ T: ∀ i ∈ℕ: x_i ≼ M


=== Real Sequence ===

The concept is usually encountered where ( T, ≼) is the set of real numbers under the usual ordering ( ℝ, ≤):

Let ⟨ x_n ⟩ be a real sequence.


Then ⟨ x_n ⟩ is bounded above  if and only if :
:∃ M ∈ℝ: ∀ i ∈ℕ: x_i ≤ M",Bounded Above
"['Definitions/Bounded Above Real Sequences', 'Definitions/Bounded Above Sequences', 'Definitions/Real Sequences']",Definition:Bounded Above,"Let $leftlangle x_n rightrangle$ be a real sequence.


Then $leftlangle x_n rightrangle$ is bounded above  if and only if :
:$exists M in mathbb R: forall i in mathbb N: x_i le M$",Definition:Bounded Above Sequence/Real,,false,"Let ⟨ x_n ⟩ be a real sequence.


Then ⟨ x_n ⟩ is bounded above  if and only if :
:∃ M ∈ℝ: ∀ i ∈ℕ: x_i ≤ M",Bounded Above
"['Definitions/Bounded Below Sets', 'Definitions/Boundedness']",Definition:Bounded Below,"Let $left( S, preceq right)$ be an ordered set.


A subset $T subseteq S$ is bounded below (in $S$)  if and only if  $T$ admits a lower bound (in $S$).


=== Subset of Real Numbers ===

The concept is usually encountered where $left( S, preceq right)$ is the set of real numbers under the usual ordering $left( mathbb R, le right)$:

Let $mathbb R$ be the set of real numbers.

A subset $T subseteq mathbb R$ is bounded below (in $mathbb R$)  if and only if  $T$ admits a lower bound (in $mathbb R$).


=== Unbounded Below ===
Let $mathbb R$ be the set of real numbers.

Let $T subseteq mathbb R$ be a subset of $mathbb R$ .


$T subseteq mathbb R$ is unbounded below (in $mathbb R$)  if and only if  it is not bounded below.",Definition:Bounded Below Set,,false,"Let ( S, ≼) be an ordered set.


A subset T ⊆ S is bounded below (in S)  if and only if  T admits a lower bound (in S).


=== Subset of Real Numbers ===

The concept is usually encountered where ( S, ≼) is the set of real numbers under the usual ordering ( ℝ, ≤):

Let ℝ be the set of real numbers.

A subset T ⊆ℝ is bounded below (in ℝ)  if and only if  T admits a lower bound (in ℝ).


=== Unbounded Below ===
Let ℝ be the set of real numbers.

Let T ⊆ℝ be a subset of ℝ .


T ⊆ℝ is unbounded below (in ℝ)  if and only if  it is not bounded below.",Bounded Below
"['Definitions/Bounded Below Sets of Real Numbers', 'Definitions/Bounded Below Sets', 'Definitions/Real Numbers']",Definition:Bounded Below,"Let $mathbb R$ be the set of real numbers.

A subset $T subseteq mathbb R$ is bounded below (in $mathbb R$)  if and only if  $T$ admits a lower bound (in $mathbb R$).


=== Unbounded Below ===
Let $mathbb R$ be the set of real numbers.

Let $T subseteq mathbb R$ be a subset of $mathbb R$ .


$T subseteq mathbb R$ is unbounded below (in $mathbb R$)  if and only if  it is not bounded below.",Definition:Bounded Below Set/Real Numbers,,false,"Let ℝ be the set of real numbers.

A subset T ⊆ℝ is bounded below (in ℝ)  if and only if  T admits a lower bound (in ℝ).


=== Unbounded Below ===
Let ℝ be the set of real numbers.

Let T ⊆ℝ be a subset of ℝ .


T ⊆ℝ is unbounded below (in ℝ)  if and only if  it is not bounded below.",Bounded Below
"['Definitions/Bounded Below Mappings', 'Definitions/Mappings', 'Definitions/Boundedness']",Definition:Bounded Below,"Let $f: S to T$ be a mapping whose codomain is an ordered set $left( T, preceq right)$.


Then $f$ is said to be bounded below (in $T$) by the lower bound $L$  if and only if :
:$forall x in S: L preceq f left(   right)x$


That is, iff $f left[ S right] = leftlbrace f left(   right)x: x in S rightrbrace$ is bounded below by $L$.


=== Real-Valued Function ===

The concept is usually encountered where $left( T, preceq right)$ is the set of real numbers under the usual ordering $left( mathbb R, le right)$:

Let $f: S to mathbb R$ be a real-valued function.


Then $f$ is bounded below on $S$ by the lower bound $L$  if and only if :
:$forall x in S: L le f left(   right)x$


That is,  if and only if  the set $leftlbrace f left(   right)x: x in S rightrbrace$ is bounded below in $mathbb R$ by $L$.",Definition:Bounded Below Mapping,,false,"Let f: S → T be a mapping whose codomain is an ordered set ( T, ≼).


Then f is said to be bounded below (in T) by the lower bound L  if and only if :
:∀ x ∈ S: L ≼ f (   )x


That is, iff f [ S ] = { f (   )x: x ∈ S } is bounded below by L.


=== Real-Valued Function ===

The concept is usually encountered where ( T, ≼) is the set of real numbers under the usual ordering ( ℝ, ≤):

Let f: S →ℝ be a real-valued function.


Then f is bounded below on S by the lower bound L  if and only if :
:∀ x ∈ S: L ≤ f (   )x


That is,  if and only if  the set { f (   )x: x ∈ S } is bounded below in ℝ by L.",Bounded Below
"['Definitions/Bounded Below Real-Valued Functions', 'Definitions/Real-Valued Functions', 'Definitions/Bounded Below Mappings']",Definition:Bounded Below,"Let $f: S to mathbb R$ be a real-valued function.


Then $f$ is bounded below on $S$ by the lower bound $L$  if and only if :
:$forall x in S: L le f left(   right)x$


That is,  if and only if  the set $leftlbrace f left(   right)x: x in S rightrbrace$ is bounded below in $mathbb R$ by $L$.",Definition:Bounded Below Mapping/Real-Valued,,false,"Let f: S →ℝ be a real-valued function.


Then f is bounded below on S by the lower bound L  if and only if :
:∀ x ∈ S: L ≤ f (   )x


That is,  if and only if  the set { f (   )x: x ∈ S } is bounded below in ℝ by L.",Bounded Below
"['Definitions/Bounded Below Sequences', 'Definitions/Boundedness', 'Definitions/Sequences']",Definition:Bounded Below,"Let $left( T, preceq right)$ be an ordered set.

Let $leftlangle x_n rightrangle$ be a sequence in $T$.


Then $leftlangle x_n rightrangle$ is bounded below  if and only if :
:$exists m in T: forall i in mathbb N: m preceq x_i$


=== Real Sequence ===

The concept is usually encountered where $left( T, preceq right)$ is the set of real numbers under the usual ordering $left( mathbb R, le right)$:

Let $leftlangle x_n rightrangle$ be a real sequence.


Then $leftlangle x_n rightrangle$ is bounded below  if and only if :
:$exists m in mathbb R: forall i in mathbb N: m le x_i$


=== Unbounded Below ===
Let $leftlangle x_n rightrangle$ be a real sequence.


$leftlangle x_n rightrangle$ is unbounded below  if and only if  there exists no $m$ in $mathbb R$ such that:
:$forall i in mathbb N: m le x_i$


Category:Definitions/Unbounded Below Sequences
Category:Definitions/Real Sequences",Definition:Bounded Below Sequence,,false,"Let ( T, ≼) be an ordered set.

Let ⟨ x_n ⟩ be a sequence in T.


Then ⟨ x_n ⟩ is bounded below  if and only if :
:∃ m ∈ T: ∀ i ∈ℕ: m ≼ x_i


=== Real Sequence ===

The concept is usually encountered where ( T, ≼) is the set of real numbers under the usual ordering ( ℝ, ≤):

Let ⟨ x_n ⟩ be a real sequence.


Then ⟨ x_n ⟩ is bounded below  if and only if :
:∃ m ∈ℝ: ∀ i ∈ℕ: m ≤ x_i


=== Unbounded Below ===
Let ⟨ x_n ⟩ be a real sequence.


⟨ x_n ⟩ is unbounded below  if and only if  there exists no m in ℝ such that:
:∀ i ∈ℕ: m ≤ x_i


Category:Definitions/Unbounded Below Sequences
Category:Definitions/Real Sequences",Bounded Below
"['Definitions/Bounded Below Real Sequences', 'Definitions/Bounded Below Sequences', 'Definitions/Real Sequences']",Definition:Bounded Below,"Let $leftlangle x_n rightrangle$ be a real sequence.


Then $leftlangle x_n rightrangle$ is bounded below  if and only if :
:$exists m in mathbb R: forall i in mathbb N: m le x_i$


=== Unbounded Below ===
Let $leftlangle x_n rightrangle$ be a real sequence.


$leftlangle x_n rightrangle$ is unbounded below  if and only if  there exists no $m$ in $mathbb R$ such that:
:$forall i in mathbb N: m le x_i$


Category:Definitions/Unbounded Below Sequences
Category:Definitions/Real Sequences",Definition:Bounded Below Sequence/Real,,false,"Let ⟨ x_n ⟩ be a real sequence.


Then ⟨ x_n ⟩ is bounded below  if and only if :
:∃ m ∈ℝ: ∀ i ∈ℕ: m ≤ x_i


=== Unbounded Below ===
Let ⟨ x_n ⟩ be a real sequence.


⟨ x_n ⟩ is unbounded below  if and only if  there exists no m in ℝ such that:
:∀ i ∈ℕ: m ≤ x_i


Category:Definitions/Unbounded Below Sequences
Category:Definitions/Real Sequences",Bounded Below
['Definitions/Rooted Trees'],Definition:Branch,"Let $T$ be a rooted tree with root node $r_T$.

A subset $Gamma$ of $T$ is a branch  if and only if  all the following conditions hold:
:$(1): quad$ The root node $r_T$ belongs to $Gamma$
:$(2): quad$ The parent of each node in $Gamma setminus leftlbrace r_T rightrbrace$ is in $Gamma$
:$(3): quad$ Each node in $Gamma$ either:
::$text {(a)}: quad$ is a leaf node of $T$
:or:
::$text {(b)}: quad$ has exactly one child node in $Gamma$.


Informally, a branch of a rooted tree is the path from the root to a leaf.


=== Finite ===
Let $T$ be a rooted tree with root node $r_T$.

Let $Gamma$ be a branch of $T$.


Then $Gamma$ is finite  if and only if  it has exactly one leaf node.

=== Infinite ===
Let $T$ be a rooted tree with root node $r_T$.

Let $Gamma$ be a branch of $T$.


Then $Gamma$ is infinite  if and only if  it has no leaf node at the end.",Definition:Rooted Tree/Branch,,false,"Let T be a rooted tree with root node r_T.

A subset Γ of T is a branch  if and only if  all the following conditions hold:
:(1): The root node r_T belongs to Γ
:(2): The parent of each node in Γ∖{ r_T } is in Γ
:(3): Each node in Γ either:
::(a): is a leaf node of T
:or:
::(b): has exactly one child node in Γ.


Informally, a branch of a rooted tree is the path from the root to a leaf.


=== Finite ===
Let T be a rooted tree with root node r_T.

Let Γ be a branch of T.


Then Γ is finite  if and only if  it has exactly one leaf node.

=== Infinite ===
Let T be a rooted tree with root node r_T.

Let Γ be a branch of T.


Then Γ is infinite  if and only if  it has no leaf node at the end.",Branch
"['Definitions/Set Theory', 'Definitions/Set Theory']",Definition:Branch,"Let $left( T, preceq right)$ be a tree.

A branch of $left( T, preceq right)$ is a maximal chain in $left( T, preceq right)$.


Category:Definitions/Set Theory",Definition:Tree (Set Theory)/Branch,,false,"Let ( T, ≼) be a tree.

A branch of ( T, ≼) is a maximal chain in ( T, ≼).


Category:Definitions/Set Theory",Branch
['Definitions/Multifunctions'],Definition:Branch,"Let $A$ and $B$ be sets.

Let $f: A to B$ be a multifunction on $A$.

Let $leftlangle S_i rightrangle_{i mathop in I}$ be a partitioning of the codomain of $f$ such that:
:$forall i in I: f restriction_{A times S_i}$ is a mapping.


Then each $f restriction_{A times S_i}$ is a branch of $f$.


=== Principal Branch ===
Let $A$ and $B$ be sets.

Let $f: A to B$ be a multifunction on $A$.

Let $leftlangle S_i rightrangle_{i mathop in I}$ be a partitioning of the codomain of $f$ into branches.


It is usual to distinguish one such branch of $f$ from the others, and label it the principal branch of $f$.


=== Principal Value ===
Let $A$ and $B$ be sets.

Let $f: A to B$ be a multifunction on $A$.

Let $x in A$ be an element of the domain of $f$.

The principal value of $x$ is the element $y$ of the principal branch of $f$ such that $f left(   right)x = y$.

=== Principal Value ===
Let $A$ and $B$ be sets.

Let $f: A to B$ be a multifunction on $A$.

Let $x in A$ be an element of the domain of $f$.

The principal value of $x$ is the element $y$ of the principal branch of $f$ such that $f left(   right)x = y$.

=== Branch Point ===
Let $U subseteq mathbb C$ be an open set.

Let $f : U to mathbb C$ be a complex multifunction.


A branch point of $f$ is a point $a$ in $U$ such that:

:$f$ has more than one value at one or more points in every neighborhood of $a$
:$f$ has exactly one value at $a$ itself.",Definition:Multifunction/Branch,,false,"Let A and B be sets.

Let f: A → B be a multifunction on A.

Let ⟨ S_i ⟩_i ∈ I be a partitioning of the codomain of f such that:
:∀ i ∈ I: f _A × S_i is a mapping.


Then each f _A × S_i is a branch of f.


=== Principal Branch ===
Let A and B be sets.

Let f: A → B be a multifunction on A.

Let ⟨ S_i ⟩_i ∈ I be a partitioning of the codomain of f into branches.


It is usual to distinguish one such branch of f from the others, and label it the principal branch of f.


=== Principal Value ===
Let A and B be sets.

Let f: A → B be a multifunction on A.

Let x ∈ A be an element of the domain of f.

The principal value of x is the element y of the principal branch of f such that f (   )x = y.

=== Principal Value ===
Let A and B be sets.

Let f: A → B be a multifunction on A.

Let x ∈ A be an element of the domain of f.

The principal value of x is the element y of the principal branch of f such that f (   )x = y.

=== Branch Point ===
Let U ⊆ℂ be an open set.

Let f : U →ℂ be a complex multifunction.


A branch point of f is a point a in U such that:

:f has more than one value at one or more points in every neighborhood of a
:f has exactly one value at a itself.",Branch
"['Definitions/Branch Points', 'Definitions/Singular Points', 'Definitions/Complex Analysis']",Definition:Branch,"Let $U subseteq mathbb C$ be an open set.

Let $f : U to mathbb C$ be a complex multifunction.


A branch point of $f$ is a point $a$ in $U$ such that:

:$f$ has more than one value at one or more points in every neighborhood of $a$
:$f$ has exactly one value at $a$ itself.",Definition:Branch Point,,false,"Let U ⊆ℂ be an open set.

Let f : U →ℂ be a complex multifunction.


A branch point of f is a point a in U such that:

:f has more than one value at one or more points in every neighborhood of a
:f has exactly one value at a itself.",Branch
"['Definitions/Branches of Curves', 'Definitions/Analytic Geometry']",Definition:Branch,A branch of a curve $mathcal C$ is a part of $mathcal C$ which is separated from another part of $mathcal C$ by a discontinuity or a singular point.,Definition:Branch of Curve,,false,A branch of a curve 𝒞 is a part of 𝒞 which is separated from another part of 𝒞 by a discontinuity or a singular point.,Branch
['Definitions/Graph Theory'],Definition:Bridge,"Let $G = left( V, E right)$ be a connected graph.

Let $e in E$ be an edge of $G$ such that the edge deletion $G - e$ is disconnected.


Then $e$ is known as a bridge of $G$.",Definition:Bridge (Graph Theory),,false,"Let G = ( V, E ) be a connected graph.

Let e ∈ E be an edge of G such that the edge deletion G - e is disconnected.


Then e is known as a bridge of G.",Bridge
"['Definitions/Bridge (Game)', 'Definitions/Examples of Games']",Definition:Bridge,"Bridge is a game for $4$ players whose mechanism depends on the fact that each of the $4$ players are dealt a hand of $13$ cards from the standard deck of $52$ cards.


For the purposes of   at its current stage of evolution, details of the play of Bridge are not immediately relevant.

As and when a deeper analysis of Bridge becomes appropriate, further details can be incorporated.",Definition:Bridge (Game),game,true,"Bridge is a game for 4 players whose mechanism depends on the fact that each of the 4 players are dealt a hand of 13 cards from the standard deck of 52 cards.


For the purposes of   at its current stage of evolution, details of the play of Bridge are not immediately relevant.

As and when a deeper analysis of Bridge becomes appropriate, further details can be incorporated.",Bridge
"['Definitions/Abstract Algebra', 'Definitions/Cancellability']",Definition:Cancellable,"Let $left( S, circ right)$ be an algebraic structure.

An element $x in left( S, circ right)$ is cancellable  if and only if :
:$forall a, b in S: x circ a = x circ b implies a = b$
:$forall a, b in S: a circ x = b circ x implies a = b$


That is,  if and only if  it is both left cancellable and right cancellable.


=== Left Cancellable ===
Let $left( S, circ right)$ be an algebraic structure.


An element $x in left( S, circ right)$ is left cancellable  if and only if :

:$forall a, b in S: x circ a = x circ b implies a = b$

=== Right Cancellable ===
Let $left( S, circ right)$ be an algebraic structure.


An element $x in left( S, circ right)$ is right cancellable  if and only if :

:$forall a, b in S: a circ x = b circ x implies a = b$",Definition:Cancellable Element,,false,"Let ( S, ∘) be an algebraic structure.

An element x ∈( S, ∘) is cancellable  if and only if :
:∀ a, b ∈ S: x ∘ a = x ∘ b  a = b
:∀ a, b ∈ S: a ∘ x = b ∘ x  a = b


That is,  if and only if  it is both left cancellable and right cancellable.


=== Left Cancellable ===
Let ( S, ∘) be an algebraic structure.


An element x ∈( S, ∘) is left cancellable  if and only if :

:∀ a, b ∈ S: x ∘ a = x ∘ b  a = b

=== Right Cancellable ===
Let ( S, ∘) be an algebraic structure.


An element x ∈( S, ∘) is right cancellable  if and only if :

:∀ a, b ∈ S: a ∘ x = b ∘ x  a = b",Cancellable
"['Definitions/Abstract Algebra', 'Definitions/Cancellability']",Definition:Cancellable,"Let $left( S, circ right)$ be an algebraic structure.


The operation $circ$ in $left( S, circ right)$ is cancellable  if and only if :
:$forall a, b, c in S: a circ b = a circ c implies b = c$
:$forall a, b, c in S: a circ c = b circ c implies a = b$


That is,  if and only if  it is both a left cancellable operation and a right cancellable operation.


=== Left Cancellable Operation ===
Let $left( S, circ right)$ be an algebraic structure.


The operation $circ$ in $left( S, circ right)$ is left cancellable  if and only if :
:$forall a, b, c in S: a circ b = a circ c implies b = c$

That is,  if and only if  all elements of $left( S, circ right)$ are left cancellable.

=== Right Cancellable Operation ===
Let $left( S, circ right)$ be an algebraic structure.


The operation $circ$ in $left( S, circ right)$ is right cancellable  if and only if :
:$forall a, b, c in S: a circ c = b circ c implies a = b$

That is,  if and only if  all elements of $left( S, circ right)$ are right cancellable.",Definition:Cancellable Operation,,false,"Let ( S, ∘) be an algebraic structure.


The operation ∘ in ( S, ∘) is cancellable  if and only if :
:∀ a, b, c ∈ S: a ∘ b = a ∘ c  b = c
:∀ a, b, c ∈ S: a ∘ c = b ∘ c  a = b


That is,  if and only if  it is both a left cancellable operation and a right cancellable operation.


=== Left Cancellable Operation ===
Let ( S, ∘) be an algebraic structure.


The operation ∘ in ( S, ∘) is left cancellable  if and only if :
:∀ a, b, c ∈ S: a ∘ b = a ∘ c  b = c

That is,  if and only if  all elements of ( S, ∘) are left cancellable.

=== Right Cancellable Operation ===
Let ( S, ∘) be an algebraic structure.


The operation ∘ in ( S, ∘) is right cancellable  if and only if :
:∀ a, b, c ∈ S: a ∘ c = b ∘ c  a = b

That is,  if and only if  all elements of ( S, ∘) are right cancellable.",Cancellable
"['Definitions/Canonical Form of Rational Number', 'Definitions/Rational Numbers', 'Definitions/Fractions', 'Definitions/Canonical Forms']",Definition:Canonical,"Let $r in mathbb Q$ be a rational number.

The canonical form of $r$ is the expression $dfrac p q$, where:
:$r = dfrac p q: p in mathbb Z, q in mathbb Z_{>0}, p perp q$
where $p perp q$ denotes that $p$ and $q$ have no common divisor except $1$.


That is, in its canonical form, $r$ is expressed as $dfrac p q$ where:

:$p$ is an integer
:$q$ is a strictly positive integer
:$p$ and $q$ are coprime.",Definition:Rational Number/Canonical Form,,false,"Let r ∈ℚ be a rational number.

The canonical form of r is the expression p q, where:
:r =  p q: p ∈ℤ, q ∈ℤ_>0, p ⊥ q
where p ⊥ q denotes that p and q have no common divisor except 1.


That is, in its canonical form, r is expressed as p q where:

:p is an integer
:q is a strictly positive integer
:p and q are coprime.",Canonical
"['Definitions/Quadratic Equations', 'Definitions/Canonical Forms']",Definition:Canonical,"The canonical form of the quadratic equation is:
:$a x^2 + b x + c = 0$
where $a$, $b$ and $c$ are constants.",Definition:Quadratic Equation/Canonical Form,,false,"The canonical form of the quadratic equation is:
:a x^2 + b x + c = 0
where a, b and c are constants.",Canonical
['Definitions/Continued Fractions'],Definition:Canonical,"Let $F$ be a field, such as the field of real numbers $mathbb R$.


=== Finite Continued Fraction ===
Let $F$ be a field, such as the field of real numbers $mathbb R$.

Let $n ge 0$ be a natural number.


Informally, a finite continued fraction of length $n$ in $F$ is an expression of the form:
:$a_0 + cfrac 1 {a_1 + cfrac 1 {a_2 + cfrac 1 {ddots cfrac {} {a_{n - 1} + cfrac 1 {a_n} } } } }$
where $a_0, a_1, a_2, ldots, a_n in F$.


Formally, a finite continued fraction of length $n$ in $F$ is a finite sequence, called sequence of partial denominators, whose domain is the integer interval $left[ 0 ,.,.,   right]n$.


A finite continued fraction should not be confused with its value, when it exists.

=== Infinite Continued Fraction ===
Let $F$ be a field, such as the field of real numbers $mathbb R$.


Informally, an infinite continued fraction in $F$ is an expression of the form:

:$a_0 + cfrac 1 {a_1 + cfrac 1 {a_2 + cfrac 1 {ddots cfrac {} {a_{n-1} + cfrac 1 {a_n + cfrac 1 {ddots}}} }}}$
where $a_0, a_1, a_2, ldots, a_n, ldots in F$.


Formally, an infinite continued fraction in $F$ is a sequence, called a sequence of partial denominators, whose domain is $mathbb N_{ge 0}$.


An infinite continued fraction should not be confused with its value, when it exists.",Definition:Continued Fraction,,false,"Let F be a field, such as the field of real numbers ℝ.


=== Finite Continued Fraction ===
Let F be a field, such as the field of real numbers ℝ.

Let n ≥ 0 be a natural number.


Informally, a finite continued fraction of length n in F is an expression of the form:
:a_0 +  1 a_1 +  1 a_2 +  1 ⋱a_n - 1 +  1 a_n
where a_0, a_1, a_2, …, a_n ∈ F.


Formally, a finite continued fraction of length n in F is a finite sequence, called sequence of partial denominators, whose domain is the integer interval [ 0  . . ]n.


A finite continued fraction should not be confused with its value, when it exists.

=== Infinite Continued Fraction ===
Let F be a field, such as the field of real numbers ℝ.


Informally, an infinite continued fraction in F is an expression of the form:

:a_0 +  1 a_1 +  1 a_2 +  1 ⋱a_n-1 +  1 a_n +  1 ⋱
where a_0, a_1, a_2, …, a_n, …∈ F.


Formally, an infinite continued fraction in F is a sequence, called a sequence of partial denominators, whose domain is ℕ_≥ 0.


An infinite continued fraction should not be confused with its value, when it exists.",Canonical
"['Definitions/Canonical Forms', 'Definitions/Matrices']",Definition:Canonical,A canonical form of a matrix is a form which all of a certain class of matrix can be reduced by transformations of a standard kind.,Definition:Canonical Form of Matrix,,false,A canonical form of a matrix is a form which all of a certain class of matrix can be reduced by transformations of a standard kind.,Canonical
"['Definitions/Canonical Forms', 'Definitions/Mathematics', 'Definitions/Computer Science']",Definition:Canonical,A canonical form of a mathematical object is a standard way of presenting that object as a mathematical expression.,Definition:Canonical Form,,false,A canonical form of a mathematical object is a standard way of presenting that object as a mathematical expression.,Canonical
"['Definitions/Chains (Order Theory)', 'Definitions/Nests', 'Definitions/Order Theory']",Definition:Chain,"Let $left( S, preceq right)$ be an ordered set.


A chain in $S$ is a totally ordered subset of $S$.


Thus a totally ordered set is itself a chain in its own right.


=== Chain of Sets ===

An important special case of a chain is where the ordering in question is the subset relation:

Let $S$ be a set.

Let $mathcal P left( S right)$ be its power set.

Let $N subseteq mathcal P left( S right)$ be a subset of $mathcal P left( S right)$.


Then $N$ is a chain (of sets)  if and only if :

:$forall X, Y in N: X subseteq Y$ or $Y subseteq X$",Definition:Chain (Order Theory),,false,"Let ( S, ≼) be an ordered set.


A chain in S is a totally ordered subset of S.


Thus a totally ordered set is itself a chain in its own right.


=== Chain of Sets ===

An important special case of a chain is where the ordering in question is the subset relation:

Let S be a set.

Let 𝒫( S ) be its power set.

Let N ⊆𝒫( S ) be a subset of 𝒫( S ).


Then N is a chain (of sets)  if and only if :

:∀ X, Y ∈ N: X ⊆ Y or Y ⊆ X",Chain
"['Definitions/Chains (Order Theory)', 'Definitions/Nests', 'Definitions/Order Theory']",Definition:Chain,"Let $S$ be a set.

Let $mathcal P left( S right)$ be its power set.

Let $N subseteq mathcal P left( S right)$ be a subset of $mathcal P left( S right)$.


Then $N$ is a chain (of sets)  if and only if :

:$forall X, Y in N: X subseteq Y$ or $Y subseteq X$",Definition:Chain (Order Theory)/Subset Relation,,false,"Let S be a set.

Let 𝒫( S ) be its power set.

Let N ⊆𝒫( S ) be a subset of 𝒫( S ).


Then N is a chain (of sets)  if and only if :

:∀ X, Y ∈ N: X ⊆ Y or Y ⊆ X",Chain
['Definitions/Ideals in Physics'],Definition:Chain,"A chain is an inelastic thread whose stiffness and width are approximated to zero.

The mass of a chain is usually defined in terms of linear mass density.


Category:Definitions/Ideals in Physics",Definition:Chain (Physics),thread,true,"A chain is an inelastic thread whose stiffness and width are approximated to zero.

The mass of a chain is usually defined in terms of linear mass density.


Category:Definitions/Ideals in Physics",Chain
['Definitions/Chain (Linear Measure)'],Definition:Chain,"The chain is an imperial unit of length.

 
 
 
 
 
 
 
 
 ",Definition:Imperial/Length/Chain,,false,"The chain is an imperial unit of length.

 
 
 
 
 
 
 
 
 ",Chain
"['Definitions/Number Theory', 'Definitions/Recreational Mathematics', 'Definitions/Aliquot Sequences', 'Definitions/Sociable Numbers']",Definition:Chain,"Let $m$ be a positive integer.

Let $s left(   right)m$ be the aliquot sum of $m$.


Define the sequence $leftlangle a_k rightrangle$ recursively as:
:$a_{k + 1} = begin{cases} m & : k = 0 \ s left(   right){a_k} & : k > 0 end{cases}$


A sociable chain is such a sequence $leftlangle a_k rightrangle$ where:
:$a_r = a_0$
for some $r > 0$.


=== Order of Sociable Chain ===
Let $m$ be a positive integer.

Let $s left({m}right)$ be the aliquot sum of $m$.


Let a sequence $leftlangle{a_k}rightrangle$ be a sociable chain.

The order of $a_k$ is the smallest $r in mathbb Z_{>0}$ such that
:$a_r = a_0$


Category:Definitions/Sociable Numbers",Definition:Sociable Chain,,false,"Let m be a positive integer.

Let s (   )m be the aliquot sum of m.


Define the sequence ⟨ a_k ⟩ recursively as:
:a_k + 1 =  m     : k = 0 
 s (   )a_k    : k > 0


A sociable chain is such a sequence ⟨ a_k ⟩ where:
:a_r = a_0
for some r > 0.


=== Order of Sociable Chain ===
Let m be a positive integer.

Let s (m) be the aliquot sum of m.


Let a sequence ⟨a_k⟩ be a sociable chain.

The order of a_k is the smallest r ∈ℤ_>0 such that
:a_r = a_0


Category:Definitions/Sociable Numbers",Chain
['Definitions/Analytic Number Theory'],Definition:Character,"Let $left( G, + right)$ be a finite abelian group.

Let $left( mathbb C_{ne 0}, times right)$ be the multiplicative group of complex numbers.


A character of $G$ is a group homomorphism:

:$chi: G to mathbb C_{ne 0}$",Definition:Character (Number Theory),,false,"Let ( G, + ) be a finite abelian group.

Let ( ℂ_ 0, ×) be the multiplicative group of complex numbers.


A character of G is a group homomorphism:

:χ: G →ℂ_ 0",Character
"['Definitions/Analytic Number Theory', 'Definitions/Dirichlet Characters']",Definition:Character,"Let $q in mathbb Z_{>1}$.

Let $left( mathbb Z / q mathbb Z right)$ denote the ring of integers modulo $q$.

Let $G = left( mathbb Z / q mathbb Z right)^times$ be the group of units of $left( mathbb Z / q mathbb Z right)$.

Let $mathbb C^times$ be the group of units of $mathbb C$.

A Dirichlet character modulo $q$ is a group homomorphism:
:$chi: G to mathbb C^times$


 

By Reduced Residue System under Multiplication forms Abelian Group, $a + q mathbb Z in G$  if and only if  $gcd left(   right){a, q} = 1$.

It is standard practice to extend $chi$ to a function on $mathbb Z$ by setting:

:$chi left(   right)a = begin{cases}
chi left(   right){a + q mathbb Z} & : gcd left(   right){a, q} = 1 \
0 & : text{otherwise}
end{cases}$

 


=== Trivial Character ===
 

=== Primitive Character ===
Let $q in mathbb Z_{>1}$.

Let $left( mathbb Z / q mathbb Z right)$ denote the ring of integers modulo $q$.

Let $G = left( mathbb Z / q mathbb Z right)^times$ be the group of units of $left( mathbb Z / q mathbb Z right)$.

Let $mathbb C^times$ be the group of units of $mathbb C$.


Let $chi_0$ be the trivial (Dirichlet) character modulo $q$.

Let $q^*$ be the least divisor of $q$ such that:
:$chi = chi_0 chi^*$
where $chi^*$ is some character modulo $q^*$.

If $q = q^*$ then $chi$ is called primitive, otherwise $chi$ is imprimitive.

 

Category:Definitions/Dirichlet Characters",Definition:Dirichlet Character,,false,"Let q ∈ℤ_>1.

Let ( ℤ / q ℤ) denote the ring of integers modulo q.

Let G = ( ℤ / q ℤ)^× be the group of units of ( ℤ / q ℤ).

Let ℂ^× be the group of units of ℂ.

A Dirichlet character modulo q is a group homomorphism:
:χ: G →ℂ^×


 

By Reduced Residue System under Multiplication forms Abelian Group, a + q ℤ∈ G  if and only if  (   )a, q = 1.

It is standard practice to extend χ to a function on ℤ by setting:

:χ(   )a = χ(   )a + q ℤ    : (   )a, q = 1 

0     : otherwise

 


=== Trivial Character ===
 

=== Primitive Character ===
Let q ∈ℤ_>1.

Let ( ℤ / q ℤ) denote the ring of integers modulo q.

Let G = ( ℤ / q ℤ)^× be the group of units of ( ℤ / q ℤ).

Let ℂ^× be the group of units of ℂ.


Let χ_0 be the trivial (Dirichlet) character modulo q.

Let q^* be the least divisor of q such that:
:χ = χ_0 χ^*
where χ^* is some character modulo q^*.

If q = q^* then χ is called primitive, otherwise χ is imprimitive.

 

Category:Definitions/Dirichlet Characters",Character
['Definitions/Quadratic Residues'],Definition:Character,"Let $p$ be an odd prime.

Let $a in mathbb Z$ be an integer such that $a not equiv 0 pmod p$.


$a$ is either a quadratic residue or a quadratic non-residue of $p$.

Whether it is or not is known as the quadratic character of $a$ modulo $p$.",Definition:Quadratic Residue/Character,,false,"Let p be an odd prime.

Let a ∈ℤ be an integer such that a ≢0  p.


a is either a quadratic residue or a quadratic non-residue of p.

Whether it is or not is known as the quadratic character of a modulo p.",Character
['Definitions/Representation Theory'],Definition:Character,"Let $left( G, cdot right)$ be a finite group.

Let $V$ be a finite dimensional  $k$-vector space.

Consider a linear representation $rho: G to mathrm {GL} left( V right)$ of $G$.

 

Let $mathrm {tr} left(   right){rho left(   right)g}$ denote the trace of $rho left(   right)g$.


The character associated with $rho$ is defined as:
:$chi: G to k$
where $chi left(   right)g = mathrm {tr} left(   right){rho left(   right)g}$, the trace of $rho left(   right)g$; which is a linear automorphism of $V$.

 ",Definition:Character (Representation Theory),,false,"Let ( G, ·) be a finite group.

Let V be a finite dimensional  k-vector space.

Consider a linear representation ρ: G →GL( V ) of G.

 

Let tr(   )ρ(   )g denote the trace of ρ(   )g.


The character associated with ρ is defined as:
:χ: G → k
where χ(   )g = tr(   )ρ(   )g, the trace of ρ(   )g; which is a linear automorphism of V.

 ",Character
['Definitions/Topology'],Definition:Character,"Let $T$ be a topological space.


The character of (the topological space) $T$ is the supremum of the set of characters of the points of $T$:
:$chi left({T}right) := sup left{ {chi left({x, T}right): x in T}right}$",Definition:Character of Topological Space,,false,"Let T be a topological space.


The character of (the topological space) T is the supremum of the set of characters of the points of T:
:χ(T) := sup{χ(x, T): x ∈ T}",Character
['Definitions/Topology'],Definition:Character,"Let $T$ be a topological space.

Let $x$ be a point of $T$.

Let $mathbb B left(   right)x$ be the set of all local bases at $x$.


The character of (the point) $x$ in $T$ is the smallest cardinality of the elements of $mathbb B left(   right)x$:
:$chi left(   right){x, T} := min leftlbrace leftlvert mathcal B rightrvert: mathcal B in mathbb B left(   right)x rightrbrace$",Definition:Character of Point in Topological Space,,false,"Let T be a topological space.

Let x be a point of T.

Let 𝔹(   )x be the set of all local bases at x.


The character of (the point) x in T is the smallest cardinality of the elements of 𝔹(   )x:
:χ(   )x, T := min{|ℬ|: ℬ∈𝔹(   )x }",Character
['Definitions/Banach Algebras'],Definition:Character,"Let $left( A, leftlVert , cdot , rightrVert  right)$ be a Banach algebra over $mathbb C$.

Let $phi : A to mathbb C$ be a non-zero algebra homomorphism on $A$.


We say that $phi$ is a character on $A$.",Definition:Character (Banach Algebra),,false,"Let ( A, ‖ · ‖) be a Banach algebra over ℂ.

Let ϕ : A →ℂ be a non-zero algebra homomorphism on A.


We say that ϕ is a character on A.",Character
"['Definitions/Characteristics of Rings', 'Definitions/Ring Theory']",Definition:Characteristic,"Let $left( R, +, circ right)$ be a ring with unity whose zero is $0_R$ and whose unity is $1_R$.


=== Definition 1 ===
Let $left( R, +, circ right)$ be a ring with unity whose zero is $0_R$ and whose unity is $1_R$.

For a natural number $n in mathbb N$, let $n cdot x$ be defined as the power of $x$ in the context of the additive group $left( R, + right)$:

:$n cdot x = begin {cases}
0_R & : n = 0 \
left( left( n - 1 right) cdot x right) + x & : n > 0
end {cases}$


The characteristic $mathrm {Char} left( R right)$ of $R$ is the smallest $n in mathbb N_{>0}$ such that $n cdot 1_R = 0_R$.

If there is no such $n$, then $mathrm {Char} left( R right) = 0$.

=== Definition 2 ===
Let $left( R, +, circ right)$ be a ring with unity whose zero is $0_R$ and whose unity is $1_R$.

Let $g: mathbb Z to R$ be the initial homomorphism, with $g left(   right)n = n cdot 1_R$.

Let $left( p right)$ be the principal ideal of $left( mathbb Z, +, times right)$ generated by $p$.


The characteristic $mathrm {Char} left( R right)$ of $R$ is the positive integer $p in mathbb Z_{ge 0}$ such that $left( p right)$ is the kernel of $g$.

=== Definition 3 ===
Let $left( R, +, circ right)$ be a ring with unity whose zero is $0_R$ and whose unity is $1_R$.


The characteristic of $R$, denoted $mathrm {Char} left( R right)$, is defined as follows.

Let $p$ be the order of $1_R$ in the additive group $left( R, + right)$ of $left( R, +, circ right)$.

If $p in mathbb Z_{>0}$, then $mathrm {Char} left( R right) := p$.

If $1_R$ is of infinite order, then $mathrm {Char} left( R right) := 0$.",Definition:Characteristic of Ring,,false,"Let ( R, +, ∘) be a ring with unity whose zero is 0_R and whose unity is 1_R.


=== Definition 1 ===
Let ( R, +, ∘) be a ring with unity whose zero is 0_R and whose unity is 1_R.

For a natural number n ∈ℕ, let n · x be defined as the power of x in the context of the additive group ( R, + ):

:n · x = 
0_R     : n = 0 
( ( n - 1 ) · x ) + x     : n > 0


The characteristic Char( R ) of R is the smallest n ∈ℕ_>0 such that n · 1_R = 0_R.

If there is no such n, then Char( R ) = 0.

=== Definition 2 ===
Let ( R, +, ∘) be a ring with unity whose zero is 0_R and whose unity is 1_R.

Let g: ℤ→ R be the initial homomorphism, with g (   )n = n · 1_R.

Let ( p ) be the principal ideal of ( ℤ, +, ×) generated by p.


The characteristic Char( R ) of R is the positive integer p ∈ℤ_≥ 0 such that ( p ) is the kernel of g.

=== Definition 3 ===
Let ( R, +, ∘) be a ring with unity whose zero is 0_R and whose unity is 1_R.


The characteristic of R, denoted Char( R ), is defined as follows.

Let p be the order of 1_R in the additive group ( R, + ) of ( R, +, ∘).

If p ∈ℤ_>0, then Char( R ) := p.

If 1_R is of infinite order, then Char( R ) := 0.",Characteristic
"['Definitions/Characteristics of Fields', 'Definitions/Field Theory']",Definition:Characteristic,"As a field is a fortiori a ring, the definition of characteristic carries over directly from that of the characteristic of a ring :

Let $left( R, +, circ right)$ be a ring with unity whose zero is $0_R$ and whose unity is $1_R$.


=== Definition 1 ===
Let $left( R, +, circ right)$ be a ring with unity whose zero is $0_R$ and whose unity is $1_R$.

For a natural number $n in mathbb N$, let $n cdot x$ be defined as the power of $x$ in the context of the additive group $left( R, + right)$:

:$n cdot x = begin {cases}
0_R & : n = 0 \
left( left( n - 1 right) cdot x right) + x & : n > 0
end {cases}$


The characteristic $mathrm {Char} left( R right)$ of $R$ is the smallest $n in mathbb N_{>0}$ such that $n cdot 1_R = 0_R$.

If there is no such $n$, then $mathrm {Char} left( R right) = 0$.

=== Definition 2 ===
Let $left( R, +, circ right)$ be a ring with unity whose zero is $0_R$ and whose unity is $1_R$.

Let $g: mathbb Z to R$ be the initial homomorphism, with $g left(   right)n = n cdot 1_R$.

Let $left( p right)$ be the principal ideal of $left( mathbb Z, +, times right)$ generated by $p$.


The characteristic $mathrm {Char} left( R right)$ of $R$ is the positive integer $p in mathbb Z_{ge 0}$ such that $left( p right)$ is the kernel of $g$.

=== Definition 3 ===
Let $left( R, +, circ right)$ be a ring with unity whose zero is $0_R$ and whose unity is $1_R$.


The characteristic of $R$, denoted $mathrm {Char} left( R right)$, is defined as follows.

Let $p$ be the order of $1_R$ in the additive group $left( R, + right)$ of $left( R, +, circ right)$.

If $p in mathbb Z_{>0}$, then $mathrm {Char} left( R right) := p$.

If $1_R$ is of infinite order, then $mathrm {Char} left( R right) := 0$.",Definition:Characteristic of Field,,false,"As a field is a fortiori a ring, the definition of characteristic carries over directly from that of the characteristic of a ring :

Let ( R, +, ∘) be a ring with unity whose zero is 0_R and whose unity is 1_R.


=== Definition 1 ===
Let ( R, +, ∘) be a ring with unity whose zero is 0_R and whose unity is 1_R.

For a natural number n ∈ℕ, let n · x be defined as the power of x in the context of the additive group ( R, + ):

:n · x = 
0_R     : n = 0 
( ( n - 1 ) · x ) + x     : n > 0


The characteristic Char( R ) of R is the smallest n ∈ℕ_>0 such that n · 1_R = 0_R.

If there is no such n, then Char( R ) = 0.

=== Definition 2 ===
Let ( R, +, ∘) be a ring with unity whose zero is 0_R and whose unity is 1_R.

Let g: ℤ→ R be the initial homomorphism, with g (   )n = n · 1_R.

Let ( p ) be the principal ideal of ( ℤ, +, ×) generated by p.


The characteristic Char( R ) of R is the positive integer p ∈ℤ_≥ 0 such that ( p ) is the kernel of g.

=== Definition 3 ===
Let ( R, +, ∘) be a ring with unity whose zero is 0_R and whose unity is 1_R.


The characteristic of R, denoted Char( R ), is defined as follows.

Let p be the order of 1_R in the additive group ( R, + ) of ( R, +, ∘).

If p ∈ℤ_>0, then Char( R ) := p.

If 1_R is of infinite order, then Char( R ) := 0.",Characteristic
['Definitions/Subgroups'],Definition:Characteristic,"Let $G$ be a group.

Let $H$ be a subgroup such that:
:$forall phi in mathrm {Aut} left( G right): phi left[ H right] = H$
where $mathrm {Aut} left( G right)$ is the automorphism group of $G$.


Then $H$ is  characteristic (in $G$), or a characteristic subgroup of $G$.",Definition:Characteristic Subgroup,,false,"Let G be a group.

Let H be a subgroup such that:
:∀ϕ∈Aut( G ): ϕ[ H ] = H
where Aut( G ) is the automorphism group of G.


Then H is  characteristic (in G), or a characteristic subgroup of G.",Characteristic
"['Definitions/Characteristic Matrices', 'Definitions/Matrices']",Definition:Characteristic,"Let $R$ be a commutative ring with unity.

Let $mathbf A$ be a square matrix over $R$ of order $n > 0$.

Let $mathbf I_n$ be the $n times n$ identity matrix.

Let $R left[ x right]$ be the polynomial ring in one variable over $R$.


The characteristic matrix of $mathbf A$ over $R left[ x right]$ is the square matrix:
:$mathbf I_n x - mathbf A$",Definition:Characteristic Matrix,,false,"Let R be a commutative ring with unity.

Let 𝐀 be a square matrix over R of order n > 0.

Let 𝐈_n be the n × n identity matrix.

Let R [ x ] be the polynomial ring in one variable over R.


The characteristic matrix of 𝐀 over R [ x ] is the square matrix:
:𝐈_n x - 𝐀",Characteristic
['Definitions/Characteristic Polynomials'],Definition:Characteristic,"A characteristic polynomial is a member of the class of polynomials which in some way allows one to sum up a number of characteristics of a particular mathematical object.


 

=== Matrix ===
Let $R$ be a commutative ring with unity.

Let $mathbf A$ be a square matrix over $R$ of order $n > 0$.

Let $mathbf I_n$ be the $n times n$ identity matrix.

Let $R left[ x right]$ be the polynomial ring in one variable over $R$.


The characteristic polynomial of $mathbf A$ is the determinant of the characteristic matrix of $mathbf A$ over $R left[ x right]$:
:$p_{mathbf A}  left(   right)x = det left(   right){mathbf I_n x - mathbf A}$

=== Linear Operator ===

Let $A$ be a commutative ring with unity.

Let $M$ be a free module over $A$ of finite rank $n > 0$.

Let $phi : M to M$ be a linear operator.

Let $A$ be a commutative ring with unity.

Let $M$ be a free module over $A$ of finite rank $n > 0$.

Let $phi : M to M$ be a linear operator.


The characteristic polynomial of $phi$ is the characteristic polynomial of the relative matrix of $phi$ with respect to a basis of $M$.

=== Field Extension ===
Let $K$ be a field.

Let $L / K$ be a finite field extension of $K$.

Then by Vector Space on Field Extension is Vector Space, $L$ is naturally a vector space over $K$.

Let $alpha in L$, and $theta_alpha$ be the linear operator:

:$theta_alpha: L to L : beta mapsto alpha beta$


The characteristic polynomial of $alpha$ with respect to the extension $L / K$ is:
:$det left[ X I_L - theta_alpha right]$

where:
:$det$ denotes the determinant of a linear operator
:$X$ is an indeterminate 
:$I_L$ is the identity mapping on $L$.

=== Element of Algebra ===
Let $A$ be a commutative ring with unity.

Let $B$ be an algebra over $A$ such that $B$ is a finite-dimensional free module over $A$.

Let $b in B$.


The characteristic polynomial of $b$ is the characteristic polynomial of the regular representation $lambda_b : B to B$ over $A$.

Category:Definitions/Characteristic Polynomials",Definition:Characteristic Polynomial,,false,"A characteristic polynomial is a member of the class of polynomials which in some way allows one to sum up a number of characteristics of a particular mathematical object.


 

=== Matrix ===
Let R be a commutative ring with unity.

Let 𝐀 be a square matrix over R of order n > 0.

Let 𝐈_n be the n × n identity matrix.

Let R [ x ] be the polynomial ring in one variable over R.


The characteristic polynomial of 𝐀 is the determinant of the characteristic matrix of 𝐀 over R [ x ]:
:p_𝐀(   )x = (   )𝐈_n x - 𝐀

=== Linear Operator ===

Let A be a commutative ring with unity.

Let M be a free module over A of finite rank n > 0.

Let ϕ : M → M be a linear operator.

Let A be a commutative ring with unity.

Let M be a free module over A of finite rank n > 0.

Let ϕ : M → M be a linear operator.


The characteristic polynomial of ϕ is the characteristic polynomial of the relative matrix of ϕ with respect to a basis of M.

=== Field Extension ===
Let K be a field.

Let L / K be a finite field extension of K.

Then by Vector Space on Field Extension is Vector Space, L is naturally a vector space over K.

Let α∈ L, and θ_α be the linear operator:

:θ_α: L → L : β↦αβ


The characteristic polynomial of α with respect to the extension L / K is:
:[ X I_L - θ_α]

where:
: denotes the determinant of a linear operator
:X is an indeterminate 
:I_L is the identity mapping on L.

=== Element of Algebra ===
Let A be a commutative ring with unity.

Let B be an algebra over A such that B is a finite-dimensional free module over A.

Let b ∈ B.


The characteristic polynomial of b is the characteristic polynomial of the regular representation λ_b : B → B over A.

Category:Definitions/Characteristic Polynomials",Characteristic
"['Definitions/Characteristic Equations', 'Definitions/Polynomial Theory', 'Definitions/Matrix Algebra', 'Definitions/Linear Algebra']",Definition:Characteristic,"Let $R$ be a commutative ring with unity.

Let $mathbf A$ be a square matrix over $R$ of order $n > 0$.

Let $mathbf I_n$ be the $n times n$ identity matrix.

Let $R left[ x right]$ be the polynomial ring in one variable over $R$.


The characteristic equation of $mathbf A$ is the equation defined as: determinant of the characteristic matrix of $mathbf A$ over $R left[ x right]$:
:$det left(   right){mathbf I_n x - mathbf A} = 0$
where $det left(   right){mathbf I_n x - mathbf A}$ is the characteristic polynomial of the characteristic matrix of $mathbf A$ over $R left[ x right]$.",Definition:Characteristic Equation of Matrix,,false,"Let R be a commutative ring with unity.

Let 𝐀 be a square matrix over R of order n > 0.

Let 𝐈_n be the n × n identity matrix.

Let R [ x ] be the polynomial ring in one variable over R.


The characteristic equation of 𝐀 is the equation defined as: determinant of the characteristic matrix of 𝐀 over R [ x ]:
:(   )𝐈_n x - 𝐀 = 0
where (   )𝐈_n x - 𝐀 is the characteristic polynomial of the characteristic matrix of 𝐀 over R [ x ].",Characteristic
"['Definitions/Auxiliary Equations', 'Definitions/Second Order ODEs']",Definition:Characteristic,"Let:
:$(1): quad y + p y' + q y = 0$
be a constant coefficient homogeneous linear second order ODE.


The auxiliary equation of $(1)$ is the quadratic equation:
:$m^2 + p m + q = 0$",Definition:Auxiliary Equation,,false,"Let:
:(1):    y + p y' + q y = 0
be a constant coefficient homogeneous linear second order ODE.


The auxiliary equation of (1) is the quadratic equation:
:m^2 + p m + q = 0",Characteristic
"['Definitions/Characteristic Functions of Sets', 'Definitions/Characteristic Functions']",Definition:Characteristic,"=== Set ===
Let $E subseteq S$.

The characteristic function of $E$ is the function $chi_E: S to leftlbrace 0, 1 rightrbrace$ defined as:
:$chi_E left(   right)x = begin {cases} 1 & : x in E  \  0 & : x notin E end {cases}$

That is:
:$chi_E left(   right)x = begin {cases} 1 & : x in E  \ 0 & : x in complement_{S} left(   right)E end {cases}$
where $complement_{S} left(   right)E$ denotes the complement of $E$ relative to $S$.


=== Support ===
Let $S$ be a set

Let $E subseteq S$ be a subset.

Let $chi_E: S to leftlbrace 0, 1 rightrbrace$ be the characteristic function of $E$.


The support of $chi_E$, denoted $mathrm {supp} left(   right){chi_E}$, is the set $E$.

That is:

:$mathrm {supp} left(   right){chi_E} = leftlbrace x in S: chi_E left(   right)x = 1 rightrbrace$

=== Relation ===
The concept of a characteristic function of a subset carries over directly to relations.


Let $mathcal R subseteq S times T$ be a relation.

The characteristic function of $mathcal R$ is the mapping $chi_mathcal R: S times T to leftlbrace 0, 1 rightrbrace$ defined as:
:$chi_mathcal R left(   right){x, y} = begin {cases} 1 & : left( x, y right) in mathcal R \ 0 & : left( x, y right) notin mathcal R end{cases}$


It can be expressed in Iverson bracket notation as:
:$chi_mathcal R left(   right){x, y} = left[ left( x, y right) in mathcal R right]$


More generally, let $ds mathbb S = prod_{i mathop = 1}^n S_i = S_1 times S_2 times ldots times S_n$ be the cartesian product of $n$ sets $S_1, S_2, ldots, S_n$.

Let $mathcal R subseteq mathbb S$ be an $n$-ary relation on $mathbb S$.

The characteristic function of $mathcal R$ is the mapping $chi_mathcal R: mathbb S to leftlbrace 0, 1 rightrbrace$ defined as:
:$chi_mathcal R left(   right){s_1, s_2, ldots, s_n} = begin {cases} 1 & : left( s_1, s_2, ldots, s_n right) in mathcal R \ 0 & : left( s_1, s_2, ldots, s_n right) notin mathcal R end {cases}$


It can be expressed in Iverson bracket notation as:
:$chi_mathcal R left(   right){s_1, s_2, ldots, s_n} = left[ left( s_1, s_2, ldots, s_n right) in mathcal R right]$",Definition:Characteristic Function (Set Theory),,false,"=== Set ===
Let E ⊆ S.

The characteristic function of E is the function χ_E: S →{ 0, 1 } defined as:
:χ_E (   )x =  1     : x ∈ E  
  0     : x ∉ E

That is:
:χ_E (   )x =  1     : x ∈ E  
 0     : x ∈∁_S(   )E
where ∁_S(   )E denotes the complement of E relative to S.


=== Support ===
Let S be a set

Let E ⊆ S be a subset.

Let χ_E: S →{ 0, 1 } be the characteristic function of E.


The support of χ_E, denoted supp(   )χ_E, is the set E.

That is:

:supp(   )χ_E = { x ∈ S: χ_E (   )x = 1 }

=== Relation ===
The concept of a characteristic function of a subset carries over directly to relations.


Let ℛ⊆ S × T be a relation.

The characteristic function of ℛ is the mapping χ_ℛ: S × T →{ 0, 1 } defined as:
:χ_ℛ(   )x, y =  1     : ( x, y ) ∈ℛ
 0     : ( x, y ) ∉ℛ


It can be expressed in Iverson bracket notation as:
:χ_ℛ(   )x, y = [ ( x, y ) ∈ℛ]


More generally, let 𝕊 = ∏_i  = 1^n S_i = S_1 × S_2 ×…× S_n be the cartesian product of n sets S_1, S_2, …, S_n.

Let ℛ⊆𝕊 be an n-ary relation on 𝕊.

The characteristic function of ℛ is the mapping χ_ℛ: 𝕊→{ 0, 1 } defined as:
:χ_ℛ(   )s_1, s_2, …, s_n =  1     : ( s_1, s_2, …, s_n ) ∈ℛ
 0     : ( s_1, s_2, …, s_n ) ∉ℛ


It can be expressed in Iverson bracket notation as:
:χ_ℛ(   )s_1, s_2, …, s_n = [ ( s_1, s_2, …, s_n ) ∈ℛ]",Characteristic
['Definitions/Logarithms'],Definition:Characteristic,"Let $n in mathbb R$ be a positive real number such that $0 < n < 1$.

Let $n$ be presented (possibly approximated) in scientific notation as:
:$a times 10^d$
where $d in mathbb Z$ is an integer.


Let $log_{10} n$ be expressed in the form:
:$log_{10} n = begin {cases} c cdotp m & : d ge 0 \ overline c cdotp m & : d < 0 end {cases}$
where:
:$c = leftlvert d rightrvert$ is the absolute value of $d$
:$m := log_{10} a$


$c$ is the characteristic of $log_{10} n$.",Definition:General Logarithm/Common/Characteristic,,false,"Let n ∈ℝ be a positive real number such that 0 < n < 1.

Let n be presented (possibly approximated) in scientific notation as:
:a × 10^d
where d ∈ℤ is an integer.


Let log_10 n be expressed in the form:
:log_10 n =  c  m     : d ≥ 0 
c m     : d < 0
where:
:c = | d | is the absolute value of d
:m := log_10 a


c is the characteristic of log_10 n.",Characteristic
"['Definitions/Characteristic Functions of Sets', 'Definitions/Set Theory']",Definition:Characteristic Function,"Let $E subseteq S$.

The characteristic function of $E$ is the function $chi_E: S to leftlbrace 0, 1 rightrbrace$ defined as:
:$chi_E left(   right)x = begin {cases} 1 & : x in E  \  0 & : x notin E end {cases}$

That is:
:$chi_E left(   right)x = begin {cases} 1 & : x in E  \ 0 & : x in complement_{S} left(   right)E end {cases}$
where $complement_{S} left(   right)E$ denotes the complement of $E$ relative to $S$.


=== Support ===
Let $S$ be a set

Let $E subseteq S$ be a subset.

Let $chi_E: S to leftlbrace 0, 1 rightrbrace$ be the characteristic function of $E$.


The support of $chi_E$, denoted $mathrm {supp} left(   right){chi_E}$, is the set $E$.

That is:

:$mathrm {supp} left(   right){chi_E} = leftlbrace x in S: chi_E left(   right)x = 1 rightrbrace$",Definition:Characteristic Function (Set Theory)/Set,,false,"Let E ⊆ S.

The characteristic function of E is the function χ_E: S →{ 0, 1 } defined as:
:χ_E (   )x =  1     : x ∈ E  
  0     : x ∉ E

That is:
:χ_E (   )x =  1     : x ∈ E  
 0     : x ∈∁_S(   )E
where ∁_S(   )E denotes the complement of E relative to S.


=== Support ===
Let S be a set

Let E ⊆ S be a subset.

Let χ_E: S →{ 0, 1 } be the characteristic function of E.


The support of χ_E, denoted supp(   )χ_E, is the set E.

That is:

:supp(   )χ_E = { x ∈ S: χ_E (   )x = 1 }",Characteristic Function
"['Definitions/Characteristic Functions of Sets', 'Definitions/Relation Theory']",Definition:Characteristic Function,"The concept of a characteristic function of a subset carries over directly to relations.


Let $mathcal R subseteq S times T$ be a relation.

The characteristic function of $mathcal R$ is the mapping $chi_mathcal R: S times T to leftlbrace 0, 1 rightrbrace$ defined as:
:$chi_mathcal R left(   right){x, y} = begin {cases} 1 & : left( x, y right) in mathcal R \ 0 & : left( x, y right) notin mathcal R end{cases}$


It can be expressed in Iverson bracket notation as:
:$chi_mathcal R left(   right){x, y} = left[ left( x, y right) in mathcal R right]$


More generally, let $ds mathbb S = prod_{i mathop = 1}^n S_i = S_1 times S_2 times ldots times S_n$ be the cartesian product of $n$ sets $S_1, S_2, ldots, S_n$.

Let $mathcal R subseteq mathbb S$ be an $n$-ary relation on $mathbb S$.

The characteristic function of $mathcal R$ is the mapping $chi_mathcal R: mathbb S to leftlbrace 0, 1 rightrbrace$ defined as:
:$chi_mathcal R left(   right){s_1, s_2, ldots, s_n} = begin {cases} 1 & : left( s_1, s_2, ldots, s_n right) in mathcal R \ 0 & : left( s_1, s_2, ldots, s_n right) notin mathcal R end {cases}$


It can be expressed in Iverson bracket notation as:
:$chi_mathcal R left(   right){s_1, s_2, ldots, s_n} = left[ left( s_1, s_2, ldots, s_n right) in mathcal R right]$",Definition:Characteristic Function (Set Theory)/Relation,,false,"The concept of a characteristic function of a subset carries over directly to relations.


Let ℛ⊆ S × T be a relation.

The characteristic function of ℛ is the mapping χ_ℛ: S × T →{ 0, 1 } defined as:
:χ_ℛ(   )x, y =  1     : ( x, y ) ∈ℛ
 0     : ( x, y ) ∉ℛ


It can be expressed in Iverson bracket notation as:
:χ_ℛ(   )x, y = [ ( x, y ) ∈ℛ]


More generally, let 𝕊 = ∏_i  = 1^n S_i = S_1 × S_2 ×…× S_n be the cartesian product of n sets S_1, S_2, …, S_n.

Let ℛ⊆𝕊 be an n-ary relation on 𝕊.

The characteristic function of ℛ is the mapping χ_ℛ: 𝕊→{ 0, 1 } defined as:
:χ_ℛ(   )s_1, s_2, …, s_n =  1     : ( s_1, s_2, …, s_n ) ∈ℛ
 0     : ( s_1, s_2, …, s_n ) ∉ℛ


It can be expressed in Iverson bracket notation as:
:χ_ℛ(   )s_1, s_2, …, s_n = [ ( s_1, s_2, …, s_n ) ∈ℛ]",Characteristic Function
"['Definitions/Characteristic Functions of Random Variables', 'Definitions/Random Variables', 'Definitions/Probability Theory', 'Definitions/Characteristic Functions']",Definition:Characteristic Function,"Let $left( Omega, unicode{x3a3}, Pr right)$ be a probability space.

Let $X$ be a real-valued random variable on $left( Omega, unicode{x3a3}, Pr right)$.


The characteristic function of $X$ is the mapping $phi: mathbb R to mathbb C$ defined by:

:$phi left(   right)t = mathsf E left( e^{i t X}  right)$

where:
:$i$ is the imaginary unit
:$mathsf E left( cdot right)$ denotes expectation.",Definition:Characteristic Function of Random Variable,,false,"Let ( Ω, x3a3, ) be a probability space.

Let X be a real-valued random variable on ( Ω, x3a3, ).


The characteristic function of X is the mapping ϕ: ℝ→ℂ defined by:

:ϕ(   )t = 𝖤( e^i t X)

where:
:i is the imaginary unit
:𝖤( ·) denotes expectation.",Characteristic Function
"['Definitions/Abstract Algebra', 'Definitions/Zero']",Definition:Cipher,"The number zero is defined as being the cardinal of the empty set.


=== Naturally Ordered Semigroup ===
Let $left( S, circ, preceq right)$ be a naturally ordered semigroup.

Then from  , $left( S, circ, preceq right)$ has a smallest element.


This smallest element of $left( S, circ, preceq right)$ is called zero and has the symbol $0$.

That is:
:$forall n in S: 0 preceq n$

=== Natural Numbers ===

=== Integers ===

=== Rational Numbers ===

=== Real Numbers ===

=== Complex Numbers ===
Let $mathbb C$ denote the set of complex numbers.

The zero of $mathbb C$ is the complex number:
:$0 + 0 i$

 ",Definition:Zero (Number),,false,"The number zero is defined as being the cardinal of the empty set.


=== Naturally Ordered Semigroup ===
Let ( S, ∘, ≼) be a naturally ordered semigroup.

Then from  , ( S, ∘, ≼) has a smallest element.


This smallest element of ( S, ∘, ≼) is called zero and has the symbol 0.

That is:
:∀ n ∈ S: 0 ≼ n

=== Natural Numbers ===

=== Integers ===

=== Rational Numbers ===

=== Real Numbers ===

=== Complex Numbers ===
Let ℂ denote the set of complex numbers.

The zero of ℂ is the complex number:
:0 + 0 i

 ",Cipher
"['Definitions/Digits', 'Definitions/Numbers']",Definition:Cipher,"Let $n$ be a number expressed in a particular number base, $b$ for example.

Then $n$ can be expressed as:

:$left[ r_m r_{m - 1} ldots r_2 r_1 r_0 . r_{-1} r_{-2} ldots right]_b$

where:
:$m$ is such that $b^m le n < b^{m + 1}$;
:all the $r_i$ are such that $0 le r_i < b$.

Each of the $r_i$ are known as the digits of $n$ (base $b$).


It is taken for granted that for base $10$ working, the digits are elements of the set of Arabic numerals: $leftlbrace 0, 1, 2, 3, 4, 5, 6, 7, 8, 9 rightrbrace$.",Definition:Digit,,false,"Let n be a number expressed in a particular number base, b for example.

Then n can be expressed as:

:[ r_m r_m - 1… r_2 r_1 r_0 . r_-1 r_-2…]_b

where:
:m is such that b^m ≤ n < b^m + 1;
:all the r_i are such that 0 ≤ r_i < b.

Each of the r_i are known as the digits of n (base b).


It is taken for granted that for base 10 working, the digits are elements of the set of Arabic numerals: { 0, 1, 2, 3, 4, 5, 6, 7, 8, 9 }.",Cipher
['Definitions/Algorism'],Definition:Cipher,"The word cipher is an archaic word meaning to calculate using the technique of algorism, that is, using digits as opposed to using an abacus.",Definition:Cipher (Algorism),,false,"The word cipher is an archaic word meaning to calculate using the technique of algorism, that is, using digits as opposed to using an abacus.",Cipher
"['Definitions/Ciphers (Cryptography)', 'Definitions/Cryptography']",Definition:Cipher,"A cipher, in the context of cryptography, is an algorithm used to transform a message into another string of symbols in order to hide its meaning from a third party.

A cipher operates by replacing each symbol in the original string with a word in some alphabet.

The specific nature of the replacement depends on the specifics of the cipher.


=== Plaintext ===
In the context of cryptography, the plaintext is the message being transformed into an encrypted string.

=== Ciphertext ===
In the context of cryptography, the ciphertext is the ciphered string after the plaintext has been processed by the cipher.

=== Key ===
In the context of cryptography, a key is a piece of information which:
:is used to convert plaintext to ciphertext
:allows the ciphertext to be transformed back into the original plaintext.

These two processes may use different keys.",Definition:Cipher (Cryptography),,false,"A cipher, in the context of cryptography, is an algorithm used to transform a message into another string of symbols in order to hide its meaning from a third party.

A cipher operates by replacing each symbol in the original string with a word in some alphabet.

The specific nature of the replacement depends on the specifics of the cipher.


=== Plaintext ===
In the context of cryptography, the plaintext is the message being transformed into an encrypted string.

=== Ciphertext ===
In the context of cryptography, the ciphertext is the ciphered string after the plaintext has been processed by the cipher.

=== Key ===
In the context of cryptography, a key is a piece of information which:
:is used to convert plaintext to ciphertext
:allows the ciphertext to be transformed back into the original plaintext.

These two processes may use different keys.",Cipher
"['Definitions/Ciphers (Cryptography)', 'Definitions/Cryptography']",Definition:Cipher,"In the context of cryptography, the ciphertext is the ciphered string after the plaintext has been processed by the cipher.",Definition:Cipher (Cryptography)/Ciphertext,,false,"In the context of cryptography, the ciphertext is the ciphered string after the plaintext has been processed by the cipher.",Cipher
"['Definitions/Circuits (Graph Theory)', 'Definitions/Graph Theory']",Definition:Circuit,"A circuit is a closed trail with at least one edge.


=== Subgraph ===
The set of vertices and edges which go to make up a circuit form a subgraph.

This subgraph itself is also referred to as a circuit.",Definition:Circuit (Graph Theory),trail,true,"A circuit is a closed trail with at least one edge.


=== Subgraph ===
The set of vertices and edges which go to make up a circuit form a subgraph.

This subgraph itself is also referred to as a circuit.",Circuit
['Definitions/Matroid Theory'],Definition:Circuit,"Let $M = left( S, mathscr I right)$ be a matroid.


A circuit of $M$ is a dependent subset of $S$ which is a minimal dependent subset with respect to the subset ordering.",Definition:Circuit (Matroid),,false,"Let M = ( S, ℐ) be a matroid.


A circuit of M is a dependent subset of S which is a minimal dependent subset with respect to the subset ordering.",Circuit
['Definitions/Electronics'],Definition:Circuit,"An electric circuit is a configuration of electrical components whose collective properties can be modelled by means of a network each of whose edges corresponds to a specific component.

Its purpose is to guide and direct energy within a device.",Definition:Electric Circuit,,false,"An electric circuit is a configuration of electrical components whose collective properties can be modelled by means of a network each of whose edges corresponds to a specific component.

Its purpose is to guide and direct energy within a device.",Circuit
['Definitions/Circumference of Geometric Figure'],Definition:Circumference,"The circumference of a geometric figure is the line that forms its boundary.


=== Circumference of Circle ===
The circumference of a circle is the line that forms its boundary.


=== Concave Circumference ===
The convex circumference of a circle $C$ is the circumference $C$ from a point outside $C$.

=== Concave Circumference ===
The concave circumference of a circle $C$ is the circumference $C$ from a point inside $C$.

=== Circumference of Sphere ===
The circumference of a sphere is the circumference of a great circle of the sphere.",Definition:Circumference of Geometric Figure,,false,"The circumference of a geometric figure is the line that forms its boundary.


=== Circumference of Circle ===
The circumference of a circle is the line that forms its boundary.


=== Concave Circumference ===
The convex circumference of a circle C is the circumference C from a point outside C.

=== Concave Circumference ===
The concave circumference of a circle C is the circumference C from a point inside C.

=== Circumference of Sphere ===
The circumference of a sphere is the circumference of a great circle of the sphere.",Circumference
"['Definitions/Circles', 'Definitions/Circumference of Geometric Figure']",Definition:Circumference,"The circumference of a circle is the line that forms its boundary.


=== Concave Circumference ===
The convex circumference of a circle $C$ is the circumference $C$ from a point outside $C$.

=== Concave Circumference ===
The concave circumference of a circle $C$ is the circumference $C$ from a point inside $C$.",Definition:Circle/Circumference,line,true,"The circumference of a circle is the line that forms its boundary.


=== Concave Circumference ===
The convex circumference of a circle C is the circumference C from a point outside C.

=== Concave Circumference ===
The concave circumference of a circle C is the circumference C from a point inside C.",Circumference
"['Definitions/Spheres', 'Definitions/Circumference of Geometric Figure']",Definition:Circumference,The circumference of a sphere is the circumference of a great circle of the sphere.,Definition:Circumference of Sphere,circumference,true,The circumference of a sphere is the circumference of a great circle of the sphere.,Circumference
['Definitions/Graph Theory'],Definition:Circumference,"Let $G$ be a graph.

The circumference of $G$ is the longest length of any cycle in $G$.


An acyclic graph is defined as having a circumference of infinity.

Category:Definitions/Graph Theory",Definition:Circumference (Graph Theory),,false,"Let G be a graph.

The circumference of G is the longest length of any cycle in G.


An acyclic graph is defined as having a circumference of infinity.

Category:Definitions/Graph Theory",Circumference
"['Definitions/Branches of Mathematics', 'Definitions/Class Theory', 'Definitions/Set Theory']",Definition:Class,Class theory is an extension of set theory which allows the creation of collections that are not sets by classes.,Definition:Class Theory,,false,Class theory is an extension of set theory which allows the creation of collections that are not sets by classes.,Class
['Definitions/Class Theory'],Definition:Class,"A class is a collection of all sets such that a particular condition holds.


In class-builder notation, this is written as:

:$leftlbrace x: p left(   right)x rightrbrace$

where $p left(   right)x$ is a statement containing $x$ as a free variable.  

This is read:
:All $x$ such that $p left(   right)x$ holds.",Definition:Class (Class Theory),sets,true,"A class is a collection of all sets such that a particular condition holds.


In class-builder notation, this is written as:

:{ x: p (   )x }

where p (   )x is a statement containing x as a free variable.  

This is read:
:All x such that p (   )x holds.",Class
"['Definitions/Class Intervals', 'Definitions/Descriptive Statistics']",Definition:Class,"Let $D$ be a finite set of $n$ observations of a quantitative variable.


=== Integer Data ===
Let $D$ be a finite collection of $n$ data regarding some quantitative variable.

Let the data in $D$ be described by integers.

Let $d_{min}$ be the value of the smallest datum in $D$.

Let $d_{max}$ be the value of the largest datum in $D$.
 
Let $P = leftlbrace x_0, x_1, x_2, ldots, x_{n - 1}, x_n rightrbrace subseteq mathbb Z$ be a subdivision of $left[ a ,.,.,   right]b$, where $a le x_0 le x_n le b$.


The integer interval $left[ a ,.,.,   right]b$, where $a le d_{min} le d_max le b$, is said to be divided into class intervals of integer intervals of the forms $left[ x_i ,.,.,   right]{x_{i + 1} }$ or $left[ x_i ,.,.,   right]{x_i}$  if and only if :

:Every datum is assigned into exactly one class interval

:Every class interval is disjoint from every other class interval

:The union of all class intervals contains the entire integer interval $left[ x_0 ,.,.,   right]{x_n}$

By convention, the first and last class intervals are not empty class intervals.

=== Real Data ===
Let $D$ be a finite collection of $n$ data regarding some quantitative variable.

Let the data in $D$ be described by rational numbers or by real numbers.

Let $d_{min}$ be the value of the smallest datum in $D$.

Let $d_{max}$ be the value of the largest datum in $D$.
 
Let $P = leftlbrace x_0, x_1, x_2, ldots, x_{n - 1}, x_n rightrbrace subseteq mathbb R$ be a subdivision of $left[ a ,.,.,   right]b$, where $a le x_0 le x_n le b$.


The closed real interval $left[ a ,.,.,   right]b$, where $a le d_{text {min}} le d_{text {max}} le b$, is said to be divided into class intervals of real intervals with endpoints $x_i$ and $x_{i + 1}$  if and only if :

:Every datum is assigned into exactly one class interval

:Every class interval is disjoint from every other class interval

:The union of all class intervals contains the entire real interval $left[ x_0 ,.,.,   right]{x_n}$


The class intervals may be any combination of open, closed, or half-open intervals that fulfill the above criteria, but usually:

:Every class interval except the last is of the form $left[ x_i ,.,.,   right]{x_{i + 1} }$

:The last class interval is of the form $left[ x_{n - 1}  ,.,.,   right]{x_n}$ 

By convention, the first and last class intervals are not empty class intervals.

=== Boundary of Class Interval ===
The class boundaries of a class interval are the endpoints of the integer interval or real interval which defines the class interval.

=== Class Mark ===
A class mark is a value within a class interval used to identify that class interval uniquely.

It is usual to use the midpoint.


=== Class Midpoint ===
When a class mark is defined to be the midpoint of the class interval it identifies, it is often called the class midpoint.

=== Empty Class Interval ===
A class interval is empty  if and only if  it is of frequency zero.

=== Relative Sizes of Class Interval ===
",Definition:Class Interval,,false,"Let D be a finite set of n observations of a quantitative variable.


=== Integer Data ===
Let D be a finite collection of n data regarding some quantitative variable.

Let the data in D be described by integers.

Let d_min be the value of the smallest datum in D.

Let d_max be the value of the largest datum in D.
 
Let P = { x_0, x_1, x_2, …, x_n - 1, x_n }⊆ℤ be a subdivision of [ a  . . ]b, where a ≤ x_0 ≤ x_n ≤ b.


The integer interval [ a  . . ]b, where a ≤ d_min≤ d_max≤ b, is said to be divided into class intervals of integer intervals of the forms [ x_i  . . ]x_i + 1 or [ x_i  . . ]x_i  if and only if :

:Every datum is assigned into exactly one class interval

:Every class interval is disjoint from every other class interval

:The union of all class intervals contains the entire integer interval [ x_0  . . ]x_n

By convention, the first and last class intervals are not empty class intervals.

=== Real Data ===
Let D be a finite collection of n data regarding some quantitative variable.

Let the data in D be described by rational numbers or by real numbers.

Let d_min be the value of the smallest datum in D.

Let d_max be the value of the largest datum in D.
 
Let P = { x_0, x_1, x_2, …, x_n - 1, x_n }⊆ℝ be a subdivision of [ a  . . ]b, where a ≤ x_0 ≤ x_n ≤ b.


The closed real interval [ a  . . ]b, where a ≤ d_min≤ d_max≤ b, is said to be divided into class intervals of real intervals with endpoints x_i and x_i + 1  if and only if :

:Every datum is assigned into exactly one class interval

:Every class interval is disjoint from every other class interval

:The union of all class intervals contains the entire real interval [ x_0  . . ]x_n


The class intervals may be any combination of open, closed, or half-open intervals that fulfill the above criteria, but usually:

:Every class interval except the last is of the form [ x_i  . . ]x_i + 1

:The last class interval is of the form [ x_n - 1 . . ]x_n 

By convention, the first and last class intervals are not empty class intervals.

=== Boundary of Class Interval ===
The class boundaries of a class interval are the endpoints of the integer interval or real interval which defines the class interval.

=== Class Mark ===
A class mark is a value within a class interval used to identify that class interval uniquely.

It is usual to use the midpoint.


=== Class Midpoint ===
When a class mark is defined to be the midpoint of the class interval it identifies, it is often called the class midpoint.

=== Empty Class Interval ===
A class interval is empty  if and only if  it is of frequency zero.

=== Relative Sizes of Class Interval ===
",Class
"['Definitions/Differential Calculus', 'Definitions/Continuity', 'Definitions/Differentiability Classes']",Definition:Class,"Let $f: mathbb R to mathbb R$ be a real function.

Then $f left(   right)x$ is of differentiability class $C^k$  if and only if :
:$dfrac {mathrm d^k} {mathrm d x^k} f left(   right)x in C$

where $C$ denotes the class of continuous real functions.


That is, $f$ is in differentiability class $k$  if and only if  there exists a $k$th derivative of $f$ which is continuous.


If $dfrac {mathrm d^k} {mathrm d x^k} f left(   right)x$ is continuous for all $k in mathbb N$, then $f left(   right)x$ is of differentiability class $C^infty$.",Definition:Differentiability Class,,false,"Let f: ℝ→ℝ be a real function.

Then f (   )x is of differentiability class C^k  if and only if :
:d^kd x^k f (   )x ∈ C

where C denotes the class of continuous real functions.


That is, f is in differentiability class k  if and only if  there exists a kth derivative of f which is continuous.


If d^kd x^k f (   )x is continuous for all k ∈ℕ, then f (   )x is of differentiability class C^∞.",Class
"['Definitions/Homotopy Classes', 'Definitions/Homotopy Theory', 'Definitions/Algebraic Topology']",Definition:Class,"Let $X$ and $Y$ be topological spaces.

Let $K subseteq X$ be any subset.

Let $f : X to Y$ be a continuous mapping.

The $K$-homotopy class of $f$ is the equivalence class of $f$ under the equivalence relation defined by homotopy relative to $K$.


=== Homotopy Class of Path ===
Let $T = left( S, tau right)$ be a topological space.

Let $f: left[ 0 ,.,.,   right]1 to S$ be a path in $T$.


The homotopy class of the path $f$ is the homotopy class of $f$ relative to $leftlbrace 0, 1 rightrbrace$.


That is, the equivalence class of $f$ under the equivalence relation defined by path-homotopy.",Definition:Homotopy Class,,false,"Let X and Y be topological spaces.

Let K ⊆ X be any subset.

Let f : X → Y be a continuous mapping.

The K-homotopy class of f is the equivalence class of f under the equivalence relation defined by homotopy relative to K.


=== Homotopy Class of Path ===
Let T = ( S, τ) be a topological space.

Let f: [ 0  . . ]1 → S be a path in T.


The homotopy class of the path f is the homotopy class of f relative to { 0, 1 }.


That is, the equivalence class of f under the equivalence relation defined by path-homotopy.",Class
"['Definitions/Equivalence Classes', 'Definitions/Equivalence Relations']",Definition:Class,"Let $S$ be a set.

Let $mathcal R subseteq S times S$ be an equivalence relation on $S$.

Let $x in S$.


Then the equivalence class of $x$ under $mathcal R$ is the set:
:$left[!left[ x right]!right]_{ }mathcal R = leftlbrace y in S: left( x, y right) in mathcal R rightrbrace$


If $mathcal R$ is an equivalence on $S$, then each $t in S$ that satisfies $left( x, t right) in mathcal R$ (or $left( t, x right) in mathcal R$) is called a $mathcal R$-relative of $x$.


That is, the equivalence class of $x$ under $mathcal R$ is the set of all $mathcal R$-relatives of $x$.


=== Representative of Equivalence Class ===
Let $S$ be a set.

Let $mathcal R subseteq S times S$ be an equivalence relation on $S$.

Let $x in S$.


Let $left[!left[ x right]!right]_{ }mathcal R$ be the equivalence class of $x$ under $mathcal R$.

Let $y in left[!left[ x right]!right]_{ }mathcal R$.

Then $y$ is a representative of $left[!left[ x right]!right]_{ }mathcal R$.",Definition:Equivalence Class,,false,"Let S be a set.

Let ℛ⊆ S × S be an equivalence relation on S.

Let x ∈ S.


Then the equivalence class of x under ℛ is the set:
:[[ x ]]_ℛ = { y ∈ S: ( x, y ) ∈ℛ}


If ℛ is an equivalence on S, then each t ∈ S that satisfies ( x, t ) ∈ℛ (or ( t, x ) ∈ℛ) is called a ℛ-relative of x.


That is, the equivalence class of x under ℛ is the set of all ℛ-relatives of x.


=== Representative of Equivalence Class ===
Let S be a set.

Let ℛ⊆ S × S be an equivalence relation on S.

Let x ∈ S.


Let [[ x ]]_ℛ be the equivalence class of x under ℛ.

Let y ∈[[ x ]]_ℛ.

Then y is a representative of [[ x ]]_ℛ.",Class
['Definitions/Conjugacy'],Definition:Class,"The equivalence classes into which the conjugacy relation divides its group into are called conjugacy classes.

The conjugacy class of an element $x in G$ can be denoted $mathrm C_{x}$.",Definition:Conjugacy Class,,false,"The equivalence classes into which the conjugacy relation divides its group into are called conjugacy classes.

The conjugacy class of an element x ∈ G can be denoted C_x.",Class
"['Definitions/Mathematical Logic', 'Definitions/Computability Theory']",Definition:Class,"In computability theory, $NP$ is a complexity class in which there exists some nondeterministic Turing machine that will either:
:halt within $p left(   right){leftlvert x rightrvert}$ steps
or:
:run forever

where:
:$p$ is a polynomial
:$leftlvert x rightrvert$ is the length of the input to the Machine.",Definition:NP Complexity Class,,false,"In computability theory, NP is a complexity class in which there exists some nondeterministic Turing machine that will either:
:halt within p (   )| x | steps
or:
:run forever

where:
:p is a polynomial
:| x | is the length of the input to the Machine.",Class
['Definitions/Predicate Logic'],Definition:Closed,"Let $P$ be a statement.

$P$ is a closed statement  if and only if  $P$ contains only bound occurrences of any variables that may appear in it.


That is, such that it contains no free occurrences of variables.",Definition:Closed Statement,,false,"Let P be a statement.

P is a closed statement  if and only if  P contains only bound occurrences of any variables that may appear in it.


That is, such that it contains no free occurrences of variables.",Closed
"['Definitions/Topology', 'Definitions/Closed Sets']",Definition:Closed,"=== Topology ===

Let $T = left( S, tau right)$ be a topological space.

Let $H subseteq S$.
Let $T = left({S, tau}right)$ be a topological space.

Let $H subseteq S$.


=== Definition 1 ===
Let $T = left( S, tau right)$ be a topological space.

Let $H subseteq S$.

$H$ is closed (in $T$)  if and only if  its complement $S setminus H$ is open in $T$. 

That is, $H$ is closed  if and only if  $left( S setminus H right) in tau$.

That is,  if and only if  $S setminus H$ is an element of the topology of $T$.

=== Definition 2 ===
Let $T = left({S, tau}right)$ be a topological space.

Let $H subseteq S$.

$H$ is closed (in $T$)  if and only if  every limit point of $H$ is also a point of $H$.

That is, by the definition of the derived set:
:$H$ is closed (in $T$)  if and only if  $H' subseteq H$
where $H'$ denotes the derived set of $H$.

=== Metric Space ===

In the context of metric spaces, the same definition applies:
Let $M = left({A, d}right)$ be a metric space.

Let $H subseteq A$.


=== Definition 1 ===
Let $M = left( A, d right)$ be a metric space.

Let $H subseteq A$.


$H$ is closed (in $M$)  if and only if  its complement $A setminus H$ is open in $M$.

=== Definition 2 ===
Let $M = left({A, d}right)$ be a metric space.

Let $H subseteq A$.


$H$ is closed (in $M$)  if and only if  every limit point of $H$ is also a point of $H$.

=== Normed Vector Space ===
Let $V = left( X, leftlVert ,cdot, rightrVert  right)$ be a normed vector space.

Let $F subset X$.


==== Definition 1 ====
Let $V = left( X, leftlVert ,cdot, rightrVert  right)$ be a normed vector space.

Let $F subset X$.


$F$ is closed in $V$  if and only if  its complement $X setminus F$ is open in $V$.

==== Definition 2 ====
Let $V = left( X, leftlVert ,cdot, rightrVert  right)$ be a normed vector space.

Let $F subset X$.


$F$ is closed (in $V$)  if and only if  every limit point of $F$ is also a point of $F$.


That is:  if and only if  $F$ contains all its limit points.

=== Complex Analysis ===
Let $S subseteq mathbb C$ be a subset of the complex plane.

$S$ is closed (in $mathbb C$)  if and only if  every limit point of $S$ is also a point of $S$.


That is:  if and only if  $S$ contains all its limit points.

=== Real Analysis ===
Let $S subseteq mathbb R$ be a subset of the set of real numbers.


Then $S$ is closed (in $mathbb R$)  if and only if  its complement $mathbb R setminus S$ is an open set.",Definition:Closed Set,,false,"=== Topology ===

Let T = ( S, τ) be a topological space.

Let H ⊆ S.
Let T = (S, τ) be a topological space.

Let H ⊆ S.


=== Definition 1 ===
Let T = ( S, τ) be a topological space.

Let H ⊆ S.

H is closed (in T)  if and only if  its complement S ∖ H is open in T. 

That is, H is closed  if and only if  ( S ∖ H ) ∈τ.

That is,  if and only if  S ∖ H is an element of the topology of T.

=== Definition 2 ===
Let T = (S, τ) be a topological space.

Let H ⊆ S.

H is closed (in T)  if and only if  every limit point of H is also a point of H.

That is, by the definition of the derived set:
:H is closed (in T)  if and only if  H' ⊆ H
where H' denotes the derived set of H.

=== Metric Space ===

In the context of metric spaces, the same definition applies:
Let M = (A, d) be a metric space.

Let H ⊆ A.


=== Definition 1 ===
Let M = ( A, d ) be a metric space.

Let H ⊆ A.


H is closed (in M)  if and only if  its complement A ∖ H is open in M.

=== Definition 2 ===
Let M = (A, d) be a metric space.

Let H ⊆ A.


H is closed (in M)  if and only if  every limit point of H is also a point of H.

=== Normed Vector Space ===
Let V = ( X, ‖ · ‖) be a normed vector space.

Let F ⊂ X.


==== Definition 1 ====
Let V = ( X, ‖ · ‖) be a normed vector space.

Let F ⊂ X.


F is closed in V  if and only if  its complement X ∖ F is open in V.

==== Definition 2 ====
Let V = ( X, ‖ · ‖) be a normed vector space.

Let F ⊂ X.


F is closed (in V)  if and only if  every limit point of F is also a point of F.


That is:  if and only if  F contains all its limit points.

=== Complex Analysis ===
Let S ⊆ℂ be a subset of the complex plane.

S is closed (in ℂ)  if and only if  every limit point of S is also a point of S.


That is:  if and only if  S contains all its limit points.

=== Real Analysis ===
Let S ⊆ℝ be a subset of the set of real numbers.


Then S is closed (in ℝ)  if and only if  its complement ℝ∖ S is an open set.",Closed
['Definitions/Closed Sets'],Definition:Closed,"Let $T = left({S, tau}right)$ be a topological space.

Let $H subseteq S$.


=== Definition 1 ===
Let $T = left( S, tau right)$ be a topological space.

Let $H subseteq S$.

$H$ is closed (in $T$)  if and only if  its complement $S setminus H$ is open in $T$. 

That is, $H$ is closed  if and only if  $left( S setminus H right) in tau$.

That is,  if and only if  $S setminus H$ is an element of the topology of $T$.

=== Definition 2 ===
Let $T = left({S, tau}right)$ be a topological space.

Let $H subseteq S$.

$H$ is closed (in $T$)  if and only if  every limit point of $H$ is also a point of $H$.

That is, by the definition of the derived set:
:$H$ is closed (in $T$)  if and only if  $H' subseteq H$
where $H'$ denotes the derived set of $H$.",Definition:Closed Set/Topology,,false,"Let T = (S, τ) be a topological space.

Let H ⊆ S.


=== Definition 1 ===
Let T = ( S, τ) be a topological space.

Let H ⊆ S.

H is closed (in T)  if and only if  its complement S ∖ H is open in T. 

That is, H is closed  if and only if  ( S ∖ H ) ∈τ.

That is,  if and only if  S ∖ H is an element of the topology of T.

=== Definition 2 ===
Let T = (S, τ) be a topological space.

Let H ⊆ S.

H is closed (in T)  if and only if  every limit point of H is also a point of H.

That is, by the definition of the derived set:
:H is closed (in T)  if and only if  H' ⊆ H
where H' denotes the derived set of H.",Closed
"['Definitions/Closed Sets', 'Definitions/Metric Spaces']",Definition:Closed,"Let $M = left({A, d}right)$ be a metric space.

Let $H subseteq A$.


=== Definition 1 ===
Let $M = left( A, d right)$ be a metric space.

Let $H subseteq A$.


$H$ is closed (in $M$)  if and only if  its complement $A setminus H$ is open in $M$.

=== Definition 2 ===
Let $M = left({A, d}right)$ be a metric space.

Let $H subseteq A$.


$H$ is closed (in $M$)  if and only if  every limit point of $H$ is also a point of $H$.",Definition:Closed Set/Metric Space,,false,"Let M = (A, d) be a metric space.

Let H ⊆ A.


=== Definition 1 ===
Let M = ( A, d ) be a metric space.

Let H ⊆ A.


H is closed (in M)  if and only if  its complement A ∖ H is open in M.

=== Definition 2 ===
Let M = (A, d) be a metric space.

Let H ⊆ A.


H is closed (in M)  if and only if  every limit point of H is also a point of H.",Closed
['Definitions/Topology'],Definition:Closed,"Let $T$ be a topological space.

Let $A subseteq T$.


Then $A$ is regular closed in $T$  if and only if :
:$A = A^{circ -}$

That is,  if and only if  $A$ equals the closure of its interior.",Definition:Regular Closed Set,,false,"Let T be a topological space.

Let A ⊆ T.


Then A is regular closed in T  if and only if :
:A = A^∘ -

That is,  if and only if  A equals the closure of its interior.",Closed
['Definitions/Topology'],Definition:Closed,"Let $X, Y$ be topological spaces.

Let $f: X to Y$ be a mapping. 


If, for any closed set $V subseteq X$, the image $f left(   right)V$ is closed in $Y$, then $f$ is referred to as a closed mapping.",Definition:Closed Mapping,,false,"Let X, Y be topological spaces.

Let f: X → Y be a mapping. 


If, for any closed set V ⊆ X, the image f (   )V is closed in Y, then f is referred to as a closed mapping.",Closed
['Definitions/Examples of Topologies'],Definition:Closed,"Let $T = left( S, tau right)$ be a topological space.

Let $p$ be a new element for $S$ such that $S^*_p := S cup leftlbrace p rightrbrace$.


Let $tau^*_p$ be the set defined as:
:$tau^*_p := leftlbrace U cup leftlbrace p rightrbrace: U in tau rightrbrace cup leftlbrace varnothing rightrbrace$

That is, $tau^*_p$ is the set of all sets formed by adding $p$ to all the open sets of $tau$ and including the empty set.


Then:
:$tau^*_p$ is the closed extension topology of $tau$
and:
:$T^*_p := left( S^*_p, tau^*_p right)$ is the closed extension space of $T = left( S, tau right)$.",Definition:Closed Extension Topology,,false,"Let T = ( S, τ) be a topological space.

Let p be a new element for S such that S^*_p := S ∪{ p }.


Let τ^*_p be the set defined as:
:τ^*_p := { U ∪{ p }: U ∈τ}∪{∅}

That is, τ^*_p is the set of all sets formed by adding p to all the open sets of τ and including the empty set.


Then:
:τ^*_p is the closed extension topology of τ
and:
:T^*_p := ( S^*_p, τ^*_p ) is the closed extension space of T = ( S, τ).",Closed
"['Definitions/Loops (Topology)', 'Definitions/Topology']",Definition:Closed,"Let $T = left( S, tau right)$ be a topological space.

Let $gamma: left[ 0 ,.,.,   right]1 to S$ be a path in $T$.

Let $gamma left(   right)0 = gamma left(   right)1$.


Then $gamma$ is called a loop (in $T$).


=== Simple Loop ===
Let $T = left( S, tau right)$ be a topological space.

Let $gamma: left[ 0 ,.,.,   right]1 to S$ be a path in $T$.


$gamma$ is a simple loop (in $T$)  if and only if :
:$gamma left(   right){t_1} ne gamma left(   right){t_2}$ for all $t_1 ,t_2 in left[ 0 ,.,.,   right)1$ with $t_1 ne t_2$
:$gamma left(   right)0 = gamma left(   right)1$

=== Base Point ===
Let $X$ be a topological space.

Let $gamma: left[ 0 ,.,.,   right]1 to X$ be a loop.


The base point of $gamma$ is $gamma left(   right)0$.

In other words, $gamma$ is said to be based at $gamma left(   right)0$.

=== Set of All Loops ===
Let $T$ be a topological space.


The set of all loops based at $p in T$ is denoted by $Omega left(   right){T, p}$.

=== Constant Loop ===
Let $T$ be a topological space.

Let $p in T$.

Let $Omega left(   right){T, p}$ denote the set of all loops based at $p$.


A constant loop $c_p$ is the loop $c_p in Omega left(   right){T, p}$ such that:

:$forall t in left[ 0 ,.,.,   right]1 : c_p left(   right)t = p$

=== Null-Homotopic Loop ===
Let $T = left( S, tau right)$ be a topological space.

Let $gamma$ be a loop in $T$.

Suppose $gamma$ is path-homotopic to a constant loop.


Then $gamma$ is said to be null-homotopic.

=== Circle Representative of Loop ===
Let $T = left( S, tau right)$ be a topological space.

Let $gamma: left[ 0 ,.,.,   right]1 to S$ be a loop in $T$.

Let $Bbb S^1 subseteq mathbb C$ be the unit circle in $mathbb C$:

:$Bbb S^1 = leftlbrace z in mathbb C : leftlvert z rightrvert = 1 rightrbrace$

Suppose $omega : left[ 0 ,.,.,   right]1 to Bbb S^1$ such that $omega left(   right)s = exp left(   right){2 pi i s}$.


Then the unique map $tilde f : Bbb S^1 to T$ such that $tilde f circ omega = f$ is called the circle representative of $f$.

=== Loop in Topological Manifold ===
Let $M$ be a topological manifold.

Let $sigma : left[ 0 ,.,.,   right]1 to M$ be a continuous path.

Let $sigma left(   right)0 = sigma left(   right)1$.


Then $sigma$ is called a loop.",Definition:Loop (Topology),,false,"Let T = ( S, τ) be a topological space.

Let γ: [ 0  . . ]1 → S be a path in T.

Let γ(   )0 = γ(   )1.


Then γ is called a loop (in T).


=== Simple Loop ===
Let T = ( S, τ) be a topological space.

Let γ: [ 0  . . ]1 → S be a path in T.


γ is a simple loop (in T)  if and only if :
:γ(   )t_1γ(   )t_2 for all t_1 ,t_2 ∈[ 0  . . )1 with t_1  t_2
:γ(   )0 = γ(   )1

=== Base Point ===
Let X be a topological space.

Let γ: [ 0  . . ]1 → X be a loop.


The base point of γ is γ(   )0.

In other words, γ is said to be based at γ(   )0.

=== Set of All Loops ===
Let T be a topological space.


The set of all loops based at p ∈ T is denoted by Ω(   )T, p.

=== Constant Loop ===
Let T be a topological space.

Let p ∈ T.

Let Ω(   )T, p denote the set of all loops based at p.


A constant loop c_p is the loop c_p ∈Ω(   )T, p such that:

:∀ t ∈[ 0  . . ]1 : c_p (   )t = p

=== Null-Homotopic Loop ===
Let T = ( S, τ) be a topological space.

Let γ be a loop in T.

Suppose γ is path-homotopic to a constant loop.


Then γ is said to be null-homotopic.

=== Circle Representative of Loop ===
Let T = ( S, τ) be a topological space.

Let γ: [ 0  . . ]1 → S be a loop in T.

Let S^1 ⊆ℂ be the unit circle in ℂ:

:S^1 = { z ∈ℂ : | z | = 1 }

Suppose ω : [ 0  . . ]1 → S^1 such that ω(   )s = exp(   )2 π i s.


Then the unique map f̃ :  S^1 → T such that f̃∘ω = f is called the circle representative of f.

=== Loop in Topological Manifold ===
Let M be a topological manifold.

Let σ : [ 0  . . ]1 → M be a continuous path.

Let σ(   )0 = σ(   )1.


Then σ is called a loop.",Closed
"['Definitions/Complex Analysis', 'Definitions/Analytic Geometry']",Definition:Closed,"=== Complex Analysis ===
Let $D subseteq mathbb C$ be a subset of the set of complex numbers.

$D$ is a closed region  if and only if  $D$ the closure of an open region.

=== Closed Region in the Plane ===
A closed region is a region complete with its boundary.


Category:Definitions/Analytic Geometry",Definition:Closed Region,,false,"=== Complex Analysis ===
Let D ⊆ℂ be a subset of the set of complex numbers.

D is a closed region  if and only if  D the closure of an open region.

=== Closed Region in the Plane ===
A closed region is a region complete with its boundary.


Category:Definitions/Analytic Geometry",Closed
"['Definitions/Mapping Theory', 'Definitions/Closedness under Mappings']",Definition:Closed,"Let $f: S to T$ be a mapping.

Let $S' subseteq S$.


Then $S'$ is closed under $f$  if and only if :

:$f left[ S' right] subseteq S'$

where $f left[ S' right]$ is the image of $S'$ under $f$.


That is:
:$x in S' implies f left(   right)x in S'$


=== Arbitrary Product ===
Let $phi: X^I to T$ be a mapping or a partial mapping, taking $I$-indexed families as arguments.

Denote with $mathrm {Dom} left( phi right)$ the domain of $phi$ (if $phi$ is a mapping, this is simply $X^I$).


A set $S$ is closed under $phi$  if and only if :

:$forall leftlangle s_i rightrangle_{i mathop in I} in S^I cap mathrm {Dom} left( phi right): phi left(   right){leftlangle s_i rightrangle_{i mathop in I} } in S$

Phrased in terms of image of a mapping, this translates to:

:$phi left(   right){S^I cap mathrm {Dom} left( phi right)} subseteq S$


Thus, in words, $S$ is closed under $phi$,  if and only if :

:Whenever $phi$ is defined for an $I$-indexed family from $S$, it maps that indexed family into $S$ again.

=== Class Theoretical Definition ===
Let $A$ and $B$ be classes such that $A$ is a subclass of $B$.

Let $g: B to B$ be a mapping on $B$.


Then $A$ is closed under $g$  if and only if :

:$forall x in A: g left(   right)x in A$",Definition:Closed under Mapping,,false,"Let f: S → T be a mapping.

Let S' ⊆ S.


Then S' is closed under f  if and only if :

:f [ S' ] ⊆ S'

where f [ S' ] is the image of S' under f.


That is:
:x ∈ S'  f (   )x ∈ S'


=== Arbitrary Product ===
Let ϕ: X^I → T be a mapping or a partial mapping, taking I-indexed families as arguments.

Denote with Dom( ϕ) the domain of ϕ (if ϕ is a mapping, this is simply X^I).


A set S is closed under ϕ  if and only if :

:∀⟨ s_i ⟩_i ∈ I∈ S^I ∩Dom( ϕ): ϕ(   )⟨ s_i ⟩_i ∈ I∈ S

Phrased in terms of image of a mapping, this translates to:

:ϕ(   )S^I ∩Dom( ϕ)⊆ S


Thus, in words, S is closed under ϕ,  if and only if :

:Whenever ϕ is defined for an I-indexed family from S, it maps that indexed family into S again.

=== Class Theoretical Definition ===
Let A and B be classes such that A is a subclass of B.

Let g: B → B be a mapping on B.


Then A is closed under g  if and only if :

:∀ x ∈ A: g (   )x ∈ A",Closed
"['Definitions/Closed Elements', 'Definitions/Closure Operators']",Definition:Closed,"Let $S$ be a set.

Let $mathrm {cl}: mathcal P left( S right) to mathcal P left( S right)$ be a closure operator.

Let $T subseteq S$ be a subset.


=== Definition 1 ===
Let $S$ be a set.

Let $mathrm {cl}: mathcal P left( S right) to mathcal P left( S right)$ be a closure operator.

Let $T subseteq S$ be a subset.


$T$ is closed (with respect to $mathrm {cl}$)  if and only if :
:$mathrm {cl} left(   right)T = T$

=== Definition 2 ===
Let $S$ be a set.

Let $mathrm {cl}: mathcal P left( S right) to mathcal P left( S right)$ be a closure operator.

Let $T subseteq S$ be a subset.


$T$ is closed (with respect to $mathrm {cl}$)  if and only if  $T$ is in the image of $mathrm {cl}$:
:$T in mathrm {Img} left( mathrm {cl} right)$",Definition:Closed Set/Closure Operator,,false,"Let S be a set.

Let cl: 𝒫( S ) →𝒫( S ) be a closure operator.

Let T ⊆ S be a subset.


=== Definition 1 ===
Let S be a set.

Let cl: 𝒫( S ) →𝒫( S ) be a closure operator.

Let T ⊆ S be a subset.


T is closed (with respect to cl)  if and only if :
:cl(   )T = T

=== Definition 2 ===
Let S be a set.

Let cl: 𝒫( S ) →𝒫( S ) be a closure operator.

Let T ⊆ S be a subset.


T is closed (with respect to cl)  if and only if  T is in the image of cl:
:T ∈Img( cl)",Closed
"['Definitions/Closure Operators', 'Definitions/Closed Elements']",Definition:Closed,"Let $left( S, preceq right)$ be an ordered set.

Let $mathrm {cl}$ be a closure operator on $S$.

Let $x in S$.


=== Definition 1 ===
Let $left( S, preceq right)$ be an ordered set.

Let $mathrm {cl}$ be a closure operator on $S$.

Let $x in S$.


The element $x$ is a closed element of $S$ (with respect to $mathrm {cl}$)  if and only if  $x$ is a fixed point of $mathrm {cl}$:
:$mathrm {cl} left(   right)x = x$

=== Definition 2 ===
Let $left( S, preceq right)$ be an ordered set.

Let $mathrm {cl}$ be a closure operator on $S$.

Let $x in S$.


The element $x$ is a closed element of $S$ (with respect to $mathrm {cl}$)  if and only if  $x$ is in the image of $mathrm {cl}$:
:$x in mathrm {Img} left( mathrm {cl} right)$",Definition:Closed Element,,false,"Let ( S, ≼) be an ordered set.

Let cl be a closure operator on S.

Let x ∈ S.


=== Definition 1 ===
Let ( S, ≼) be an ordered set.

Let cl be a closure operator on S.

Let x ∈ S.


The element x is a closed element of S (with respect to cl)  if and only if  x is a fixed point of cl:
:cl(   )x = x

=== Definition 2 ===
Let ( S, ≼) be an ordered set.

Let cl be a closure operator on S.

Let x ∈ S.


The element x is a closed element of S (with respect to cl)  if and only if  x is in the image of cl:
:x ∈Img( cl)",Closed
['Definitions/Real Intervals'],Definition:Closed,"Let $a, b in mathbb R$.

The closed (real) interval from $a$ to $b$ is defined as:

:$left[ a ,.,.,   right]b = leftlbrace x in mathbb R: a le x le b rightrbrace$",Definition:Real Interval/Closed,,false,"Let a, b ∈ℝ.

The closed (real) interval from a to b is defined as:

:[ a  . . ]b = { x ∈ℝ: a ≤ x ≤ b }",Closed
['Definitions/Walks'],Definition:Closed,"A closed walk is a walk whose first vertex is the same as the last.

That is, it is a walk which ends where it starts.",Definition:Walk (Graph Theory)/Closed,,false,"A closed walk is a walk whose first vertex is the same as the last.

That is, it is a walk which ends where it starts.",Closed
"['Definitions/Cycles (Graph Theory)', 'Definitions/Circuits (Graph Theory)', 'Definitions/Paths (Graph Theory)', 'Definitions/Graph Theory']",Definition:Closed,"A cycle is a circuit in which no vertex except the first (which is also the last) appears more than once.


An $n$-cycle is a cycle with $n$ vertices.


=== Subgraph ===
The set of vertices and edges which go to make up a cycle form a subgraph.

This subgraph itself is also referred to as a cycle.

=== Odd Cycle ===
An odd cycle is a cycle with odd length, that is, with an odd number of edges.

=== Even Cycle ===
An even cycle is a cycle with even length, that is, with an even number of edges.",Definition:Cycle (Graph Theory),,false,"A cycle is a circuit in which no vertex except the first (which is also the last) appears more than once.


An n-cycle is a cycle with n vertices.


=== Subgraph ===
The set of vertices and edges which go to make up a cycle form a subgraph.

This subgraph itself is also referred to as a cycle.

=== Odd Cycle ===
An odd cycle is a cycle with odd length, that is, with an odd number of edges.

=== Even Cycle ===
An even cycle is a cycle with even length, that is, with an even number of edges.",Closed
"['Definitions/Circuits (Graph Theory)', 'Definitions/Graph Theory']",Definition:Closed,"A circuit is a closed trail with at least one edge.


=== Subgraph ===
The set of vertices and edges which go to make up a circuit form a subgraph.

This subgraph itself is also referred to as a circuit.",Definition:Circuit (Graph Theory),,false,"A circuit is a closed trail with at least one edge.


=== Subgraph ===
The set of vertices and edges which go to make up a circuit form a subgraph.

This subgraph itself is also referred to as a circuit.",Closed
['Definitions/Algebraic Closure'],Definition:Closed,"Let $left( S, circ right)$ be an algebraic structure.


Then $S$ has the property of closure under $circ$  if and only if :

:$forall left( x, y right) in S times S: x circ y in S$


$S$ is said to be closed under $circ$, or just that $left( S, circ right)$ is closed.",Definition:Closure (Abstract Algebra)/Algebraic Structure,,false,"Let ( S, ∘) be an algebraic structure.


Then S has the property of closure under ∘  if and only if :

:∀( x, y ) ∈ S × S: x ∘ y ∈ S


S is said to be closed under ∘, or just that ( S, ∘) is closed.",Closed
"['Definitions/Linear Algebra', 'Definitions/Vector Algebra']",Definition:Closed,"Let $left( S, circ right)_R$ be an $R$-algebraic structure over a ring $R$.

Let $T subseteq S$ such that $forall lambda in R: forall x in T: lambda circ x in T$.


Then $T$ is closed for scalar product.


If $T$ is also closed for operations on $S$, then it is called a closed subset of $S$.",Definition:Closure (Abstract Algebra)/Scalar Product,,false,"Let ( S, ∘)_R be an R-algebraic structure over a ring R.

Let T ⊆ S such that ∀λ∈ R: ∀ x ∈ T: λ∘ x ∈ T.


Then T is closed for scalar product.


If T is also closed for operations on S, then it is called a closed subset of S.",Closed
['Definitions/Field Extensions'],Definition:Closed,"Let $K$ be a field.


Then $K$ is algebraically closed  if and only if :


=== Definition 1 ===
Let $K$ be a field.


$K$ is algebraically closed  if and only if :

The only algebraic field extension of $K$ is $K$ itself.

=== Definition 2 ===
Let $K$ be a field.


$K$ is algebraically closed  if and only if :

Every irreducible polynomial $f$ over $K$ has degree $1$.

=== Definition 3 ===
Let $K$ be a field.


$K$ is algebraically closed  if and only if :

Every polynomial $f$ over $K$ of strictly positive degree has a root in $K$.",Definition:Algebraically Closed Field,,false,"Let K be a field.


Then K is algebraically closed  if and only if :


=== Definition 1 ===
Let K be a field.


K is algebraically closed  if and only if :

The only algebraic field extension of K is K itself.

=== Definition 2 ===
Let K be a field.


K is algebraically closed  if and only if :

Every irreducible polynomial f over K has degree 1.

=== Definition 3 ===
Let K be a field.


K is algebraically closed  if and only if :

Every polynomial f over K of strictly positive degree has a root in K.",Closed
"['Definitions/Algebraic Number Theory', 'Definitions/Commutative Algebra']",Definition:Closed,"=== Ring Extension ===
Let $phi : A hookrightarrow B$ be a ring extension.

Let $C$ be the integral closure of $A$ in $B$.


Then $A$ is integrally closed in $B$  if and only if  $C = phi(A)$.

=== Integral Domain ===
Let $R$ be an integral domain.


Then $R$ is integrally closed  if and only if  it is integrally closed in its field of fractions.",Definition:Integrally Closed,,false,"=== Ring Extension ===
Let ϕ : A ↪ B be a ring extension.

Let C be the integral closure of A in B.


Then A is integrally closed in B  if and only if  C = ϕ(A).

=== Integral Domain ===
Let R be an integral domain.


Then R is integrally closed  if and only if  it is integrally closed in its field of fractions.",Closed
['Definitions/Localization of Rings'],Definition:Closed,"Let $left( A, +, circ right)$ be a ring with unity $1_A$ and zero $0_A$.

Let $S subseteq A$ be a subset.


Then $S$ is multiplicatively closed  if and only if :

:$(1): quad 1_A in S$
:$(2): quad x, y in S implies x circ y in S$",Definition:Multiplicatively Closed Subset of Ring,,false,"Let ( A, +, ∘) be a ring with unity 1_A and zero 0_A.

Let S ⊆ A be a subset.


Then S is multiplicatively closed  if and only if :

:(1):    1_A ∈ S
:(2):    x, y ∈ S  x ∘ y ∈ S",Closed
['Definitions/Closure Operators'],Definition:Closure,"=== Ordering ===
=== Definition 1 ===

Let $left( S, preceq right)$ be an ordered set.


A closure operator on $S$ is a mapping:
:$mathrm {cl}: S to S$
which satisfies the closure axioms as follows for all elements $x, y in S$:
 


=== Definition 2 ===
Let $left( S, preceq right)$ be an ordered set.


A closure operator on $S$ is a mapping:
:$mathrm {cl}: S to S$
which satisfies the following condition for all elements $x, y in S$:
:$x preceq mathrm {cl} left(   right)y iff mathrm {cl} left(   right)x preceq mathrm {cl} left(   right)y$

=== Power Set ===

When the ordering in question is the subset relation on a power set, the definition can be expressed as follows:

Let $S$ be a set.

Let $mathcal P left( S right)$ denote the power set of $S$.


A closure operator on $S$ is a mapping:
:$mathrm {cl}: mathcal P left( S right) to mathcal P left( S right)$
which satisfies the closure axioms as follows for all sets $X, Y subseteq S$:
 ",Definition:Closure Operator,,false,"=== Ordering ===
=== Definition 1 ===

Let ( S, ≼) be an ordered set.


A closure operator on S is a mapping:
:cl: S → S
which satisfies the closure axioms as follows for all elements x, y ∈ S:
 


=== Definition 2 ===
Let ( S, ≼) be an ordered set.


A closure operator on S is a mapping:
:cl: S → S
which satisfies the following condition for all elements x, y ∈ S:
:x ≼cl(   )y cl(   )x ≼cl(   )y

=== Power Set ===

When the ordering in question is the subset relation on a power set, the definition can be expressed as follows:

Let S be a set.

Let 𝒫( S ) denote the power set of S.


A closure operator on S is a mapping:
:cl: 𝒫( S ) →𝒫( S )
which satisfies the closure axioms as follows for all sets X, Y ⊆ S:
 ",Closure
['Definitions/Closure Operators'],Definition:Closure,"Let $S$ be a set.

Let $mathrm {cl}$ be a closure operator on $S$.

Let $T subseteq S$ be a subset of $S$.


The closure of $T$ is its image $mathrm {cl} left(   right)T$.",Definition:Closure of Set under Closure Operator,,false,"Let S be a set.

Let cl be a closure operator on S.

Let T ⊆ S be a subset of S.


The closure of T is its image cl(   )T.",Closure
"['Definitions/Order Theory', 'Definitions/Upper Closures']",Definition:Closure,"=== Upper Closure of an Element ===
Let $left( S, preccurlyeq right)$ be an ordered set.

Let $a in S$.


The upper closure of $a$ (in $S$) is defined as:

:$a^succcurlyeq := leftlbrace b in S: a preccurlyeq b rightrbrace$


That is, $a^succcurlyeq$ is the set of all elements of $S$ that succeed $a$.

=== Upper Closure of a Set ===
Let $left( S, preceq right)$ be an ordered set or preordered set.

Let $T subseteq S$.


The upper closure of $T$ (in $S$) is defined as:

:$T^succeq := bigcup leftlbrace t^succeq: t in T rightrbrace$
where $t^succeq$ denotes the upper closure of $t$ in $S$.

That is:
:$T^succeq := leftlbrace u in S: exists t in T: t preceq u rightrbrace$",Definition:Upper Closure,,false,"=== Upper Closure of an Element ===
Let ( S, ≼) be an ordered set.

Let a ∈ S.


The upper closure of a (in S) is defined as:

:a^≽ := { b ∈ S: a ≼ b }


That is, a^≽ is the set of all elements of S that succeed a.

=== Upper Closure of a Set ===
Let ( S, ≼) be an ordered set or preordered set.

Let T ⊆ S.


The upper closure of T (in S) is defined as:

:T^≽ := ⋃{ t^≽: t ∈ T }
where t^≽ denotes the upper closure of t in S.

That is:
:T^≽ := { u ∈ S: ∃ t ∈ T: t ≼ u }",Closure
"['Definitions/Order Theory', 'Definitions/Lower Closures']",Definition:Closure,"=== Lower Closure of Element ===
Let $left( S, preccurlyeq right)$ be an ordered set.

Let $a in S$.


The lower closure of $a$ (in $S$) is defined as:

:$a^preccurlyeq := leftlbrace b in S: b preccurlyeq a rightrbrace$


That is, $a^preccurlyeq$ is the set of all elements of $S$ that precede $a$.


=== Class Theory ===
 
Let $A$ be a class under an ordering $preccurlyeq$.

Let $a in A$.


The lower closure of $a$ (in $A$) is defined as:

:$a^preccurlyeq := leftlbrace b in A: b preccurlyeq a rightrbrace$

=== Lower Closure of Subset ===
Let $left( S, preccurlyeq right)$ be an ordered set or preordered set.

Let $T subseteq S$.


The lower closure of $T$ (in $S$) is defined as:

:$T^preccurlyeq := bigcup leftlbrace t^preccurlyeq: t in T rightrbrace$
where $t^preccurlyeq$ is the lower closure of $t$.

That is:
:$T^preccurlyeq := leftlbrace l in S: exists t in T: l preccurlyeq t rightrbrace$",Definition:Lower Closure,,false,"=== Lower Closure of Element ===
Let ( S, ≼) be an ordered set.

Let a ∈ S.


The lower closure of a (in S) is defined as:

:a^≼ := { b ∈ S: b ≼ a }


That is, a^≼ is the set of all elements of S that precede a.


=== Class Theory ===
 
Let A be a class under an ordering ≼.

Let a ∈ A.


The lower closure of a (in A) is defined as:

:a^≼ := { b ∈ A: b ≼ a }

=== Lower Closure of Subset ===
Let ( S, ≼) be an ordered set or preordered set.

Let T ⊆ S.


The lower closure of T (in S) is defined as:

:T^≼ := ⋃{ t^≼: t ∈ T }
where t^≼ is the lower closure of t.

That is:
:T^≼ := { l ∈ S: ∃ t ∈ T: l ≼ t }",Closure
['Definitions/Algebraic Closure'],Definition:Closure,"Let $left( S, circ right)$ be an algebraic structure.


Then $S$ has the property of closure under $circ$  if and only if :

:$forall left( x, y right) in S times S: x circ y in S$


$S$ is said to be closed under $circ$, or just that $left( S, circ right)$ is closed.",Definition:Closure (Abstract Algebra)/Algebraic Structure,,false,"Let ( S, ∘) be an algebraic structure.


Then S has the property of closure under ∘  if and only if :

:∀( x, y ) ∈ S × S: x ∘ y ∈ S


S is said to be closed under ∘, or just that ( S, ∘) is closed.",Closure
"['Definitions/Algebraic Number Theory', 'Definitions/Commutative Algebra']",Definition:Closure,"Let $A$ be an extension of a commutative ring with unity $R$.


Let $C$ be the set of all elements of $A$ that are integral over $R$.

Then $C$ is called the integral closure of $R$ in $A$.",Definition:Integral Closure,,false,"Let A be an extension of a commutative ring with unity R.


Let C be the set of all elements of A that are integral over R.

Then C is called the integral closure of R in A.",Closure
"['Definitions/Generated Normal Subgroups', 'Definitions/Normal Subgroups', 'Definitions/Normality in Groups', 'Definitions/Generated Subgroups', 'Definitions/Generators of Groups', 'Definitions/Subgroups']",Definition:Closure,"Let $G$ be a group.

Let $S subseteq G$ be a subset.


=== Definition 1 ===
Let $G$ be a group. 

Let $S subseteq G$ be a subset.


The normal subgroup generated by $S$, denoted ${leftlangle S^G rightrangle}$,  is the intersection of all normal subgroups of $G$ containing $S$.

=== Definition 2 ===
 

Let $G$ be a group. 

Let $S subseteq G$ be a subset.


The normal subgroup generated by $S$, denoted ${leftlangle S^G rightrangle}$,  is the smallest normal subgroup of $G$ containing $S$:
:${leftlangle S^G rightrangle} = {leftlangle x S x^{-1}: x in G rightrangle}$",Definition:Generated Normal Subgroup,,false,"Let G be a group.

Let S ⊆ G be a subset.


=== Definition 1 ===
Let G be a group. 

Let S ⊆ G be a subset.


The normal subgroup generated by S, denoted ⟨ S^G ⟩,  is the intersection of all normal subgroups of G containing S.

=== Definition 2 ===
 

Let G be a group. 

Let S ⊆ G be a subset.


The normal subgroup generated by S, denoted ⟨ S^G ⟩,  is the smallest normal subgroup of G containing S:
:⟨ S^G ⟩ = ⟨ x S x^-1: x ∈ G ⟩",Closure
"['Definitions/Topology', 'Definitions/Set Closures']",Definition:Closure,"Let $T = left( S, tau right)$ be a topological space.

Let $H subseteq S$.


=== Definition 1 ===
Let $T = left( S, tau right)$ be a topological space.

Let $H subseteq S$.


The closure of $H$ (in $T$) is defined as:
:$H^- := H cup H'$
where $H'$ is the derived set of $H$.


That is, $H^-$ is the union of $H$ and its limit points.

=== Definition 2 ===
Let $T = left( S, tau right)$ be a topological space.

Let $H subseteq S$.


The closure of $H$ (in $T$) is defined as:
:$ds H^- := bigcap leftlbrace K supseteq H: K right.$ is closed in $left. T rightrbrace$


That is, $H^-$ is the intersection of all closed sets in $T$ which contain $H$.

=== Definition 3 ===
Let $T = left( S, tau right)$ be a topological space.

Let $H subseteq S$.


The closure of $H$ (in $T$), denoted $H^-$, is defined as the smallest closed set of $T$ that contains $H$.

=== Definition 4 ===
Let $T = left( S, tau right)$ be a topological space.

Let $H subseteq S$.


The closure of $H$ (in $T$) is defined as the union of $H$ and its boundary in $T$:
:$H^- := H cup partial H$

=== Definition 5 ===
Let $T = left( S, tau right)$ be a topological space.

Let $H subseteq S$.


The closure of $H$ (in $T$) is the union of the set of all isolated points of $H$ and the set of all limit points of $H$:
:$H^- := H^i cup H'$

=== Definition 6 ===
Let $T = left( S, tau right)$ be a topological space.

Let $H subseteq S$.


The closure of $H$ (in $T$), denoted $H^-$, is the set of all adherent points of $H$.


=== Adherent Point ===
Let $A subseteq S$.

Let $T = left( S, tau right)$ be a topological space.

Let $H subseteq S$.


A point $x in S$ is an adherent point of $H$  if and only if  every neighborhood $N$ of $x$ satisfies:
:$H cap N ne varnothing$",Definition:Closure (Topology),,false,"Let T = ( S, τ) be a topological space.

Let H ⊆ S.


=== Definition 1 ===
Let T = ( S, τ) be a topological space.

Let H ⊆ S.


The closure of H (in T) is defined as:
:H^- := H ∪ H'
where H' is the derived set of H.


That is, H^- is the union of H and its limit points.

=== Definition 2 ===
Let T = ( S, τ) be a topological space.

Let H ⊆ S.


The closure of H (in T) is defined as:
:H^- := ⋂{ K ⊇ H: K . is closed in . T }


That is, H^- is the intersection of all closed sets in T which contain H.

=== Definition 3 ===
Let T = ( S, τ) be a topological space.

Let H ⊆ S.


The closure of H (in T), denoted H^-, is defined as the smallest closed set of T that contains H.

=== Definition 4 ===
Let T = ( S, τ) be a topological space.

Let H ⊆ S.


The closure of H (in T) is defined as the union of H and its boundary in T:
:H^- := H ∪∂ H

=== Definition 5 ===
Let T = ( S, τ) be a topological space.

Let H ⊆ S.


The closure of H (in T) is the union of the set of all isolated points of H and the set of all limit points of H:
:H^- := H^i ∪ H'

=== Definition 6 ===
Let T = ( S, τ) be a topological space.

Let H ⊆ S.


The closure of H (in T), denoted H^-, is the set of all adherent points of H.


=== Adherent Point ===
Let A ⊆ S.

Let T = ( S, τ) be a topological space.

Let H ⊆ S.


A point x ∈ S is an adherent point of H  if and only if  every neighborhood N of x satisfies:
:H ∩ N ∅",Closure
"['Definitions/Metric Spaces', 'Definitions/Set Closures']",Definition:Closure,"Let $M = left( A, d right)$ be a metric space.

Let $H subseteq A$.

Let $H'$ be the set of limit points of $H$.

Let $H^i$ be the set of isolated points of $H$.


The closure of $H$ (in $M$) is the union of all isolated points of $H$ and all limit points of $H$:
:$H^- := H' cup H^i$",Definition:Closure (Topology)/Metric Space,,false,"Let M = ( A, d ) be a metric space.

Let H ⊆ A.

Let H' be the set of limit points of H.

Let H^i be the set of isolated points of H.


The closure of H (in M) is the union of all isolated points of H and all limit points of H:
:H^- := H' ∪ H^i",Closure
"['Definitions/Set Closures', 'Definitions/Normed Vector Spaces']",Definition:Closure,"Let $M = left( X, leftlVert , cdot , rightrVert  right)$ be a normed vector space.

Let $S subseteq X$.


The closure of $S$ (in $M$) is the union of $S$ and $S'$, the set of all limit points of $S$:
:$S^- := S cup S'$",Definition:Closure/Normed Vector Space,,false,"Let M = ( X, ‖ · ‖) be a normed vector space.

Let S ⊆ X.


The closure of S (in M) is the union of S and S', the set of all limit points of S:
:S^- := S ∪ S'",Closure
['Definitions/Relational Closures'],Definition:Closure,"=== Definition 1 ===
Let $x$ be a set.

Then the transitive closure of $x$ is the smallest transitive superset of $x$.

The following is not equivalent to the above, but they are almost the same.

=== Definition 2 ===
Let $x$ be a set.

For each natural number $n in mathbb N_{ge 0}$ let:

: $bigcup^n x = underbrace{bigcup bigcup cdots bigcup}_n x$


Then the transitive closure of $x$ is the union of the sets:
:$left{ {x}right}, x, bigcup x, bigcup^2 x, dots, bigcup^n x, dots$


More precisely:

Let $F$ be the mapping on the universal class defined by letting:
:$F left({a}right) = bigcup a$
for each set $a$.

Let $G$ be the mapping on the natural numbers defined recursively by letting:

: $G left({0}right) = left{ {x}right}$
: $G left({n^+}right) = F left({G left({n}right)}right)$

for each natural number $n$.

Then the transitive closure of $x$ is defined as the union of the image of $G$.",Definition:Transitive Closure (Set Theory),,false,"=== Definition 1 ===
Let x be a set.

Then the transitive closure of x is the smallest transitive superset of x.

The following is not equivalent to the above, but they are almost the same.

=== Definition 2 ===
Let x be a set.

For each natural number n ∈ℕ_≥ 0 let:

: ⋃^n x = ⋃⋃⋯⋃_n x


Then the transitive closure of x is the union of the sets:
:{x}, x, ⋃ x, ⋃^2 x, …, ⋃^n x, …


More precisely:

Let F be the mapping on the universal class defined by letting:
:F (a) = ⋃ a
for each set a.

Let G be the mapping on the natural numbers defined recursively by letting:

: G (0) = {x}
: G (n^+) = F (G (n))

for each natural number n.

Then the transitive closure of x is defined as the union of the image of G.",Closure
"['Definitions/Relation Theory', 'Definitions/Reflexive Relations', 'Definitions/Reflexive Closures']",Definition:Closure,"=== Definition 1 ===
Let $mathcal R$ be a relation on a set $S$.


The reflexive closure of $mathcal R$ is denoted $mathcal R^=$, and is defined as:

:$mathcal R^= := mathcal R cup leftlbrace left( x, x right): x in S rightrbrace$

That is:

:$mathcal R^= := mathcal R cup Delta_S$
where $Delta_S$ is the diagonal relation on $S$.

=== Definition 2 ===
Let $mathcal R$ be a relation on a set $S$.


The reflexive closure of $mathcal R$ is defined as the smallest reflexive relation on $S$ that contains $mathcal R$ as a subset.


The reflexive closure of $mathcal R$ is denoted $mathcal R^=$.

=== Definition 3 ===
Let $mathcal R$ be a relation on a set $S$.

Let $mathcal Q$ be the set of all reflexive relations on $S$ that contain $mathcal R$.

The reflexive closure of $mathcal R$ is denoted $mathcal R^=$, and is defined as:

:$mathcal R^= := bigcap mathcal Q$

That is:

:$mathcal R^=$ is the intersection of all reflexive relations on $S$ containing $mathcal R$.",Definition:Reflexive Closure,,false,"=== Definition 1 ===
Let ℛ be a relation on a set S.


The reflexive closure of ℛ is denoted ℛ^=, and is defined as:

:ℛ^= := ℛ∪{( x, x ): x ∈ S }

That is:

:ℛ^= := ℛ∪Δ_S
where Δ_S is the diagonal relation on S.

=== Definition 2 ===
Let ℛ be a relation on a set S.


The reflexive closure of ℛ is defined as the smallest reflexive relation on S that contains ℛ as a subset.


The reflexive closure of ℛ is denoted ℛ^=.

=== Definition 3 ===
Let ℛ be a relation on a set S.

Let 𝒬 be the set of all reflexive relations on S that contain ℛ.

The reflexive closure of ℛ is denoted ℛ^=, and is defined as:

:ℛ^= := ⋂𝒬

That is:

:ℛ^= is the intersection of all reflexive relations on S containing ℛ.",Closure
"['Definitions/Relation Theory', 'Definitions/Symmetric Closures', 'Definitions/Closure Operators']",Definition:Closure,"Let $mathcal R$ be a relation on a set $S$.


=== Definition 1 ===
Let $mathcal R$ be a relation on a set $S$.


The symmetric closure of $mathcal R$ is denoted $mathcal R^leftrightarrow$, and is defined as the union of $mathcal R$ with its inverse:

:$mathcal R^leftrightarrow = mathcal R cup mathcal R^{-1}$

=== Definition 2 ===
Let $mathcal R$ be a relation on a set $S$.


The symmetric closure of $mathcal R$ is denoted $mathcal R^leftrightarrow$, and is defined as the smallest symmetric relation on $S$ which contains $mathcal R$.",Definition:Symmetric Closure,,false,"Let ℛ be a relation on a set S.


=== Definition 1 ===
Let ℛ be a relation on a set S.


The symmetric closure of ℛ is denoted ℛ^↔, and is defined as the union of ℛ with its inverse:

:ℛ^↔ = ℛ∪ℛ^-1

=== Definition 2 ===
Let ℛ be a relation on a set S.


The symmetric closure of ℛ is denoted ℛ^↔, and is defined as the smallest symmetric relation on S which contains ℛ.",Closure
"['Definitions/Transitive Closures', 'Definitions/Transitive Relations', 'Definitions/Examples of Closure Operators']",Definition:Closure,"=== Smallest Transitive Superset ===
Let $mathcal R$ be a relation on a set $S$.


The transitive closure of $mathcal R$ is defined as the smallest transitive relation on $S$ which contains $mathcal R$ as a subset.


The transitive closure of $mathcal R$ is denoted $mathcal R^+$.

=== Intersection of Transitive Supersets ===
Let $mathcal R$ be a relation on a set $S$.


The transitive closure of $mathcal R$ is defined as the intersection of all transitive relations on $S$ which contain $mathcal R$.


The transitive closure of $mathcal R$ is denoted $mathcal R^+$.

=== Finite Chain ===
Let $mathcal R$ be a relation on a set or class $S$.


The transitive closure of $mathcal R$ is the relation $mathcal R^+$ defined as follows:

For $x, y in S$, $x mathrel {mathcal R^+} y$  if and only if  for some $n in mathbb N_{>0}$ there exist $s_0, s_1, dots, s_n in S$ such that $s_0 = x$, $s_n = y$, and:

 
 
 
 
 
 


That is:

:$forall k in mathbb N_n: s_k mathrel mathcal R s_{k + 1}$

=== Union of Compositions ===
Let $mathcal R$ be a relation on a set $S$.

Let:

:$mathcal R^n := begin{cases}
mathcal R & : n = 1 \
mathcal R^{n-1} circ mathcal R & : n > 1
end{cases}$

where $circ$ denotes composition of relations.

Finally, let:

:$ds mathcal R^+ = bigcup_{i mathop = 1}^infty mathcal R^i$


Then $mathcal R^+$ is called the transitive closure of $mathcal R$.",Definition:Transitive Closure (Relation Theory),,false,"=== Smallest Transitive Superset ===
Let ℛ be a relation on a set S.


The transitive closure of ℛ is defined as the smallest transitive relation on S which contains ℛ as a subset.


The transitive closure of ℛ is denoted ℛ^+.

=== Intersection of Transitive Supersets ===
Let ℛ be a relation on a set S.


The transitive closure of ℛ is defined as the intersection of all transitive relations on S which contain ℛ.


The transitive closure of ℛ is denoted ℛ^+.

=== Finite Chain ===
Let ℛ be a relation on a set or class S.


The transitive closure of ℛ is the relation ℛ^+ defined as follows:

For x, y ∈ S, x ℛ^+ y  if and only if  for some n ∈ℕ_>0 there exist s_0, s_1, …, s_n ∈ S such that s_0 = x, s_n = y, and:

 
 
 
 
 
 


That is:

:∀ k ∈ℕ_n: s_k ℛ s_k + 1

=== Union of Compositions ===
Let ℛ be a relation on a set S.

Let:

:ℛ^n := ℛ    : n = 1 
ℛ^n-1∘ℛ    : n > 1

where ∘ denotes composition of relations.

Finally, let:

:ℛ^+ = ⋃_i  = 1^∞ℛ^i


Then ℛ^+ is called the transitive closure of ℛ.",Closure
"['Definitions/Codomains (Relation Theory)', 'Definitions/Relation Theory']",Definition:Codomain,"=== Relation ===
The codomain of a relation $mathcal R subseteq S times T$ is $T$.

It can be denoted $mathrm {Cdm} left( mathcal R right)$.

=== Mapping ===

The term codomain is usually seen when the relation in question is actually a mapping:
Let $S$ and $T$ be sets.

Let $f: S to T$ be a mapping.

The codomain of $f$ is $T$.

It is denoted on   by $mathrm {Cdm} left( f right)$.",Definition:Codomain (Relation Theory),,false,"=== Relation ===
The codomain of a relation ℛ⊆ S × T is T.

It can be denoted Cdm( ℛ).

=== Mapping ===

The term codomain is usually seen when the relation in question is actually a mapping:
Let S and T be sets.

Let f: S → T be a mapping.

The codomain of f is T.

It is denoted on   by Cdm( f ).",Codomain
['Definitions/Morphisms'],Definition:Codomain,"Let $f: X to Y$ be a morphism.

Then the codomain of $f$ is defined to be the object $Y$.

This is usually denoted $Y = mathrm {Cdm} left( f right)$.

Category:Definitions/Morphisms",Definition:Codomain (Category Theory),,false,"Let f: X → Y be a morphism.

Then the codomain of f is defined to be the object Y.

This is usually denoted Y = Cdm( f ).

Category:Definitions/Morphisms",Codomain
['Definitions/Truth Tables'],Definition:Column,"A row of a truth table is one of the vertical lines headed by a statement form presenting all the truth values that the statement form takes.

Each entry in the column corresponds to one specific combination of truth values taken by the propositional variables that the statement form comprises.",Definition:Truth Table/Column,,false,"A row of a truth table is one of the vertical lines headed by a statement form presenting all the truth values that the statement form takes.

Each entry in the column corresponds to one specific combination of truth values taken by the propositional variables that the statement form comprises.",Column
['Definitions/Matrices'],Definition:Column,"Let $mathbf A$ be an $m times n$ matrix.

For each $j in left[ 1 ,.,.,   right]n$, the columns of $mathbf A$ are the ordered $m$-tuples:
: $c_j = left( a_{1 j}, a_{2 j}, ldots, a_{m j}  right)$

where $c_j$ is called the $j$th column of $mathbf A$.


A column of an $m times n$ matrix can also be treated as a $m times 1$ column matrix in its own right:
:$c_j = begin {bmatrix} a_{1 j} \ a_{2 j} \ vdots \ a_{m j} end {bmatrix}$
for $j = 1, 2, ldots, n$.",Definition:Matrix/Column,,false,"Let 𝐀 be an m × n matrix.

For each j ∈[ 1  . . ]n, the columns of 𝐀 are the ordered m-tuples:
: c_j = ( a_1 j, a_2 j, …, a_m j)

where c_j is called the jth column of 𝐀.


A column of an m × n matrix can also be treated as a m × 1 column matrix in its own right:
:c_j = [ a_1 j; a_2 j;     ⋮; a_m j ]
for j = 1, 2, …, n.",Column
['Definitions/Latin Squares'],Definition:Column,"Let $mathbf L$ be a Latin square.

The columns of $mathbf L$ are the lines of elements reading down the page.",Definition:Latin Square/Column,,false,"Let 𝐋 be a Latin square.

The columns of 𝐋 are the lines of elements reading down the page.",Column
['Definitions/Logarithms'],Definition:Common,"Logarithms base $10$ are often referred to as common logarithms.


=== Notation for Negative Logarithm ===
Let $n in mathbb R$ be a real number such that $0 < n < 1$.

Let $n$ be presented (possibly approximated) in scientific notation as:
:$a times 10^{-d}$
where $d in mathbb Z_{>0}$ is a (strictly) positive integer.

Let $log_{10} n$ denote the common logarithm of $n$.

Then it is the standard convention to express $log_{10} n$ in the form:
:$log_{10} n = overline d cdotp m$

where $m := log_{10} a$ is the mantissa of $log_{10} n$.


The overline notation is commonly read as bar, that is:
:$overline 2$ is read as bar two.",Definition:General Logarithm/Common,,false,"Logarithms base 10 are often referred to as common logarithms.


=== Notation for Negative Logarithm ===
Let n ∈ℝ be a real number such that 0 < n < 1.

Let n be presented (possibly approximated) in scientific notation as:
:a × 10^-d
where d ∈ℤ_>0 is a (strictly) positive integer.

Let log_10 n denote the common logarithm of n.

Then it is the standard convention to express log_10 n in the form:
:log_10 n = d m

where m := log_10 a is the mantissa of log_10 n.


The overline notation is commonly read as bar, that is:
:2 is read as bar two.",Common
"['Definitions/Arithmetic Mean', 'Definitions/Pythagorean Means', 'Definitions/Algebra', 'Definitions/Measures of Central Tendency']",Definition:Common,"Let $x_1, x_2, ldots, x_n in mathbb R$ be real numbers.

The arithmetic mean of $x_1, x_2, ldots, x_n$ is defined as:

:$ds A_n := dfrac 1 n sum_{k mathop = 1}^n x_k$

That is, to find out the arithmetic mean of a set of numbers, add them all up and divide by how many there are.",Definition:Arithmetic Mean,,false,"Let x_1, x_2, …, x_n ∈ℝ be real numbers.

The arithmetic mean of x_1, x_2, …, x_n is defined as:

:A_n :=  1 n ∑_k  = 1^n x_k

That is, to find out the arithmetic mean of a set of numbers, add them all up and divide by how many there are.",Common
"['Definitions/Common Denominators', 'Definitions/Proof Techniques', 'Definitions/Arithmetic', 'Definitions/Algebra']",Definition:Common,"Consider the expression:
:$dfrac a b + dfrac c d$

where $a$, $b$, $c$ and $d$ are any expressions whatsoever which evaluate to a number such that neither $c$ nor $d$ evaluate to zero.

In order to be able to perform the required addition, it is necessary to put the expressions $dfrac a b$ and $dfrac c d$ over a common denominator.


Hence the operation is:
:to multiply both the numerator (top) and denominator (bottom) of $dfrac a b$ by $d$
and in the same operation:
:to multiply both the numerator (top) and denominator (bottom) of $dfrac c d$ by $b$

in order to obtain the expression:
:$dfrac {a d} {b d} + dfrac {b c} {b d}$


Hence one may perform the operation as:
:$dfrac {a d + b c} {b d}$

and either evaluate or simplify appropriately.


=== Lowest Common Denominator ===
Let $dfrac a b$ and $dfrac c d$ be fractions.

The lowest common denominator of $dfrac a b$ and $dfrac c d$ is the lowest common multiple of the denominators of $dfrac a b$ and $dfrac c d$:
:$operatorname {lcm} leftlbrace b, d rightrbrace$",Definition:Common Denominator,,false,"Consider the expression:
:a b +  c d

where a, b, c and d are any expressions whatsoever which evaluate to a number such that neither c nor d evaluate to zero.

In order to be able to perform the required addition, it is necessary to put the expressions a b and c d over a common denominator.


Hence the operation is:
:to multiply both the numerator (top) and denominator (bottom) of a b by d
and in the same operation:
:to multiply both the numerator (top) and denominator (bottom) of c d by b

in order to obtain the expression:
:a db d + b cb d


Hence one may perform the operation as:
:a d + b cb d

and either evaluate or simplify appropriately.


=== Lowest Common Denominator ===
Let a b and c d be fractions.

The lowest common denominator of a b and c d is the lowest common multiple of the denominators of a b and c d:
:lcm{ b, d }",Common
"['Definitions/Lowest Common Denominator', 'Definitions/Common Denominators']",Definition:Common,"Let $dfrac a b$ and $dfrac c d$ be fractions.

The lowest common denominator of $dfrac a b$ and $dfrac c d$ is the lowest common multiple of the denominators of $dfrac a b$ and $dfrac c d$:
:$operatorname {lcm} leftlbrace b, d rightrbrace$",Definition:Common Denominator/Lowest,,false,"Let a b and c d be fractions.

The lowest common denominator of a b and c d is the lowest common multiple of the denominators of a b and c d:
:lcm{ b, d }",Common
"['Definitions/Common Divisors', 'Definitions/Number Theory', 'Definitions/Integral Domains']",Definition:Common,"=== Integral Domain ===
Let $left( D, +, times right)$ be an integral domain.

Let $S subseteq D$ be a finite subset of $D$.


Let $c in D$ such that $c$ divides all the elements of $S$, that is:

:$forall x in S: c mathrel backslash x$


Then $c$ is a common divisor of all the elements in $S$.

=== Integers ===

The definition is usually applied when the integral domain in question is the set of integers $mathbb Z$, thus:

Let $S$ be a finite set of integers, that is:

:$S = leftlbrace x_1, x_2, ldots, x_n: forall k in mathbb N^*_n: x_k in mathbb Z rightrbrace$


Let $c in mathbb Z$ such that $c$ divides all the elements of $S$, that is:

:$forall x in S: c mathrel backslash x$


Then $c$ is a common divisor of all the elements in $S$.

=== Real Numbers ===

The definition can also be applied when the integral domain in question is the real numbers $mathbb R$, thus:

Let $S$ be a finite set of real numbers, that is:

:$S = leftlbrace x_1, x_2, ldots, x_n: forall k in mathbb N^*_n: x_k in mathbb R rightrbrace$


Let $c in mathbb R$ such that $c$ divides all the elements of $S$, that is:

:$forall x in S: c mathrel backslash x$


Then $c$ is a common divisor of all the elements in $S$.",Definition:Common Divisor,,false,"=== Integral Domain ===
Let ( D, +, ×) be an integral domain.

Let S ⊆ D be a finite subset of D.


Let c ∈ D such that c divides all the elements of S, that is:

:∀ x ∈ S: c  x


Then c is a common divisor of all the elements in S.

=== Integers ===

The definition is usually applied when the integral domain in question is the set of integers ℤ, thus:

Let S be a finite set of integers, that is:

:S = { x_1, x_2, …, x_n: ∀ k ∈ℕ^*_n: x_k ∈ℤ}


Let c ∈ℤ such that c divides all the elements of S, that is:

:∀ x ∈ S: c  x


Then c is a common divisor of all the elements in S.

=== Real Numbers ===

The definition can also be applied when the integral domain in question is the real numbers ℝ, thus:

Let S be a finite set of real numbers, that is:

:S = { x_1, x_2, …, x_n: ∀ k ∈ℕ^*_n: x_k ∈ℝ}


Let c ∈ℝ such that c divides all the elements of S, that is:

:∀ x ∈ S: c  x


Then c is a common divisor of all the elements in S.",Common
"['Definitions/Common Divisors', 'Definitions/Real Analysis']",Definition:Common,"Let $S$ be a finite set of real numbers, that is:

:$S = leftlbrace x_1, x_2, ldots, x_n: forall k in mathbb N^*_n: x_k in mathbb R rightrbrace$


Let $c in mathbb R$ such that $c$ divides all the elements of $S$, that is:

:$forall x in S: c mathrel backslash x$


Then $c$ is a common divisor of all the elements in $S$.",Definition:Common Divisor/Real Numbers,,false,"Let S be a finite set of real numbers, that is:

:S = { x_1, x_2, …, x_n: ∀ k ∈ℕ^*_n: x_k ∈ℝ}


Let c ∈ℝ such that c divides all the elements of S, that is:

:∀ x ∈ S: c  x


Then c is a common divisor of all the elements in S.",Common
"['Definitions/Greatest Common Divisor', 'Definitions/Number Theory', 'Definitions/Integral Domains', 'Definitions/Polynomial Theory']",Definition:Common,"=== Integral Domain ===
Let $left( D, +, times right)$ be an integral domain whose zero is $0$.

Let $a, b in D: a ne 0 lor b ne 0$.

Let $d mathrel backslash a$ denote that $d$ is a divisor of $a$.


Let $d in D$ have the following properties:

:$(1): quad d mathrel backslash a land d mathrel backslash b$
:$(2): quad c mathrel backslash a land c mathrel backslash b implies c mathrel backslash d$

Then $d$ is called a greatest common divisor of $a$ and $b$ (abbreviated GCD or gcd) and denoted $gcd leftlbrace a, b rightrbrace$.


That is, in the integral domain $D$, $d$ is the GCD of $a$ and $b$  if and only if :
:$d$ is a common divisor of $a$ and $b$
:Any other common divisor of $a$ and $b$ also divides $d$.


When $a = b = 0$, $gcd leftlbrace a, b rightrbrace$ is undefined.


We see that, trivially:
:$gcd leftlbrace a, b rightrbrace = gcd leftlbrace b, a rightrbrace$
so the set notation is justified.

=== Integers ===

When the integral domain in question is the integers $mathbb Z$, the GCD is often defined differently, as follows:
Let $a, b in mathbb Z: a ne 0 lor b ne 0$.


The greatest common divisor of $a$ and $b$ is defined as:

:the largest $d in mathbb Z_{>0}$ such that $d mathrel backslash a$ and $d mathrel backslash b$

where $mathrel backslash$ denotes divisibility.

This is denoted $gcd leftlbrace a, b rightrbrace$.


When $a = b = 0$, $gcd leftlbrace a, b rightrbrace$ is undefined.


=== General Definition ===

This definition can be extended to any (finite) number of integers.
Let $S = leftlbrace a_1, a_2, ldots, a_n rightrbrace subseteq mathbb Z$ such that $exists x in S: x ne 0$ (that is, at least one element of $S$ is non-zero).

=== Definition 1 ===
Let $S = leftlbrace a_1, a_2, ldots, a_n rightrbrace subseteq mathbb Z$ such that $exists x in S: x ne 0$ (that is, at least one element of $S$ is non-zero).


The greatest common divisor of $S$:
:$gcd left( S right) = gcd leftlbrace a_1, a_2, ldots, a_n rightrbrace$

is defined as the largest $d in mathbb Z_{>0}$ such that:
:$forall x in S: d mathrel backslash x$
where $mathrel backslash$ denotes divisibility.


By convention:
:$gcd left(   right)varnothing = 1$

=== Definition 2 ===
Let $S = leftlbrace a_1, a_2, ldots, a_n rightrbrace subseteq mathbb Z$ such that $exists x in S: x ne 0$ (that is, at least one element of $S$ is non-zero).


The greatest common divisor of $S$:
:$gcd left( S right) = gcd leftlbrace a_1, a_2, ldots, a_n rightrbrace$

is defined as the (strictly) positive integer $d in mathbb Z_{>0}$ such that:

 
 
 
 

where $mathrel backslash$ denotes divisibility.


By convention:
:$gcd left(   right)varnothing = 1$


By convention:
:$gcd left(   right)varnothing = 1$

=== Polynomial Ring over Field ===
Let $F$ be a field.

Let $P, Q, R in F left[ X right]$ be polynomials.


Then $R$ is the greatest common divisor of $P$ and $Q$  if and only if  it is a monic greatest common divisor.

This is denoted $gcd leftlbrace P, Q rightrbrace = R$.

=== Real Numbers ===

The concept can be extended to the set of real numbers:
Let $a, b in mathbb R$ be commensurable.

Then there exists a greatest element $d in mathbb R_{>0}$ such that:
:$d mathrel backslash a$
:$d mathrel backslash b$
where $d mathrel backslash a$ denotes that $d$ is a divisor of $a$.


This is called the greatest common divisor of $a$ and $b$ and denoted $gcd leftlbrace a, b rightrbrace$.",Definition:Greatest Common Divisor,,false,"=== Integral Domain ===
Let ( D, +, ×) be an integral domain whose zero is 0.

Let a, b ∈ D: a  0  b  0.

Let d  a denote that d is a divisor of a.


Let d ∈ D have the following properties:

:(1):    d  a  d  b
:(2):    c  a  c  b  c  d

Then d is called a greatest common divisor of a and b (abbreviated GCD or gcd) and denoted { a, b }.


That is, in the integral domain D, d is the GCD of a and b  if and only if :
:d is a common divisor of a and b
:Any other common divisor of a and b also divides d.


When a = b = 0, { a, b } is undefined.


We see that, trivially:
:{ a, b } = { b, a }
so the set notation is justified.

=== Integers ===

When the integral domain in question is the integers ℤ, the GCD is often defined differently, as follows:
Let a, b ∈ℤ: a  0  b  0.


The greatest common divisor of a and b is defined as:

:the largest d ∈ℤ_>0 such that d  a and d  b

where  denotes divisibility.

This is denoted { a, b }.


When a = b = 0, { a, b } is undefined.


=== General Definition ===

This definition can be extended to any (finite) number of integers.
Let S = { a_1, a_2, …, a_n }⊆ℤ such that ∃ x ∈ S: x  0 (that is, at least one element of S is non-zero).

=== Definition 1 ===
Let S = { a_1, a_2, …, a_n }⊆ℤ such that ∃ x ∈ S: x  0 (that is, at least one element of S is non-zero).


The greatest common divisor of S:
:( S ) = { a_1, a_2, …, a_n }

is defined as the largest d ∈ℤ_>0 such that:
:∀ x ∈ S: d  x
where  denotes divisibility.


By convention:
:(   )∅ = 1

=== Definition 2 ===
Let S = { a_1, a_2, …, a_n }⊆ℤ such that ∃ x ∈ S: x  0 (that is, at least one element of S is non-zero).


The greatest common divisor of S:
:( S ) = { a_1, a_2, …, a_n }

is defined as the (strictly) positive integer d ∈ℤ_>0 such that:

 
 
 
 

where  denotes divisibility.


By convention:
:(   )∅ = 1


By convention:
:(   )∅ = 1

=== Polynomial Ring over Field ===
Let F be a field.

Let P, Q, R ∈ F [ X ] be polynomials.


Then R is the greatest common divisor of P and Q  if and only if  it is a monic greatest common divisor.

This is denoted { P, Q } = R.

=== Real Numbers ===

The concept can be extended to the set of real numbers:
Let a, b ∈ℝ be commensurable.

Then there exists a greatest element d ∈ℝ_>0 such that:
:d  a
:d  b
where d  a denotes that d is a divisor of a.


This is called the greatest common divisor of a and b and denoted { a, b }.",Common
"['Definitions/Euclidean Number Theory', 'Definitions/Real Analysis', 'Definitions/Greatest Common Divisor']",Definition:Common,"Let $a, b in mathbb R$ be commensurable.

Then there exists a greatest element $d in mathbb R_{>0}$ such that:
:$d mathrel backslash a$
:$d mathrel backslash b$
where $d mathrel backslash a$ denotes that $d$ is a divisor of $a$.


This is called the greatest common divisor of $a$ and $b$ and denoted $gcd leftlbrace a, b rightrbrace$.",Definition:Greatest Common Divisor/Real Numbers,,false,"Let a, b ∈ℝ be commensurable.

Then there exists a greatest element d ∈ℝ_>0 such that:
:d  a
:d  b
where d  a denotes that d is a divisor of a.


This is called the greatest common divisor of a and b and denoted { a, b }.",Common
['Definitions/Number Theory'],Definition:Common,"Let $S$ be a finite set of non-zero integers, that is:

:$S = leftlbrace x_1, x_2, ldots, x_n: forall k in mathbb N^*_n: x_k in mathbb Z, x_k ne 0 rightrbrace$


Let $m in mathbb Z$ such that all the elements of $S$ divide $m$, that is:

:$forall x in S: x mathrel backslash m$


Then $m$ is a common multiple of all the elements in $S$.

 ",Definition:Common Multiple,,false,"Let S be a finite set of non-zero integers, that is:

:S = { x_1, x_2, …, x_n: ∀ k ∈ℕ^*_n: x_k ∈ℤ, x_k  0 }


Let m ∈ℤ such that all the elements of S divide m, that is:

:∀ x ∈ S: x  m


Then m is a common multiple of all the elements in S.

 ",Common
"['Definitions/Lowest Common Multiple', 'Definitions/Number Theory', 'Definitions/Integral Domains']",Definition:Common,"=== Integral Domain ===
Let $D$ be an integral domain and let $a, b in D$ be nonzero.

$l$ is the lowest common multiple of $a$ and $b$  if and only if :
:$(1): quad$ both $a$ and $b$ divide $l$
:$(2): quad$ if $m$ is another element such that $a$ and $b$ divide $m$, then $l$ divides $m$.

=== Integers ===
=== Definition 1 ===
For all $a, b in mathbb Z: a b ne 0$, there exists a smallest $m in mathbb Z: m > 0$ such that $a mathrel backslash m$ and $b mathrel backslash m$.

This $m$ is called the lowest common multiple of $a$ and $b$, and denoted $operatorname {lcm} leftlbrace a, b rightrbrace$.

=== Definition 2 ===
Let $a, b in mathbb Z$ be integers such that $a b ne 0$.

Then the lowest common multiple of $a$ and $b$ is the (strictly) positive integer $m$ which satisfies:
:$(1): quad a mathrel backslash m$ and $b mathrel backslash m$
:$(2): quad $If there exists $c in mathbb Z_{>0}$ such that $a mathrel backslash c$ and $b mathrel backslash c$, then $m le c$
where $mathrel backslash$ denotes divisibility.

=== General Definition ===

This definition can be extended to any (finite) number of integers.
Let $S = leftlbrace a_1, a_2, ldots, a_n rightrbrace subseteq mathbb Z$ such that $ds prod_{a mathop in S} a = 0$ (that is, all elements of $S$ are non-zero).

Then the lowest common multiple of $S$:
:$operatorname {lcm} left(   right)S = operatorname {lcm} leftlbrace a_1, a_2, ldots, a_n rightrbrace$

is defined as the smallest $m in mathbb Z_{>0}$ such that:
:$forall x in S: x mathrel backslash m$

where $mathrel backslash$ denotes divisibility.",Definition:Lowest Common Multiple,,false,"=== Integral Domain ===
Let D be an integral domain and let a, b ∈ D be nonzero.

l is the lowest common multiple of a and b  if and only if :
:(1): both a and b divide l
:(2): if m is another element such that a and b divide m, then l divides m.

=== Integers ===
=== Definition 1 ===
For all a, b ∈ℤ: a b  0, there exists a smallest m ∈ℤ: m > 0 such that a  m and b  m.

This m is called the lowest common multiple of a and b, and denoted lcm{ a, b }.

=== Definition 2 ===
Let a, b ∈ℤ be integers such that a b  0.

Then the lowest common multiple of a and b is the (strictly) positive integer m which satisfies:
:(1):    a  m and b  m
:(2):If there exists c ∈ℤ_>0 such that a  c and b  c, then m ≤ c
where  denotes divisibility.

=== General Definition ===

This definition can be extended to any (finite) number of integers.
Let S = { a_1, a_2, …, a_n }⊆ℤ such that ∏_a ∈ S a = 0 (that is, all elements of S are non-zero).

Then the lowest common multiple of S:
:lcm(   )S = lcm{ a_1, a_2, …, a_n }

is defined as the smallest m ∈ℤ_>0 such that:
:∀ x ∈ S: x  m

where  denotes divisibility.",Common
['Definitions/Arithmetic Sequences'],Definition:Common,"Let $leftlangle a_k rightrangle$ be the arithmetic sequence:

:$a_k = a_0 + k d$ for $k = 0, 1, 2, ldots, n - 1$


The term $d$ is the common difference of $leftlangle a_k rightrangle$.",Definition:Arithmetic Sequence/Common Difference,,false,"Let ⟨ a_k ⟩ be the arithmetic sequence:

:a_k = a_0 + k d for k = 0, 1, 2, …, n - 1


The term d is the common difference of ⟨ a_k ⟩.",Common
['Definitions/Geometric Sequences'],Definition:Common,"Let $leftlangle x_n rightrangle$ be a geometric sequence in $mathbb R$ defined as:
:$x_n = a r^n$ for $n = 0, 1, 2, 3, ldots$


The parameter:
:$r in mathbb R: r ne 0$
is called the common ratio of $leftlangle x_n rightrangle$.",Definition:Geometric Sequence/Common Ratio,,false,"Let ⟨ x_n ⟩ be a geometric sequence in ℝ defined as:
:x_n = a r^n for n = 0, 1, 2, 3, …


The parameter:
:r ∈ℝ: r  0
is called the common ratio of ⟨ x_n ⟩.",Common
"['Definitions/Vulgar Fractions', 'Definitions/Fractions']",Definition:Common,A vulgar fraction is a fraction representing a rational number whose numerator and denominator are both integers.,Definition:Fraction/Vulgar,,false,A vulgar fraction is a fraction representing a rational number whose numerator and denominator are both integers.,Common
['Definitions/Euclidean Geometry'],Definition:Common,"Let $A$ and $B$ be planes.

The common section of $A$ and $B$ is the intersection of $A$ and $B$.",Definition:Common Section,,false,"Let A and B be planes.

The common section of A and B is the intersection of A and B.",Common
"['Definitions/Commutative Algebras', 'Definitions/Algebras', 'Definitions/Commutativity']",Definition:Commutative Algebra,"Let $R$ be a commutative ring.

Let $left( A_R, oplus right)$ be an algebra over $R$.


Then $left( A_R, oplus right)$ is a commutative algebra  if and only if  $oplus$ is a commutative operation.

That is:

:$forall a, b in A_R: a oplus b = b oplus a$",Definition:Commutative Algebra (Abstract Algebra),,false,"Let R be a commutative ring.

Let ( A_R, ⊕) be an algebra over R.


Then ( A_R, ⊕) is a commutative algebra  if and only if  ⊕ is a commutative operation.

That is:

:∀ a, b ∈ A_R: a ⊕ b = b ⊕ a",Commutative Algebra
"['Definitions/Branches of Mathematics', 'Definitions/Commutative Algebra', 'Definitions/Abstract Algebra', 'Definitions/Ring Theory', 'Definitions/Module Theory', 'Definitions/Algebraic Geometry', 'Definitions/Algebraic Number Theory', 'Definitions/Commutativity']",Definition:Commutative Algebra,Commutative algebra is the branch of abstract algebra concerned with commutative and unitary rings.,Definition:Commutative Algebra (Mathematical Branch),algebra,true,Commutative algebra is the branch of abstract algebra concerned with commutative and unitary rings.,Commutative Algebra
['Definitions/Compact Spaces'],Definition:Compact,"=== Euclidean Space ===
Let $mathbb R^n$ denote Euclidean $n$-space.

Let $H subseteq mathbb R^n$.


Then $H$ is compact in $mathbb R^n$  if and only if  $H$ is closed and bounded.


=== Real Analysis ===

The same definition applies when $n = 1$, that is, for the real number line:

Let $mathbb R$ be the real number line considered as a topological space under the Euclidean topology.

Let $H subseteq mathbb R$.

Let $mathbb R$ be the real number line considered as a topological space under the Euclidean topology.

Let $H subseteq mathbb R$.


=== Definition 1 ===
Let $mathbb R$ be the real number line considered as a topological space under the Euclidean topology.

Let $H subseteq mathbb R$.


$H$ is compact in $mathbb R$  if and only if  $H$ is closed and bounded.

=== Definition 2 ===
Let $mathbb R$ be the real number line considered as a topological space under the Euclidean topology.

Let $H subseteq mathbb R$.


$H$ is compact in $mathbb R$  if and only if :
:when $H$ is the union of a set of neighborhoods which are open in $H$,
:then $H$ is also the union of a finite number of these neighborhoods.

=== Complex Analysis ===
Let $D$ be a subset of the complex plane $mathbb C$.


Then $D$ is compact (in $mathbb C$)  if and only if :
:$D$ is closed in $mathbb C$
and
:$D$ is bounded in $mathbb C$.

=== Topology ===
=== Definition 1 ===
A topological space $T = left( S, tau right)$ is compact  if and only if  every open cover for $S$ has a finite subcover.

=== Definition 2 ===
A topological space $T = left( S, tau right)$ is compact  if and only if  it satisfies the Finite Intersection Axiom.

=== Definition 3 ===
A topological space $T = left( S, tau right)$ is compact  if and only if  $tau$ has a sub-basis $mathcal B$ such that:
:from every cover of $S$ by elements of $mathcal B$, a finite subcover of $S$ can be selected.

=== Definition 4 ===
A topological space $T = left( S, tau right)$ is compact  if and only if  every filter on $S$ has a limit point in $S$.

=== Definition 5 ===
A topological space $T = left( S, tau right)$ is compact  if and only if  every ultrafilter on $S$ converges.

=== Metric Space ===
Let $M = left( A, d right)$ be a metric space.

Let $tau$ denote the topology on $A$ induced by $d$.


Then $M$ is compact  if and only if  $left( A, tau right)$ is a compact topological space.


=== Complex Plane ===
Let $D$ be a subset of the complex plane $mathbb C$.


Then $D$ is compact (in $mathbb C$)  if and only if :
:$D$ is closed in $mathbb C$
and
:$D$ is bounded in $mathbb C$.

=== Normed Vector Space ===
Let $left( X, leftlVert ,cdot, rightrVert  right)$ be a normed vector space. 

Let $K subseteq X$.


Then $K$ is compact  if and only if  every sequence in $K$ has a convergent subsequence with limit $L in K$.

That is, if:

:$leftlangle x_n rightrangle_{n mathop in mathbb N} :forall n in mathbb N : x_n in K implies exists leftlangle x_{n_k}  rightrangle_{k mathop in mathbb N} : exists L in K: ds  lim_{k mathop to infty} x_{n_k} = L$


=== Compact Subspace ===
Let $M = left( X, leftlVert ,cdot, rightrVert right)$ be a normed vector space.

Let $K subseteq X$ be a subset of $X$.


The normed vector subspace $M_K = left( K, leftlVert ,cdot, rightrVert_K right)$ is compact in $M$  if and only if  $M_K$ is itself a compact normed vector space.",Definition:Compact Space,,false,"=== Euclidean Space ===
Let ℝ^n denote Euclidean n-space.

Let H ⊆ℝ^n.


Then H is compact in ℝ^n  if and only if  H is closed and bounded.


=== Real Analysis ===

The same definition applies when n = 1, that is, for the real number line:

Let ℝ be the real number line considered as a topological space under the Euclidean topology.

Let H ⊆ℝ.

Let ℝ be the real number line considered as a topological space under the Euclidean topology.

Let H ⊆ℝ.


=== Definition 1 ===
Let ℝ be the real number line considered as a topological space under the Euclidean topology.

Let H ⊆ℝ.


H is compact in ℝ  if and only if  H is closed and bounded.

=== Definition 2 ===
Let ℝ be the real number line considered as a topological space under the Euclidean topology.

Let H ⊆ℝ.


H is compact in ℝ  if and only if :
:when H is the union of a set of neighborhoods which are open in H,
:then H is also the union of a finite number of these neighborhoods.

=== Complex Analysis ===
Let D be a subset of the complex plane ℂ.


Then D is compact (in ℂ)  if and only if :
:D is closed in ℂ
and
:D is bounded in ℂ.

=== Topology ===
=== Definition 1 ===
A topological space T = ( S, τ) is compact  if and only if  every open cover for S has a finite subcover.

=== Definition 2 ===
A topological space T = ( S, τ) is compact  if and only if  it satisfies the Finite Intersection Axiom.

=== Definition 3 ===
A topological space T = ( S, τ) is compact  if and only if  τ has a sub-basis ℬ such that:
:from every cover of S by elements of ℬ, a finite subcover of S can be selected.

=== Definition 4 ===
A topological space T = ( S, τ) is compact  if and only if  every filter on S has a limit point in S.

=== Definition 5 ===
A topological space T = ( S, τ) is compact  if and only if  every ultrafilter on S converges.

=== Metric Space ===
Let M = ( A, d ) be a metric space.

Let τ denote the topology on A induced by d.


Then M is compact  if and only if  ( A, τ) is a compact topological space.


=== Complex Plane ===
Let D be a subset of the complex plane ℂ.


Then D is compact (in ℂ)  if and only if :
:D is closed in ℂ
and
:D is bounded in ℂ.

=== Normed Vector Space ===
Let ( X, ‖ · ‖) be a normed vector space. 

Let K ⊆ X.


Then K is compact  if and only if  every sequence in K has a convergent subsequence with limit L ∈ K.

That is, if:

:⟨ x_n ⟩_n ∈ℕ :∀ n ∈ℕ : x_n ∈ K ∃⟨ x_n_k⟩_k ∈ℕ : ∃ L ∈ K: lim_k →∞ x_n_k = L


=== Compact Subspace ===
Let M = ( X, ‖ · ‖) be a normed vector space.

Let K ⊆ X be a subset of X.


The normed vector subspace M_K = ( K, ‖ · ‖_K ) is compact in M  if and only if  M_K is itself a compact normed vector space.",Compact
"['Definitions/Compact Spaces', 'Definitions/Complex Analysis']",Definition:Compact,"Let $D$ be a subset of the complex plane $mathbb C$.


Then $D$ is compact (in $mathbb C$)  if and only if :
:$D$ is closed in $mathbb C$
and
:$D$ is bounded in $mathbb C$.",Definition:Compact Space/Metric Space/Complex,,false,"Let D be a subset of the complex plane ℂ.


Then D is compact (in ℂ)  if and only if :
:D is closed in ℂ
and
:D is bounded in ℂ.",Compact
"['Definitions/Euclidean Space', 'Definitions/Compact Spaces']",Definition:Compact,"Let $mathbb R^n$ denote Euclidean $n$-space.

Let $H subseteq mathbb R^n$.


Then $H$ is compact in $mathbb R^n$  if and only if  $H$ is closed and bounded.


=== Real Analysis ===

The same definition applies when $n = 1$, that is, for the real number line:

Let $mathbb R$ be the real number line considered as a topological space under the Euclidean topology.

Let $H subseteq mathbb R$.

Let $mathbb R$ be the real number line considered as a topological space under the Euclidean topology.

Let $H subseteq mathbb R$.


=== Definition 1 ===
Let $mathbb R$ be the real number line considered as a topological space under the Euclidean topology.

Let $H subseteq mathbb R$.


$H$ is compact in $mathbb R$  if and only if  $H$ is closed and bounded.

=== Definition 2 ===
Let $mathbb R$ be the real number line considered as a topological space under the Euclidean topology.

Let $H subseteq mathbb R$.


$H$ is compact in $mathbb R$  if and only if :
:when $H$ is the union of a set of neighborhoods which are open in $H$,
:then $H$ is also the union of a finite number of these neighborhoods.

=== Complex Analysis ===
Let $D$ be a subset of the complex plane $mathbb C$.


Then $D$ is compact (in $mathbb C$)  if and only if :
:$D$ is closed in $mathbb C$
and
:$D$ is bounded in $mathbb C$.",Definition:Compact Space/Euclidean Space,,false,"Let ℝ^n denote Euclidean n-space.

Let H ⊆ℝ^n.


Then H is compact in ℝ^n  if and only if  H is closed and bounded.


=== Real Analysis ===

The same definition applies when n = 1, that is, for the real number line:

Let ℝ be the real number line considered as a topological space under the Euclidean topology.

Let H ⊆ℝ.

Let ℝ be the real number line considered as a topological space under the Euclidean topology.

Let H ⊆ℝ.


=== Definition 1 ===
Let ℝ be the real number line considered as a topological space under the Euclidean topology.

Let H ⊆ℝ.


H is compact in ℝ  if and only if  H is closed and bounded.

=== Definition 2 ===
Let ℝ be the real number line considered as a topological space under the Euclidean topology.

Let H ⊆ℝ.


H is compact in ℝ  if and only if :
:when H is the union of a set of neighborhoods which are open in H,
:then H is also the union of a finite number of these neighborhoods.

=== Complex Analysis ===
Let D be a subset of the complex plane ℂ.


Then D is compact (in ℂ)  if and only if :
:D is closed in ℂ
and
:D is bounded in ℂ.",Compact
"['Definitions/Real Analysis', 'Definitions/Compact Spaces', 'Definitions/Compact Space (Real Analysis)']",Definition:Compact,"Let $mathbb R$ be the real number line considered as a topological space under the Euclidean topology.

Let $H subseteq mathbb R$.


=== Definition 1 ===
Let $mathbb R$ be the real number line considered as a topological space under the Euclidean topology.

Let $H subseteq mathbb R$.


$H$ is compact in $mathbb R$  if and only if  $H$ is closed and bounded.

=== Definition 2 ===
Let $mathbb R$ be the real number line considered as a topological space under the Euclidean topology.

Let $H subseteq mathbb R$.


$H$ is compact in $mathbb R$  if and only if :
:when $H$ is the union of a set of neighborhoods which are open in $H$,
:then $H$ is also the union of a finite number of these neighborhoods.",Definition:Compact Space/Real Analysis,,false,"Let ℝ be the real number line considered as a topological space under the Euclidean topology.

Let H ⊆ℝ.


=== Definition 1 ===
Let ℝ be the real number line considered as a topological space under the Euclidean topology.

Let H ⊆ℝ.


H is compact in ℝ  if and only if  H is closed and bounded.

=== Definition 2 ===
Let ℝ be the real number line considered as a topological space under the Euclidean topology.

Let H ⊆ℝ.


H is compact in ℝ  if and only if :
:when H is the union of a set of neighborhoods which are open in H,
:then H is also the union of a finite number of these neighborhoods.",Compact
"['Definitions/Compact Spaces', 'Definitions/Topology']",Definition:Compact,"=== Definition 1 ===
A topological space $T = left( S, tau right)$ is compact  if and only if  every open cover for $S$ has a finite subcover.

=== Definition 2 ===
A topological space $T = left( S, tau right)$ is compact  if and only if  it satisfies the Finite Intersection Axiom.

=== Definition 3 ===
A topological space $T = left( S, tau right)$ is compact  if and only if  $tau$ has a sub-basis $mathcal B$ such that:
:from every cover of $S$ by elements of $mathcal B$, a finite subcover of $S$ can be selected.

=== Definition 4 ===
A topological space $T = left( S, tau right)$ is compact  if and only if  every filter on $S$ has a limit point in $S$.

=== Definition 5 ===
A topological space $T = left( S, tau right)$ is compact  if and only if  every ultrafilter on $S$ converges.",Definition:Compact Space/Topology,,false,"=== Definition 1 ===
A topological space T = ( S, τ) is compact  if and only if  every open cover for S has a finite subcover.

=== Definition 2 ===
A topological space T = ( S, τ) is compact  if and only if  it satisfies the Finite Intersection Axiom.

=== Definition 3 ===
A topological space T = ( S, τ) is compact  if and only if  τ has a sub-basis ℬ such that:
:from every cover of S by elements of ℬ, a finite subcover of S can be selected.

=== Definition 4 ===
A topological space T = ( S, τ) is compact  if and only if  every filter on S has a limit point in S.

=== Definition 5 ===
A topological space T = ( S, τ) is compact  if and only if  every ultrafilter on S converges.",Compact
['Definitions/Compact Spaces'],Definition:Compact,"Let $T = left( S, tau right)$ be a topological space.

Let $H subseteq S$ be a subset of $S$.


=== Definition 1 ===
Let $T = left( S, tau right)$ be a topological space.

Let $H subseteq S$ be a subset of $S$.


The topological subspace $T_H = left( H, tau_H right)$ is compact in $T$  if and only if  $T_H$ is itself a compact topological space.

=== Definition 2 ===

Let $T = left( S, tau right)$ be a topological space.

Let $H subseteq S$ be a subset of $S$.


$H$ is compact in $T$  if and only if  every open cover $mathcal C subseteq tau$ for $H$ has a finite subcover.",Definition:Compact Space/Topology/Subspace,,false,"Let T = ( S, τ) be a topological space.

Let H ⊆ S be a subset of S.


=== Definition 1 ===
Let T = ( S, τ) be a topological space.

Let H ⊆ S be a subset of S.


The topological subspace T_H = ( H, τ_H ) is compact in T  if and only if  T_H is itself a compact topological space.

=== Definition 2 ===

Let T = ( S, τ) be a topological space.

Let H ⊆ S be a subset of S.


H is compact in T  if and only if  every open cover 𝒞⊆τ for H has a finite subcover.",Compact
['Definitions/Compact Spaces'],Definition:Compact,"Let $M = left( A, d right)$ be a metric space.

Let $tau$ denote the topology on $A$ induced by $d$.


Then $M$ is compact  if and only if  $left( A, tau right)$ is a compact topological space.


=== Complex Plane ===
Let $D$ be a subset of the complex plane $mathbb C$.


Then $D$ is compact (in $mathbb C$)  if and only if :
:$D$ is closed in $mathbb C$
and
:$D$ is bounded in $mathbb C$.",Definition:Compact Space/Metric Space,,false,"Let M = ( A, d ) be a metric space.

Let τ denote the topology on A induced by d.


Then M is compact  if and only if  ( A, τ) is a compact topological space.


=== Complex Plane ===
Let D be a subset of the complex plane ℂ.


Then D is compact (in ℂ)  if and only if :
:D is closed in ℂ
and
:D is bounded in ℂ.",Compact
['Definitions/Normed Vector Spaces'],Definition:Compact,"Let $left( X, leftlVert ,cdot, rightrVert  right)$ be a normed vector space. 

Let $K subseteq X$.


Then $K$ is compact  if and only if  every sequence in $K$ has a convergent subsequence with limit $L in K$.

That is, if:

:$leftlangle x_n rightrangle_{n mathop in mathbb N} :forall n in mathbb N : x_n in K implies exists leftlangle x_{n_k}  rightrangle_{k mathop in mathbb N} : exists L in K: ds  lim_{k mathop to infty} x_{n_k} = L$


=== Compact Subspace ===
Let $M = left( X, leftlVert ,cdot, rightrVert right)$ be a normed vector space.

Let $K subseteq X$ be a subset of $X$.


The normed vector subspace $M_K = left( K, leftlVert ,cdot, rightrVert_K right)$ is compact in $M$  if and only if  $M_K$ is itself a compact normed vector space.",Definition:Compact Space/Normed Vector Space,,false,"Let ( X, ‖ · ‖) be a normed vector space. 

Let K ⊆ X.


Then K is compact  if and only if  every sequence in K has a convergent subsequence with limit L ∈ K.

That is, if:

:⟨ x_n ⟩_n ∈ℕ :∀ n ∈ℕ : x_n ∈ K ∃⟨ x_n_k⟩_k ∈ℕ : ∃ L ∈ K: lim_k →∞ x_n_k = L


=== Compact Subspace ===
Let M = ( X, ‖ · ‖) be a normed vector space.

Let K ⊆ X be a subset of X.


The normed vector subspace M_K = ( K, ‖ · ‖_K ) is compact in M  if and only if  M_K is itself a compact normed vector space.",Compact
['Definitions/Order Theory'],Definition:Compact,"Let $left( S, preceq right)$ be an ordered set.

Let $x in S$.


Then $x$ is compact (element)  if and only if  $x ll x$

where $ll$ denotes the way below relation.",Definition:Compact Element,,false,"Let ( S, ≼) be an ordered set.

Let x ∈ S.


Then x is compact (element)  if and only if  x ≪ x

where ≪ denotes the way below relation.",Compact
['Definitions/Convergence'],Definition:Compact,"Let $X$ be a topological space.

Let $M$ be a metric space.

Let $leftlangle{f_n}rightrangle$ be a sequence of mappings $f_n : X to M$.

Let $f: X to M$ be a mapping.


Then $f_n$ converges compactly to $f$  if and only if  $f_n$ converges uniformly to $f$ on every compact subset of $X$.",Definition:Compact Convergence,,false,"Let X be a topological space.

Let M be a metric space.

Let ⟨f_n⟩ be a sequence of mappings f_n : X → M.

Let f: X → M be a mapping.


Then f_n converges compactly to f  if and only if  f_n converges uniformly to f on every compact subset of X.",Compact
"['Definitions/Topology', 'Definitions/Compact Spaces']",Definition:Compact,"Let $left( X, tau_1 right)$ be a topological space.

Let $left( Y, tau_2 right)$ be a compact space.

Let $f: X to Y$ be a topological embedding.

Let $mathrm {Img} left( f right)$ be everywhere dense in $Y$.


Then either $f$ or $left( Y, tau_2 right)$ may be called a compactification of $left( X, tau_1 right)$.",Definition:Compactification,,false,"Let ( X, τ_1 ) be a topological space.

Let ( Y, τ_2 ) be a compact space.

Let f: X → Y be a topological embedding.

Let Img( f ) be everywhere dense in Y.


Then either f or ( Y, τ_2 ) may be called a compactification of ( X, τ_1 ).",Compact
['Definitions/Compatible Relations'],Definition:Compatible,"Let $left( S, circ right)$ be a closed algebraic structure.

Let $mathcal R$ be a relation on $S$.


Then $mathcal R$ is compatible with $circ$  if and only if :

:$forall x, y, z in S: x mathrel mathcal R y implies left( x circ z right) mathrel mathcal R left( y circ z right)$

:$forall x, y, z in S: x mathrel mathcal R y implies left( z circ x right) mathrel mathcal R left( z circ y right)$",Definition:Relation Compatible with Operation,,false,"Let ( S, ∘) be a closed algebraic structure.

Let ℛ be a relation on S.


Then ℛ is compatible with ∘  if and only if :

:∀ x, y, z ∈ S: x ℛ y ( x ∘ z ) ℛ( y ∘ z )

:∀ x, y, z ∈ S: x ℛ y ( z ∘ x ) ℛ( z ∘ y )",Compatible
['Definitions/Compatible Relations'],Definition:Compatible,"Let $left( S, circ right)$ be a closed algebraic structure.

Let $mathcal R$ be a relation in $S$.


Then $mathcal R$ is strongly compatible with $circ$  if and only if :

:$forall x, y, z in S: x mathrel mathcal R y iff left( x circ z right) mathrel mathcal R left( y circ z right)$

:$forall x, y, z in S: x mathrel mathcal R y iff left( z circ x right) mathrel mathcal R left( z circ y right)$.


That is,  if and only if  $mathcal R$ is compatible with $circ$ and conversely compatible with $circ$.",Definition:Relation Strongly Compatible with Operation,,false,"Let ( S, ∘) be a closed algebraic structure.

Let ℛ be a relation in S.


Then ℛ is strongly compatible with ∘  if and only if :

:∀ x, y, z ∈ S: x ℛ y ( x ∘ z ) ℛ( y ∘ z )

:∀ x, y, z ∈ S: x ℛ y ( z ∘ x ) ℛ( z ∘ y ).


That is,  if and only if  ℛ is compatible with ∘ and conversely compatible with ∘.",Compatible
['Definitions/Set Theory'],Definition:Compatible,"Let $F$ be a (unary) operation which can be applied to sets.


Then $F$ is compatible with set equivalence  if and only if :

:$F left[ A right] = F left[ B right] iff A sim B$

where:
:$A$ and $B$ are arbitrary sets
:$F left[ A right]$ denotes the image of $A$ under $F$
:$sim$ denotes set equivalence.",Definition:Operation Compatible with Set Equivalence,,false,"Let F be a (unary) operation which can be applied to sets.


Then F is compatible with set equivalence  if and only if :

:F [ A ] = F [ B ]  A ∼ B

where:
:A and B are arbitrary sets
:F [ A ] denotes the image of A under F
:∼ denotes set equivalence.",Compatible
"['Definitions/Ordered Rings', 'Definitions/Ordered Integral Domains']",Definition:Compatible,"Let $left( R, +, circ right)$ be a ring whose zero is $0_R$.


An ordering $preccurlyeq$ on $R$ is compatible with the ring structure of $R$  if and only if  $preccurlyeq$ satisies the ring compatible ordering axioms:
 ",Definition:Ordering Compatible with Ring Structure,,false,"Let ( R, +, ∘) be a ring whose zero is 0_R.


An ordering ≼ on R is compatible with the ring structure of R  if and only if  ≼ satisies the ring compatible ordering axioms:
 ",Compatible
['Definitions/Ring Theory'],Definition:Compatible,"Let $left( M, cdot right)$ be a semigroup.

Let $left( R, +, circ right)$ be a ring.

Let $leftlangle R_n rightrangle_{n mathop in M}$ be a gradation of type $M$ on the additive group of $R$.


The gradation is compatible with the ring structure  if and only if 
:$forall m, n in M : forall x in S_m, y in S_n: x circ y in S_{m cdot n}$

and so:

:$S_m S_n subseteq S_ {m cdot n}$",Definition:Gradation Compatible with Ring Structure,,false,"Let ( M, ·) be a semigroup.

Let ( R, +, ∘) be a ring.

Let ⟨ R_n ⟩_n ∈ M be a gradation of type M on the additive group of R.


The gradation is compatible with the ring structure  if and only if 
:∀ m, n ∈ M : ∀ x ∈ S_m, y ∈ S_n: x ∘ y ∈ S_m · n

and so:

:S_m S_n ⊆ S_ m · n",Compatible
['Definitions/Compatible Relations'],Definition:Compatible,"Let $left( S, circ right)$ be a closed algebraic structure.

Let $mathcal R$ be a relation in $S$.


Then $mathcal R$ is conversely compatible with $circ$  if and only if :

:$forall x, y, z in S: left( x circ z right) mathrel mathcal R left( y circ z right) implies x mathrel mathcal R y$

:$forall x, y, z in S: left( z circ x right) mathrel mathcal R left( z circ y right) implies x mathrel mathcal R y$",Definition:Relation Conversely Compatible with Operation,,false,"Let ( S, ∘) be a closed algebraic structure.

Let ℛ be a relation in S.


Then ℛ is conversely compatible with ∘  if and only if :

:∀ x, y, z ∈ S: ( x ∘ z ) ℛ( y ∘ z )  x ℛ y

:∀ x, y, z ∈ S: ( z ∘ x ) ℛ( z ∘ y )  x ℛ y",Compatible
['Definitions/Abstract Algebra'],Definition:Compatible,"A relation $mathcal R$ is universally compatible on a set $S$  if and only if  it is compatible with every closed operation that can be defined on $S$.

Category:Definitions/Abstract Algebra",Definition:Universally Compatible Relation,,false,"A relation ℛ is universally compatible on a set S  if and only if  it is compatible with every closed operation that can be defined on S.

Category:Definitions/Abstract Algebra",Compatible
"['Definitions/Abstract Algebra', 'Definitions/Equivalence Relations']",Definition:Compatible,"Let $left( S, circ right)$ be an algebraic structure.

Let $mathcal R$ be an equivalence relation on $S$.


Then $mathcal R$ is a congruence relation for $circ$  if and only if :

:$forall x_1, x_2, y_1, y_2 in S: left( x_1 mathrel mathcal R x_2 right) land left( y_1 mathrel mathcal R y_2 right) implies left( x_1 circ y_1 right) mathrel mathcal R left( x_2 circ y_2 right)$",Definition:Congruence Relation,,false,"Let ( S, ∘) be an algebraic structure.

Let ℛ be an equivalence relation on S.


Then ℛ is a congruence relation for ∘  if and only if :

:∀ x_1, x_2, y_1, y_2 ∈ S: ( x_1 ℛ x_2 ) ( y_1 ℛ y_2 ) ( x_1 ∘ y_1 ) ℛ( x_2 ∘ y_2 )",Compatible
['Definitions/Uniformities'],Definition:Compatible,"Let $mathcal U_1$ and $mathcal U_2$ be quasiuniformities on a set $S$.

Let $left( left( S, mathcal U_1 right), tau_1 right)$ and $left( left( S, mathcal U_2 right), tau_2 right)$ be the quasiuniform spaces generated by $mathcal U_1$ and $mathcal U_2$.


Then $mathcal U_1$ and $mathcal U_2$ are compatible (with each other)  if and only if  their topologies are equal.

That is,  if and only if  $tau_1 = tau_2$.",Definition:Compatible Quasiuniformities,,false,"Let 𝒰_1 and 𝒰_2 be quasiuniformities on a set S.

Let ( ( S, 𝒰_1 ), τ_1 ) and ( ( S, 𝒰_2 ), τ_2 ) be the quasiuniform spaces generated by 𝒰_1 and 𝒰_2.


Then 𝒰_1 and 𝒰_2 are compatible (with each other)  if and only if  their topologies are equal.

That is,  if and only if  τ_1 = τ_2.",Compatible
"['Definitions/Compatible Atlases', 'Definitions/Manifolds']",Definition:Compatible,"Let $M$ be a topological space.

Let $mathscr F, mathscr G$ be $d$-dimensional atlases of class $C^k$ on $M$.


=== Definition 1 ===
Let $M$ be a topological space.

Let $mathscr F, mathscr G$ be $d$-dimensional atlases of class $C^k$ on $M$.


$mathscr F, mathscr G$ are $C^k$-compatible  if and only if  their union $mathscr F cup mathscr G$ is an atlas of class $C^k$.

=== Definition 2 ===
Let $M$ be a topological space.

Let $mathscr F, mathscr G$ be $d$-dimensional atlases of class $C^k$ on $M$.


$mathscr F$ and $mathscr G$ are $C^k$-compatible  if and only if  every pair of charts $left( U, phi right) in mathscr F$ and $left( V, psi right) in mathscr G$ are $C^k$-compatible.",Definition:Compatible Atlases,,false,"Let M be a topological space.

Let ℱ, 𝒢 be d-dimensional atlases of class C^k on M.


=== Definition 1 ===
Let M be a topological space.

Let ℱ, 𝒢 be d-dimensional atlases of class C^k on M.


ℱ, 𝒢 are C^k-compatible  if and only if  their union ℱ∪𝒢 is an atlas of class C^k.

=== Definition 2 ===
Let M be a topological space.

Let ℱ, 𝒢 be d-dimensional atlases of class C^k on M.


ℱ and 𝒢 are C^k-compatible  if and only if  every pair of charts ( U, ϕ) ∈ℱ and ( V, ψ) ∈𝒢 are C^k-compatible.",Compatible
['Definitions/Manifolds'],Definition:Compatible,"Let $M$ be a topological space.

Let $d$ be a natural number.

Let $left( U, phi right)$ and $left( V, psi right)$ be $d$-dimensional charts of $M$.


Then $left( U, phi right)$ and $left( V, psi right)$ are $C^k$-compatible  if and only if  their transition mapping:
:$psi circ phi^{-1}: phi left(   right){U cap V} to psi left(   right){U cap V}$
is of class $C^k$.


=== Smoothly Compatible Charts ===
Let $M$ be a topological space.

Let $d$ be a natural number.

Let $left( U, phi right)$ and $left( V, psi right)$ be $d$-dimensional charts of $M$.


$left( U, phi right)$ and $left( V, psi right)$ are smoothly compatible  if and only if  their transition mapping:
:$psi circ phi^{-1} : phi left(   right){U cap V} to psi left(   right){U cap V}$
is of class $C^infty$.


Category:Definitions/Manifolds",Definition:Compatible Charts,,false,"Let M be a topological space.

Let d be a natural number.

Let ( U, ϕ) and ( V, ψ) be d-dimensional charts of M.


Then ( U, ϕ) and ( V, ψ) are C^k-compatible  if and only if  their transition mapping:
:ψ∘ϕ^-1: ϕ(   )U ∩ V→ψ(   )U ∩ V
is of class C^k.


=== Smoothly Compatible Charts ===
Let M be a topological space.

Let d be a natural number.

Let ( U, ϕ) and ( V, ψ) be d-dimensional charts of M.


( U, ϕ) and ( V, ψ) are smoothly compatible  if and only if  their transition mapping:
:ψ∘ϕ^-1 : ϕ(   )U ∩ V→ψ(   )U ∩ V
is of class C^∞.


Category:Definitions/Manifolds",Compatible
['Definitions/Manifolds'],Definition:Compatible,"Let $M$ be a topological space.

Let $d$ be a natural number.

Let $left( U, phi right)$ and $left( V, psi right)$ be $d$-dimensional charts of $M$.


$left( U, phi right)$ and $left( V, psi right)$ are smoothly compatible  if and only if  their transition mapping:
:$psi circ phi^{-1} : phi left(   right){U cap V} to psi left(   right){U cap V}$
is of class $C^infty$.


Category:Definitions/Manifolds",Definition:Compatible Charts/Smooth,,false,"Let M be a topological space.

Let d be a natural number.

Let ( U, ϕ) and ( V, ψ) be d-dimensional charts of M.


( U, ϕ) and ( V, ψ) are smoothly compatible  if and only if  their transition mapping:
:ψ∘ϕ^-1 : ϕ(   )U ∩ V→ψ(   )U ∩ V
is of class C^∞.


Category:Definitions/Manifolds",Compatible
['Definitions/Manifolds'],Definition:Compatible,"Let $M$ be a topological space.

Let $A$ be a $d$-dimensional $C^k$-atlas on $M$.

Let $left( U, phi right)$ be a $d$-dimensional chart of $M$.


Then $left( U, phi right)$ is $C^k$-compatible with $A$  if and only if  $left( U, phi right)$ is $C^k$-compatible with every chart of $A$.

Category:Definitions/Manifolds",Definition:Chart Compatible with Atlas,,false,"Let M be a topological space.

Let A be a d-dimensional C^k-atlas on M.

Let ( U, ϕ) be a d-dimensional chart of M.


Then ( U, ϕ) is C^k-compatible with A  if and only if  ( U, ϕ) is C^k-compatible with every chart of A.

Category:Definitions/Manifolds",Compatible
"['Definitions/Module Theory', 'Definitions/Compatible Module Structures']",Definition:Compatible,"Let $A$ and $B$ be rings.

Let $left( M, + right)$ be an abelian group.

Let $* : A times M to M$ and $circledast: B times M to M$ be left or right linear ring actions so that:
:$(1): quad left( M, +, * right)$ is a left or right module over $A$
:$(2): quad left( M, +, circledast right)$ is a left or right module over $B$


=== Definition 1 ===
Let $A$ and $B$ be rings.

Let $left( M, + right)$ be an abelian group.

Let $* : A times M to M$ and $circledast: B times M to M$ be left or right linear ring actions so that:
:$(1): quad left( M, +, * right)$ is a left or right module over $A$
:$(2): quad left( M, +, circledast right)$ is a left or right module over $B$


The module structures are compatible  if and only if  for all $a in A$, $b in B$, the homotheties $h_a$ and $h_b$ commute.

That is, for all $m in M$, $a in A$, $b in B$:
:$a * left( b circledast m right) = b circledast left( a * m right)$

=== Definition 2 ===
Let $A$ and $B$ be rings.

Let $left( M, + right)$ be an abelian group.

Let $* : A times M to M$ and $circledast: B times M to M$ be left or right linear ring actions so that:
:$(1): quad left( M, +, * right)$ is a left or right module over $A$
:$(2): quad left( M, +, circledast right)$ is a left or right module over $B$


The module structures are compatible  if and only if  for all $a in A$, the homothety $h_a : M to M$ is an endomorphism of the $B$-module $M$.

That is,  if and only if  the image of the ring representation $A to operatorname {End}  left(   right)M$ is contained in the endomorphism ring $operatorname {End}_B  left(   right)M$.

=== Definition 3 ===
Let $A$ and $B$ be rings.

Let $left( M, + right)$ be an abelian group.

Let $* : A times M to M$ and $circledast: B times M to M$ be left or right linear ring actions so that:
:$(1): quad left( M, +, * right)$ is a left or right module over $A$
:$(2): quad left( M, +, circledast right)$ is a left or right module over $B$


The module structures are compatible  if and only if  for all $b in A$, the homothety $h_b : M to M$ is an endomorphism of the $A$-module $M$.

That is,  if and only if  the image of the ring representation $B to operatorname {End}  left(   right)M$ is contained in the endomorphism ring $operatorname {End}_A left(   right)M$.",Definition:Compatible Module Structures,,false,"Let A and B be rings.

Let ( M, + ) be an abelian group.

Let * : A × M → M and ⊛: B × M → M be left or right linear ring actions so that:
:(1):   ( M, +, * ) is a left or right module over A
:(2):   ( M, +, ⊛) is a left or right module over B


=== Definition 1 ===
Let A and B be rings.

Let ( M, + ) be an abelian group.

Let * : A × M → M and ⊛: B × M → M be left or right linear ring actions so that:
:(1):   ( M, +, * ) is a left or right module over A
:(2):   ( M, +, ⊛) is a left or right module over B


The module structures are compatible  if and only if  for all a ∈ A, b ∈ B, the homotheties h_a and h_b commute.

That is, for all m ∈ M, a ∈ A, b ∈ B:
:a * ( b ⊛ m ) = b ⊛( a * m )

=== Definition 2 ===
Let A and B be rings.

Let ( M, + ) be an abelian group.

Let * : A × M → M and ⊛: B × M → M be left or right linear ring actions so that:
:(1):   ( M, +, * ) is a left or right module over A
:(2):   ( M, +, ⊛) is a left or right module over B


The module structures are compatible  if and only if  for all a ∈ A, the homothety h_a : M → M is an endomorphism of the B-module M.

That is,  if and only if  the image of the ring representation A →End(   )M is contained in the endomorphism ring End_B  (   )M.

=== Definition 3 ===
Let A and B be rings.

Let ( M, + ) be an abelian group.

Let * : A × M → M and ⊛: B × M → M be left or right linear ring actions so that:
:(1):   ( M, +, * ) is a left or right module over A
:(2):   ( M, +, ⊛) is a left or right module over B


The module structures are compatible  if and only if  for all b ∈ A, the homothety h_b : M → M is an endomorphism of the A-module M.

That is,  if and only if  the image of the ring representation B →End(   )M is contained in the endomorphism ring End_A (   )M.",Compatible
['Definitions/Angles'],Definition:Complement,":

Let $angle BAC$ be a right angle.

Let $angle BAD + angle DAC = angle BAC$.

That is:
:$angle DAC = angle BAC - angle BAD$


Then $angle DAC$ is the complement of $angle BAD$.


Hence, for any angle $alpha$ (whether less than a right angle or not), the complement of $alpha$ is $dfrac pi 2 - alpha$.

Measured in degrees, the complement of $alpha$ is $90^circ - alpha$.


If $alpha$ is the complement of $beta$, then it follows that $beta$ is the complement of $alpha$.

Hence we can say that $alpha$ and $beta$ are complementary.


It can be seen from this that the complement of an angle greater than a right angle is negative.


Thus complementary angles are two angles whose measures add up to the measure of a right angle.

That is, their measurements add up to $90$ degrees or $dfrac pi 2$ radians.",Definition:Complementary Angles,,false,":

Let ∠ BAC be a right angle.

Let ∠ BAD + ∠ DAC = ∠ BAC.

That is:
:∠ DAC = ∠ BAC - ∠ BAD


Then ∠ DAC is the complement of ∠ BAD.


Hence, for any angle α (whether less than a right angle or not), the complement of α is π 2 - α.

Measured in degrees, the complement of α is 90^∘ - α.


If α is the complement of β, then it follows that β is the complement of α.

Hence we can say that α and β are complementary.


It can be seen from this that the complement of an angle greater than a right angle is negative.


Thus complementary angles are two angles whose measures add up to the measure of a right angle.

That is, their measurements add up to 90 degrees or π 2 radians.",Complement
['Definitions/Parallelograms'],Definition:Complement,"Let $ABDC$ and $EFHG$ be two parallelograms with the same angles, which share a diagonal, such that $ABDC cap EFHG ne varnothing$.

:

Then the two parallelograms $CIGK$ and $BJHL$ are known as the complements of the parallelograms $ABDC$ and $EFHG$.",Definition:Complements of Parallelograms,,false,"Let ABDC and EFHG be two parallelograms with the same angles, which share a diagonal, such that ABDC ∩ EFHG ∅.

:

Then the two parallelograms CIGK and BJHL are known as the complements of the parallelograms ABDC and EFHG.",Complement
['Definitions/Relation Theory'],Definition:Complement,"Let $mathcal R subseteq S times T$ be a relation.


The complement of $mathcal R$ is the relative complement of $mathcal R$ with respect to $S times T$:
:$complement_{S times T} left(   right)mathcal R := leftlbrace left( s, t right) in S times T: left( s, t right) notin mathcal R rightrbrace$


If the sets $S$ and $T$ are implicit, then $complement left(   right)mathcal R$ can be used.",Definition:Complement of Relation,,false,"Let ℛ⊆ S × T be a relation.


The complement of ℛ is the relative complement of ℛ with respect to S × T:
:∁_S × T(   )ℛ := {( s, t ) ∈ S × T: ( s, t ) ∉ℛ}


If the sets S and T are implicit, then ∁(   )ℛ can be used.",Complement
"['Definitions/Set Complement', 'Definitions/Set Theory']",Definition:Complement,"The set complement (or, when the context is established, just complement) of a set $S$ in a universe $mathbb U$ is defined as:

:$complement left(   right)S = complement_{mathbb U} left(   right)S = mathbb U setminus S$

See the definition of Relative Complement for the definition of $complement_{mathbb U} left(   right)S$.


Thus the complement of a set $S$ is the relative complement of $S$ in the universe, or the complement of $S$ relative to the universe.

A common alternative to the symbology $complement left(   right)S$, which we will sometimes use, is $overline S$.",Definition:Set Complement,,false,"The set complement (or, when the context is established, just complement) of a set S in a universe 𝕌 is defined as:

:∁(   )S = ∁_𝕌(   )S = 𝕌∖ S

See the definition of Relative Complement for the definition of ∁_𝕌(   )S.


Thus the complement of a set S is the relative complement of S in the universe, or the complement of S relative to the universe.

A common alternative to the symbology ∁(   )S, which we will sometimes use, is S.",Complement
"['Definitions/Set Theory', 'Definitions/Relative Complement']",Definition:Complement,"Let $S$ be a set, and let $T subseteq S$, that is: let $T$ be a subset of $S$.

Then the set difference $S setminus T$ can be written $complement_{S} left(   right)T$, and is called the relative complement of $T$ in $S$, or the complement of $T$ relative to $S$.

Thus:
:$complement_{S} left(   right)T = leftlbrace x in S : x notin T rightrbrace$",Definition:Relative Complement,,false,"Let S be a set, and let T ⊆ S, that is: let T be a subset of S.

Then the set difference S ∖ T can be written ∁_S(   )T, and is called the relative complement of T in S, or the complement of T relative to S.

Thus:
:∁_S(   )T = { x ∈ S : x ∉ T }",Complement
['Definitions/Propositional Logic'],Definition:Complement,"The (logical) complement of a propositional formula $mathbf A$ is the negation of $mathbf A$, that is, $neg mathbf A$.

Conversely, the complement of $neg mathbf A$ is defined to be $mathbf A$.


=== Complementary Pair ===
For any propositional formula $mathbf A$, the set $left{{mathbf A, neg mathbf A}right}$ is called a complementary pair of formulas.",Definition:Logical Complement,,false,"The (logical) complement of a propositional formula 𝐀 is the negation of 𝐀, that is, 𝐀.

Conversely, the complement of 𝐀 is defined to be 𝐀.


=== Complementary Pair ===
For any propositional formula 𝐀, the set {𝐀, 𝐀} is called a complementary pair of formulas.",Complement
['Definitions/Lattice Theory'],Definition:Complement,"Let $left( S, vee, wedge, preceq right)$ be a bounded lattice.

Denote by $bot$ and $top$ the bottom and top of $S$, respectively.

Let $a in S$.


Then $b in S$ is called a complement of $a$  if and only if :

:$b vee a = top$
:$b wedge a = bot$


If $a$ has a unique complement, it is denoted by $neg a$.


=== Complemented Lattice ===
Let $left( S, vee, wedge, preceq right)$ be a bounded lattice.

Suppose that every $a in S$ admits a complement.


Then $left( S, vee, wedge, preceq right)$ is called a complemented lattice.",Definition:Complement (Lattice Theory),,false,"Let ( S, ∨, ∧, ≼) be a bounded lattice.

Denote by  and ⊤ the bottom and top of S, respectively.

Let a ∈ S.


Then b ∈ S is called a complement of a  if and only if :

:b ∨ a = ⊤
:b ∧ a =


If a has a unique complement, it is denoted by a.


=== Complemented Lattice ===
Let ( S, ∨, ∧, ≼) be a bounded lattice.

Suppose that every a ∈ S admits a complement.


Then ( S, ∨, ∧, ≼) is called a complemented lattice.",Complement
"['Definitions/Complements of Graphs', 'Definitions/Graph Theory']",Definition:Complement,"=== Simple Graph ===
Let $G = left( V, E right)$ be a simple graph.

The complement of $G$ is the simple graph $overline G = left( V, overline E right)$ which consists of:
:The same vertex set $V$ of $G$
:The set $overline E$ defined such that $leftlbrace u, v rightrbrace in overline E iff leftlbrace u, v rightrbrace notin E$, where $u$ and $v$ are distinct.

=== Loop-Graph ===
Let $G = left( V, E right)$ be a loop-graph.

The complement of $G$ is the loop-graph $overline G = left( V, overline E right)$ which consists of:
:The same vertex set $V$ of $G$;
:The set $overline E$ defined such that:
::$leftlbrace u, v rightrbrace in overline E iff leftlbrace u, v rightrbrace notin E$
::$leftlbrace v, v rightrbrace in overline E iff leftlbrace v, v rightrbrace notin E$


That is, the complement $overline G$ of a loop-graph $G$ has loops on all vertices where there are no loops in $G$.",Definition:Complement (Graph Theory),,false,"=== Simple Graph ===
Let G = ( V, E ) be a simple graph.

The complement of G is the simple graph G = ( V, E) which consists of:
:The same vertex set V of G
:The set E defined such that { u, v }∈E{ u, v }∉ E, where u and v are distinct.

=== Loop-Graph ===
Let G = ( V, E ) be a loop-graph.

The complement of G is the loop-graph G = ( V, E) which consists of:
:The same vertex set V of G;
:The set E defined such that:
::{ u, v }∈E{ u, v }∉ E
::{ v, v }∈E{ v, v }∉ E


That is, the complement G of a loop-graph G has loops on all vertices where there are no loops in G.",Complement
"['Definitions/Complete Metric Spaces', 'Definitions/Metric Spaces']",Definition:Complete,"=== Definition 1 ===
A metric space $M = left( A, d right)$ is complete  if and only if  every Cauchy sequence is convergent.

=== Definition 2 ===
A metric space $M = left( A, d right)$ is complete  if and only if  the intersection of every nested sequence of closed balls whose radii tend to zero is non-empty.",Definition:Complete Metric Space,,false,"=== Definition 1 ===
A metric space M = ( A, d ) is complete  if and only if  every Cauchy sequence is convergent.

=== Definition 2 ===
A metric space M = ( A, d ) is complete  if and only if  the intersection of every nested sequence of closed balls whose radii tend to zero is non-empty.",Complete
"['Definitions/Complete Metric Spaces', 'Definitions/Topology']",Definition:Complete,"Let $T = left( S, tau right)$ be a topological space.

Let $M = left( S, d right)$ be a complete metric space such that $left( S, tau right)$ is the topological space induced by $d$.


If there exists such a complete metric space, then $T$ is described as topologically complete.",Definition:Topologically Complete Space,,false,"Let T = ( S, τ) be a topological space.

Let M = ( S, d ) be a complete metric space such that ( S, τ) is the topological space induced by d.


If there exists such a complete metric space, then T is described as topologically complete.",Complete
"['Definitions/Lattice Theory', 'Definitions/Complete Lattices']",Definition:Complete,"=== Definition 1 ===
Let $left( S, preceq right)$ be a lattice.


Then $left( S, preceq right)$ is a complete lattice  if and only if :

:$forall T subseteq S: T$ admits both a supremum and an infimum.

=== Definition 2 ===
Let $left( S, preceq right)$ be an ordered set.


Then $left( S, preceq right)$ is a complete lattice  if and only if :

:$forall S' subseteq S: inf S', sup S' in S$

That is,  if and only if  all subsets of $S$ have both a supremum and an infimum.",Definition:Complete Lattice,,false,"=== Definition 1 ===
Let ( S, ≼) be a lattice.


Then ( S, ≼) is a complete lattice  if and only if :

:∀ T ⊆ S: T admits both a supremum and an infimum.

=== Definition 2 ===
Let ( S, ≼) be an ordered set.


Then ( S, ≼) is a complete lattice  if and only if :

:∀ S' ⊆ S: inf S', sup S' ∈ S

That is,  if and only if  all subsets of S have both a supremum and an infimum.",Complete
"['Definitions/Dedekind Completeness Property', 'Definitions/Order Theory']",Definition:Complete,"Let $left( S, preceq right)$ be an ordered set.

Then $left( S, preceq right)$ has the Dedekind completeness property  if and only if  every non-empty subset of $S$ that is bounded above admits a supremum (in $S$).",Definition:Dedekind Completeness Property,,false,"Let ( S, ≼) be an ordered set.

Then ( S, ≼) has the Dedekind completeness property  if and only if  every non-empty subset of S that is bounded above admits a supremum (in S).",Complete
"['Definitions/Set Theory', 'Definitions/Order Theory']",Definition:Complete,An inductive ordered set is an ordered set in which every chain has an upper bound.,Definition:Inductive Ordered Set,,false,An inductive ordered set is an ordered set in which every chain has an upper bound.,Complete
['Definitions/Truth Functions'],Definition:Complete,"Let $S$ be a set of truth functions.


Then $S$ is functionally complete  if and only if  all possible truth functions are definable from $S$.",Definition:Functionally Complete,,false,"Let S be a set of truth functions.


Then S is functionally complete  if and only if  all possible truth functions are definable from S.",Complete
"['Definitions/Complete Proof Systems', 'Definitions/Proof Systems']",Definition:Complete,"Let $mathcal L$ be a logical language.

Let $mathscr P$ be a proof system for $mathcal L$, and let $mathscr M$ be a formal semantics for $mathcal L$.


Then $mathscr P$ is said to be complete for $mathscr M$  if and only if :

:Every $mathscr M$-tautology is a $mathscr P$-theorem.

Symbolically, this can be expressed as the statement that, for every logical formula $phi$ of $mathcal L$:

:$models_{mathscr M} phi$ implies $vdash_{mathscr P} phi$


=== Strongly Complete Proof System ===
Let $mathcal L$ be a logical language.

Let $mathscr P$ be a proof system for $mathcal L$, and let $mathscr M$ be a formal semantics for $mathcal L$.


$mathscr P$ is strongly complete for $mathscr M$  if and only if :

:Every $mathscr M$-semantic consequence is a $mathscr P$-provable consequence.

Symbolically, this can be expressed as the statement that, for every collection $mathcal F$ of logical formulas, and every logical formula $phi$ of $mathcal L$:

:$mathcal F models_{mathscr M} phi$ implies $mathcal F vdash_{mathscr P} phi$",Definition:Complete Proof System,,false,"Let ℒ be a logical language.

Let 𝒫 be a proof system for ℒ, and let ℳ be a formal semantics for ℒ.


Then 𝒫 is said to be complete for ℳ  if and only if :

:Every ℳ-tautology is a 𝒫-theorem.

Symbolically, this can be expressed as the statement that, for every logical formula ϕ of ℒ:

:_ℳϕ implies ⊢_𝒫ϕ


=== Strongly Complete Proof System ===
Let ℒ be a logical language.

Let 𝒫 be a proof system for ℒ, and let ℳ be a formal semantics for ℒ.


𝒫 is strongly complete for ℳ  if and only if :

:Every ℳ-semantic consequence is a 𝒫-provable consequence.

Symbolically, this can be expressed as the statement that, for every collection ℱ of logical formulas, and every logical formula ϕ of ℒ:

:ℱ_ℳϕ implies ℱ⊢_𝒫ϕ",Complete
"['Definitions/Model Theory', 'Definitions/Formal Semantics']",Definition:Complete,"Let $mathcal L$ be a language.

Let $mathscr M$ be a formal semantics for $mathcal L$.

Let $T$ be an $mathcal L$-theory.


$T$ is complete (with respect to $mathcal L$ and $mathscr M$)  if and only if :
:$T$ is satisfiable for $mathscr M$
:for every $mathcal L$-sentence $phi$, either $T models_{mathscr M} phi$ or $T models_{mathscr M} neg phi$
where $T models_{mathscr M} phi$ denotes semantic entailment.",Definition:Complete Theory,,false,"Let ℒ be a language.

Let ℳ be a formal semantics for ℒ.

Let T be an ℒ-theory.


T is complete (with respect to ℒ and ℳ)  if and only if :
:T is satisfiable for ℳ
:for every ℒ-sentence ϕ, either T _ℳϕ or T _ℳϕ
where T _ℳϕ denotes semantic entailment.",Complete
['Definitions/Complete Graphs'],Definition:Complete,"Let $G = left( V, E right)$ be a simple graph such that every vertex is adjacent to every other vertex.

Then $G$ is called complete.


The complete graph of order $p$ is denoted $K_p$.",Definition:Complete Graph,,false,"Let G = ( V, E ) be a simple graph such that every vertex is adjacent to every other vertex.

Then G is called complete.


The complete graph of order p is denoted K_p.",Complete
"['Definitions/Complete Bipartite Graphs', 'Definitions/Bipartite Graphs', 'Definitions/Graph Theory']",Definition:Complete,"A complete bipartite graph is a bipartite graph $G = left( A mid B, E right)$ in which every vertex in $A$ is adjacent to every vertex in $B$.

The complete bipartite graph where $A$ has $m$ vertices and $B$ has $n$ vertices is denoted $K_{m, n}$.",Definition:Complete Bipartite Graph,,false,"A complete bipartite graph is a bipartite graph G = ( A | B, E ) in which every vertex in A is adjacent to every vertex in B.

The complete bipartite graph where A has m vertices and B has n vertices is denoted K_m, n.",Complete
['Definitions/Order Theory'],Definition:Completion,"Let $left( S, preceq_S right)$ be an ordered set.


An ordered set $left( T, preceq_T right)$ is an order completion of $S$  if and only if :

:$(1):quad S subseteq T$

:$(2):quad {preceq_T restriction_S} = {preceq_S}$, where $restriction$ denotes restriction

:$(3):quad left( T, preceq_T right)$ is a complete ordered set

:$(4):quad$ For all ordered sets $left( T', preceq_{T'}  right)$ satisfying $(1), (2)$ and $(3)$, there is a unique increasing injection $phi: T' to T$",Definition:Order Completion,,false,"Let ( S, ≼_S ) be an ordered set.


An ordered set ( T, ≼_T ) is an order completion of S  if and only if :

:(1):   S ⊆ T

:(2):  ≼_T _S = ≼_S, where  denotes restriction

:(3):  ( T, ≼_T ) is a complete ordered set

:(4): For all ordered sets ( T', ≼_T') satisfying (1), (2) and (3), there is a unique increasing injection ϕ: T' → T",Completion
['Definitions/Measure Theory'],Definition:Completion,"Let $left( X, unicode{x3a3}, mu right), left( tilde X, unicode{x3a3}^*, bar mu right)$ be measure spaces.

Then:
:$left( tilde X, unicode{x3a3}^*, bar mu right)$ is a completion of $left( X, unicode{x3a3}, mu right)$
or:
:$left( tilde X, unicode{x3a3}^*, bar mu right)$ completes $left( X, unicode{x3a3}, mu right)$

 if and only if  the following conditions hold:

:$(1): quad left( tilde X, unicode{x3a3}^*, bar mu right)$ is a complete measure space
:$(2): quad tilde X = X$
:$(3): quad unicode{x3a3}$ is a sub-$sigma$-algebra of $unicode{x3a3}^*$
:$(4): quad forall E in unicode{x3a3}: bar mu left(   right)E = mu left(   right)E$, that is: $bar mu restriction_unicode{x3a3} = mu$",Definition:Completion (Measure Space),,false,"Let ( X, x3a3, μ), ( X̃, x3a3^*, μ̅) be measure spaces.

Then:
:( X̃, x3a3^*, μ̅) is a completion of ( X, x3a3, μ)
or:
:( X̃, x3a3^*, μ̅) completes ( X, x3a3, μ)

 if and only if  the following conditions hold:

:(1):   ( X̃, x3a3^*, μ̅) is a complete measure space
:(2):   X̃ = X
:(3):   x3a3 is a sub-σ-algebra of x3a3^*
:(4):   ∀ E ∈x3a3: μ̅(   )E = μ(   )E, that is: μ̅_x3a3 = μ",Completion
['Definitions/Complete Metric Spaces'],Definition:Completion,"Let $M_1 = left( A, d right)$ and $M_2 = left( tilde A, tilde d right)$ be metric spaces.

Then $M_2$ is a completion of $M_1$, or $M_2$ completes $M_1$,  if and only if :
:$(1): quad M_2$ is a complete metric space
:$(2): quad A subseteq tilde A$
:$(3): quad A$ is dense in $M_2$
:$(4): quad forall x, y in A : tilde d left(   right){x, y} = d left(   right){x, y}$. In terms of restriction of functions, this says that $tilde d {restriction_A}  left(   right)= d$.


It is immediate from this definition that a completion of a metric space $M_1$ consists of:
:A complete metric space $M_2$
:An isometry $phi : A to tilde A$
such that $phi left(   right)A = leftlbrace phi left(   right)x: x in A rightrbrace$ is dense in $M_2$.

An isometry is often required to be bijective, so here one should consider $phi$ as a mapping from $A$ to the image of $phi$.

Therefore to insist that $phi$ be an isometry, in this context, is to say that $phi$ must be an injection that preserves the metric of $M_1$.",Definition:Completion (Metric Space),,false,"Let M_1 = ( A, d ) and M_2 = ( Ã, d̃) be metric spaces.

Then M_2 is a completion of M_1, or M_2 completes M_1,  if and only if :
:(1):    M_2 is a complete metric space
:(2):    A ⊆Ã
:(3):    A is dense in M_2
:(4):   ∀ x, y ∈ A : d̃(   )x, y = d (   )x, y. In terms of restriction of functions, this says that d̃_A(   )= d.


It is immediate from this definition that a completion of a metric space M_1 consists of:
:A complete metric space M_2
:An isometry ϕ : A →Ã
such that ϕ(   )A = {ϕ(   )x: x ∈ A } is dense in M_2.

An isometry is often required to be bijective, so here one should consider ϕ as a mapping from A to the image of ϕ.

Therefore to insist that ϕ be an isometry, in this context, is to say that ϕ must be an injection that preserves the metric of M_1.",Completion
['Definitions/Normed Division Rings'],Definition:Completion,"Let $left( R_1, leftlVert , cdot , rightrVert_1 right)$ and $left( R_2, leftlVert , cdot , rightrVert_2 right)$ be normed division rings.

Let $M_1 = left( R_1, d_1 right)$ and $M_2 = left( R_2, d_2 right)$ be the metric spaces where $d_1: R_1 times R_1 to mathbb R_{ge 0}$ and $d_2: R_2 times R_2 to mathbb R_{ge 0}$ are the metrics induced by $leftlVert , cdot , rightrVert_1$ and $leftlVert , cdot , rightrVert_2$ respectively.


Then $left( R_2, leftlVert , cdot , rightrVert_2 right)$ is a completion of $left( R_1, leftlVert , cdot , rightrVert_1 right)$  if and only if :
:$(1): quad$ there exists a distance-preserving ring monomorphism $phi: R_1 to R_2$
:$(2): quad M_2$ is a metric completion of $phi left(   right){M_1}$.


That is, $left( R_2, leftlVert ,cdot, rightrVert_2 right)$ is a completion of $left( R_1,leftlVert ,cdot, rightrVert_1 right)$  if and only if :
:$(a): quad M_2$ is a complete metric space
:$(b): quad$ there exists a distance-preserving ring monomorphism $phi: R_1 to R_2$
:$(c): quad phi left(   right){R_1}$ is a dense subspace in $M_2$.",Definition:Completion (Normed Division Ring),,false,"Let ( R_1, ‖ · ‖_1 ) and ( R_2, ‖ · ‖_2 ) be normed division rings.

Let M_1 = ( R_1, d_1 ) and M_2 = ( R_2, d_2 ) be the metric spaces where d_1: R_1 × R_1 →ℝ_≥ 0 and d_2: R_2 × R_2 →ℝ_≥ 0 are the metrics induced by ‖ · ‖_1 and ‖ · ‖_2 respectively.


Then ( R_2, ‖ · ‖_2 ) is a completion of ( R_1, ‖ · ‖_1 )  if and only if :
:(1): there exists a distance-preserving ring monomorphism ϕ: R_1 → R_2
:(2):    M_2 is a metric completion of ϕ(   )M_1.


That is, ( R_2, ‖ · ‖_2 ) is a completion of ( R_1,‖ · ‖_1 )  if and only if :
:(a):    M_2 is a complete metric space
:(b): there exists a distance-preserving ring monomorphism ϕ: R_1 → R_2
:(c):   ϕ(   )R_1 is a dense subspace in M_2.",Completion
"['Definitions/Branches of Mathematics', 'Definitions/Complex Analysis', 'Definitions/Analysis']",Definition:Complex,Complex analysis is a branch of mathematics that studies complex functions.,Definition:Analysis/Complex,mathematics,true,Complex analysis is a branch of mathematics that studies complex functions.,Complex
"['Definitions/Complex Numbers', 'Definitions/Standard Number Fields', 'Definitions/Numbers', 'Definitions/Complex Analysis', 'Definitions/Analysis']",Definition:Complex,"
=== Informal Definition ===
A complex number is a number in the form $a + b i$ or $a + i b$ where:
:$a$ and $b$ are real numbers
:$i$ is a square root of $-1$, that is, $i = sqrt {-1}$.

=== Formal Definition ===
A complex number is an ordered pair $left( x, y right)$ where $x, y in mathbb R$ are real numbers, on which the operations of addition and multiplication are defined as follows:


=== Complex Addition ===
Let $left( x_1, y_1 right)$ and $left( x_2, y_2 right)$ be complex numbers.

Then $left( x_1, y_1 right) + left( x_2, y_2 right)$ is defined as:

:$left( x_1, y_1 right) + left( x_2, y_2 right):= left( x_1 + x_2, y_1 + y_2 right)$

=== Complex Multiplication ===
Let $left( x_1, y_1 right)$ and $left( x_2, y_2 right)$ be complex numbers.


Then $left( x_1, y_1 right) left( x_2, y_2 right)$ is defined as:

:$left( x_1, y_1 right) left( x_2, y_2 right) := left( x_1 x_2 - y_1 y_2, x_1 y_2 + y_1 x_2 right)$

=== Scalar Product ===
Let $left( x, y right)$ be a complex number.

Let $m in mathbb R$ be a real number.


Then $m left( x, y right)$ is defined as:

:$m left( x, y right) := left( m x, m y right)$

=== Construction from Cayley-Dickson Construction ===
The complex numbers can be defined by the Cayley-Dickson construction from the set of real numbers $mathbb R$.

From Real Numbers form Algebra, $mathbb R$ forms a nicely normed $*$-algebra.

Let $a, b in mathbb R$.

Then $left( a, b right) in mathbb C$, where:

:$left( a, b right) left( c, d right) = left( a c - d overline b, overline a d + c b right)$
:$overline {left( a, b right) } = left( overline a, -b right)$
where:
:$overline a$ is the conjugate of $a$
and 
:$overline {left( a, b right) }$ is the conjugation operation on $mathbb C$.

From Real Numbers form Algebra, $overline a = a$ and so the above translate into:

:$left( a, b right) left( c, d right) = left( a c - d b, a d + c b right)$
:$overline {left( a, b right) } = left( a, -b right)$


It is clear by direct comparison with the formal definition that this construction genuinely does generate the complex numbers.",Definition:Complex Number,,false,"
=== Informal Definition ===
A complex number is a number in the form a + b i or a + i b where:
:a and b are real numbers
:i is a square root of -1, that is, i = √(-1).

=== Formal Definition ===
A complex number is an ordered pair ( x, y ) where x, y ∈ℝ are real numbers, on which the operations of addition and multiplication are defined as follows:


=== Complex Addition ===
Let ( x_1, y_1 ) and ( x_2, y_2 ) be complex numbers.

Then ( x_1, y_1 ) + ( x_2, y_2 ) is defined as:

:( x_1, y_1 ) + ( x_2, y_2 ):= ( x_1 + x_2, y_1 + y_2 )

=== Complex Multiplication ===
Let ( x_1, y_1 ) and ( x_2, y_2 ) be complex numbers.


Then ( x_1, y_1 ) ( x_2, y_2 ) is defined as:

:( x_1, y_1 ) ( x_2, y_2 ) := ( x_1 x_2 - y_1 y_2, x_1 y_2 + y_1 x_2 )

=== Scalar Product ===
Let ( x, y ) be a complex number.

Let m ∈ℝ be a real number.


Then m ( x, y ) is defined as:

:m ( x, y ) := ( m x, m y )

=== Construction from Cayley-Dickson Construction ===
The complex numbers can be defined by the Cayley-Dickson construction from the set of real numbers ℝ.

From Real Numbers form Algebra, ℝ forms a nicely normed *-algebra.

Let a, b ∈ℝ.

Then ( a, b ) ∈ℂ, where:

:( a, b ) ( c, d ) = ( a c - d b, a d + c b )
:( a, b )  = ( a, -b )
where:
:a is the conjugate of a
and 
:( a, b ) is the conjugation operation on ℂ.

From Real Numbers form Algebra, a = a and so the above translate into:

:( a, b ) ( c, d ) = ( a c - d b, a d + c b )
:( a, b )  = ( a, -b )


It is clear by direct comparison with the formal definition that this construction genuinely does generate the complex numbers.",Complex
"['Definitions/Complex Analysis', 'Definitions/Complex Functions']",Definition:Complex,"A complex function is a function whose domain and codomain are subsets of the set of complex numbers $mathbb C$.


=== Independent Variable ===
Let $f: mathbb C to mathbb C$ be a complex function.

Let $f left(   right)z = w$.


Then $z$ is referred to as an independent variable (of $f$).

=== Dependent Variable ===
Let $f: mathbb C to mathbb C$ be a complex function.

Let $f left(   right)z = w$.


Then $w$ is referred to as the dependent variable (of $f$).",Definition:Complex Function,,false,"A complex function is a function whose domain and codomain are subsets of the set of complex numbers ℂ.


=== Independent Variable ===
Let f: ℂ→ℂ be a complex function.

Let f (   )z = w.


Then z is referred to as an independent variable (of f).

=== Dependent Variable ===
Let f: ℂ→ℂ be a complex function.

Let f (   )z = w.


Then w is referred to as the dependent variable (of f).",Complex
"['Definitions/Complex Conjugates', 'Definitions/Complex Numbers', 'Definitions/Complex Analysis']",Definition:Complex,"Let $z = a + i b$ be a complex number.


Then the (complex) conjugate of $z$ is denoted $overline z$ and is defined as:

:$overline z := a - i b$


That is, you get the complex conjugate of a complex number by negating its imaginary part.


=== Complex Conjugation ===
The operation of complex conjugation is the mapping:
: $overline cdot: mathbb C to mathbb C: z mapsto overline z$.
where $overline z$ is the complex conjugate of $z$.


That is, it maps a complex number to its complex conjugate.


Category:Definitions/Complex Conjugates",Definition:Complex Conjugate,,false,"Let z = a + i b be a complex number.


Then the (complex) conjugate of z is denoted z and is defined as:

:z := a - i b


That is, you get the complex conjugate of a complex number by negating its imaginary part.


=== Complex Conjugation ===
The operation of complex conjugation is the mapping:
: ·: ℂ→ℂ: z ↦z.
where z is the complex conjugate of z.


That is, it maps a complex number to its complex conjugate.


Category:Definitions/Complex Conjugates",Complex
"['Definitions/Abstract Algebra', 'Definitions/Group Theory', 'Definitions/Subset Products']",Definition:Complex,"Let $left( S, circ right)$ be an algebraic structure.


We can define an operation on the power set $mathcal P left( S right)$ as follows:

:$forall A, B in mathcal P left( S right): A circ_mathcal P B = leftlbrace a circ b: a in A, b in B rightrbrace$


This is called the operation induced on $mathcal P left( S right)$ by $circ$, and $A circ_mathcal P B$ is called the subset product of $A$ and $B$.


It is usual to write $A circ B$ for $A circ_mathcal P B$.


=== Subset Product with Singleton ===

When one of the subsets in a subset product is a singleton, we can (and often do) dispose of the set braces. Thus:
Let $left( S, circ right)$ be an algebraic structure.


Let $A subseteq S$ be a subset of $S$.

Then:
:$(1): quad a circ S := leftlbrace a rightrbrace circ S$
:$(2): quad S circ a := S circ leftlbrace a rightrbrace$

where $leftlbrace a rightrbrace circ S$ and $S circ leftlbrace a rightrbrace$ denote the subset product of $leftlbrace a rightrbrace$ with $S$.


That is:
:$a circ S = leftlbrace a circ s: s in S rightrbrace$
:$S circ a = leftlbrace s circ a: s in S rightrbrace$",Definition:Subset Product,,false,"Let ( S, ∘) be an algebraic structure.


We can define an operation on the power set 𝒫( S ) as follows:

:∀ A, B ∈𝒫( S ): A ∘_𝒫 B = { a ∘ b: a ∈ A, b ∈ B }


This is called the operation induced on 𝒫( S ) by ∘, and A ∘_𝒫 B is called the subset product of A and B.


It is usual to write A ∘ B for A ∘_𝒫 B.


=== Subset Product with Singleton ===

When one of the subsets in a subset product is a singleton, we can (and often do) dispose of the set braces. Thus:
Let ( S, ∘) be an algebraic structure.


Let A ⊆ S be a subset of S.

Then:
:(1):    a ∘ S := { a }∘ S
:(2):    S ∘ a := S ∘{ a }

where { a }∘ S and S ∘{ a } denote the subset product of { a } with S.


That is:
:a ∘ S = { a ∘ s: s ∈ S }
:S ∘ a = { s ∘ a: s ∈ S }",Complex
"['Definitions/Group Theory', 'Definitions/Subsets', 'Definitions/Complexes of Groups']",Definition:Complex,"Let $G$ be a group.

Let $K subseteq G$ be a subset of $G$.


Then $K$ is referred to by some sources as a complex of elements of $G$.",Definition:Complex (Group Theory),,false,"Let G be a group.

Let K ⊆ G be a subset of G.


Then K is referred to by some sources as a complex of elements of G.",Complex
['Definitions/Homological Algebra'],Definition:Complex,"Let $R$ be a commutative ring with unity.

Let $ds M = bigoplus_{n mathop in mathbb Z} M^n$ be a $mathbb Z$-graded $R$-module that is also a differential module with differential $mathrm d$.


Then $M$ is a differential complex if $mathrm d$ satisfies:

:$mathrm d left(   right){M^n} subseteq M^{n + 1}$

for all $n in mathbb Z$.


The notation $mathrm d_n := mathrm d restriction_{M_n}$ is often seen.",Definition:Differential Complex,,false,"Let R be a commutative ring with unity.

Let M = ⊕_n ∈ℤ M^n be a ℤ-graded R-module that is also a differential module with differential d.


Then M is a differential complex if d satisfies:

:d(   )M^n⊆ M^n + 1

for all n ∈ℤ.


The notation d_n := d_M_n is often seen.",Complex
"['Definitions/Complex Fractions', 'Definitions/Fractions']",Definition:Complex,A complex fraction is a fraction such that the numerator or denominator or both are themselves fractions.,Definition:Fraction/Complex,,false,A complex fraction is a fraction such that the numerator or denominator or both are themselves fractions.,Complex
['Definitions/Compound Statements'],Definition:Component,A substatement of a compound statement is one of the statements that comprise it.,Definition:Compound Statement/Substatement,,false,A substatement of a compound statement is one of the statements that comprise it.,Component
['Definitions/Set Partitions'],Definition:Component,"Let $S$ be a set.

Let $mathbb S = leftlbrace S_1 mid S_2 mid cdots rightrbrace$ be a partition of $S$.


The elements $S_1, S_2, ldots in mathbb S$ are known as the components of the partition.


Category:Definitions/Set Partitions",Definition:Set Partition/Component,,false,"Let S be a set.

Let 𝕊 = { S_1 | S_2 |⋯} be a partition of S.


The elements S_1, S_2, …∈𝕊 are known as the components of the partition.


Category:Definitions/Set Partitions",Component
['Definitions/Connected Sets'],Definition:Component,"Let $T = left({S, tau}right)$ be a topological space.

Let the relation $sim $ be defined on $T$ as follows:

:$x sim y$  if and only if  $x$ and $y$ are connected in $T$.

That is,  if and only if  there exists a connected set of $T$ that contains both $x$ and $y$.


=== Definition 1 ===
Let $T = left( S, tau right)$ be a topological space.

Let the relation $sim $ be defined on $T$ as follows:

:$x sim y$  if and only if  $x$ and $y$ are connected in $T$.

That is,  if and only if  there exists a connected set of $T$ that contains both $x$ and $y$.


From Connectedness of Points is Equivalence Relation, $sim$ is an equivalence relation.

From the Fundamental Theorem on Equivalence Relations, the points in $T$ can be partitioned into equivalence classes.

These equivalence classes are called the (connected) components of $T$.


If $x in S$, then the component of $T$ containing $x$ (that is, the set of points $y in S$ with $x sim y$) is denoted by $operatorname {Comp}_x left(   right)T$.

=== Definition 2 ===
Let $T = left( S, tau right)$ be a topological space.

Let the relation $sim $ be defined on $T$ as follows:

:$x sim y$  if and only if  $x$ and $y$ are connected in $T$.

That is,  if and only if  there exists a connected set of $T$ that contains both $x$ and $y$.


The component of $T$ containing $x$ is defined as:

:$ds operatorname{Comp}_x left(   right)T = bigcup leftlbrace A subseteq S: x in A land A right.$ is connected $left.  rightrbrace$

=== Definition 3 ===
Let $T = left( S, tau right)$ be a topological space.

Let the relation $sim $ be defined on $T$ as follows:

:$x sim y$  if and only if  $x$ and $y$ are connected in $T$.

That is,  if and only if  there exists a connected set of $T$ that contains both $x$ and $y$.


The component of $T$ containing $x$ is defined as:
: the maximal connected set of $T$ that contains $x$.",Definition:Component (Topology),,false,"Let T = (S, τ) be a topological space.

Let the relation ∼ be defined on T as follows:

:x ∼ y  if and only if  x and y are connected in T.

That is,  if and only if  there exists a connected set of T that contains both x and y.


=== Definition 1 ===
Let T = ( S, τ) be a topological space.

Let the relation ∼ be defined on T as follows:

:x ∼ y  if and only if  x and y are connected in T.

That is,  if and only if  there exists a connected set of T that contains both x and y.


From Connectedness of Points is Equivalence Relation, ∼ is an equivalence relation.

From the Fundamental Theorem on Equivalence Relations, the points in T can be partitioned into equivalence classes.

These equivalence classes are called the (connected) components of T.


If x ∈ S, then the component of T containing x (that is, the set of points y ∈ S with x ∼ y) is denoted by Comp_x (   )T.

=== Definition 2 ===
Let T = ( S, τ) be a topological space.

Let the relation ∼ be defined on T as follows:

:x ∼ y  if and only if  x and y are connected in T.

That is,  if and only if  there exists a connected set of T that contains both x and y.


The component of T containing x is defined as:

:Comp_x (   )T = ⋃{ A ⊆ S: x ∈ A  A . is connected .  }

=== Definition 3 ===
Let T = ( S, τ) be a topological space.

Let the relation ∼ be defined on T as follows:

:x ∼ y  if and only if  x and y are connected in T.

That is,  if and only if  there exists a connected set of T that contains both x and y.


The component of T containing x is defined as:
: the maximal connected set of T that contains x.",Component
['Definitions/Path-Connected Spaces'],Definition:Component,"Let $T$ be a topological space.


=== Equivalence Class ===
Let $T$ be a topological space.


Let $sim$ be the equivalence relation on $T$ defined as:

:$x sim y iff x$ and $y$ are path-connected.

The equivalence classes of $sim$ are called the path components of $T$.

If $x in T$, then the path component of $T$ containing $x$ (that is, the set of points $y in T$ with $x sim y$) can be denoted by $operatorname{PC}_x left(   right)T$.


From Path-Connectedness is Equivalence Relation, $sim $ is an equivalence relation. 

From the Fundamental Theorem on Equivalence Relations, the points in $T$ can be partitioned into equivalence classes.

=== Union of Path-Connected Sets ===
Let $T$ be a topological space.


The path component of $T$ containing $x$ is defined as:

:$ds operatorname{PC}_x left(   right)T = bigcup leftlbrace A subseteq S: x in A land A right.$ is path-connected $left.  rightrbrace$

Category:Definitions/Path-Connected Spaces

=== Maximal Path-Connected Set ===
Let $T$ be a topological space.


The path component of $T$ containing $x$ is defined as:
:the maximal path-connected set of $T$ that contains $x$.",Definition:Path Component,,false,"Let T be a topological space.


=== Equivalence Class ===
Let T be a topological space.


Let ∼ be the equivalence relation on T defined as:

:x ∼ y  x and y are path-connected.

The equivalence classes of ∼ are called the path components of T.

If x ∈ T, then the path component of T containing x (that is, the set of points y ∈ T with x ∼ y) can be denoted by PC_x (   )T.


From Path-Connectedness is Equivalence Relation, ∼ is an equivalence relation. 

From the Fundamental Theorem on Equivalence Relations, the points in T can be partitioned into equivalence classes.

=== Union of Path-Connected Sets ===
Let T be a topological space.


The path component of T containing x is defined as:

:PC_x (   )T = ⋃{ A ⊆ S: x ∈ A  A . is path-connected .  }

Category:Definitions/Path-Connected Spaces

=== Maximal Path-Connected Set ===
Let T be a topological space.


The path component of T containing x is defined as:
:the maximal path-connected set of T that contains x.",Component
['Definitions/Arc-Connected Spaces'],Definition:Component,"Let $T$ be a topological space.

Let us define the relation $sim$ on $T$ as follows:

:$x sim y iff x$ and $y$ are arc-connected.


We have that $sim $ is an equivalence relation, so from the Fundamental Theorem on Equivalence Relations, the points in $T$ can be partitioned into equivalence classes.

These equivalence classes are called the arc components of $T$.


If $x in T$, then the arc component of $T$ containing $x$ (that is, the set of points $y in T$ with $x sim y$) can be denoted by $operatorname {AC}_x left(   right)T$.",Definition:Arc Component,,false,"Let T be a topological space.

Let us define the relation ∼ on T as follows:

:x ∼ y  x and y are arc-connected.


We have that ∼ is an equivalence relation, so from the Fundamental Theorem on Equivalence Relations, the points in T can be partitioned into equivalence classes.

These equivalence classes are called the arc components of T.


If x ∈ T, then the arc component of T containing x (that is, the set of points y ∈ T with x ∼ y) can be denoted by AC_x (   )T.",Component
['Definitions/Irreducible Spaces'],Definition:Component,"Let $T = left( S, tau right)$ be a topological space.


A subset $Y subseteq S$ is an irreducible component of $T$  if and only if :
:$Y$ is irreducible
:$Y$ is not a proper subset of an irreducible subset of $S$.

That is,  if and only if :
:$Y$ is maximal in the ordered set of irreducible subsets of $S$, ordered by the subset relation.",Definition:Irreducible Component,,false,"Let T = ( S, τ) be a topological space.


A subset Y ⊆ S is an irreducible component of T  if and only if :
:Y is irreducible
:Y is not a proper subset of an irreducible subset of S.

That is,  if and only if :
:Y is maximal in the ordered set of irreducible subsets of S, ordered by the subset relation.",Component
['Definitions/Graph Theory'],Definition:Component,"Let $G$ be a graph.

Let $H$ be a subgraph of $G$ such that:

:$H$ is connected

:$H$ is not contained in any connected subgraph of $G$ which has more vertices or edges than $H$ has.


Then $H$ is a component of $G$.",Definition:Component of Graph,,false,"Let G be a graph.

Let H be a subgraph of G such that:

:H is connected

:H is not contained in any connected subgraph of G which has more vertices or edges than H has.


Then H is a component of G.",Component
['Definitions/Vectors'],Definition:Component,"Let $mathbf a$ be a vector quantity embedded in an $n$-dimensional Cartesian coordinate system $C_n$.


Let $mathbf a$ be represented with its initial point at the origin of $C_n$.

Let $mathbf e_1, mathbf e_2, ldots, mathbf e_n$ be the unit vectors in the positive direction of the coordinate axes of $C_n$.

Then:
:$mathbf a = a_1 mathbf e_1 + a_2 mathbf e_2 + cdots + a_3 mathbf e_n$

where:
:$a_1 mathbf e_1, a_2 mathbf e_2, ldots, a_3 mathbf e_n$ are the component vectors of $mathbf a$ in the directions of $mathbf e_1, mathbf e_2, ldots, mathbf e_n$ 
:$a_1, a_2, ldots, a_3$ are the components of $mathbf a$ in the directions of $mathbf e_1, mathbf e_2, ldots, mathbf e_n$.


The number of components in $mathbf a$ is determined by the number of dimensions in the Cartesian coordinate system of its frame of reference.


A vector quantity with $n$ components can be referred to as an $n$-vector.


It is usually more convenient to write $mathbf a$ as the ordered tuple $left( a_1, a_2, ldots, a_n right)$ instead of $mathbf a = a_1 mathbf e_1 + a_2 mathbf e_2 + cdots + a_3 mathbf e_n$.


There are two special cases:


=== Cartesian Plane ===
Let $mathbf a$ be a vector quantity embedded in a Cartesian plane $P$.


Let $mathbf a$ be represented with its initial point at the origin of $P$.

Let $mathbf i$ and $mathbf j$ be the unit vectors in the positive direction of the $x$-axis and $y$-axis.

Then:
:$mathbf a = x mathbf i + y mathbf j$

where:
:$x mathbf i$ and $y mathbf j$ are the component vectors of $mathbf a$ in the $mathbf i$ and $mathbf j$ directions
:$x$ and $y$ are the components of $mathbf a$ in the $mathbf i$ and $mathbf j$ directions.


It is usually more convenient to write $mathbf a$ as the ordered pair $left( x, y right)$ instead of $mathbf a = x mathbf i + y mathbf j$.

=== Cartesian $3$-Space ===
Let $mathbf a$ be a vector quantity embedded in Cartesian $3$-space $S$.

Let $mathbf i$, $mathbf j$ and $mathbf k$ be the unit vectors in the positive directions of the $x$-axis, $y$-axis and $z$-axis respectively.

Then:
:$mathbf a = x mathbf i + y mathbf j + z mathbf k$

where:
:$x mathbf i$, $y mathbf j$ and $z mathbf k$ are the component vectors of $mathbf a$ in the $mathbf i, mathbf j, mathbf k$ directions
:$x$, $y$ and $z$ are the components of $mathbf a$ in the $mathbf i$, $mathbf j$ and $mathbf k$ directions.

It is usual to arrange that the coordinate axes form a right-handed Cartesian $3$-space.


It is usually more convenient to write $mathbf a$ as the ordered tuple $left( x, y, z right)$ instead of $mathbf a = x mathbf i + y mathbf j + z mathbf k$.


=== $x$ Component ===
Let:
:$mathbf a = x mathbf i + y mathbf j + z mathbf k$ be a vector quantity embedded in Cartesian $3$-space $S$

where $mathbf i$, $mathbf j$ and $mathbf k$ be the unit vectors in the positive directions of the $x$-axis, $y$-axis and $z$-axis respectively.


The value $x$ is known as the $x$ component of $mathbf a$.

=== $y$ Component ===
Let:
:$mathbf a = x mathbf i + y mathbf j + z mathbf k$ be a vector quantity embedded in Cartesian $3$-space $S$

where $mathbf i$, $mathbf j$ and $mathbf k$ be the unit vectors in the positive directions of the $x$-axis, $y$-axis and $z$-axis respectively.


The value $y$ is known as the $y$ component of $mathbf a$.

=== $z$ Component ===
Let:
:$mathbf a = x mathbf i + y mathbf j + z mathbf k$ be a vector quantity embedded in Cartesian $3$-space $S$

where $mathbf i$, $mathbf j$ and $mathbf k$ be the unit vectors in the positive directions of the $x$-axis, $y$-axis and $z$-axis respectively.


The value $z$ is known as the $z$ component of $mathbf a$.",Definition:Vector Quantity/Component,,false,"Let 𝐚 be a vector quantity embedded in an n-dimensional Cartesian coordinate system C_n.


Let 𝐚 be represented with its initial point at the origin of C_n.

Let 𝐞_1, 𝐞_2, …, 𝐞_n be the unit vectors in the positive direction of the coordinate axes of C_n.

Then:
:𝐚 = a_1 𝐞_1 + a_2 𝐞_2 + ⋯ + a_3 𝐞_n

where:
:a_1 𝐞_1, a_2 𝐞_2, …, a_3 𝐞_n are the component vectors of 𝐚 in the directions of 𝐞_1, 𝐞_2, …, 𝐞_n 
:a_1, a_2, …, a_3 are the components of 𝐚 in the directions of 𝐞_1, 𝐞_2, …, 𝐞_n.


The number of components in 𝐚 is determined by the number of dimensions in the Cartesian coordinate system of its frame of reference.


A vector quantity with n components can be referred to as an n-vector.


It is usually more convenient to write 𝐚 as the ordered tuple ( a_1, a_2, …, a_n ) instead of 𝐚 = a_1 𝐞_1 + a_2 𝐞_2 + ⋯ + a_3 𝐞_n.


There are two special cases:


=== Cartesian Plane ===
Let 𝐚 be a vector quantity embedded in a Cartesian plane P.


Let 𝐚 be represented with its initial point at the origin of P.

Let 𝐢 and 𝐣 be the unit vectors in the positive direction of the x-axis and y-axis.

Then:
:𝐚 = x 𝐢 + y 𝐣

where:
:x 𝐢 and y 𝐣 are the component vectors of 𝐚 in the 𝐢 and 𝐣 directions
:x and y are the components of 𝐚 in the 𝐢 and 𝐣 directions.


It is usually more convenient to write 𝐚 as the ordered pair ( x, y ) instead of 𝐚 = x 𝐢 + y 𝐣.

=== Cartesian 3-Space ===
Let 𝐚 be a vector quantity embedded in Cartesian 3-space S.

Let 𝐢, 𝐣 and 𝐤 be the unit vectors in the positive directions of the x-axis, y-axis and z-axis respectively.

Then:
:𝐚 = x 𝐢 + y 𝐣 + z 𝐤

where:
:x 𝐢, y 𝐣 and z 𝐤 are the component vectors of 𝐚 in the 𝐢, 𝐣, 𝐤 directions
:x, y and z are the components of 𝐚 in the 𝐢, 𝐣 and 𝐤 directions.

It is usual to arrange that the coordinate axes form a right-handed Cartesian 3-space.


It is usually more convenient to write 𝐚 as the ordered tuple ( x, y, z ) instead of 𝐚 = x 𝐢 + y 𝐣 + z 𝐤.


=== x Component ===
Let:
:𝐚 = x 𝐢 + y 𝐣 + z 𝐤 be a vector quantity embedded in Cartesian 3-space S

where 𝐢, 𝐣 and 𝐤 be the unit vectors in the positive directions of the x-axis, y-axis and z-axis respectively.


The value x is known as the x component of 𝐚.

=== y Component ===
Let:
:𝐚 = x 𝐢 + y 𝐣 + z 𝐤 be a vector quantity embedded in Cartesian 3-space S

where 𝐢, 𝐣 and 𝐤 be the unit vectors in the positive directions of the x-axis, y-axis and z-axis respectively.


The value y is known as the y component of 𝐚.

=== z Component ===
Let:
:𝐚 = x 𝐢 + y 𝐣 + z 𝐤 be a vector quantity embedded in Cartesian 3-space S

where 𝐢, 𝐣 and 𝐤 be the unit vectors in the positive directions of the x-axis, y-axis and z-axis respectively.


The value z is known as the z component of 𝐚.",Component
"['Definitions/Electrical Components', 'Definitions/Electronics']",Definition:Component,An electrical component is a device whose purpose is to control electricity in a specific manner.,Definition:Electrical Component,,false,An electrical component is a device whose purpose is to control electricity in a specific manner.,Component
['Definitions/Binary Operations'],Definition:Composite,"=== Indexed Iteration ===
Let $left( G, * right)$ be a magma.

Let $a, b in mathbb Z$ be integers.

Let $left[ a ,.,.,   right]b$ be the integer interval between $a$ and $b$.

Let $f: left[ a ,.,.,   right]b to G$ be a mapping.


The indexed iteration of $*$ of $f$ from $a$ to $b$ is recursively defined and denoted:

:$ds prod_{k mathop = a}^b f left(   right)k = begin {cases} f left(   right)a & : b = a \ left( ds prod_{k mathop = a}^{b - 1} f left(   right)k right) * f left(   right)b & : b > a end {cases}$


For each ordered $n$-tuple $left( a_1, a_2, ldots, a_n right) in S^n$, the composite of $left( a_1, a_2, ldots, a_n right)$ for $oplus$ is the value at $left( a_1, a_2, ldots, a_n right)$ of the $n$-ary operation defined by $oplus$.


This composite is recursively defined and denoted:

 
 
 
 
 


=== Degenerate case ===

 

Let $left( G, * right)$ be a unitary magma with identity $e$.

Let $a, b in mathbb Z$ be integers such that $a < b$.


Then:
:$ds prod_{i mathop = a}^b f left(   right)i = e$

=== Iteration over Finite Set ===
Let $left( G, * right)$ be a commutative semigroup.

Let $S$ be a finite non-empty set.

Let $f: S to G$ be a mapping.

Let $n in mathbb N$ be the cardinality of $S$.

Let $g: mathbb N_{

=== Commutative Monoid ===

Let $G$ be a commutative monoid.

Let $S$ be a non-empty set.

Let $f: S to G$ be a mapping

 

=== Iteration over Set with Finite Support ===
Let $left( G, * right)$ be a commutative monoid.

Let $S$ be a set.

Let $f: S to G$ be a mapping.

Let the support $mathrm {supp} left(   right)f$ be finite.


The iteration of $*$ of $f$ over $S$, denoted $ds prod_{s mathop in S} f left(   right)s$, is the iteration over the finite set $mathrm {supp} left(   right)f$ of $f$:
:$ds prod_{s mathop in S} f left(   right)s = prod_{s mathop in mathrm {supp} left(   right)f} f left(   right)s$",Definition:Iterated Binary Operation,,false,"=== Indexed Iteration ===
Let ( G, * ) be a magma.

Let a, b ∈ℤ be integers.

Let [ a  . . ]b be the integer interval between a and b.

Let f: [ a  . . ]b → G be a mapping.


The indexed iteration of * of f from a to b is recursively defined and denoted:

:∏_k  = a^b f (   )k =  f (   )a     : b = a 
( ∏_k  = a^b - 1 f (   )k ) * f (   )b     : b > a


For each ordered n-tuple ( a_1, a_2, …, a_n ) ∈ S^n, the composite of ( a_1, a_2, …, a_n ) for ⊕ is the value at ( a_1, a_2, …, a_n ) of the n-ary operation defined by ⊕.


This composite is recursively defined and denoted:

 
 
 
 
 


=== Degenerate case ===

 

Let ( G, * ) be a unitary magma with identity e.

Let a, b ∈ℤ be integers such that a < b.


Then:
:∏_i  = a^b f (   )i = e

=== Iteration over Finite Set ===
Let ( G, * ) be a commutative semigroup.

Let S be a finite non-empty set.

Let f: S → G be a mapping.

Let n ∈ℕ be the cardinality of S.

Let g: ℕ_

=== Commutative Monoid ===

Let G be a commutative monoid.

Let S be a non-empty set.

Let f: S → G be a mapping

 

=== Iteration over Set with Finite Support ===
Let ( G, * ) be a commutative monoid.

Let S be a set.

Let f: S → G be a mapping.

Let the support supp(   )f be finite.


The iteration of * of f over S, denoted ∏_s ∈ S f (   )s, is the iteration over the finite set supp(   )f of f:
:∏_s ∈ S f (   )s = ∏_s ∈supp(   )f f (   )s",Composite
"['Definitions/Composite Numbers', 'Definitions/Number Theory']",Definition:Composite,"A composite number $c$ is a positive integer that has strictly more than two positive divisors.

That is, an integer greater than $1$ which is not prime is defined as composite.


 
: 
:A composite number is that which is measured by some number.
 ''
 

=== Sequence of Composite Numbers ===
 ",Definition:Composite Number,,false,"A composite number c is a positive integer that has strictly more than two positive divisors.

That is, an integer greater than 1 which is not prime is defined as composite.


 
: 
:A composite number is that which is measured by some number.
 ”
 

=== Sequence of Composite Numbers ===
 ",Composite
"['Definitions/Mapping Theory', 'Definitions/Composite Mappings']",Definition:Composition,"Let $f_1: S_1 to S_2$ and $f_2: S_2 to S_3$ be mappings such that the domain of $f_2$ is the same set as the codomain of $f_1$.


=== Definition 1 ===
Let $S_1$, $S_2$ and $S_3$ be sets.

Let $f_1: S_1 to S_2$ and $f_2: S_2 to S_3$ be mappings such that the domain of $f_2$ is the same set as the codomain of $f_1$.


The composite mapping $f_2 circ f_1$ is defined as:

:$forall x in S_1: left( f_2 circ f_1 right)  left(   right)x := f_2 left(   right){f_1 left(   right)x}$


:


=== Commutative Diagram ===
Let $S_1$, $S_2$ and $S_3$ be sets.

Let $f_1: S_1 to S_2$ and $f_2: S_2 to S_3$ be mappings such that the domain of $f_2$ is the same set as the codomain of $f_1$.


The concept of composition of mappings can be illustrated by means of a commutative diagram.

This diagram illustrates the specific example of $f_2 circ f_1$:

::$begin{xy}xymatrix@+1em{
 S_1
  ar[r]^*+{f_1}
  ar@{-->}[rd]_*[l]+{f_2 mathop circ f_1}
&
 S_2
  ar[d]^*+{f_2}

\
&
 S_3
}end{xy}$

=== Definition 2 ===
Let $S_1$, $S_2$ and $S_3$ be sets.

Let $f_1: S_1 to S_2$ and $f_2: S_2 to S_3$ be mappings such that the domain of $f_2$ is the same set as the codomain of $f_1$.


The composite of $f_1$ and $f_2$ is defined and denoted as:

:$f_2 circ f_1 := leftlbrace left( x, z right) in S_1 times S_3: left( f_1 left(   right)x, z right) in f_2 rightrbrace$


:


=== Commutative Diagram ===
Let $S_1$, $S_2$ and $S_3$ be sets.

Let $f_1: S_1 to S_2$ and $f_2: S_2 to S_3$ be mappings such that the domain of $f_2$ is the same set as the codomain of $f_1$.


The concept of composition of mappings can be illustrated by means of a commutative diagram.

This diagram illustrates the specific example of $f_2 circ f_1$:

::$begin{xy}xymatrix@+1em{
 S_1
  ar[r]^*+{f_1}
  ar@{-->}[rd]_*[l]+{f_2 mathop circ f_1}
&
 S_2
  ar[d]^*+{f_2}

\
&
 S_3
}end{xy}$

=== Definition 3 ===
Let $S_1$, $S_2$ and $S_3$ be sets.

Let $f_1: S_1 to S_2$ and $f_2: S_2 to S_3$ be mappings such that the domain of $f_2$ is the same set as the codomain of $f_1$.


The composite of $f_1$ and $f_2$ is defined and denoted as:

:$f_2 circ f_1 := leftlbrace left( x, z right) in S_1 times S_3: exists y in S_2: f_1 left(   right)x = y land f_2 left(   right)y = z rightrbrace$


That is:
:$f_2 circ f_1 := leftlbrace left( x, z right) in S_1 times S_3: exists y in S_2: left( x, y right) in f_1 land left( y, z right) in f_2 rightrbrace$


:


=== Commutative Diagram ===
Let $S_1$, $S_2$ and $S_3$ be sets.

Let $f_1: S_1 to S_2$ and $f_2: S_2 to S_3$ be mappings such that the domain of $f_2$ is the same set as the codomain of $f_1$.


The concept of composition of mappings can be illustrated by means of a commutative diagram.

This diagram illustrates the specific example of $f_2 circ f_1$:

::$begin{xy}xymatrix@+1em{
 S_1
  ar[r]^*+{f_1}
  ar@{-->}[rd]_*[l]+{f_2 mathop circ f_1}
&
 S_2
  ar[d]^*+{f_2}

\
&
 S_3
}end{xy}$

:

=== General Definition ===
Let $f_1: S_1 to S_2, f_2: S_2 to S_3, ldots, f_n: S_n to S_{n + 1}$ be mappings such that the domain of $f_k$ is the same set as the codomain of $f_{k - 1}$.


Then the composite of $f_1, f_2, ldots, f_n$ is defined and denoted as:

 
 
 
 

=== Commutative Diagram ===
Let $S_1$, $S_2$ and $S_3$ be sets.

Let $f_1: S_1 to S_2$ and $f_2: S_2 to S_3$ be mappings such that the domain of $f_2$ is the same set as the codomain of $f_1$.


The concept of composition of mappings can be illustrated by means of a commutative diagram.

This diagram illustrates the specific example of $f_2 circ f_1$:

::$begin{xy}xymatrix@+1em{
 S_1
  ar[r]^*+{f_1}
  ar@{-->}[rd]_*[l]+{f_2 mathop circ f_1}
&
 S_2
  ar[d]^*+{f_2}

\
&
 S_3
}end{xy}$

=== Composition as a Binary Operation ===
Let $left[ S to S right]$ be the set of all mappings from a set $S$ to itself.

Then the concept of composite mapping defines a binary operation on $left[ S to S right]$:

:$forall f, g in left[ S to S right]: g circ f = leftlbrace left( s, t right): s in S, left( f left( s right), t right) in g rightrbrace in left[ S to S right]$


Thus, for every pair $left( f, g right)$ of mappings in $left[ S to S right]$, the composition $g circ f$ is another element of $left[ S to S right]$.",Definition:Composition of Mappings,,false,"Let f_1: S_1 → S_2 and f_2: S_2 → S_3 be mappings such that the domain of f_2 is the same set as the codomain of f_1.


=== Definition 1 ===
Let S_1, S_2 and S_3 be sets.

Let f_1: S_1 → S_2 and f_2: S_2 → S_3 be mappings such that the domain of f_2 is the same set as the codomain of f_1.


The composite mapping f_2 ∘ f_1 is defined as:

:∀ x ∈ S_1: ( f_2 ∘ f_1 )  (   )x := f_2 (   )f_1 (   )x


:


=== Commutative Diagram ===
Let S_1, S_2 and S_3 be sets.

Let f_1: S_1 → S_2 and f_2: S_2 → S_3 be mappings such that the domain of f_2 is the same set as the codomain of f_1.


The concept of composition of mappings can be illustrated by means of a commutative diagram.

This diagram illustrates the specific example of f_2 ∘ f_1:

::@+1em
 S_1
  [r]^*+f_1@–>[rd]_*[l]+f_2 ∘ f_1   
 S_2
  [d]^*+f_2
   
 S_3

=== Definition 2 ===
Let S_1, S_2 and S_3 be sets.

Let f_1: S_1 → S_2 and f_2: S_2 → S_3 be mappings such that the domain of f_2 is the same set as the codomain of f_1.


The composite of f_1 and f_2 is defined and denoted as:

:f_2 ∘ f_1 := {( x, z ) ∈ S_1 × S_3: ( f_1 (   )x, z ) ∈ f_2 }


:


=== Commutative Diagram ===
Let S_1, S_2 and S_3 be sets.

Let f_1: S_1 → S_2 and f_2: S_2 → S_3 be mappings such that the domain of f_2 is the same set as the codomain of f_1.


The concept of composition of mappings can be illustrated by means of a commutative diagram.

This diagram illustrates the specific example of f_2 ∘ f_1:

::@+1em
 S_1
  [r]^*+f_1@–>[rd]_*[l]+f_2 ∘ f_1   
 S_2
  [d]^*+f_2
   
 S_3

=== Definition 3 ===
Let S_1, S_2 and S_3 be sets.

Let f_1: S_1 → S_2 and f_2: S_2 → S_3 be mappings such that the domain of f_2 is the same set as the codomain of f_1.


The composite of f_1 and f_2 is defined and denoted as:

:f_2 ∘ f_1 := {( x, z ) ∈ S_1 × S_3: ∃ y ∈ S_2: f_1 (   )x = y  f_2 (   )y = z }


That is:
:f_2 ∘ f_1 := {( x, z ) ∈ S_1 × S_3: ∃ y ∈ S_2: ( x, y ) ∈ f_1 ( y, z ) ∈ f_2 }


:


=== Commutative Diagram ===
Let S_1, S_2 and S_3 be sets.

Let f_1: S_1 → S_2 and f_2: S_2 → S_3 be mappings such that the domain of f_2 is the same set as the codomain of f_1.


The concept of composition of mappings can be illustrated by means of a commutative diagram.

This diagram illustrates the specific example of f_2 ∘ f_1:

::@+1em
 S_1
  [r]^*+f_1@–>[rd]_*[l]+f_2 ∘ f_1   
 S_2
  [d]^*+f_2
   
 S_3

:

=== General Definition ===
Let f_1: S_1 → S_2, f_2: S_2 → S_3, …, f_n: S_n → S_n + 1 be mappings such that the domain of f_k is the same set as the codomain of f_k - 1.


Then the composite of f_1, f_2, …, f_n is defined and denoted as:

 
 
 
 

=== Commutative Diagram ===
Let S_1, S_2 and S_3 be sets.

Let f_1: S_1 → S_2 and f_2: S_2 → S_3 be mappings such that the domain of f_2 is the same set as the codomain of f_1.


The concept of composition of mappings can be illustrated by means of a commutative diagram.

This diagram illustrates the specific example of f_2 ∘ f_1:

::@+1em
 S_1
  [r]^*+f_1@–>[rd]_*[l]+f_2 ∘ f_1   
 S_2
  [d]^*+f_2
   
 S_3

=== Composition as a Binary Operation ===
Let [ S → S ] be the set of all mappings from a set S to itself.

Then the concept of composite mapping defines a binary operation on [ S → S ]:

:∀ f, g ∈[ S → S ]: g ∘ f = {( s, t ): s ∈ S, ( f ( s ), t ) ∈ g }∈[ S → S ]


Thus, for every pair ( f, g ) of mappings in [ S → S ], the composition g ∘ f is another element of [ S → S ].",Composition
['Definitions/Relation Theory'],Definition:Composition,"Let $mathcal R_1 subseteq S_1 times T_1$ and $mathcal R_2 subseteq S_2 times T_2$ be relations.


Then the composite of $mathcal R_1$ and $mathcal R_2$ is defined and denoted as:

:$mathcal R_2 circ mathcal R_1 := leftlbrace left( x, z right) in S_1 times T_2: exists y in S_2 cap T_1: left( x, y right) in mathcal R_1 land left( y, z right) in mathcal R_2 rightrbrace$


 

 

It is clear that the composite relation $mathcal R_2 circ mathcal R_1$ can also be defined as:

:$mathcal R_2 circ mathcal R_1 left(   right){S_1} = mathcal R_2 left(   right){mathcal R_1 left(   right){S_1} }$


Note that:
:$(1): quad mathcal R_2 circ mathcal R_1 subseteq S_1 times T_2$
:$(2): quad$ The domain of $mathcal R_2 circ mathcal R_1$ equals the domain of $mathcal R_1$, that is, $S_1$
:$(3): quad$ The codomain of $mathcal R_2 circ mathcal R_1$ equals the codomain of $mathcal R_2$, that is, $T_2$.",Definition:Composition of Relations,,false,"Let ℛ_1 ⊆ S_1 × T_1 and ℛ_2 ⊆ S_2 × T_2 be relations.


Then the composite of ℛ_1 and ℛ_2 is defined and denoted as:

:ℛ_2 ∘ℛ_1 := {( x, z ) ∈ S_1 × T_2: ∃ y ∈ S_2 ∩ T_1: ( x, y ) ∈ℛ_1 ( y, z ) ∈ℛ_2 }


 

 

It is clear that the composite relation ℛ_2 ∘ℛ_1 can also be defined as:

:ℛ_2 ∘ℛ_1 (   )S_1 = ℛ_2 (   )ℛ_1 (   )S_1


Note that:
:(1):   ℛ_2 ∘ℛ_1 ⊆ S_1 × T_2
:(2): The domain of ℛ_2 ∘ℛ_1 equals the domain of ℛ_1, that is, S_1
:(3): The codomain of ℛ_2 ∘ℛ_1 equals the codomain of ℛ_2, that is, T_2.",Composition
"['Definitions/Composition Series', 'Definitions/Normal Series', 'Definitions/Finite Groups']",Definition:Composition,"Let $G$ be a finite group.

=== Definition 1 ===
Let $G$ be a finite group.


A composition series for $G$ is a normal series for $G$ which has no proper refinement.


=== Composition Length ===
 

=== Composition Factor ===
 

=== Definition 2 ===
Let $G$ be a finite group.


A composition series for $G$ is a sequence of normal subgroups of $G$:
:$leftlbrace e rightrbrace = G_0 lhd G_1 lhd cdots lhd G_n = G$
where:
:$G_{i - 1} lhd G_i$ denotes that $G_{i - 1}$ is a proper normal subgroup of $G_i$
such that:
:for all $i in leftlbrace 1, 2, ldots, n rightrbrace$, $G_{i - 1}$ is a proper maximal normal subgroup of $G_i$.


=== Composition Length ===
 

=== Composition Factor ===
 

=== Composition Length ===
 

=== Composition Factor ===
 ",Definition:Composition Series,,false,"Let G be a finite group.

=== Definition 1 ===
Let G be a finite group.


A composition series for G is a normal series for G which has no proper refinement.


=== Composition Length ===
 

=== Composition Factor ===
 

=== Definition 2 ===
Let G be a finite group.


A composition series for G is a sequence of normal subgroups of G:
:{ e } = G_0  G_1 ⋯ G_n = G
where:
:G_i - 1 G_i denotes that G_i - 1 is a proper normal subgroup of G_i
such that:
:for all i ∈{ 1, 2, …, n }, G_i - 1 is a proper maximal normal subgroup of G_i.


=== Composition Length ===
 

=== Composition Factor ===
 

=== Composition Length ===
 

=== Composition Factor ===
 ",Composition
"['Definitions/Category Theory', 'Definitions/Morphisms']",Definition:Composition,"Let $mathbf C$ be a metacategory.

Let $left({g, f}right)$ be a pair of composable morphisms.


Then the composition of $f$ and $g$ is a morphism $g circ f$ of $mathbf C$ subject to:

:$operatorname{dom} left({g circ f}right) = operatorname{dom} f$
:$operatorname{cod} left({g circ f}right) = operatorname{cod} g$


This composition of morphisms can be thought of as an abstraction of both composition of mappings and transitive relations.",Definition:Composition of Morphisms,,false,"Let 𝐂 be a metacategory.

Let (g, f) be a pair of composable morphisms.


Then the composition of f and g is a morphism g ∘ f of 𝐂 subject to:

:dom(g ∘ f) = dom f
:cod(g ∘ f) = cod g


This composition of morphisms can be thought of as an abstraction of both composition of mappings and transitive relations.",Composition
['Definitions/Category Theory'],Definition:Composition,"Let $mathbf C, mathbf D$ and $mathbf E$ be metacategories.

Let $F: mathbf C to mathbf D$ and $G: mathbf D to mathbf E$ be (covariant) functors.


The composition of $G$ with $F$ is the functor $GF: mathbf C to mathbf E$ defined by:

:For all objects $C$ of $mathbf C$: $hskip{2.9cm} GF left({C}right) := G left({FC}right)$
:For all morphisms $f: C_1 to C_2$ of $mathbf C$: $quad GF left({f}right) := G left({Ff}right)$

$GF$ is said to be a composite functor.",Definition:Composition of Functors,,false,"Let 𝐂, 𝐃 and 𝐄 be metacategories.

Let F: 𝐂→𝐃 and G: 𝐃→𝐄 be (covariant) functors.


The composition of G with F is the functor GF: 𝐂→𝐄 defined by:

:For all objects C of 𝐂: 2.9cm GF (C) := G (FC)
:For all morphisms f: C_1 → C_2 of 𝐂: GF (f) := G (Ff)

GF is said to be a composite functor.",Composition
"['Definitions/Slice Categories', 'Definitions/Category Theory']",Definition:Composition,"Let $mathbf C$ be a metacategory.

Let $C$ and $D$ be objects of $mathbf C$.

Let $mathbf C / C$ and $mathbf C / D$ be the associated slice categories.


Let $g: C to D$ be a morphism of $mathbf C$.

Then $g$ defines a composition functor $g_* : mathbf C / C to mathbf C / D$:

 
 
 
 


That it is in fact a functor is shown on Composition Functor on Slice Categories is Functor.


The effect of $g_*$ is captured in the following commutative diagram:

::$begin{xy}
<-3em,0em>*+{X} = ""X"",
<3em,0em>*+{X'} = ""X2"",
<0em,-4em>*+{C} = ""C"",
<0em,-8em>*+{D} = ""D"",

""X"";""X2"" **@{-} ?>*@{>} ?*!/_1em/{a},
""X"";""C"" **@{-} ?>*@{>} ?<>(.4)*!/^.6em/{f},
""X2"";""C"" **@{-} ?>*@{>} ?<>(.4)*!/_.6em/{f'},
""C"";""D"" **@{-} ?>*@{>} ?<>(.4)*!/^.6em/{g},

""X"";""D"" **crv{<-5em,-4em>} ?>*@{>} ?*!/^1.6em/{g_* f = \ g circ f},
""X2"";""D"" **crv{<5em,-4em>} ?>*@{>} ?*!/_1.6em/{g_* f' = \ g circ f'},
end{xy}$",Definition:Composition Functor on Slice Categories,,false,"Let 𝐂 be a metacategory.

Let C and D be objects of 𝐂.

Let 𝐂 / C and 𝐂 / D be the associated slice categories.


Let g: C → D be a morphism of 𝐂.

Then g defines a composition functor g_* : 𝐂 / C →𝐂 / D:

 
 
 
 


That it is in fact a functor is shown on Composition Functor on Slice Categories is Functor.


The effect of g_* is captured in the following commutative diagram:

::<-3em,0em>*+X = ""X"",
<3em,0em>*+X' = ""X2"",
<0em,-4em>*+C = ""C"",
<0em,-8em>*+D = ""D"",

""X"";""X2"" **@- ?>*@> ?*!/_1em/a,
""X"";""C"" **@- ?>*@> ?<>(.4)*!/^.6em/f,
""X2"";""C"" **@- ?>*@> ?<>(.4)*!/_.6em/f',
""C"";""D"" **@- ?>*@> ?<>(.4)*!/^.6em/g,

""X"";""D"" **<-5em,-4em> ?>*@> ?*!/^1.6em/g_* f = 
 g ∘ f,
""X2"";""D"" **<5em,-4em> ?>*@> ?*!/_1.6em/g_* f' = 
 g ∘ f',",Composition
['Definitions/Categories of Subobjects'],Definition:Composition,"Let $mathbf C$ be a metacategory.

Let $C$ and $D$ be objects of $mathbf C$.

Let $mathbf{Sub}_{mathbf C} left({C}right)$ and $mathbf{Sub}_{mathbf C} left({D}right)$ be the associated categories of subobjects.


Let $g: C to D$ be a monomorphism of $mathbf C$.

Then $g$ defines a composition functor $g_* : mathbf{Sub}_{mathbf C} left({C}right) to mathbf{Sub}_{mathbf C} left({D}right)$:

 
 
 
 


That it is in fact a functor is shown on Composition Functor on Categories of Subobjects is Functor.


The effect of $g_*$ is captured in the following commutative diagram:

::$begin{xy}
<-3em,0em>*+{X} = ""X"",
<3em,0em>*+{X'} = ""X2"",
<0em,-4em>*+{C} = ""C"",
<0em,-8em>*+{D} = ""D"",

""X"";""X2"" **@{-} ?>*@{>} ?*!/_1em/{a},
""X"";""C"" **@{-} ?>*@{>} ?<>(.4)*!/^.6em/{f},
""X2"";""C"" **@{-} ?>*@{>} ?<>(.4)*!/_.6em/{f'},
""C"";""D"" **@{-} ?>*@{>} ?<>(.4)*!/^.6em/{g},

""X"";""D"" **crv{<-5em,-4em>} ?>*@{>} ?*!/^1.6em/{g_* f = \ g circ f},
""X2"";""D"" **crv{<5em,-4em>} ?>*@{>} ?*!/_1.6em/{g_* f' = \ g circ f'},
end{xy}$",Definition:Composition Functor on Categories of Subobjects,,false,"Let 𝐂 be a metacategory.

Let C and D be objects of 𝐂.

Let 𝐒𝐮𝐛_𝐂(C) and 𝐒𝐮𝐛_𝐂(D) be the associated categories of subobjects.


Let g: C → D be a monomorphism of 𝐂.

Then g defines a composition functor g_* : 𝐒𝐮𝐛_𝐂(C) →𝐒𝐮𝐛_𝐂(D):

 
 
 
 


That it is in fact a functor is shown on Composition Functor on Categories of Subobjects is Functor.


The effect of g_* is captured in the following commutative diagram:

::<-3em,0em>*+X = ""X"",
<3em,0em>*+X' = ""X2"",
<0em,-4em>*+C = ""C"",
<0em,-8em>*+D = ""D"",

""X"";""X2"" **@- ?>*@> ?*!/_1em/a,
""X"";""C"" **@- ?>*@> ?<>(.4)*!/^.6em/f,
""X2"";""C"" **@- ?>*@> ?<>(.4)*!/_.6em/f',
""C"";""D"" **@- ?>*@> ?<>(.4)*!/^.6em/g,

""X"";""D"" **<-5em,-4em> ?>*@> ?*!/^1.6em/g_* f = 
 g ∘ f,
""X2"";""D"" **<5em,-4em> ?>*@> ?*!/_1.6em/g_* f' = 
 g ∘ f',",Composition
['Definitions/Combinatorics'],Definition:Composition,"A $k$-composition of a (strictly) positive integer $n in mathbb Z_{> 0}$ is an ordered $k$-tuple:
:$c = left( c_1, c_2, ldots, c_k right)$
such that:
:$(1): quad c_1 + c_2 + cdots + c_k = n$
:$(2): quad forall i in left[ 1 ,.,.,   right]k: c_i in mathbb Z_{>0}$, that is, all the $c_i$ are strictly positive integers.

Category:Definitions/Combinatorics",Definition:Composition (Combinatorics),,false,"A k-composition of a (strictly) positive integer n ∈ℤ_> 0 is an ordered k-tuple:
:c = ( c_1, c_2, …, c_k )
such that:
:(1):    c_1 + c_2 + ⋯ + c_k = n
:(2):   ∀ i ∈[ 1  . . ]k: c_i ∈ℤ_>0, that is, all the c_i are strictly positive integers.

Category:Definitions/Combinatorics",Composition
['Definitions/Euclidean Algebra'],Definition:Composition,"Let $R = a : b$ be a ratio.

Then the composition of $R$ is the ratio $a + b : b$.


 
: 
:Composition of a ratio means taking the antecedent together with the consequent as one in relation to the consequent by itself.
 
 

Category:Definitions/Euclidean Algebra",Definition:Composition of Ratio,,false,"Let R = a : b be a ratio.

Then the composition of R is the ratio a + b : b.


 
: 
:Composition of a ratio means taking the antecedent together with the consequent as one in relation to the consequent by itself.
 
 

Category:Definitions/Euclidean Algebra",Composition
['Definitions/Polygons'],Definition:Concave,"Let $P$ be a polygon.

$P$ is a concave polygon  if and only if :
:at least one internal angle of $P$ is greater than $180 ^circ$.",Definition:Concave Polygon,,false,"Let P be a polygon.

P is a concave polygon  if and only if :
:at least one internal angle of P is greater than 180 ^∘.",Concave
"['Definitions/Concave Polyhedra', 'Definitions/Polyhedra']",Definition:Concave,"Let $P$ be a polyhedron.

=== Definition 1 ===
Let $P$ be a polyhedron.

$P$ is a concave polyhedron  if and only if :
:at least one face of $P$ lies in a plane which intersects at least one other face.

=== Definition 2 ===
Let $P$ be a polyhedron.

$P$ is a concave polyhedron  if and only if :
:at least one face of $P$ lies in a plane such that $P$ does not lie completely on one side of that plane.",Definition:Concave Polyhedron,,false,"Let P be a polyhedron.

=== Definition 1 ===
Let P be a polyhedron.

P is a concave polyhedron  if and only if :
:at least one face of P lies in a plane which intersects at least one other face.

=== Definition 2 ===
Let P be a polyhedron.

P is a concave polyhedron  if and only if :
:at least one face of P lies in a plane such that P does not lie completely on one side of that plane.",Concave
"['Definitions/Concave Real Functions', 'Definitions/Real Functions', 'Definitions/Real Analysis']",Definition:Concave,"Let $f$ be a real function which is defined on a real interval $I$.

=== Definition 1 ===
Let $f$ be a real function which is defined on a real interval $I$.


$f$ is concave on $I$  if and only if :

:$forall x, y in I: forall alpha, beta in mathbb R_{>0}, alpha + beta = 1: f left(   right){alpha x + beta y} ge alpha f left(   right)x + beta f left(   right)y$


:

=== Strictly Concave ===
Let $f$ be a real function which is defined on a real interval $I$.


$f$ is strictly concave on $I$  if and only if :

:$forall x, y in I, x ne y: forall alpha, beta in mathbb R_{>0}, alpha + beta = 1: f left({alpha x + beta y}right) > alpha f left({x}right) + beta f left({y}right)$


:


The geometric interpretation is that any point on the chord drawn on the graph of any strictly concave function always lies below the graph.

=== Definition 2 ===
Let $f$ be a real function which is defined on a real interval $I$.


$f$ is concave on $I$  if and only if :

:$ds forall x_1, x_2, x_3 in I: x_1 < x_2< x_3: frac {f left(   right){x_2} - f left(   right){x_1} } {x_2 - x_1} ge frac {f left(   right){x_3} - f left(   right){x_2} } {x_3 - x_2}$


Hence a geometrical interpretation: the slope of $P_1 P_2$ is greater than that of $P_2 P_3$:


:


=== Strictly Concave ===
Let $f$ be a real function which is defined on a real interval $I$.


$f$ is strictly concave on $I$  if and only if :

:$forall x_1, x_2, x_3 in I: x_1 < x_2 < x_3: dfrac {f left({x_2}right) - f left({x_1}right)} {x_2 - x_1} > dfrac {f left({x_3}right) - f left({x_2}right)} {x_3 - x_2}$


Hence a geometrical interpretation: the slope of $P_1 P_2$ is greater than that of $P_2 P_3$:


:

=== Definition 3 ===
Let $f$ be a real function which is defined on a real interval $I$.


$f$ is concave on $I$  if and only if :

:$forall x_1, x_2, x_3 in I: x_1 < x_2< x_3: dfrac {f left(   right){x_2} - f left(   right){x_1} } {x_2 - x_1} ge dfrac {f left(   right){x_3} - f left(   right){x_1} } {x_3 - x_1}$


Hence a geometrical interpretation: the slope of $P_1 P_2$ is greater than that of $P_1 P_3$:


:


=== Strictly Concave ===
Let $f$ be a real function which is defined on a real interval $I$.


$f$ is strictly concave on $I$  if and only if :

:$forall x_1, x_2, x_3 in I: x_1 < x_2 < x_3: dfrac {f left({x_2}right) - f left({x_1}right)} {x_2 - x_1} > dfrac {f left({x_3}right) - f left({x_1}right)} {x_3 - x_1}$


Hence a geometrical interpretation: the slope of $P_1 P_2$ is greater than that of $P_1 P_3$:


:",Definition:Concave Real Function,,false,"Let f be a real function which is defined on a real interval I.

=== Definition 1 ===
Let f be a real function which is defined on a real interval I.


f is concave on I  if and only if :

:∀ x, y ∈ I: ∀α, β∈ℝ_>0, α + β = 1: f (   )α x + β y≥α f (   )x + β f (   )y


:

=== Strictly Concave ===
Let f be a real function which is defined on a real interval I.


f is strictly concave on I  if and only if :

:∀ x, y ∈ I, x  y: ∀α, β∈ℝ_>0, α + β = 1: f (α x + β y) > α f (x) + β f (y)


:


The geometric interpretation is that any point on the chord drawn on the graph of any strictly concave function always lies below the graph.

=== Definition 2 ===
Let f be a real function which is defined on a real interval I.


f is concave on I  if and only if :

:∀ x_1, x_2, x_3 ∈ I: x_1 < x_2< x_3: f (   )x_2 - f (   )x_1/x_2 - x_1≥f (   )x_3 - f (   )x_2/x_3 - x_2


Hence a geometrical interpretation: the slope of P_1 P_2 is greater than that of P_2 P_3:


:


=== Strictly Concave ===
Let f be a real function which is defined on a real interval I.


f is strictly concave on I  if and only if :

:∀ x_1, x_2, x_3 ∈ I: x_1 < x_2 < x_3: f (x_2) - f (x_1)x_2 - x_1 > f (x_3) - f (x_2)x_3 - x_2


Hence a geometrical interpretation: the slope of P_1 P_2 is greater than that of P_2 P_3:


:

=== Definition 3 ===
Let f be a real function which is defined on a real interval I.


f is concave on I  if and only if :

:∀ x_1, x_2, x_3 ∈ I: x_1 < x_2< x_3: f (   )x_2 - f (   )x_1x_2 - x_1≥f (   )x_3 - f (   )x_1x_3 - x_1


Hence a geometrical interpretation: the slope of P_1 P_2 is greater than that of P_1 P_3:


:


=== Strictly Concave ===
Let f be a real function which is defined on a real interval I.


f is strictly concave on I  if and only if :

:∀ x_1, x_2, x_3 ∈ I: x_1 < x_2 < x_3: f (x_2) - f (x_1)x_2 - x_1 > f (x_3) - f (x_1)x_3 - x_1


Hence a geometrical interpretation: the slope of P_1 P_2 is greater than that of P_1 P_3:


:",Concave
['Definitions/Cones'],Definition:Cone,"A cone is a three-dimensional geometric figure which consists of the set of all straight lines joining the boundary of a plane figure $PQR$ to a point $A$ not in the same plane of $PQR$:


:",Definition:Cone (Geometry),figure,true,"A cone is a three-dimensional geometric figure which consists of the set of all straight lines joining the boundary of a plane figure PQR to a point A not in the same plane of PQR:


:",Cone
['Definitions/Category Theory'],Definition:Cone,"Let $mathbf C$ be a metacategory.

Let $D: mathbf J to mathbf C$ be a $mathbf J$-diagram in $mathbf C$.


A cone to $D$ comprises an object $C$ of $mathbf C$, and a morphism:

:$c_j: C to D_j$

for each object of $mathbf J$, such that for each morphism $alpha: i to j$ of $mathbf J$:

::$begin{xy}xymatrix@+0.5em@L+2px{
 C
  ar[d]_*+{c_i}
  ar[dr]^*+{c_j}

\
 D_i
  ar[r]_*+{D_alpha}
&
 D_j
}end{xy}$

is a commutative diagram.

 ",Definition:Cone (Category Theory),,false,"Let 𝐂 be a metacategory.

Let D: 𝐉→𝐂 be a 𝐉-diagram in 𝐂.


A cone to D comprises an object C of 𝐂, and a morphism:

:c_j: C → D_j

for each object of 𝐉, such that for each morphism α: i → j of 𝐉:

::@+0.5em@L+2px
 C
  [d]_*+c_i[dr]^*+c_j

 D_i
  [r]_*+D_α   
 D_j

is a commutative diagram.

 ",Cone
"['Definitions/Congruence (Geometry)', 'Definitions/Geometry']",Definition:Congruence,"In the field of Euclidean geometry, two geometric figures are congruent  if and only if :

:they are, informally speaking, both ""the same size and shape""

:they differ only in position in space

:one figure can be overlaid on the other figure with a series of rotations, translations, and reflections.


Specifically:
:all corresponding angles of the congruent figures must have the same measurement
:all corresponding sides of the congruent figures must be be the same length.",Definition:Congruence (Geometry),,false,"In the field of Euclidean geometry, two geometric figures are congruent  if and only if :

:they are, informally speaking, both ""the same size and shape""

:they differ only in position in space

:one figure can be overlaid on the other figure with a series of rotations, translations, and reflections.


Specifically:
:all corresponding angles of the congruent figures must have the same measurement
:all corresponding sides of the congruent figures must be be the same length.",Congruence
['Definitions/Metric Spaces'],Definition:Congruence,"Let $left( X, d right)$ be a metric space.

Two subsets $A, B subseteq X$ of $X$ are said to be congruent  if and only if  there exists an isometry $f: X to X$ such that $f^to left(   right)A = B$.

Such an isometry is called a congruence.",Definition:Congruence (Metric Spaces),,false,"Let ( X, d ) be a metric space.

Two subsets A, B ⊆ X of X are said to be congruent  if and only if  there exists an isometry f: X → X such that f^→(   )A = B.

Such an isometry is called a congruence.",Congruence
['Definitions/Ideal Theory'],Definition:Congruence,"Let $left( R, +, circ right)$ be a ring, and let $J$ be an ideal of $R$.


The notation:

:$a equiv b pmod J$

is used to mean:

:$a + left( -b right) in J$",Definition:Congruence Modulo an Ideal,,false,"Let ( R, +, ∘) be a ring, and let J be an ideal of R.


The notation:

:a ≡ b  J

is used to mean:

:a + ( -b ) ∈ J",Congruence
"['Definitions/Abstract Algebra', 'Definitions/Equivalence Relations']",Definition:Congruence,"Let $left( S, circ right)$ be an algebraic structure.

Let $mathcal R$ be an equivalence relation on $S$.


Then $mathcal R$ is a congruence relation for $circ$  if and only if :

:$forall x_1, x_2, y_1, y_2 in S: left( x_1 mathrel mathcal R x_2 right) land left( y_1 mathrel mathcal R y_2 right) implies left( x_1 circ y_1 right) mathrel mathcal R left( x_2 circ y_2 right)$",Definition:Congruence Relation,,false,"Let ( S, ∘) be an algebraic structure.

Let ℛ be an equivalence relation on S.


Then ℛ is a congruence relation for ∘  if and only if :

:∀ x_1, x_2, y_1, y_2 ∈ S: ( x_1 ℛ x_2 ) ( y_1 ℛ y_2 ) ( x_1 ∘ y_1 ) ℛ( x_2 ∘ y_2 )",Congruence
"['Definitions/Congruence (Number Theory)', 'Definitions/Modulo Arithmetic', 'Definitions/Number Theory']",Definition:Congruence,"Let $z in mathbb R$.


=== Definition by Remainder after Division ===
Let $z in mathbb R$.


We define a relation $mathcal R_z$ on the set of all $x, y in mathbb R$:
:$mathcal R_z := leftlbrace left( x, y right) in mathbb R times mathbb R: exists k in mathbb Z: x = y + k z rightrbrace$


This relation is called congruence modulo $z$, and the real number $z$ is called the modulus.


When $left( x, y right) in mathcal R_z$, we write:
:$x equiv y pmod z$
and say:
:$x$ is congruent to $y$ modulo $z$.


Similarly, when $left( x, y right) notin mathcal R_z$, we write:
:$x not equiv y pmod z$
and say:
:$x$ is not congruent (or incongruent) to $y$ modulo $z$.

=== Definition by Modulo Operation ===
Let $z in mathbb R$.

Let $bmod$ be defined as the modulo operation:

:$x bmod y := begin {cases} x - y leftlfloor dfrac x y rightrfloor & : y ne 0 \ x & : y = 0 end {cases}$


Then congruence modulo $z$ is the relation on $mathbb R$ defined as:
:$forall x, y in mathbb R: x equiv y pmod z iff x bmod z = y bmod z$ 


The real number $z$ is called the modulus.

=== Definition by Integer Multiple ===
Let $z in mathbb R$.

Let $x, y in mathbb R$.


Then $x$ is congruent to $y$ modulo $z$  if and only if  their difference is an integer multiple of $z$:
:$x equiv y pmod z iff exists k in mathbb Z: x - y = k z$",Definition:Congruence (Number Theory),,false,"Let z ∈ℝ.


=== Definition by Remainder after Division ===
Let z ∈ℝ.


We define a relation ℛ_z on the set of all x, y ∈ℝ:
:ℛ_z := {( x, y ) ∈ℝ×ℝ: ∃ k ∈ℤ: x = y + k z }


This relation is called congruence modulo z, and the real number z is called the modulus.


When ( x, y ) ∈ℛ_z, we write:
:x ≡ y  z
and say:
:x is congruent to y modulo z.


Similarly, when ( x, y ) ∉ℛ_z, we write:
:x ≢y  z
and say:
:x is not congruent (or incongruent) to y modulo z.

=== Definition by Modulo Operation ===
Let z ∈ℝ.

Let  be defined as the modulo operation:

:x  y :=  x - y ⌊ x y ⌋    : y  0 
 x     : y = 0


Then congruence modulo z is the relation on ℝ defined as:
:∀ x, y ∈ℝ: x ≡ y  z  x  z = y  z 


The real number z is called the modulus.

=== Definition by Integer Multiple ===
Let z ∈ℝ.

Let x, y ∈ℝ.


Then x is congruent to y modulo z  if and only if  their difference is an integer multiple of z:
:x ≡ y  z ∃ k ∈ℤ: x - y = k z",Congruence
"['Definitions/Matrix Congruence', 'Definitions/Matrix Equivalence', 'Definitions/Matrix Algebra', 'Definitions/Linear Algebra']",Definition:Congruence,"Let $R$ be a commutative ring with unity.

Let $n$ be a positive integer.

Let $mathbf A$ and $mathbf B$ be square matrices of order $n$ over $R$.


Then:
:$mathbf A$ and $mathbf B$ are congruent
 if and only if :
:there exists an invertible matrix $mathbf P in R^{n times n}$ such that $mathbf B = mathbf P^intercal mathbf A mathbf P$
where $mathbf P^intercal$ denotes the transpose of $mathbf P$.",Definition:Matrix Congruence,,false,"Let R be a commutative ring with unity.

Let n be a positive integer.

Let 𝐀 and 𝐁 be square matrices of order n over R.


Then:
:𝐀 and 𝐁 are congruent
 if and only if :
:there exists an invertible matrix 𝐏∈ R^n × n such that 𝐁 = 𝐏^⊺𝐀𝐏
where 𝐏^⊺ denotes the transpose of 𝐏.",Congruence
"['Definitions/Conjugate Angles', 'Definitions/Angles']",Definition:Conjugate,"The conjugate of an angle $theta$ is the angle $phi$ such that:
:$theta + phi = 2 pi$
where $theta$ and $pi$ are expressed in radians.

That is, it is the angle that makes the given angle equal to a full angle.


Equivalently, the conjugate of an angle $theta$ is the angle $phi$ such that:
:$theta + phi = 360 ^circ$
where $theta$ and $pi$ are expressed in degrees.


Thus, conjugate angles are two angles whose measures add up to the measure of $4$ right angles.

That is, their measurements add up to $360$ degrees or $2 pi$ radians.",Definition:Conjugate Angles,,false,"The conjugate of an angle θ is the angle ϕ such that:
:θ + ϕ = 2 π
where θ and π are expressed in radians.

That is, it is the angle that makes the given angle equal to a full angle.


Equivalently, the conjugate of an angle θ is the angle ϕ such that:
:θ + ϕ = 360 ^∘
where θ and π are expressed in degrees.


Thus, conjugate angles are two angles whose measures add up to the measure of 4 right angles.

That is, their measurements add up to 360 degrees or 2 π radians.",Conjugate
['Definitions/Hyperbolas'],Definition:Conjugate," 

:


Consider a hyperbola $K$ whose foci are $F_1$ and $F_2$.


Let $PQ$ and $RS$ be line segments constructed through the vertices of $K$ parallel to the minor axis of $K$ and intersecting the asymptotes of $K$ at $P$, $Q$, $R$ and $S$ as above.

Construct the line segments $PR$ and $QS$.

Let $C_1$ and $C_2$ be the points of intersection of $PR$ and $QS$ with the minor axis of $K$.


The conjugate axis of $K$ is the line segment $C_1 C_2$.",Definition:Hyperbola/Conjugate Axis,,false," 

:


Consider a hyperbola K whose foci are F_1 and F_2.


Let PQ and RS be line segments constructed through the vertices of K parallel to the minor axis of K and intersecting the asymptotes of K at P, Q, R and S as above.

Construct the line segments PR and QS.

Let C_1 and C_2 be the points of intersection of PR and QS with the minor axis of K.


The conjugate axis of K is the line segment C_1 C_2.",Conjugate
"['Definitions/Conjugate Points', 'Definitions/Polars of Points']",Definition:Conjugate,"Let $mathcal K$ be a conic section.

Let $P$ and $Q$ be points in the plane of $mathcal K$.

Let:
:$P$ lie on the polar of $Q$
:$Q$ lie on the polar of $P$.


$P$ and $Q$ are known as conjugate points with respect to $mathcal K$.


=== Conjugate Points with respect to Circle ===
Let $mathcal C$ be a circle.

Let $P$ and $Q$ be points in the plane of $mathcal C$.

Let:
:$P$ lie on the polar of $Q$
:$Q$ lie on the polar of $P$.


$P$ and $Q$ are known as conjugate points with respect to $mathcal C$.",Definition:Conjugate Points (Geometry),,false,"Let 𝒦 be a conic section.

Let P and Q be points in the plane of 𝒦.

Let:
:P lie on the polar of Q
:Q lie on the polar of P.


P and Q are known as conjugate points with respect to 𝒦.


=== Conjugate Points with respect to Circle ===
Let 𝒞 be a circle.

Let P and Q be points in the plane of 𝒞.

Let:
:P lie on the polar of Q
:Q lie on the polar of P.


P and Q are known as conjugate points with respect to 𝒞.",Conjugate
"['Definitions/Conjugate Lines', 'Definitions/Polars of Points', 'Definitions/Conic Sections']",Definition:Conjugate,"Let $mathcal C$ be a circle.

Let $mathcal P$ and $mathcal Q$ be the straight lines in the plane of $mathcal C$.


Let $P$ and $Q$ be the poles of $mathcal P$ and $mathcal Q$ with respect to $mathcal C$ respectively.

Let $P$ and $Q$ be such that $P$ lies on $mathcal Q$ and $Q$ lies on $mathcal P$.

Then $mathcal P$ and $mathcal Q$ are known as conjugate lines with respect to $mathcal C$.",Definition:Conjugate Lines,,false,"Let 𝒞 be a circle.

Let 𝒫 and 𝒬 be the straight lines in the plane of 𝒞.


Let P and Q be the poles of 𝒫 and 𝒬 with respect to 𝒞 respectively.

Let P and Q be such that P lies on 𝒬 and Q lies on 𝒫.

Then 𝒫 and 𝒬 are known as conjugate lines with respect to 𝒞.",Conjugate
"['Definitions/Conjugate Diameters', 'Definitions/Conic Sections']",Definition:Conjugate,"Let $K$ be a conic section.

Let $D_1$ and $D_2$ be diameters of $K$ such that:
:$D_1$ belongs to the system of parallel chords whose midpoints define $D_2$
and:
:$D_2$ belongs to the system of parallel chords whose midpoints define $D_1$.

Then $D_1$ and $D_2$ are known as conjugate diameters.",Definition:Conjugate Diameters,,false,"Let K be a conic section.

Let D_1 and D_2 be diameters of K such that:
:D_1 belongs to the system of parallel chords whose midpoints define D_2
and:
:D_2 belongs to the system of parallel chords whose midpoints define D_1.

Then D_1 and D_2 are known as conjugate diameters.",Conjugate
['Definitions/Conjugations on Algebras'],Definition:Conjugate,"Let $A = left( A_F, oplus right)$ be an algebra over a field $F$.

Let $C: A_F to A_F$ be a conjugation on $A$.

Let $a in A$.


Then $C left(   right)a$ is called the conjugate of $a$.",Definition:Conjugation on Algebra/Conjugate,,false,"Let A = ( A_F, ⊕) be an algebra over a field F.

Let C: A_F → A_F be a conjugation on A.

Let a ∈ A.


Then C (   )a is called the conjugate of a.",Conjugate
"['Definitions/Conjugacy', 'Definitions/Group Theory']",Definition:Conjugate,"Let $left({G, circ}right)$ be a group.

=== Conjugate of an Element ===
Let $left( G, circ right)$ be a group.


=== Definition 1 ===
Let $left( G, circ right)$ be a group.


The  conjugacy relation $sim$ is defined on $G$ as:
:$forall left( x, y right) in G times G: x sim y iff exists a in G: a circ x = y circ a$


This can be voiced as:
:$x$ is the conjugate of $y$ (by $a$ in $G$)
or:
:$x$ is conjugate to $y$ (by $a$ in $G$)

=== Definition 2 ===
Let $left( G, circ right)$ be a group.


The  conjugacy relation $sim$ is defined on $G$ as:
:$forall left( x, y right) in G times G: x sim y iff exists a in G: a circ x circ a^{-1} = y$


This can be voiced as:
:$x$ is the conjugate of $y$ (by $a$ in $G$)
or:
:$x$ is conjugate to $y$ (by $a$ in $G$)

This can be voiced as:
:$x$ is the conjugate of $y$ (by $a$ in $G$)
or:
:$x$ is conjugate to $y$ (by $a$ in $G$)

=== Conjugate of a Set ===
Let $left( G, circ right)$ be a group.

Let $S subseteq G, a in G$.

Then the $G$-conjugate of $S$ by $a$ is:

:$S^a := leftlbrace y in G: exists x in S: y = a circ x circ a^{-1}  rightrbrace = a circ S circ a^{-1}$


That is, $S^a$ is the set of all elements of $G$ that are the conjugates of elements of $S$ by $a$.


When $G$ is the only group under consideration, we usually just refer to the conjugate of $S$ by $a$.

Category:Definitions/Conjugacy
Category:Definitions/Group Theory",Definition:Conjugate (Group Theory),,false,"Let (G, ∘) be a group.

=== Conjugate of an Element ===
Let ( G, ∘) be a group.


=== Definition 1 ===
Let ( G, ∘) be a group.


The  conjugacy relation ∼ is defined on G as:
:∀( x, y ) ∈ G × G: x ∼ y ∃ a ∈ G: a ∘ x = y ∘ a


This can be voiced as:
:x is the conjugate of y (by a in G)
or:
:x is conjugate to y (by a in G)

=== Definition 2 ===
Let ( G, ∘) be a group.


The  conjugacy relation ∼ is defined on G as:
:∀( x, y ) ∈ G × G: x ∼ y ∃ a ∈ G: a ∘ x ∘ a^-1 = y


This can be voiced as:
:x is the conjugate of y (by a in G)
or:
:x is conjugate to y (by a in G)

This can be voiced as:
:x is the conjugate of y (by a in G)
or:
:x is conjugate to y (by a in G)

=== Conjugate of a Set ===
Let ( G, ∘) be a group.

Let S ⊆ G, a ∈ G.

Then the G-conjugate of S by a is:

:S^a := { y ∈ G: ∃ x ∈ S: y = a ∘ x ∘ a^-1} = a ∘ S ∘ a^-1


That is, S^a is the set of all elements of G that are the conjugates of elements of S by a.


When G is the only group under consideration, we usually just refer to the conjugate of S by a.

Category:Definitions/Conjugacy
Category:Definitions/Group Theory",Conjugate
"['Definitions/Generated Normal Subgroups', 'Definitions/Normal Subgroups', 'Definitions/Normality in Groups', 'Definitions/Generated Subgroups', 'Definitions/Generators of Groups', 'Definitions/Subgroups']",Definition:Conjugate,"Let $G$ be a group.

Let $S subseteq G$ be a subset.


=== Definition 1 ===
Let $G$ be a group. 

Let $S subseteq G$ be a subset.


The normal subgroup generated by $S$, denoted ${leftlangle S^G rightrangle}$,  is the intersection of all normal subgroups of $G$ containing $S$.

=== Definition 2 ===
 

Let $G$ be a group. 

Let $S subseteq G$ be a subset.


The normal subgroup generated by $S$, denoted ${leftlangle S^G rightrangle}$,  is the smallest normal subgroup of $G$ containing $S$:
:${leftlangle S^G rightrangle} = {leftlangle x S x^{-1}: x in G rightrangle}$",Definition:Generated Normal Subgroup,,false,"Let G be a group.

Let S ⊆ G be a subset.


=== Definition 1 ===
Let G be a group. 

Let S ⊆ G be a subset.


The normal subgroup generated by S, denoted ⟨ S^G ⟩,  is the intersection of all normal subgroups of G containing S.

=== Definition 2 ===
 

Let G be a group. 

Let S ⊆ G be a subset.


The normal subgroup generated by S, denoted ⟨ S^G ⟩,  is the smallest normal subgroup of G containing S:
:⟨ S^G ⟩ = ⟨ x S x^-1: x ∈ G ⟩",Conjugate
['Definitions/Quadratic Irrationals'],Definition:Conjugate,"Let $alpha = r + s sqrt n$ be a quadratic irrational.


Then its conjugate is defined as:
:$tilde alpha = r - s sqrt n$


Thus $alpha$ and $tilde alpha$ are known as conjugate quadratic irrationals.


Notation may vary.",Definition:Conjugate of Quadratic Irrational,,false,"Let α = r + s √(n) be a quadratic irrational.


Then its conjugate is defined as:
:α̃= r - s √(n)


Thus α and α̃ are known as conjugate quadratic irrationals.


Notation may vary.",Conjugate
"['Definitions/Complex Conjugates', 'Definitions/Complex Numbers', 'Definitions/Complex Analysis']",Definition:Conjugate,"Let $z = a + i b$ be a complex number.


Then the (complex) conjugate of $z$ is denoted $overline z$ and is defined as:

:$overline z := a - i b$


That is, you get the complex conjugate of a complex number by negating its imaginary part.


=== Complex Conjugation ===
The operation of complex conjugation is the mapping:
: $overline cdot: mathbb C to mathbb C: z mapsto overline z$.
where $overline z$ is the complex conjugate of $z$.


That is, it maps a complex number to its complex conjugate.


Category:Definitions/Complex Conjugates",Definition:Complex Conjugate,,false,"Let z = a + i b be a complex number.


Then the (complex) conjugate of z is denoted z and is defined as:

:z := a - i b


That is, you get the complex conjugate of a complex number by negating its imaginary part.


=== Complex Conjugation ===
The operation of complex conjugation is the mapping:
: ·: ℂ→ℂ: z ↦z.
where z is the complex conjugate of z.


That is, it maps a complex number to its complex conjugate.


Category:Definitions/Complex Conjugates",Conjugate
['Definitions/Quaternions'],Definition:Conjugate,"Let $mathbf x = a mathbf 1 + b mathbf i + c mathbf j + d mathbf k$ be a quaternion.


The conjugate quaternion of $mathbf x$ is defined as:
:$overline {mathbf x} = a mathbf 1 - b mathbf i - c mathbf j - d mathbf k$.


=== Matrix Form ===
Let $mathbf x$ be a quaternion defined in matrix form as:
:$mathbf x = begin{bmatrix} a + bi & c + di \ -c + di & a - bi end{bmatrix}$


The conjugate quaternion of $mathbf x$ is defined as:
:$overline {mathbf x} = begin{bmatrix} a - bi & -c - di \ c - di & a + bi end{bmatrix}$


That is, if:
:$mathbf x = begin{bmatrix} p & q \ -overline q & overline p end{bmatrix}$

then:
:$overline {mathbf x} = begin{bmatrix} overline p & -q \ overline q & p end{bmatrix}$

Category:Definitions/Quaternions

=== Ordered Pair of Complex Numbers ===
Let $mathbf x$ be a quaternion defined as an ordered pair $left({a, b}right)$  of complex numbers.


The conjugate quaternion of $mathbf x$ is defined as:
:$overline {mathbf x} = overline {left({a, b}right)} = left({overline a, -b}right)$",Definition:Conjugate Quaternion,,false,"Let 𝐱 = a 1 + b 𝐢 + c 𝐣 + d 𝐤 be a quaternion.


The conjugate quaternion of 𝐱 is defined as:
:𝐱 = a 1 - b 𝐢 - c 𝐣 - d 𝐤.


=== Matrix Form ===
Let 𝐱 be a quaternion defined in matrix form as:
:𝐱 = [  a + bi  c + di; -c + di  a - bi ]


The conjugate quaternion of 𝐱 is defined as:
:𝐱 = [  a - bi -c - di;  c - di  a + bi ]


That is, if:
:𝐱 = [  p  q; -q  p ]

then:
:𝐱 = [  p -q;  q  p ]

Category:Definitions/Quaternions

=== Ordered Pair of Complex Numbers ===
Let 𝐱 be a quaternion defined as an ordered pair (a, b)  of complex numbers.


The conjugate quaternion of 𝐱 is defined as:
:𝐱 = (a, b) = (a, -b)",Conjugate
['Definitions/Calculus of Variations'],Definition:Conjugate,"=== Definition 1 ===
Let:
:$-dfrac mathrm d {mathrm d x}  left(   right){P h'} + Q h = 0$

with boundary conditions:
:$h left(   right)a = 0, quad h left(   right)c = 0, quad a < c le b$

Suppose:
:$h left(   right)x = 0 quad neg forall x in left[ a ,.,.,   right]b$

Suppose:
:$h left(   right)a = 0, quad h left(   right){tilde a} = 0, quad a ne tilde a$


Then the point $tilde a$ is called conjugate to the point $a$   solution to the aforementioned differential equation.

 

=== Definition 2 ===
Let $y = y left(   right)x$ and $y^* = y^* left(   right)x$ be extremal functions.

Let:

:$M = left( a, y left(   right)a right)$

:$tilde M = left( tilde a, y left(   right){tilde a}  right)$

Let $y$ and $y^*$ both pass through the point $M$.

Let:

:$y^* left(   right){x - tilde a} - y left(   right){x - tilde a} = epsilon leftlvert y^* left(   right){x - tilde a} - y left(   right){x - tilde a}  rightrvert_1$

where:

:$leftlvert y^* left(   right){x - tilde a} - y left(   right){x - tilde a}  rightrvert_1 to 0 implies epsilon to 0$


Then $tilde M$ is conjugate to $M$.

 

=== Definition 3 ===
Let $y = y left(   right)x$ and $y = tilde y left(   right)x$ be extremal functions.

Let:

:$M = left( a, y left(   right)a right)$

:$tilde M = left( tilde a, y left(   right){tilde a}  right)$

Let both $y = y left(   right)x$ and $y = tilde y left(   right)x$ pass through the point $M$.

Let 

:$ds lim_{leftlVert y left(   right)x - tilde y left(   right)x rightrVert_{1, infty} to 0} left[ left( x, y left(   right)x right): y left(   right)x - tilde y left(   right)x = 0 right] = tilde M$

In other words, let $tilde M$ be the limit points of intersection of $y = y left(   right)x$ and $y = tilde y left(   right)x$ as $leftlVert y left(   right)x - tilde y left(   right)x rightrVert_{1, infty} to 0$.

 


Then $tilde M$ is conjugate to $M$.

=== Dependent on $N$ Functions ===
Let $K$ be a functional such that:

:$ds K left[ h right] = int_a^b left( mathbf h'mathbf P mathbf h' + mathbf h mathbf Q mathbf h right) ,mathrm d x$

Consider Euler's equation related to the functional $K$:

:$-dfrac mathrm d {mathrm d x}  left(   right){mathbf P mathbf h'} + mathbf Q mathbf h = 0$

where $mathbf P$ and $mathbf Q$ are symmetric matrices.

Let the general solution to this equation be:

:$leftlbrace mathbf h^{left( i right)} = left( leftlangle h_{ij}  rightrangle  right): i,j in mathbb N_{le N}  rightrbrace$

Let:

:$exists j: forall k ne j: left( mathbf h^{left( j right)}  left(   right)a = 0 right) land left( h_{j j}' left(   right)a = 1, h'_{j k} = 0 right)$

Let the determinant, built from $h_{ij}$, be such that:

:$leftlvert h_{i j}  rightrvert left( tilde a right) = 0$

Here $i$ denotes rows, and $j$ denotes columns.


Then $tilde a$ is said to be conjugate to point $a$   the functional $K$.

=== With Respect to Original Functional ===
Let:

:$ds int_a^b F left(   right){x, y, y'}$

be the original functional.

Let $tilde a$ be conjugate to $a$.

Let:
:$ds int_a^b left( P h'^2 + Q h^2 right) ,mathrm d x$

be the second variation of $ds int_a^b F left(   right){x, y, y'}$.


Then $tilde a$ is conjugate to $a$   to the original functional $ds int_a^b F left(   right){x, y, y'}$.

 ",Definition:Conjugate Point (Calculus of Variations),,false,"=== Definition 1 ===
Let:
:-dd x(   )P h' + Q h = 0

with boundary conditions:
:h (   )a = 0,    h (   )c = 0,    a < c ≤ b

Suppose:
:h (   )x = 0   ∀ x ∈[ a  . . ]b

Suppose:
:h (   )a = 0,    h (   )ã = 0,    a ã


Then the point ã is called conjugate to the point a   solution to the aforementioned differential equation.

 

=== Definition 2 ===
Let y = y (   )x and y^* = y^* (   )x be extremal functions.

Let:

:M = ( a, y (   )a )

:M̃ = ( ã, y (   )ã)

Let y and y^* both pass through the point M.

Let:

:y^* (   )x - ã - y (   )x - ã = ϵ| y^* (   )x - ã - y (   )x - ã|_1

where:

:| y^* (   )x - ã - y (   )x - ã|_1 → 0 ϵ→ 0


Then M̃ is conjugate to M.

 

=== Definition 3 ===
Let y = y (   )x and y = ỹ(   )x be extremal functions.

Let:

:M = ( a, y (   )a )

:M̃ = ( ã, y (   )ã)

Let both y = y (   )x and y = ỹ(   )x pass through the point M.

Let 

:lim_‖ y (   )x - ỹ(   )x ‖_1, ∞→ 0[ ( x, y (   )x ): y (   )x - ỹ(   )x = 0 ] = M̃

In other words, let M̃ be the limit points of intersection of y = y (   )x and y = ỹ(   )x as ‖ y (   )x - ỹ(   )x ‖_1, ∞→ 0.

 


Then M̃ is conjugate to M.

=== Dependent on N Functions ===
Let K be a functional such that:

:K [ h ] = ∫_a^b ( 𝐡'𝐏𝐡' + 𝐡𝐐𝐡)  d x

Consider Euler's equation related to the functional K:

:-dd x(   )𝐏𝐡' + 𝐐𝐡 = 0

where 𝐏 and 𝐐 are symmetric matrices.

Let the general solution to this equation be:

:{𝐡^( i ) = ( ⟨ h_ij⟩): i,j ∈ℕ_≤ N}

Let:

:∃ j: ∀ k  j: ( 𝐡^( j )(   )a = 0 ) ( h_j j' (   )a = 1, h'_j k = 0 )

Let the determinant, built from h_ij, be such that:

:| h_i j|( ã) = 0

Here i denotes rows, and j denotes columns.


Then ã is said to be conjugate to point a   the functional K.

=== With Respect to Original Functional ===
Let:

:∫_a^b F (   )x, y, y'

be the original functional.

Let ã be conjugate to a.

Let:
:∫_a^b ( P h'^2 + Q h^2 )  d x

be the second variation of ∫_a^b F (   )x, y, y'.


Then ã is conjugate to a   to the original functional ∫_a^b F (   )x, y, y'.

 ",Conjugate
['Definitions/Complex Conjugates'],Definition:Conjugate Pair,"Let $z in mathbb C$ be a complex number.

Let $overline z$ be the complex conjugate of $z$.


Then $z$ and $overline z$ are a conjugate pair.",Definition:Complex Conjugate/Conjugate Pair,,false,"Let z ∈ℂ be a complex number.

Let z be the complex conjugate of z.


Then z and z are a conjugate pair.",Conjugate Pair
"['Definitions/Harmonic Conjugates', 'Definitions/Harmonic Ranges']",Definition:Conjugate Pair,"=== Harmonic Range ===
Let $AB$ and $PQ$ be line segments on a straight line such that $left( AB, PQ right)$ is a harmonic range.

Then $P$ and $Q$ are said to be harmonic conjugates with respect to $A$ and $B$.

=== Harmonic Pencil ===
Let $AB$ and $PQ$ be line segments on a straight line such that $left( AB, PQ right)$ is a harmonic range.

Let $O$ be a point which is not on the straight line $AB$.

Let $O left(   right){AB, PQ}$ be the harmonic pencil formed from $O$ and $left( AB, PQ right)$.


:


The rays $OP$ and $OQ$ are said to be harmonic conjugates with respect to $OA$ and $OB$.",Definition:Harmonic Conjugates,,false,"=== Harmonic Range ===
Let AB and PQ be line segments on a straight line such that ( AB, PQ ) is a harmonic range.

Then P and Q are said to be harmonic conjugates with respect to A and B.

=== Harmonic Pencil ===
Let AB and PQ be line segments on a straight line such that ( AB, PQ ) is a harmonic range.

Let O be a point which is not on the straight line AB.

Let O (   )AB, PQ be the harmonic pencil formed from O and ( AB, PQ ).


:


The rays OP and OQ are said to be harmonic conjugates with respect to OA and OB.",Conjugate Pair
"['Definitions/Connected Relations', 'Definitions/Relation Theory']",Definition:Connected,"Let $mathcal R subseteq S times S$ be a relation on a set $S$.


Then $mathcal R$ is connected  if and only if :
:$forall a, b in S: a ne b implies left( a, b right) in mathcal R lor left( b, a right) in mathcal R$


That is,  if and only if  every pair of distinct elements is comparable.",Definition:Connected Relation,,false,"Let ℛ⊆ S × S be a relation on a set S.


Then ℛ is connected  if and only if :
:∀ a, b ∈ S: a  b ( a, b ) ∈ℛ( b, a ) ∈ℛ


That is,  if and only if  every pair of distinct elements is comparable.",Connected
['Definitions/Connected Spaces'],Definition:Connected,"=== Topological Space ===
Let $T = left( S, tau right)$ be a non-empty topological space.


=== Definition 1 ===
Let $T = left( S, tau right)$ be a topological space.


$T$ is connected  if and only if  it admits no separation.


That is, $T$ is connected  if and only if  there exist no open sets $A, B in tau$ such that $A, B ne varnothing$, $A cup B = S$ and $A cap B = varnothing$.

=== Definition 2 ===
Let $T = left( S, tau right)$ be a topological space.


$T$ is connected  if and only if  it has no two disjoint nonempty closed sets whose union is $S$.

=== Definition 3 ===
Let $T = left( S, tau right)$ be a topological space.


$T$ is connected  if and only if  its only subsets whose boundary is empty are $S$ and $varnothing$.

=== Definition 4 ===
Let $T = left( S, tau right)$ be a topological space.


$T$ is connected  if and only if  its only clopen sets are $S$ and $varnothing$.

=== Definition 5 ===
Let $T = left( S, tau right)$ be a non-empty topological space.


$T$ is connected  if and only if  there are no two non-empty separated sets whose union is $S$.

=== Definition 6 ===

Let $T = left( S, tau right)$ be a topological space.


$T$ is connected  if and only if  there exists no continuous surjection from $T$ onto a discrete two-point space.

=== Definition 7 ===
Let $T = left( S, tau right)$ be a topological space.


$T$ is connected  if and only if :
:there do not exist disjoint, non-empty open sets $X$ and $Y$ of $T$ such that $X cup Y = S$.

=== Set of Topological Space ===
Let $T = left( S, tau right)$ be a topological space.

Let $H subseteq S$ be a non-empty subset of $S$.


=== Definition 1 ===

Let $T = left( S, tau right)$ be a topological space.

Let $H subseteq S$ be a non-empty subset of $S$.


$H$ is a connected set of $T$  if and only if  it is not the union of any two non-empty separated sets of $T$.


=== Definition 2 ===
Let $T = left( S, tau right)$ be a topological space.

Let $H subseteq S$ be a non-empty subset of $S$.


$H$ is a connected set of $T$  if and only if  it is not disconnected in $T$.

=== Definition 3 ===
Let $T = left( S, tau right)$ be a topological space.

Let $H subseteq S$ be a non-empty subset of $S$.


$H$ is a connected set of $T$  if and only if :
:the topological subspace $left( H, tau_H right)$ of $T$ is a connected topological space.

=== Definition 4 ===
Let $T = left( S, tau right)$ be a topological space.

Let $H subseteq S$ be a non-empty subset of $S$.


$H$ is a connected set of $T$  if and only if :
:there do not exist disjoint, non-empty subsets $X$ and $Y$ of $H$ such that $X cup Y = H$ such that:
::no limit point of $X$ is an element of $Y$
::no limit point of $Y$ is an element of $X$.

=== Definition 5 ===
Let $T = left( S, tau right)$ be a topological space.

Let $H subseteq S$ be a non-empty subset of $S$.


$H$ is a connected set of $T$  if and only if :
:$H$ cannot be partitioned into $2$ non-empty subsets so that each subset has no element in common with the closure of the other.

=== Points in Topological Space ===
Let $T = left( S, tau right)$ be a topological space.

Let $a, b in S$.


Then $a$ and $b$ are connected (in $T$)  if and only if  there exists a connected set in $T$ containing both $a$ and $b$.",Definition:Connected (Topology),,false,"=== Topological Space ===
Let T = ( S, τ) be a non-empty topological space.


=== Definition 1 ===
Let T = ( S, τ) be a topological space.


T is connected  if and only if  it admits no separation.


That is, T is connected  if and only if  there exist no open sets A, B ∈τ such that A, B ∅, A ∪ B = S and A ∩ B = ∅.

=== Definition 2 ===
Let T = ( S, τ) be a topological space.


T is connected  if and only if  it has no two disjoint nonempty closed sets whose union is S.

=== Definition 3 ===
Let T = ( S, τ) be a topological space.


T is connected  if and only if  its only subsets whose boundary is empty are S and ∅.

=== Definition 4 ===
Let T = ( S, τ) be a topological space.


T is connected  if and only if  its only clopen sets are S and ∅.

=== Definition 5 ===
Let T = ( S, τ) be a non-empty topological space.


T is connected  if and only if  there are no two non-empty separated sets whose union is S.

=== Definition 6 ===

Let T = ( S, τ) be a topological space.


T is connected  if and only if  there exists no continuous surjection from T onto a discrete two-point space.

=== Definition 7 ===
Let T = ( S, τ) be a topological space.


T is connected  if and only if :
:there do not exist disjoint, non-empty open sets X and Y of T such that X ∪ Y = S.

=== Set of Topological Space ===
Let T = ( S, τ) be a topological space.

Let H ⊆ S be a non-empty subset of S.


=== Definition 1 ===

Let T = ( S, τ) be a topological space.

Let H ⊆ S be a non-empty subset of S.


H is a connected set of T  if and only if  it is not the union of any two non-empty separated sets of T.


=== Definition 2 ===
Let T = ( S, τ) be a topological space.

Let H ⊆ S be a non-empty subset of S.


H is a connected set of T  if and only if  it is not disconnected in T.

=== Definition 3 ===
Let T = ( S, τ) be a topological space.

Let H ⊆ S be a non-empty subset of S.


H is a connected set of T  if and only if :
:the topological subspace ( H, τ_H ) of T is a connected topological space.

=== Definition 4 ===
Let T = ( S, τ) be a topological space.

Let H ⊆ S be a non-empty subset of S.


H is a connected set of T  if and only if :
:there do not exist disjoint, non-empty subsets X and Y of H such that X ∪ Y = H such that:
::no limit point of X is an element of Y
::no limit point of Y is an element of X.

=== Definition 5 ===
Let T = ( S, τ) be a topological space.

Let H ⊆ S be a non-empty subset of S.


H is a connected set of T  if and only if :
:H cannot be partitioned into 2 non-empty subsets so that each subset has no element in common with the closure of the other.

=== Points in Topological Space ===
Let T = ( S, τ) be a topological space.

Let a, b ∈ S.


Then a and b are connected (in T)  if and only if  there exists a connected set in T containing both a and b.",Connected
['Definitions/Connected Spaces'],Definition:Connected,"Let $T = left( S, tau right)$ be a non-empty topological space.


=== Definition 1 ===
Let $T = left( S, tau right)$ be a topological space.


$T$ is connected  if and only if  it admits no separation.


That is, $T$ is connected  if and only if  there exist no open sets $A, B in tau$ such that $A, B ne varnothing$, $A cup B = S$ and $A cap B = varnothing$.

=== Definition 2 ===
Let $T = left( S, tau right)$ be a topological space.


$T$ is connected  if and only if  it has no two disjoint nonempty closed sets whose union is $S$.

=== Definition 3 ===
Let $T = left( S, tau right)$ be a topological space.


$T$ is connected  if and only if  its only subsets whose boundary is empty are $S$ and $varnothing$.

=== Definition 4 ===
Let $T = left( S, tau right)$ be a topological space.


$T$ is connected  if and only if  its only clopen sets are $S$ and $varnothing$.

=== Definition 5 ===
Let $T = left( S, tau right)$ be a non-empty topological space.


$T$ is connected  if and only if  there are no two non-empty separated sets whose union is $S$.

=== Definition 6 ===

Let $T = left( S, tau right)$ be a topological space.


$T$ is connected  if and only if  there exists no continuous surjection from $T$ onto a discrete two-point space.

=== Definition 7 ===
Let $T = left( S, tau right)$ be a topological space.


$T$ is connected  if and only if :
:there do not exist disjoint, non-empty open sets $X$ and $Y$ of $T$ such that $X cup Y = S$.",Definition:Connected (Topology)/Topological Space,,false,"Let T = ( S, τ) be a non-empty topological space.


=== Definition 1 ===
Let T = ( S, τ) be a topological space.


T is connected  if and only if  it admits no separation.


That is, T is connected  if and only if  there exist no open sets A, B ∈τ such that A, B ∅, A ∪ B = S and A ∩ B = ∅.

=== Definition 2 ===
Let T = ( S, τ) be a topological space.


T is connected  if and only if  it has no two disjoint nonempty closed sets whose union is S.

=== Definition 3 ===
Let T = ( S, τ) be a topological space.


T is connected  if and only if  its only subsets whose boundary is empty are S and ∅.

=== Definition 4 ===
Let T = ( S, τ) be a topological space.


T is connected  if and only if  its only clopen sets are S and ∅.

=== Definition 5 ===
Let T = ( S, τ) be a non-empty topological space.


T is connected  if and only if  there are no two non-empty separated sets whose union is S.

=== Definition 6 ===

Let T = ( S, τ) be a topological space.


T is connected  if and only if  there exists no continuous surjection from T onto a discrete two-point space.

=== Definition 7 ===
Let T = ( S, τ) be a topological space.


T is connected  if and only if :
:there do not exist disjoint, non-empty open sets X and Y of T such that X ∪ Y = S.",Connected
"['Definitions/Connected Sets', 'Definitions/Topology']",Definition:Connected,"Let $T = left( S, tau right)$ be a topological space.

Let $H subseteq S$ be a non-empty subset of $S$.


=== Definition 1 ===

Let $T = left( S, tau right)$ be a topological space.

Let $H subseteq S$ be a non-empty subset of $S$.


$H$ is a connected set of $T$  if and only if  it is not the union of any two non-empty separated sets of $T$.


=== Definition 2 ===
Let $T = left( S, tau right)$ be a topological space.

Let $H subseteq S$ be a non-empty subset of $S$.


$H$ is a connected set of $T$  if and only if  it is not disconnected in $T$.

=== Definition 3 ===
Let $T = left( S, tau right)$ be a topological space.

Let $H subseteq S$ be a non-empty subset of $S$.


$H$ is a connected set of $T$  if and only if :
:the topological subspace $left( H, tau_H right)$ of $T$ is a connected topological space.

=== Definition 4 ===
Let $T = left( S, tau right)$ be a topological space.

Let $H subseteq S$ be a non-empty subset of $S$.


$H$ is a connected set of $T$  if and only if :
:there do not exist disjoint, non-empty subsets $X$ and $Y$ of $H$ such that $X cup Y = H$ such that:
::no limit point of $X$ is an element of $Y$
::no limit point of $Y$ is an element of $X$.

=== Definition 5 ===
Let $T = left( S, tau right)$ be a topological space.

Let $H subseteq S$ be a non-empty subset of $S$.


$H$ is a connected set of $T$  if and only if :
:$H$ cannot be partitioned into $2$ non-empty subsets so that each subset has no element in common with the closure of the other.",Definition:Connected (Topology)/Set,,false,"Let T = ( S, τ) be a topological space.

Let H ⊆ S be a non-empty subset of S.


=== Definition 1 ===

Let T = ( S, τ) be a topological space.

Let H ⊆ S be a non-empty subset of S.


H is a connected set of T  if and only if  it is not the union of any two non-empty separated sets of T.


=== Definition 2 ===
Let T = ( S, τ) be a topological space.

Let H ⊆ S be a non-empty subset of S.


H is a connected set of T  if and only if  it is not disconnected in T.

=== Definition 3 ===
Let T = ( S, τ) be a topological space.

Let H ⊆ S be a non-empty subset of S.


H is a connected set of T  if and only if :
:the topological subspace ( H, τ_H ) of T is a connected topological space.

=== Definition 4 ===
Let T = ( S, τ) be a topological space.

Let H ⊆ S be a non-empty subset of S.


H is a connected set of T  if and only if :
:there do not exist disjoint, non-empty subsets X and Y of H such that X ∪ Y = H such that:
::no limit point of X is an element of Y
::no limit point of Y is an element of X.

=== Definition 5 ===
Let T = ( S, τ) be a topological space.

Let H ⊆ S be a non-empty subset of S.


H is a connected set of T  if and only if :
:H cannot be partitioned into 2 non-empty subsets so that each subset has no element in common with the closure of the other.",Connected
['Definitions/Connected Spaces'],Definition:Connected,"Let $T = left( S, tau right)$ be a topological space.

Let $a, b in S$.


Then $a$ and $b$ are connected (in $T$)  if and only if  there exists a connected set in $T$ containing both $a$ and $b$.",Definition:Connected (Topology)/Points,,false,"Let T = ( S, τ) be a topological space.

Let a, b ∈ S.


Then a and b are connected (in T)  if and only if  there exists a connected set in T containing both a and b.",Connected
['Definitions/Topology'],Definition:Connected,"The connected sum of two manifolds $A^n, B^n$ of dimension $n$ is defined as follows:

Let $Bbb D^n$ be a closed n-disk.

Let $alpha: Bbb D^n to A^n$ be a continuous (or, in the case of smooth manifolds, a smooth) injection.

Let $beta: Bbb D^n to B^n$ be a similar function.  


Define the set:
:$S = left( A^n setminus alpha left(   right){left( Bbb D^n right)^circ}  right) cup left( B^n setminus beta left(   right){left( Bbb D^n right)^circ}  right)$
where:
:$setminus$ denotes set difference
:$left( Bbb D^n right)^circ$ denotes the interior of $Bbb  D^n$.


Define an equivalence relation $sim$ on $S$ as:
:$x sim y iff left( left( x = y right) lor left( alpha^{-1}  left(   right)x = beta^{-1}  left(   right)y right)  right)$


Since the interiors of the disks were removed from the manifolds, it necessarily follows that:
:$alpha^{-1}  left(   right)x, beta^{-1}  left(   right)y in partial Bbb D^n$


The connected sum $A^n # B^n$ is defined as the quotient space of $S$ under $sim$.

Category:Definitions/Topology",Definition:Connected Sum,,false,"The connected sum of two manifolds A^n, B^n of dimension n is defined as follows:

Let D^n be a closed n-disk.

Let α:  D^n → A^n be a continuous (or, in the case of smooth manifolds, a smooth) injection.

Let β:  D^n → B^n be a similar function.  


Define the set:
:S = ( A^n ∖α(   )(  D^n )^∘) ∪( B^n ∖β(   )(  D^n )^∘)
where:
:∖ denotes set difference
:(  D^n )^∘ denotes the interior of D^n.


Define an equivalence relation ∼ on S as:
:x ∼ y ( ( x = y ) ( α^-1(   )x = β^-1(   )y )  )


Since the interiors of the disks were removed from the manifolds, it necessarily follows that:
:α^-1(   )x, β^-1(   )y ∈∂ D^n


The connected sum A^n # B^n is defined as the quotient space of S under ∼.

Category:Definitions/Topology",Connected
['Definitions/Complex Analysis'],Definition:Connected,"Let $D subseteq mathbb C$ be a subset of the set of complex numbers.


=== Definition 1 ===
Let $D subseteq mathbb C$ be a subset of the set of complex numbers.


$D$ is connected  if and only if  every pair of points in $D$ can be joined by a staircase contour.

=== Definition 2 ===
Let $D subseteq mathbb C$ be a subset of the set of complex numbers.


$D$ is connected  if and only if  every pair of points in $D$ can be joined by a polygonal path all points of which are in $D$.",Definition:Connected Set (Complex Analysis),,false,"Let D ⊆ℂ be a subset of the set of complex numbers.


=== Definition 1 ===
Let D ⊆ℂ be a subset of the set of complex numbers.


D is connected  if and only if  every pair of points in D can be joined by a staircase contour.

=== Definition 2 ===
Let D ⊆ℂ be a subset of the set of complex numbers.


D is connected  if and only if  every pair of points in D can be joined by a polygonal path all points of which are in D.",Connected
"['Definitions/Connectedness (Graph Theory)', 'Definitions/Graph Theory']",Definition:Connected,"=== Vertices ===
Let $G$ be a graph.

Two vertices $u, v in G$ are connected  if and only if  either:

:$(1): quad u = v$
:$(2): quad u ne v$, and there exists a walk between them.

=== Graph ===
Let $G$ be a graph.

Then $G$ is connected  if and only if  every pair of vertices in $G$ is connected.


=== Disconnected ===
Let $G = left( V, E right)$ be a graph.

Then $G$ is disconnected  if and only if  it is not connected.

That is,  if and only if  there exists (at least) two vertices $u, v in V$ such that $u$ and $v$ are not connected.",Definition:Connected (Graph Theory),,false,"=== Vertices ===
Let G be a graph.

Two vertices u, v ∈ G are connected  if and only if  either:

:(1):    u = v
:(2):    u  v, and there exists a walk between them.

=== Graph ===
Let G be a graph.

Then G is connected  if and only if  every pair of vertices in G is connected.


=== Disconnected ===
Let G = ( V, E ) be a graph.

Then G is disconnected  if and only if  it is not connected.

That is,  if and only if  there exists (at least) two vertices u, v ∈ V such that u and v are not connected.",Connected
['Definitions/Connectedness (Graph Theory)'],Definition:Connected,"Let $G$ be a graph.

Then $G$ is connected  if and only if  every pair of vertices in $G$ is connected.


=== Disconnected ===
Let $G = left( V, E right)$ be a graph.

Then $G$ is disconnected  if and only if  it is not connected.

That is,  if and only if  there exists (at least) two vertices $u, v in V$ such that $u$ and $v$ are not connected.",Definition:Connected (Graph Theory)/Graph,,false,"Let G be a graph.

Then G is connected  if and only if  every pair of vertices in G is connected.


=== Disconnected ===
Let G = ( V, E ) be a graph.

Then G is disconnected  if and only if  it is not connected.

That is,  if and only if  there exists (at least) two vertices u, v ∈ V such that u and v are not connected.",Connected
"['Definitions/Connectedness (Graph Theory)', 'Definitions/Vertices of Graphs']",Definition:Connected,"Let $G$ be a graph.

Two vertices $u, v in G$ are connected  if and only if  either:

:$(1): quad u = v$
:$(2): quad u ne v$, and there exists a walk between them.",Definition:Connected (Graph Theory)/Vertices,,false,"Let G be a graph.

Two vertices u, v ∈ G are connected  if and only if  either:

:(1):    u = v
:(2):    u  v, and there exists a walk between them.",Connected
"['Definitions/Provable Consequences', 'Definitions/Logical Implication', 'Definitions/Proof Systems']",Definition:Consequence,"Let $mathscr P$ be a proof system for a formal language $mathcal L$.

Let $mathcal F$ be a collection of WFFs of $mathcal L$.


Denote with $mathscr P left(   right)mathcal F$ the proof system obtained from $mathscr P$ by adding all the WFFs from $mathcal F$ as axioms.

Let $phi$ be a theorem of $mathscr P left(   right)mathcal F$.


Then $phi$ is called a provable consequence of $mathcal F$, and this is denoted as:

:$mathcal F vdash_{mathscr P} phi$


Note in particular that for $mathcal F = varnothing$, this notation agrees with the notation for a $mathscr P$-theorem:

:$vdash_{mathscr P} phi$",Definition:Provable Consequence,,false,"Let 𝒫 be a proof system for a formal language ℒ.

Let ℱ be a collection of WFFs of ℒ.


Denote with 𝒫(   )ℱ the proof system obtained from 𝒫 by adding all the WFFs from ℱ as axioms.

Let ϕ be a theorem of 𝒫(   )ℱ.


Then ϕ is called a provable consequence of ℱ, and this is denoted as:

:ℱ⊢_𝒫ϕ


Note in particular that for ℱ = ∅, this notation agrees with the notation for a 𝒫-theorem:

:⊢_𝒫ϕ",Consequence
"['Definitions/Semantic Consequences', 'Definitions/Formal Semantics', 'Definitions/Logical Implication']",Definition:Consequence,"Let $mathscr M$ be a formal semantics for a formal language $mathcal L$.

Let $mathcal F$ be a collection of WFFs of $mathcal L$.


Let $mathscr M left(   right)mathcal F$ be the formal semantics obtained from $mathscr M$ by retaining only the structures of $mathscr M$ that are models of $mathcal F$.

Let $phi$ be a tautology for $mathscr M left(   right)mathcal F$.


Then $phi$ is called a semantic consequence of $mathcal F$, and this is denoted as:

:$mathcal F models_{mathscr M} phi$


That is to say, $phi$ is a semantic consequence of $mathcal F$  if and only if , for each $mathscr M$-structure $mathcal M$:

:$mathcal M models_{mathscr M} mathcal F$ implies $mathcal M models_{mathscr M} phi$

where $models_{mathscr M}$ is the models relation.


Note in particular that for $mathcal F = varnothing$, the notation agrees with the notation for a $mathscr M$-tautology:

:$models_{mathscr M} phi$


The concept naturally generalises to sets of formulas $mathcal G$ on the  :

:$mathcal F models_{mathscr M} mathcal G$

 if and only if  $mathcal F models_{mathscr M} phi$ for every $phi in mathcal G$.


 

 ",Definition:Semantic Consequence,,false,"Let ℳ be a formal semantics for a formal language ℒ.

Let ℱ be a collection of WFFs of ℒ.


Let ℳ(   )ℱ be the formal semantics obtained from ℳ by retaining only the structures of ℳ that are models of ℱ.

Let ϕ be a tautology for ℳ(   )ℱ.


Then ϕ is called a semantic consequence of ℱ, and this is denoted as:

:ℱ_ℳϕ


That is to say, ϕ is a semantic consequence of ℱ  if and only if , for each ℳ-structure ℳ:

:ℳ_ℳℱ implies ℳ_ℳϕ

where _ℳ is the models relation.


Note in particular that for ℱ = ∅, the notation agrees with the notation for a ℳ-tautology:

:_ℳϕ


The concept naturally generalises to sets of formulas 𝒢 on the  :

:ℱ_ℳ𝒢

 if and only if  ℱ_ℳϕ for every ϕ∈𝒢.


 

 ",Consequence
['Definitions/Game Theory'],Definition:Consequence,A consequence is a state in a game which results from a move made by a player in that game made according to the rules.,Definition:Consequence (Game Theory),state,true,A consequence is a state in a game which results from a move made by a player in that game made according to the rules.,Consequence
['Definitions/Game Theory'],Definition:Consequence,"Let $G$ be a game.

Let $P$ be a player of $G$.

Let $A$ be the set of moves available to $P$.

Let $C$ be the set of consequences of those moves.


A consequence function for $P$ is a mapping from the set $A$ to the set $C$:
:$g: A to C$",Definition:Consequence Function,,false,"Let G be a game.

Let P be a player of G.

Let A be the set of moves available to P.

Let C be the set of consequences of those moves.


A consequence function for P is a mapping from the set A to the set C:
:g: A → C",Consequence
"['Definitions/Proof Systems', 'Definitions/Logical Consistency']",Definition:Consistent,"Let $mathcal L$ be a logical language.

Let $mathscr P$ be a proof system for $mathcal L$.


=== Proof System ===
Let $mathcal L$ be a logical language.

Let $mathscr P$ be a proof system for $mathcal L$.


Then $mathscr P$ is consistent  if and only if :

:There exists a logical formula $phi$ such that $not vdash_{mathscr P} phi$

That is, some logical formula $phi$ is not a theorem of $mathscr P$.


=== Propositional Logic ===
Let $mathcal L_0$ be the language of propositional logic.

Let $mathscr P$ be a proof system for $mathcal L_0$.


=== Definition 1 ===
Let $mathcal L_0$ be the language of propositional logic.

Let $mathscr P$ be a proof system for $mathcal L_0$.


Then $mathscr P$ is consistent  if and only if :

:There exists a logical formula $phi$ such that $not vdash_{mathscr P} phi$

That is, some logical formula $phi$ is not a theorem of $mathscr P$.


Category:Definitions/Proof Systems
Category:Definitions/Propositional Logic

=== Definition 2 ===
Let $mathcal L$ be the language of propositional logic.

Let $mathscr P$ be a proof system for $mathcal L_0$.

Suppose that in $mathscr P$, the Rule of Explosion (Variant 3) holds.


Then $mathscr P$ is consistent  if and only if :

:For every logical formula $phi$, not both of $phi$ and $neg phi$ are theorems of $mathscr P$

=== Set of Formulas ===
Let $mathcal L$ be a logical language.

Let $mathscr P$ be a proof system for $mathcal L$.

Let $mathcal F$ be a collection of logical formulas.


Then $mathcal F$ is consistent for $mathscr P$  if and only if :

:There exists a logical formula $phi$ such that $mathcal F nvdash_{mathscr P} phi$.

That is, some logical formula $phi$ is not a provable consequence of $mathcal F$.


=== Propositional Logic ===
 
Let $mathcal L_0$ be the language of propositional logic.

Let $mathscr P$ be a proof system for $mathcal L_0$.

Let $mathcal F$ be a collection of logical formulas.


=== Definition 1 ===
Let $mathcal L_0$ be the language of propositional logic.

Let $mathscr P$ be a proof system for $mathcal L_0$.

Let $mathcal F$ be a collection of logical formulas.


Then $mathcal F$ is consistent for $mathscr P$  if and only if :

:There exists a logical formula $phi$ such that $mathcal F not vdash_{mathscr P} phi$

That is, some logical formula $phi$ is not a $mathscr P$-provable consequence of $mathcal F$.


Category:Definitions/Proof Systems
Category:Definitions/Propositional Logic

=== Definition 2 ===
Let $mathcal L$ be the language of propositional logic.

Let $mathscr P$ be a proof system for $mathcal L_0$.

Let $mathcal F$ be a collection of logical formulas.

Suppose that in $mathscr P$, the Rule of Explosion (Variant 3) holds.


Then $mathcal F$ is consistent for $mathscr P$  if and only if :

:For every logical formula $phi$, not both of $phi$ and $neg phi$ are $mathscr P$-provable consequences of $mathcal F$",Definition:Consistent (Logic),,false,"Let ℒ be a logical language.

Let 𝒫 be a proof system for ℒ.


=== Proof System ===
Let ℒ be a logical language.

Let 𝒫 be a proof system for ℒ.


Then 𝒫 is consistent  if and only if :

:There exists a logical formula ϕ such that ⊬_𝒫ϕ

That is, some logical formula ϕ is not a theorem of 𝒫.


=== Propositional Logic ===
Let ℒ_0 be the language of propositional logic.

Let 𝒫 be a proof system for ℒ_0.


=== Definition 1 ===
Let ℒ_0 be the language of propositional logic.

Let 𝒫 be a proof system for ℒ_0.


Then 𝒫 is consistent  if and only if :

:There exists a logical formula ϕ such that ⊬_𝒫ϕ

That is, some logical formula ϕ is not a theorem of 𝒫.


Category:Definitions/Proof Systems
Category:Definitions/Propositional Logic

=== Definition 2 ===
Let ℒ be the language of propositional logic.

Let 𝒫 be a proof system for ℒ_0.

Suppose that in 𝒫, the Rule of Explosion (Variant 3) holds.


Then 𝒫 is consistent  if and only if :

:For every logical formula ϕ, not both of ϕ and ϕ are theorems of 𝒫

=== Set of Formulas ===
Let ℒ be a logical language.

Let 𝒫 be a proof system for ℒ.

Let ℱ be a collection of logical formulas.


Then ℱ is consistent for 𝒫  if and only if :

:There exists a logical formula ϕ such that ℱ⊬_𝒫ϕ.

That is, some logical formula ϕ is not a provable consequence of ℱ.


=== Propositional Logic ===
 
Let ℒ_0 be the language of propositional logic.

Let 𝒫 be a proof system for ℒ_0.

Let ℱ be a collection of logical formulas.


=== Definition 1 ===
Let ℒ_0 be the language of propositional logic.

Let 𝒫 be a proof system for ℒ_0.

Let ℱ be a collection of logical formulas.


Then ℱ is consistent for 𝒫  if and only if :

:There exists a logical formula ϕ such that ℱ⊬_𝒫ϕ

That is, some logical formula ϕ is not a 𝒫-provable consequence of ℱ.


Category:Definitions/Proof Systems
Category:Definitions/Propositional Logic

=== Definition 2 ===
Let ℒ be the language of propositional logic.

Let 𝒫 be a proof system for ℒ_0.

Let ℱ be a collection of logical formulas.

Suppose that in 𝒫, the Rule of Explosion (Variant 3) holds.


Then ℱ is consistent for 𝒫  if and only if :

:For every logical formula ϕ, not both of ϕ and ϕ are 𝒫-provable consequences of ℱ",Consistent
"['Definitions/Consistent Simultaneous Equations', 'Definitions/Simultaneous Equations']",Definition:Consistent,"A system of simultaneous equations is referred to as consistent  if and only if  it has at least one solution.

That is,  if and only if  there exists a set of values for its variables such that all the equations are satisfied.


=== Inconsistent ===
A set of equations is described as inconsistent  if and only if  they are not consistent

That is, there exists no set of values for its variables such that all the equations are satisfied.",Definition:Consistent Simultaneous Equations,,false,"A system of simultaneous equations is referred to as consistent  if and only if  it has at least one solution.

That is,  if and only if  there exists a set of values for its variables such that all the equations are satisfied.


=== Inconsistent ===
A set of equations is described as inconsistent  if and only if  they are not consistent

That is, there exists no set of values for its variables such that all the equations are satisfied.",Consistent
"['Definitions/Consistent Estimators', 'Definitions/Estimators']",Definition:Consistent,"Let $X_1, X_2, ldots, X_n$ be random variables.

Let the joint distribution of $X_1, X_2, ldots, X_n$ be indexed by a population parameter $theta$.

Let $hat theta$ be an estimator of $theta$.

Then $hat theta$ is consistent  if and only if :
:$ds lim_{n mathop to infty} Pr left(   right){leftlvert hat theta - theta rightrvert ge epsilon} = 0$
for all $epsilon > 0$.",Definition:Consistent Estimator,,false,"Let X_1, X_2, …, X_n be random variables.

Let the joint distribution of X_1, X_2, …, X_n be indexed by a population parameter θ.

Let θ̂ be an estimator of θ.

Then θ̂ is consistent  if and only if :
:lim_n →∞(   )|θ̂- θ|≥ϵ = 0
for all ϵ > 0.",Consistent
"['Definitions/Polynomial Theory', 'Definitions/Content of Polynomial']",Definition:Content,"=== Integer Polynomial ===
Let $f in mathbb Z left[ X right]$ be a polynomial with integer coefficients.

Then the content of $f$, denoted $mathrm {cont} left( f right)$, is the greatest common divisor of the coefficients of $f$.

=== Rational Polynomial ===
Let $f in mathbb Q left[ X right]$ be a polynomial with rational coefficients.


The content of $f$ is defined as:
:$mathrm {cont} left( f right) := dfrac {mathrm {cont} left( n f right) } n$
where $n in mathbb N$ is such that $n f in mathbb Z left[ X right]$.

=== Polynomial in GCD Domain ===
Let $D$ be a GCD domain.

Let $K$ be the field of quotients of $D$.

Let $f in K left[ X right]$ be a polynomial.

Let $a in D$ be such that $a f in D left[ X right]$.

Let $d$ be the greatest common divisor of the coefficients of $a f$.

Then we define the content of $f$ to be:
:$mathrm {cont} left( f right) := dfrac d a$",Definition:Content of Polynomial,,false,"=== Integer Polynomial ===
Let f ∈ℤ[ X ] be a polynomial with integer coefficients.

Then the content of f, denoted cont( f ), is the greatest common divisor of the coefficients of f.

=== Rational Polynomial ===
Let f ∈ℚ[ X ] be a polynomial with rational coefficients.


The content of f is defined as:
:cont( f ) := cont( n f )  n
where n ∈ℕ is such that n f ∈ℤ[ X ].

=== Polynomial in GCD Domain ===
Let D be a GCD domain.

Let K be the field of quotients of D.

Let f ∈ K [ X ] be a polynomial.

Let a ∈ D be such that a f ∈ D [ X ].

Let d be the greatest common divisor of the coefficients of a f.

Then we define the content of f to be:
:cont( f ) :=  d a",Content
"['Definitions/Polynomial Theory', 'Definitions/Content of Polynomial']",Definition:Content,"Let $f in mathbb Z left[ X right]$ be a polynomial with integer coefficients.

Then the content of $f$, denoted $mathrm {cont} left( f right)$, is the greatest common divisor of the coefficients of $f$.",Definition:Content of Polynomial/Integer,,false,"Let f ∈ℤ[ X ] be a polynomial with integer coefficients.

Then the content of f, denoted cont( f ), is the greatest common divisor of the coefficients of f.",Content
['Definitions/Content of Polynomial'],Definition:Content,"Let $f in mathbb Q left[ X right]$ be a polynomial with rational coefficients.


The content of $f$ is defined as:
:$mathrm {cont} left( f right) := dfrac {mathrm {cont} left( n f right) } n$
where $n in mathbb N$ is such that $n f in mathbb Z left[ X right]$.",Definition:Content of Polynomial/Rational,,false,"Let f ∈ℚ[ X ] be a polynomial with rational coefficients.


The content of f is defined as:
:cont( f ) := cont( n f )  n
where n ∈ℕ is such that n f ∈ℤ[ X ].",Content
"['Definitions/Continuous Mappings', 'Definitions/Mappings', 'Definitions/Continuity']",Definition:Continuous,"The concept of continuity makes precise the intuitive notion that a function has no ""jumps"" at a given point.

Loosely speaking, in the case of a real function, continuity at a point is defined as the property that the graph of the function does not have a ""break"" at the point.

Thus, a small change in the independent variable causes a similar small change in the dependent variable

This concept appears throughout mathematics and correspondingly has many variations and generalizations.",Definition:Continuous Mapping,,false,"The concept of continuity makes precise the intuitive notion that a function has no ""jumps"" at a given point.

Loosely speaking, in the case of a real function, continuity at a point is defined as the property that the graph of the function does not have a ""break"" at the point.

Thus, a small change in the independent variable causes a similar small change in the dependent variable

This concept appears throughout mathematics and correspondingly has many variations and generalizations.",Continuous
['Definitions/Order Theory'],Definition:Continuous,"Let $L = left({X, preceq}right)$ be an ordered set.

Let $S = left({Y, preceq'}right)$ be an ordered subset of $L$.


Then $S$ is continuous lattice subframe of $L$  if and only if 
:$S$ inherits infima and directed suprema.",Definition:Continuous Lattice Subframe,,false,"Let L = (X, ≼) be an ordered set.

Let S = (Y, ≼') be an ordered subset of L.


Then S is continuous lattice subframe of L  if and only if 
:S inherits infima and directed suprema.",Continuous
['Definitions/Sample Statistics'],Definition:Continuous,Data which can be described with a continuous variable obtained by the process of measurement are known as continuous data.,Definition:Sample Statistic/Continuous,,false,Data which can be described with a continuous variable obtained by the process of measurement are known as continuous data.,Continuous
"['Definitions/Continuous Real Functions', 'Definitions/Continuous Functions', 'Definitions/Continuous Mappings', 'Definitions/Real Functions']",Definition:Continuous,"=== Informal Definition ===
The concept of continuity makes precise the intuitive notion that a function has no ""jumps"" or ""holes"" at a given point.

Loosely speaking, a real function $f$ is continuous at a point $p$  if and only if  the graph of $f$ does not have a ""break"" at $p$.

=== Continuity at a Point ===
Let $A subseteq mathbb R$ be a subset of the real numbers.

Let $f: A to mathbb R$ be a real function.

Let $x in A$ be a point of $A$.


$f$ is continuous at $x$  if and only if  the limit $ds lim_{y mathop to x} f left(   right)y$ exists and:
:$ds lim_{y mathop to x} f left(   right)y = f left(   right)x$

=== Continuous Everywhere ===
Let $f: mathbb R to mathbb R$ be a real function.


Then $f$ is everywhere continuous  if and only if  $f$ is continuous at every point in $mathbb R$.

=== Continuity on a Subset of Domain ===
Let $A subseteq mathbb R$ be any subset of the real numbers.

Let $f: A to mathbb R$ be a real function.


Then $f$ is continuous on $A$  if and only if  $f$ is continuous at every point of $A$.",Definition:Continuous Real Function,,false,"=== Informal Definition ===
The concept of continuity makes precise the intuitive notion that a function has no ""jumps"" or ""holes"" at a given point.

Loosely speaking, a real function f is continuous at a point p  if and only if  the graph of f does not have a ""break"" at p.

=== Continuity at a Point ===
Let A ⊆ℝ be a subset of the real numbers.

Let f: A →ℝ be a real function.

Let x ∈ A be a point of A.


f is continuous at x  if and only if  the limit lim_y → x f (   )y exists and:
:lim_y → x f (   )y = f (   )x

=== Continuous Everywhere ===
Let f: ℝ→ℝ be a real function.


Then f is everywhere continuous  if and only if  f is continuous at every point in ℝ.

=== Continuity on a Subset of Domain ===
Let A ⊆ℝ be any subset of the real numbers.

Let f: A →ℝ be a real function.


Then f is continuous on A  if and only if  f is continuous at every point of A.",Continuous
['Definitions/Continuous Real Functions'],Definition:Continuous,"Let $A subseteq mathbb R$ be an open subset of the real numbers $mathbb R$.

Let $f: A to mathbb R$ be a real function.


Let $x_0 in A$. 

Then $f$ is said to be left-continuous at $x_0$  if and only if  the limit from the left of $f left(   right)x$ as $x to x_0$ exists and:

:$ds lim_{substack {x mathop to x_0^- \ x_0 mathop in A} } f left(   right)x = f left(   right){x_0}$

where $ds lim_{x mathop to x_0^-}$ is a limit from the left.


Furthermore, $f$ is said to be left-continuous  if and only if :

:$forall x_0 in A$, $f$ is left-continuous at $x_0$",Definition:Continuous Real Function/Left-Continuous,,false,"Let A ⊆ℝ be an open subset of the real numbers ℝ.

Let f: A →ℝ be a real function.


Let x_0 ∈ A. 

Then f is said to be left-continuous at x_0  if and only if  the limit from the left of f (   )x as x → x_0 exists and:

:lim_x → x_0^- 
 x_0 ∈ A f (   )x = f (   )x_0

where lim_x → x_0^- is a limit from the left.


Furthermore, f is said to be left-continuous  if and only if :

:∀ x_0 ∈ A, f is left-continuous at x_0",Continuous
['Definitions/Continuous Real Functions'],Definition:Continuous,"Let $S subseteq mathbb R$ be an open subset of the real numbers $mathbb R$.

Let $f: S to mathbb R$ be a real function.


Let $x_0 in S$. 

Then $f$ is said to be right-continuous at $x_0$  if and only if  the limit from the right of $f left(   right)x$ as $x to x_0$ exists and:

:$ds lim_{substack {x mathop to x_0^+ \ x_0 mathop in A}} f left(   right)x = f left(   right){x_0}$

where $ds lim_{x mathop to x_0^+}$ is a limit from the right.


Furthermore, $f$ is said to be right-continuous  if and only if :

:$forall x_0 in S$, $f$ is right-continuous at $x_0$",Definition:Continuous Real Function/Right-Continuous,,false,"Let S ⊆ℝ be an open subset of the real numbers ℝ.

Let f: S →ℝ be a real function.


Let x_0 ∈ S. 

Then f is said to be right-continuous at x_0  if and only if  the limit from the right of f (   )x as x → x_0 exists and:

:lim_x → x_0^+ 
 x_0 ∈ A f (   )x = f (   )x_0

where lim_x → x_0^+ is a limit from the right.


Furthermore, f is said to be right-continuous  if and only if :

:∀ x_0 ∈ S, f is right-continuous at x_0",Continuous
"['Definitions/Continuity', 'Definitions/Real-Valued Functions']",Definition:Continuous,"Let $mathbb R^n$ be the cartesian $n$-space.

Let $f: mathbb R^n to mathbb R$ be a real-valued function on $mathbb R^n$.


Then $f$ is continuous on $mathbb R^n$  if and only if :
:$forall a in mathbb R^n: forall epsilon in mathbb R_{>0}: exists delta in mathbb R_{>0}: forall x in mathbb R^n: d left(   right){x, a} < delta implies leftlvert f left(   right)x - f left(   right)a rightrvert < epsilon$
where $d left(   right){x, a}$ is the distance function on $mathbb R^n$:

:$ds d: mathbb R^n to mathbb R: d left(   right){x, y} := sqrt {sum_{i mathop = 1}^n left( x_i - y_i right)^2}$

where $x = left( x_1, x_2, ldots, x_n right), y = left( y_1, y_2, ldots, y_n right)$ are general elements of $mathbb R^n$.",Definition:Continuous Real-Valued Vector Function,,false,"Let ℝ^n be the cartesian n-space.

Let f: ℝ^n →ℝ be a real-valued function on ℝ^n.


Then f is continuous on ℝ^n  if and only if :
:∀ a ∈ℝ^n: ∀ϵ∈ℝ_>0: ∃δ∈ℝ_>0: ∀ x ∈ℝ^n: d (   )x, a < δ| f (   )x - f (   )a | < ϵ
where d (   )x, a is the distance function on ℝ^n:

:d: ℝ^n →ℝ: d (   )x, y := √(∑_i  = 1^n ( x_i - y_i )^2)

where x = ( x_1, x_2, …, x_n ), y = ( y_1, y_2, …, y_n ) are general elements of ℝ^n.",Continuous
"['Definitions/Continuous Complex Functions', 'Definitions/Continuous Functions', 'Definitions/Continuous Mappings', 'Definitions/Complex Functions']",Definition:Continuous,"As the complex plane is a metric space, the same definition of continuity applies to complex functions as to metric spaces.



Let $A_1, A_2 subseteq mathbb C$ be subsets of the complex plane.

Let $f: A_1 to A_2$ be a complex function from $A_1$ to $A_2$.

Let $a in A_1$.


=== Definition using Limit ===
Let $A_1, A_2 subseteq mathbb C$ be subsets of the complex plane.

Let $f: A_1 to A_2$ be a complex function from $A_1$ to $A_2$.

Let $a in A_1$.


$f$ is continuous at (the point) $a$  if and only if :
:The limit of $f left(   right)z$ as $z to a$ exists, and
:$ds lim_{z mathop to a} f left(   right)z = f left(   right)a$

=== Epsilon-Delta Definition ===
Let $A_1, A_2 subseteq mathbb C$ be subsets of the complex plane.

Let $f: A_1 to A_2$ be a complex function from $A_1$ to $A_2$.

Let $a in A_1$.


$f$ is continuous at (the point) $a$  if and only if :

:$forall epsilon > 0: exists delta > 0: forall z in A_1: leftlvert z - a rightrvert < delta implies leftlvert f left(   right)z - f left(   right)a rightrvert < epsilon$

=== Epsilon-Neighborhood Definition ===
Let $A_1, A_2 subseteq mathbb C$ be subsets of the complex plane.

Let $f: A_1 to A_2$ be a complex function from $A_1$ to $A_2$.

Let $a in A_1$.


Let $A_1$ be open in $mathbb C$.


$f$ is continuous at (the point) $a$  if and only if :
:$forall mathcal N_epsilon left(   right){f left(   right)a}: exists mathcal N_delta left(   right)a: f left[ mathcal N_delta left(   right)a right] subseteq mathcal N_epsilon left(   right){f left(   right)a}$
where $mathcal N_epsilon left(   right)a$ is the $epsilon$-neighborhood of $a$ in $A_1$.


That is, for every $epsilon$-neighborhood of $f left(   right)a$ in $mathbb C$, there exists a $delta$-neighborhood of $a$ in $mathbb C$ whose image is a subset of that $epsilon$-neighborhood.

=== Open Sets Definition ===
Let $A_1, A_2 subseteq mathbb C$ be subsets of the complex plane.

Let $f: A_1 to A_2$ be a complex function from $A_1$ to $A_2$.


Let $A_1$ be open in $mathbb C$.


$f$ is continuous  if and only if :
:for every set $U subseteq mathbb C$ which is open in $mathbb C$, $f^{-1} left[ U right]$ is open in $mathbb C$.",Definition:Continuous Complex Function,,false,"As the complex plane is a metric space, the same definition of continuity applies to complex functions as to metric spaces.



Let A_1, A_2 ⊆ℂ be subsets of the complex plane.

Let f: A_1 → A_2 be a complex function from A_1 to A_2.

Let a ∈ A_1.


=== Definition using Limit ===
Let A_1, A_2 ⊆ℂ be subsets of the complex plane.

Let f: A_1 → A_2 be a complex function from A_1 to A_2.

Let a ∈ A_1.


f is continuous at (the point) a  if and only if :
:The limit of f (   )z as z → a exists, and
:lim_z → a f (   )z = f (   )a

=== Epsilon-Delta Definition ===
Let A_1, A_2 ⊆ℂ be subsets of the complex plane.

Let f: A_1 → A_2 be a complex function from A_1 to A_2.

Let a ∈ A_1.


f is continuous at (the point) a  if and only if :

:∀ϵ > 0: ∃δ > 0: ∀ z ∈ A_1: | z - a | < δ| f (   )z - f (   )a | < ϵ

=== Epsilon-Neighborhood Definition ===
Let A_1, A_2 ⊆ℂ be subsets of the complex plane.

Let f: A_1 → A_2 be a complex function from A_1 to A_2.

Let a ∈ A_1.


Let A_1 be open in ℂ.


f is continuous at (the point) a  if and only if :
:∀𝒩_ϵ(   )f (   )a: ∃𝒩_δ(   )a: f [ 𝒩_δ(   )a ] ⊆𝒩_ϵ(   )f (   )a
where 𝒩_ϵ(   )a is the ϵ-neighborhood of a in A_1.


That is, for every ϵ-neighborhood of f (   )a in ℂ, there exists a δ-neighborhood of a in ℂ whose image is a subset of that ϵ-neighborhood.

=== Open Sets Definition ===
Let A_1, A_2 ⊆ℂ be subsets of the complex plane.

Let f: A_1 → A_2 be a complex function from A_1 to A_2.


Let A_1 be open in ℂ.


f is continuous  if and only if :
:for every set U ⊆ℂ which is open in ℂ, f^-1[ U ] is open in ℂ.",Continuous
['Definitions/Calculus of Variations'],Definition:Continuous,"Let $S$ be a set of mappings.

Let $y in S$ be a mapping.

Let $J left[ y right]: S to mathbb R$ be a functional.

Suppose:
  
:$forall epsilon in mathbb R_{>0}: exists delta in mathbb R_{>0}: leftlvert y - y_0 rightrvert < delta implies leftlvert J left[ y right] - J left[ y_0 right]  rightrvert < epsilon$


Then $J left[ y right]$ is said to be a continuous functional and is continuous at the point $y_0 in S$.",Definition:Continuity/Functional,,false,"Let S be a set of mappings.

Let y ∈ S be a mapping.

Let J [ y ]: S →ℝ be a functional.

Suppose:
  
:∀ϵ∈ℝ_>0: ∃δ∈ℝ_>0: | y - y_0 | < δ| J [ y ] - J [ y_0 ]  | < ϵ


Then J [ y ] is said to be a continuous functional and is continuous at the point y_0 ∈ S.",Continuous
"['Definitions/Continuous Mappings on Metric Spaces', 'Definitions/Continuous Mappings', 'Definitions/Metric Spaces']",Definition:Continuous,"Let $M_1 = left( A_1, d_1 right)$ and $M_2 = left( A_2, d_2 right)$ be metric spaces.

Let $f: A_1 to A_2$ be a mapping from $A_1$ to $A_2$.

Let $a in A_1$ be a point in $A_1$.


=== Continuous at a Point ===
Let $M_1 = left( A_1, d_1 right)$ and $M_2 = left( A_2, d_2 right)$ be metric spaces.

Let $f: A_1 to A_2$ be a mapping from $A_1$ to $A_2$.

Let $a in A_1$ be a point in $A_1$.


=== $epsilon$-$delta$ Definition ===

Let $M_1 = left( A_1, d_1 right)$ and $M_2 = left( A_2, d_2 right)$ be metric spaces.

Let $f: A_1 to A_2$ be a mapping from $A_1$ to $A_2$.

Let $a in A_1$ be a point in $A_1$.


$f$ is continuous at (the point) $a$ (with respect to the metrics $d_1$ and $d_2$)  if and only if :
:$forall epsilon in mathbb R_{>0}: exists delta in mathbb R_{>0}: forall x in A_1: d_1 left(   right){x, a} < delta implies d_2 left(   right){f left(   right)x, f left(   right)a} < epsilon$
where $mathbb R_{>0}$ denotes the set of all strictly positive real numbers.

=== Definition by Limits ===
Let $M_1 = left( A_1, d_1 right)$ and $M_2 = left( A_2, d_2 right)$ be metric spaces.

Let $f: A_1 to A_2$ be a mapping from $A_1$ to $A_2$.

Let $a in A_1$ be a point in $A_1$.


$f$ is continuous at (the point) $a$ (with respect to the metrics $d_1$ and $d_2$)  if and only if :
:$(1): quad$ The limit of $f left(   right)x$ as $x to a$ exists
:$(2): quad ds lim_{x mathop to a} f left(   right)x = f left(   right)a$.

=== $epsilon$-Ball Definition ===
Let $M_1 = left( A_1, d_1 right)$ and $M_2 = left( A_2, d_2 right)$ be metric spaces.

Let $f: A_1 to A_2$ be a mapping from $A_1$ to $A_2$.

Let $a in A_1$ be a point in $A_1$.


$f$ is continuous at (the point) $a$ (with respect to the metrics $d_1$ and $d_2$)  if and only if :
:$forall epsilon in mathbb R_{>0}: exists delta in mathbb R_{>0}: f left[ B_delta left(   right){a; d_1}  right] subseteq B_epsilon left(   right){f left(   right)a; d_2}$
where $B_epsilon left(   right){f left(   right)a; d_2}$ denotes the open $epsilon$-ball of $f left(   right)a$ with respect to the metric $d_2$, and similarly for $B_delta left(   right){a; d_1}$.

=== Definition by Neighborhoods ===
Let $M_1 = left( A_1, d_1 right)$ and $M_2 = left( A_2, d_2 right)$ be metric spaces.

Let $f: A_1 to A_2$ be a mapping from $A_1$ to $A_2$.

Let $a in A_1$ be a point in $A_1$.


$f$ is continuous at (the point) $a$ (with respect to the metrics $d_1$ and $d_2$)  if and only if :
:for each neighborhood $N'$ of $f left(   right)a$ in $M_2$ there exists a corresponding neighborhood $N$ of $a$ in $M_1$ such that $f left[ N right] subseteq N'$.

=== Continuous on a Space ===
Let $M_1 = left( A_1, d_1 right)$ and $M_2 = left( A_2, d_2 right)$ be metric spaces.

Let $f: A_1 to A_2$ be a mapping from $A_1$ to $A_2$.


=== Definition 1 ===

Let $M_1 = left( A_1, d_1 right)$ and $M_2 = left( A_2, d_2 right)$ be metric spaces.

Let $f: A_1 to A_2$ be a mapping from $A_1$ to $A_2$.


$f$ is continuous from $left( A_1, d_1 right)$ to $left( A_2, d_2 right)$  if and only if  it is continuous at every point $x in A_1$.

=== Definition 2 ===
Let $M_1 = left( A_1, d_1 right)$ and $M_2 = left( A_2, d_2 right)$ be metric spaces.

Let $f: A_1 to A_2$ be a mapping from $A_1$ to $A_2$.


$f$ is continuous from $left( A_1, d_1 right)$ to $left( A_2, d_2 right)$  if and only if :
:for every $U subseteq A_2$ which is open in $M_2$, $f^{-1} left[ U right]$ is open in $M_1$.


By definition, this is equivalent to the continuity of $f$ with respect to the induced topologies on $A_1$ and $A_2$.",Definition:Continuous Mapping (Metric Space),,false,"Let M_1 = ( A_1, d_1 ) and M_2 = ( A_2, d_2 ) be metric spaces.

Let f: A_1 → A_2 be a mapping from A_1 to A_2.

Let a ∈ A_1 be a point in A_1.


=== Continuous at a Point ===
Let M_1 = ( A_1, d_1 ) and M_2 = ( A_2, d_2 ) be metric spaces.

Let f: A_1 → A_2 be a mapping from A_1 to A_2.

Let a ∈ A_1 be a point in A_1.


=== ϵ-δ Definition ===

Let M_1 = ( A_1, d_1 ) and M_2 = ( A_2, d_2 ) be metric spaces.

Let f: A_1 → A_2 be a mapping from A_1 to A_2.

Let a ∈ A_1 be a point in A_1.


f is continuous at (the point) a (with respect to the metrics d_1 and d_2)  if and only if :
:∀ϵ∈ℝ_>0: ∃δ∈ℝ_>0: ∀ x ∈ A_1: d_1 (   )x, a < δ d_2 (   )f (   )x, f (   )a < ϵ
where ℝ_>0 denotes the set of all strictly positive real numbers.

=== Definition by Limits ===
Let M_1 = ( A_1, d_1 ) and M_2 = ( A_2, d_2 ) be metric spaces.

Let f: A_1 → A_2 be a mapping from A_1 to A_2.

Let a ∈ A_1 be a point in A_1.


f is continuous at (the point) a (with respect to the metrics d_1 and d_2)  if and only if :
:(1): The limit of f (   )x as x → a exists
:(2):   lim_x → a f (   )x = f (   )a.

=== ϵ-Ball Definition ===
Let M_1 = ( A_1, d_1 ) and M_2 = ( A_2, d_2 ) be metric spaces.

Let f: A_1 → A_2 be a mapping from A_1 to A_2.

Let a ∈ A_1 be a point in A_1.


f is continuous at (the point) a (with respect to the metrics d_1 and d_2)  if and only if :
:∀ϵ∈ℝ_>0: ∃δ∈ℝ_>0: f [ B_δ(   )a; d_1] ⊆ B_ϵ(   )f (   )a; d_2
where B_ϵ(   )f (   )a; d_2 denotes the open ϵ-ball of f (   )a with respect to the metric d_2, and similarly for B_δ(   )a; d_1.

=== Definition by Neighborhoods ===
Let M_1 = ( A_1, d_1 ) and M_2 = ( A_2, d_2 ) be metric spaces.

Let f: A_1 → A_2 be a mapping from A_1 to A_2.

Let a ∈ A_1 be a point in A_1.


f is continuous at (the point) a (with respect to the metrics d_1 and d_2)  if and only if :
:for each neighborhood N' of f (   )a in M_2 there exists a corresponding neighborhood N of a in M_1 such that f [ N ] ⊆ N'.

=== Continuous on a Space ===
Let M_1 = ( A_1, d_1 ) and M_2 = ( A_2, d_2 ) be metric spaces.

Let f: A_1 → A_2 be a mapping from A_1 to A_2.


=== Definition 1 ===

Let M_1 = ( A_1, d_1 ) and M_2 = ( A_2, d_2 ) be metric spaces.

Let f: A_1 → A_2 be a mapping from A_1 to A_2.


f is continuous from ( A_1, d_1 ) to ( A_2, d_2 )  if and only if  it is continuous at every point x ∈ A_1.

=== Definition 2 ===
Let M_1 = ( A_1, d_1 ) and M_2 = ( A_2, d_2 ) be metric spaces.

Let f: A_1 → A_2 be a mapping from A_1 to A_2.


f is continuous from ( A_1, d_1 ) to ( A_2, d_2 )  if and only if :
:for every U ⊆ A_2 which is open in M_2, f^-1[ U ] is open in M_1.


By definition, this is equivalent to the continuity of f with respect to the induced topologies on A_1 and A_2.",Continuous
['Definitions/Continuity'],Definition:Continuous,"Let $T_1 = left({S_1, tau_1}right)$ and $T_2 = left({S_2, tau_2}right)$ be topological spaces.

Let $A, B subseteq S_1$ be subsets of $S_1$ such that $A subseteq B$.

Let $f: A to S_2$ and $g: B to S_2$ be continuous mappings.


Then $g$ is a continuous extension of $f$  if and only if :
:$forall s in A: f left({s}right) = g left({s}right)$


That is, a continuous extension of $f$ is a continuous mapping on a superset which agrees with $f$ on the domain of $f$.


Simply, it is a continuous mapping which is an extension.


=== Real Function ===
Let $A$, $B subseteq mathbb R$ be subsets of the real numbers such that $A subseteq B$.

Let $f: A to mathbb R$ and $g: B to mathbb R$ be continuous real functions.


Then $g$ is a continuous extension of $f$  if and only if :
:$forall x in A : f left(   right)x = g left(   right)x$


Category:Definitions/Continuous Real Functions

Category:Definitions/Continuity",Definition:Continuous Extension,,false,"Let T_1 = (S_1, τ_1) and T_2 = (S_2, τ_2) be topological spaces.

Let A, B ⊆ S_1 be subsets of S_1 such that A ⊆ B.

Let f: A → S_2 and g: B → S_2 be continuous mappings.


Then g is a continuous extension of f  if and only if :
:∀ s ∈ A: f (s) = g (s)


That is, a continuous extension of f is a continuous mapping on a superset which agrees with f on the domain of f.


Simply, it is a continuous mapping which is an extension.


=== Real Function ===
Let A, B ⊆ℝ be subsets of the real numbers such that A ⊆ B.

Let f: A →ℝ and g: B →ℝ be continuous real functions.


Then g is a continuous extension of f  if and only if :
:∀ x ∈ A : f (   )x = g (   )x


Category:Definitions/Continuous Real Functions

Category:Definitions/Continuity",Continuous
"['Definitions/Continuous Mappings (Topology)', 'Definitions/Continuous Mappings', 'Definitions/Continuity', 'Definitions/Topology']",Definition:Continuous,"Let $T_1 = left( S_1, tau_1 right)$ and $T_2 = left( S_2, tau_2 right)$ be topological spaces.

Let $f: S_1 to S_2$ be a mapping from $S_1$ to $S_2$.",Definition:Continuous Mapping (Topology),,false,"Let T_1 = ( S_1, τ_1 ) and T_2 = ( S_2, τ_2 ) be topological spaces.

Let f: S_1 → S_2 be a mapping from S_1 to S_2.",Continuous
"['Definitions/Category Theory', 'Definitions/Limits and Colimits']",Definition:Continuous,"Let $mathbf C$, $mathbf D$ be metacategories.

Let $F: mathbf C to mathbf D$ be a functor.


Then $F$ is continuous  if and only if  for all diagrams $D: mathbf J to mathbf C$ with limit ${varprojlim ,}_j , D_j$:

:$F left(   right){{varprojlim ,}_j , D_j} cong {varprojlim ,}_j , F D_j$

where $F D: mathbf J to mathbf D$ is the diagram obtained by composition of $F$ with $D$, and $mathbf J$ is an arbitrary metacategory.",Definition:Continuous Functor,,false,"Let 𝐂, 𝐃 be metacategories.

Let F: 𝐂→𝐃 be a functor.


Then F is continuous  if and only if  for all diagrams D: 𝐉→𝐂 with limit _j   D_j:

:F (   ) _j   D_j≅ _j   F D_j

where F D: 𝐉→𝐃 is the diagram obtained by composition of F with D, and 𝐉 is an arbitrary metacategory.",Continuous
['Definitions/Geometric Figures'],Definition:Convex,"A geometric figure is convex  if and only if  a line segment joining two points on its boundary lies entirely inside it.


=== Convex Polygon ===
Let $P$ be a polygon.

$P$ is a convex polygon  if and only if :
:For all points $A$ and $B$ located inside $P$, the line $AB$ is also inside $P$.

=== Convex Polyhedron ===
Let $P$ be a polyhedron.


$P$ is a convex polyhedron  if and only if :
:For all points $A$ and $B$ located inside $P$, the line $AB$ is also inside $P$.",Definition:Convex Geometric Figure,,false,"A geometric figure is convex  if and only if  a line segment joining two points on its boundary lies entirely inside it.


=== Convex Polygon ===
Let P be a polygon.

P is a convex polygon  if and only if :
:For all points A and B located inside P, the line AB is also inside P.

=== Convex Polyhedron ===
Let P be a polyhedron.


P is a convex polyhedron  if and only if :
:For all points A and B located inside P, the line AB is also inside P.",Convex
"['Definitions/Convex Polygons', 'Definitions/Polygons']",Definition:Convex,"=== Definition 1 ===
Let $P$ be a polygon.

$P$ is a convex polygon  if and only if :
:For all points $A$ and $B$ located inside $P$, the line $AB$ is also inside $P$.

=== Definition 2 ===
Let $P$ be a polygon.

$P$ is a convex polygon  if and only if :
:every internal angle of $P$ is not greater than $180 ^circ$.

=== Definition 3 ===
Let $P$ be a polygon.

$P$ is a convex polygon  if and only if :
:the region enclosed by $P$ lies entirely on the same side of each side of $P$


 

=== Definition 4 ===
Let $P$ be a polygon.

$P$ is a convex polygon  if and only if :
:the region enclosed by $P$ is the intersection of a finite number of half-planes.


Note that an intersection of a finite number of half-planes is not necessarily a polygon.

 

=== Definition 5 ===
Let $P$ be a polygon.

$P$ is a convex polygon  if and only if :
:the region enclosed by $P$ is the intersection of all half-planes that contain $P$ and that are created by all the lines that are tangent to $P$.



By tangent we mean any line $l$ that contain one or more point of $P$ and has $P$ entirely in one of the half-planes created by $l$. 

In this sense any line, that is spanned by a side of $P$, is tangent to $P$.",Definition:Convex Polygon,,false,"=== Definition 1 ===
Let P be a polygon.

P is a convex polygon  if and only if :
:For all points A and B located inside P, the line AB is also inside P.

=== Definition 2 ===
Let P be a polygon.

P is a convex polygon  if and only if :
:every internal angle of P is not greater than 180 ^∘.

=== Definition 3 ===
Let P be a polygon.

P is a convex polygon  if and only if :
:the region enclosed by P lies entirely on the same side of each side of P


 

=== Definition 4 ===
Let P be a polygon.

P is a convex polygon  if and only if :
:the region enclosed by P is the intersection of a finite number of half-planes.


Note that an intersection of a finite number of half-planes is not necessarily a polygon.

 

=== Definition 5 ===
Let P be a polygon.

P is a convex polygon  if and only if :
:the region enclosed by P is the intersection of all half-planes that contain P and that are created by all the lines that are tangent to P.



By tangent we mean any line l that contain one or more point of P and has P entirely in one of the half-planes created by l. 

In this sense any line, that is spanned by a side of P, is tangent to P.",Convex
"['Definitions/Convex Polyhedra', 'Definitions/Polyhedra']",Definition:Convex,"Let $P$ be a polyhedron.


=== Definition 1 ===
Let $P$ be a polyhedron.


$P$ is a convex polyhedron  if and only if :
:For all points $A$ and $B$ located inside $P$, the line $AB$ is also inside $P$.

=== Definition 2 ===
Let $P$ be a polyhedron.


$P$ is a convex polyhedron  if and only if :
:For every face of $P$, the plane in which it is embedded does not intersect the interior of $P$.

=== Definition 3 ===
Let $P$ be a polyhedron.


$P$ is a convex polyhedron  if and only if :
:For each face of $P$, the whole of $P$ lies on one side of the plane of that face.",Definition:Convex Polyhedron,,false,"Let P be a polyhedron.


=== Definition 1 ===
Let P be a polyhedron.


P is a convex polyhedron  if and only if :
:For all points A and B located inside P, the line AB is also inside P.

=== Definition 2 ===
Let P be a polyhedron.


P is a convex polyhedron  if and only if :
:For every face of P, the plane in which it is embedded does not intersect the interior of P.

=== Definition 3 ===
Let P be a polyhedron.


P is a convex polyhedron  if and only if :
:For each face of P, the whole of P lies on one side of the plane of that face.",Convex
"['Definitions/Order Theory', 'Definitions/Convex Sets (Order Theory)']",Definition:Convex,"=== Definition 1 ===
A subset $A$ of an ordered set $left( S, preceq right)$ is convex (in $S$)  if and only if :
:$forall x, y in A: forall z in S: x preceq z preceq y implies z in A$

=== Definition 2 ===
A subset $A$ of an ordered set $left( S, preceq right)$ is convex (in $S$)  if and only if :
:$forall x, y in A: forall z in S: x prec z prec y implies z in A$",Definition:Convex Set (Order Theory),,false,"=== Definition 1 ===
A subset A of an ordered set ( S, ≼) is convex (in S)  if and only if :
:∀ x, y ∈ A: ∀ z ∈ S: x ≼ z ≼ y  z ∈ A

=== Definition 2 ===
A subset A of an ordered set ( S, ≼) is convex (in S)  if and only if :
:∀ x, y ∈ A: ∀ z ∈ S: x ≺ z ≺ y  z ∈ A",Convex
"['Definitions/Vector Spaces', 'Definitions/Convex Sets (Vector Spaces)', 'Definitions/Convex Analysis']",Definition:Convex,"Let $Bbb F in leftlbrace mathbb R, mathbb C rightrbrace$.

Let $V$ be a vector space over $Bbb F$.

Let $C subseteq V$.

=== Definition 1 ===
Let $Bbb F in leftlbrace mathbb R, mathbb C rightrbrace$.

Let $V$ be a vector space over $Bbb F$.

Let $C subseteq V$.


We say that $C$ is convex  if and only if :

:$t x + left( 1 - t right) y in C$

for each $x, y in C$ and $t in left[ 0 ,.,.,   right]1$.

=== Definition 2 ===
Let $Bbb F in leftlbrace mathbb R, mathbb C rightrbrace$.

Let $V$ be a vector space over $Bbb F$.

Let $C subseteq V$.


We say that $C$ is convex  if and only if :

:$t C + left( 1 - t right) C subseteq C$

for each $t in left[ 0 ,.,.,   right]1$, where $t C + left( 1 - t right) C$ denotes a linear combination of subsets.


=== Line Segment ===
Let $V$ be a vector space over $mathbb R$ or $mathbb C$.

Let $x, y in V$.


The set:

:$leftlbrace t x + left( 1 - t right) y: t in left[ 0 ,.,.,   right]1 rightrbrace$ 

is called the (straight) line segment joining $x$ and $y$.


A convex set can thus be described as a set containing all straight line segments between its elements.",Definition:Convex Set (Vector Space),,false,"Let F ∈{ℝ, ℂ}.

Let V be a vector space over F.

Let C ⊆ V.

=== Definition 1 ===
Let F ∈{ℝ, ℂ}.

Let V be a vector space over F.

Let C ⊆ V.


We say that C is convex  if and only if :

:t x + ( 1 - t ) y ∈ C

for each x, y ∈ C and t ∈[ 0  . . ]1.

=== Definition 2 ===
Let F ∈{ℝ, ℂ}.

Let V be a vector space over F.

Let C ⊆ V.


We say that C is convex  if and only if :

:t C + ( 1 - t ) C ⊆ C

for each t ∈[ 0  . . ]1, where t C + ( 1 - t ) C denotes a linear combination of subsets.


=== Line Segment ===
Let V be a vector space over ℝ or ℂ.

Let x, y ∈ V.


The set:

:{ t x + ( 1 - t ) y: t ∈[ 0  . . ]1 } 

is called the (straight) line segment joining x and y.


A convex set can thus be described as a set containing all straight line segments between its elements.",Convex
"['Definitions/Convex Real Functions', 'Definitions/Real Functions', 'Definitions/Real Analysis']",Definition:Convex,"Let $f$ be a real function which is defined on a real interval $I$.

=== Definition 1 ===
Let $f$ be a real function which is defined on a real interval $I$.


$f$ is convex on $I$  if and only if :

:$forall x, y in I: forall alpha, beta in mathbb R_{>0}, alpha + beta = 1: f left(   right){alpha x + beta y} le alpha f left(   right)x + beta f left(   right)y$


:


The geometric interpretation is that any point on the chord drawn on the graph of any convex function always lies on or above the graph.


=== Strictly Convex ===
Let $f$ be a real function which is defined on a real interval $I$.


$f$ is strictly convex on $I$  if and only if :

:$forall x, y in I, x ne y: forall alpha, beta in mathbb R_{>0}, alpha + beta = 1: f left(   right){alpha x + beta y} < alpha f left(   right)x + beta f left(   right)y$


:


The geometric interpretation is that any point on the chord drawn on the graph of any convex function always lies above the graph.

=== Definition 2 ===
Let $f$ be a real function which is defined on a real interval $I$.


$f$ is convex on $I$  if and only if :

:$forall x_1, x_2, x_3 in I: x_1 < x_2 < x_3: dfrac {f left(   right){x_2} - f left(   right){x_1} } {x_2 - x_1} le dfrac {f left(   right){x_3} - f left(   right){x_2} } {x_3 - x_2}$


Hence a geometrical interpretation: the slope of $P_1 P_2$ is less than or equal to that of $P_2 P_3$:


:


=== Strictly Convex ===
Let $f$ be a real function which is defined on a real interval $I$.


$f$ is strictly convex on $I$  if and only if :

:$forall x_1, x_2, x_3 in I: x_1 < x_2 < x_3: dfrac {f left({x_2}right) - f left({x_1}right)} {x_2 - x_1} < dfrac {f left({x_3}right) - f left({x_2}right)} {x_3 - x_2}$


Hence a geometrical interpretation: the slope of $P_1 P_2$ is less than that of $P_2 P_3$:


:

=== Definition 3 ===
Let $f$ be a real function which is defined on a real interval $I$.


$f$ is convex on $I$  if and only if :

:$forall x_1, x_2, x_3 in I: x_1 < x_2 < x_3: dfrac {f left(   right){x_2} - f left(   right){x_1} } {x_2 - x_1} le dfrac {f left(   right){x_3} - f left(   right){x_1} } {x_3 - x_1}$


Hence a geometrical interpretation: the slope of $P_1 P_2$ is less than or equal to that of $P_1 P_3$:


:


=== Strictly Convex ===
Let $f$ be a real function which is defined on a real interval $I$.


$f$ is strictly convex on $I$  if and only if :

:$forall x_1, x_2, x_3 in I: x_1 < x_2 < x_3: dfrac {f left({x_2}right) - f left({x_1}right)} {x_2 - x_1} < dfrac {f left({x_3}right) - f left({x_1}right)} {x_3 - x_1}$


Hence a geometrical interpretation: the slope of $P_1 P_2$ is less than that of $P_1 P_3$:


:",Definition:Convex Real Function,,false,"Let f be a real function which is defined on a real interval I.

=== Definition 1 ===
Let f be a real function which is defined on a real interval I.


f is convex on I  if and only if :

:∀ x, y ∈ I: ∀α, β∈ℝ_>0, α + β = 1: f (   )α x + β y≤α f (   )x + β f (   )y


:


The geometric interpretation is that any point on the chord drawn on the graph of any convex function always lies on or above the graph.


=== Strictly Convex ===
Let f be a real function which is defined on a real interval I.


f is strictly convex on I  if and only if :

:∀ x, y ∈ I, x  y: ∀α, β∈ℝ_>0, α + β = 1: f (   )α x + β y < α f (   )x + β f (   )y


:


The geometric interpretation is that any point on the chord drawn on the graph of any convex function always lies above the graph.

=== Definition 2 ===
Let f be a real function which is defined on a real interval I.


f is convex on I  if and only if :

:∀ x_1, x_2, x_3 ∈ I: x_1 < x_2 < x_3: f (   )x_2 - f (   )x_1x_2 - x_1≤f (   )x_3 - f (   )x_2x_3 - x_2


Hence a geometrical interpretation: the slope of P_1 P_2 is less than or equal to that of P_2 P_3:


:


=== Strictly Convex ===
Let f be a real function which is defined on a real interval I.


f is strictly convex on I  if and only if :

:∀ x_1, x_2, x_3 ∈ I: x_1 < x_2 < x_3: f (x_2) - f (x_1)x_2 - x_1 < f (x_3) - f (x_2)x_3 - x_2


Hence a geometrical interpretation: the slope of P_1 P_2 is less than that of P_2 P_3:


:

=== Definition 3 ===
Let f be a real function which is defined on a real interval I.


f is convex on I  if and only if :

:∀ x_1, x_2, x_3 ∈ I: x_1 < x_2 < x_3: f (   )x_2 - f (   )x_1x_2 - x_1≤f (   )x_3 - f (   )x_1x_3 - x_1


Hence a geometrical interpretation: the slope of P_1 P_2 is less than or equal to that of P_1 P_3:


:


=== Strictly Convex ===
Let f be a real function which is defined on a real interval I.


f is strictly convex on I  if and only if :

:∀ x_1, x_2, x_3 ∈ I: x_1 < x_2 < x_3: f (x_2) - f (x_1)x_2 - x_1 < f (x_3) - f (x_1)x_3 - x_1


Hence a geometrical interpretation: the slope of P_1 P_2 is less than that of P_1 P_3:


:",Convex
"['Definitions/Order Theory', 'Definitions/Convex Sets (Order Theory)']",Definition:Convex Set,"=== Definition 1 ===
A subset $A$ of an ordered set $left( S, preceq right)$ is convex (in $S$)  if and only if :
:$forall x, y in A: forall z in S: x preceq z preceq y implies z in A$

=== Definition 2 ===
A subset $A$ of an ordered set $left( S, preceq right)$ is convex (in $S$)  if and only if :
:$forall x, y in A: forall z in S: x prec z prec y implies z in A$",Definition:Convex Set (Order Theory),,false,"=== Definition 1 ===
A subset A of an ordered set ( S, ≼) is convex (in S)  if and only if :
:∀ x, y ∈ A: ∀ z ∈ S: x ≼ z ≼ y  z ∈ A

=== Definition 2 ===
A subset A of an ordered set ( S, ≼) is convex (in S)  if and only if :
:∀ x, y ∈ A: ∀ z ∈ S: x ≺ z ≺ y  z ∈ A",Convex Set
"['Definitions/Vector Spaces', 'Definitions/Convex Sets (Vector Spaces)', 'Definitions/Convex Analysis']",Definition:Convex Set,"Let $Bbb F in leftlbrace mathbb R, mathbb C rightrbrace$.

Let $V$ be a vector space over $Bbb F$.

Let $C subseteq V$.

=== Definition 1 ===
Let $Bbb F in leftlbrace mathbb R, mathbb C rightrbrace$.

Let $V$ be a vector space over $Bbb F$.

Let $C subseteq V$.


We say that $C$ is convex  if and only if :

:$t x + left( 1 - t right) y in C$

for each $x, y in C$ and $t in left[ 0 ,.,.,   right]1$.

=== Definition 2 ===
Let $Bbb F in leftlbrace mathbb R, mathbb C rightrbrace$.

Let $V$ be a vector space over $Bbb F$.

Let $C subseteq V$.


We say that $C$ is convex  if and only if :

:$t C + left( 1 - t right) C subseteq C$

for each $t in left[ 0 ,.,.,   right]1$, where $t C + left( 1 - t right) C$ denotes a linear combination of subsets.


=== Line Segment ===
Let $V$ be a vector space over $mathbb R$ or $mathbb C$.

Let $x, y in V$.


The set:

:$leftlbrace t x + left( 1 - t right) y: t in left[ 0 ,.,.,   right]1 rightrbrace$ 

is called the (straight) line segment joining $x$ and $y$.


A convex set can thus be described as a set containing all straight line segments between its elements.",Definition:Convex Set (Vector Space),,false,"Let F ∈{ℝ, ℂ}.

Let V be a vector space over F.

Let C ⊆ V.

=== Definition 1 ===
Let F ∈{ℝ, ℂ}.

Let V be a vector space over F.

Let C ⊆ V.


We say that C is convex  if and only if :

:t x + ( 1 - t ) y ∈ C

for each x, y ∈ C and t ∈[ 0  . . ]1.

=== Definition 2 ===
Let F ∈{ℝ, ℂ}.

Let V be a vector space over F.

Let C ⊆ V.


We say that C is convex  if and only if :

:t C + ( 1 - t ) C ⊆ C

for each t ∈[ 0  . . ]1, where t C + ( 1 - t ) C denotes a linear combination of subsets.


=== Line Segment ===
Let V be a vector space over ℝ or ℂ.

Let x, y ∈ V.


The set:

:{ t x + ( 1 - t ) y: t ∈[ 0  . . ]1 } 

is called the (straight) line segment joining x and y.


A convex set can thus be described as a set containing all straight line segments between its elements.",Convex Set
"['Definitions/Convolution Integrals', 'Definitions/Integral Calculus']",Definition:Convolution,"Let $f$ and $g$ be real functions which are integrable.

The convolution integral of $f$ and $g$ is defined as:
:$ds f left(   right)t * g left(   right)t := int_{-infty}^infty f left(   right)u g left(   right){t - u} ,mathrm d u$


=== Positive Real Domain ===
Let $f$ and $g$ be functions which are integrable.

Let $f$ and $g$ be supported on the positive real numbers $mathbb R_{ge 0}$ only.

The convolution integral of $f$ and $g$ may be defined as:
:$ds f left(   right)t * g left(   right)t := int_0^t f left(   right)u g left(   right){t - u} ,mathrm d u$

=== Cross-Correlation ===
Let $f$ and $g$ be real functions which are integrable.


The cross-correlation of $f$ and $g$ is defined as:
:$ds f left(   right)t star g left(   right)t := int_{-infty}^infty f left(   right)u g left(   right){t + u} ,mathrm d u$",Definition:Convolution Integral,,false,"Let f and g be real functions which are integrable.

The convolution integral of f and g is defined as:
:f (   )t * g (   )t := ∫_-∞^∞ f (   )u g (   )t - u d u


=== Positive Real Domain ===
Let f and g be functions which are integrable.

Let f and g be supported on the positive real numbers ℝ_≥ 0 only.

The convolution integral of f and g may be defined as:
:f (   )t * g (   )t := ∫_0^t f (   )u g (   )t - u d u

=== Cross-Correlation ===
Let f and g be real functions which are integrable.


The cross-correlation of f and g is defined as:
:f (   )t ⋆ g (   )t := ∫_-∞^∞ f (   )u g (   )t + u d u",Convolution
['Definitions/Measure Theory'],Definition:Convolution,"Let $mathcal B^n$ be the Borel $sigma$-algebra on $mathbb R^n$, and let $lambda^n$ be Lebesgue measure on $mathbb R^n$.


=== Convolution of Measurable Functions ===
Let $mathcal B^n$ be the Borel $sigma$-algebra on $mathbb R^n$, and let $lambda^n$ be Lebesgue measure on $mathbb R^n$.

Let $f, g: mathbb R^n to mathbb R$ be $mathcal B^n$-measurable functions such that for all $x in mathbb R^n$:

:$ds int_{mathbb R^n} f left(   right){x - y} g left(   right)y ,mathrm d lambda^n left(   right)y$

is finite.


The convolution of $f$ and $g$, denoted $f * g$, is the mapping defined by:

:$ds f * g: mathbb R^n to mathbb R, f * g left(   right)x := int_{mathbb R^n} f left(   right){x - y} g left(   right)y ,mathrm d lambda^n left(   right)y$

=== Convolution of Measurable Function and Measure ===
Let $mu$ be a measure on the Borel $sigma$-algebra $mathcal B^n$ on $mathbb R^n$.

Let $f: mathbb R^n to mathbb R$ be a $mathcal B^n$-measurable function such that for all $x in mathbb R^n$:

:$ds int_{mathbb R^n} f left(   right){x - y} ,mathrm d mu left(   right)y$

is finite.


The convolution of $f$ and $mu$ is the mapping $f * mu: mathbb R^n to mathbb R$ defined as:

:$ds forall x in mathbb R^n: f * mu left(   right)x := int_{mathbb R^n} f left(   right){x - y} ,mathrm d mu left(   right)y$

=== Convolution of Measures ===
Let $mu$ and $nu$ be measures on the Borel $sigma$-algebra $mathcal B^n$ on $mathbb R^n$.


The convolution of $mu$ and $nu$, denoted $mu * nu$, is the measure defined by:

:$ds mu * nu: mathcal B^n to overline mathbb R, mu * nu left(   right)B := int chi_B left(   right){x + y} ,mathrm d mu left(   right)x ,mathrm d nu left(   right)y$
where $chi_B$ is the characteristic function of $B$.",Definition:Convolution (Measure Theory),,false,"Let ℬ^n be the Borel σ-algebra on ℝ^n, and let λ^n be Lebesgue measure on ℝ^n.


=== Convolution of Measurable Functions ===
Let ℬ^n be the Borel σ-algebra on ℝ^n, and let λ^n be Lebesgue measure on ℝ^n.

Let f, g: ℝ^n →ℝ be ℬ^n-measurable functions such that for all x ∈ℝ^n:

:∫_ℝ^n f (   )x - y g (   )y  dλ^n (   )y

is finite.


The convolution of f and g, denoted f * g, is the mapping defined by:

:f * g: ℝ^n →ℝ, f * g (   )x := ∫_ℝ^n f (   )x - y g (   )y  dλ^n (   )y

=== Convolution of Measurable Function and Measure ===
Let μ be a measure on the Borel σ-algebra ℬ^n on ℝ^n.

Let f: ℝ^n →ℝ be a ℬ^n-measurable function such that for all x ∈ℝ^n:

:∫_ℝ^n f (   )x - y dμ(   )y

is finite.


The convolution of f and μ is the mapping f * μ: ℝ^n →ℝ defined as:

:∀ x ∈ℝ^n: f * μ(   )x := ∫_ℝ^n f (   )x - y dμ(   )y

=== Convolution of Measures ===
Let μ and ν be measures on the Borel σ-algebra ℬ^n on ℝ^n.


The convolution of μ and ν, denoted μ * ν, is the measure defined by:

:μ * ν: ℬ^n →R, μ * ν(   )B := ∫χ_B (   )x + y dμ(   )x  dν(   )y
where χ_B is the characteristic function of B.",Convolution
['Definitions/Measure Theory'],Definition:Convolution,"Let $mathcal B^n$ be the Borel $sigma$-algebra on $mathbb R^n$, and let $lambda^n$ be Lebesgue measure on $mathbb R^n$.

Let $f, g: mathbb R^n to mathbb R$ be $mathcal B^n$-measurable functions such that for all $x in mathbb R^n$:

:$ds int_{mathbb R^n} f left(   right){x - y} g left(   right)y ,mathrm d lambda^n left(   right)y$

is finite.


The convolution of $f$ and $g$, denoted $f * g$, is the mapping defined by:

:$ds f * g: mathbb R^n to mathbb R, f * g left(   right)x := int_{mathbb R^n} f left(   right){x - y} g left(   right)y ,mathrm d lambda^n left(   right)y$",Definition:Convolution of Measurable Functions,,false,"Let ℬ^n be the Borel σ-algebra on ℝ^n, and let λ^n be Lebesgue measure on ℝ^n.

Let f, g: ℝ^n →ℝ be ℬ^n-measurable functions such that for all x ∈ℝ^n:

:∫_ℝ^n f (   )x - y g (   )y  dλ^n (   )y

is finite.


The convolution of f and g, denoted f * g, is the mapping defined by:

:f * g: ℝ^n →ℝ, f * g (   )x := ∫_ℝ^n f (   )x - y g (   )y  dλ^n (   )y",Convolution
['Definitions/Measure Theory'],Definition:Convolution,"Let $mu$ be a measure on the Borel $sigma$-algebra $mathcal B^n$ on $mathbb R^n$.

Let $f: mathbb R^n to mathbb R$ be a $mathcal B^n$-measurable function such that for all $x in mathbb R^n$:

:$ds int_{mathbb R^n} f left(   right){x - y} ,mathrm d mu left(   right)y$

is finite.


The convolution of $f$ and $mu$ is the mapping $f * mu: mathbb R^n to mathbb R$ defined as:

:$ds forall x in mathbb R^n: f * mu left(   right)x := int_{mathbb R^n} f left(   right){x - y} ,mathrm d mu left(   right)y$",Definition:Convolution of Measurable Function and Measure,,false,"Let μ be a measure on the Borel σ-algebra ℬ^n on ℝ^n.

Let f: ℝ^n →ℝ be a ℬ^n-measurable function such that for all x ∈ℝ^n:

:∫_ℝ^n f (   )x - y dμ(   )y

is finite.


The convolution of f and μ is the mapping f * μ: ℝ^n →ℝ defined as:

:∀ x ∈ℝ^n: f * μ(   )x := ∫_ℝ^n f (   )x - y dμ(   )y",Convolution
['Definitions/Measure Theory'],Definition:Convolution,"Let $mu$ and $nu$ be measures on the Borel $sigma$-algebra $mathcal B^n$ on $mathbb R^n$.


The convolution of $mu$ and $nu$, denoted $mu * nu$, is the measure defined by:

:$ds mu * nu: mathcal B^n to overline mathbb R, mu * nu left(   right)B := int chi_B left(   right){x + y} ,mathrm d mu left(   right)x ,mathrm d nu left(   right)y$
where $chi_B$ is the characteristic function of $B$.",Definition:Convolution of Measures,,false,"Let μ and ν be measures on the Borel σ-algebra ℬ^n on ℝ^n.


The convolution of μ and ν, denoted μ * ν, is the measure defined by:

:μ * ν: ℬ^n →R, μ * ν(   )B := ∫χ_B (   )x + y dμ(   )x  dν(   )y
where χ_B is the characteristic function of B.",Convolution
"['Definitions/Analytic Number Theory', 'Definitions/Dirichlet Convolution', 'Definitions/Number Theory']",Definition:Convolution,"Let $f, g$ be arithmetic functions.


=== Definition 1 ===
Let $f, g$ be arithmetic functions.


The Dirichlet convolution of $f$ and $g$ is the arithmetic function:
:$ds left( f * g right)  left(   right)n = sum_{d mathop mathrel backslash n} f left(   right)d g left(   right){frac n d}$
where the summation runs over the set of positive divisors $d$ of $n$.

=== Definition 2 ===
Let $f, g$ be arithmetic functions.


The Dirichlet convolution of $f$ and $g$ is the arithmetic function:
:$ds left( f * g right)  left(   right)n = sum_{a b mathop = n} f left(   right)a g left(   right)b$
where the summation runs over all pairs of positive integers $left( a, b right)$ with $a b = n$.",Definition:Dirichlet Convolution,,false,"Let f, g be arithmetic functions.


=== Definition 1 ===
Let f, g be arithmetic functions.


The Dirichlet convolution of f and g is the arithmetic function:
:( f * g )  (   )n = ∑_d  n f (   )d g (   )n/d
where the summation runs over the set of positive divisors d of n.

=== Definition 2 ===
Let f, g be arithmetic functions.


The Dirichlet convolution of f and g is the arithmetic function:
:( f * g )  (   )n = ∑_a b  = n f (   )a g (   )b
where the summation runs over all pairs of positive integers ( a, b ) with a b = n.",Convolution
['Definitions/Monoids'],Definition:Convolution,"Let $left( M, cdot right)$ be a divisor-finite monoid.

Let $left( R, +, times right)$ be a non-associative ring.

Let $f, g : M to R$ be mappings.


The convolution of $f$ and $g$ is the mapping $f * g: M to R$ defined as:
:$forall m in M: left( f * g right)  left(   right)m := ds sum_{x y mathop = m} f left(   right)x times g left(   right)y$
where the summation is over the finite set $leftlbrace left( x, y right) in M^2: x y = m rightrbrace$.",Definition:Convolution of Mappings on Divisor-Finite Monoid,,false,"Let ( M, ·) be a divisor-finite monoid.

Let ( R, +, ×) be a non-associative ring.

Let f, g : M → R be mappings.


The convolution of f and g is the mapping f * g: M → R defined as:
:∀ m ∈ M: ( f * g )  (   )m := ∑_x y  = m f (   )x × g (   )y
where the summation is over the finite set {( x, y ) ∈ M^2: x y = m }.",Convolution
['Definitions/Coordinate Systems'],Definition:Coordinate,"Let $leftlangle a_n rightrangle$ be a coordinate system of a unitary $R$-module $G$.

Let $ds x in G: x = sum_{k mathop = 1}^n lambda_k a_k$.

The scalars $lambda_1, lambda_2, ldots, lambda_n$ can be referred to as the coordinates of $x$ relative to $leftlangle a_n rightrangle$.


=== Elements of Ordered Pair ===
Let $left( a, b right)$ be an ordered pair.

The following terminology is used:
:$a$ is called the first coordinate
:$b$ is called the second coordinate.

This definition is compatible with the equivalent definition in the context of Cartesian coordinate systems.",Definition:Coordinate System/Coordinate,,false,"Let ⟨ a_n ⟩ be a coordinate system of a unitary R-module G.

Let x ∈ G: x = ∑_k  = 1^n λ_k a_k.

The scalars λ_1, λ_2, …, λ_n can be referred to as the coordinates of x relative to ⟨ a_n ⟩.


=== Elements of Ordered Pair ===
Let ( a, b ) be an ordered pair.

The following terminology is used:
:a is called the first coordinate
:b is called the second coordinate.

This definition is compatible with the equivalent definition in the context of Cartesian coordinate systems.",Coordinate
['Definitions/Cartesian Product'],Definition:Coordinate,"Let $ds prod_{i mathop in I} S_i$ be a cartesian product.

Let $j in I$, and let $s = leftlangle s_i rightrangle_{i mathop in I} in ds prod_{i mathop in I} S_i$.


Then $s_j$ is called the $j$th coordinate of $s$.


If the indexing set $I$ consists of ordinary numbers $1, 2, ldots, n$, one speaks about, for example, the first, second, or $n$th coordinate.

For an element $left( s, t right) in S times T$ of a binary cartesian product, $s$ is the first coordinate, and $t$ is the second coordinate.",Definition:Cartesian Product/Coordinate,,false,"Let ∏_i ∈ I S_i be a cartesian product.

Let j ∈ I, and let s = ⟨ s_i ⟩_i ∈ I∈∏_i ∈ I S_i.


Then s_j is called the jth coordinate of s.


If the indexing set I consists of ordinary numbers 1, 2, …, n, one speaks about, for example, the first, second, or nth coordinate.

For an element ( s, t ) ∈ S × T of a binary cartesian product, s is the first coordinate, and t is the second coordinate.",Coordinate
"['Definitions/Coterminal Angles', 'Definitions/Angles']",Definition:Coterminal,"Coterminal angles are angles which are rotations between the same $2$ lines.

That is, they are angles with the same arms.",Definition:Coterminal Angles,angles,true,"Coterminal angles are angles which are rotations between the same 2 lines.

That is, they are angles with the same arms.",Coterminal
['Definitions/Adjacent (Polygons)'],Definition:Coterminal,Two sides of a polygon that meet at the same vertex are adjacent to each other.,Definition:Polygon/Adjacent/Sides,,false,Two sides of a polygon that meet at the same vertex are adjacent to each other.,Coterminal
['Definitions/Adjacent (Polyhedra)'],Definition:Coterminal,Two edges of a polyhedron that intersect at a particular vertex are referred to as adjacent to each other.,Definition:Polyhedron/Adjacent/Edge to Edge,,false,Two edges of a polyhedron that intersect at a particular vertex are referred to as adjacent to each other.,Coterminal
"['Definitions/Edges of Graphs', 'Definitions/Adjacency (Graph Theory)']",Definition:Coterminal,"=== Undirected Graph ===
Let $G = left( V, E right)$ be an undirected graph.

Two edges $e_1, e_2 in E$ of $G$ adjacent  if and only if  there exists a vertex $v in V$ to which they are both incident.


Otherwise they are non-adjacent.

=== Digraph ===
Let $G = left( V, E right)$ be a digraph.

Two arcs $e_1, e_2 in E$ of $G$ adjacent  if and only if  there exists a vertex $v in V$ to which they are both incident.


Otherwise they are non-adjacent.

=== Non-Adjacent ===
Let $G = left( V, E right)$ be a graph.

Two edges $u, v in V$ of $G$ are non-adjacent  if and only if  they are not adjacent.",Definition:Adjacent (Graph Theory)/Edges,,false,"=== Undirected Graph ===
Let G = ( V, E ) be an undirected graph.

Two edges e_1, e_2 ∈ E of G adjacent  if and only if  there exists a vertex v ∈ V to which they are both incident.


Otherwise they are non-adjacent.

=== Digraph ===
Let G = ( V, E ) be a digraph.

Two arcs e_1, e_2 ∈ E of G adjacent  if and only if  there exists a vertex v ∈ V to which they are both incident.


Otherwise they are non-adjacent.

=== Non-Adjacent ===
Let G = ( V, E ) be a graph.

Two edges u, v ∈ V of G are non-adjacent  if and only if  they are not adjacent.",Coterminal
"['Definitions/Couples (Mechanics)', 'Definitions/Mechanics']",Definition:Couple,"A couple, in the context of mechanics, is a system of $2$ forces that are:
:equal in magnitude
:exactly opposite in direction
:with different lines of action.",Definition:Couple (Mechanics),,false,"A couple, in the context of mechanics, is a system of 2 forces that are:
:equal in magnitude
:exactly opposite in direction
:with different lines of action.",Couple
['Definitions/Hypocycloids'],Definition:Couple,"A Tusi couple is a hypocycloid with $2$ cusps.


:",Definition:Tusi Couple,,false,"A Tusi couple is a hypocycloid with 2 cusps.


:",Couple
['Definitions/Topology'],Definition:Critical Point,"Let $f: X to Y$ be a smooth map of manifolds.


A point $x in X$ is called a critical point of $f$  if and only if  $mathrm d f_x: T_x left[ X right] to T_y left[ Y right]$ is not surjective at $x$.

 

Category:Definitions/Topology",Definition:Critical Point (Topology),,false,"Let f: X → Y be a smooth map of manifolds.


A point x ∈ X is called a critical point of f  if and only if  d f_x: T_x [ X ] → T_y [ Y ] is not surjective at x.

 

Category:Definitions/Topology",Critical Point
['Definitions/Smooth Manifolds'],Definition:Critical Point,"Let $M$ be a smooth manifold.

Let $f in mathcal C^infty left(   right)M : M to mathbb R$ be a smooth real-valued function.

Let $p in M$ be a base point in $M$.

Let $,mathrm d f_p$ be the differential of $f$ at $p$.

Suppose $,mathrm d f_p = 0$.


Then $p$ is called a critical point of $f$.
 ",Definition:Critical Point/Smooth Manifold,,false,"Let M be a smooth manifold.

Let f ∈𝒞^∞(   )M : M →ℝ be a smooth real-valued function.

Let p ∈ M be a base point in M.

Let d f_p be the differential of f at p.

Suppose d f_p = 0.


Then p is called a critical point of f.
 ",Critical Point
"['Definitions/Riemannian Manifolds', 'Definitions/Curves']",Definition:Critical Point,"Let $left( M, g right)$ be a Riemannian manifold.

Let $I = left[ a ,.,.,   right]b$ be a closed real interval.

Let $J subseteq mathbb R$ be an open real interval.

Let $gamma : I to M$ be an admissible curve.

Let $L_g$ be the Riemannian length of some admissible curve.

Let $Gamma : J times I to M$ be the proper variation of $gamma$ such that:

:$forall s in J, forall t in I : left( s, t right) stackrel {Gamma}{mapsto} Gamma_s left(   right)t$

Suppose:

:$forall Gamma : ds dfrac d {d s} L_g left(   right){Gamma_s} = 0$


Then $gamma$ is called the critical point of $L_g$.",Definition:Critical Point of Riemannian Length,,false,"Let ( M, g ) be a Riemannian manifold.

Let I = [ a  . . ]b be a closed real interval.

Let J ⊆ℝ be an open real interval.

Let γ : I → M be an admissible curve.

Let L_g be the Riemannian length of some admissible curve.

Let Γ : J × I → M be the proper variation of γ such that:

:∀ s ∈ J, ∀ t ∈ I : ( s, t ) Γ↦Γ_s (   )t

Suppose:

:∀Γ :  d d s L_g (   )Γ_s = 0


Then γ is called the critical point of L_g.",Critical Point
"['Definitions/Critical Points (Analysis)', 'Definitions/Real Analysis']",Definition:Critical Point,"A critical point is a point on a graph where the curve has a vertical tangent.

That is, where limit of the derivative tends to infinity.",Definition:Critical Point (Analysis),point,true,"A critical point is a point on a graph where the curve has a vertical tangent.

That is, where limit of the derivative tends to infinity.",Critical Point
['Definitions/Cubes'],Definition:Cubic,Cubic is an adjective which means in the shape of a cube.,Definition:Cubic (Geometry),,false,Cubic is an adjective which means in the shape of a cube.,Cubic
"['Definitions/Cubic Equations', 'Definitions/Polynomial Equations']",Definition:Cubic,"A cubic equation is a polynomial equation of the form:
: $a x^3 + b x^2 + c x + d = 0$


=== Discriminant ===
 

=== Resolvent Equation ===
Let $P$ be the cubic equation:
:$a x^3 + b x^2 + c x + d = 0$ with $a ne 0$


Let:
:$y = x + dfrac b {3 a}$
:$Q = dfrac {3 a c - b^2} {9 a^2}$
:$R = dfrac {9 a b c - 27 a^2 d - 2 b^3} {54 a^3}$

Let $y = u + v$ where $u v = -Q$.


The resolvent equation of the cubic is given by:
:$u^6 - 2 R u^3 - Q^3$",Definition:Cubic Equation,,false,"A cubic equation is a polynomial equation of the form:
: a x^3 + b x^2 + c x + d = 0


=== Discriminant ===
 

=== Resolvent Equation ===
Let P be the cubic equation:
:a x^3 + b x^2 + c x + d = 0 with a  0


Let:
:y = x +  b 3 a
:Q = 3 a c - b^29 a^2
:R = 9 a b c - 27 a^2 d - 2 b^354 a^3

Let y = u + v where u v = -Q.


The resolvent equation of the cubic is given by:
:u^6 - 2 R u^3 - Q^3",Cubic
['Definitions/Polynomial Theory'],Definition:Cubic,A cubic polynomial is a polynomial of degree $3$.,Definition:Cubic Polynomial,,false,A cubic polynomial is a polynomial of degree 3.,Cubic
"['Definitions/Cubic Graphs', 'Definitions/Regular Graphs', 'Definitions/Graph Theory']",Definition:Cubic,"A cubic graph is a $3$-regular graph, that is, a graph whose vertices all have degree $3$.",Definition:Cubic Graph,,false,"A cubic graph is a 3-regular graph, that is, a graph whose vertices all have degree 3.",Cubic
"['Definitions/Cuts', 'Definitions/Real Analysis', 'Definitions/Order Theory']",Definition:Cut,"Let $alpha subset mathbb Q$ be a subset of the set of rational numbers $mathbb Q$ which has the following properties:

:$(1): quad alpha ne varnothing$ and $alpha ne mathbb Q$, that is: $alpha$ contains at least one rational number but not all rational numbers

:$(2): quad$ If $p in alpha$ and $q in mathbb Q$ such that $q < p$, then $q in alpha$

:$(3): quad alpha$ does not contain a greatest element.


Then $alpha$ is called a cut.


=== Lower Number ===
Let $alpha$ be a cut.

Let $p in alpha$.


Then $p$ is referred to as a lower number of $alpha$.

=== Upper Number ===
Let $alpha$ be a cut.

Let $q in mathbb Q$ such that $q notin alpha$.


Then $p$ is referred to as an upper number of $alpha$.

=== Rational Cut ===
Let $r in mathbb Q$ be a rational number.

Let $alpha$ be the cut consisting of all rational numbers $p$ such that $p < r$.


Then $alpha$ is referred to as a rational cut.


To express the fact that $alpha$ is a rational cut, the notation $alpha = r^*$ can be used.",Definition:Cut (Analysis),,false,"Let α⊂ℚ be a subset of the set of rational numbers ℚ which has the following properties:

:(1):   α∅ and αℚ, that is: α contains at least one rational number but not all rational numbers

:(2): If p ∈α and q ∈ℚ such that q < p, then q ∈α

:(3):   α does not contain a greatest element.


Then α is called a cut.


=== Lower Number ===
Let α be a cut.

Let p ∈α.


Then p is referred to as a lower number of α.

=== Upper Number ===
Let α be a cut.

Let q ∈ℚ such that q ∉α.


Then p is referred to as an upper number of α.

=== Rational Cut ===
Let r ∈ℚ be a rational number.

Let α be the cut consisting of all rational numbers p such that p < r.


Then α is referred to as a rational cut.


To express the fact that α is a rational cut, the notation α = r^* can be used.",Cut
"['Definitions/Order Theory', 'Definitions/Real Analysis', 'Definitions/Dedekind Cuts']",Definition:Cut,"Let $left( S, preceq right)$ be a totally ordered set.


=== Definition 1 ===
Let $left( S, preceq right)$ be a totally ordered set.


A Dedekind cut of $left( S, preceq right)$ is a non-empty proper subset $L subsetneq S$ such that:
:$(1): quad forall x in L: forall y in S: y prec x implies y in L$ ($L$ is a lower section in $S$)
:$(2): quad forall x in L: exists y in L: x prec y$

=== Definition 2 ===
Let $left( S, preceq right)$ be a totally ordered set.


A Dedekind cut of $left( S, preceq right)$ is an ordered pair $left( L, R right)$ such that:
:$(1): quad leftlbrace L, R rightrbrace$ is a partition of $S$.
:$(2): quad L$ does not have a greatest element.
:$(3): quad forall x in L: forall y in R: x prec y$.",Definition:Dedekind Cut,,false,"Let ( S, ≼) be a totally ordered set.


=== Definition 1 ===
Let ( S, ≼) be a totally ordered set.


A Dedekind cut of ( S, ≼) is a non-empty proper subset L ⊊ S such that:
:(1):   ∀ x ∈ L: ∀ y ∈ S: y ≺ x  y ∈ L (L is a lower section in S)
:(2):   ∀ x ∈ L: ∃ y ∈ L: x ≺ y

=== Definition 2 ===
Let ( S, ≼) be a totally ordered set.


A Dedekind cut of ( S, ≼) is an ordered pair ( L, R ) such that:
:(1):   { L, R } is a partition of S.
:(2):    L does not have a greatest element.
:(3):   ∀ x ∈ L: ∀ y ∈ R: x ≺ y.",Cut
['Definitions/Vertices of Graphs'],Definition:Cut,"Let $G = left( V, E right)$ be a connected graph.

Let $v$ be a vertex of $G$.


Then $v$ is a cut-vertex of $G$  if and only if  the vertex deletion $G - v$ is a vertex cut of $G$.

That is, such that $G - v$ is disconnected.


Thus, a cut-vertex of $G$ is a singleton vertex cut of $G$.",Definition:Cut-Vertex,,false,"Let G = ( V, E ) be a connected graph.

Let v be a vertex of G.


Then v is a cut-vertex of G  if and only if  the vertex deletion G - v is a vertex cut of G.

That is, such that G - v is disconnected.


Thus, a cut-vertex of G is a singleton vertex cut of G.",Cut
"['Definitions/Connected Spaces', 'Definitions/Disconnected Sets']",Definition:Cut,"Let $T = left( S, tau right)$ be a topological space.

Let $H subseteq S$ be a connected set in $T$ and let $p in H$.

Let $p in H$ such that $H setminus leftlbrace p rightrbrace$ is disconnected, where $setminus$ denotes set difference.


Then $p$ is a cut point of $H$.",Definition:Cut Point,,false,"Let T = ( S, τ) be a topological space.

Let H ⊆ S be a connected set in T and let p ∈ H.

Let p ∈ H such that H ∖{ p } is disconnected, where ∖ denotes set difference.


Then p is a cut point of H.",Cut
['Definitions/Vertices of Graphs'],Definition:Cut,"Let $G = left( V, E right)$ be a graph.


A vertex cut of $G$ is a set of vertices $W subseteq V left(   right)G$ such that the vertex deletion $G setminus W$ is disconnected.",Definition:Vertex Cut,,false,"Let G = ( V, E ) be a graph.


A vertex cut of G is a set of vertices W ⊆ V (   )G such that the vertex deletion G ∖ W is disconnected.",Cut
['Definitions/Edges of Graphs'],Definition:Cut,"Let $G$ be a graph.


An edge cut of $G$ is a set of edges $W subseteq E left(   right)G$ such that the edge deletion $G setminus W$ is disconnected.",Definition:Edge Cut,,false,"Let G be a graph.


An edge cut of G is a set of edges W ⊆ E (   )G such that the edge deletion G ∖ W is disconnected.",Cut
['Definitions/Permutation Theory'],Definition:Cycle,"Let $mathbb N_k$ be used to denote the initial segment of natural numbers:
:$mathbb N_k = left[ 1 ,.,.,   right]k = leftlbrace 1, 2, 3, ldots, k rightrbrace$

Let $rho: mathbb N_n to mathbb N_n$ be a permutation of $n$ letters.


The $k$-cycle $rho$ is denoted:
:$begin {pmatrix} i & rho left(   right)i & ldots & rho^{k - 1}  left(   right)i end{pmatrix}$


From Existence and Uniqueness of Cycle Decomposition, all permutations can be defined as the product of disjoint cycles.

As Disjoint Permutations Commute, the order in which they are performed does not matter.


So, for a given permutation $rho$, the cycle notation for $rho$ consists of all the disjoint cycles into which $rho$ can be decomposed, concatenated as a product.

It is conventional to omit $1$-cycles from the expression, and to write those cycles with lowest starting number first.


=== Canonical Representation ===
The permutation:

:$begin{pmatrix}
  1 & 2 & 3 & 4 & 5 \
  2 & 1 & 4 & 3 & 5 
end{pmatrix}$

can be expressed in cycle notation as:

:$begin{pmatrix} 1 & 2 end{pmatrix} begin{pmatrix} 3 & 4 end{pmatrix}$

or as:

:$begin{pmatrix} 3 & 4 end{pmatrix} begin{pmatrix} 5 end{pmatrix} begin{pmatrix} 1 & 2 end{pmatrix}$

or as:

:$begin{pmatrix} 4 & 3 end{pmatrix} begin{pmatrix} 2 & 1 end{pmatrix}$

etc.

However, only the first is conventional. This is known as the canonical representation.",Definition:Permutation on n Letters/Cycle Notation,,false,"Let ℕ_k be used to denote the initial segment of natural numbers:
:ℕ_k = [ 1  . . ]k = { 1, 2, 3, …, k }

Let ρ: ℕ_n →ℕ_n be a permutation of n letters.


The k-cycle ρ is denoted:
:[             i       ρ(   )i             … ρ^k - 1(   )i ]


From Existence and Uniqueness of Cycle Decomposition, all permutations can be defined as the product of disjoint cycles.

As Disjoint Permutations Commute, the order in which they are performed does not matter.


So, for a given permutation ρ, the cycle notation for ρ consists of all the disjoint cycles into which ρ can be decomposed, concatenated as a product.

It is conventional to omit 1-cycles from the expression, and to write those cycles with lowest starting number first.


=== Canonical Representation ===
The permutation:

:[ 1 2 3 4 5; 2 1 4 3 5 ]

can be expressed in cycle notation as:

:[ 1 2 ][ 3 4 ]

or as:

:[ 3 4 ][ 5 ][ 1 2 ]

or as:

:[ 4 3 ][ 2 1 ]

etc.

However, only the first is conventional. This is known as the canonical representation.",Cycle
"['Definitions/Cycles (Graph Theory)', 'Definitions/Circuits (Graph Theory)', 'Definitions/Paths (Graph Theory)', 'Definitions/Graph Theory']",Definition:Cycle,"A cycle is a circuit in which no vertex except the first (which is also the last) appears more than once.


An $n$-cycle is a cycle with $n$ vertices.


=== Subgraph ===
The set of vertices and edges which go to make up a cycle form a subgraph.

This subgraph itself is also referred to as a cycle.

=== Odd Cycle ===
An odd cycle is a cycle with odd length, that is, with an odd number of edges.

=== Even Cycle ===
An even cycle is a cycle with even length, that is, with an even number of edges.",Definition:Cycle (Graph Theory),circuit,true,"A cycle is a circuit in which no vertex except the first (which is also the last) appears more than once.


An n-cycle is a cycle with n vertices.


=== Subgraph ===
The set of vertices and edges which go to make up a cycle form a subgraph.

This subgraph itself is also referred to as a cycle.

=== Odd Cycle ===
An odd cycle is a cycle with odd length, that is, with an odd number of edges.

=== Even Cycle ===
An even cycle is a cycle with even length, that is, with an even number of edges.",Cycle
['Definitions/Differential Equations'],Definition:Cycle,"A cycle, or periodic solution, is a solution of a differential equation which is a periodic function.

Category:Definitions/Differential Equations",Definition:Cycle (Periodic Solution),,false,"A cycle, or periodic solution, is a solution of a differential equation which is a periodic function.

Category:Definitions/Differential Equations",Cycle
['Definitions/Continued Fractions'],Definition:Cycle,"The repeating block in a periodic (or purely periodic) continued fraction $F$ is called the cycle of $F$.


=== Cycle Length ===
The number of partial denominators in a cycle of a periodic continued fraction is called the cycle length.",Definition:Periodic Continued Fraction/Cycle,,false,"The repeating block in a periodic (or purely periodic) continued fraction F is called the cycle of F.


=== Cycle Length ===
The number of partial denominators in a cycle of a periodic continued fraction is called the cycle length.",Cycle
"['Definitions/Cyclic Groups', 'Definitions/Abelian Groups', 'Definitions/Group Theory']",Definition:Cyclic,"=== Definition 1 ===
The group $G$ is cyclic  if and only if  every element of $G$ can be expressed as the power of one element of $G$:
:$exists g in G: forall h in G: h = g^n$
for some $n in mathbb Z$.

=== Definition 2 ===
The group $G$ is cyclic  if and only if  it is generated by one element $g in G$:
:$G = {leftlangle g rightrangle}$",Definition:Cyclic Group,,false,"=== Definition 1 ===
The group G is cyclic  if and only if  every element of G can be expressed as the power of one element of G:
:∃ g ∈ G: ∀ h ∈ G: h = g^n
for some n ∈ℤ.

=== Definition 2 ===
The group G is cyclic  if and only if  it is generated by one element g ∈ G:
:G = ⟨ g ⟩",Cyclic
['Definitions/Polygons'],Definition:Cyclic,A cyclic polygon is a polygon $P$ which can be inscribed in a circle.,Definition:Cyclic Polygon,,false,A cyclic polygon is a polygon P which can be inscribed in a circle.,Cyclic
"['Definitions/Quadrilaterals', 'Definitions/Circumscribe']",Definition:Cyclic,"A cyclic quadrilateral is a quadrilateral which can be circumscribed:

:


This is an example of a quadrilateral which cannot be circumscribed, and so therefore is not cyclic:

:",Definition:Cyclic Quadrilateral,,false,"A cyclic quadrilateral is a quadrilateral which can be circumscribed:

:


This is an example of a quadrilateral which cannot be circumscribed, and so therefore is not cyclic:

:",Cyclic
['Definitions/Topology'],Definition:Decomposition,"A set $S subseteq mathbb R^n$ is decomposable in $m$ sets $A_1, ldots, A_m subset mathbb R^n$  if and only if  there exist isometries $phi_1, ldots, phi_m: mathbb R^n to mathbb R^n$ such that:

:$(1):quad ds S = bigcup_{k mathop = 1}^m phi_k left(   right){A_k}$ 
:$(2):quad forall i ne j: phi_i left(   right){A_i} cap phi_j left(   right){A_j} = varnothing$

Such a union is known as a decomposition.

 ",Definition:Decomposable Set,,false,"A set S ⊆ℝ^n is decomposable in m sets A_1, …, A_m ⊂ℝ^n  if and only if  there exist isometries ϕ_1, …, ϕ_m: ℝ^n →ℝ^n such that:

:(1):   S = ⋃_k  = 1^m ϕ_k (   )A_k 
:(2):  ∀ i  j: ϕ_i (   )A_i∩ϕ_j (   )A_j = ∅

Such a union is known as a decomposition.

 ",Decomposition
['Definitions/Internal Direct Products'],Definition:Decomposition,"Let $left( S_1, circ {restriction_{S_1} }  right), left( S_2, circ {restriction_{S_2} }  right), ldots, left( S_n, circ {restriction_{S_n} }  right)$ be closed algebraic substructures of an algebraic structure $left( S, circ right)$

where $circ {restriction_{S_1} }, circ {restriction_{S_2} }, ldots, circ {restriction_{S_n} }$ are the operations induced by the restrictions of $circ$ to $S_1, S_2, ldots, S_n$ respectively.

Let $left( S, circ right)$ be the internal direct product of $S_1$, $S_2, ldots, S_n$.


The set of algebraic substructures $left( S_1, circ {restriction_{S_1} }  right), left( S_2, circ {restriction_{S_2} }  right), ldots, left( S_n, circ {restriction_{S_n} }  right)$ whose (external) direct product is isomorphic with $left( S, circ right)$ is called a decomposition of $S$.",Definition:Internal Direct Product/Decomposition,,false,"Let ( S_1, ∘_S_1), ( S_2, ∘_S_2), …, ( S_n, ∘_S_n) be closed algebraic substructures of an algebraic structure ( S, ∘)

where ∘_S_1, ∘_S_2, …, ∘_S_n are the operations induced by the restrictions of ∘ to S_1, S_2, …, S_n respectively.

Let ( S, ∘) be the internal direct product of S_1, S_2, …, S_n.


The set of algebraic substructures ( S_1, ∘_S_1), ( S_2, ∘_S_2), …, ( S_n, ∘_S_n) whose (external) direct product is isomorphic with ( S, ∘) is called a decomposition of S.",Decomposition
['Definitions/Internal Group Direct Products'],Definition:Decomposition,"Let $left( H_1, circ {restriction_{H_1} }  right), left( H_2, circ {restriction_{H_2} }  right), ldots, left( H_n, circ {restriction_{H_n} }  right)$ be subgroups of a group $left( G, circ right)$

where $circ {restriction_{H_1} }, circ {restriction_{H_2} }, ldots, circ {restriction_{H_n} }$ are the operations induced by the restrictions of $circ$ to $H_1, H_2, ldots, H_n$ respectively.

Let $left( G, circ right)$ be the internal group direct product of $H_1$, $H_2, ldots, H_n$.


The set of subgroups $left( H_1, circ {restriction_{H_1} }  right), left( H_2, circ {restriction_{H_2} }  right), ldots, left( H_n, circ {restriction_{H_n} }  right)$ whose group direct product is isomorphic with $left( G, circ right)$ is called a decomposition of $G$.",Definition:Internal Group Direct Product/Decomposition,,false,"Let ( H_1, ∘_H_1), ( H_2, ∘_H_2), …, ( H_n, ∘_H_n) be subgroups of a group ( G, ∘)

where ∘_H_1, ∘_H_2, …, ∘_H_n are the operations induced by the restrictions of ∘ to H_1, H_2, …, H_n respectively.

Let ( G, ∘) be the internal group direct product of H_1, H_2, …, H_n.


The set of subgroups ( H_1, ∘_H_1), ( H_2, ∘_H_2), …, ( H_n, ∘_H_n) whose group direct product is isomorphic with ( G, ∘) is called a decomposition of G.",Decomposition
"['Definitions/Prime Decompositions', 'Definitions/Prime Numbers', 'Definitions/Factorization', 'Definitions/Number Theory']",Definition:Decomposition,"Let $n > 1 in mathbb Z$.


From the Fundamental Theorem of Arithmetic, $n$ has a unique factorization of the form:

 
 
 
 

where:
:$p_1 < p_2 < cdots < p_r$ are distinct primes
:$k_1, k_2, ldots, k_r$ are (strictly) positive integers.


This unique expression is known as the prime decomposition of $n$.


=== Multiplicity ===
Let $n > 1 in mathbb Z$.

Let:
:$n = p_1^{k_1} p_2^{k_2} cdots p_r^{k_r}$
be the prime decomposition of $n$, where:
:$p_1 < p_2 < cdots < p_r$ are distinct primes
:$k_1, k_2, ldots, k_r$ are (strictly) positive integers.


For each $p_j in leftlbrace p_1, p_2, ldots, p_r rightrbrace$, its power $k_j$ is known as the multiplicity of $p_j$.",Definition:Prime Decomposition,,false,"Let n > 1 ∈ℤ.


From the Fundamental Theorem of Arithmetic, n has a unique factorization of the form:

 
 
 
 

where:
:p_1 < p_2 < ⋯ < p_r are distinct primes
:k_1, k_2, …, k_r are (strictly) positive integers.


This unique expression is known as the prime decomposition of n.


=== Multiplicity ===
Let n > 1 ∈ℤ.

Let:
:n = p_1^k_1 p_2^k_2⋯ p_r^k_r
be the prime decomposition of n, where:
:p_1 < p_2 < ⋯ < p_r are distinct primes
:k_1, k_2, …, k_r are (strictly) positive integers.


For each p_j ∈{ p_1, p_2, …, p_r }, its power k_j is known as the multiplicity of p_j.",Decomposition
"['Definitions/Partial Fractions Expansions', 'Definitions/Algebra', 'Definitions/Real Analysis', 'Definitions/Analysis']",Definition:Decomposition,"Let $R left(   right)x = dfrac {P left(   right)x} {Q left(   right)x}$ be a rational function, where $P left(   right)x$ and $Q left(   right)x$ are expressible as polynomial functions.

Let $Q left(   right)x$ be expressible as:
:$Q left(   right)x = ds prod_{k mathop = 1}^n q_k left(   right)x$
where the $q_k left(   right)x$ are themselves polynomial functions of degree at least $1$.


Let $R left(   right)x$ be expressible as:
:$R left(   right)x = r left(   right)x ds sum_{k mathop = 0}^n dfrac {p_k left(   right)x} {q_k left(   right)x}$
where:
:$r left(   right)x$ is a polynomial function which may or may not be the null polynomial, or be of degree $0$ (that is, a constant)
:each of the $p_k left(   right)x$ are polynomial functions
:the degree of $p_k left(   right)x$ is strictly less than the degree of $q_k left(   right)x$ for all $k$.


Then $r left(   right)x ds sum_{k mathop = 0}^n dfrac {p_k left(   right)x} {q_k left(   right)x}$ is a partial fractions expansion of $R left(   right)x$.",Definition:Partial Fractions Expansion,,false,"Let R (   )x = P (   )xQ (   )x be a rational function, where P (   )x and Q (   )x are expressible as polynomial functions.

Let Q (   )x be expressible as:
:Q (   )x = ∏_k  = 1^n q_k (   )x
where the q_k (   )x are themselves polynomial functions of degree at least 1.


Let R (   )x be expressible as:
:R (   )x = r (   )x ∑_k  = 0^n p_k (   )xq_k (   )x
where:
:r (   )x is a polynomial function which may or may not be the null polynomial, or be of degree 0 (that is, a constant)
:each of the p_k (   )x are polynomial functions
:the degree of p_k (   )x is strictly less than the degree of q_k (   )x for all k.


Then r (   )x ∑_k  = 0^n p_k (   )xq_k (   )x is a partial fractions expansion of R (   )x.",Decomposition
['Definitions/Hilbert Spaces'],Definition:Definite,"Let $mathbb C$ be the field of complex numbers.

Let $mathrm F$ be a subfield of $mathbb C$.

Let $V$ be a vector space over $mathrm F$

Let $leftlangle cdot,   rightranglecdot: V times V to mathbb F$ be a mapping.


Then $leftlangle cdot,   rightranglecdot: V times V to mathbb F$ is non-negative definite  if and only if :

:$forall x in V: leftlangle x,   rightrangle x in mathbb R_{ge 0}$


That is, the image of $leftlangle x,   rightrangle x$ is always a non-negative real number.",Definition:Non-Negative Definite Mapping,,false,"Let ℂ be the field of complex numbers.

Let F be a subfield of ℂ.

Let V be a vector space over F

Let ⟨·,   ⟩·: V × V →𝔽 be a mapping.


Then ⟨·,   ⟩·: V × V →𝔽 is non-negative definite  if and only if :

:∀ x ∈ V: ⟨ x,   ⟩ x ∈ℝ_≥ 0


That is, the image of ⟨ x,   ⟩ x is always a non-negative real number.",Definite
"['Definitions/Definite Integrals', 'Definitions/Integral Calculus']",Definition:Definite,"Let $left[ a ,.,.,   right]b$ be a closed real interval.

Let $f: left[ a ,.,.,   right]b to mathbb R$ be a real function.


=== Riemann Integral ===
Let $left[ a ,.,.,   right]b$ be a closed real interval.

Let $f: left[ a ,.,.,   right]b to mathbb R$ be a real function.

Let $Delta$ be a finite subdivision of $left[ a ,.,.,   right]b$, $Delta = leftlbrace x_0, ldots, x_n rightrbrace$, $x_0 = a$ and $x_n = b$.

Let there for $Delta$ be a corresponding sequence $C$ of sample points $c_i$, $C = left( c_1, ldots, c_n right)$, where $c_i in left[ x_{i - 1}  ,.,.,   right]{x_i}$ for every $i in leftlbrace 1, ldots, n rightrbrace$.

Let $S left(   right){f; Delta, C}$ denote the Riemann sum of $f$ for the subdivision $Delta$ and the sample point sequence $C$.


Then $f$ is said to be (properly) Riemann integrable on $left[ a ,.,.,   right]b$  if and only if :
:$exists L in mathbb R: forall epsilon in mathbb R_{>0}: exists delta in mathbb R_{>0}: forall$ finite subdivisions $Delta$ of $left[ a ,.,.,   right]b: forall$ sample point sequences $C$ of $Delta: leftlVert Delta rightrVert < delta implies leftlvert S left(   right){f; Delta, C} - L rightrvert < epsilon$
where $leftlVert Delta rightrVert$ denotes the norm of $Delta$.


The real number $L$ is called the Riemann integral of $f$ over $left[ a ,.,.,   right]b$ and is denoted:
:$ds int_a^b f left(   right)x ,mathrm d x$


More usually (and informally), we say:
:$f$ is (Riemann) integrable over $left[ a ,.,.,   right]b$.


=== Riemann Integral as Integral Operator ===
Let $C left[ a ,.,.,   right]b$ be the space of continuous functions.

Let $x in C left[ a ,.,.,   right]b$ be a Riemann integrable function.

Let $mathbb R$ be the set of real numbers.


The Riemann integral operator, denoted by $I$, is the mapping $I : C left[ a ,.,.,   right]b to mathbb R$ such that:

:$ds I left(   right)x := int_a^b x left(   right)t ,mathrm d t$

where $ds int_a^b x left(   right)t ,mathrm d t$ is the Riemann integral.

=== Darboux Integral ===
Let $left[ a ,.,.,   right]b$ be a closed real interval.

Let $f: left[ a ,.,.,   right]b to mathbb R$ be a real function.

Let $f$ be bounded on $left[ a ,.,.,   right]b$.


Suppose that:
:$ds underline {int_a^b} f left(   right)x ,mathrm d x = overline {int_a^b} f left(   right)x ,mathrm d x$
where $ds underline {int_a^b}$ and $ds overline {int_a^b}$ denote the lower Darboux integral and upper Darboux integral, respectively.


Then the definite (Darboux) integral of $f$ over $left[ a ,.,.,   right]b$ is defined as:
:$ds int_a^b f left(   right)x ,mathrm d x = underline {int_a^b} f left(   right)x ,mathrm d x = overline {int_a^b} f left(   right)x ,mathrm d x$


$f$ is formally defined as (properly) integrable over $left[ a ,.,.,   right]b$ in the sense of Darboux, or (properly) Darboux integrable over $left[ a ,.,.,   right]b$.


More usually (and informally), we say:
:$f$ is (Darboux) integrable over $left[ a ,.,.,   right]b$.",Definition:Definite Integral,,false,"Let [ a  . . ]b be a closed real interval.

Let f: [ a  . . ]b →ℝ be a real function.


=== Riemann Integral ===
Let [ a  . . ]b be a closed real interval.

Let f: [ a  . . ]b →ℝ be a real function.

Let Δ be a finite subdivision of [ a  . . ]b, Δ = { x_0, …, x_n }, x_0 = a and x_n = b.

Let there for Δ be a corresponding sequence C of sample points c_i, C = ( c_1, …, c_n ), where c_i ∈[ x_i - 1 . . ]x_i for every i ∈{ 1, …, n }.

Let S (   )f; Δ, C denote the Riemann sum of f for the subdivision Δ and the sample point sequence C.


Then f is said to be (properly) Riemann integrable on [ a  . . ]b  if and only if :
:∃ L ∈ℝ: ∀ϵ∈ℝ_>0: ∃δ∈ℝ_>0: ∀ finite subdivisions Δ of [ a  . . ]b: ∀ sample point sequences C of Δ: ‖Δ‖ < δ| S (   )f; Δ, C - L | < ϵ
where ‖Δ‖ denotes the norm of Δ.


The real number L is called the Riemann integral of f over [ a  . . ]b and is denoted:
:∫_a^b f (   )x  d x


More usually (and informally), we say:
:f is (Riemann) integrable over [ a  . . ]b.


=== Riemann Integral as Integral Operator ===
Let C [ a  . . ]b be the space of continuous functions.

Let x ∈ C [ a  . . ]b be a Riemann integrable function.

Let ℝ be the set of real numbers.


The Riemann integral operator, denoted by I, is the mapping I : C [ a  . . ]b →ℝ such that:

:I (   )x := ∫_a^b x (   )t  d t

where ∫_a^b x (   )t  d t is the Riemann integral.

=== Darboux Integral ===
Let [ a  . . ]b be a closed real interval.

Let f: [ a  . . ]b →ℝ be a real function.

Let f be bounded on [ a  . . ]b.


Suppose that:
:∫_a^b f (   )x  d x = ∫_a^b f (   )x  d x
where ∫_a^b and ∫_a^b denote the lower Darboux integral and upper Darboux integral, respectively.


Then the definite (Darboux) integral of f over [ a  . . ]b is defined as:
:∫_a^b f (   )x  d x = ∫_a^b f (   )x  d x = ∫_a^b f (   )x  d x


f is formally defined as (properly) integrable over [ a  . . ]b in the sense of Darboux, or (properly) Darboux integrable over [ a  . . ]b.


More usually (and informally), we say:
:f is (Darboux) integrable over [ a  . . ]b.",Definite
"['Definitions/Language Definitions', 'Definitions/Degenerate Cases']",Definition:Degenerate,"A degenerate case is a specific manifestation of a particular type of object being included in another, usually simpler, type of object.",Definition:Degenerate Case,,false,"A degenerate case is a specific manifestation of a particular type of object being included in another, usually simpler, type of object.",Degenerate
"['Definitions/Point-Circles', 'Definitions/Degenerate Conics', 'Definitions/Circles']",Definition:Degenerate,"A point-circle is the locus in the Cartesian plane of an equation of the form:

:$(1): quad left( x - a right)^2 + left( y - b right)^2 = 0$

where $a$ and $b$ are real constants.


There is only one point in the Cartesian plane which satisfies $(1)$, and that is the point $left( a, b right)$.

It can be considered to be a circle whose radius is equal to zero.",Definition:Point-Circle,,false,"A point-circle is the locus in the Cartesian plane of an equation of the form:

:(1):   ( x - a )^2 + ( y - b )^2 = 0

where a and b are real constants.


There is only one point in the Cartesian plane which satisfies (1), and that is the point ( a, b ).

It can be considered to be a circle whose radius is equal to zero.",Degenerate
"['Definitions/Degenerate Conics', 'Definitions/Hyperbolas', 'Definitions/Examples of Degenerate Cases']",Definition:Degenerate,":

Let $C$ be a double napped right circular cone whose base is $B$.

Let $theta$ be half the opening angle of $C$.

That is, let $theta$ be the angle between the axis of $C$ and a generatrix of $C$.

Let a plane $D$ intersect $C$.

Let $phi$ be the inclination of $D$ to the axis of $C$.


Let $K$ be the set of points which forms the intersection of $C$ with $D$.

Then $K$ is a conic section, whose nature depends on $phi$.


=== Circle ===
:

Let $C$ be a double napped right circular cone whose base is $B$.

Let $theta$ be half the opening angle of $C$.

That is, let $theta$ be the angle between the axis of $C$ and a generatrix of $C$.

Let a plane $D$ intersect $C$.

Let $phi$ be the inclination of $D$ to the axis of $C$.


Let $K$ be the set of points which forms the intersection of $C$ with $D$.

Then $K$ is a conic section, whose nature depends on $phi$.


=== Circle ===



:


Let $phi = dfrac pi 2$, thereby making $D$ perpendicular to the axis of $C$.

Then $D$ and $B$ are parallel, and so $K$ is a circle.


=== Transverse Section ===


=== Parabola ===



:


Let $phi = theta$.

Then $K$ is a parabola.

=== Ellipse ===



:


Let $theta < phi < dfrac pi 2$.

That is, the angle between $D$ and the axis of $C$ is between that for which $K$ is a circle and that which $K$ is a parabola.

Then $K$ is an ellipse.

=== Hyperbola ===



:


Let $phi < theta$.

Then $K$ is a hyperbola.

Note that in this case $D$ intersects $C$ in two places: one for each nappe of $C$.

=== Degenerate Hyperbola ===


:


Let $phi < theta$, that is: so as to make $K$ a hyperbola.

However, let $D$ pass through the apex of $C$.

Then $K$ degenerates into a pair of intersecting straight lines.


:


Let $phi = dfrac pi 2$, thereby making $D$ perpendicular to the axis of $C$.

Then $D$ and $B$ are parallel, and so $K$ is a circle.


=== Transverse Section ===
:

Let $C$ be a double napped right circular cone whose base is $B$.

Let $theta$ be half the opening angle of $C$.

That is, let $theta$ be the angle between the axis of $C$ and a generatrix of $C$.

Let a plane $D$ intersect $C$.

Let $phi$ be the inclination of $D$ to the axis of $C$.


Let $K$ be the set of points which forms the intersection of $C$ with $D$.

Then $K$ is a conic section, whose nature depends on $phi$.


=== Circle ===


=== Parabola ===


=== Ellipse ===


=== Hyperbola ===


=== Degenerate Hyperbola ===



:


Let $phi = dfrac pi 2$, thereby making $D$ perpendicular to the axis of $C$.

The plane $D$ which is parallel to $B$, whose intersection with the cone is a circle, is known as a transverse section of the cone.

=== Parabola ===
:

Let $C$ be a double napped right circular cone whose base is $B$.

Let $theta$ be half the opening angle of $C$.

That is, let $theta$ be the angle between the axis of $C$ and a generatrix of $C$.

Let a plane $D$ intersect $C$.

Let $phi$ be the inclination of $D$ to the axis of $C$.


Let $K$ be the set of points which forms the intersection of $C$ with $D$.

Then $K$ is a conic section, whose nature depends on $phi$.


=== Circle ===



:


Let $phi = dfrac pi 2$, thereby making $D$ perpendicular to the axis of $C$.

Then $D$ and $B$ are parallel, and so $K$ is a circle.


=== Transverse Section ===


=== Parabola ===



:


Let $phi = theta$.

Then $K$ is a parabola.

=== Ellipse ===



:


Let $theta < phi < dfrac pi 2$.

That is, the angle between $D$ and the axis of $C$ is between that for which $K$ is a circle and that which $K$ is a parabola.

Then $K$ is an ellipse.

=== Hyperbola ===



:


Let $phi < theta$.

Then $K$ is a hyperbola.

Note that in this case $D$ intersects $C$ in two places: one for each nappe of $C$.

=== Degenerate Hyperbola ===


:


Let $phi < theta$, that is: so as to make $K$ a hyperbola.

However, let $D$ pass through the apex of $C$.

Then $K$ degenerates into a pair of intersecting straight lines.


:


Let $phi = theta$.

Then $K$ is a parabola.

=== Ellipse ===
:

Let $C$ be a double napped right circular cone whose base is $B$.

Let $theta$ be half the opening angle of $C$.

That is, let $theta$ be the angle between the axis of $C$ and a generatrix of $C$.

Let a plane $D$ intersect $C$.

Let $phi$ be the inclination of $D$ to the axis of $C$.


Let $K$ be the set of points which forms the intersection of $C$ with $D$.

Then $K$ is a conic section, whose nature depends on $phi$.


=== Circle ===



:


Let $phi = dfrac pi 2$, thereby making $D$ perpendicular to the axis of $C$.

Then $D$ and $B$ are parallel, and so $K$ is a circle.


=== Transverse Section ===


=== Parabola ===



:


Let $phi = theta$.

Then $K$ is a parabola.

=== Ellipse ===



:


Let $theta < phi < dfrac pi 2$.

That is, the angle between $D$ and the axis of $C$ is between that for which $K$ is a circle and that which $K$ is a parabola.

Then $K$ is an ellipse.

=== Hyperbola ===



:


Let $phi < theta$.

Then $K$ is a hyperbola.

Note that in this case $D$ intersects $C$ in two places: one for each nappe of $C$.

=== Degenerate Hyperbola ===


:


Let $phi < theta$, that is: so as to make $K$ a hyperbola.

However, let $D$ pass through the apex of $C$.

Then $K$ degenerates into a pair of intersecting straight lines.


:


Let $theta < phi < dfrac pi 2$.

That is, the angle between $D$ and the axis of $C$ is between that for which $K$ is a circle and that which $K$ is a parabola.

Then $K$ is an ellipse.

=== Hyperbola ===
:

Let $C$ be a double napped right circular cone whose base is $B$.

Let $theta$ be half the opening angle of $C$.

That is, let $theta$ be the angle between the axis of $C$ and a generatrix of $C$.

Let a plane $D$ intersect $C$.

Let $phi$ be the inclination of $D$ to the axis of $C$.


Let $K$ be the set of points which forms the intersection of $C$ with $D$.

Then $K$ is a conic section, whose nature depends on $phi$.


=== Circle ===



:


Let $phi = dfrac pi 2$, thereby making $D$ perpendicular to the axis of $C$.

Then $D$ and $B$ are parallel, and so $K$ is a circle.


=== Transverse Section ===


=== Parabola ===



:


Let $phi = theta$.

Then $K$ is a parabola.

=== Ellipse ===



:


Let $theta < phi < dfrac pi 2$.

That is, the angle between $D$ and the axis of $C$ is between that for which $K$ is a circle and that which $K$ is a parabola.

Then $K$ is an ellipse.

=== Hyperbola ===



:


Let $phi < theta$.

Then $K$ is a hyperbola.

Note that in this case $D$ intersects $C$ in two places: one for each nappe of $C$.

=== Degenerate Hyperbola ===


:


Let $phi < theta$, that is: so as to make $K$ a hyperbola.

However, let $D$ pass through the apex of $C$.

Then $K$ degenerates into a pair of intersecting straight lines.


:


Let $phi < theta$.

Then $K$ is a hyperbola.

Note that in this case $D$ intersects $C$ in two places: one for each nappe of $C$.

=== Degenerate Hyperbola ===
:

Let $C$ be a double napped right circular cone whose base is $B$.

Let $theta$ be half the opening angle of $C$.

That is, let $theta$ be the angle between the axis of $C$ and a generatrix of $C$.

Let a plane $D$ intersect $C$.

Let $phi$ be the inclination of $D$ to the axis of $C$.


Let $K$ be the set of points which forms the intersection of $C$ with $D$.

Then $K$ is a conic section, whose nature depends on $phi$.


=== Circle ===



:


Let $phi = dfrac pi 2$, thereby making $D$ perpendicular to the axis of $C$.

Then $D$ and $B$ are parallel, and so $K$ is a circle.


=== Transverse Section ===


=== Parabola ===



:


Let $phi = theta$.

Then $K$ is a parabola.

=== Ellipse ===



:


Let $theta < phi < dfrac pi 2$.

That is, the angle between $D$ and the axis of $C$ is between that for which $K$ is a circle and that which $K$ is a parabola.

Then $K$ is an ellipse.

=== Hyperbola ===



:


Let $phi < theta$.

Then $K$ is a hyperbola.

Note that in this case $D$ intersects $C$ in two places: one for each nappe of $C$.

=== Degenerate Hyperbola ===


:


Let $phi < theta$, that is: so as to make $K$ a hyperbola.

However, let $D$ pass through the apex of $C$.

Then $K$ degenerates into a pair of intersecting straight lines.

:


Let $phi < theta$, that is: so as to make $K$ a hyperbola.

However, let $D$ pass through the apex of $C$.

Then $K$ degenerates into a pair of intersecting straight lines.

:


Let $phi < theta$, that is: so as to make $K$ a hyperbola.

However, let $D$ pass through the apex of $C$.

Then $K$ degenerates into a pair of intersecting straight lines.",Definition:Conic Section/Intersection with Cone/Degenerate Hyperbola,,false,":

Let C be a double napped right circular cone whose base is B.

Let θ be half the opening angle of C.

That is, let θ be the angle between the axis of C and a generatrix of C.

Let a plane D intersect C.

Let ϕ be the inclination of D to the axis of C.


Let K be the set of points which forms the intersection of C with D.

Then K is a conic section, whose nature depends on ϕ.


=== Circle ===
:

Let C be a double napped right circular cone whose base is B.

Let θ be half the opening angle of C.

That is, let θ be the angle between the axis of C and a generatrix of C.

Let a plane D intersect C.

Let ϕ be the inclination of D to the axis of C.


Let K be the set of points which forms the intersection of C with D.

Then K is a conic section, whose nature depends on ϕ.


=== Circle ===



:


Let ϕ = π 2, thereby making D perpendicular to the axis of C.

Then D and B are parallel, and so K is a circle.


=== Transverse Section ===


=== Parabola ===



:


Let ϕ = θ.

Then K is a parabola.

=== Ellipse ===



:


Let θ < ϕ < π 2.

That is, the angle between D and the axis of C is between that for which K is a circle and that which K is a parabola.

Then K is an ellipse.

=== Hyperbola ===



:


Let ϕ < θ.

Then K is a hyperbola.

Note that in this case D intersects C in two places: one for each nappe of C.

=== Degenerate Hyperbola ===


:


Let ϕ < θ, that is: so as to make K a hyperbola.

However, let D pass through the apex of C.

Then K degenerates into a pair of intersecting straight lines.


:


Let ϕ = π 2, thereby making D perpendicular to the axis of C.

Then D and B are parallel, and so K is a circle.


=== Transverse Section ===
:

Let C be a double napped right circular cone whose base is B.

Let θ be half the opening angle of C.

That is, let θ be the angle between the axis of C and a generatrix of C.

Let a plane D intersect C.

Let ϕ be the inclination of D to the axis of C.


Let K be the set of points which forms the intersection of C with D.

Then K is a conic section, whose nature depends on ϕ.


=== Circle ===


=== Parabola ===


=== Ellipse ===


=== Hyperbola ===


=== Degenerate Hyperbola ===



:


Let ϕ = π 2, thereby making D perpendicular to the axis of C.

The plane D which is parallel to B, whose intersection with the cone is a circle, is known as a transverse section of the cone.

=== Parabola ===
:

Let C be a double napped right circular cone whose base is B.

Let θ be half the opening angle of C.

That is, let θ be the angle between the axis of C and a generatrix of C.

Let a plane D intersect C.

Let ϕ be the inclination of D to the axis of C.


Let K be the set of points which forms the intersection of C with D.

Then K is a conic section, whose nature depends on ϕ.


=== Circle ===



:


Let ϕ = π 2, thereby making D perpendicular to the axis of C.

Then D and B are parallel, and so K is a circle.


=== Transverse Section ===


=== Parabola ===



:


Let ϕ = θ.

Then K is a parabola.

=== Ellipse ===



:


Let θ < ϕ < π 2.

That is, the angle between D and the axis of C is between that for which K is a circle and that which K is a parabola.

Then K is an ellipse.

=== Hyperbola ===



:


Let ϕ < θ.

Then K is a hyperbola.

Note that in this case D intersects C in two places: one for each nappe of C.

=== Degenerate Hyperbola ===


:


Let ϕ < θ, that is: so as to make K a hyperbola.

However, let D pass through the apex of C.

Then K degenerates into a pair of intersecting straight lines.


:


Let ϕ = θ.

Then K is a parabola.

=== Ellipse ===
:

Let C be a double napped right circular cone whose base is B.

Let θ be half the opening angle of C.

That is, let θ be the angle between the axis of C and a generatrix of C.

Let a plane D intersect C.

Let ϕ be the inclination of D to the axis of C.


Let K be the set of points which forms the intersection of C with D.

Then K is a conic section, whose nature depends on ϕ.


=== Circle ===



:


Let ϕ = π 2, thereby making D perpendicular to the axis of C.

Then D and B are parallel, and so K is a circle.


=== Transverse Section ===


=== Parabola ===



:


Let ϕ = θ.

Then K is a parabola.

=== Ellipse ===



:


Let θ < ϕ < π 2.

That is, the angle between D and the axis of C is between that for which K is a circle and that which K is a parabola.

Then K is an ellipse.

=== Hyperbola ===



:


Let ϕ < θ.

Then K is a hyperbola.

Note that in this case D intersects C in two places: one for each nappe of C.

=== Degenerate Hyperbola ===


:


Let ϕ < θ, that is: so as to make K a hyperbola.

However, let D pass through the apex of C.

Then K degenerates into a pair of intersecting straight lines.


:


Let θ < ϕ < π 2.

That is, the angle between D and the axis of C is between that for which K is a circle and that which K is a parabola.

Then K is an ellipse.

=== Hyperbola ===
:

Let C be a double napped right circular cone whose base is B.

Let θ be half the opening angle of C.

That is, let θ be the angle between the axis of C and a generatrix of C.

Let a plane D intersect C.

Let ϕ be the inclination of D to the axis of C.


Let K be the set of points which forms the intersection of C with D.

Then K is a conic section, whose nature depends on ϕ.


=== Circle ===



:


Let ϕ = π 2, thereby making D perpendicular to the axis of C.

Then D and B are parallel, and so K is a circle.


=== Transverse Section ===


=== Parabola ===



:


Let ϕ = θ.

Then K is a parabola.

=== Ellipse ===



:


Let θ < ϕ < π 2.

That is, the angle between D and the axis of C is between that for which K is a circle and that which K is a parabola.

Then K is an ellipse.

=== Hyperbola ===



:


Let ϕ < θ.

Then K is a hyperbola.

Note that in this case D intersects C in two places: one for each nappe of C.

=== Degenerate Hyperbola ===


:


Let ϕ < θ, that is: so as to make K a hyperbola.

However, let D pass through the apex of C.

Then K degenerates into a pair of intersecting straight lines.


:


Let ϕ < θ.

Then K is a hyperbola.

Note that in this case D intersects C in two places: one for each nappe of C.

=== Degenerate Hyperbola ===
:

Let C be a double napped right circular cone whose base is B.

Let θ be half the opening angle of C.

That is, let θ be the angle between the axis of C and a generatrix of C.

Let a plane D intersect C.

Let ϕ be the inclination of D to the axis of C.


Let K be the set of points which forms the intersection of C with D.

Then K is a conic section, whose nature depends on ϕ.


=== Circle ===



:


Let ϕ = π 2, thereby making D perpendicular to the axis of C.

Then D and B are parallel, and so K is a circle.


=== Transverse Section ===


=== Parabola ===



:


Let ϕ = θ.

Then K is a parabola.

=== Ellipse ===



:


Let θ < ϕ < π 2.

That is, the angle between D and the axis of C is between that for which K is a circle and that which K is a parabola.

Then K is an ellipse.

=== Hyperbola ===



:


Let ϕ < θ.

Then K is a hyperbola.

Note that in this case D intersects C in two places: one for each nappe of C.

=== Degenerate Hyperbola ===


:


Let ϕ < θ, that is: so as to make K a hyperbola.

However, let D pass through the apex of C.

Then K degenerates into a pair of intersecting straight lines.

:


Let ϕ < θ, that is: so as to make K a hyperbola.

However, let D pass through the apex of C.

Then K degenerates into a pair of intersecting straight lines.

:


Let ϕ < θ, that is: so as to make K a hyperbola.

However, let D pass through the apex of C.

Then K degenerates into a pair of intersecting straight lines.",Degenerate
"['Definitions/Degenerate Conics', 'Definitions/Parabolas', 'Definitions/Examples of Degenerate Cases']",Definition:Degenerate,"A degenerate parabola is the conic section whose slicing plane passes through the apex of the cone and is thus tangent to the cone

Hence it consists of a single straight line.",Definition:Degenerate Parabola,,false,"A degenerate parabola is the conic section whose slicing plane passes through the apex of the cone and is thus tangent to the cone

Hence it consists of a single straight line.",Degenerate
"['Definitions/Bilinear Forms (Linear Algebra)', 'Definitions/Examples of Degenerate Cases']",Definition:Degenerate,"Let $mathbb K$ be a field.

Let $V$ be a vector space over $mathbb K$.

Let $b : V times V to mathbb K$ be a bilinear form on $V$.


Then $b$ is degenerate   there exists $v in Vsetminus leftlbrace 0 rightrbrace$ such that $b left(   right){v, u} = 0$ for all $u in V$.


=== Nondegenerate Bilinear Form ===
Let $mathbb K$ be a field.

Let $V$ be a vector space over $mathbb K$.


A bilinear form on $V$ which is not degenerate is nondegenerate.",Definition:Degenerate Bilinear Form,,false,"Let 𝕂 be a field.

Let V be a vector space over 𝕂.

Let b : V × V →𝕂 be a bilinear form on V.


Then b is degenerate   there exists v ∈ V∖{ 0 } such that b (   )v, u = 0 for all u ∈ V.


=== Nondegenerate Bilinear Form ===
Let 𝕂 be a field.

Let V be a vector space over 𝕂.


A bilinear form on V which is not degenerate is nondegenerate.",Degenerate
"['Definitions/Boolean Algebras', 'Definitions/Examples of Degenerate Cases']",Definition:Degenerate,"Let $left( S, vee, wedge, neg right)$ be a Boolean algebra.


Then $left( S, vee, wedge, neg right)$ is said to be degenerate  if and only if  $S$ is a singleton.",Definition:Degenerate Boolean Algebra,,false,"Let ( S, ∨, ∧, ) be a Boolean algebra.


Then ( S, ∨, ∧, ) is said to be degenerate  if and only if  S is a singleton.",Degenerate
"['Definitions/Connected Sets', 'Definitions/Examples of Degenerate Cases']",Definition:Degenerate,"Let $T = left( S, tau right)$ be a topological space.

Let $H subseteq S$ be a subset of $T$.


$H$ is a degenerate connected set of $T$  if and only if  it is a connected set of $T$ containing exactly one element.


=== Non-Degenerate Connected Set ===
Let $T = left({S, tau}right)$ be a topological space.


A non-degenerate connected set of $T$ is a connected set of $T$ containing more than one element.


Category:Definitions/Connected Sets

=== Degenerate Connected Space ===

When $H = S$ itself, the entire space can be referred to in this way:

Let $T = left( S, tau right)$ be a topological space.


$T$ is a degenerate connected space  if and only if  it contains exactly one element.",Definition:Degenerate Connected Set,,false,"Let T = ( S, τ) be a topological space.

Let H ⊆ S be a subset of T.


H is a degenerate connected set of T  if and only if  it is a connected set of T containing exactly one element.


=== Non-Degenerate Connected Set ===
Let T = (S, τ) be a topological space.


A non-degenerate connected set of T is a connected set of T containing more than one element.


Category:Definitions/Connected Sets

=== Degenerate Connected Space ===

When H = S itself, the entire space can be referred to in this way:

Let T = ( S, τ) be a topological space.


T is a degenerate connected space  if and only if  it contains exactly one element.",Degenerate
['Definitions/Continua (Topology)'],Definition:Degenerate,"Let $T = left( S, tau right)$ be a topological space.

A degenerate continuum of $T$ is a continuum in $T$ containing exactly one element.


=== Non-Degenerate Continuum ===
Let $T = left( S, tau right)$ be a topological space.


A non-degenerate continuum of $T$ is a continuum in $T$ containing more than one element.

Category:Definitions/Continua (Topology)",Definition:Degenerate Continuum,,false,"Let T = ( S, τ) be a topological space.

A degenerate continuum of T is a continuum in T containing exactly one element.


=== Non-Degenerate Continuum ===
Let T = ( S, τ) be a topological space.


A non-degenerate continuum of T is a continuum in T containing more than one element.

Category:Definitions/Continua (Topology)",Degenerate
"['Definitions/Probability Theory', 'Definitions/Examples of Degenerate Cases']",Definition:Degenerate,"Let $X$ be a discrete random variable on a probability space.


Then $X$ has a degenerate distribution with parameter $r$  if and only if :

:$Omega_X = leftlbrace r rightrbrace$

:$Pr left(   right){X = k} = begin {cases}
1 & : k = r \
0 & : k ne r
end {cases}$

That is, there is only value that $X$ can take, namely $r$, which it takes with certainty.


 

It trivially gives rise to a probability mass function satisfying $Pr left(   right)Omega = 1$.

Equally trivially, it has an expectation of $r$ and a variance of $0$.",Definition:Degenerate Distribution,,false,"Let X be a discrete random variable on a probability space.


Then X has a degenerate distribution with parameter r  if and only if :

:Ω_X = { r }

:(   )X = k = 
1     : k = r 

0     : k  r

That is, there is only value that X can take, namely r, which it takes with certainty.


 

It trivially gives rise to a probability mass function satisfying (   )Ω = 1.

Equally trivially, it has an expectation of r and a variance of 0.",Degenerate
['Definitions/Monomials'],Definition:Degree,"The degree of a monomial is defined as:
:$ds sum_{j mathop in J} k_j$
that is, the modulus of the corresponding multiindex.


Category:Definitions/Monomials",Definition:Monomial of Free Commutative Monoid/Degree,defined,true,"The degree of a monomial is defined as:
:∑_j ∈ J k_j
that is, the modulus of the corresponding multiindex.


Category:Definitions/Monomials",Degree
['Definitions/Algebraic Numbers'],Definition:Degree,"Let $alpha$ be an algebraic number.

By definition, $alpha$ is the root of at least one polynomial $P_n$ with rational coefficients.


The degree of $alpha$ is the degree of the minimal polynomial $P_n$ whose coefficients are all in $mathbb Q$.


=== Algebraic Number over Field ===

Sources which define an algebraic number over a more general field define degree in the following terms:

Let $F$ be a field.

Let $z in mathbb C$ be algebraic over $F$.


The degree of $alpha$ is the degree of the minimal polynomial $m left(   right)x$ whose coefficients are all in $F$.",Definition:Algebraic Number/Degree,,false,"Let α be an algebraic number.

By definition, α is the root of at least one polynomial P_n with rational coefficients.


The degree of α is the degree of the minimal polynomial P_n whose coefficients are all in ℚ.


=== Algebraic Number over Field ===

Sources which define an algebraic number over a more general field define degree in the following terms:

Let F be a field.

Let z ∈ℂ be algebraic over F.


The degree of α is the degree of the minimal polynomial m (   )x whose coefficients are all in F.",Degree
"['Definitions/Algebraic Numbers', 'Definitions/Field Extensions']",Definition:Degree,"Let $F$ be a field.

Let $z in mathbb C$ be algebraic over $F$.


The degree of $alpha$ is the degree of the minimal polynomial $m left(   right)x$ whose coefficients are all in $F$.",Definition:Algebraic Number over Field/Degree,,false,"Let F be a field.

Let z ∈ℂ be algebraic over F.


The degree of α is the degree of the minimal polynomial m (   )x whose coefficients are all in F.",Degree
['Definitions/Homogeneous Functions'],Definition:Degree,"Let $V$ and $W$ be two vector spaces over a field $F$.

Let $f: V to W$ be a homogeneous function of degree $n$ from $V$ to $W$:
:$f left({alpha mathbf v}right) = alpha^n f left({mathbf v}right)$
for all nonzero $mathbf v in V$ and $alpha in F$.


The element $n in mathbb N$ is the degree of $f$.


Category:Definitions/Homogeneous Functions",Definition:Homogeneous Function/Degree,,false,"Let V and W be two vector spaces over a field F.

Let f: V → W be a homogeneous function of degree n from V to W:
:f (α𝐯) = α^n f (𝐯)
for all nonzero 𝐯∈ V and α∈ F.


The element n ∈ℕ is the degree of f.


Category:Definitions/Homogeneous Functions",Degree
['Definitions/Homogeneous Functions'],Definition:Degree,"Let $f: mathbb R^2 to mathbb R$ be a homogeneous function of two variables:

:$exists n in mathbb Z: forall t in mathbb R: f left(   right){t x, t y} = t^n f left(   right){x, y}$


The integer $n$ is known as the degree of $f$.",Definition:Homogeneous Function/Real Space/Degree,,false,"Let f: ℝ^2 →ℝ be a homogeneous function of two variables:

:∃ n ∈ℤ: ∀ t ∈ℝ: f (   )t x, t y = t^n f (   )x, y


The integer n is known as the degree of f.",Degree
['Definitions/Group Theory'],Definition:Degree,"Let $G$ be an abelian group.

Let $Delta$ be a set.


A gradation of type $Delta$ on $G$ is a family of subgroups $leftlangle G_lambda rightrangle_{lambda mathop in Delta}$ of which $G$ is the internal direct sum.",Definition:Gradation on Abelian Group,,false,"Let G be an abelian group.

Let Δ be a set.


A gradation of type Δ on G is a family of subgroups ⟨ G_λ⟩_λ∈Δ of which G is the internal direct sum.",Degree
['Definitions/Field Extensions'],Definition:Degree,"Let $E / F$ be a field extension.


The degree of $E / F$, denoted $left[ E :   right]F$, is the dimension of $E / F$ when $E$ is viewed as a vector space over $F$.


=== Finite ===
Let $E / F$ be a field extension.


$E / F$ is a finite field extension  if and only if  its degree $left[ E :   right]F$ is finite.

=== Infinite ===
Let $E / F$ be a field extension.


$E / F$ is an infinite field extension  if and only if  its degree $left[ E :   right]F$ is not finite.


Category:Definitions/Field Extensions",Definition:Field Extension/Degree,,false,"Let E / F be a field extension.


The degree of E / F, denoted [ E :   ]F, is the dimension of E / F when E is viewed as a vector space over F.


=== Finite ===
Let E / F be a field extension.


E / F is a finite field extension  if and only if  its degree [ E :   ]F is finite.

=== Infinite ===
Let E / F be a field extension.


E / F is an infinite field extension  if and only if  its degree [ E :   ]F is not finite.


Category:Definitions/Field Extensions",Degree
['Definitions/Field Extensions'],Definition:Degree,"Let $E / F$ be a field extension.

Let $alpha in E$ be algebraic over $F$.


The degree of $alpha$ is the degree of the minimal polynomial $mu_F left(   right)alpha$ whose coefficients are all in $F$.",Definition:Degree of Algebraic Element,,false,"Let E / F be a field extension.

Let α∈ E be algebraic over F.


The degree of α is the degree of the minimal polynomial μ_F (   )α whose coefficients are all in F.",Degree
['Definitions/Field Extensions'],Definition:Degree,"Let $K$ be a field, and let $L/K$ be a field extension of $K$.

The transcendence degree of $L/K$ is the largest cardinality of an algebraically independent subset $A subseteq L$.

Category:Definitions/Field Extensions",Definition:Transcendence Degree,,false,"Let K be a field, and let L/K be a field extension of K.

The transcendence degree of L/K is the largest cardinality of an algebraically independent subset A ⊆ L.

Category:Definitions/Field Extensions",Degree
"['Definitions/Degrees of Vertices', 'Definitions/Vertices of Graphs']",Definition:Degree,"Let $G = left( V, E right)$ be an undirected graph.

Let $v in V$ be a vertex of $G$.


The degree of $v$ in $G$ is the number of edges to which it is incident.

It is denoted $deg_G left(   right)v$, or just $deg left(   right)v$ if it is clear from the context which graph is being referred to.


That is:
:$deg_G left(   right)v = leftlvert leftlbrace u in V : leftlbrace u, v rightrbrace in E rightrbrace  rightrvert$


=== Even Vertex ===
Let $G = left( V, E right)$ be an undirected graph.

Let $v in V$ be a vertex of $G$.


If the degree of $v$ is even, then $v$ is called an even vertex.

=== Odd Vertex ===
Let $G = left( V, E right)$ be an undirected graph.

Let $v in V$ be a vertex of $G$.


If the degree of $v$ is odd, then $v$ is an odd vertex.

=== Isolated Vertex ===
Let $G = left( V, E right)$ be an undirected graph.

Let $v in V$ be a vertex of $G$.


If the degree of $v$ is zero, then $v$ is an isolated vertex.",Definition:Degree of Vertex,,false,"Let G = ( V, E ) be an undirected graph.

Let v ∈ V be a vertex of G.


The degree of v in G is the number of edges to which it is incident.

It is denoted _G (   )v, or just (   )v if it is clear from the context which graph is being referred to.


That is:
:_G (   )v = |{ u ∈ V : { u, v }∈ E }|


=== Even Vertex ===
Let G = ( V, E ) be an undirected graph.

Let v ∈ V be a vertex of G.


If the degree of v is even, then v is called an even vertex.

=== Odd Vertex ===
Let G = ( V, E ) be an undirected graph.

Let v ∈ V be a vertex of G.


If the degree of v is odd, then v is an odd vertex.

=== Isolated Vertex ===
Let G = ( V, E ) be an undirected graph.

Let v ∈ V be a vertex of G.


If the degree of v is zero, then v is an isolated vertex.",Degree
"['Definitions/Degrees of Arc', 'Definitions/Arc Length', 'Definitions/Units of Measurement']",Definition:Degree,"The degree (of arc) is a unit of measurement of the length of an arc of a circle.

It is defined as the length of the arc which subtends $1$ degree (of angle) at the center of the circle.",Definition:Degree of Arc,unit of measurement,true,"The degree (of arc) is a unit of measurement of the length of an arc of a circle.

It is defined as the length of the arc which subtends 1 degree (of angle) at the center of the circle.",Degree
"['Definitions/Degrees of Algebraic Curves', 'Definitions/Algebraic Curves']",Definition:Degree,The degree of an algebraic curve is defined as the highest degree of the polynomial equations defining it.,Definition:Algebraic Curve/Degree,,false,The degree of an algebraic curve is defined as the highest degree of the polynomial equations defining it.,Degree
"['Definitions/Celsius', 'Definitions/Degrees of Temperature', 'Definitions/Temperature', 'Definitions/Units of Measurement']",Definition:Degree,"Celsius is a temperature scale.

Its two reference points are:
:$0 , ^circ mathrm C$, which is set at the melting point of water.
:$100 , ^circ mathrm C$, which is set at the boiling point of water, as defined at sea level and standard atmospheric pressure.


A temperature measured in Celsius is often referred to as so many degrees Celsius.


=== Conversion Factors ===


=== Symbol ===
The symbol for the degree Celsius is $, ^circ mathrm C$.",Definition:Celsius,,false,"Celsius is a temperature scale.

Its two reference points are:
:0   ^∘C, which is set at the melting point of water.
:100   ^∘C, which is set at the boiling point of water, as defined at sea level and standard atmospheric pressure.


A temperature measured in Celsius is often referred to as so many degrees Celsius.


=== Conversion Factors ===


=== Symbol ===
The symbol for the degree Celsius is ^∘C.",Degree
"['Definitions/Fahrenheit', 'Definitions/Temperature', 'Definitions/Units of Measurement']",Definition:Degree,"Fahrenheit is a temperature scale.

Its two reference points are:
:$32 , ^circ mathrm F$, which is set at the melting point of water.
:$212 , ^circ mathrm F$, which is set at the boiling point of water, as defined at sea level and standard atmospheric pressure.


A temperature measured in Fahrenheit is often referred to as so many degrees Fahrenheit.


=== Conversion Factors ===


=== Symbol ===
 ",Definition:Fahrenheit,,false,"Fahrenheit is a temperature scale.

Its two reference points are:
:32   ^∘F, which is set at the melting point of water.
:212   ^∘F, which is set at the boiling point of water, as defined at sea level and standard atmospheric pressure.


A temperature measured in Fahrenheit is often referred to as so many degrees Fahrenheit.


=== Conversion Factors ===


=== Symbol ===
 ",Degree
['Definitions/Splines'],Definition:Degree,"Let $left[ a ,.,.,   right]b$ be a closed real interval.

Let $T := leftlbrace a = t_0, t_1, t_2, ldots, t_{n - 1}, t_n = b rightrbrace$ form a subdivision of $left[ a ,.,.,   right]b$.

Let $S: left[ a ,.,.,   right]b to mathbb R$ be a spline function on $left[ a ,.,.,   right]b$ on $T$.


The degree of $S$ is the maximum degree of the polynomials $P_k$ fitted between $t_k$ and $t_{k + 1}$.


=== Order ===
Let $left[ a ,.,.,   right]b$ be a closed real interval.

Let $T := leftlbrace a = t_0, t_1, t_2, ldots, t_{n - 1}, t_n = b rightrbrace$ form a subdivision of $left[ a ,.,.,   right]b$.

Let $S: left[ a ,.,.,   right]b to mathbb R$ be a spline function on $left[ a ,.,.,   right]b$ on $T$.


Some sources, instead of referring to the degree of a spline, use the order.

Let the maximum degree of the polynomials $P_k$ fitted between $t_k$ and $t_{k + 1}$ be $n$.

The order of $S$ is then $n + 1$.


Category:Definitions/Splines

Category:Definitions/Splines",Definition:Spline Function/Degree,,false,"Let [ a  . . ]b be a closed real interval.

Let T := { a = t_0, t_1, t_2, …, t_n - 1, t_n = b } form a subdivision of [ a  . . ]b.

Let S: [ a  . . ]b →ℝ be a spline function on [ a  . . ]b on T.


The degree of S is the maximum degree of the polynomials P_k fitted between t_k and t_k + 1.


=== Order ===
Let [ a  . . ]b be a closed real interval.

Let T := { a = t_0, t_1, t_2, …, t_n - 1, t_n = b } form a subdivision of [ a  . . ]b.

Let S: [ a  . . ]b →ℝ be a spline function on [ a  . . ]b on T.


Some sources, instead of referring to the degree of a spline, use the order.

Let the maximum degree of the polynomials P_k fitted between t_k and t_k + 1 be n.

The order of S is then n + 1.


Category:Definitions/Splines

Category:Definitions/Splines",Degree
"['Definitions/Degrees of Freedom (Statistics)', 'Definitions/Statistics']",Definition:Degrees of Freedom,"The number of degrees of freedom is essentially the number of independent units of information in a sample relevant to the estimation of a parameter or calculation of a statistic.

One approach is to regard the $n$ observations as the initial data, one of which is used to determine the total or mean.

As the mean must be known before we can determine deviations from it, there are $n - 1$ degrees of freedom left to estimate the variance.

Hence, in the sense that the total is fixed, only $n - 1$ values can be assigned arbitrarily, as the remaining one is then fixed to ensure the correct total.",Definition:Degrees of Freedom (Statistics),number,true,"The number of degrees of freedom is essentially the number of independent units of information in a sample relevant to the estimation of a parameter or calculation of a statistic.

One approach is to regard the n observations as the initial data, one of which is used to determine the total or mean.

As the mean must be known before we can determine deviations from it, there are n - 1 degrees of freedom left to estimate the variance.

Hence, in the sense that the total is fixed, only n - 1 values can be assigned arbitrarily, as the remaining one is then fixed to ensure the correct total.",Degrees of Freedom
"['Definitions/Degrees of Freedom (Physics)', 'Definitions/Physics']",Definition:Degrees of Freedom,"Consider an oscillating system $S$ disturbed from equilibrium.

=== Definition 1 ===
Consider an oscillating system $S$ disturbed from equilibrium.


The number of degrees of freedom of $S$ is the number of normal modes of oscillation of $S$.

=== Definition 2 ===
Consider an oscillating system $S$ disturbed from equilibrium.


The number of degrees of freedom of $S$ is the number of independent variables needed to specify the configuration of $S$ at any time.",Definition:Degrees of Freedom (Physics),,false,"Consider an oscillating system S disturbed from equilibrium.

=== Definition 1 ===
Consider an oscillating system S disturbed from equilibrium.


The number of degrees of freedom of S is the number of normal modes of oscillation of S.

=== Definition 2 ===
Consider an oscillating system S disturbed from equilibrium.


The number of degrees of freedom of S is the number of independent variables needed to specify the configuration of S at any time.",Degrees of Freedom
['Definitions/Quadrilaterals'],Definition:Deltoid,"A deltoid is a dart such that:
:the $2$ sides which are adjacent to the reflex angle are equal to each other
:the other $2$ sides are also equal to each other.


:

In the above diagram, the figure on the right is a deltoid.",Definition:Quadrilateral/Dart/Deltoid,dart,true,"A deltoid is a dart such that:
:the 2 sides which are adjacent to the reflex angle are equal to each other
:the other 2 sides are also equal to each other.


:

In the above diagram, the figure on the right is a deltoid.",Deltoid
['Definitions/Hypocycloids'],Definition:Deltoid,"A deltoid is a hypocycloid with $3$ cusps.


:",Definition:Deltoid (Hypocycloid),hypocycloid,true,"A deltoid is a hypocycloid with 3 cusps.


:",Deltoid
"['Definitions/Everywhere Dense', 'Definitions/Denseness', 'Definitions/Topology']",Definition:Dense,"Let $T = left( S, tau right)$ be a topological space.

Let $H subseteq S$ be a subset.


=== Definition 1 ===
Let $T = left( S, tau right)$ be a topological space.

Let $H subseteq S$ be a subset.


The subset $H$ is (everywhere) dense in $T$  if and only if :
:$H^- = S$
where $H^-$ is the closure of $H$.


That is,  if and only if  every point in $S$ is a point or a limit point of $H$.

=== Definition 2 ===
Let $T = left( S, tau right)$ be a topological space.

Let $H subseteq S$ be a subset.


The subset $H$ is (everywhere) dense in $T$  if and only if  the intersection of $H$ with every non-empty open set of $T$ is non-empty:
:$forall U in tau setminus leftlbrace varnothing rightrbrace: H cap U ne varnothing$

=== Definition 3 ===
Let $T = left( S, tau right)$ be a topological space.

Let $H subseteq S$ be a subset.


The subset $H$ is (everywhere) dense in $T$  if and only if  every neighborhood of every point of $S$ contains at least one point of $H$.",Definition:Everywhere Dense,,false,"Let T = ( S, τ) be a topological space.

Let H ⊆ S be a subset.


=== Definition 1 ===
Let T = ( S, τ) be a topological space.

Let H ⊆ S be a subset.


The subset H is (everywhere) dense in T  if and only if :
:H^- = S
where H^- is the closure of H.


That is,  if and only if  every point in S is a point or a limit point of H.

=== Definition 2 ===
Let T = ( S, τ) be a topological space.

Let H ⊆ S be a subset.


The subset H is (everywhere) dense in T  if and only if  the intersection of H with every non-empty open set of T is non-empty:
:∀ U ∈τ∖{∅}: H ∩ U ∅

=== Definition 3 ===
Let T = ( S, τ) be a topological space.

Let H ⊆ S be a subset.


The subset H is (everywhere) dense in T  if and only if  every neighborhood of every point of S contains at least one point of H.",Dense
"['Definitions/Topology', 'Definitions/Denseness']",Definition:Dense,"Let $T = left( S, tau right)$ be a topological space.

Let $H subseteq S$.


Then $H$ is dense-in-itself  if and only if  it contains no isolated points.",Definition:Dense-in-itself,,false,"Let T = ( S, τ) be a topological space.

Let H ⊆ S.


Then H is dense-in-itself  if and only if  it contains no isolated points.",Dense
"['Definitions/Nowhere Dense', 'Definitions/Denseness', 'Definitions/Topology']",Definition:Dense,"Let $T = left( S, tau right)$ be a topological space.

Let $H subseteq S$.


=== Definition 1 ===
Let $T = left( S, tau right)$ be a topological space.

Let $H subseteq S$.


$H$ is nowhere dense in $T$  if and only if :
:$left( H^- right)^circ = varnothing$
where $H^-$ denotes the closure of $H$ and $H^circ$ denotes its interior.


That is, $H$ is nowhere dense in $T$  if and only if  the interior of its closure is empty.

Another way of putting it is that $H$ is nowhere dense in $T$  if and only if  it consists entirely of boundary.

=== Definition 2 ===
Let $T = left( S, tau right)$ be a topological space.

Let $H subseteq S$.


$H$ is nowhere dense in $T$  if and only if :
:$H^-$ contains no open set of $T$ which is non-empty
where $H^-$ denotes the closure of $H$.",Definition:Nowhere Dense,,false,"Let T = ( S, τ) be a topological space.

Let H ⊆ S.


=== Definition 1 ===
Let T = ( S, τ) be a topological space.

Let H ⊆ S.


H is nowhere dense in T  if and only if :
:( H^- )^∘ = ∅
where H^- denotes the closure of H and H^∘ denotes its interior.


That is, H is nowhere dense in T  if and only if  the interior of its closure is empty.

Another way of putting it is that H is nowhere dense in T  if and only if  it consists entirely of boundary.

=== Definition 2 ===
Let T = ( S, τ) be a topological space.

Let H ⊆ S.


H is nowhere dense in T  if and only if :
:H^- contains no open set of T which is non-empty
where H^- denotes the closure of H.",Dense
"['Definitions/Order Theory', 'Definitions/Densely Ordered']",Definition:Dense,"Let $left( S, preceq right)$ be an ordered set.


Then $left( S, preceq right)$ is defined as densely ordered  if and only if  strictly between every two elements of $S$ there exists another element of $S$:

:$forall a, b in S: a prec b implies exists c in S: a prec c prec b$
where $a prec b$ denotes that $a preceq b$ but $a ne b$.


=== Densely Ordered Subset ===
Let $left( S, preceq right)$ be an ordered set.


A subset $T subseteq S$ is said to be densely ordered in $left( S, preceq right)$  if and only if :
:$forall a, b in S: a prec b implies exists c in T: a prec c prec b$",Definition:Densely Ordered,,false,"Let ( S, ≼) be an ordered set.


Then ( S, ≼) is defined as densely ordered  if and only if  strictly between every two elements of S there exists another element of S:

:∀ a, b ∈ S: a ≺ b ∃ c ∈ S: a ≺ c ≺ b
where a ≺ b denotes that a ≼ b but a  b.


=== Densely Ordered Subset ===
Let ( S, ≼) be an ordered set.


A subset T ⊆ S is said to be densely ordered in ( S, ≼)  if and only if :
:∀ a, b ∈ S: a ≺ b ∃ c ∈ T: a ≺ c ≺ b",Dense
['Definitions/Lattice Theory'],Definition:Dense,"Let $L = left( S, wedge, preceq right)$ be a bounded below meet semilattice.

=== Dense Element ===
Let $L = left( S, wedge, preceq right)$ be a bounded below meet semilattice.

Let $x in S$.


Then $x$ is dense  if and only if 
:$forall y in S: y ne bot implies x wedge y ne bot$

where $bot$ denotes the smallest element in $L$.

=== Dense Subset ===
Let $L = left( S, wedge, preceq right)$ be a bounded below meet semilattice.

Let $A$ be a subset of $S$.


Then $A$ is dense  if and only if  it includes only dense elements.

That means that  if and only if  $forall x in A: x$ is a dense element.

Category:Definitions/Lattice Theory",Definition:Dense (Lattice Theory),,false,"Let L = ( S, ∧, ≼) be a bounded below meet semilattice.

=== Dense Element ===
Let L = ( S, ∧, ≼) be a bounded below meet semilattice.

Let x ∈ S.


Then x is dense  if and only if 
:∀ y ∈ S: y  x ∧ y

where  denotes the smallest element in L.

=== Dense Subset ===
Let L = ( S, ∧, ≼) be a bounded below meet semilattice.

Let A be a subset of S.


Then A is dense  if and only if  it includes only dense elements.

That means that  if and only if  ∀ x ∈ A: x is a dense element.

Category:Definitions/Lattice Theory",Dense
"['Definitions/Density (Physics)', 'Definitions/Physics', 'Definitions/Physical Quantities', 'Definitions/Examples of Scalar Quantities']",Definition:Density,"Density is a physical quantity that is variously specified according to context.

Its general meaning is that of some physical quantity per unit volume, or area, or length.


=== Mass Density ===
Mass density is a physical quantity.

The mass density of a body is its mass per unit volume.


For a homogeneous body it is found by finding its total mass and dividing it by its total volume:
:$rho = dfrac m V$
where:
:$m$ is the body's mass
:$V$ is the body's volume

However, if the substance of the body varies throughout, then its mass density may be a function of position within the body.


=== Symbol ===


=== Dimension ===
The dimension of (mass) density is $mathsf {M L}^{-3}$: mass per unit volume.


Category:Definitions/Mass Density
Category:Definitions/Dimensions of Measurement

=== Units ===
* The SI units of mass density are $mathrm {kg} , mathrm m^{-3}$ (kilograms per cubic metre).

* The CGS units of mass density are $mathrm g , mathrm{cm}^{-3}$ or, less formally: $mathrm g / mathrm {cc}$ (grams per cubic centimetre).

* The FPS units of mass density are $mathrm {lb} , mathrm {ft}^{-3}$ (pounds per cubic foot).


=== Conversion Factors ===


=== Area Mass Density ===
The area mass density of a two-dimensional body is its mass per unit area.


=== Symbol ===
The usual symbol used to denote area mass density is $rho_A$ (Greek letter rho).

However, some sources simply use $rho$ if the context makes it clear that it refers to area mass density rather than volume mass density.

Occasionally, $sigma$ (Greek letter sigma) is also used, but this is more commonly used for surface charge density.


Category:Definitions/Area Mass Density

=== Dimension ===
The dimension of area mass density is $mathsf M mathsf L^{-2}$: mass per unit area.


Category:Definitions/Area Mass Density
Category:Definitions/Dimensions of Measurement

=== Units ===
* The SI units of area mass density are $mathrm {kg}  mathrm m^{-2}$ (kilograms per square metre).

* The CGS units of area mass density are $mathrm g  mathrm{cm}^{-2}$ (grams per square centimetre).

Thus:
:$1  mathrm g  mathrm{cm}^{-2} = 10  mathrm{kg}  mathrm m^{-2}$


Category:Definitions/Area Mass Density
Category:Definitions/Units of Measurement

=== Linear Mass Density ===
The linear mass density of a one-dimensional body is its mass per unit length.


=== Symbol ===
The usual symbol used to denote linear mass density is $mu$ (Greek letter mu).

Sometimes $lambda$ (Greek letter lambda) is also used.


Category:Definitions/Linear Mass Density

=== Dimension ===
The dimension of linear mass density is $mathsf M mathsf L^{-1}$: mass per unit length.


Category:Definitions/Linear Mass Density
Category:Definitions/Dimensions of Measurement

=== Units ===
* The SI units of linear mass density are $mathrm {kg}  mathrm m^{-1}$ (kilograms per metre).

* The CGS units of linear mass density are $mathrm g  mathrm{cm}^{-1}$ (grams per centimetre).

Thus:
:$1  mathrm{kg}  mathrm m^{-1} = 10  mathrm g  mathrm{cm}^{-1}$


Category:Definitions/Linear Mass Density
Category:Definitions/Units of Measurement

=== Electric Charge Density ===
Let $A$ be a point in space in which an electric field acts.

Let $delta V$ be a volume element containing $A$.


The (electric) charge density $rho left(   right){mathbf r}$ at $A$ is defined as:
 
 
 
 

where:
:$Q$ denotes the electric charge within $delta V$
:$mathbf r$ denotes the position vector of $A$.


Thus the electric charge density is the quantity of electric charge per unit volume, at any given point in that volume:


=== Symbol ===


=== Dimension ===
The dimension of electric charge density is $mathsf {I T L}^{-3}$: electric charge per unit volume.

=== Units ===
The SI units of electric charge density are $mathrm C , mathrm m^{-3}$ (coulombs per cubic metre).


Category:Definitions/Electric Charge Density
Category:Definitions/Units of Measurement",Definition:Density (Physics),quantity,true,"Density is a physical quantity that is variously specified according to context.

Its general meaning is that of some physical quantity per unit volume, or area, or length.


=== Mass Density ===
Mass density is a physical quantity.

The mass density of a body is its mass per unit volume.


For a homogeneous body it is found by finding its total mass and dividing it by its total volume:
:ρ =  m V
where:
:m is the body's mass
:V is the body's volume

However, if the substance of the body varies throughout, then its mass density may be a function of position within the body.


=== Symbol ===


=== Dimension ===
The dimension of (mass) density is 𝖬 𝖫^-3: mass per unit volume.


Category:Definitions/Mass Density
Category:Definitions/Dimensions of Measurement

=== Units ===
* The SI units of mass density are kg m^-3 (kilograms per cubic metre).

* The CGS units of mass density are g cm^-3 or, less formally: g / cc (grams per cubic centimetre).

* The FPS units of mass density are lb ft^-3 (pounds per cubic foot).


=== Conversion Factors ===


=== Area Mass Density ===
The area mass density of a two-dimensional body is its mass per unit area.


=== Symbol ===
The usual symbol used to denote area mass density is ρ_A (Greek letter rho).

However, some sources simply use ρ if the context makes it clear that it refers to area mass density rather than volume mass density.

Occasionally, σ (Greek letter sigma) is also used, but this is more commonly used for surface charge density.


Category:Definitions/Area Mass Density

=== Dimension ===
The dimension of area mass density is 𝖬𝖫^-2: mass per unit area.


Category:Definitions/Area Mass Density
Category:Definitions/Dimensions of Measurement

=== Units ===
* The SI units of area mass density are kg m^-2 (kilograms per square metre).

* The CGS units of area mass density are g cm^-2 (grams per square centimetre).

Thus:
:1  g cm^-2 = 10  kg m^-2


Category:Definitions/Area Mass Density
Category:Definitions/Units of Measurement

=== Linear Mass Density ===
The linear mass density of a one-dimensional body is its mass per unit length.


=== Symbol ===
The usual symbol used to denote linear mass density is μ (Greek letter mu).

Sometimes λ (Greek letter lambda) is also used.


Category:Definitions/Linear Mass Density

=== Dimension ===
The dimension of linear mass density is 𝖬𝖫^-1: mass per unit length.


Category:Definitions/Linear Mass Density
Category:Definitions/Dimensions of Measurement

=== Units ===
* The SI units of linear mass density are kg m^-1 (kilograms per metre).

* The CGS units of linear mass density are g cm^-1 (grams per centimetre).

Thus:
:1  kg m^-1 = 10  g cm^-1


Category:Definitions/Linear Mass Density
Category:Definitions/Units of Measurement

=== Electric Charge Density ===
Let A be a point in space in which an electric field acts.

Let δ V be a volume element containing A.


The (electric) charge density ρ(   )𝐫 at A is defined as:
 
 
 
 

where:
:Q denotes the electric charge within δ V
:𝐫 denotes the position vector of A.


Thus the electric charge density is the quantity of electric charge per unit volume, at any given point in that volume:


=== Symbol ===


=== Dimension ===
The dimension of electric charge density is 𝖨 𝖳 𝖫^-3: electric charge per unit volume.

=== Units ===
The SI units of electric charge density are C m^-3 (coulombs per cubic metre).


Category:Definitions/Electric Charge Density
Category:Definitions/Units of Measurement",Density
"['Definitions/Mass Density', 'Definitions/Density (Physics)', 'Definitions/Physics', 'Definitions/Physical Quantities', 'Definitions/Examples of Scalar Quantities']",Definition:Density,"Mass density is a physical quantity.

The mass density of a body is its mass per unit volume.


For a homogeneous body it is found by finding its total mass and dividing it by its total volume:
:$rho = dfrac m V$
where:
:$m$ is the body's mass
:$V$ is the body's volume

However, if the substance of the body varies throughout, then its mass density may be a function of position within the body.


=== Symbol ===


=== Dimension ===
The dimension of (mass) density is $mathsf {M L}^{-3}$: mass per unit volume.


Category:Definitions/Mass Density
Category:Definitions/Dimensions of Measurement

=== Units ===
* The SI units of mass density are $mathrm {kg} , mathrm m^{-3}$ (kilograms per cubic metre).

* The CGS units of mass density are $mathrm g , mathrm{cm}^{-3}$ or, less formally: $mathrm g / mathrm {cc}$ (grams per cubic centimetre).

* The FPS units of mass density are $mathrm {lb} , mathrm {ft}^{-3}$ (pounds per cubic foot).


=== Conversion Factors ===
",Definition:Mass Density,,false,"Mass density is a physical quantity.

The mass density of a body is its mass per unit volume.


For a homogeneous body it is found by finding its total mass and dividing it by its total volume:
:ρ =  m V
where:
:m is the body's mass
:V is the body's volume

However, if the substance of the body varies throughout, then its mass density may be a function of position within the body.


=== Symbol ===


=== Dimension ===
The dimension of (mass) density is 𝖬 𝖫^-3: mass per unit volume.


Category:Definitions/Mass Density
Category:Definitions/Dimensions of Measurement

=== Units ===
* The SI units of mass density are kg m^-3 (kilograms per cubic metre).

* The CGS units of mass density are g cm^-3 or, less formally: g / cc (grams per cubic centimetre).

* The FPS units of mass density are lb ft^-3 (pounds per cubic foot).


=== Conversion Factors ===
",Density
"['Definitions/Area Mass Density', 'Definitions/Mass Density', 'Definitions/Physical Quantities']",Definition:Density,"The area mass density of a two-dimensional body is its mass per unit area.


=== Symbol ===
The usual symbol used to denote area mass density is $rho_A$ (Greek letter rho).

However, some sources simply use $rho$ if the context makes it clear that it refers to area mass density rather than volume mass density.

Occasionally, $sigma$ (Greek letter sigma) is also used, but this is more commonly used for surface charge density.


Category:Definitions/Area Mass Density

=== Dimension ===
The dimension of area mass density is $mathsf M mathsf L^{-2}$: mass per unit area.


Category:Definitions/Area Mass Density
Category:Definitions/Dimensions of Measurement

=== Units ===
* The SI units of area mass density are $mathrm {kg}  mathrm m^{-2}$ (kilograms per square metre).

* The CGS units of area mass density are $mathrm g  mathrm{cm}^{-2}$ (grams per square centimetre).

Thus:
:$1  mathrm g  mathrm{cm}^{-2} = 10  mathrm{kg}  mathrm m^{-2}$


Category:Definitions/Area Mass Density
Category:Definitions/Units of Measurement",Definition:Mass Density/Area,,false,"The area mass density of a two-dimensional body is its mass per unit area.


=== Symbol ===
The usual symbol used to denote area mass density is ρ_A (Greek letter rho).

However, some sources simply use ρ if the context makes it clear that it refers to area mass density rather than volume mass density.

Occasionally, σ (Greek letter sigma) is also used, but this is more commonly used for surface charge density.


Category:Definitions/Area Mass Density

=== Dimension ===
The dimension of area mass density is 𝖬𝖫^-2: mass per unit area.


Category:Definitions/Area Mass Density
Category:Definitions/Dimensions of Measurement

=== Units ===
* The SI units of area mass density are kg m^-2 (kilograms per square metre).

* The CGS units of area mass density are g cm^-2 (grams per square centimetre).

Thus:
:1  g cm^-2 = 10  kg m^-2


Category:Definitions/Area Mass Density
Category:Definitions/Units of Measurement",Density
"['Definitions/Linear Mass Density', 'Definitions/Mass Density', 'Definitions/Physical Quantities']",Definition:Density,"The linear mass density of a one-dimensional body is its mass per unit length.


=== Symbol ===
The usual symbol used to denote linear mass density is $mu$ (Greek letter mu).

Sometimes $lambda$ (Greek letter lambda) is also used.


Category:Definitions/Linear Mass Density

=== Dimension ===
The dimension of linear mass density is $mathsf M mathsf L^{-1}$: mass per unit length.


Category:Definitions/Linear Mass Density
Category:Definitions/Dimensions of Measurement

=== Units ===
* The SI units of linear mass density are $mathrm {kg}  mathrm m^{-1}$ (kilograms per metre).

* The CGS units of linear mass density are $mathrm g  mathrm{cm}^{-1}$ (grams per centimetre).

Thus:
:$1  mathrm{kg}  mathrm m^{-1} = 10  mathrm g  mathrm{cm}^{-1}$


Category:Definitions/Linear Mass Density
Category:Definitions/Units of Measurement",Definition:Mass Density/Linear,,false,"The linear mass density of a one-dimensional body is its mass per unit length.


=== Symbol ===
The usual symbol used to denote linear mass density is μ (Greek letter mu).

Sometimes λ (Greek letter lambda) is also used.


Category:Definitions/Linear Mass Density

=== Dimension ===
The dimension of linear mass density is 𝖬𝖫^-1: mass per unit length.


Category:Definitions/Linear Mass Density
Category:Definitions/Dimensions of Measurement

=== Units ===
* The SI units of linear mass density are kg m^-1 (kilograms per metre).

* The CGS units of linear mass density are g cm^-1 (grams per centimetre).

Thus:
:1  kg m^-1 = 10  g cm^-1


Category:Definitions/Linear Mass Density
Category:Definitions/Units of Measurement",Density
"['Definitions/Electric Charge Density', 'Definitions/Electric Charge', 'Definitions/Electrostatics', 'Definitions/Examples of Scalar Quantities']",Definition:Density,"Let $A$ be a point in space in which an electric field acts.

Let $delta V$ be a volume element containing $A$.


The (electric) charge density $rho left(   right){mathbf r}$ at $A$ is defined as:
 
 
 
 

where:
:$Q$ denotes the electric charge within $delta V$
:$mathbf r$ denotes the position vector of $A$.


Thus the electric charge density is the quantity of electric charge per unit volume, at any given point in that volume:


=== Symbol ===


=== Dimension ===
The dimension of electric charge density is $mathsf {I T L}^{-3}$: electric charge per unit volume.

=== Units ===
The SI units of electric charge density are $mathrm C , mathrm m^{-3}$ (coulombs per cubic metre).


Category:Definitions/Electric Charge Density
Category:Definitions/Units of Measurement",Definition:Electric Charge Density,,false,"Let A be a point in space in which an electric field acts.

Let δ V be a volume element containing A.


The (electric) charge density ρ(   )𝐫 at A is defined as:
 
 
 
 

where:
:Q denotes the electric charge within δ V
:𝐫 denotes the position vector of A.


Thus the electric charge density is the quantity of electric charge per unit volume, at any given point in that volume:


=== Symbol ===


=== Dimension ===
The dimension of electric charge density is 𝖨 𝖳 𝖫^-3: electric charge per unit volume.

=== Units ===
The SI units of electric charge density are C m^-3 (coulombs per cubic metre).


Category:Definitions/Electric Charge Density
Category:Definitions/Units of Measurement",Density
['Definitions/Analytic Number Theory'],Definition:Density,"Let $mathcal P$ be a set of prime numbers.

For $s in mathbb C$, let $ds f left(   right)s = sum_{p mathop in mathcal P}: p^{-s}$.


$S$ has Dirichlet density $alpha$  if and only if :

:$ds lim_{s mathop to 1^+} leftlbrace frac {f left(   right)s} {ln left(   right){s - 1} }  rightrbrace = -alpha$

where $1^+$ indicates a limit from above along the real line.

 ",Definition:Dirichlet Density,,false,"Let 𝒫 be a set of prime numbers.

For s ∈ℂ, let f (   )s = ∑_p ∈𝒫: p^-s.


S has Dirichlet density α  if and only if :

:lim_s → 1^+{f (   )s/ln(   )s - 1} = -α

where 1^+ indicates a limit from above along the real line.

 ",Density
"['Definitions/Diagonals of Polyhedra', 'Definitions/Polyhedra']",Definition:Diagonal,A diagonal of a polyhedron $P$ is a straight line connecting $2$ vertices of $P$ which are not adjacent to the same face.,Definition:Diagonal of Polyhedron,,false,A diagonal of a polyhedron P is a straight line connecting 2 vertices of P which are not adjacent to the same face.,Diagonal
['Definitions/Parallelograms'],Definition:Diagonal,"Let $ABCD$ be a parallelogram:

:

The diameters of $ABCD$ are the lines $AC$ and $BD$ joining their opposite vertices.",Definition:Diameter of Parallelogram,,false,"Let ABCD be a parallelogram:

:

The diameters of ABCD are the lines AC and BD joining their opposite vertices.",Diagonal
"['Definitions/Diagonal Relation', 'Definitions/Examples of Equivalence Relations', 'Definitions/Examples of Relations']",Definition:Diagonal,"Let $S$ be a set.

The diagonal relation on $S$ is the relation $Delta_S$ on $S$ defined as:

:$Delta_S = leftlbrace left( x, x right): x in S rightrbrace subseteq S times S$

Alternatively:

:$Delta_S = leftlbrace left( x, y right): x, y in S: x = y rightrbrace$


=== Class Theory ===
 
Let $V$ be a basic universe.

The diagonal relation on $V$ is the relation $Delta_V$ on $V$ defined as:

:$Delta_V = leftlbrace left( x, x right): x in V rightrbrace$

Alternatively:

:$Delta_V = leftlbrace left( x, y right): x, y in V: x = y rightrbrace$",Definition:Diagonal Relation,,false,"Let S be a set.

The diagonal relation on S is the relation Δ_S on S defined as:

:Δ_S = {( x, x ): x ∈ S }⊆ S × S

Alternatively:

:Δ_S = {( x, y ): x, y ∈ S: x = y }


=== Class Theory ===
 
Let V be a basic universe.

The diagonal relation on V is the relation Δ_V on V defined as:

:Δ_V = {( x, x ): x ∈ V }

Alternatively:

:Δ_V = {( x, y ): x, y ∈ V: x = y }",Diagonal
['Definitions/Mapping Theory'],Definition:Diagonal,"Let $S$ be a set.

Let $S times S$ be the Cartesian product of $S$ with itself.


Then the diagonal mapping on $S$ is defined as $Delta: S to S times S$:
:$forall x in S: Delta left({x}right) = left({x, x}right)$


Clearly $Delta$ is an injection, and is not a surjection unless $S$ is a singleton.

",Definition:Diagonal Mapping,,false,"Let S be a set.

Let S × S be the Cartesian product of S with itself.


Then the diagonal mapping on S is defined as Δ: S → S × S:
:∀ x ∈ S: Δ(x) = (x, x)


Clearly Δ is an injection, and is not a surjection unless S is a singleton.

",Diagonal
"['Definitions/Diagonal Elements', 'Definitions/Main Diagonal', 'Definitions/Matrices']",Definition:Diagonal,The elements of the main diagonal of a matrix or a determinant are called the diagonal elements.,Definition:Main Diagonal/Diagonal Elements,,false,The elements of the main diagonal of a matrix or a determinant are called the diagonal elements.,Diagonal
"['Definitions/Diagonal Matrices', 'Definitions/Square Matrices', 'Definitions/Matrices']",Definition:Diagonal,"Let $mathbf A = begin{bmatrix}
a_{11} & a_{12} & cdots & a_{1n} \
a_{21} & a_{22} & cdots & a_{2n} \
vdots & vdots & ddots & vdots \
a_{n1} & a_{n2} & cdots & a_{nn} \
end{bmatrix}$ be a square matrix of order $n$.

Then $mathbf A$ is a diagonal matrix  if and only if  all elements of $mathbf A$ are zero except for possibly its diagonal elements.


Thus $mathbf A = begin{bmatrix}
a_{11} & 0 & cdots & 0 \
0 & a_{22} & cdots & 0 \
vdots & vdots & ddots & vdots \
0 & 0 & cdots & a_{nn} \
end{bmatrix}$.


It follows by the definition of triangular matrix that a diagonal matrix is both an upper triangular matrix and a lower triangular matrix.",Definition:Diagonal Matrix,,false,"Let 𝐀 = [ a_11 a_12    ⋯ a_1n; a_21 a_22    ⋯ a_2n;    ⋮    ⋮    ⋱    ⋮; a_n1 a_n2    ⋯ a_nn;      ] be a square matrix of order n.

Then 𝐀 is a diagonal matrix  if and only if  all elements of 𝐀 are zero except for possibly its diagonal elements.


Thus 𝐀 = [ a_11    0    ⋯    0;    0 a_22    ⋯    0;    ⋮    ⋮    ⋱    ⋮;    0    0    ⋯ a_nn;      ].


It follows by the definition of triangular matrix that a diagonal matrix is both an upper triangular matrix and a lower triangular matrix.",Diagonal
"['Definitions/Diagonalizable Matrices', 'Definitions/Matrices']",Definition:Diagonal,"A diagonalizable matrix $mathbf A$ is a square matrix which is similar to a diagonal matrix.

That is, $mathbf A$ is diagonalizable  if and only if  there exists an invertible matrix $mathbf X$ such that $mathbf X^-1 mathbf A mathbf X$ is a diagonal matrix.",Definition:Diagonalizable Matrix,,false,"A diagonalizable matrix 𝐀 is a square matrix which is similar to a diagonal matrix.

That is, 𝐀 is diagonalizable  if and only if  there exists an invertible matrix 𝐗 such that 𝐗^-1 𝐀𝐗 is a diagonal matrix.",Diagonal
"['Definitions/Diagrams', 'Definitions/Proof Techniques']",Definition:Diagram,"A diagram is a graphical technique for illustrating a concept in picture form.

It is generally considered that its use should be limited to that of an aid to understanding, and should not be used in order to prove something.",Definition:Diagram (Graphical Technique),technique,true,"A diagram is a graphical technique for illustrating a concept in picture form.

It is generally considered that its use should be limited to that of an aid to understanding, and should not be used in order to prove something.",Diagram
['Definitions/Category Theory'],Definition:Diagram,"Let $mathbf J$ and $mathbf C$ be metacategories.


A diagram of type $mathbf J$ in $mathbf C$ is a functor $D: mathbf J to mathbf C$.


=== Index Category ===

In this context, $mathbf J$ is referred to as the index category.

Its objects are typically denoted by lowercase letters, $i, j$ etc.


Furthermore, one writes $D_i$ in place of the formally more correct $D left({i}right)$.

Similarly, for $alpha: i to j$ a morphism one writes $D_alpha$ in place of $D left({alpha}right)$.",Definition:Diagram (Category Theory),,false,"Let 𝐉 and 𝐂 be metacategories.


A diagram of type 𝐉 in 𝐂 is a functor D: 𝐉→𝐂.


=== Index Category ===

In this context, 𝐉 is referred to as the index category.

Its objects are typically denoted by lowercase letters, i, j etc.


Furthermore, one writes D_i in place of the formally more correct D (i).

Similarly, for α: i → j a morphism one writes D_α in place of D (α).",Diagram
"['Definitions/Conic Sections', 'Definitions/Diameters of Conic Sections']",Definition:Diameter,"Let $mathcal K$ be a conic section.

A diameter of $mathcal K$ is the locus of the midpoints of a system of parallel chords of $mathcal K$.


=== Ellipse ===
Let $mathcal K$ be an ellipse.

A diameter of $mathcal K$ is the locus of the midpoints of a system of parallel chords of $mathcal K$.


:

=== Hyperbola ===
Let $mathcal K$ be a hyperbola.

A diameter of $mathcal K$ is the locus of the midpoints of a system of parallel chords of $mathcal K$.


:

=== Parabola ===
Let $mathcal K$ be a parabola.

A diameter of $mathcal K$ is the locus of the midpoints of a system of parallel chords of $mathcal K$.


:",Definition:Diameter of Conic Section,,false,"Let 𝒦 be a conic section.

A diameter of 𝒦 is the locus of the midpoints of a system of parallel chords of 𝒦.


=== Ellipse ===
Let 𝒦 be an ellipse.

A diameter of 𝒦 is the locus of the midpoints of a system of parallel chords of 𝒦.


:

=== Hyperbola ===
Let 𝒦 be a hyperbola.

A diameter of 𝒦 is the locus of the midpoints of a system of parallel chords of 𝒦.


:

=== Parabola ===
Let 𝒦 be a parabola.

A diameter of 𝒦 is the locus of the midpoints of a system of parallel chords of 𝒦.


:",Diameter
"['Definitions/Circles', 'Definitions/Diameters of Conic Sections']",Definition:Diameter,":

 
: 
:A diameter of the circle is any straight line drawn through the center and terminated in both directions by the circumference of the circle, and such a straight line also bisects the center.
 ''
 

In the above diagram, the line $CD$ is a diameter.",Definition:Circle/Diameter,,false,":

 
: 
:A diameter of the circle is any straight line drawn through the center and terminated in both directions by the circumference of the circle, and such a straight line also bisects the center.
 ”
 

In the above diagram, the line CD is a diameter.",Diameter
"['Definitions/Ellipses', 'Definitions/Diameters of Conic Sections']",Definition:Diameter,"Let $mathcal K$ be an ellipse.

A diameter of $mathcal K$ is the locus of the midpoints of a system of parallel chords of $mathcal K$.


:",Definition:Diameter of Ellipse,,false,"Let 𝒦 be an ellipse.

A diameter of 𝒦 is the locus of the midpoints of a system of parallel chords of 𝒦.


:",Diameter
"['Definitions/Hyperbolas', 'Definitions/Diameters of Conic Sections']",Definition:Diameter,"Let $mathcal K$ be a hyperbola.

A diameter of $mathcal K$ is the locus of the midpoints of a system of parallel chords of $mathcal K$.


:",Definition:Diameter of Hyperbola,,false,"Let 𝒦 be a hyperbola.

A diameter of 𝒦 is the locus of the midpoints of a system of parallel chords of 𝒦.


:",Diameter
"['Definitions/Diameters of Conic Sections', 'Definitions/Parabolas']",Definition:Diameter,"Let $mathcal K$ be a parabola.

A diameter of $mathcal K$ is the locus of the midpoints of a system of parallel chords of $mathcal K$.


:",Definition:Diameter of Parabola,,false,"Let 𝒦 be a parabola.

A diameter of 𝒦 is the locus of the midpoints of a system of parallel chords of 𝒦.


:",Diameter
['Definitions/Spheres'],Definition:Diameter,"By the definition of a sphere, there is one point inside it such that the distance between that point and any given point on the surface of the sphere are equal, and that point is called the center of the sphere.


The diameter of a sphere is the length of any straight line drawn from a point on the surface to another point on the surface through the center.


 
: 
:A diameter of the sphere is any straight line drawn through the centre and terminated in both directions by the surface of the sphere.
 ''
 ",Definition:Sphere/Geometry/Diameter,,false,"By the definition of a sphere, there is one point inside it such that the distance between that point and any given point on the surface of the sphere are equal, and that point is called the center of the sphere.


The diameter of a sphere is the length of any straight line drawn from a point on the surface to another point on the surface through the center.


 
: 
:A diameter of the sphere is any straight line drawn through the centre and terminated in both directions by the surface of the sphere.
 ”
 ",Diameter
['Definitions/Parallelograms'],Definition:Diameter,"Let $ABCD$ be a parallelogram:

:

The diameters of $ABCD$ are the lines $AC$ and $BD$ joining their opposite vertices.",Definition:Diameter of Parallelogram,,false,"Let ABCD be a parallelogram:

:

The diameters of ABCD are the lines AC and BD joining their opposite vertices.",Diameter
['Definitions/Geometric Figures'],Definition:Diameter,The diameter of a geometric figure is the greatest length that can be formed between two opposite parallel straight lines that can be drawn tangent to its boundary.,Definition:Geometric Figure/Diameter,,false,The diameter of a geometric figure is the greatest length that can be formed between two opposite parallel straight lines that can be drawn tangent to its boundary.,Diameter
['Definitions/Metric Spaces'],Definition:Diameter,"Let $M = left( A, d right)$ be a metric space.

Let $S subseteq A$ be subset of $A$.


Then the diameter of $S$ is the extended real number defined by:

:$mathrm {diam} left(   right)S := begin {cases} sup leftlbrace d left(   right){x, y}: x, y in S rightrbrace & : text {if this quantity is finite} \ + infty & : text {otherwise} end {cases}$


Thus, by the definition of the supremum, the diameter is the smallest real number $D$ such that any two points of $S$ are at most a distance $D$ apart.

If $d: S^2 to mathbb R$ does not admit a supremum, then $mathrm {diam} left(   right)S$ is infinite.",Definition:Diameter of Subset of Metric Space,,false,"Let M = ( A, d ) be a metric space.

Let S ⊆ A be subset of A.


Then the diameter of S is the extended real number defined by:

:diam(   )S := sup{ d (   )x, y: x, y ∈ S }    : if this quantity is finite
 + ∞    : otherwise


Thus, by the definition of the supremum, the diameter is the smallest real number D such that any two points of S are at most a distance D apart.

If d: S^2 →ℝ does not admit a supremum, then diam(   )S is infinite.",Diameter
"['Definitions/Set Theory', 'Definitions/Set Difference']",Definition:Difference,"The (set) difference between two sets $S$ and $T$ is written $S setminus T$, and means the set that consists of the elements of $S$ which are not elements of $T$:
:$x in S setminus T iff x in S land x notin T$


It can also be defined as:
:$S setminus T = leftlbrace x in S: x notin T rightrbrace$
:$S setminus T = leftlbrace x: x in S land x notin T rightrbrace$",Definition:Set Difference,,false,"The (set) difference between two sets S and T is written S ∖ T, and means the set that consists of the elements of S which are not elements of T:
:x ∈ S ∖ T  x ∈ S  x ∉ T


It can also be defined as:
:S ∖ T = { x ∈ S: x ∉ T }
:S ∖ T = { x: x ∈ S  x ∉ T }",Difference
['Definitions/Subtraction'],Definition:Difference,"Let $a$ and $b$ be real numbers.

The absolute difference between $a$ and $b$ is defined and denoted as:
:$leftlvert a - b rightrvert$
where $leftlvert , cdot , rightrvert$ is the absolute value function.",Definition:Absolute Difference,,false,"Let a and b be real numbers.

The absolute difference between a and b is defined and denoted as:
:| a - b |
where | · | is the absolute value function.",Difference
"['Definitions/Topological Manifolds', 'Definitions/Differentiable Manifolds']",Definition:Differentiable,"Let $M$ be a second-countable locally Euclidean space of dimension $d$. 

Let $mathscr F$ be a $d$-dimensional differentiable structure on $M$ of class $mathcal C^k$, where $k ge 1$.


Then $left( M, mathscr F right)$ is a differentiable manifold of class $mathcal C^k$ and dimension $d$.",Definition:Topological Manifold/Differentiable Manifold,,false,"Let M be a second-countable locally Euclidean space of dimension d. 

Let ℱ be a d-dimensional differentiable structure on M of class 𝒞^k, where k ≥ 1.


Then ( M, ℱ) is a differentiable manifold of class 𝒞^k and dimension d.",Differentiable
['Definitions/Differential Calculus'],Definition:Differentiable,"=== Real Function ===
=== At a Point ===

Let $f$ be a real function defined on an open interval $left( a ,.,.,   right)b$.

Let $xi$ be a point in $left( a ,.,.,   right)b$.


==== Definition 1 ====
Let $f$ be a real function defined on an open interval $left( a ,.,.,   right)b$.

Let $xi$ be a point in $left( a ,.,.,   right)b$.


$f$ is differentiable at the point $xi$  if and only if  the limit:
:$ds lim_{x mathop to xi} frac {f left(   right)x - f left(   right)xi} {x - xi}$
exists.

==== Definition 2 ====
Let $f$ be a real function defined on an open interval $left( a ,.,.,   right)b$.

Let $xi$ be a point in $left( a ,.,.,   right)b$.


$f$ is differentiable at the point $xi$  if and only if  the limit:
:$ds lim_{h mathop to 0} frac {f left(   right){xi + h} - f left(   right)xi} h$
exists.


Category:Definitions/Differentiable Real Functions

These limits, if they exist, are called the derivative of $f$ at $xi$.

=== On an Open Interval ===
Let $f$ be a real function defined on an open interval $left( a ,.,.,   right)b$.


Then $f$ is differentiable on $left( a ,.,.,   right)b$  if and only if  $f$ is differentiable at each point of $left( a ,.,.,   right)b$.


=== On a Closed Interval ===
Let $f$ be a real function defined on a closed interval $left[ a ,.,.,   right]b$.

Let $f$ be differentiable on the open interval $left( a ,.,.,   right)b$.


If the following limit from the right exists:

:$ds lim_{x mathop to a^+} frac {f left(   right)x - f left(   right)a} {x - a}$

as well as this limit from the left:
:$ds lim_{x mathop to b^-} frac {f left(   right)x - f left(   right)b} {x - b}$

then $f$ is differentiable on the closed interval $left[ a ,.,.,   right]b$.


Similar definitions for differentiability on a half-open interval can be expressed for a real function which has either a limit from the right at $a$ or a limit from the left at $b$, but not both.

=== On the Real Number Line ===
Let $f$ be a real function defined on $mathbb R$.

By definition, $mathbb R$ is an (unbounded) open interval.


Let $f$ be differentiable on the open interval $mathbb R$.

That is, let $f$ be differentiable at every point of $mathbb R$.


Then $f$ is differentiable everywhere (on $mathbb R$).


Category:Definitions/Differentiable Real Functions

=== Complex Function ===
=== At a Point ===

Let $U subset mathbb C$ be an open set.

Let $f : U to mathbb C$ be a complex function.

Let $z_0 in U$ be a point in $U$.


Then $f$ is complex-differentiable at $z_0$  if and only if  the limit:

:$ds lim_{h mathop to 0} frac {f left(   right){z_0+h} - f left(   right){z_0}} h$

exists as a finite number.


This limit, if it exists, is called the derivative of $f$ at $z_0$.

=== In an Open Set ===
Let $U subseteq mathbb C$ be an open set.

Let $f : U to mathbb C$ be a complex function.


Then $f$ is holomorphic in $U$  if and only if  $f$ is differentiable at each point of $U$.

We also say that $f$ is complex-differentiable in $U$.


 

Category:Definitions/Complex Differential Calculus

=== Real-Valued Function ===
=== At a Point ===

Let $U$ be an open subset of $mathbb R^n$. 

Let $leftlVert cdot rightrVert $ denote the Euclidean norm on $mathbb R^n$.

Let $f: U to mathbb R$ be a real-valued function.

Let $x in U$.

==== Definition 1 ====
Let $U$ be an open subset of $mathbb R^n$.

Let $leftlVert cdot rightrVert $ denote the Euclidean norm on $mathbb R^n$.

Let $f: U to mathbb R$ be a real-valued function.

Let $x in U$.



$f$ is differentiable at $x$  if and only if  there exist $alpha_1, ldots, alpha_n in mathbb R$ and a real-valued function $r: U setminus leftlbrace x rightrbrace to mathbb R$ such that:

:$(1):quad f left(   right){x + h} = f left(   right)x + alpha_1 h_1 + cdots + alpha_n h_n + r left(   right)h leftlVert h rightrVert$

:$(2):quad ds lim_{h mathop to 0} r left(   right)h = 0$


Category:Definitions/Differentiable Real-Valued Functions

==== Definition 2 ====
Let $U$ be an open subset of $mathbb R^n$. 

Let $f: U to mathbb R$ be a real-valued function.

Let $leftlVert cdot rightrVert $ denote the Euclidean norm on $mathbb R^n$.

Let $x in U$.



$f$ is differentiable at $x$  if and only if  there exists a linear transformation $T: mathbb R^n to mathbb R$ and a real-valued function $r: U setminus leftlbrace x rightrbrace to mathbb R$ such that:

:$(1): quad f left(   right){x + h} = f left(   right)x + T left(   right)h + r left(   right)h leftlVert h rightrVert$
:$(2): quad ds lim_{h mathop to 0} r left(   right)h = 0$


Category:Definitions/Differentiable Real-Valued Functions

=== In an Open Set ===
Let $mathbb X$ be an open subset of $mathbb R^n$. 

Let $f: mathbb X to mathbb R$ be a real-valued function.


Then $f$ is differentiable in the open set $mathbb X$  if and only if  $f$ is differentiable at each point of $mathbb X$.

=== Vector-Valued Function ===
Let $m, n ge 1$ be natural numbers.

=== At a Point ===

Let $mathbb X$ be an open subset of $mathbb R^n$. 

Let $f = left( f_1, f_2, ldots, f_m right)^intercal: mathbb X to mathbb R^m$ be a vector valued function.


$f$ is differentiable at $x in mathbb R^n$  if and only if  there exists a linear transformation $T: mathbb R^n to mathbb R^m$ and a mapping $r : U to mathbb R^m$ such that:

:$(1): quad f left(   right){x + h} = f left(   right)x + T left(   right)h + r left(   right)h cdot leftlVert h rightrVert$

:$(2): quad ds lim_{h mathop to 0} r left(   right)h = 0$


Category:Definitions/Differentiable Vector-Valued Functions

=== In an Open Set ===
Let $mathbb X$ be an open subset of $mathbb R^n$. 

Let $f = left( f_1, f_2, ldots, f_m right)^intercal: mathbb X to mathbb R^m$ be a vector valued function.

Let $S subseteq mathbb X$.


Then $f$ is differentiable in the open set $S$  if and only if  $f$ is differentiable at each $x$ in $S$.


This can be denoted $f in mathcal C^1 left(   right){S, mathbb R^m}$.

=== Function With Values in Normed Space ===
Let $U subset mathbb R$ be an open set.

Let $left( X, leftlVert , cdot , rightrVert_X right)$ be a normed vector space.

A function $f : U to X$ is (strongly) differentiable at $x in U$  if and only if  there exists $f' left(   right)x in X$ such that:

:$ds lim_{h mathop to 0} leftlVert frac {f left(   right){x + h} - f left(   right)x} h - f' left(   right)x rightrVert_X = 0$

Moreover, $f$ is called (strongly) differentiable if it is differentiable at every point of $U$.

=== Between Differentiable Manifolds ===
Let $M$ and $N$ be differentiable manifolds.

Let $f : M to N$ be continuous.

=== Definition 1 ===

$f$  is differentiable  if and only if  for every pair of charts $(U, phi)$ and $(V,psi)$ of $M$ and $N$:
:$psicirc fcirc phi^{-1} : phi ( U cap f^{-1}(V)) to psi(V)$
is differentiable.


=== Definition 2 ===


$f$  is differentiable  if and only if  $f$ is  differentiable at every point of $M$.


=== At a Point ===
Let $M$ and $N$ be differentiable manifolds.

Let $f: M to N$ be continuous.

Let $p in M$.


==== Definition 1 ====
$f$ is differentiable at $p$  if and only if  for every pair of charts $left( U, phi right)$ and $left( V, psi right)$ of $M$ and $N$ with $p in U$ and $f left(   right)p in V$:
:$psi circ f circ phi^{-1}: phi left(   right){U cap f^{-1}  left(   right)V} to psi left(   right)V$
is differentiable at $phi left(   right)p$.


Category:Definitions/Differentiable Manifolds

==== Definition 2 ====
$f$ is differentiable at $p$  if and only if  there exists a pair of charts $left( U, phi right)$ and $left( V, psi right)$ of $M$ and $N$ with $p in U$ and $f left(   right)p in V$ such that:
:$psi circ f circ phi^{-1}: phi left(   right){U cap f^{-1}  left(   right)V} to psi left(   right)V$
is differentiable at $phi left(   right)p$.


Category:Definitions/Differentiable Manifolds",Definition:Differentiable Mapping,,false,"=== Real Function ===
=== At a Point ===

Let f be a real function defined on an open interval ( a  . . )b.

Let ξ be a point in ( a  . . )b.


==== Definition 1 ====
Let f be a real function defined on an open interval ( a  . . )b.

Let ξ be a point in ( a  . . )b.


f is differentiable at the point ξ  if and only if  the limit:
:lim_x →ξf (   )x - f (   )ξ/x - ξ
exists.

==== Definition 2 ====
Let f be a real function defined on an open interval ( a  . . )b.

Let ξ be a point in ( a  . . )b.


f is differentiable at the point ξ  if and only if  the limit:
:lim_h → 0f (   )ξ + h - f (   )ξ/h
exists.


Category:Definitions/Differentiable Real Functions

These limits, if they exist, are called the derivative of f at ξ.

=== On an Open Interval ===
Let f be a real function defined on an open interval ( a  . . )b.


Then f is differentiable on ( a  . . )b  if and only if  f is differentiable at each point of ( a  . . )b.


=== On a Closed Interval ===
Let f be a real function defined on a closed interval [ a  . . ]b.

Let f be differentiable on the open interval ( a  . . )b.


If the following limit from the right exists:

:lim_x → a^+f (   )x - f (   )a/x - a

as well as this limit from the left:
:lim_x → b^-f (   )x - f (   )b/x - b

then f is differentiable on the closed interval [ a  . . ]b.


Similar definitions for differentiability on a half-open interval can be expressed for a real function which has either a limit from the right at a or a limit from the left at b, but not both.

=== On the Real Number Line ===
Let f be a real function defined on ℝ.

By definition, ℝ is an (unbounded) open interval.


Let f be differentiable on the open interval ℝ.

That is, let f be differentiable at every point of ℝ.


Then f is differentiable everywhere (on ℝ).


Category:Definitions/Differentiable Real Functions

=== Complex Function ===
=== At a Point ===

Let U ⊂ℂ be an open set.

Let f : U →ℂ be a complex function.

Let z_0 ∈ U be a point in U.


Then f is complex-differentiable at z_0  if and only if  the limit:

:lim_h → 0f (   )z_0+h - f (   )z_0/h

exists as a finite number.


This limit, if it exists, is called the derivative of f at z_0.

=== In an Open Set ===
Let U ⊆ℂ be an open set.

Let f : U →ℂ be a complex function.


Then f is holomorphic in U  if and only if  f is differentiable at each point of U.

We also say that f is complex-differentiable in U.


 

Category:Definitions/Complex Differential Calculus

=== Real-Valued Function ===
=== At a Point ===

Let U be an open subset of ℝ^n. 

Let ‖·‖ denote the Euclidean norm on ℝ^n.

Let f: U →ℝ be a real-valued function.

Let x ∈ U.

==== Definition 1 ====
Let U be an open subset of ℝ^n.

Let ‖·‖ denote the Euclidean norm on ℝ^n.

Let f: U →ℝ be a real-valued function.

Let x ∈ U.



f is differentiable at x  if and only if  there exist α_1, …, α_n ∈ℝ and a real-valued function r: U ∖{ x }→ℝ such that:

:(1):   f (   )x + h = f (   )x + α_1 h_1 + ⋯ + α_n h_n + r (   )h ‖ h ‖

:(2):  lim_h → 0 r (   )h = 0


Category:Definitions/Differentiable Real-Valued Functions

==== Definition 2 ====
Let U be an open subset of ℝ^n. 

Let f: U →ℝ be a real-valued function.

Let ‖·‖ denote the Euclidean norm on ℝ^n.

Let x ∈ U.



f is differentiable at x  if and only if  there exists a linear transformation T: ℝ^n →ℝ and a real-valued function r: U ∖{ x }→ℝ such that:

:(1):    f (   )x + h = f (   )x + T (   )h + r (   )h ‖ h ‖
:(2):   lim_h → 0 r (   )h = 0


Category:Definitions/Differentiable Real-Valued Functions

=== In an Open Set ===
Let 𝕏 be an open subset of ℝ^n. 

Let f: 𝕏→ℝ be a real-valued function.


Then f is differentiable in the open set 𝕏  if and only if  f is differentiable at each point of 𝕏.

=== Vector-Valued Function ===
Let m, n ≥ 1 be natural numbers.

=== At a Point ===

Let 𝕏 be an open subset of ℝ^n. 

Let f = ( f_1, f_2, …, f_m )^⊺: 𝕏→ℝ^m be a vector valued function.


f is differentiable at x ∈ℝ^n  if and only if  there exists a linear transformation T: ℝ^n →ℝ^m and a mapping r : U →ℝ^m such that:

:(1):    f (   )x + h = f (   )x + T (   )h + r (   )h ·‖ h ‖

:(2):   lim_h → 0 r (   )h = 0


Category:Definitions/Differentiable Vector-Valued Functions

=== In an Open Set ===
Let 𝕏 be an open subset of ℝ^n. 

Let f = ( f_1, f_2, …, f_m )^⊺: 𝕏→ℝ^m be a vector valued function.

Let S ⊆𝕏.


Then f is differentiable in the open set S  if and only if  f is differentiable at each x in S.


This can be denoted f ∈𝒞^1 (   )S, ℝ^m.

=== Function With Values in Normed Space ===
Let U ⊂ℝ be an open set.

Let ( X, ‖ · ‖_X ) be a normed vector space.

A function f : U → X is (strongly) differentiable at x ∈ U  if and only if  there exists f' (   )x ∈ X such that:

:lim_h → 0‖f (   )x + h - f (   )x/h - f' (   )x ‖_X = 0

Moreover, f is called (strongly) differentiable if it is differentiable at every point of U.

=== Between Differentiable Manifolds ===
Let M and N be differentiable manifolds.

Let f : M → N be continuous.

=== Definition 1 ===

f  is differentiable  if and only if  for every pair of charts (U, ϕ) and (V,ψ) of M and N:
:ψ∘ f∘ϕ^-1 : ϕ ( U ∩ f^-1(V)) →ψ(V)
is differentiable.


=== Definition 2 ===


f  is differentiable  if and only if  f is  differentiable at every point of M.


=== At a Point ===
Let M and N be differentiable manifolds.

Let f: M → N be continuous.

Let p ∈ M.


==== Definition 1 ====
f is differentiable at p  if and only if  for every pair of charts ( U, ϕ) and ( V, ψ) of M and N with p ∈ U and f (   )p ∈ V:
:ψ∘ f ∘ϕ^-1: ϕ(   )U ∩ f^-1(   )V→ψ(   )V
is differentiable at ϕ(   )p.


Category:Definitions/Differentiable Manifolds

==== Definition 2 ====
f is differentiable at p  if and only if  there exists a pair of charts ( U, ϕ) and ( V, ψ) of M and N with p ∈ U and f (   )p ∈ V such that:
:ψ∘ f ∘ϕ^-1: ϕ(   )U ∩ f^-1(   )V→ψ(   )V
is differentiable at ϕ(   )p.


Category:Definitions/Differentiable Manifolds",Differentiable
['Definitions/Calculus of Variations'],Definition:Differentiable,"Let $S$ be a normed linear space of mappings.

Let $y, h in S: mathbb R to mathbb R$ be real functions.

Let $J left[ y right]$, $phi left[ y; h right]$ be functionals.

Let $Delta J left[ y; h right]$ be an increment of the functional $J$ such that:

:$Delta J left[ y; h right] = phi left[ y;h right] + epsilon leftlVert h rightrVert$

where $epsilon = epsilon left[ y; h right]$ is a functional, and $leftlVert h rightrVert$ is the norm of $S$.

Suppose $phi left[ y; h right]$ is a linear   $h$ and:

:$ds lim_{leftlVert h rightrVert mathop to 0} epsilon = 0$


Then the functional $J left[ y right] $ is said to be differentiable.",Definition:Differentiable Functional,,false,"Let S be a normed linear space of mappings.

Let y, h ∈ S: ℝ→ℝ be real functions.

Let J [ y ], ϕ[ y; h ] be functionals.

Let Δ J [ y; h ] be an increment of the functional J such that:

:Δ J [ y; h ] = ϕ[ y;h ] + ϵ‖ h ‖

where ϵ = ϵ[ y; h ] is a functional, and ‖ h ‖ is the norm of S.

Suppose ϕ[ y; h ] is a linear   h and:

:lim_‖ h ‖→ 0ϵ = 0


Then the functional J [ y ] is said to be differentiable.",Differentiable
"['Definitions/Differential Calculus', 'Definitions/Differentiability Classes']",Definition:Differentiable,"A differentiable function $f$ is continuously differentiable  if and only if  $f$ is of differentiability class $C^1$.

That is, if the first order derivative of $f$ (and possibly higher) is continuous.


=== Real Function ===
Let $Isubsetmathbb R$ be an open interval.


Then $f$ is continuously differentiable on $I$  if and only if  $f$ is differentiable on $I$ and its derivative is continuous on $I$.


=== Real-Valued Function ===
Let $U$ be an open subset of $mathbb R^n$. 

Let $f: U to mathbb R$ be a real-valued function.


Then $f$ is continuously differentiable in the open set $U$  if and only if :
:$(1): quad f$ is differentiable in $U$.
:$(2): quad$ the partial derivatives of $f$ are continuous in $U$.


This can be denoted:
:$f in mathcal C^1 left(   right){mathbb X, mathbb R}$


=== Vector-Valued Function ===
Let $U subset mathbb R^n$ be an open set.

Let $f: U to mathbb R^m$ be a vector-valued function.


Then $f$ is continuously differentiable in $U$  if and only if  $f$ is differentiable in $U$ and its partial derivatives are continuous in $U$.


Category:Definitions/Differentiable Vector-Valued Functions",Definition:Continuously Differentiable,,false,"A differentiable function f is continuously differentiable  if and only if  f is of differentiability class C^1.

That is, if the first order derivative of f (and possibly higher) is continuous.


=== Real Function ===
Let I⊂ℝ be an open interval.


Then f is continuously differentiable on I  if and only if  f is differentiable on I and its derivative is continuous on I.


=== Real-Valued Function ===
Let U be an open subset of ℝ^n. 

Let f: U →ℝ be a real-valued function.


Then f is continuously differentiable in the open set U  if and only if :
:(1):    f is differentiable in U.
:(2): the partial derivatives of f are continuous in U.


This can be denoted:
:f ∈𝒞^1 (   )𝕏, ℝ


=== Vector-Valued Function ===
Let U ⊂ℝ^n be an open set.

Let f: U →ℝ^m be a vector-valued function.


Then f is continuously differentiable in U  if and only if  f is differentiable in U and its partial derivatives are continuous in U.


Category:Definitions/Differentiable Vector-Valued Functions",Differentiable
"['Definitions/Dihedrals (Geometry)', 'Definitions/Solid Geometry']",Definition:Dihedral,"A dihedral is a configuration in solid geometry formed by two half-planes meeting at a common straight line.


=== Dihedral Angle ===
A dihedral angle is the angle contained by two straight lines drawn perpendicular to the common section at the same point, one in each of the two half-planes forming a dihedral.",Definition:Dihedral (Geometry),configuration,true,"A dihedral is a configuration in solid geometry formed by two half-planes meeting at a common straight line.


=== Dihedral Angle ===
A dihedral angle is the angle contained by two straight lines drawn perpendicular to the common section at the same point, one in each of the two half-planes forming a dihedral.",Dihedral
"['Definitions/Dihedral Angles', 'Definitions/Dihedrals (Geometry)']",Definition:Dihedral,"A dihedral angle is the angle contained by two straight lines drawn perpendicular to the common section at the same point, one in each of the two half-planes forming a dihedral.",Definition:Dihedral (Geometry)/Angle,,false,"A dihedral angle is the angle contained by two straight lines drawn perpendicular to the common section at the same point, one in each of the two half-planes forming a dihedral.",Dihedral
"['Definitions/Dihedral Groups', 'Definitions/Examples of Groups', 'Definitions/Examples of Symmetry Groups']",Definition:Dihedral,"The dihedral group $D_n$ of order $2 n$ is the group of symmetries of the regular $n$-gon.


=== Even Polygon ===
 

:

=== Odd Polygon ===
 

:",Definition:Dihedral Group,,false,"The dihedral group D_n of order 2 n is the group of symmetries of the regular n-gon.


=== Even Polygon ===
 

:

=== Odd Polygon ===
 

:",Dihedral
"['Definitions/Dimensions (Geometry)', 'Definitions/Geometry']",Definition:Dimension,The dimension of a (geometrical) space is the minimum number of coordinates needed to specify a point in it.,Definition:Dimension (Geometry),,false,The dimension of a (geometrical) space is the minimum number of coordinates needed to specify a point in it.,Dimension
"['Definitions/Dimension (Linear Algebra)', 'Definitions/Module Theory', 'Definitions/Linear Algebra']",Definition:Dimension,"=== Module ===
Let $R$ be a ring with unity.

Let $G$ be a unitary $R$-module which has a basis of $n$ elements.

Then $G$ is said to have a dimension of $n$ or to be $n$-dimensional.


=== Symbol ===


=== Finite Dimensional Module ===
Let $G$ be a (unitary) module which is $n$-dimensional for some $n in mathbb N_{>0}$.

Then $G$ is finite dimensional.

=== Vector Space ===

Let $K$ be a division ring.

Let $V$ be a vector space over $K$.

Let $K$ be a division ring.

Let $V$ be a vector space over $K$.


The dimension of $V$ is the number of vectors in a basis for $V$.

 ",Definition:Dimension (Linear Algebra),,false,"=== Module ===
Let R be a ring with unity.

Let G be a unitary R-module which has a basis of n elements.

Then G is said to have a dimension of n or to be n-dimensional.


=== Symbol ===


=== Finite Dimensional Module ===
Let G be a (unitary) module which is n-dimensional for some n ∈ℕ_>0.

Then G is finite dimensional.

=== Vector Space ===

Let K be a division ring.

Let V be a vector space over K.

Let K be a division ring.

Let V be a vector space over K.


The dimension of V is the number of vectors in a basis for V.

 ",Dimension
"['Definitions/Orders of Matrices', 'Definitions/Matrices']",Definition:Dimension,"Let $left[ a right]_{m n}$ be an $m times n$ matrix.

Then the parameters $m$ and $n$ are known as the order of the matrix.


=== Square Matrix ===
Let $mathbf A$ be an $n times n$ square matrix.

That is, let $mathbf A$ have $n$ rows (and by definition $n$ columns).


Then the order of $mathbf A$ is defined as being $n$.

=== Column Matrix ===
Let $mathbf A$ be an $n times 1$ column matrix.

Then the order of $mathbf A$ is defined as being $n$.


Category:Definitions/Orders of Matrices
Category:Definitions/Column Matrices

=== Row Matrix ===
Let $mathbf A$ be a $1 times n$ row matrix.

Then the order of $mathbf A$ is defined as being $n$.


Category:Definitions/Orders of Matrices
Category:Definitions/Row Matrices",Definition:Matrix/Order,,false,"Let [ a ]_m n be an m × n matrix.

Then the parameters m and n are known as the order of the matrix.


=== Square Matrix ===
Let 𝐀 be an n × n square matrix.

That is, let 𝐀 have n rows (and by definition n columns).


Then the order of 𝐀 is defined as being n.

=== Column Matrix ===
Let 𝐀 be an n × 1 column matrix.

Then the order of 𝐀 is defined as being n.


Category:Definitions/Orders of Matrices
Category:Definitions/Column Matrices

=== Row Matrix ===
Let 𝐀 be a 1 × n row matrix.

Then the order of 𝐀 is defined as being n.


Category:Definitions/Orders of Matrices
Category:Definitions/Row Matrices",Dimension
['Definitions/Hilbert Spaces'],Definition:Dimension,"Let $H$ be a Hilbert space, and let $E$ be a basis of $H$.


Then the dimension $dim H$ of $H$ is defined as $leftlvert E rightrvert$, the cardinality of $E$.",Definition:Dimension (Hilbert Space),,false,"Let H be a Hilbert space, and let E be a basis of H.


Then the dimension H of H is defined as | E |, the cardinality of E.",Dimension
['Definitions/Topology'],Definition:Dimension,"=== Locally Euclidean Space ===
Let $M$ be a locally Euclidean space. 

Let $left( U, kappa right)$ be a coordinate chart such that: 
:$kappa: U to kappa left(   right)U subseteq mathbb R^n$
for some $n in mathbb N$.


Then the natural number $n$ is called the dimension of $M$.

=== Hausdorff Dimension ===
 

=== Lebesgue Covering Dimension ===
 

=== Inductive Dimension ===
 

Category:Definitions/Topology",Definition:Dimension (Topology),,false,"=== Locally Euclidean Space ===
Let M be a locally Euclidean space. 

Let ( U, κ) be a coordinate chart such that: 
:κ: U →κ(   )U ⊆ℝ^n
for some n ∈ℕ.


Then the natural number n is called the dimension of M.

=== Hausdorff Dimension ===
 

=== Lebesgue Covering Dimension ===
 

=== Inductive Dimension ===
 

Category:Definitions/Topology",Dimension
['Definitions/Topology'],Definition:Dimension,"Let $M$ be a locally Euclidean space. 

Let $left( U, kappa right)$ be a coordinate chart such that: 
:$kappa: U to kappa left(   right)U subseteq mathbb R^n$
for some $n in mathbb N$.


Then the natural number $n$ is called the dimension of $M$.",Definition:Dimension (Topology)/Locally Euclidean Space,,false,"Let M be a locally Euclidean space. 

Let ( U, κ) be a coordinate chart such that: 
:κ: U →κ(   )U ⊆ℝ^n
for some n ∈ℕ.


Then the natural number n is called the dimension of M.",Dimension
"['Definitions/Hausdorff-Besicovitch Dimension', 'Definitions/Fractals']",Definition:Dimension,"Let $mathbb R^n$ be the $n$-dimensional Euclidean space.


Let $F subseteq mathbb R^n$.

The Hausdorff-Besicovitch dimension of $F$ is defined as:

 
 
 
 
where $mathcal H^s left(   right)cdot$ denotes the $s$-dimensional Hausdorff measure on $mathbb R^n$.",Definition:Hausdorff-Besicovitch Dimension,,false,"Let ℝ^n be the n-dimensional Euclidean space.


Let F ⊆ℝ^n.

The Hausdorff-Besicovitch dimension of F is defined as:

 
 
 
 
where ℋ^s (   )· denotes the s-dimensional Hausdorff measure on ℝ^n.",Dimension
['Definitions/Representation Theory'],Definition:Dimension,"Let $left( k, +, circ right)$ be a field.

Let $V$ be a vector space over $k$ of finite dimension.

Let $mathrm {GL} left( V right)$ be the general linear group of $V$.

Let $left( G, cdot right)$ be a finite group.

Let $rho: G to mathrm {GL} left( V right)$ be a linear representation of $G$ on $V$.


The dimension or degree of $rho$, written $deg left(   right)rho$ is the dimension of the vector space $V$.

Category:Definitions/Representation Theory",Definition:Dimension (Representation Theory),,false,"Let ( k, +, ∘) be a field.

Let V be a vector space over k of finite dimension.

Let GL( V ) be the general linear group of V.

Let ( G, ·) be a finite group.

Let ρ: G →GL( V ) be a linear representation of G on V.


The dimension or degree of ρ, written (   )ρ is the dimension of the vector space V.

Category:Definitions/Representation Theory",Dimension
['Definitions/Algebraic Geometry'],Definition:Dimension,"=== Krull Dimension of a Ring ===
Let $left( R, +, circ right)$ be a commutative ring with unity.


The Krull dimension of $R$ is the supremum of lengths of chains of prime ideals, ordered by the subset relation:
 
 
 
 
where:
:$mathrm {ht}  left(   right){mathfrak p}$ is the height of $mathfrak p$
:$mathrm {Spec} left( R right)$ is the prime spectrum of $R$



In particular, the Krull dimension is $infty$ if there exist arbitrarily long chains.

=== Krull Dimension of a Topological Space ===
Let $T$ be a topological space.


Its Krull dimension $dim_{mathrm {Krull} }  left(   right)T$ is the supremum of lengths of chains of closed irreducible sets of $T$, ordered by the subset relation.

Thus, the Krull dimension is $infty$ if there exist arbitrarily long chains.

 

Category:Definitions/Algebraic Geometry",Definition:Krull Dimension,,false,"=== Krull Dimension of a Ring ===
Let ( R, +, ∘) be a commutative ring with unity.


The Krull dimension of R is the supremum of lengths of chains of prime ideals, ordered by the subset relation:
 
 
 
 
where:
:ht(   )𝔭 is the height of 𝔭
:Spec( R ) is the prime spectrum of R



In particular, the Krull dimension is ∞ if there exist arbitrarily long chains.

=== Krull Dimension of a Topological Space ===
Let T be a topological space.


Its Krull dimension _Krull(   )T is the supremum of lengths of chains of closed irreducible sets of T, ordered by the subset relation.

Thus, the Krull dimension is ∞ if there exist arbitrarily long chains.

 

Category:Definitions/Algebraic Geometry",Dimension
"['Definitions/Ring Theory', 'Definitions/Ideal Theory', 'Definitions/Commutative Algebra']",Definition:Dimension,"Let $left( R, +, circ right)$ be a commutative ring with unity.


The Krull dimension of $R$ is the supremum of lengths of chains of prime ideals, ordered by the subset relation:
 
 
 
 
where:
:$mathrm {ht}  left(   right){mathfrak p}$ is the height of $mathfrak p$
:$mathrm {Spec} left( R right)$ is the prime spectrum of $R$



In particular, the Krull dimension is $infty$ if there exist arbitrarily long chains.",Definition:Krull Dimension of Ring,,false,"Let ( R, +, ∘) be a commutative ring with unity.


The Krull dimension of R is the supremum of lengths of chains of prime ideals, ordered by the subset relation:
 
 
 
 
where:
:ht(   )𝔭 is the height of 𝔭
:Spec( R ) is the prime spectrum of R



In particular, the Krull dimension is ∞ if there exist arbitrarily long chains.",Dimension
['Definitions/Irreducible Spaces'],Definition:Dimension,"Let $T$ be a topological space.


Its Krull dimension $dim_{mathrm {Krull} }  left(   right)T$ is the supremum of lengths of chains of closed irreducible sets of $T$, ordered by the subset relation.

Thus, the Krull dimension is $infty$ if there exist arbitrarily long chains.",Definition:Krull Dimension of Topological Space,,false,"Let T be a topological space.


Its Krull dimension _Krull(   )T is the supremum of lengths of chains of closed irreducible sets of T, ordered by the subset relation.

Thus, the Krull dimension is ∞ if there exist arbitrarily long chains.",Dimension
"['Definitions/Order of Differential Equation', 'Definitions/Differential Equations']",Definition:Dimension,The order of a differential equation is defined as being the order of the highest order derivative that is present in the equation.,Definition:Differential Equation/Order,,false,The order of a differential equation is defined as being the order of the highest order derivative that is present in the equation.,Dimension
['Definitions/Configuration Spaces'],Definition:Dimension,The dimension of a configuration space $S$ is the number of degrees of freedom of the system defined by $S$.,Definition:Configuration Space/Dimension,,false,The dimension of a configuration space S is the number of degrees of freedom of the system defined by S.,Dimension
"['Definitions/Dimensions of Measurement', 'Definitions/Dimensional Analysis', 'Definitions/Measurement', 'Definitions/Physical Quantities', 'Definitions/Physics', 'Definitions/Applied Mathematics']",Definition:Dimension,"Every physical quantity has a dimension associated with it.

No attempt is made here to provide an abstract definition of this term. Instead, it will be defined by example.


=== Fundamental Dimensions ===
The SI-recommended fundamental dimensions are:

 
 
 
 
 
 
 
 
 

=== Units ===
Compare with units of measurement.

This concept of dimension is more abstract than that of units, which are standard quantities of the particular dimension in question.


=== Examples ===
 

Category:Definitions/Units of Measurement
Category:Definitions/Dimensions of Measurement",Definition:Dimension (Measurement),,false,"Every physical quantity has a dimension associated with it.

No attempt is made here to provide an abstract definition of this term. Instead, it will be defined by example.


=== Fundamental Dimensions ===
The SI-recommended fundamental dimensions are:

 
 
 
 
 
 
 
 
 

=== Units ===
Compare with units of measurement.

This concept of dimension is more abstract than that of units, which are standard quantities of the particular dimension in question.


=== Examples ===
 

Category:Definitions/Units of Measurement
Category:Definitions/Dimensions of Measurement",Dimension
['Definitions/Images'],Definition:Direct Image,"Let $f: S to T$ be a mapping.


=== Definition 1 ===
The image of a mapping $f: S to T$ is the set:

:$mathrm {Img} left( f right) = leftlbrace t in T: exists s in S: f left(   right)s = t rightrbrace$

That is, it is the set of values taken by $f$.

=== Definition 2 ===
The image of a mapping $f: S to T$ is the set:

:$mathrm {Img} left( f right) = f left[ S right]$

where $f left[ S right]$ is the image of $S$ under $f$.


=== Class Theory ===

 
Let $V$ be a basic universe.

Let $A subseteq V$ and $B subseteq V$ be classes.

Let $f: A to B$ be a class mapping.

The image of $mathcal R$ is defined and denoted as:
:$mathrm {Img} left( mathcal R right) := leftlbrace y in V: exists x in V: left( x, y right) in mathcal R rightrbrace$

That is, it is the class of all $y$ such that $left( x, y right) in mathcal R$ for at least one $x$.",Definition:Image (Relation Theory)/Mapping/Mapping,,false,"Let f: S → T be a mapping.


=== Definition 1 ===
The image of a mapping f: S → T is the set:

:Img( f ) = { t ∈ T: ∃ s ∈ S: f (   )s = t }

That is, it is the set of values taken by f.

=== Definition 2 ===
The image of a mapping f: S → T is the set:

:Img( f ) = f [ S ]

where f [ S ] is the image of S under f.


=== Class Theory ===

 
Let V be a basic universe.

Let A ⊆ V and B ⊆ V be classes.

Let f: A → B be a class mapping.

The image of ℛ is defined and denoted as:
:Img( ℛ) := { y ∈ V: ∃ x ∈ V: ( x, y ) ∈ℛ}

That is, it is the class of all y such that ( x, y ) ∈ℛ for at least one x.",Direct Image
"['Definitions/Images', 'Definitions/Relations']",Definition:Direct Image,"Let $mathcal R subseteq S times T$ be a relation.


The image of $mathcal R$ is defined as:

:$mathrm {Img} left( mathcal R right) := mathcal R left[ S right] = leftlbrace t in T: exists s in S: left( s, t right) in mathcal R rightrbrace$


=== General Definition ===
Let $ds prod_{i mathop = 1}^n S_i$ be the cartesian product of sets $S_1$ to $S_n$.

Let $ds mathcal R subseteq prod_{i mathop = 1}^n S_i$ be an $n$-ary relation on $ds prod_{i mathop = 1}^n S_i$.

The image of $mathcal R$ is the set defined as:
:$mathrm {Img} left( mathcal R right) := leftlbrace s_n in S_n: exists left( s_1, s_2, ldots, s_{n - 1}  right) in ds prod_{i mathop = 1}^{n - 1} S_i: left( s_1, s_2, ldots, s_n right) in mathcal R rightrbrace$


The concept is usually encountered when $mathcal R$ is an endorelation on $S$:
:$mathrm {Img} left( mathcal R right) := leftlbrace s_n in S: exists left( s_1, s_2, ldots, s_{n - 1}  right) in S^{n - 1}: left( s_1, s_2, ldots, s_n right) in mathcal R rightrbrace$

=== Class Theory ===

 
Let $V$ be a basic universe.

Let $mathcal R subseteq V times V$ be a relation in $V$.

The image of $mathcal R$ is defined and denoted as:
:$mathrm {Img} left( mathcal R right) := leftlbrace y in V: exists x in V: left( x, y right) in mathcal R rightrbrace$

That is, it is the class of all $y$ such that $left( x, y right) in mathcal R$ for at least one $x$.",Definition:Image (Relation Theory)/Relation/Relation,,false,"Let ℛ⊆ S × T be a relation.


The image of ℛ is defined as:

:Img( ℛ) := ℛ[ S ] = { t ∈ T: ∃ s ∈ S: ( s, t ) ∈ℛ}


=== General Definition ===
Let ∏_i  = 1^n S_i be the cartesian product of sets S_1 to S_n.

Let ℛ⊆∏_i  = 1^n S_i be an n-ary relation on ∏_i  = 1^n S_i.

The image of ℛ is the set defined as:
:Img( ℛ) := { s_n ∈ S_n: ∃( s_1, s_2, …, s_n - 1) ∈∏_i  = 1^n - 1 S_i: ( s_1, s_2, …, s_n ) ∈ℛ}


The concept is usually encountered when ℛ is an endorelation on S:
:Img( ℛ) := { s_n ∈ S: ∃( s_1, s_2, …, s_n - 1) ∈ S^n - 1: ( s_1, s_2, …, s_n ) ∈ℛ}

=== Class Theory ===

 
Let V be a basic universe.

Let ℛ⊆ V × V be a relation in V.

The image of ℛ is defined and denoted as:
:Img( ℛ) := { y ∈ V: ∃ x ∈ V: ( x, y ) ∈ℛ}

That is, it is the class of all y such that ( x, y ) ∈ℛ for at least one x.",Direct Image
"['Definitions/Induced Mappings', 'Definitions/Images', 'Definitions/Direct Image Mappings']",Definition:Direct Image,"Let $S$ and $T$ be sets.

Let $mathcal P left( S right)$ and $mathcal P left( T right)$ be their power sets.


=== Relation ===
Let $S$ and $T$ be sets.

Let $mathcal P left( S right)$ and $mathcal P left( T right)$ be their power sets.

Let $mathcal R subseteq S times T$ be a relation on $S times T$.


The direct image mapping of $mathcal R$ is the mapping $mathcal R^to: mathcal P left( S right) to mathcal P left( T right)$ that sends a subset $X subseteq T$ to its image under $mathcal R$:

:$forall X in mathcal P left( S right): mathcal R^to left(   right)X = begin {cases} leftlbrace t in T: exists s in X: left( s, t right) in mathcal R rightrbrace & : X ne varnothing \ varnothing & : X = varnothing end {cases}$

=== Mapping ===
Let $S$ and $T$ be sets.

Let $mathcal P left( S right)$ and $mathcal P left( T right)$ be their power sets.

Let $f subseteq S times T$ be a mapping from $S$ to $T$.


The direct image mapping of $f$ is the mapping $f^to: mathcal P left( S right) to mathcal P left( T right)$ that sends a subset $X subseteq S$ to its image under $f$:
:$forall X in mathcal P left( S right): f^to left(   right)X = begin {cases} leftlbrace t in T: exists s in X: f left(   right)s = t rightrbrace & : X ne varnothing \ varnothing & : X = varnothing end {cases}$


=== Direct Image Mapping as Set of Images of Subsets ===

 

The direct image mapping of $f$ can be seen to be the set of images of all the subsets of the domain of $f$:

:$forall X subseteq S: f left[ X right] = f^to left(   right)X$


Both approaches to this concept are used in  .",Definition:Direct Image Mapping,,false,"Let S and T be sets.

Let 𝒫( S ) and 𝒫( T ) be their power sets.


=== Relation ===
Let S and T be sets.

Let 𝒫( S ) and 𝒫( T ) be their power sets.

Let ℛ⊆ S × T be a relation on S × T.


The direct image mapping of ℛ is the mapping ℛ^→: 𝒫( S ) →𝒫( T ) that sends a subset X ⊆ T to its image under ℛ:

:∀ X ∈𝒫( S ): ℛ^→(   )X = { t ∈ T: ∃ s ∈ X: ( s, t ) ∈ℛ}    : X ∅
∅    : X = ∅

=== Mapping ===
Let S and T be sets.

Let 𝒫( S ) and 𝒫( T ) be their power sets.

Let f ⊆ S × T be a mapping from S to T.


The direct image mapping of f is the mapping f^→: 𝒫( S ) →𝒫( T ) that sends a subset X ⊆ S to its image under f:
:∀ X ∈𝒫( S ): f^→(   )X = { t ∈ T: ∃ s ∈ X: f (   )s = t }    : X ∅
∅    : X = ∅


=== Direct Image Mapping as Set of Images of Subsets ===

 

The direct image mapping of f can be seen to be the set of images of all the subsets of the domain of f:

:∀ X ⊆ S: f [ X ] = f^→(   )X


Both approaches to this concept are used in  .",Direct Image
['Definitions/Direct Image Mappings'],Definition:Direct Image,"Let $S$ and $T$ be sets.

Let $mathcal P left( S right)$ and $mathcal P left( T right)$ be their power sets.

Let $f subseteq S times T$ be a mapping from $S$ to $T$.


The direct image mapping of $f$ is the mapping $f^to: mathcal P left( S right) to mathcal P left( T right)$ that sends a subset $X subseteq S$ to its image under $f$:
:$forall X in mathcal P left( S right): f^to left(   right)X = begin {cases} leftlbrace t in T: exists s in X: f left(   right)s = t rightrbrace & : X ne varnothing \ varnothing & : X = varnothing end {cases}$


=== Direct Image Mapping as Set of Images of Subsets ===

 

The direct image mapping of $f$ can be seen to be the set of images of all the subsets of the domain of $f$:

:$forall X subseteq S: f left[ X right] = f^to left(   right)X$


Both approaches to this concept are used in  .",Definition:Direct Image Mapping/Mapping,,false,"Let S and T be sets.

Let 𝒫( S ) and 𝒫( T ) be their power sets.

Let f ⊆ S × T be a mapping from S to T.


The direct image mapping of f is the mapping f^→: 𝒫( S ) →𝒫( T ) that sends a subset X ⊆ S to its image under f:
:∀ X ∈𝒫( S ): f^→(   )X = { t ∈ T: ∃ s ∈ X: f (   )s = t }    : X ∅
∅    : X = ∅


=== Direct Image Mapping as Set of Images of Subsets ===

 

The direct image mapping of f can be seen to be the set of images of all the subsets of the domain of f:

:∀ X ⊆ S: f [ X ] = f^→(   )X


Both approaches to this concept are used in  .",Direct Image
['Definitions/Direct Image Mappings'],Definition:Direct Image,"Let $S$ and $T$ be sets.

Let $mathcal P left( S right)$ and $mathcal P left( T right)$ be their power sets.

Let $mathcal R subseteq S times T$ be a relation on $S times T$.


The direct image mapping of $mathcal R$ is the mapping $mathcal R^to: mathcal P left( S right) to mathcal P left( T right)$ that sends a subset $X subseteq T$ to its image under $mathcal R$:

:$forall X in mathcal P left( S right): mathcal R^to left(   right)X = begin {cases} leftlbrace t in T: exists s in X: left( s, t right) in mathcal R rightrbrace & : X ne varnothing \ varnothing & : X = varnothing end {cases}$",Definition:Direct Image Mapping/Relation,,false,"Let S and T be sets.

Let 𝒫( S ) and 𝒫( T ) be their power sets.

Let ℛ⊆ S × T be a relation on S × T.


The direct image mapping of ℛ is the mapping ℛ^→: 𝒫( S ) →𝒫( T ) that sends a subset X ⊆ T to its image under ℛ:

:∀ X ∈𝒫( S ): ℛ^→(   )X = { t ∈ T: ∃ s ∈ X: ( s, t ) ∈ℛ}    : X ∅
∅    : X = ∅",Direct Image
['Definitions/Sheaf Theory'],Definition:Direct Image,"Let $X$ be a topological space.

Let $mathbf C$ be a category.

Let $mathcal F$ be a $mathbf C$-valued sheaf on $X$.


The direct image of $mathcal F$ is the direct image of the presheaf $mathcal F$.",Definition:Direct Image of Sheaf,,false,"Let X be a topological space.

Let 𝐂 be a category.

Let ℱ be a 𝐂-valued sheaf on X.


The direct image of ℱ is the direct image of the presheaf ℱ.",Direct Image
"['Definitions/Digraphs', 'Definitions/Graph Theory']",Definition:Directed,"A digraph is a graph each of whose edges has a direction:



In the above graph, the vertices are $v_1, v_2, v_3$ and $v_4$.


=== Arc ===
Let $G = left( V, E right)$ be a digraph.

The arcs are the elements of $E$.

Informally, an arc is a line that joins one vertex to another.


If $e in E$ is an arc joining the vertex $u$ to the vertex $v$, it is denoted $u v$.


=== Endvertex ===
Let $D = left( V, E right)$ be a digraph.

Let $e = u v$ be an arc of $D$, that is, $e in E$.


The endvertices of $e$ are the vertices $u$ and $v$.


=== Initial Vertex ===
Let $D = left( V, E right)$ be a digraph.

Let $e = u v$ be an arc of $D$, that is, $e in E$.


The initial vertex of $e$ is the endvertex $u$ which $e$ is incident from.

=== Final Vertex ===
Let $D = left( V, E right)$ be a digraph.

Let $e = u v$ be an arc of $D$, that is, $e in E$.


The final vertex of $e$ is the endvertex $v$ which $e$ is incident to.

In the above graph, the arcs are $v_1 v_2, v_2 v_4, v_4 v_3, v_4 v_1$ and $v_1 v_4$.


As can be seen, in this general definition it is allowable for an arc to go in both directions between a given pair of vertices.


=== Formal Definition ===



A directed graph or digraph $D$ is a non-empty set $V$ together with an antireflexive relation $E$ on $V$.

The elements of $E$ are the arcs.


Thus the above digraph can be defined as:

:$D = left( V, E right):$
::$V = leftlbrace v_1, v_2, v_3, v_4 rightrbrace$
::$E = leftlbrace left( v_1, v_2 right), left( v_2, v_4 right), left( v_4, v_3 right), left( v_4, v_1 right), left( v_1, v_4 right)  rightrbrace$

=== Category-Theoretic Definition ===
Let $mathbf {Set}$ be the category of sets.

A digraph is an arrangement of the following form in $mathbf{Set}$:

::$begin{xy}
<0em,0em>*{E} = ""E"",
<5em,0em>*{V} = ""V"",

""E""+/^.3em/+/r1em/;""V""+/^.3em/+/l1em/ **@{-} ?>*@{>} ?*!/_.6em/{s},
""E""+/_.3em/+/r1em/;""V""+/_.3em/+/l1em/ **@{-} ?>*@{>} ?*!/^.6em/{t},
end{xy}$

=== Symmetric Digraph ===
Let $D = left( V, E right)$ be a digraph such that the relation $E$ in $D$ is symmetric.

Then $D$ is called a symmetric digraph.


It follows from the definition of a (simple) graph that a symmetric digraph is in fact the same thing as an undirected graph.

=== Simple Digraph ===
Let $D = left( V, E right)$ be a digraph.

If the relation $E$ in $D$ is also specifically asymmetric, then $D$ is called a simple digraph.

That is, in a simple digraph there are no pairs of arcs (like there are between $v_1$ and $v_4$ in the diagram above) which go in both directions between two vertices.",Definition:Digraph,,false,"A digraph is a graph each of whose edges has a direction:



In the above graph, the vertices are v_1, v_2, v_3 and v_4.


=== Arc ===
Let G = ( V, E ) be a digraph.

The arcs are the elements of E.

Informally, an arc is a line that joins one vertex to another.


If e ∈ E is an arc joining the vertex u to the vertex v, it is denoted u v.


=== Endvertex ===
Let D = ( V, E ) be a digraph.

Let e = u v be an arc of D, that is, e ∈ E.


The endvertices of e are the vertices u and v.


=== Initial Vertex ===
Let D = ( V, E ) be a digraph.

Let e = u v be an arc of D, that is, e ∈ E.


The initial vertex of e is the endvertex u which e is incident from.

=== Final Vertex ===
Let D = ( V, E ) be a digraph.

Let e = u v be an arc of D, that is, e ∈ E.


The final vertex of e is the endvertex v which e is incident to.

In the above graph, the arcs are v_1 v_2, v_2 v_4, v_4 v_3, v_4 v_1 and v_1 v_4.


As can be seen, in this general definition it is allowable for an arc to go in both directions between a given pair of vertices.


=== Formal Definition ===



A directed graph or digraph D is a non-empty set V together with an antireflexive relation E on V.

The elements of E are the arcs.


Thus the above digraph can be defined as:

:D = ( V, E ):
::V = { v_1, v_2, v_3, v_4 }
::E = {( v_1, v_2 ), ( v_2, v_4 ), ( v_4, v_3 ), ( v_4, v_1 ), ( v_1, v_4 )  }

=== Category-Theoretic Definition ===
Let 𝐒𝐞𝐭 be the category of sets.

A digraph is an arrangement of the following form in 𝐒𝐞𝐭:

::<0em,0em>*E = ""E"",
<5em,0em>*V = ""V"",

""E""+/^.3em/+/r1em/;""V""+/^.3em/+/l1em/ **@- ?>*@> ?*!/_.6em/s,
""E""+/_.3em/+/r1em/;""V""+/_.3em/+/l1em/ **@- ?>*@> ?*!/^.6em/t,

=== Symmetric Digraph ===
Let D = ( V, E ) be a digraph such that the relation E in D is symmetric.

Then D is called a symmetric digraph.


It follows from the definition of a (simple) graph that a symmetric digraph is in fact the same thing as an undirected graph.

=== Simple Digraph ===
Let D = ( V, E ) be a digraph.

If the relation E in D is also specifically asymmetric, then D is called a simple digraph.

That is, in a simple digraph there are no pairs of arcs (like there are between v_1 and v_4 in the diagram above) which go in both directions between two vertices.",Directed
['Definitions/Preorder Theory'],Definition:Directed,"Let $left( S, precsim right)$ be a preordered set.


Then $left( S, precsim right)$ is a directed preordering  if and only if  every pair of elements of $S$ has an upper bound in $S$:
:$forall x, y in S: exists z in S: x precsim z$ and $y precsim z$",Definition:Directed Preordering,,false,"Let ( S, ≾) be a preordered set.


Then ( S, ≾) is a directed preordering  if and only if  every pair of elements of S has an upper bound in S:
:∀ x, y ∈ S: ∃ z ∈ S: x ≾ z and y ≾ z",Directed
['Definitions/Preorder Theory'],Definition:Directed,"Let $left( S, precsim right)$ be a preordered set.

Let $H$ be a non-empty subset of $S$.

Then $H$ is a directed subset of $S$  if and only if :

:$forall x, y in H: exists z in H: x precsim z$ and $y precsim z$",Definition:Directed Subset,,false,"Let ( S, ≾) be a preordered set.

Let H be a non-empty subset of S.

Then H is a directed subset of S  if and only if :

:∀ x, y ∈ H: ∃ z ∈ H: x ≾ z and y ≾ z",Directed
['Definitions/Vector Analysis'],Definition:Directed,"Let $mathbb R^n$ be a real cartesian space of $n$ dimensions.

Let $rho: left[{a ,.,., b}right] to mathbb R^n$ be a smooth path in $mathbb R^n$.


The directed smooth curve with parameterization $rho$ is defined as an equivalence class of smooth paths as follows:

A smooth path $sigma: left[{a ,.,., b}right] to mathbb R^n$ belongs to the equivalence class of $rho$  if and only if :
: there exists a bijective differentiable strictly increasing real function:
:: $phi: left[{c ,.,., d}right] to left[{a ,.,., b}right]$
: such that $sigma = rho circ phi$.


It follows from Directed Smooth Curve Relation is Equivalence and Fundamental Theorem on Equivalence Relations that this does in fact define an equivalence class.


If a directed smooth curve is only defined by a smooth path $rho$, then it is often denoted with the same symbol $rho$.


=== Parameterization ===
Let $mathbb R^n$ be a real cartesian space of $n$ dimensions.

Let $C$ be a directed smooth curve in $mathbb R^n$.

Let $rho: left[ a ,.,.,   right]b to mathbb C$ be a smooth path in $mathbb R^n$.


Then $rho$ is a parameterization of $C$  if and only if  $rho$ is an element of the equivalence class that constitutes $C$.


=== Complex Plane ===

The definition carries over to the complex plane, in which context it is usually applied:

Let $C$ be a directed smooth curve in the complex plane $mathbb C$.

Let $gamma: left[ a ,.,.,   right]b to mathbb C$ be a smooth path in $mathbb C$.


Then $gamma$ is a parameterization of $C$  if and only if  $gamma$ is a representative of the equivalence class that constitutes $C$.


=== Reparameterization ===
Let $gamma : left[ a ,.,.,   right]b to mathbb C$ be a smooth path in $mathbb C$.

Let $C$ be a directed smooth curve in the complex plane $mathbb C$ parameterized by $gamma$.

Let $phi: left[ c ,.,.,   right]d to left[ a ,.,.,   right]b$ be a bijective differentiable strictly increasing real function.

Let $sigma : left[ c ,.,.,   right]d to mathbb C$ be defined by:

:$sigma = gamma circ phi$ 


Then $sigma$ is called a reparameterization of $C$.

=== Endpoints ===
Let $mathbb R^n$ be a real cartesian space of $n$ dimensions.

Let $C$ be a directed smooth curve in $mathbb R^n$.


Let $C$ be parameterized by a smooth path $rho: left[{a ,.,., b}right] to mathbb C$.


Then:
: $rho left({a}right)$ is the start point of $C$

: $rho left({b}right)$ is the end point of $C$.


Collectively, $rho left({a}right)$ and $rho left({b}right)$ are known as the endpoints of $rho$.


=== Complex Plane ===

The definition carries over to the complex plane, in which context it is usually applied:

Let $C$ be a directed smooth curve in the complex plane $mathbb C$.

Let $C$ be parameterized by a smooth path $gamma: left[{a ,.,., b}right] to mathbb C$.


Then:
: $gamma left({a}right)$ is the start point of $C$

: $gamma left({b}right)$ is the end point of $C$.


Collectively, $gamma left({a}right)$ and $gamma left({b}right)$ are known as the endpoints of $rho$.",Definition:Directed Smooth Curve,,false,"Let ℝ^n be a real cartesian space of n dimensions.

Let ρ: [a  . .  b] →ℝ^n be a smooth path in ℝ^n.


The directed smooth curve with parameterization ρ is defined as an equivalence class of smooth paths as follows:

A smooth path σ: [a  . .  b] →ℝ^n belongs to the equivalence class of ρ  if and only if :
: there exists a bijective differentiable strictly increasing real function:
:: ϕ: [c  . .  d] →[a  . .  b]
: such that σ = ρ∘ϕ.


It follows from Directed Smooth Curve Relation is Equivalence and Fundamental Theorem on Equivalence Relations that this does in fact define an equivalence class.


If a directed smooth curve is only defined by a smooth path ρ, then it is often denoted with the same symbol ρ.


=== Parameterization ===
Let ℝ^n be a real cartesian space of n dimensions.

Let C be a directed smooth curve in ℝ^n.

Let ρ: [ a  . . ]b →ℂ be a smooth path in ℝ^n.


Then ρ is a parameterization of C  if and only if  ρ is an element of the equivalence class that constitutes C.


=== Complex Plane ===

The definition carries over to the complex plane, in which context it is usually applied:

Let C be a directed smooth curve in the complex plane ℂ.

Let γ: [ a  . . ]b →ℂ be a smooth path in ℂ.


Then γ is a parameterization of C  if and only if  γ is a representative of the equivalence class that constitutes C.


=== Reparameterization ===
Let γ : [ a  . . ]b →ℂ be a smooth path in ℂ.

Let C be a directed smooth curve in the complex plane ℂ parameterized by γ.

Let ϕ: [ c  . . ]d →[ a  . . ]b be a bijective differentiable strictly increasing real function.

Let σ : [ c  . . ]d →ℂ be defined by:

:σ = γ∘ϕ 


Then σ is called a reparameterization of C.

=== Endpoints ===
Let ℝ^n be a real cartesian space of n dimensions.

Let C be a directed smooth curve in ℝ^n.


Let C be parameterized by a smooth path ρ: [a  . .  b] →ℂ.


Then:
: ρ(a) is the start point of C

: ρ(b) is the end point of C.


Collectively, ρ(a) and ρ(b) are known as the endpoints of ρ.


=== Complex Plane ===

The definition carries over to the complex plane, in which context it is usually applied:

Let C be a directed smooth curve in the complex plane ℂ.

Let C be parameterized by a smooth path γ: [a  . .  b] →ℂ.


Then:
: γ(a) is the start point of C

: γ(b) is the end point of C.


Collectively, γ(a) and γ(b) are known as the endpoints of ρ.",Directed
"['Definitions/Directed Line Segments', 'Definitions/Straight Lines', 'Definitions/Analytic Geometry', 'Definitions/Vectors', 'Definitions/Applied Mathematics']",Definition:Directed,"A directed line segment is a line segment endowed with the additional property of direction.

It is often used in the context of applied mathematics to represent a vector quantity.


 

 ",Definition:Directed Line Segment,,false,"A directed line segment is a line segment endowed with the additional property of direction.

It is often used in the context of applied mathematics to represent a vector quantity.


 

 ",Directed
['Definitions/Digraphs'],Definition:Directed,"There are two versions of the Directed Hamilton Cycle Problem.


=== Function Version ===
:Given a digraph $G$ with $n$ vertices, to find a Hamilton cycle in $G$.

=== Decision Version ===
:Given a digraph $G$ with $n$ vertices, to determine whether $G$ has a Hamilton cycle.",Definition:Directed Hamilton Cycle Problem,,false,"There are two versions of the Directed Hamilton Cycle Problem.


=== Function Version ===
:Given a digraph G with n vertices, to find a Hamilton cycle in G.

=== Decision Version ===
:Given a digraph G with n vertices, to determine whether G has a Hamilton cycle.",Directed
"['Definitions/Digraphs', 'Definitions/Walks']",Definition:Directed,"Let $G = left( V, A right)$ be a digraph.


A directed walk in $G$ is a finite or infinite sequence $leftlangle x_k rightrangle$ such that:

:$forall k in mathbb N: k + 1 in mathrm {Dom} left( leftlangle x_k rightrangle  right): left( x_k, x_{k + 1}  right) in A$",Definition:Directed Walk,,false,"Let G = ( V, A ) be a digraph.


A directed walk in G is a finite or infinite sequence ⟨ x_k ⟩ such that:

:∀ k ∈ℕ: k + 1 ∈Dom( ⟨ x_k ⟩): ( x_k, x_k + 1) ∈ A",Directed
['Definitions/Network Theory'],Definition:Directed,"A directed network is a network whose underlying graph is a digraph:


:",Definition:Network/Directed,,false,"A directed network is a network whose underlying graph is a digraph:


:",Directed
['Definitions/Examples of Limits and Colimits'],Definition:Directed,"Let $mathbf C$ be a category.

Let $I$ be a directed ordered set.

Let $mathbf I$ be the order category of $I$.

Let $D : mathbf I to mathbf C$ be a diagram.


The direct limit of $D$ is the colimit of the diagram $D$.",Definition:Direct Limit,,false,"Let 𝐂 be a category.

Let I be a directed ordered set.

Let 𝐈 be the order category of I.

Let D : 𝐈→𝐂 be a diagram.


The direct limit of D is the colimit of the diagram D.",Directed
"['Definitions/Directrices of Ruled Surfaces', 'Definitions/Directrices', 'Definitions/Analytic Geometry', 'Definitions/Geometry']",Definition:Directrix,"A directrix is a curve which defines the generators of a ruled surface.


=== Directrix of Cone ===
Let $K$ be a cone.

Let $B$ be the base of $K$.


The boundary of $B$ is known as the directrix of $K$.

=== Directrix of Cylindrical Surface ===
The directrix of a cylindrical surface $S$ is the curve $C$ through which pass all the parallel straight lines forming $S$.",Definition:Directrix of Ruled Surface,curve,true,"A directrix is a curve which defines the generators of a ruled surface.


=== Directrix of Cone ===
Let K be a cone.

Let B be the base of K.


The boundary of B is known as the directrix of K.

=== Directrix of Cylindrical Surface ===
The directrix of a cylindrical surface S is the curve C through which pass all the parallel straight lines forming S.",Directrix
"['Definitions/Directrices', 'Definitions/Conic Sections']",Definition:Directrix,"Let $K$ be a conic section specified in terms of:
:a given straight line $D$
:a given point $F$
:a given constant $e$

where $K$ is the locus of points $P$ such that the distance $p$ from $P$ to $D$ and the distance $q$ from $P$ to $F$ are related by the condition:
:$q = e p$


The line $D$ is known as the directrix of the conic section.",Definition:Conic Section/Directrix,,false,"Let K be a conic section specified in terms of:
:a given straight line D
:a given point F
:a given constant e

where K is the locus of points P such that the distance p from P to D and the distance q from P to F are related by the condition:
:q = e p


The line D is known as the directrix of the conic section.",Directrix
"['Definitions/Conchoids', 'Definitions/Directrices']",Definition:Directrix,"Let $mathcal K$ be a conchoid.

Let $mathcal C$ be the curve   which the conchoid is constructed.


$mathcal C$ is known as the directrix of $mathcal K$.


=== Also known as ===
",Definition:Conchoid/Directrix,,false,"Let 𝒦 be a conchoid.

Let 𝒞 be the curve   which the conchoid is constructed.


𝒞 is known as the directrix of 𝒦.


=== Also known as ===
",Directrix
['Definitions/Disconnected Spaces'],Definition:Disconnected,"=== Topological Space ===
Let $T = left( S, tau right)$ be a topological space.


=== Definition $1$ ===
Let $T = left( S, tau right)$ be a topological space.


$T$ is disconnected  if and only if  $T$ is not connected.

=== Definition $2$ ===
Let $T = left( S, tau right)$ be a topological space.


$T$ is disconnected  if and only if  there exist non-empty open sets $U, V in tau$ such that:
:$S = U cup V$
:$U cap V = varnothing$

That is, if there exists a partition of $S$ into open sets of $T$.

=== Subset of Topological Space ===

Let $T = left( S, tau right)$ be a topological space.

Let $H subseteq S$ be a non-empty subset of $S$.

Let $T = left( S, tau right)$ be a topological space.

Let $H subseteq S$ be a non-empty subset of $S$.


$H$ is a disconnected set of $T$  if and only if  it is not a connected set of $T$.

=== Points in Topological Space ===
Let $T = left({S, tau}right)$ be a topological space.

Let $a, b in S$.


Then $a$ and $b$ are disconnected (in $T$)  if and only if  they are not connected (in $T$).


Category:Definitions/Disconnected Spaces",Definition:Disconnected (Topology),,false,"=== Topological Space ===
Let T = ( S, τ) be a topological space.


=== Definition 1 ===
Let T = ( S, τ) be a topological space.


T is disconnected  if and only if  T is not connected.

=== Definition 2 ===
Let T = ( S, τ) be a topological space.


T is disconnected  if and only if  there exist non-empty open sets U, V ∈τ such that:
:S = U ∪ V
:U ∩ V = ∅

That is, if there exists a partition of S into open sets of T.

=== Subset of Topological Space ===

Let T = ( S, τ) be a topological space.

Let H ⊆ S be a non-empty subset of S.

Let T = ( S, τ) be a topological space.

Let H ⊆ S be a non-empty subset of S.


H is a disconnected set of T  if and only if  it is not a connected set of T.

=== Points in Topological Space ===
Let T = (S, τ) be a topological space.

Let a, b ∈ S.


Then a and b are disconnected (in T)  if and only if  they are not connected (in T).


Category:Definitions/Disconnected Spaces",Disconnected
"['Definitions/Disconnected Graphs', 'Definitions/Connectedness (Graph Theory)']",Definition:Disconnected,"Let $G = left( V, E right)$ be a graph.

Then $G$ is disconnected  if and only if  it is not connected.

That is,  if and only if  there exists (at least) two vertices $u, v in V$ such that $u$ and $v$ are not connected.",Definition:Connected (Graph Theory)/Graph/Disconnected,,false,"Let G = ( V, E ) be a graph.

Then G is disconnected  if and only if  it is not connected.

That is,  if and only if  there exists (at least) two vertices u, v ∈ V such that u and v are not connected.",Disconnected
"['Definitions/Discontinuous Mappings', 'Definitions/Continuity']",Definition:Discontinuous,"=== Discontinuous Real Function ===
Let $f$ be a real function.

Then $f$ is discontinuous  if and only if  there exists at least one $a in mathbb R$ at which $f$ is discontinuous.


=== At a Point ===
Let $A subseteq mathbb R$ be a subset of the real numbers.

Let $f : A to mathbb R$ be a real function.

Let $ain A$.


Then $f$ is discontinuous at $a$  if and only if  $f$ is not continuous at $a$.

=== Discontinuous Topological Space ===
Let $T_1 = left( A_1, tau_1 right)$ and $T_2 = left( A_2, tau_2 right)$ be topological spaces.

Let $f: A_1 to A_2$ $x in T_1$ be a mapping from $A_1$ to $A_2$.


Then by definition $f$ is continuous at $x$ if for every neighborhood $N$ of $f left(   right)x$ there exists a neighborhood $M$ of $x$ such that $f left[ M right] subseteq N$.

Therefore, $f$ is discontinuous at $x$ if for some neighborhood $N$ of $f left(   right)x$ and every neighborhood $M$ of $x$:
:$f left[ M right] nsubseteq N$

The point $x$ is called a discontinuity of $f$.",Definition:Discontinuous Mapping,,false,"=== Discontinuous Real Function ===
Let f be a real function.

Then f is discontinuous  if and only if  there exists at least one a ∈ℝ at which f is discontinuous.


=== At a Point ===
Let A ⊆ℝ be a subset of the real numbers.

Let f : A →ℝ be a real function.

Let a∈ A.


Then f is discontinuous at a  if and only if  f is not continuous at a.

=== Discontinuous Topological Space ===
Let T_1 = ( A_1, τ_1 ) and T_2 = ( A_2, τ_2 ) be topological spaces.

Let f: A_1 → A_2 x ∈ T_1 be a mapping from A_1 to A_2.


Then by definition f is continuous at x if for every neighborhood N of f (   )x there exists a neighborhood M of x such that f [ M ] ⊆ N.

Therefore, f is discontinuous at x if for some neighborhood N of f (   )x and every neighborhood M of x:
:f [ M ] ⊈ N

The point x is called a discontinuity of f.",Discontinuous
['Definitions/Discontinuous Mappings'],Definition:Discontinuous,"Let $f$ be a real function.

Then $f$ is discontinuous  if and only if  there exists at least one $a in mathbb R$ at which $f$ is discontinuous.


=== At a Point ===
Let $A subseteq mathbb R$ be a subset of the real numbers.

Let $f : A to mathbb R$ be a real function.

Let $ain A$.


Then $f$ is discontinuous at $a$  if and only if  $f$ is not continuous at $a$.",Definition:Discontinuous Mapping/Real Function,,false,"Let f be a real function.

Then f is discontinuous  if and only if  there exists at least one a ∈ℝ at which f is discontinuous.


=== At a Point ===
Let A ⊆ℝ be a subset of the real numbers.

Let f : A →ℝ be a real function.

Let a∈ A.


Then f is discontinuous at a  if and only if  f is not continuous at a.",Discontinuous
"['Definitions/Discontinuous Mappings', 'Definitions/Discontinuities (Real Analysis)']",Definition:Discontinuous,"Let $A subseteq mathbb R$ be a subset of the real numbers.

Let $f : A to mathbb R$ be a real function.

Let $ain A$.


Then $f$ is discontinuous at $a$  if and only if  $f$ is not continuous at $a$.",Definition:Discontinuous Mapping/Real Function/Point,,false,"Let A ⊆ℝ be a subset of the real numbers.

Let f : A →ℝ be a real function.

Let a∈ A.


Then f is discontinuous at a  if and only if  f is not continuous at a.",Discontinuous
['Definitions/Discontinuous Mappings'],Definition:Discontinuous,"Let $T_1 = left( A_1, tau_1 right)$ and $T_2 = left( A_2, tau_2 right)$ be topological spaces.

Let $f: A_1 to A_2$ $x in T_1$ be a mapping from $A_1$ to $A_2$.


Then by definition $f$ is continuous at $x$ if for every neighborhood $N$ of $f left(   right)x$ there exists a neighborhood $M$ of $x$ such that $f left[ M right] subseteq N$.

Therefore, $f$ is discontinuous at $x$ if for some neighborhood $N$ of $f left(   right)x$ and every neighborhood $M$ of $x$:
:$f left[ M right] nsubseteq N$

The point $x$ is called a discontinuity of $f$.",Definition:Discontinuous Mapping/Topological Space/Point,,false,"Let T_1 = ( A_1, τ_1 ) and T_2 = ( A_2, τ_2 ) be topological spaces.

Let f: A_1 → A_2 x ∈ T_1 be a mapping from A_1 to A_2.


Then by definition f is continuous at x if for every neighborhood N of f (   )x there exists a neighborhood M of x such that f [ M ] ⊆ N.

Therefore, f is discontinuous at x if for some neighborhood N of f (   )x and every neighborhood M of x:
:f [ M ] ⊈ N

The point x is called a discontinuity of f.",Discontinuous
"['Definitions/Branches of Mathematics', 'Definitions/Discrete Mathematics']",Definition:Discrete,Discrete mathematics is the branch of mathematics which studies mathematical structures that are discrete rather than continuous.,Definition:Discrete Mathematics,mathematics,true,Discrete mathematics is the branch of mathematics which studies mathematical structures that are discrete rather than continuous.,Discrete
['Definitions/Examples of Categories'],Definition:Discrete,"Let $mathcal C$ be a metacategory.


Then $mathcal C$ is said to be discrete  if and only if  it comprises only identity morphisms.

If the collection $mathcal C$ constitutes the objects of $mathbf C$, then $mathbf C$ may also be denoted $mathbf {Dis}  left(   right)mathcal C$.",Definition:Discrete Category,,false,"Let 𝒞 be a metacategory.


Then 𝒞 is said to be discrete  if and only if  it comprises only identity morphisms.

If the collection 𝒞 constitutes the objects of 𝐂, then 𝐂 may also be denoted 𝐃𝐢𝐬(   )𝒞.",Discrete
"['Definitions/Examples of Topologies', 'Definitions/Discrete Topology']",Definition:Discrete,"Let $S ne varnothing$ be a set.

Let $tau = mathcal P left( S right)$ be the power set of $S$.

That is, let $tau$ be the set of all subsets of $S$:
:$tau := leftlbrace H: H subseteq S rightrbrace$


Then $tau$ is called the discrete topology on $S$ and $left( S, tau right) = left( S, mathcal P left( S right) right)$ the discrete space on $S$, or just a discrete space.


=== Finite Discrete Topology ===
Let $S ne varnothing$ be a set.

Let $tau = mathcal P left( S right)$ be the power set of $S$.

That is, let $tau$ be the set of all subsets of $S$:
:$tau := leftlbrace H: H subseteq S rightrbrace$


Let $S$ be a finite set.

Then $tau = mathcal P left( S right)$ is a finite discrete topology, and $left( S, tau right) = left( S, mathcal P left( S right) right)$ is a finite discrete space.

=== Infinite Discrete Topology ===
Let $S ne varnothing$ be a set.

Let $tau = mathcal P left( S right)$ be the power set of $S$.

That is, let $tau$ be the set of all subsets of $S$:
:$tau := leftlbrace H: H subseteq S rightrbrace$


Let $S$ be an infinite set.

Then $tau = mathcal P left( S right)$ is an infinite discrete topology, and $left( S, tau right) = left( S, mathcal P left( S right) right)$ is an infinite discrete space.


=== Countable Discrete Topology ===
Let $S ne varnothing$ be an infinite set.

Let $tau = mathcal P left( S right)$ be the power set of $S$.

That is, let $tau$ be the set of all subsets of $S$:
:$tau := leftlbrace H: H subseteq S rightrbrace$


Let $S$ be a countably infinite set.

Then $tau = mathcal P left( S right)$ is a countable discrete topology, and $left( S, tau right) = left( S, mathcal P left( S right) right)$ is a countable discrete space.

=== Uncountable Discrete Topology ===
Let $S ne varnothing$ be a set.

Let $tau = mathcal P left( S right)$ be the power set of $S$.

That is, let $tau$ be the set of all subsets of $S$:
:$tau := leftlbrace H: H subseteq S rightrbrace$


Let $S$ be an uncountably infinite set.

Then $tau = mathcal P left( S right)$ is an uncountable discrete topology, and $left( S, tau right) = left( S, mathcal P left( S right) right)$ is an uncountable discrete space.",Definition:Discrete Topology,,false,"Let S ∅ be a set.

Let τ = 𝒫( S ) be the power set of S.

That is, let τ be the set of all subsets of S:
:τ := { H: H ⊆ S }


Then τ is called the discrete topology on S and ( S, τ) = ( S, 𝒫( S ) ) the discrete space on S, or just a discrete space.


=== Finite Discrete Topology ===
Let S ∅ be a set.

Let τ = 𝒫( S ) be the power set of S.

That is, let τ be the set of all subsets of S:
:τ := { H: H ⊆ S }


Let S be a finite set.

Then τ = 𝒫( S ) is a finite discrete topology, and ( S, τ) = ( S, 𝒫( S ) ) is a finite discrete space.

=== Infinite Discrete Topology ===
Let S ∅ be a set.

Let τ = 𝒫( S ) be the power set of S.

That is, let τ be the set of all subsets of S:
:τ := { H: H ⊆ S }


Let S be an infinite set.

Then τ = 𝒫( S ) is an infinite discrete topology, and ( S, τ) = ( S, 𝒫( S ) ) is an infinite discrete space.


=== Countable Discrete Topology ===
Let S ∅ be an infinite set.

Let τ = 𝒫( S ) be the power set of S.

That is, let τ be the set of all subsets of S:
:τ := { H: H ⊆ S }


Let S be a countably infinite set.

Then τ = 𝒫( S ) is a countable discrete topology, and ( S, τ) = ( S, 𝒫( S ) ) is a countable discrete space.

=== Uncountable Discrete Topology ===
Let S ∅ be a set.

Let τ = 𝒫( S ) be the power set of S.

That is, let τ be the set of all subsets of S:
:τ := { H: H ⊆ S }


Let S be an uncountably infinite set.

Then τ = 𝒫( S ) is an uncountable discrete topology, and ( S, τ) = ( S, 𝒫( S ) ) is an uncountable discrete space.",Discrete
['Definitions/Topological Groups'],Definition:Discrete,"A discrete group is a topological group whose topology is discrete.

Category:Definitions/Topological Groups",Definition:Discrete Group,,false,"A discrete group is a topological group whose topology is discrete.

Category:Definitions/Topological Groups",Discrete
"['Definitions/Subgroups', 'Definitions/Topological Groups', 'Definitions/Real Numbers']",Definition:Discrete,"Let $G$ be a subgroup of the additive group of real numbers.


Then $G$ is discrete  if and only if :
:$forall g in G: exists epsilon in mathbb R_{>0}: left( g - epsilon ,.,.,   right){g + epsilon} cap G = leftlbrace g rightrbrace$

That is, there exists a neighborhood of $g$ which contains no other elements of $G$.


Category:Definitions/Subgroups
Category:Definitions/Topological Groups
Category:Definitions/Real Numbers",Definition:Discrete Subgroup/Real Numbers,,false,"Let G be a subgroup of the additive group of real numbers.


Then G is discrete  if and only if :
:∀ g ∈ G: ∃ϵ∈ℝ_>0: ( g - ϵ . . )g + ϵ∩ G = { g }

That is, there exists a neighborhood of g which contains no other elements of G.


Category:Definitions/Subgroups
Category:Definitions/Topological Groups
Category:Definitions/Real Numbers",Discrete
"['Definitions/Standard Discrete Metric', 'Definitions/Examples of Metric Spaces']",Definition:Discrete,"The standard discrete metric on a set $S$ is the metric satisfying:

:$d left(   right){x, y} = begin {cases} 0 & : x = y \ 1 & : x ne y end {cases}$


This can be expressed using the Kronecker delta notation as:
:$d left(   right){x, y} = 1 - delta_{x y}$


The resulting metric space $M = left( S, d right)$ is the standard discrete metric space on $S$.",Definition:Standard Discrete Metric,,false,"The standard discrete metric on a set S is the metric satisfying:

:d (   )x, y =  0     : x = y 
 1     : x  y


This can be expressed using the Kronecker delta notation as:
:d (   )x, y = 1 - δ_x y


The resulting metric space M = ( S, d ) is the standard discrete metric space on S.",Discrete
['Definitions/Topology'],Definition:Discrete,"Let $T = left( S, tau right)$ be a topological space.

Let $mathcal F$ be a set of subsets of $S$.


Then $mathcal F$ is discrete  if and only if  each element of $S$ has a neighborhood which intersects at most one of the sets in $mathcal F$.",Definition:Discrete Set of Subsets,,false,"Let T = ( S, τ) be a topological space.

Let ℱ be a set of subsets of S.


Then ℱ is discrete  if and only if  each element of S has a neighborhood which intersects at most one of the sets in ℱ.",Discrete
"['Definitions/Branches of Mathematics', 'Definitions/Discrete Geometry', 'Definitions/Geometry']",Definition:Discrete,Discrete geometry is a branch of geometry that studies constructive methods of discrete geometric objects.,Definition:Discrete Geometry,geometry,true,Discrete geometry is a branch of geometry that studies constructive methods of discrete geometric objects.,Discrete
"['Definitions/Measure Theory', 'Definitions/Measures', 'Definitions/Measures']",Definition:Discrete,"Let $left( X, unicode{x3a3}, mu right)$ be a measure space.


Then $mu$ is said to be a discrete measure  if and only if  it is a series of Dirac measures.

That is,  if and only if  there exist:
:a sequence $leftlangle x_n rightrangle_{n mathop in mathbb N}$ in $X$
and:
:a sequence $leftlangle lambda_n rightrangle_{n mathop in mathbb N}$ in $mathbb R$

such that:

:$(1):quad forall E in unicode{x3a3}: mu left(   right)E = ds sum_{n mathop in mathbb N} lambda_n , delta_{x_n}  left(   right)E$

where $delta_{x_n}$ denotes the Dirac measure at $x_n$.


By Series of Measures is Measure, defining $mu$ by $(1)$ yields a measure.",Definition:Discrete Measure,,false,"Let ( X, x3a3, μ) be a measure space.


Then μ is said to be a discrete measure  if and only if  it is a series of Dirac measures.

That is,  if and only if  there exist:
:a sequence ⟨ x_n ⟩_n ∈ℕ in X
and:
:a sequence ⟨λ_n ⟩_n ∈ℕ in ℝ

such that:

:(1):  ∀ E ∈x3a3: μ(   )E = ∑_n ∈ℕλ_n  δ_x_n(   )E

where δ_x_n denotes the Dirac measure at x_n.


By Series of Measures is Measure, defining μ by (1) yields a measure.",Discrete
"['Definitions/Discrete Probability Distributions', 'Definitions/Probability Distributions', 'Definitions/Probability Theory']",Definition:Discrete,A discrete probability distribution is a probability distribution of a discrete random variable.,Definition:Discrete Probability Distribution,,false,A discrete probability distribution is a probability distribution of a discrete random variable.,Discrete
['Definitions/Probability Theory'],Definition:Discrete,"Let $mathcal E$ be an experiment.

Let $Omega$ denote the sample space of $mathcal E$.


If $Omega$ is a countable set, whether finite or infinite, then it is known as a discrete sample space.",Definition:Sample Space/Discrete,,false,"Let ℰ be an experiment.

Let Ω denote the sample space of ℰ.


If Ω is a countable set, whether finite or infinite, then it is known as a discrete sample space.",Discrete
"['Definitions/Descriptive Statistics', 'Definitions/Variables']",Definition:Discrete,A discrete variable is a variable which is not continuous.,Definition:Variable/Discrete,,false,A discrete variable is a variable which is not continuous.,Discrete
['Definitions/Sample Statistics'],Definition:Discrete,Data which can be described with a discrete variable are known as discrete data.,Definition:Sample Statistic/Discrete,,false,Data which can be described with a discrete variable are known as discrete data.,Discrete
"['Definitions/Discriminants of Polynomials', 'Definitions/Discriminants', 'Definitions/Polynomial Theory']",Definition:Discriminant," 


Let $k$ be a field.

Let $f left(   right)X in k left[ X right]$ be a polynomial of degree $n$.

Let $overline k$ be an algebraic closure of $k$.

Let the roots of $f$ in $overline k$ be $alpha_1, alpha_2, ldots, alpha_n$.


Then the discriminant $Delta left(   right)f$ of $f$ is defined as:

:$ds Delta left(   right)f := prod_{1 mathop le i mathop < j mathop le n} left( alpha_i - alpha_j right)^2$


=== Quadratic Equation ===

The concept is usually encountered in the context of a quadratic equation $a x^2 + b x + c = 0$:
Consider the quadratic equation:
:$a x^2 + b x + c = 0$

The expression $b^2 - 4 a c$ is called the discriminant of the equation.

=== Cubic Equation ===

In the context of a cubic equation $a x^3 + b x^2 + c x + d = 0$:
 ",Definition:Discriminant of Polynomial,,false," 


Let k be a field.

Let f (   )X ∈ k [ X ] be a polynomial of degree n.

Let k be an algebraic closure of k.

Let the roots of f in k be α_1, α_2, …, α_n.


Then the discriminant Δ(   )f of f is defined as:

:Δ(   )f := ∏_1 ≤ i  < j ≤ n( α_i - α_j )^2


=== Quadratic Equation ===

The concept is usually encountered in the context of a quadratic equation a x^2 + b x + c = 0:
Consider the quadratic equation:
:a x^2 + b x + c = 0

The expression b^2 - 4 a c is called the discriminant of the equation.

=== Cubic Equation ===

In the context of a cubic equation a x^3 + b x^2 + c x + d = 0:
 ",Discriminant
"['Definitions/Bilinear Forms (Linear Algebra)', 'Definitions/Discriminants']",Definition:Discriminant,"Let $mathbb K$ be a field.

Let $V$ be a vector space over $mathbb K$ of finite dimension $n>0$.

Let $b : Vtimes V to mathbb K$ be a bilinear form on $V$.

Let $A$ be the matrix of $b$ relative to an ordered basis of $V$.


If $b$ is nondegenerate, its discriminant is the equivalence class of the determinant $det A$ in the quotient group $dfrac {mathbb K^times} {left( mathbb K^times right)^2}$.

If $b$ is degenerate, its discriminant is $0$.",Definition:Discriminant of Bilinear Form,,false,"Let 𝕂 be a field.

Let V be a vector space over 𝕂 of finite dimension n>0.

Let b : V× V →𝕂 be a bilinear form on V.

Let A be the matrix of b relative to an ordered basis of V.


If b is nondegenerate, its discriminant is the equivalence class of the determinant A in the quotient group 𝕂^×( 𝕂^×)^2.

If b is degenerate, its discriminant is 0.",Discriminant
"['Definitions/Discriminants of Conic Sections', 'Definitions/Discriminants', 'Definitions/Conic Sections']",Definition:Discriminant,"Let $K$ be a conic section embedded in a Cartesian plane with the general equation:
:$a x^2 + 2 h x y + b y^2 + 2 g x + 2 f y + c = 0$
where $a, b, c, f, g, h in mathbb R$.


The discriminant of $K$ is defined as the determinant calculated as:
:$Delta = begin {vmatrix} a & h & g \ h & b & f \ g & f & c end {vmatrix}$",Definition:Discriminant of Conic Section,,false,"Let K be a conic section embedded in a Cartesian plane with the general equation:
:a x^2 + 2 h x y + b y^2 + 2 g x + 2 f y + c = 0
where a, b, c, f, g, h ∈ℝ.


The discriminant of K is defined as the determinant calculated as:
:Δ =  a     h     g 
 h     b     f 
 g     f     c",Discriminant
"['Definitions/Discriminant Functions', 'Definitions/Descriptive Statistics', 'Definitions/Discriminants']",Definition:Discriminant,"A discriminant function is a function which assigns a given individual to one of a number of populations according to the data appertaining to that individual.

It is based on measurements on individuals for whom the population to which each one belongs is known.

It is chosen to minimize the probabilities or costs of misclassification.",Definition:Discriminant Function,,false,"A discriminant function is a function which assigns a given individual to one of a number of populations according to the data appertaining to that individual.

It is based on measurements on individuals for whom the population to which each one belongs is known.

It is chosen to minimize the probabilities or costs of misclassification.",Discriminant
"['Definitions/Dispersion (Statistics)', 'Definitions/Statistics']",Definition:Dispersion,"Let $S$ be a sample of a population in the context of statistics.

The dispersion of $S$ is a general term meaning how much the data describing the sample are spread out.


The word can also be applied to a random variable.


Measures of dispersion include the following:


=== Range ===
Let $S$ be a set of observations of a quantitative variable.


The range of $S$ is defined as:
:$R left(   right)S := max left(   right)S - min left(   right)S$

where $max left(   right)S$ and $min left(   right)S$ are the greatest value of $S$ and the least value of $S$ respectively.

=== Interquartile Range ===
The interquartile range is a measure of dispersion in statistics.


Let $Q_1$ and $Q_3$ be first quartile and third quartile respectively.

The interquartile range is defined and denoted as:

:$operatorname {IQR} := Q_3 - Q_1$

=== Mean Absolute Deviation ===
Let $S = leftlbrace x_1, x_2, ldots, x_n rightrbrace$ be a set of observations.

Let $bar x$ denote a measure of central tendency of $S$.


The mean absolute deviation   $bar x$ of $S$ is defined as the arithmetic mean of the absolute values of the deviation of the elements of $S$ from $bar x$ :

:$ds sum_{i mathop = 1}^n dfrac 1 n leftlvert x_i - bar x rightrvert$


=== Discrete Random Variable ===
Let $X$ be a discrete random variable.

Let $bar x$ denote a measure of central tendency of $X$.


The mean absolute deviation of $X$ is the first absolute moment of $X$ about $bar x$.

=== Continuous Random Variable ===
Let $X$ be a continuous random variable.

Let $m$ denote the median of $X$.

Let the frequency function of $X$ be $f$.


The mean absolute deviation of $X$ is defined as:
:$ds int_{-infty}^{+infty} leftlvert x - m rightrvert f left(   right)x ,mathrm d x$

=== Variance ===
=== Discrete Random Variable ===
Let $X$ be a discrete random variable.

Then the variance of $X$, written $mathsf {var} left( X right)$, is a measure of how much the values of $X$ varies from the expectation $mathsf E left( X right)$, and is defined as:

=== Definition 1 ===
Let $X$ be a discrete random variable.

Then the variance of $X$, written $mathsf {var} left( X right)$, is a measure of how much the values of $X$ varies from the expectation $mathsf E left( X right)$, and is defined as:

:$mathsf {var} left( X right) := mathsf E left( left( X - mathsf E left( X right) right)^2 right)$

That is: it is the expectation of the squares of the deviations from the expectation.

=== Definition 2 ===

Using $mu = mathsf E left( X right)$, we can consider $left( X - mu right)^2$ as a function of $X$ and apply Expectation of Function of Discrete Random Variable, to obtain:
Let $X$ be a discrete random variable.

Then the variance of $X$, written $mathsf {var} left( X right)$, is defined as:

:$ds mathsf {var} left( X right) := sum_{x mathop in Omega_X} left( x - mu^2 right) Pr left(   right){X = x}$
where:
:$mu := mathsf E left( X right)$ is the expectation of $X$
:$Omega_X$ is the image of $X$
:$Pr left(   right){X = x}$ is the probability mass function of $X$.

=== Definition 3 ===

Far easier to work with than the above definition is the result:
Let $X$ be a discrete random variable.

Let $mathsf E left( X right)$ be the expectation of $X$.

Then the variance of $X$, written $mathsf {var} left( X right)$, is defined as:

:$mathsf {var} left( X right) := mathsf E left( X^2 right) - left( mathsf E left( X right) right)^2$

=== Continuous Random Variable ===
Let $X$ be a continuous random variable. 

Then the variance of $X$, written $mathsf {var} left( X right)$, is a measure of how much the values of $X$ varies from the expectation $mathsf E left( X right)$, and is defined as:

:$mathsf {var} left( X right) := mathsf E left( left( X - mathsf E left( X right) right)^2 right)$

That is, the expectation of the squares of the deviations from the expectation.


Letting $mu = mathsf E left( X right)$, this is often given as: 

:$mathsf {var} left( X right) = mathsf E left( left( X - mu right)^2 right)$

=== Standard Deviation ===
Let $X$ be a random variable.

Then the standard deviation of $X$, written $sigma_X$ or $sigma$, is defined as the principal square root of the variance of $X$:

:$sigma_X := sqrt {mathsf {var} left( X right)}$",Definition:Dispersion (Statistics),,false,"Let S be a sample of a population in the context of statistics.

The dispersion of S is a general term meaning how much the data describing the sample are spread out.


The word can also be applied to a random variable.


Measures of dispersion include the following:


=== Range ===
Let S be a set of observations of a quantitative variable.


The range of S is defined as:
:R (   )S := max(   )S - min(   )S

where max(   )S and min(   )S are the greatest value of S and the least value of S respectively.

=== Interquartile Range ===
The interquartile range is a measure of dispersion in statistics.


Let Q_1 and Q_3 be first quartile and third quartile respectively.

The interquartile range is defined and denoted as:

:IQR := Q_3 - Q_1

=== Mean Absolute Deviation ===
Let S = { x_1, x_2, …, x_n } be a set of observations.

Let x̅ denote a measure of central tendency of S.


The mean absolute deviation   x̅ of S is defined as the arithmetic mean of the absolute values of the deviation of the elements of S from x̅ :

:∑_i  = 1^n  1 n | x_i - x̅|


=== Discrete Random Variable ===
Let X be a discrete random variable.

Let x̅ denote a measure of central tendency of X.


The mean absolute deviation of X is the first absolute moment of X about x̅.

=== Continuous Random Variable ===
Let X be a continuous random variable.

Let m denote the median of X.

Let the frequency function of X be f.


The mean absolute deviation of X is defined as:
:∫_-∞^+∞| x - m | f (   )x  d x

=== Variance ===
=== Discrete Random Variable ===
Let X be a discrete random variable.

Then the variance of X, written 𝗏𝖺𝗋( X ), is a measure of how much the values of X varies from the expectation 𝖤( X ), and is defined as:

=== Definition 1 ===
Let X be a discrete random variable.

Then the variance of X, written 𝗏𝖺𝗋( X ), is a measure of how much the values of X varies from the expectation 𝖤( X ), and is defined as:

:𝗏𝖺𝗋( X ) := 𝖤( ( X - 𝖤( X ) )^2 )

That is: it is the expectation of the squares of the deviations from the expectation.

=== Definition 2 ===

Using μ = 𝖤( X ), we can consider ( X - μ)^2 as a function of X and apply Expectation of Function of Discrete Random Variable, to obtain:
Let X be a discrete random variable.

Then the variance of X, written 𝗏𝖺𝗋( X ), is defined as:

:𝗏𝖺𝗋( X ) := ∑_x ∈Ω_X( x - μ^2 ) (   )X = x
where:
:μ := 𝖤( X ) is the expectation of X
:Ω_X is the image of X
:(   )X = x is the probability mass function of X.

=== Definition 3 ===

Far easier to work with than the above definition is the result:
Let X be a discrete random variable.

Let 𝖤( X ) be the expectation of X.

Then the variance of X, written 𝗏𝖺𝗋( X ), is defined as:

:𝗏𝖺𝗋( X ) := 𝖤( X^2 ) - ( 𝖤( X ) )^2

=== Continuous Random Variable ===
Let X be a continuous random variable. 

Then the variance of X, written 𝗏𝖺𝗋( X ), is a measure of how much the values of X varies from the expectation 𝖤( X ), and is defined as:

:𝗏𝖺𝗋( X ) := 𝖤( ( X - 𝖤( X ) )^2 )

That is, the expectation of the squares of the deviations from the expectation.


Letting μ = 𝖤( X ), this is often given as: 

:𝗏𝖺𝗋( X ) = 𝖤( ( X - μ)^2 )

=== Standard Deviation ===
Let X be a random variable.

Then the standard deviation of X, written σ_X or σ, is defined as the principal square root of the variance of X:

:σ_X := √(𝗏𝖺𝗋( X ))",Dispersion
['Definitions/Totally Disconnected Spaces'],Definition:Dispersion,"Let $T = left( S, tau right)$ be a topological space.

Let $H subseteq S$ be a connected set in $T$ and let $p in H$.

Let $p in H$ such that $H setminus leftlbrace p rightrbrace$ is totally disconnected, where $setminus$ denotes set difference.


Then $p$ is a dispersion point of $H$.",Definition:Dispersion Point,,false,"Let T = ( S, τ) be a topological space.

Let H ⊆ S be a connected set in T and let p ∈ H.

Let p ∈ H such that H ∖{ p } is totally disconnected, where ∖ denotes set difference.


Then p is a dispersion point of H.",Dispersion
['Definitions/Complex Analysis'],Definition:Distance,"Let $x, y in mathbb C$ be complex numbers.

Let $leftlvert x - y rightrvert$ be the complex modulus of $x - y$.


Then the function $d: mathbb C^2 to mathbb R$:
:$d left(   right){x, y} = leftlvert x - y rightrvert$
is called the distance between $x$ and $y$.",Definition:Distance/Points/Complex Numbers,,false,"Let x, y ∈ℂ be complex numbers.

Let | x - y | be the complex modulus of x - y.


Then the function d: ℂ^2 →ℝ:
:d (   )x, y = | x - y |
is called the distance between x and y.",Distance
"['Definitions/Distance Functions', 'Definitions/Metric Spaces']",Definition:Distance,"Let $left( A, d right)$ be a metric space.

The mapping $d: A times A to mathbb R$ is referred to as a distance function on $A$.


Here, $d: A times A to mathbb R$ is a real-valued function satisfying the metric space axioms:
 ",Definition:Metric Space/Distance Function,,false,"Let ( A, d ) be a metric space.

The mapping d: A × A →ℝ is referred to as a distance function on A.


Here, d: A × A →ℝ is a real-valued function satisfying the metric space axioms:
 ",Distance
"['Definitions/Metric Spaces', 'Definitions/Distance Function']",Definition:Distance,"Let $M = left( A, d right)$ be a metric space.

Let $x in A$.

Let $S, T$ be subsets of $A$.


The distance between $x$ and $S$ is defined and annotated $ds d left(   right){x, S} = inf_{y mathop in S} left( d left(   right){x, y}  right)$.

The distance between $S$ and $T$ is defined and annotated $ds d left(   right){S, T} = inf_{substack {x mathop in S \ y mathop in T} } left( d left(   right){x, y}  right)$.",Definition:Distance/Sets/Metric Spaces,,false,"Let M = ( A, d ) be a metric space.

Let x ∈ A.

Let S, T be subsets of A.


The distance between x and S is defined and annotated d (   )x, S = inf_y ∈ S( d (   )x, y).

The distance between S and T is defined and annotated d (   )S, T = inf_x ∈ S 
 y ∈ T( d (   )x, y).",Distance
['Definitions/Length'],Definition:Distance,The distance between two points $A$ and $B$ in space is defined as the length of a straight line that would be drawn from $A$ to $B$.,Definition:Linear Measure/Distance,,false,The distance between two points A and B in space is defined as the length of a straight line that would be drawn from A to B.,Distance
['Definitions/Angles'],Definition:Distance,"The angular distance between two points $A$ and $B$   a reference point $P$ is the angle between the straight lines $AP$ and $BP$, that is:
:$angle APB$",Definition:Angular Distance,,false,"The angular distance between two points A and B   a reference point P is the angle between the straight lines AP and BP, that is:
:∠ APB",Distance
"['Definitions/Hamming Distance', 'Definitions/Linear Codes']",Definition:Distance,"Let $u$ and $v$ be two codewords of a linear code.

The Hamming distance between $u$ and $v$ is the number of corresponding terms at which $u$ and $v$ are different.",Definition:Hamming Distance,,false,"Let u and v be two codewords of a linear code.

The Hamming distance between u and v is the number of corresponding terms at which u and v are different.",Distance
"['Definitions/Distance (Graph Theory)', 'Definitions/Graph Theory']",Definition:Distance,"=== Undirected Graph ===
Let $G = left( V, E right)$ be an undirected graph.

Let $u, v in V$ be vertices of $V$.


The distance between $u$ and $v$ is the length of the shortest path from $u$ to $v$.

=== Digraph ===
Let $G = left( V, E right)$ be a digraph.

Let $u, v in V$ be vertices of $V$.


The distance from $u$ to $v$ is the length of the shortest path from $u$ to $v$.",Definition:Distance (Graph Theory),,false,"=== Undirected Graph ===
Let G = ( V, E ) be an undirected graph.

Let u, v ∈ V be vertices of V.


The distance between u and v is the length of the shortest path from u to v.

=== Digraph ===
Let G = ( V, E ) be a digraph.

Let u, v ∈ V be vertices of V.


The distance from u to v is the length of the shortest path from u to v.",Distance
"['Definitions/Distributive Operations', 'Definitions/Abstract Algebra']",Definition:Distributive,"Let $S$ be a set on which is defined two binary operations, defined on all the elements of $S times S$, which we will denote as $circ$ and $*$.


The operation $circ$ is distributive over $*$, or distributes over $*$,  if and only if :
:$circ$ is right distributive over $*$
and:
:$circ$ is left distributive over $*$.


=== Left Distributive ===
Let $S$ be a set on which is defined two binary operations, defined on all the elements of $S times S$, denoted here as $circ$ and $*$.

The operation $circ$ is left distributive over the operation $*$  if and only if :

:$forall a, b, c in S: a circ left( b * c right) = left( a circ b right) * left( a circ c right)$

=== Right Distributive ===
Let $S$ be a set on which is defined two binary operations, defined on all the elements of $S times S$, denoted here as $circ$ and $*$.

The operation $circ$ is right distributive over the operation $*$  if and only if :

:$forall a, b, c in S: left( a * b right) circ c = left( a circ c right) * left( b circ c right)$

So as to streamline what may turn into cumbersome language, some further definitions:

=== Distributand ===
Let $S$ be a set on which is defined two binary operations, defined on all the elements of $S times S$, which we will denote as $circ$ and $*$.

Let $circ$ be distributive over $*$.

Then $*$ is a distributand of $circ$.


=== Linguistic Note ===


Category:Definitions/Distributive Operations

=== Distributor ===
Let $S$ be a set on which is defined two binary operations, defined on all the elements of $S times S$, which we will denote as $circ$ and $*$.

Let $circ$ be distributive over $*$.

Then $circ$ is a distributor of $*$.


Category:Definitions/Distributive Operations",Definition:Distributive Operation,,false,"Let S be a set on which is defined two binary operations, defined on all the elements of S × S, which we will denote as ∘ and *.


The operation ∘ is distributive over *, or distributes over *,  if and only if :
:∘ is right distributive over *
and:
:∘ is left distributive over *.


=== Left Distributive ===
Let S be a set on which is defined two binary operations, defined on all the elements of S × S, denoted here as ∘ and *.

The operation ∘ is left distributive over the operation *  if and only if :

:∀ a, b, c ∈ S: a ∘( b * c ) = ( a ∘ b ) * ( a ∘ c )

=== Right Distributive ===
Let S be a set on which is defined two binary operations, defined on all the elements of S × S, denoted here as ∘ and *.

The operation ∘ is right distributive over the operation *  if and only if :

:∀ a, b, c ∈ S: ( a * b ) ∘ c = ( a ∘ c ) * ( b ∘ c )

So as to streamline what may turn into cumbersome language, some further definitions:

=== Distributand ===
Let S be a set on which is defined two binary operations, defined on all the elements of S × S, which we will denote as ∘ and *.

Let ∘ be distributive over *.

Then * is a distributand of ∘.


=== Linguistic Note ===


Category:Definitions/Distributive Operations

=== Distributor ===
Let S be a set on which is defined two binary operations, defined on all the elements of S × S, which we will denote as ∘ and *.

Let ∘ be distributive over *.

Then ∘ is a distributor of *.


Category:Definitions/Distributive Operations",Distributive
"['Definitions/Distributive Lattices', 'Definitions/Order Theory', 'Definitions/Lattice Theory']",Definition:Distributive,"Let $left( S, vee, wedge, preceq right)$ be a lattice.


=== Definition 1 ===
Let $left( S, vee, wedge, preceq right)$ be a lattice.


Then $left( S, vee, wedge, preceq right)$ is distributive  if and only if  $left( S, vee, wedge, preceq right)$ satisfies one of the distributive lattice axioms:
 

=== Definition 2 ===
Let $left( S, vee, wedge, preceq right)$ be a lattice.


Then $left( S, vee, wedge, preceq right)$ is distributive  if and only if  $left( S, vee, wedge, preceq right)$ satisfies all of the distributive lattice axioms:
 ",Definition:Distributive Lattice,,false,"Let ( S, ∨, ∧, ≼) be a lattice.


=== Definition 1 ===
Let ( S, ∨, ∧, ≼) be a lattice.


Then ( S, ∨, ∧, ≼) is distributive  if and only if  ( S, ∨, ∧, ≼) satisfies one of the distributive lattice axioms:
 

=== Definition 2 ===
Let ( S, ∨, ∧, ≼) be a lattice.


Then ( S, ∨, ∧, ≼) is distributive  if and only if  ( S, ∨, ∧, ≼) satisfies all of the distributive lattice axioms:
 ",Distributive
"['Definitions/Divergent Products', 'Definitions/Infinite Products']",Definition:Divergence,"Let $left( mathbb K, leftlVert ,cdot, rightrVert  right)$ be a valued field.

Let $leftlangle a_n rightrangle$ be a sequence of elements of $mathbb K$.


If either:

:there exist infinitely many $n in mathbb N$ with $a_n = 0$

:there exists $n_0 in mathbb N$ with $a_n ne 0$ for all $n > n_0$ and the sequence of partial products of $ds prod_{n mathop = n_0 + 1}^infty a_n$ converges to $0$

the product diverges to $0$, and we assign the value:
:$ds prod_{n mathop = 1}^infty a_n = 0$


Category:Definitions/Divergent Products
Category:Definitions/Infinite Products",Definition:Divergent Product/Divergence to Zero,,false,"Let ( 𝕂, ‖ · ‖) be a valued field.

Let ⟨ a_n ⟩ be a sequence of elements of 𝕂.


If either:

:there exist infinitely many n ∈ℕ with a_n = 0

:there exists n_0 ∈ℕ with a_n  0 for all n > n_0 and the sequence of partial products of ∏_n  = n_0 + 1^∞ a_n converges to 0

the product diverges to 0, and we assign the value:
:∏_n  = 1^∞ a_n = 0


Category:Definitions/Divergent Products
Category:Definitions/Infinite Products",Divergence
"['Definitions/Divergence Operator', 'Definitions/Vector Analysis', 'Definitions/Vector Calculus']",Definition:Divergence,"=== Physical Interpretation ===
Let $mathbf V$ be a vector field acting over a region of space $R$.

The divergence of $mathbf V$ at a point $P$ is the total flux away from $P$ per unit volume.

It is a scalar field.

=== Geometrical Representation ===
Let $R$ be a region of space embedded in a Cartesian coordinate frame.

Let $mathbf V$ be a vector field acting over $R$.


The divergence of $mathbf V$ at a point $A$ in $R$ is defined as:

 
 
 
 

where:
:$nabla$ denotes the Del operator
:$cdot$ denotes the dot product
:$V_x$, $V_y$ and $V_z$ denote the magnitudes of the components of $mathbf V$ at $A$ in the directions of the coordinate axes $x$, $y$ and $z$ respectively.

=== Real Cartesian Space ===
Let $mathbb R^n left(   right){x_1, x_2, ldots, x_n}$ denote the real Cartesian space of $n$ dimensions.

Let $left( mathbf e_1, mathbf e_2, ldots, mathbf e_n right)$ be the standard ordered basis on $mathbb R^n$.

Let $mathbf f = left( f_1 left(   right){mathbf x}, f_2 left(   right){mathbf x}, ldots, f_n left(   right){mathbf x}  right): mathbb R^n to mathbb R^n$ be a vector-valued function on $mathbb R^n$.


Let the partial derivative of $mathbf f$ with respect to $x_k$ exist for all $f_k$.


The divergence of $mathbf f$ is defined as:

 
 
 
 
 


=== Cartesian $3$-Space ===

In $3$ dimensions with the standard ordered basis $left( mathbf i, mathbf j, mathbf k right)$, this is usually rendered:

Let $R$ be a region of Cartesian $3$-space $mathbb R^3$.

Let $mathbf V left(   right){x, y, z}$ be a vector field acting over $R$.


The divergence of $mathbf V$ is defined as:

 
 
 
 

=== Riemannian Manifold ===
Let $left( M, g right)$ be a Riemannian manifold equiped with a metric $g$.

Let $mathbf X : mathcal C^infty left(   right)M to mathcal C^infty left(   right)M$ be a smooth vector field.


The divergence of $mathbf X$ is defined as:

 
 
 
 

where:
:$star_g$ is the Hodge star operator of $left( M, g right)$
:$mathrm d_{mathrm d R}$ is de Rham differential.

=== Integral Form ===
Let $R$ be a region of space embedded in a Cartesian coordinate frame.

Let $mathbf V$ be a vector field acting over $R$.


The divergence of $mathbf V$ at a point $A$ in $R$ is defined as:

:$ds operatorname {div} mathbf V := lim_{delta tau mathop to 0} dfrac {int_S mathbf V cdot mathrm d S} {delta tau}$

where:
:$S$ is the surface of a volume element $delta tau$ containing $A$
:$cdot$ denotes the dot product
:$ds int_S$ denotes the surface integral over $S$.",Definition:Divergence Operator,,false,"=== Physical Interpretation ===
Let 𝐕 be a vector field acting over a region of space R.

The divergence of 𝐕 at a point P is the total flux away from P per unit volume.

It is a scalar field.

=== Geometrical Representation ===
Let R be a region of space embedded in a Cartesian coordinate frame.

Let 𝐕 be a vector field acting over R.


The divergence of 𝐕 at a point A in R is defined as:

 
 
 
 

where:
:∇ denotes the Del operator
:· denotes the dot product
:V_x, V_y and V_z denote the magnitudes of the components of 𝐕 at A in the directions of the coordinate axes x, y and z respectively.

=== Real Cartesian Space ===
Let ℝ^n (   )x_1, x_2, …, x_n denote the real Cartesian space of n dimensions.

Let ( 𝐞_1, 𝐞_2, …, 𝐞_n ) be the standard ordered basis on ℝ^n.

Let 𝐟 = ( f_1 (   )𝐱, f_2 (   )𝐱, …, f_n (   )𝐱): ℝ^n →ℝ^n be a vector-valued function on ℝ^n.


Let the partial derivative of 𝐟 with respect to x_k exist for all f_k.


The divergence of 𝐟 is defined as:

 
 
 
 
 


=== Cartesian 3-Space ===

In 3 dimensions with the standard ordered basis ( 𝐢, 𝐣, 𝐤), this is usually rendered:

Let R be a region of Cartesian 3-space ℝ^3.

Let 𝐕(   )x, y, z be a vector field acting over R.


The divergence of 𝐕 is defined as:

 
 
 
 

=== Riemannian Manifold ===
Let ( M, g ) be a Riemannian manifold equiped with a metric g.

Let 𝐗 : 𝒞^∞(   )M →𝒞^∞(   )M be a smooth vector field.


The divergence of 𝐗 is defined as:

 
 
 
 

where:
:⋆_g is the Hodge star operator of ( M, g )
:d_d R is de Rham differential.

=== Integral Form ===
Let R be a region of space embedded in a Cartesian coordinate frame.

Let 𝐕 be a vector field acting over R.


The divergence of 𝐕 at a point A in R is defined as:

:div𝐕 := lim_δτ→ 0∫_S 𝐕·d Sδτ

where:
:S is the surface of a volume element δτ containing A
:· denotes the dot product
:∫_S denotes the surface integral over S.",Divergence
"['Definitions/Divergent Sequences', 'Definitions/Divergence', 'Definitions/Sequences']",Definition:Divergent,"A sequence which is not convergent is divergent.

 

=== Divergent Real Sequence ===
A real sequence which is not convergent is divergent.",Definition:Divergent Sequence,,false,"A sequence which is not convergent is divergent.

 

=== Divergent Real Sequence ===
A real sequence which is not convergent is divergent.",Divergent
"['Definitions/Divergent Series', 'Definitions/Divergence', 'Definitions/Series']",Definition:Divergent,A series which is not convergent is divergent.,Definition:Divergent Series,,false,A series which is not convergent is divergent.,Divergent
['Definitions/Real Analysis'],Definition:Divergent,A function which is not convergent is divergent.,Definition:Divergent Function,,false,A function which is not convergent is divergent.,Divergent
"['Definitions/Divergent Improper Integrals', 'Definitions/Improper Integrals', 'Definitions/Integral Calculus']",Definition:Divergent,"An improper integral of a real function $f$ is said to diverge if any of the following hold:

:$(1): quad f$ is continuous on $left[ a ,.,.,   right)to$ and the limit $ds lim_{b mathop to +infty} int_a^b f left(   right)x ,mathrm d x$ does not exist

:$(2): quad f$ is continuous on $left( gets ,.,.,   right]b$ and the limit $ds lim_{a mathop to -infty} int_a^b f left(   right)x ,mathrm d x$ does not exist

:$(3): quad f$ is continuous on $left[ a ,.,.,   right)b$, has an infinite discontinuity at $b$, and the limit $ds lim_{c mathop to b^-} int_a^c f left(   right)x ,mathrm d x$ does not exist

:$(4): quad f$ is continuous on $left( a ,.,.,   right]b$, has an infinite discontinuity at $a$, and the limit $ds lim_{c mathop to a^+} int_c^b f left(   right)x ,mathrm d x$ does not exist.",Definition:Divergent Improper Integral,,false,"An improper integral of a real function f is said to diverge if any of the following hold:

:(1):    f is continuous on [ a  . . )→ and the limit lim_b → +∞∫_a^b f (   )x  d x does not exist

:(2):    f is continuous on (  . . ]b and the limit lim_a → -∞∫_a^b f (   )x  d x does not exist

:(3):    f is continuous on [ a  . . )b, has an infinite discontinuity at b, and the limit lim_c → b^-∫_a^c f (   )x  d x does not exist

:(4):    f is continuous on ( a  . . ]b, has an infinite discontinuity at a, and the limit lim_c → a^+∫_c^b f (   )x  d x does not exist.",Divergent
"['Definitions/Divergent Products', 'Definitions/Infinite Products', 'Definitions/Divergence']",Definition:Divergent,"An infinite product which is not convergent is divergent.

 


=== Divergence to zero ===
Let $left( mathbb K, leftlVert ,cdot, rightrVert  right)$ be a valued field.

Let $leftlangle a_n rightrangle$ be a sequence of elements of $mathbb K$.


If either:

:there exist infinitely many $n in mathbb N$ with $a_n = 0$

:there exists $n_0 in mathbb N$ with $a_n ne 0$ for all $n > n_0$ and the sequence of partial products of $ds prod_{n mathop = n_0 + 1}^infty a_n$ converges to $0$

the product diverges to $0$, and we assign the value:
:$ds prod_{n mathop = 1}^infty a_n = 0$


Category:Definitions/Divergent Products
Category:Definitions/Infinite Products",Definition:Divergent Product,,false,"An infinite product which is not convergent is divergent.

 


=== Divergence to zero ===
Let ( 𝕂, ‖ · ‖) be a valued field.

Let ⟨ a_n ⟩ be a sequence of elements of 𝕂.


If either:

:there exist infinitely many n ∈ℕ with a_n = 0

:there exists n_0 ∈ℕ with a_n  0 for all n > n_0 and the sequence of partial products of ∏_n  = n_0 + 1^∞ a_n converges to 0

the product diverges to 0, and we assign the value:
:∏_n  = 1^∞ a_n = 0


Category:Definitions/Divergent Products
Category:Definitions/Infinite Products",Divergent
"['Definitions/Divisors', 'Definitions/Number Theory']",Definition:Divisor,"Let $left( mathbb Z, +, times right)$ be the ring of integers.

Let $x, y in mathbb Z$.


Then $x$ divides $y$ is defined as:
:$x mathrel backslash y iff exists t in mathbb Z: y = t times x$


=== Aliquot Part ===
An aliquot part of an integer $n$ is a divisor of $n$ which is strictly less than $n$.

=== Aliquant Part ===
An aliquant part of an integer $n$ is a positive integer which is less than $n$ but is not a divisor of $n$.",Definition:Divisor (Algebra)/Integer,,false,"Let ( ℤ, +, ×) be the ring of integers.

Let x, y ∈ℤ.


Then x divides y is defined as:
:x  y ∃ t ∈ℤ: y = t × x


=== Aliquot Part ===
An aliquot part of an integer n is a divisor of n which is strictly less than n.

=== Aliquant Part ===
An aliquant part of an integer n is a positive integer which is less than n but is not a divisor of n.",Divisor
['Definitions/Prime Numbers'],Definition:Divisor,"Let $n in mathbb Z$ be an integer.

Then $p$ is a prime factor of $n$  if and only if :
: $(1): quad p$ is a prime number
: $(2): quad p$ is a divisor (that is, factor) of $n$.",Definition:Prime Factor,,false,"Let n ∈ℤ be an integer.

Then p is a prime factor of n  if and only if :
: (1):    p is a prime number
: (2):    p is a divisor (that is, factor) of n.",Divisor
"['Definitions/Divisibility', 'Definitions/Factorization']",Definition:Divisor,"Let $left( R, +, circ right)$ be an ring with unity whose zero is $0_R$ and whose unity is $1_R$.

Let $x, y in D$.

We define the term $x$ divides $y$ in $R$ as follows:
:$x mathrel {mathrel backslash_R} y iff exists t in R: y = t circ x$


When no ambiguity results, the subscript is usually dropped, and $x$ divides $y$ in $R$ is just written $x mathrel backslash y$.",Definition:Divisor (Algebra)/Ring with Unity,,false,"Let ( R, +, ∘) be an ring with unity whose zero is 0_R and whose unity is 1_R.

Let x, y ∈ D.

We define the term x divides y in R as follows:
:x _R y ∃ t ∈ R: y = t ∘ x


When no ambiguity results, the subscript is usually dropped, and x divides y in R is just written x  y.",Divisor
"['Definitions/Divisors of Polynomials', 'Definitions/Divisors', 'Definitions/Polynomial Theory']",Definition:Divisor,"Let $D$ be an integral domain.

Let $D left[ x right]$ be the polynomial ring in one variable over $D$.

Let $f, g in D left[ x right]$ be polynomials.


Then:
:$f$ divides $g$
:$f$ is a divisor of $g$
:$g$ is divisible by $f$
 if and only if :
:$exists h in D left[ x right] : g = f h$


This is denoted:
:$f mathrel backslash g$


=== Notation ===
The conventional notation for $x$ is a divisor of $y$ is ""$x mid y$"", but there is a growing trend to follow the notation ""$x mathrel backslash y$"", as espoused by   etc.

From  :
:The notation '$m mid n$' is actually much more common than '$m mathrel backslash n$' in current mathematics literature. But vertical lines are overused -- for absolute values, set delimiters, conditional probabilities, etc. -- and backward slashes are underused. Moreover, '$m mathrel backslash n$' gives an impression that $m$ is the denominator of an implied ratio. So we shall boldly let our divisibility symbol lean leftward.


An unfortunate unwelcome side-effect of this notational convention is that to indicate non-divisibility, the conventional technique of implementing $/$ through the notation looks awkward with $mathrel backslash$, so $not ! backslash$ is eschewed in favour of $nmid$.


Some sources use $ vert mkern -10mu {raise 3pt -}  $ or similar to denote non-divisibility.


Category:Definitions/Divisors",Definition:Divisor of Polynomial,,false,"Let D be an integral domain.

Let D [ x ] be the polynomial ring in one variable over D.

Let f, g ∈ D [ x ] be polynomials.


Then:
:f divides g
:f is a divisor of g
:g is divisible by f
 if and only if :
:∃ h ∈ D [ x ] : g = f h


This is denoted:
:f  g


=== Notation ===
The conventional notation for x is a divisor of y is ""x | y"", but there is a growing trend to follow the notation ""x  y"", as espoused by   etc.

From  :
:The notation 'm | n' is actually much more common than 'm  n' in current mathematics literature. But vertical lines are overused – for absolute values, set delimiters, conditional probabilities, etc. – and backward slashes are underused. Moreover, 'm  n' gives an impression that m is the denominator of an implied ratio. So we shall boldly let our divisibility symbol lean leftward.


An unfortunate unwelcome side-effect of this notational convention is that to indicate non-divisibility, the conventional technique of implementing / through the notation looks awkward with , so  is eschewed in favour of ∤.


Some sources use | -10mu  3pt - or similar to denote non-divisibility.


Category:Definitions/Divisors",Divisor
['Definitions/Division'],Definition:Divisor,"Let $c = a / b$ denote the division operation on two elements $a$ and $b$ of a field or a Euclidean domain.

The element $b$ is the divisor of $a$.",Definition:Division/Divisor,,false,"Let c = a / b denote the division operation on two elements a and b of a field or a Euclidean domain.

The element b is the divisor of a.",Divisor
['Definitions/Relation Theory'],Definition:Domain,"=== Relation ===
Let $mathcal R subseteq S times T$ be a relation.

The domain of $mathcal R$ is defined and denoted as:
:$mathrm {Dom} left( mathcal R right) := leftlbrace s in S: exists t in T: left( s, t right) in mathcal R rightrbrace$


That is, it is the same as what is defined here as the preimage of $mathcal R$.


=== General Definition ===
Let $ds prod_{i mathop = 1}^n S_i$ be the cartesian product of sets $S_1$ to $S_n$.

Let $ds mathcal R subseteq prod_{i mathop = 1}^n S_i$ be an $n$-ary relation on $ds prod_{i mathop = 1}^n S_i$.

The domain of $mathcal R$ is the set defined as:
:$ds mathrm {Dom} left( mathcal R right) := leftlbrace left( s_1, s_2, ldots, s_{n - 1}  right) in prod_{i mathop = 1}^{n - 1} S_i: exists s_n in S_n: left( s_1, s_2, ldots, s_n right) in mathcal R rightrbrace$


The concept is usually encountered when $mathcal R$ is an endorelation on $S$:
:$ds mathrm {Dom} left( mathcal R right) := leftlbrace left( s_1, s_2, ldots, s_{n - 1}  right) in S^{n - 1}: exists s_n in S_n: left( s_1, s_2, ldots, s_n right) in mathcal R rightrbrace$

=== Class Theory ===
 
Let $V$ be a basic universe.

Let $mathcal R subseteq V times V$ be a relation in $V$.

The domain of $mathcal R$ is defined and denoted as:
:$mathrm {Dom} left( mathcal R right) := leftlbrace x in V: exists y in V: left( x, y right) in mathcal R rightrbrace$

That is, it is the class of all $x$ such that $left( x, y right) in mathcal R$ for at least one $y$.

=== Mapping ===

The term domain is usually seen when the relation in question is actually a mapping.

Let $f: S to T$ be a mapping.

The domain of $f$ is $S$, and can be denoted $mathrm {Dom} left( f right)$.

=== Binary Operation ===
Let $circ: S times S to T$ be a binary operation.

The domain of $circ$ is the set $S$ and can be denoted $mathrm {Dom} left( circ right)$.


This definition can be considered as the same as that for the domain of a mapping, where the domain would be defined as $S times S$.

Category:Definitions/Abstract Algebra",Definition:Domain (Relation Theory),,false,"=== Relation ===
Let ℛ⊆ S × T be a relation.

The domain of ℛ is defined and denoted as:
:Dom( ℛ) := { s ∈ S: ∃ t ∈ T: ( s, t ) ∈ℛ}


That is, it is the same as what is defined here as the preimage of ℛ.


=== General Definition ===
Let ∏_i  = 1^n S_i be the cartesian product of sets S_1 to S_n.

Let ℛ⊆∏_i  = 1^n S_i be an n-ary relation on ∏_i  = 1^n S_i.

The domain of ℛ is the set defined as:
:Dom( ℛ) := {( s_1, s_2, …, s_n - 1) ∈∏_i  = 1^n - 1 S_i: ∃ s_n ∈ S_n: ( s_1, s_2, …, s_n ) ∈ℛ}


The concept is usually encountered when ℛ is an endorelation on S:
:Dom( ℛ) := {( s_1, s_2, …, s_n - 1) ∈ S^n - 1: ∃ s_n ∈ S_n: ( s_1, s_2, …, s_n ) ∈ℛ}

=== Class Theory ===
 
Let V be a basic universe.

Let ℛ⊆ V × V be a relation in V.

The domain of ℛ is defined and denoted as:
:Dom( ℛ) := { x ∈ V: ∃ y ∈ V: ( x, y ) ∈ℛ}

That is, it is the class of all x such that ( x, y ) ∈ℛ for at least one y.

=== Mapping ===

The term domain is usually seen when the relation in question is actually a mapping.

Let f: S → T be a mapping.

The domain of f is S, and can be denoted Dom( f ).

=== Binary Operation ===
Let ∘: S × S → T be a binary operation.

The domain of ∘ is the set S and can be denoted Dom( ∘).


This definition can be considered as the same as that for the domain of a mapping, where the domain would be defined as S × S.

Category:Definitions/Abstract Algebra",Domain
"['Definitions/Algebra', 'Definitions/Variables']",Definition:Domain,"The collection of all possible objects that a variable may refer to has to be specified.

This collection is the domain of the variable.",Definition:Variable/Domain,,false,"The collection of all possible objects that a variable may refer to has to be specified.

This collection is the domain of the variable.",Domain
['Definitions/Morphisms'],Definition:Domain,"Let $f: X to Y$ be a morphism.

Then the domain of $f$ is defined to be the object $X$.

This is usually denoted $X = mathrm {Dom} left( f right)$ or $X = D left(   right)f$.",Definition:Domain (Category Theory),,false,"Let f: X → Y be a morphism.

Then the domain of f is defined to be the object X.

This is usually denoted X = Dom( f ) or X = D (   )f.",Domain
"['Definitions/Integral Domains', 'Definitions/Ring Theory']",Definition:Domain,"=== Definition 1 ===
An integral domain $left( D, +, circ right)$ is:

:a commutative ring which is non-null
:with a unity
:in which there are no (proper) zero divisors, that is:
::: $forall x, y in D: x circ y = 0_D implies x = 0_D text{ or } y = 0_D$

that is, in which all non-zero elements are cancellable.

=== Definition 2 ===
An integral domain $left( D, +, circ right)$ is a commutative ring such that $left( D^*, circ right)$ is a monoid, all of whose elements are cancellable.

In this context, $D^*$ denotes the ring $D$ without zero: $D setminus leftlbrace 0_D rightrbrace$.

=== Integral Domain Axioms ===
 ",Definition:Integral Domain,,false,"=== Definition 1 ===
An integral domain ( D, +, ∘) is:

:a commutative ring which is non-null
:with a unity
:in which there are no (proper) zero divisors, that is:
::: ∀ x, y ∈ D: x ∘ y = 0_D  x = 0_D  or  y = 0_D

that is, in which all non-zero elements are cancellable.

=== Definition 2 ===
An integral domain ( D, +, ∘) is a commutative ring such that ( D^*, ∘) is a monoid, all of whose elements are cancellable.

In this context, D^* denotes the ring D without zero: D ∖{ 0_D }.

=== Integral Domain Axioms ===
 ",Domain
['Definitions/Set Theory'],Definition:Dominate,"Let $S$ and $T$ be sets.


=== Definition 1 ===
Let $S$ and $T$ be sets.


Then $S$ is dominated by $T$  if and only if  there exists an injection from $S$ to $T$.


The notation $S preccurlyeq T$ is used to indicate that $S$ dominates $T$.

=== Definition 2 ===
Let $S$ and $T$ be sets.


Then $S$ is dominated by $T$  if and only if  $S$ is equivalent to some subset of $T$.

That is,  if and only if  there exists a bijection $f: S to T'$ for some $T' subseteq T$.

The notation $S preccurlyeq T$ is used to indicate that $S$ dominates $T$.

=== Strictly Dominated ===
Let $S, T$ be sets.


$S$ is strictly dominated by set $T$  if and only if  $S preccurlyeq T$ but $neg T preccurlyeq S$.

This can be written $S prec T$ or $S < T$.


Category:Definitions/Set Theory",Definition:Dominate (Set Theory),,false,"Let S and T be sets.


=== Definition 1 ===
Let S and T be sets.


Then S is dominated by T  if and only if  there exists an injection from S to T.


The notation S ≼ T is used to indicate that S dominates T.

=== Definition 2 ===
Let S and T be sets.


Then S is dominated by T  if and only if  S is equivalent to some subset of T.

That is,  if and only if  there exists a bijection f: S → T' for some T' ⊆ T.

The notation S ≼ T is used to indicate that S dominates T.

=== Strictly Dominated ===
Let S, T be sets.


S is strictly dominated by set T  if and only if  S ≼ T but T ≼ S.

This can be written S ≺ T or S < T.


Category:Definitions/Set Theory",Dominate
"['Definitions/Analysis', 'Definitions/Complex Analysis']",Definition:Dominate,"Let $leftlangle a_n rightrangle$ be a sequence in $mathbb R$.

Let $leftlangle z_n rightrangle$ be a sequence in $mathbb C$.


Then $leftlangle a_n rightrangle$ dominates $leftlangle z_n rightrangle$  if and only if :
:$forall n in mathbb N: leftlvert z_n rightrvert le a_n$

Category:Definitions/Analysis
Category:Definitions/Complex Analysis",Definition:Dominate (Analysis),,false,"Let ⟨ a_n ⟩ be a sequence in ℝ.

Let ⟨ z_n ⟩ be a sequence in ℂ.


Then ⟨ a_n ⟩ dominates ⟨ z_n ⟩  if and only if :
:∀ n ∈ℕ: | z_n |≤ a_n

Category:Definitions/Analysis
Category:Definitions/Complex Analysis",Dominate
['Definitions/Dram (Avoirdupois)'],Definition:Dram,"The dram is an avoirdupois unit of mass.

=== Conversion Factors ===
",Definition:Avoirdupois/Dram,,false,"The dram is an avoirdupois unit of mass.

=== Conversion Factors ===
",Dram
['Definitions/Drachm'],Definition:Dram,"The drachm is an apothecaries' unit of mass.

=== Conversion Factors ===
",Definition:Apothecaries' Weights and Measures/Mass/Drachm,,false,"The drachm is an apothecaries' unit of mass.

=== Conversion Factors ===
",Dram
['Definitions/Relation Theory'],Definition:Dual,"=== Inverse of Complement ===
Let $mathcal R subseteq S times T$ be a binary relation.


Then the dual of $mathcal R$ is denoted $mathcal R^d$ and is defined as:

:$mathcal R^d := left( overline mathcal R right)^{-1}$

where:
:$overline mathcal R$ denotes the complement of $mathcal R$
:$left( overline mathcal R right)^{-1}$ denotes the inverse of the complement of $mathcal R$.

=== Complement of Inverse ===
Let $mathcal R subseteq S times T$ be a binary relation.


Then the dual of $mathcal R$ is denoted $mathcal R^d$ and is defined as:

:$mathcal R^d := overline {left( mathcal R^{-1}  right) }$

where:
:$mathcal R^{-1}$ denotes the inverse of $mathcal R$
:$overline {left( mathcal R^{-1}  right) }$ denotes the complement of the inverse of $mathcal R$.


Category:Definitions/Relation Theory",Definition:Dual Relation,,false,"=== Inverse of Complement ===
Let ℛ⊆ S × T be a binary relation.


Then the dual of ℛ is denoted ℛ^d and is defined as:

:ℛ^d := ( R )^-1

where:
:R denotes the complement of ℛ
:( R )^-1 denotes the inverse of the complement of ℛ.

=== Complement of Inverse ===
Let ℛ⊆ S × T be a binary relation.


Then the dual of ℛ is denoted ℛ^d and is defined as:

:ℛ^d := ( ℛ^-1)

where:
:ℛ^-1 denotes the inverse of ℛ
:( ℛ^-1) denotes the complement of the inverse of ℛ.


Category:Definitions/Relation Theory",Dual
"['Definitions/Dual Orderings', 'Definitions/Order Theory']",Definition:Dual,"Let $left( S, preceq right)$ be an ordered set.

Let $succeq$ be the inverse relation to $preceq$.

That is, for all $a, b in S$:

:$a succeq b$  if and only if  $b preceq a$


Then $succeq$ is called the dual ordering of $preceq$.


=== Dual Ordered Set ===
Let $left( S, preceq right)$ be an ordered set.

Let $succeq$ be the dual ordering of $preceq$.


The ordered set $left( S, succeq right)$ is called the dual ordered set (or just dual) of $left( S, preceq right)$.


That it indeed is an ordered set is a consequence of Dual Ordering is Ordering.

=== Notation for Dual Ordering ===
To denote the dual of an ordering, the conventional technique is to reverse the symbol.

Thus:
:$succeq$ denotes $preceq^{-1}$
:$succcurlyeq$ denotes $preccurlyeq^{-1}$
:$curlyeqsucc$ denotes $curlyeqprec^{-1}$

and so:
:$a preceq b iff b succeq a$
:$a preccurlyeq b iff b succcurlyeq a$
:$a curlyeqprec b iff b curlyeqsucc a$


Similarly for the standard symbols used to denote an ordering on numbers:
:$ge$ denotes $le^{-1}$
:$geqslant$ denotes $leqslant^{-1}$
:$eqslantgtr$ denotes $eqslantless^{-1}$

and so on.

=== Notation for Dual Strict Ordering ===
To denote the dual of an strict ordering, the conventional technique is to reverse the symbol.

Thus:
:$succ$ denotes $prec^{-1}$

and so:
:$a prec b iff b succ a$


Similarly for the standard symbol used to denote a strict ordering on numbers:
:$>$ denotes $<^{-1}$

and so on.",Definition:Dual Ordering,,false,"Let ( S, ≼) be an ordered set.

Let ≽ be the inverse relation to ≼.

That is, for all a, b ∈ S:

:a ≽ b  if and only if  b ≼ a


Then ≽ is called the dual ordering of ≼.


=== Dual Ordered Set ===
Let ( S, ≼) be an ordered set.

Let ≽ be the dual ordering of ≼.


The ordered set ( S, ≽) is called the dual ordered set (or just dual) of ( S, ≼).


That it indeed is an ordered set is a consequence of Dual Ordering is Ordering.

=== Notation for Dual Ordering ===
To denote the dual of an ordering, the conventional technique is to reverse the symbol.

Thus:
:≽ denotes ≼^-1
:≽ denotes ≼^-1
:⋟ denotes ⋞^-1

and so:
:a ≼ b  b ≽ a
:a ≼ b  b ≽ a
:a ⋞ b  b ⋟ a


Similarly for the standard symbols used to denote an ordering on numbers:
:≥ denotes ≤^-1
:⩾ denotes ⩽^-1
:⪖ denotes ⪕^-1

and so on.

=== Notation for Dual Strict Ordering ===
To denote the dual of an strict ordering, the conventional technique is to reverse the symbol.

Thus:
:≻ denotes ≺^-1

and so:
:a ≺ b  b ≻ a


Similarly for the standard symbol used to denote a strict ordering on numbers:
:> denotes <^-1

and so on.",Dual
['Definitions/Order Theory'],Definition:Dual,"Let $left( S, preceq right)$ be an ordered set.

Let $succeq$ be the dual ordering to $preceq$.

Let $unicode{x3a3}$ be any statement pertaining to $left( S, preceq right)$ (be it in natural language or a formal language).


The dual statement of $unicode{x3a3}$, denoted $unicode{x3a3}^*$, is the statement obtained from replacing every reference to $preceq$ in $unicode{x3a3}$ with a reference to its dual $succeq$.

This dual statement may then be turned into a statement about $preceq$ again by applying the equivalences on Dual Pairs (Order Theory).",Definition:Dual Statement (Order Theory),,false,"Let ( S, ≼) be an ordered set.

Let ≽ be the dual ordering to ≼.

Let x3a3 be any statement pertaining to ( S, ≼) (be it in natural language or a formal language).


The dual statement of x3a3, denoted x3a3^*, is the statement obtained from replacing every reference to ≼ in x3a3 with a reference to its dual ≽.

This dual statement may then be turned into a statement about ≼ again by applying the equivalences on Dual Pairs (Order Theory).",Dual
['Definitions/Order Theory'],Definition:Dual,"Let $left( S, preceq_S right)$ and $left( T, preceq_T right)$ be ordered sets.

Let $phi: S to T$ be a bijection.


Then $phi$ is a dual isomorphism between $left( S, preceq_S right)$ and $left( T, preceq_T right)$  if and only if  $phi$ and $phi^{-1}$ are decreasing mappings.


If there is a dual isomorphism between $left( S, preceq_S right)$ and $left( T, preceq_T right)$, then $left( S, preceq_S right)$ is dual to $left( T, preceq_T right)$.

Equivalently, $left( S, preceq_S right)$ is dual to $left( T, preceq_T right)$  if and only if  $S$ with the dual ordering is isomorphic to $T$.",Definition:Dual Isomorphism (Order Theory),,false,"Let ( S, ≼_S ) and ( T, ≼_T ) be ordered sets.

Let ϕ: S → T be a bijection.


Then ϕ is a dual isomorphism between ( S, ≼_S ) and ( T, ≼_T )  if and only if  ϕ and ϕ^-1 are decreasing mappings.


If there is a dual isomorphism between ( S, ≼_S ) and ( T, ≼_T ), then ( S, ≼_S ) is dual to ( T, ≼_T ).

Equivalently, ( S, ≼_S ) is dual to ( T, ≼_T )  if and only if  S with the dual ordering is isomorphic to T.",Dual
['Definitions/Order Embeddings'],Definition:Dual,"Let $left( S, preceq_1 right)$ and $left( T, preceq_2 right)$ be ordered sets.


A dual order embedding is a mapping $phi: S to T$ such that:

:$forall x, y in S: x preceq_1 y iff phi left(   right)y preceq_2 phi left(   right)x$


That is:
:if $phi$ is an order embedding of $left( S, preceq_1 right)$ into $left( T, succeq_2 right)$
where $succeq_2$ is the dual of $preceq_2$.

Category:Definitions/Order Embeddings",Definition:Dual Order Embedding,,false,"Let ( S, ≼_1 ) and ( T, ≼_2 ) be ordered sets.


A dual order embedding is a mapping ϕ: S → T such that:

:∀ x, y ∈ S: x ≼_1 y ϕ(   )y ≼_2 ϕ(   )x


That is:
:if ϕ is an order embedding of ( S, ≼_1 ) into ( T, ≽_2 )
where ≽_2 is the dual of ≼_2.

Category:Definitions/Order Embeddings",Dual
['Definitions/Order Theory'],Definition:Dual,"Let $left({S, preceq}right)$ be an ordered set.

If $S$ is dual to $S$, then $S$ is self-dual.",Definition:Self-Dual (Order Theory),,false,"Let (S, ≼) be an ordered set.

If S is dual to S, then S is self-dual.",Dual
"['Definitions/Linear Forms (Linear Algebra)', 'Definitions/Algebraic Duals']",Definition:Dual,"Let $left( R, +, times right)$ be a commutative ring.

Let $left( G, +_G, circ right)_R$ be an $R$-module.

Let $left( R, +_R, circ right)_R$ denote the $R$-module $R$.


The $R$-module $mathcal L_R left(   right){G, R}$ of all linear forms on $G$ is usually denoted $G^*$ and is called the algebraic dual of $G$.


=== Double Dual ===
Let $R$ be a commutative ring.

Let $G$ be an $R$-module.


The double dual $G^{**}$ of $G$ is the dual of its dual $G^*$.


Category:Definitions/Algebraic Duals",Definition:Algebraic Dual,,false,"Let ( R, +, ×) be a commutative ring.

Let ( G, +_G, ∘)_R be an R-module.

Let ( R, +_R, ∘)_R denote the R-module R.


The R-module ℒ_R (   )G, R of all linear forms on G is usually denoted G^* and is called the algebraic dual of G.


=== Double Dual ===
Let R be a commutative ring.

Let G be an R-module.


The double dual G^** of G is the dual of its dual G^*.


Category:Definitions/Algebraic Duals",Dual
['Definitions/Algebraic Duals'],Definition:Dual,"Let $R$ be a commutative ring.

Let $G$ be an $R$-module.


The double dual $G^{**}$ of $G$ is the dual of its dual $G^*$.


Category:Definitions/Algebraic Duals",Definition:Algebraic Dual/Double Dual,,false,"Let R be a commutative ring.

Let G be an R-module.


The double dual G^** of G is the dual of its dual G^*.


Category:Definitions/Algebraic Duals",Dual
['Definitions/Linear Algebra'],Definition:Dual,"Let $R$ be a commutative ring with unity.

Let $left( G, +_G, circ right)_R$ be an $n$-dimensional module over $R$.

Let $leftlangle a_n rightrangle$ be an ordered basis of $G$.

Let $G^*$ be the algebraic dual of $G$.


Then there is an ordered basis $leftlangle a'_n rightrangle$ of $G^*$ satisfying $forall i, j in left[ 1 ,.,.,   right]n: a'_i left(   right){a_j} = delta_{i j}$.


This ordered basis $leftlangle a'_n rightrangle$ of $G^*$ is called the ordered basis of $G^*$ dual to $leftlangle a_n rightrangle$, or the ordered dual basis of $G^*$.",Definition:Ordered Dual Basis,,false,"Let R be a commutative ring with unity.

Let ( G, +_G, ∘)_R be an n-dimensional module over R.

Let ⟨ a_n ⟩ be an ordered basis of G.

Let G^* be the algebraic dual of G.


Then there is an ordered basis ⟨ a'_n ⟩ of G^* satisfying ∀ i, j ∈[ 1  . . ]n: a'_i (   )a_j = δ_i j.


This ordered basis ⟨ a'_n ⟩ of G^* is called the ordered basis of G^* dual to ⟨ a_n ⟩, or the ordered dual basis of G^*.",Dual
"['Definitions/Vector Spaces', 'Definitions/Algebraic Duals']",Definition:Dual,"Let $V$ be a vector space.

Let $phi: V to mathbb R$ be a linear mapping.


The set of all $phi$ is called a dual space (of $V$) and is denoted by $V^*$.",Definition:Dual Vector Space,,false,"Let V be a vector space.

Let ϕ: V →ℝ be a linear mapping.


The set of all ϕ is called a dual space (of V) and is denoted by V^*.",Dual
"['Definitions/Normed Vector Spaces', 'Definitions/Functional Analysis', 'Definitions/Normed Dual Spaces']",Definition:Dual,"Let $left( X, leftlVert cdot rightrVert_X right)$ be a normed vector space.

Let $X^ast$ be the vector space of bounded linear functionals on $X$. 

Let $leftlVert cdot rightrVert_{X^ast}$ be the norm on bounded linear functionals.


We say that $left( X^ast, leftlVert cdot rightrVert_{X^ast}  right)$ is the normed dual space of $X$.",Definition:Normed Dual Space,,false,"Let ( X, ‖·‖_X ) be a normed vector space.

Let X^∗ be the vector space of bounded linear functionals on X. 

Let ‖·‖_X^∗ be the norm on bounded linear functionals.


We say that ( X^∗, ‖·‖_X^∗) is the normed dual space of X.",Dual
"['Definitions/Normed Dual Spaces', 'Definitions/Second Normed Duals']",Definition:Dual,"Let $left( X, leftlVert cdot rightrVert_X right)$ be a normed vector space.

Let $left( X^ast, leftlVert cdot rightrVert_{X^ast}  right)$ be the normed dual of $left( X, leftlVert cdot rightrVert_X right)$.


We define the second normed dual, written $left( X^{ast ast}, leftlVert cdot rightrVert_{X^{ast ast} }  right)$ as the normed dual of $left( X^ast, leftlVert cdot rightrVert_{X^ast}  right)$.",Definition:Second Normed Dual,,false,"Let ( X, ‖·‖_X ) be a normed vector space.

Let ( X^∗, ‖·‖_X^∗) be the normed dual of ( X, ‖·‖_X ).


We define the second normed dual, written ( X^∗∗, ‖·‖_X^∗∗) as the normed dual of ( X^∗, ‖·‖_X^∗).",Dual
"['Definitions/Category Theory', 'Definitions/Examples of Categories']",Definition:Dual,"Let $mathbf C$ be a metacategory.


Its dual category, denoted $mathbf C^{text{op} }$, is defined as follows:

 

It can be seen that this comes down to the metacategory obtained by reversing the direction of all morphisms of $mathbf C$.",Definition:Dual Category,,false,"Let 𝐂 be a metacategory.


Its dual category, denoted 𝐂^op, is defined as follows:

 

It can be seen that this comes down to the metacategory obtained by reversing the direction of all morphisms of 𝐂.",Dual
['Definitions/Category Theory'],Definition:Dual,"=== Morphisms-Only Category Theory ===

Let $unicode{x3a3}$ be a statement in the language of category theory.

The dual statement $unicode{x3a3}^*$ of $unicode{x3a3}$ is the statement obtained from substituting:

 
 
 
 
 


=== Object Category Theory ===

In the more convenient description of metacategories by using objects, the dual statement $unicode{x3a3}^*$ of $unicode{x3a3}$ then becomes the statement obtained from substituting:

 
 
 
 
 
 


=== Example ===

For example, if $unicode{x3a3}$ is the statement:

:$exists g: g circ f = operatorname{id}_{mathrm {Dom} left( f right)}$

describing that $f$ is a split mono, then $unicode{x3a3}^*$ becomes:

:$exists g: f circ g = operatorname{id}_{mathrm {Cdm} left( f right)}$

which precisely expresses $f$ to be a split epi.


For a set $mathcal E$ of statements, write:

:$mathcal E^* := leftlbrace unicode{x3a3}^*: unicode{x3a3} in mathcal E rightrbrace$ 

for the set comprising of the dual statement of those in $mathcal E$.",Definition:Dual Statement (Category Theory),,false,"=== Morphisms-Only Category Theory ===

Let x3a3 be a statement in the language of category theory.

The dual statement x3a3^* of x3a3 is the statement obtained from substituting:

 
 
 
 
 


=== Object Category Theory ===

In the more convenient description of metacategories by using objects, the dual statement x3a3^* of x3a3 then becomes the statement obtained from substituting:

 
 
 
 
 
 


=== Example ===

For example, if x3a3 is the statement:

:∃ g: g ∘ f = id_Dom( f )

describing that f is a split mono, then x3a3^* becomes:

:∃ g: f ∘ g = id_Cdm( f )

which precisely expresses f to be a split epi.


For a set ℰ of statements, write:

:ℰ^* := {x3a3^*: x3a3∈ℰ} 

for the set comprising of the dual statement of those in ℰ.",Dual
['Definitions/Polyhedra'],Definition:Dual,"Let $P$ be a polyhedron.

The dual polyhedron $D$ of $P$ is the polyhedron which can be constructed as follows:

:$(1): quad$ The vertices of $D$ are the centroids of the faces of $P$.

:$(2): quad$ For each edge of $P$ which is adjacent to two faces $F_1$ and $F_2$ of $P$, an edge of $D$ is constructed which is adjacent to the vertices of $D$ forming the centroids of $F_1$ and $F_2$.",Definition:Dual Polyhedron,,false,"Let P be a polyhedron.

The dual polyhedron D of P is the polyhedron which can be constructed as follows:

:(1): The vertices of D are the centroids of the faces of P.

:(2): For each edge of P which is adjacent to two faces F_1 and F_2 of P, an edge of D is constructed which is adjacent to the vertices of D forming the centroids of F_1 and F_2.",Dual
"['Definitions/Eccentricity of Conic Section', 'Definitions/Conic Sections']",Definition:Eccentricity,"Let $K$ be a conic section specified in terms of:
:a given straight line $D$
:a given point $F$
:a given constant $e$

where $K$ is the locus of points $P$ such that the distance $p$ from $P$ to $D$ and the distance $q$ from $P$ to $F$ are related by the condition:
:$q = e p$


The constant $e$ is known as the eccentricity of the conic section.",Definition:Conic Section/Eccentricity,,false,"Let K be a conic section specified in terms of:
:a given straight line D
:a given point F
:a given constant e

where K is the locus of points P such that the distance p from P to D and the distance q from P to F are related by the condition:
:q = e p


The constant e is known as the eccentricity of the conic section.",Eccentricity
['Definitions/Graph Theory'],Definition:Eccentricity,"Let $G = left( V, E right)$ be a graph.

Let $v in V$ be a vertex of $G$.


The eccentricity of $v$ is the maximum distance from $v$ to another vertex of $G$:


That is:
:$E left(   right)v = ds max_{u mathop in V} D left(   right){v, u}$

where $D left(   right){v, u}$ denotes the distance from $v$ to $u$.",Definition:Eccentricity of Vertex,,false,"Let G = ( V, E ) be a graph.

Let v ∈ V be a vertex of G.


The eccentricity of v is the maximum distance from v to another vertex of G:


That is:
:E (   )v = max_u ∈ V D (   )v, u

where D (   )v, u denotes the distance from v to u.",Eccentricity
"['Definitions/Edges of Graphs', 'Definitions/Edges']",Definition:Edge," 


Let $G = left( V, E right)$ be a graph.

The edges are the elements of $E$.


In the above, the edges are $AB, AE, BE, CD, CE, CF, DE, DF, FG$.


=== Join ===
Let $G = left( V, E right)$ be a graph.

Let $u$ and $v$ be vertices of $G$.

Let $e = u v$ be an edge of $G$.


Then $e$ joins the vertices $u$ and $v$.",Definition:Graph (Graph Theory)/Edge,,false," 


Let G = ( V, E ) be a graph.

The edges are the elements of E.


In the above, the edges are AB, AE, BE, CD, CE, CF, DE, DF, FG.


=== Join ===
Let G = ( V, E ) be a graph.

Let u and v be vertices of G.

Let e = u v be an edge of G.


Then e joins the vertices u and v.",Edge
"['Definitions/Edges of Polyhedra', 'Definitions/Polyhedra', 'Definitions/Edges']",Definition:Edge,The edges of a polyhedron are the sides of the polygons which constitute its faces.,Definition:Polyhedron/Edge,,false,The edges of a polyhedron are the sides of the polygons which constitute its faces.,Edge
"['Definitions/Polyhedral Angles', 'Definitions/Edges']",Definition:Edge,Each of the half-lines that form a polyhedral angle is known as an edge of the polyhedral angle.,Definition:Polyhedral Angle/Edge,,false,Each of the half-lines that form a polyhedral angle is known as an edge of the polyhedral angle.,Edge
"['Definitions/Half-Planes', 'Definitions/Edges']",Definition:Edge,"Let $mathcal P$ denote the plane.

Let $mathcal L$ denote an infinite straight line in $mathcal P$.

Let $mathcal H$ denote one of the half-planes into which $mathcal L$ divides $mathcal P$.

Then $mathcal L$ is called the edge of $mathcal H$.",Definition:Half-Plane/Edge,,false,"Let 𝒫 denote the plane.

Let ℒ denote an infinite straight line in 𝒫.

Let ℋ denote one of the half-planes into which ℒ divides 𝒫.

Then ℒ is called the edge of ℋ.",Edge
"['Definitions/Morphisms', 'Definitions/Category Theory']",Definition:Edge,"Let $mathbf C$ be a metacategory.


A morphism of $mathbf C$ is an object $f$, together with:

* A domain $operatorname {dom} f$, which is an object of $mathbf C$
* A codomain $operatorname {cod} f$, also an object of $mathbf C$


The collection of all morphisms of $mathbf C$ is denoted $mathbf C_1$.


If $A$ is the domain of $f$ and $B$ is its codomain, this is mostly represented by writing:

:$f: A to B$ or $A stackrel f longrightarrow B$",Definition:Morphism,,false,"Let 𝐂 be a metacategory.


A morphism of 𝐂 is an object f, together with:

* A domain dom f, which is an object of 𝐂
* A codomain cod f, also an object of 𝐂


The collection of all morphisms of 𝐂 is denoted 𝐂_1.


If A is the domain of f and B is its codomain, this is mostly represented by writing:

:f: A → B or A  f ⟶ B",Edge
"['Definitions/Efficiency (Statistics)', 'Definitions/Statistics', 'Definitions/Efficiency']",Definition:Efficiency,"Let $T_0$ and $T_1$ both be statistics used as estimators.

Efficiency is a comparison of the variances of $T_0$ and $T_1$.


Thus $T_0$ is of higher efficiency than $T_1$  if and only if  $T_0$ has a smaller variance than $T_1$.",Definition:Efficiency (Statistics),,false,"Let T_0 and T_1 both be statistics used as estimators.

Efficiency is a comparison of the variances of T_0 and T_1.


Thus T_0 is of higher efficiency than T_1  if and only if  T_0 has a smaller variance than T_1.",Efficiency
"['Definitions/Efficiency (Experimental Design)', 'Definitions/Experimental Designs', 'Definitions/Efficiency']",Definition:Efficiency,One design is more efficient than another if the same precision can be achieved with the same resources.,Definition:Efficiency (Experimental Design),,false,One design is more efficient than another if the same precision can be achieved with the same resources.,Efficiency
"['Definitions/Efficiency (Physics)', 'Definitions/Physics', 'Definitions/Efficiency']",Definition:Efficiency,"Let $S$ be a system.

=== Definition 1 ===
Let $S$ be a system.


The efficiency of $S$ is defined as:
:$mathcal E = dfrac {E_O} {E_I} times 100 %$
where:
:$E_I$ denotes the energy input to $S$ over a particular time interval $T$
:$E_O$ denotes the energy output from $S$ over that same time interval $T$.

=== Definition 2 ===
Let $S$ be a system.


The efficiency of $S$ is defined as:
:$mathcal E = dfrac {W_L} {W_E} times 100 %$
where:
:$W_L$ denotes the work done by the load of $S$
:$W_E$ denotes the work done by the effort of $S$.",Definition:Efficiency (Physics),,false,"Let S be a system.

=== Definition 1 ===
Let S be a system.


The efficiency of S is defined as:
:ℰ = E_OE_I× 100 %
where:
:E_I denotes the energy input to S over a particular time interval T
:E_O denotes the energy output from S over that same time interval T.

=== Definition 2 ===
Let S be a system.


The efficiency of S is defined as:
:ℰ = W_LW_E× 100 %
where:
:W_L denotes the work done by the load of S
:W_E denotes the work done by the effort of S.",Efficiency
"['Definitions/Elevation of Point', 'Definitions/Elevation']",Definition:Elevation,The elevation of a point $P$ is the height of $P$ above some reference horizontal baseline or plane.,Definition:Elevation of Point,,false,The elevation of a point P is the height of P above some reference horizontal baseline or plane.,Elevation
"['Definitions/Angles of Elevation', 'Definitions/Angles', 'Definitions/Elevation']",Definition:Elevation,"Let $A$ and $B$ be points in space such that $A$ is higher than $B$.

The angle of elevation of $A$ from $B$ is the angle between the line $AB$ and the horizontal.",Definition:Angle of Elevation,,false,"Let A and B be points in space such that A is higher than B.

The angle of elevation of A from B is the angle between the line AB and the horizontal.",Elevation
"['Definitions/Embeddings (Topology)', 'Definitions/Homeomorphisms (Topological Spaces)', 'Definitions/Topology']",Definition:Embedding,"Let $A, B$ be topological spaces.

Let $f: A to B$ be a mapping.

Let the image of $f$ be given the subspace topology.

Let the restriction $f {restriction_{A times fleft[ A right] }}$ of $f$ to its image be a homeomorphism.


Then $f$ is an embedding (of $A$ into $B$).",Definition:Embedding (Topology),,false,"Let A, B be topological spaces.

Let f: A → B be a mapping.

Let the image of f be given the subspace topology.

Let the restriction f _A × f[ A ] of f to its image be a homeomorphism.


Then f is an embedding (of A into B).",Embedding
"['Definitions/Model Theory for Predicate Logic', 'Definitions/Automorphisms']",Definition:Embedding,"Let $mathcal M$ and $mathcal N$ be $mathcal L$-structures with universes $M$ and $N$ respectively.


$j: mathcal M to mathcal N$ is an $mathcal L$-embedding  if and only if  it is an injective map $M to N$ which preserves interpretations of all symbols in $mathcal L$; that is, such that:
:$j left(   right){f^mathcal M left(   right){a_1, dots, a_{n_f} } } = f^mathcal N left(   right){j left(   right){a_1}, ldots, j left(   right){a_{n_f} } }$ for all function symbols $f$ in $mathcal L$ and $a_1, dots, a_{n_f}$ in $M$
:$left( a_1, ldots, a_{n_R}  right) in R^mathcal M iff left( j left(   right){a_1}, dots, j left(   right){a_{n_R} }  right) in R^mathcal N$ for all relation symbols $R$ in $mathcal L$ and $a_1, dots, a_{n_R}$ in $M$
:$j left(   right){c^mathcal M} = c^mathcal N$ for all constant symbols $c$ in $mathcal L$.


=== Partial Embedding ===

A common method of constructing isomorphisms and elementary embeddings in proofs is to recursively define them a finite number of elements at a time.  For this  purpose, it is useful to have a definition of embeddings using functions which are only defined on a subset of $M$:


Let $A subseteq M$ be a subset of $M$.


$j: A to mathcal N$ is a partial $mathcal L$-embedding  if and only if  it is an injective map $A to N$ which preserves interpretations of all symbols in $mathcal L$ applied to elements of $A$; that is, such that:
:$j left(   right){f^mathcal M left(   right){a_1, dots, a_{n_f} } } = f^mathcal N left(   right){j left(   right){a_1}, ldots, j left(   right){a_{n_f} } }$ for  all function symbols $f$ in $mathcal L$ and $a_1, dots, a_{n_f}$ in $A$
:$left( a_1, ldots, a_{n_R}  right) in R^mathcal M iff left( j left(   right){a_1}, dots, j left(   right){a_{n_R} }  right) in R^mathcal N$ for all relation symbols $R$ in $mathcal L$ and $a_1, dots, a_{n_R}$ in $A$
:$j left(   right){c^mathcal M} = c^mathcal N$ for all constant symbols $c$ in $mathcal L$.


=== Isomorphism ===

$j: mathcal M to mathcal N$ is an $mathcal L$-isomorphism  if and only if  it is a bijective $mathcal L$-embedding.


=== Automorphism ===

$j: mathcal M to mathcal N$ is an $mathcal L$-automorphism  if and only if  it is an $mathcal L$-isomorphism and $mathcal M = mathcal N$.


It is often useful to talk about automorphisms which are constant on subsets of $M$.  So, there is a definition and a notation for doing so:

Let $A subseteq M$ be a subset of $M$, and let $b in M$.


An $mathcal L$-automorphism $j$ is an $A$-automorphism  if and only if  $j left(   right)a = a$ for all $ain A$.

An $mathcal L$-automorphism $j$ is an $A, b$-automorphism  if and only if  it is an $left( A cup leftlbrace b rightrbrace right)$-automorphism; that is: $j left(   right)a = a$ for all $a in A$ and also $j left(   right)b = b$.",Definition:Embedding (Model Theory),,false,"Let ℳ and 𝒩 be ℒ-structures with universes M and N respectively.


j: ℳ→𝒩 is an ℒ-embedding  if and only if  it is an injective map M → N which preserves interpretations of all symbols in ℒ; that is, such that:
:j (   )f^ℳ(   )a_1, …, a_n_f = f^𝒩(   )j (   )a_1, …, j (   )a_n_f for all function symbols f in ℒ and a_1, …, a_n_f in M
:( a_1, …, a_n_R) ∈ R^ℳ( j (   )a_1, …, j (   )a_n_R) ∈ R^𝒩 for all relation symbols R in ℒ and a_1, …, a_n_R in M
:j (   )c^ℳ = c^𝒩 for all constant symbols c in ℒ.


=== Partial Embedding ===

A common method of constructing isomorphisms and elementary embeddings in proofs is to recursively define them a finite number of elements at a time.  For this  purpose, it is useful to have a definition of embeddings using functions which are only defined on a subset of M:


Let A ⊆ M be a subset of M.


j: A →𝒩 is a partial ℒ-embedding  if and only if  it is an injective map A → N which preserves interpretations of all symbols in ℒ applied to elements of A; that is, such that:
:j (   )f^ℳ(   )a_1, …, a_n_f = f^𝒩(   )j (   )a_1, …, j (   )a_n_f for  all function symbols f in ℒ and a_1, …, a_n_f in A
:( a_1, …, a_n_R) ∈ R^ℳ( j (   )a_1, …, j (   )a_n_R) ∈ R^𝒩 for all relation symbols R in ℒ and a_1, …, a_n_R in A
:j (   )c^ℳ = c^𝒩 for all constant symbols c in ℒ.


=== Isomorphism ===

j: ℳ→𝒩 is an ℒ-isomorphism  if and only if  it is a bijective ℒ-embedding.


=== Automorphism ===

j: ℳ→𝒩 is an ℒ-automorphism  if and only if  it is an ℒ-isomorphism and ℳ = 𝒩.


It is often useful to talk about automorphisms which are constant on subsets of M.  So, there is a definition and a notation for doing so:

Let A ⊆ M be a subset of M, and let b ∈ M.


An ℒ-automorphism j is an A-automorphism  if and only if  j (   )a = a for all a∈ A.

An ℒ-automorphism j is an A, b-automorphism  if and only if  it is an ( A ∪{ b })-automorphism; that is: j (   )a = a for all a ∈ A and also j (   )b = b.",Embedding
"['Definitions/Galois Theory', 'Definitions/Field Theory']",Definition:Embedding,"Let $K$ and $L$ be fields.

A (field) monomorphism $phi: K to L$ is called an embedding of $K$ in $L$.",Definition:Embedding (Galois Theory),,false,"Let K and L be fields.

A (field) monomorphism ϕ: K → L is called an embedding of K in L.",Embedding
"['Definitions/Monomorphisms (Abstract Algebra)', 'Definitions/Ring Homomorphisms']",Definition:Embedding,"Let $left( R, +, circ right)$ and $left( S, oplus, * right)$ be rings.

Let $phi: R to S$ be a (ring) homomorphism.


Then $phi$ is a ring monomorphism  if and only if  $phi$ is an injection.",Definition:Ring Monomorphism,,false,"Let ( R, +, ∘) and ( S, ⊕, * ) be rings.

Let ϕ: R → S be a (ring) homomorphism.


Then ϕ is a ring monomorphism  if and only if  ϕ is an injection.",Embedding
['Definitions/Category Theory'],Definition:Embedding,"Let $C$ and $D$ be categories.

Let $F : C to D$ be a functor.


=== Definition 1 ===

The functor $F$ is an embedding  if and only if  it is:
* injective on objects
* faithful


=== Definition 2 ===

The functor $F$ is an embedding  if and only if  it is injective on morphisms.


=== Definition 3 ===

The functor $F$ is an embedding  if and only if  it is a monomorphisms in the category of categories.",Definition:Embedding of Categories,,false,"Let C and D be categories.

Let F : C → D be a functor.


=== Definition 1 ===

The functor F is an embedding  if and only if  it is:
* injective on objects
* faithful


=== Definition 2 ===

The functor F is an embedding  if and only if  it is injective on morphisms.


=== Definition 3 ===

The functor F is an embedding  if and only if  it is a monomorphisms in the category of categories.",Embedding
"['Definitions/Empty Set', 'Definitions/Set Theory']",Definition:Empty,"The empty set is a set which has no elements.

That is, $x in varnothing$ is false, whatever $x$ is.


It is usually denoted by some variant of a zero with a line through it, for example $varnothing$ or $emptyset$, and can always be represented as $leftlbrace  rightrbrace$.",Definition:Empty Set,,false,"The empty set is a set which has no elements.

That is, x ∈∅ is false, whatever x is.


It is usually denoted by some variant of a zero with a line through it, for example ∅ or ∅, and can always be represented as {}.",Empty
['Definitions/Class Theory'],Definition:Empty,"A class is defined as being empty  if and only if  it has no elements.

That is:
:$forall x: x notin A$
or:
:$neg exists x: x in A$


The empty class is usually denoted $varnothing$ or $emptyset$.

On   the preferred symbol is $varnothing$.",Definition:Empty Class (Class Theory),,false,"A class is defined as being empty  if and only if  it has no elements.

That is:
:∀ x: x ∉ A
or:
:∃ x: x ∈ A


The empty class is usually denoted ∅ or ∅.

On   the preferred symbol is ∅.",Empty
['Definitions/Mapping Theory'],Definition:Empty,"Let $T$ be a set.


Then the mapping $e: varnothing to T$ whose domain is the empty set and whose codomain is $T$ is called the empty mapping:
:$e subseteq varnothing times T = varnothing$",Definition:Empty Mapping,,false,"Let T be a set.


Then the mapping e: ∅→ T whose domain is the empty set and whose codomain is T is called the empty mapping:
:e ⊆∅× T = ∅",Empty
"['Definitions/Null Relation', 'Definitions/Empty Set', 'Definitions/Examples of Relations']",Definition:Empty,"The null relation is a relation $mathcal R$ in $S$ to $T$ such that $mathcal R$ is the empty set:
:$mathcal R subseteq S times T: mathcal R = varnothing$


That is, no element of $S$ relates to any element in $T$:
:$mathcal R: S times T: forall left( s, t right) in S times T: neg s mathrel mathcal R t$",Definition:Null Relation,,false,"The null relation is a relation ℛ in S to T such that ℛ is the empty set:
:ℛ⊆ S × T: ℛ = ∅


That is, no element of S relates to any element in T:
:ℛ: S × T: ∀( s, t ) ∈ S × T:  s ℛ t",Empty
"['Definitions/Edgeless Graphs', 'Definitions/Graph Theory']",Definition:Empty,"An edgeless graph is a graph with no edges.

That is, an edgeless graph is a graph of size zero.

Equivalently, an edgeless graph is a graph whose vertices are all isolated.


The edgeless graph of order $n$ is denoted $N_n$ and can be referred to as the $n$-edgeless graph.",Definition:Edgeless Graph,,false,"An edgeless graph is a graph with no edges.

That is, an edgeless graph is a graph of size zero.

Equivalently, an edgeless graph is a graph whose vertices are all isolated.


The edgeless graph of order n is denoted N_n and can be referred to as the n-edgeless graph.",Empty
['Definitions/Summations'],Definition:Empty,"Take the summation:
:$ds sum_{Phi left(   right)j} a_j$
where $Phi left(   right)j$ is a propositional function of $j$.

Suppose that there are no values of $j$ for which $Phi left(   right)j$ is true.

Then $ds sum_{Phi left(   right)j} a_j$ is defined as being $0$.

This summation is called a vacuous summation.


This is because:
:$forall a: a + 0 = a$
where $a$ is a number.

Hence for all $j$ for which $Phi left(   right)j$ is false, the sum is unaffected.


This is most frequently seen in the form:
:$ds sum_{j mathop = m}^n a_j = 0$
where $m > n$.

In this case, $j$ can not at the same time be both greater than or equal to $m$ and less than or equal to $n$.


Some sources consider such a treatment as abuse of notation.",Definition:Summation/Vacuous Summation,,false,"Take the summation:
:∑_Φ(   )j a_j
where Φ(   )j is a propositional function of j.

Suppose that there are no values of j for which Φ(   )j is true.

Then ∑_Φ(   )j a_j is defined as being 0.

This summation is called a vacuous summation.


This is because:
:∀ a: a + 0 = a
where a is a number.

Hence for all j for which Φ(   )j is false, the sum is unaffected.


This is most frequently seen in the form:
:∑_j  = m^n a_j = 0
where m > n.

In this case, j can not at the same time be both greater than or equal to m and less than or equal to n.


Some sources consider such a treatment as abuse of notation.",Empty
['Definitions/Class Intervals'],Definition:Empty,A class interval is empty  if and only if  it is of frequency zero.,Definition:Class Interval/Empty,,false,A class interval is empty  if and only if  it is of frequency zero.,Empty
['Definitions/Examples of Categories'],Definition:Empty,"The category $mathbf 0$, zero, is the empty category:


:$qquad$


with:
:no objects
and consequently:
:no morphisms.",Definition:Zero (Category),,false,"The category 0, zero, is the empty category:


:


with:
:no objects
and consequently:
:no morphisms.",Empty
['Definitions/Class Theory'],Definition:Empty Class,"A class is defined as being empty  if and only if  it has no elements.

That is:
:$forall x: x notin A$
or:
:$neg exists x: x in A$


The empty class is usually denoted $varnothing$ or $emptyset$.

On   the preferred symbol is $varnothing$.",Definition:Empty Class (Class Theory),,false,"A class is defined as being empty  if and only if  it has no elements.

That is:
:∀ x: x ∉ A
or:
:∃ x: x ∈ A


The empty class is usually denoted ∅ or ∅.

On   the preferred symbol is ∅.",Empty Class
['Definitions/Class Intervals'],Definition:Empty Class,A class interval is empty  if and only if  it is of frequency zero.,Definition:Class Interval/Empty,,false,A class interval is empty  if and only if  it is of frequency zero.,Empty Class
"['Definitions/Uncertainty', 'Definitions/Information Theory', 'Definitions/Probability Theory']",Definition:Entropy,"Let $X$ be a discrete random variable.

Let $X$ take a finite number of values with probabilities $p_1, p_2, dotsc, p_n$.


The uncertainty of $X$ is defined as:

:$H left(   right)X = ds -sum_k p_k lg p_k$

where:
:$lg$ denotes logarithm base $2$
:the summation is over those $k$ where $p_k > 0$.


=== Units ===
The unit of measurement used to quantify uncertainty is the bit.


Category:Definitions/Uncertainty",Definition:Uncertainty,,false,"Let X be a discrete random variable.

Let X take a finite number of values with probabilities p_1, p_2, …, p_n.


The uncertainty of X is defined as:

:H (   )X =  -∑_k p_k  p_k

where:
: denotes logarithm base 2
:the summation is over those k where p_k > 0.


=== Units ===
The unit of measurement used to quantify uncertainty is the bit.


Category:Definitions/Uncertainty",Entropy
['Definitions/Probability Theory'],Definition:Entropy,"Differential entropy extends the concept of entropy to continuous random variables.

Let $X$ be a continuous random variable.

Let $X$ have probability density function $f_X$. 

Then the differential entropy of $X$, $h left(   right)X$ measured in nats, is given by: 

:$ds h left(   right)X = -int_{-infty}^infty f_X left(   right)x ln f_X left(   right)x ,mathrm d x$


Where $f_X left(   right)x = 0$, we take $f_X left(   right)x ln f_X left(   right)x = 0$ by convention.",Definition:Differential Entropy,,false,"Differential entropy extends the concept of entropy to continuous random variables.

Let X be a continuous random variable.

Let X have probability density function f_X. 

Then the differential entropy of X, h (   )X measured in nats, is given by: 

:h (   )X = -∫_-∞^∞ f_X (   )x ln f_X (   )x  d x


Where f_X (   )x = 0, we take f_X (   )x ln f_X (   )x = 0 by convention.",Entropy
"['Definitions/Physics', 'Definitions/Physical Quantities', 'Definitions/Examples of Scalar Quantities']",Definition:Entropy,"Entropy is a property of a thermodynamic system.

It quantifies the number $Omega$ of microstates that are consistent with the macroscopic quantities that characterize the system.


The entropy of a system is equal to the expectation of the value:
:$k ln P$
where:
:$k$ is a constant which relates the mean kinetic energy and absolute temperature of the system
:$P$ is the coefficient of probability of the system.


 ",Definition:Entropy (Physics),property,true,"Entropy is a property of a thermodynamic system.

It quantifies the number Ω of microstates that are consistent with the macroscopic quantities that characterize the system.


The entropy of a system is equal to the expectation of the value:
:k ln P
where:
:k is a constant which relates the mean kinetic energy and absolute temperature of the system
:P is the coefficient of probability of the system.


 ",Entropy
"['Definitions/Geometry', 'Definitions/Circles', 'Definitions/Epicycles']",Definition:Epicycle,"An epicycle is the orbit described by a body moving in a uniform circular motion around a point which is itself moving in a uniform circular motion around another point.

:

That point may itself also be moving in a uniform circular motion around yet another point.",Definition:Epicycle (Ptolemaic Astronomy),orbit,true,"An epicycle is the orbit described by a body moving in a uniform circular motion around a point which is itself moving in a uniform circular motion around another point.

:

That point may itself also be moving in a uniform circular motion around yet another point.",Epicycle
['Definitions/Epicycloids'],Definition:Epicycle,"Let an epicycloid be generated by rolling a circle $C_1$ around the outside of another circle $C_2$.


:


The circle $C_1$ can be referred to as the epicycle of the epicycloid.",Definition:Epicycloid/Generator/Epicycle,,false,"Let an epicycloid be generated by rolling a circle C_1 around the outside of another circle C_2.


:


The circle C_1 can be referred to as the epicycle of the epicycloid.",Epicycle
"['Definitions/Hypocycloids', 'Definitions/Epicycles']",Definition:Epicycle,"Let a hypocycloid be generated by rolling circle $C_1$ around the inside of another (larger) circle $C_2$.


:


The circle $C_1$ can be referred to as the epicycle of the hypocycloid.",Definition:Hypocycloid/Generator/Epicycle,,false,"Let a hypocycloid be generated by rolling circle C_1 around the inside of another (larger) circle C_2.


:


The circle C_1 can be referred to as the epicycle of the hypocycloid.",Epicycle
"['Definitions/Epimorphisms (Abstract Algebra)', 'Definitions/Homomorphisms (Abstract Algebra)', 'Definitions/Surjections', 'Definitions/Epimorphisms']",Definition:Epimorphism,"A homomorphism which is a surjection is an epimorphism.


=== Semigroup Epimorphism ===
Let $left( S, circ right)$ and $left( T, * right)$ be semigroups.

Let $phi: S to T$ be a (semigroup) homomorphism.


Then $phi$ is a semigroup epimorphism  if and only if  $phi$ is a surjection.

=== Monoid Epimorphism ===
Let $left( S, circ right)$ and $left( T, * right)$ be monoids.

Let $phi: S to T$ be a (monoid) homomorphism.


Then $phi$ is a monoid epimorphism  if and only if  $phi$ is a surjection.

=== Group Epimorphism ===
Let $left( G, circ right)$ and $left( H, * right)$ be groups.

Let $phi: G to H$ be a (group) homomorphism.


Then $phi$ is a group epimorphism  if and only if  $phi$ is a surjection.

=== Ring Epimorphism ===
Let $left( R, +, circ right)$ and $left( S, oplus, * right)$ be rings.

Let $phi: R to S$ be a (ring) homomorphism.


Then $phi$ is a ring epimorphism  if and only if  $phi$ is a surjection.

=== Field Epimorphism ===
Let $left( F, +, circ right)$ and $left( K, oplus, * right)$ be fields.

Let $phi: R to S$ be a (field) homomorphism.


Then $phi$ is a field epimorphism  if and only if  $phi$ is a surjection.

=== $R$-Algebraic Structure Epimorphism ===
Let $left( S, ast_1, ast_2, ldots, ast_n, circ right)_R$ and $left( T, odot_1, odot_2, ldots, odot_n, otimes right)_R$ be $R$-algebraic structures.


Then $phi: S to T$ is an $R$-algebraic structure epimorphism  if and only if :

:$(1): quad phi$ is a surjection
:$(2): quad forall k: k in left[ 1 ,.,.,   right]n: forall x, y in S: phi left(   right){x ast_k y} = phi left(   right)x odot_k phi left(   right)y$
:$(3): quad forall x in S: forall lambda in R: phi left(   right){lambda circ x} = lambda otimes phi left(   right)x$


This definition also applies to modules, and also to vector spaces.",Definition:Epimorphism (Abstract Algebra),,false,"A homomorphism which is a surjection is an epimorphism.


=== Semigroup Epimorphism ===
Let ( S, ∘) and ( T, * ) be semigroups.

Let ϕ: S → T be a (semigroup) homomorphism.


Then ϕ is a semigroup epimorphism  if and only if  ϕ is a surjection.

=== Monoid Epimorphism ===
Let ( S, ∘) and ( T, * ) be monoids.

Let ϕ: S → T be a (monoid) homomorphism.


Then ϕ is a monoid epimorphism  if and only if  ϕ is a surjection.

=== Group Epimorphism ===
Let ( G, ∘) and ( H, * ) be groups.

Let ϕ: G → H be a (group) homomorphism.


Then ϕ is a group epimorphism  if and only if  ϕ is a surjection.

=== Ring Epimorphism ===
Let ( R, +, ∘) and ( S, ⊕, * ) be rings.

Let ϕ: R → S be a (ring) homomorphism.


Then ϕ is a ring epimorphism  if and only if  ϕ is a surjection.

=== Field Epimorphism ===
Let ( F, +, ∘) and ( K, ⊕, * ) be fields.

Let ϕ: R → S be a (field) homomorphism.


Then ϕ is a field epimorphism  if and only if  ϕ is a surjection.

=== R-Algebraic Structure Epimorphism ===
Let ( S, ∗_1, ∗_2, …, ∗_n, ∘)_R and ( T, ⊙_1, ⊙_2, …, ⊙_n, ⊗)_R be R-algebraic structures.


Then ϕ: S → T is an R-algebraic structure epimorphism  if and only if :

:(1):   ϕ is a surjection
:(2):   ∀ k: k ∈[ 1  . . ]n: ∀ x, y ∈ S: ϕ(   )x ∗_k y = ϕ(   )x ⊙_k ϕ(   )y
:(3):   ∀ x ∈ S: ∀λ∈ R: ϕ(   )λ∘ x = λ⊗ϕ(   )x


This definition also applies to modules, and also to vector spaces.",Epimorphism
['Definitions/Category Theory'],Definition:Epimorphism,"Let $mathbf C$ be a metacategory.

An epimorphism is a morphism $f in mathbf C_1$ such that:

: $g circ f = h circ f implies g = h$

for all morphisms $g, h in mathbf C_1$ for which these compositions are defined.


That is, an epimorphism is a morphism which is right cancellable. 



One writes $f: C twoheadrightarrow D$ to denote that $f$ is an epimorphism.",Definition:Epimorphism (Category Theory),,false,"Let 𝐂 be a metacategory.

An epimorphism is a morphism f ∈𝐂_1 such that:

: g ∘ f = h ∘ f  g = h

for all morphisms g, h ∈𝐂_1 for which these compositions are defined.


That is, an epimorphism is a morphism which is right cancellable. 



One writes f: C ↠ D to denote that f is an epimorphism.",Epimorphism
"['Definitions/Geographical Equator', 'Definitions/Geographical Coordinates', 'Definitions/Equator']",Definition:Equator,"The (geographical) equator is the great circle described on the surface of Earth whose plane is perpendicular to Earth's axis of rotation.


:",Definition:Geographical Equator,,false,"The (geographical) equator is the great circle described on the surface of Earth whose plane is perpendicular to Earth's axis of rotation.


:",Equator
"['Definitions/Celestial Equator', 'Definitions/Celestial Sphere', 'Definitions/Equator']",Definition:Equator,"Consider the celestial sphere with observer $O$.

Let $P$ be the north celestial pole.


The great circle whose plane is perpendicular to $OP$ is known as the celestial equator.


:",Definition:Celestial Equator,,false,"Consider the celestial sphere with observer O.

Let P be the north celestial pole.


The great circle whose plane is perpendicular to OP is known as the celestial equator.


:",Equator
"['Definitions/Galactic Equator', 'Definitions/Galactic Coordinate System', 'Definitions/Spherical Astronomy', 'Definitions/Equator']",Definition:Equator,"The galactic equator is the great circle that is the intersection of the plane of the Milky Way with the celestial sphere.


=== Galactic Poles ===
The galactic poles are the two poles of the great circle that is the galactic equator.

=== Galactic Axis ===
The galactic axis is the axis of the great circle that is the galactic equator.",Definition:Galactic Equator,,false,"The galactic equator is the great circle that is the intersection of the plane of the Milky Way with the celestial sphere.


=== Galactic Poles ===
The galactic poles are the two poles of the great circle that is the galactic equator.

=== Galactic Axis ===
The galactic axis is the axis of the great circle that is the galactic equator.",Equator
"['Definitions/Equiangular Polygons', 'Definitions/Equiangular', 'Definitions/Polygons']",Definition:Equiangular,An equiangular polygon is a polygon in which all the vertices have the same angle.,Definition:Polygon/Equiangular,,false,An equiangular polygon is a polygon in which all the vertices have the same angle.,Equiangular
"['Definitions/Geometric Figures', 'Definitions/Equiangular']",Definition:Equiangular,Two geometric figures are equiangular (with each other) when the angles of each pair of their corresponding vertices are equal.,Definition:Equiangular Geometric Figures,,false,Two geometric figures are equiangular (with each other) when the angles of each pair of their corresponding vertices are equal.,Equiangular
"['Definitions/Logarithmic Spiral', 'Definitions/Spirals']",Definition:Equiangular,"The logarithmic spiral is the locus of the equation expressed in Polar coordinates as:
:$r = a e^{b theta}$


:",Definition:Logarithmic Spiral,,false,"The logarithmic spiral is the locus of the equation expressed in Polar coordinates as:
:r = a e^b θ


:",Equiangular
"['Definitions/Rectangular Hyperbolas', 'Definitions/Hyperbolas']",Definition:Equiangular,"A rectangular hyperbola is a hyperbola whose transverse axis is equal to its conjugate axis.


=== Standard Form ===
Let $K$ be a Rectangular hyperbola embedded in a cartesian plane.

$K$ is in standard form  if and only if :
:$(1)$ its major axis is aligned with the straight line $y = x$
:$(2)$ its minor axis is aligned with the straight line $y = -x$.


:",Definition:Rectangular Hyperbola,,false,"A rectangular hyperbola is a hyperbola whose transverse axis is equal to its conjugate axis.


=== Standard Form ===
Let K be a Rectangular hyperbola embedded in a cartesian plane.

K is in standard form  if and only if :
:(1) its major axis is aligned with the straight line y = x
:(2) its minor axis is aligned with the straight line y = -x.


:",Equiangular
"['Definitions/Conformal Transformations', 'Definitions/Analytic Geometry', 'Definitions/Mapping Theory']",Definition:Equiangular,"Let $T$ be a transformation of the plane.

Let $T$ have the property that:
:for all pairs of curves $mathcal C_1$ and $mathcal C_2$ which intersect at angle $theta$, the images of $mathcal C_1$ and $mathcal C_2$ under $T$ also intersect at angle $theta$.

Then $T$ is defined as being a conformal transformation.",Definition:Conformal Transformation,,false,"Let T be a transformation of the plane.

Let T have the property that:
:for all pairs of curves 𝒞_1 and 𝒞_2 which intersect at angle θ, the images of 𝒞_1 and 𝒞_2 under T also intersect at angle θ.

Then T is defined as being a conformal transformation.",Equiangular
['Definitions/Stochastic Processes'],Definition:Equilibrium,"Let $S$ be a stochastic process.

Suppose that the observations of the time series to which $S$ gives rise have a constant mean level.

Then $S$ is said to be in (statistical) equilibrium.",Definition:Statistical Equilibrium,,false,"Let S be a stochastic process.

Suppose that the observations of the time series to which S gives rise have a constant mean level.

Then S is said to be in (statistical) equilibrium.",Equilibrium
"['Definitions/Equilibrium (Mechanics)', 'Definitions/Mechanics', 'Definitions/Physics', 'Definitions/Equilibrium']",Definition:Equilibrium,"Let $B$ be a particle, a system of particles, or a body.

Let $B$ be such that:
:it is subject to neither acceleration nor angular acceleration
:the resultant of the external forces acting on $S$ is zero
:the sum of all the moments of all the external forces acting on $S$ is also zero.

Then $B$ is said to be in equilibrium.


=== Stable Equilibrium ===
Let $B$ be a particle, a system of particles, or a body which is in equilibrium.

Let $B$ be such that if a sufficiently small displacement is applied, then it returns to its original position.


$B$ is then said to be in stable equilibrium.

=== Unstable Equilibrium ===
Let $B$ be a particle, a system of particles, or a body which is in equilibrium.

Let $B$ be such that if a displacement is applied, however small, $B$ moves to a position different from its original position.


$B$ is then said to be in unstable equilibrium.

=== Neutral Equilibrium ===
Let $B$ be a particle, a system of particles, or a body which is in equilibrium.

Let $B$ be such that if a small displacement is applied, $B$ remains in its new position.


$B$ is then said to be in neutral equilibrium.",Definition:Equilibrium (Mechanics),,false,"Let B be a particle, a system of particles, or a body.

Let B be such that:
:it is subject to neither acceleration nor angular acceleration
:the resultant of the external forces acting on S is zero
:the sum of all the moments of all the external forces acting on S is also zero.

Then B is said to be in equilibrium.


=== Stable Equilibrium ===
Let B be a particle, a system of particles, or a body which is in equilibrium.

Let B be such that if a sufficiently small displacement is applied, then it returns to its original position.


B is then said to be in stable equilibrium.

=== Unstable Equilibrium ===
Let B be a particle, a system of particles, or a body which is in equilibrium.

Let B be such that if a displacement is applied, however small, B moves to a position different from its original position.


B is then said to be in unstable equilibrium.

=== Neutral Equilibrium ===
Let B be a particle, a system of particles, or a body which is in equilibrium.

Let B be such that if a small displacement is applied, B remains in its new position.


B is then said to be in neutral equilibrium.",Equilibrium
"['Definitions/Stable Equilibrium', 'Definitions/Equilibrium (Mechanics)']",Definition:Equilibrium,"Let $B$ be a particle, a system of particles, or a body which is in equilibrium.

Let $B$ be such that if a sufficiently small displacement is applied, then it returns to its original position.


$B$ is then said to be in stable equilibrium.",Definition:Equilibrium (Mechanics)/Stable,,false,"Let B be a particle, a system of particles, or a body which is in equilibrium.

Let B be such that if a sufficiently small displacement is applied, then it returns to its original position.


B is then said to be in stable equilibrium.",Equilibrium
"['Definitions/Unstable Equilibrium', 'Definitions/Equilibrium (Mechanics)']",Definition:Equilibrium,"Let $B$ be a particle, a system of particles, or a body which is in equilibrium.

Let $B$ be such that if a displacement is applied, however small, $B$ moves to a position different from its original position.


$B$ is then said to be in unstable equilibrium.",Definition:Equilibrium (Mechanics)/Unstable,,false,"Let B be a particle, a system of particles, or a body which is in equilibrium.

Let B be such that if a displacement is applied, however small, B moves to a position different from its original position.


B is then said to be in unstable equilibrium.",Equilibrium
"['Definitions/Neutral Equilibrium', 'Definitions/Equilibrium (Mechanics)']",Definition:Equilibrium,"Let $B$ be a particle, a system of particles, or a body which is in equilibrium.

Let $B$ be such that if a small displacement is applied, $B$ remains in its new position.


$B$ is then said to be in neutral equilibrium.",Definition:Equilibrium (Mechanics)/Neutral,,false,"Let B be a particle, a system of particles, or a body which is in equilibrium.

Let B be such that if a small displacement is applied, B remains in its new position.


B is then said to be in neutral equilibrium.",Equilibrium
['Definitions/Mechanics'],Definition:Equilibrium,"The equilibrium position of a body $B$ attached to a spring $S$ is the position it occupies when $S$ is exerting no force upon $B$.

For an ideal spring obeying Hooke's Law $mathbf F = -k mathbf x$, the equilibrium position is set to be the point $mathbf x = boldsymbol 0$.",Definition:Spring/Equilibrium Position,,false,"The equilibrium position of a body B attached to a spring S is the position it occupies when S is exerting no force upon B.

For an ideal spring obeying Hooke's Law 𝐅 = -k 𝐱, the equilibrium position is set to be the point 𝐱 =  0.",Equilibrium
['Definitions/Game Theory'],Definition:Equilibrium,An equilibrium point is a stable outcome of a game associated with a particular set of strategies.,Definition:Equilibrium Point,,false,An equilibrium point is a stable outcome of a game associated with a particular set of strategies.,Equilibrium
"['Definitions/Equilibrium Strategies', 'Definitions/Strategies']",Definition:Equilibrium,"A system of strategies, one for each player, is in equilibrium  if and only if  they result in an equilibrium point.

Such strategies are known as equilibrium strategies.


In the singular, an equilibrium strategy is one that contributes to an equilibrium point.",Definition:Equilibrium Strategy,,false,"A system of strategies, one for each player, is in equilibrium  if and only if  they result in an equilibrium point.

Such strategies are known as equilibrium strategies.


In the singular, an equilibrium strategy is one that contributes to an equilibrium point.",Equilibrium
['Definitions/Game Theory'],Definition:Equilibrium,"Let a strategic game $G$ be modelled by:
:$G = leftlangle N, leftlangle   rightrangle, leftlangle A_i rightrangle rightrangle {succsim_i}$


A Nash equilibrium of $G$ is a profile $a^* in A$ of moves which has the property that:
:$forall i in N: forall a_i in A_i: left( a^*_{-i}, a^*_i right) succsim_i left( a^*_{-i}, a_i right)$


Thus, for $a^*$ to be a Nash equilibrium, no player $i$ has a move yielding a preferable outcome to that when $a^*_i$ is chosen, given that every other player $j$ has chosen his own equilibrium move.

That is, no player can profitably deviate, if no other player also deviates.

 ",Definition:Nash Equilibrium,,false,"Let a strategic game G be modelled by:
:G = ⟨ N, ⟨⟩, ⟨ A_i ⟩⟩≿_i


A Nash equilibrium of G is a profile a^* ∈ A of moves which has the property that:
:∀ i ∈ N: ∀ a_i ∈ A_i: ( a^*_-i, a^*_i ) ≿_i ( a^*_-i, a_i )


Thus, for a^* to be a Nash equilibrium, no player i has a move yielding a preferable outcome to that when a^*_i is chosen, given that every other player j has chosen his own equilibrium move.

That is, no player can profitably deviate, if no other player also deviates.

 ",Equilibrium
"['Definitions/Logical Equivalence', 'Definitions/Logic']",Definition:Equivalence,"If two statements $p$ and $q$ are such that:

:$p vdash q$, that is: $p$ therefore $q$
:$q vdash p$, that is: $q$ therefore $p$

then $p$ and $q$ are said to be (logically) equivalent.


That is:
:$p dashv vdash q$
means:
:$p vdash q$ and $q vdash p$


Note that because the conclusion of an argument is a single statement, there can be only one statement on either side of the $dashv vdash$ sign.


In symbolic logic, the notion of logical equivalence occurs in the form of provable equivalence and semantic equivalence.


=== Provable Equivalence ===
Let $mathscr P$ be a proof system for a formal language $mathcal L$.

Let $phi, psi$ be $mathcal L$-WFFs.


Then $phi$ and $psi$ are $mathscr P$-provably equivalent  if and only if :

:$phi vdash_{mathscr P} psi$ and $psi vdash_{mathscr P} phi$

that is,  if and only if  they are $mathscr P$-provable consequences of one another.


The provable equivalence of $phi$ and $psi$ can be denoted by:

:$phi dashv vdash_{mathscr P} psi$

=== Semantic Equivalence ===
Let $mathscr M$ be a formal semantics for a formal language $mathcal L$.

Let $phi, psi$ be $mathcal L$-WFFs.


Then $phi$ and $psi$ are $mathscr M$-semantically equivalent  if and only if :

:$phi models_{mathscr M} psi$ and $psi models_{mathscr M} phi$

that is,  if and only if  they are $mathscr M$-semantic consequences of one another.


Equivalently, $phi$ and $psi$ are $mathscr M$-semantically equivalent  if and only if , for each $mathscr M$-structure $mathcal M$:

:$mathcal M models_{mathscr M} phi$  if and only if  $mathcal M models_{mathscr M} psi$


 

 ",Definition:Logical Equivalence,,false,"If two statements p and q are such that:

:p ⊢ q, that is: p therefore q
:q ⊢ p, that is: q therefore p

then p and q are said to be (logically) equivalent.


That is:
:p ⊣⊢ q
means:
:p ⊢ q and q ⊢ p


Note that because the conclusion of an argument is a single statement, there can be only one statement on either side of the ⊣⊢ sign.


In symbolic logic, the notion of logical equivalence occurs in the form of provable equivalence and semantic equivalence.


=== Provable Equivalence ===
Let 𝒫 be a proof system for a formal language ℒ.

Let ϕ, ψ be ℒ-WFFs.


Then ϕ and ψ are 𝒫-provably equivalent  if and only if :

:ϕ⊢_𝒫ψ and ψ⊢_𝒫ϕ

that is,  if and only if  they are 𝒫-provable consequences of one another.


The provable equivalence of ϕ and ψ can be denoted by:

:ϕ⊣⊢_𝒫ψ

=== Semantic Equivalence ===
Let ℳ be a formal semantics for a formal language ℒ.

Let ϕ, ψ be ℒ-WFFs.


Then ϕ and ψ are ℳ-semantically equivalent  if and only if :

:ϕ_ℳψ and ψ_ℳϕ

that is,  if and only if  they are ℳ-semantic consequences of one another.


Equivalently, ϕ and ψ are ℳ-semantically equivalent  if and only if , for each ℳ-structure ℳ:

:ℳ_ℳϕ  if and only if  ℳ_ℳψ


 

 ",Equivalence
['Definitions/Formal Semantics'],Definition:Equivalence,"Let $mathscr M$ be a formal semantics for a formal language $mathcal L$.

Let $phi, psi$ be $mathcal L$-WFFs.


Then $phi$ and $psi$ are $mathscr M$-semantically equivalent  if and only if :

:$phi models_{mathscr M} psi$ and $psi models_{mathscr M} phi$

that is,  if and only if  they are $mathscr M$-semantic consequences of one another.


Equivalently, $phi$ and $psi$ are $mathscr M$-semantically equivalent  if and only if , for each $mathscr M$-structure $mathcal M$:

:$mathcal M models_{mathscr M} phi$  if and only if  $mathcal M models_{mathscr M} psi$


 

 ",Definition:Semantic Equivalence,,false,"Let ℳ be a formal semantics for a formal language ℒ.

Let ϕ, ψ be ℒ-WFFs.


Then ϕ and ψ are ℳ-semantically equivalent  if and only if :

:ϕ_ℳψ and ψ_ℳϕ

that is,  if and only if  they are ℳ-semantic consequences of one another.


Equivalently, ϕ and ψ are ℳ-semantically equivalent  if and only if , for each ℳ-structure ℳ:

:ℳ_ℳϕ  if and only if  ℳ_ℳψ


 

 ",Equivalence
['Definitions/Proof Systems'],Definition:Equivalence,"Let $mathscr P$ be a proof system for a formal language $mathcal L$.

Let $phi, psi$ be $mathcal L$-WFFs.


Then $phi$ and $psi$ are $mathscr P$-provably equivalent  if and only if :

:$phi vdash_{mathscr P} psi$ and $psi vdash_{mathscr P} phi$

that is,  if and only if  they are $mathscr P$-provable consequences of one another.


The provable equivalence of $phi$ and $psi$ can be denoted by:

:$phi dashv vdash_{mathscr P} psi$",Definition:Provable Equivalence,,false,"Let 𝒫 be a proof system for a formal language ℒ.

Let ϕ, ψ be ℒ-WFFs.


Then ϕ and ψ are 𝒫-provably equivalent  if and only if :

:ϕ⊢_𝒫ψ and ψ⊢_𝒫ϕ

that is,  if and only if  they are 𝒫-provable consequences of one another.


The provable equivalence of ϕ and ψ can be denoted by:

:ϕ⊣⊢_𝒫ψ",Equivalence
"['Definitions/Set Equivalence', 'Definitions/Set Theory']",Definition:Equivalence,"Let $S$ and $T$ be sets.

Then $S$ and $T$ are equivalent  if and only if :
:there exists a bijection $f: S to T$ between the elements of $S$ and those of $T$.

That is,  if and only if  they have the same cardinality.


This can be written $S sim T$.


If $S$ and $T$ are not equivalent we write $S nsim T$.",Definition:Set Equivalence,,false,"Let S and T be sets.

Then S and T are equivalent  if and only if :
:there exists a bijection f: S → T between the elements of S and those of T.

That is,  if and only if  they have the same cardinality.


This can be written S ∼ T.


If S and T are not equivalent we write S  T.",Equivalence
"['Definitions/Matrix Theory', 'Definitions/Elementary Row Operations']",Definition:Equivalence,"Two matrices $mathbf A = left[ a right]_{m n}, mathbf B = left[ b right]_{m n}$ are row equivalent if one can be obtained from the other by a finite sequence of elementary row operations.

This relationship can be denoted $mathbf A sim mathbf B$.",Definition:Row Equivalence,,false,"Two matrices 𝐀 = [ a ]_m n, 𝐁 = [ b ]_m n are row equivalent if one can be obtained from the other by a finite sequence of elementary row operations.

This relationship can be denoted 𝐀∼𝐁.",Equivalence
['Definitions/Equivalence Relations'],Definition:Equivalence,"Let $mathcal R$ be a relation on a set $S$.


=== Definition 1 ===
Let $mathcal R$ be a relation on a set $S$.


Let $mathcal R$ be:

:$(1): quad$ reflexive
:$(2): quad$ symmetric
:$(3): quad$ transitive

Then $mathcal R$ is an equivalence relation on $S$.

=== Definition 2 ===
Let $mathcal R subseteq S times S$ be a relation on a set $S$.


$mathcal R$ is an equivalence relation  if and only if :

:$Delta_S cup mathcal R^{-1} cup mathcal R circ mathcal R subseteq mathcal R$

where:
:$Delta_S$ denotes the diagonal relation on $S$
:$mathcal R^{-1}$ denotes the inverse relation
:$circ$ denotes composition of relations",Definition:Equivalence Relation,,false,"Let ℛ be a relation on a set S.


=== Definition 1 ===
Let ℛ be a relation on a set S.


Let ℛ be:

:(1): reflexive
:(2): symmetric
:(3): transitive

Then ℛ is an equivalence relation on S.

=== Definition 2 ===
Let ℛ⊆ S × S be a relation on a set S.


ℛ is an equivalence relation  if and only if :

:Δ_S ∪ℛ^-1∪ℛ∘ℛ⊆ℛ

where:
:Δ_S denotes the diagonal relation on S
:ℛ^-1 denotes the inverse relation
:∘ denotes composition of relations",Equivalence
"['Definitions/Euclidean Geometry', 'Definitions/Geometry', 'Definitions/Pure Mathematics', 'Definitions/Branches of Mathematics']",Definition:Euclidean,"Euclidean geometry is the branch of geometry in which the parallel postulate applies.

An assumption which is currently under question is whether or not ordinary space is itself Euclidean.


Euclidean geometry adheres to Euclid's postulates.",Definition:Euclidean Geometry,geometry,true,"Euclidean geometry is the branch of geometry in which the parallel postulate applies.

An assumption which is currently under question is whether or not ordinary space is itself Euclidean.


Euclidean geometry adheres to Euclid's postulates.",Euclidean
"['Definitions/Euclidean Space', 'Definitions/Linear Algebra', 'Definitions/Examples of Topologies', 'Definitions/Geometry']",Definition:Euclidean,"Let $S$ be one of the standard number fields $mathbb Q$, $mathbb R$, $mathbb C$.

Let $S^n$ be a cartesian space for $n in mathbb N_{ge 1}$.

Let $d: S^n times S^n to mathbb R$ be the usual (Euclidean) metric on $S^n$.

Then $left( S^n, d right)$ is a Euclidean space.",Definition:Euclidean Space,,false,"Let S be one of the standard number fields ℚ, ℝ, ℂ.

Let S^n be a cartesian space for n ∈ℕ_≥ 1.

Let d: S^n × S^n →ℝ be the usual (Euclidean) metric on S^n.

Then ( S^n, d ) is a Euclidean space.",Euclidean
"['Definitions/Euclidean Space', 'Definitions/Linear Algebra', 'Definitions/Geometry']",Definition:Euclidean,"Let $S$ be one of the standard number fields $mathbb Q$, $mathbb R$, $mathbb C$.

Let $S^n$ be a cartesian space for $n in mathbb N_{ge 1}$.

Let $M = left( S^n, d right)$ be a Euclidean space.


The topology $tau_d$ induced by the Euclidean metric $d$ is called the Euclidean topology.",Definition:Euclidean Space/Euclidean Topology,,false,"Let S be one of the standard number fields ℚ, ℝ, ℂ.

Let S^n be a cartesian space for n ∈ℕ_≥ 1.

Let M = ( S^n, d ) be a Euclidean space.


The topology τ_d induced by the Euclidean metric d is called the Euclidean topology.",Euclidean
"['Definitions/Euclidean Metric', 'Definitions/Examples of Metric Spaces']",Definition:Euclidean,"Let $M_{1'} = left( A_{1'}, d_{1'}  right)$ and $M_{2'} = left( A_{2'}, d_{2'}  right)$ be metric spaces.

Let $A_{1'} times A_{2'}$ be the cartesian product of $A_{1'}$ and $A_{2'}$.


The Euclidean metric on $A_{1'} times A_{2'}$ is defined as:

:$d_2 left(   right){x, y} := left( left( d_{1'}  left(   right){x_1, y_1}  right)^2 + left( d_{2'}  left(   right){x_2, y_2}  right)^2 right)^{1/2}$

where $x = left( x_1, x_2 right), y = left( y_1, y_2 right) in A_{1'} times A_{2'}$.


=== General Definition ===
Let $M_{1'} = left( A_{1'}, d_{1'}  right), M_{2'} = left( A_{2'}, d_{2'}  right), ldots, M_{n'} = left( A_{n'}, d_{n'}  right)$ be metric spaces.

Let $ds mathcal A = prod_{i mathop = 1}^n A_{i'}$ be the cartesian product of $A_{1'}, A_{2'}, ldots, A_{n'}$.


The Euclidean metric on $ds mathcal A = prod_{i mathop = 1}^n A_{i'}$ is defined as:

:$ds d_2 left(   right){x, y} := left( sum_{i mathop = 1}^n left( d_{i'}  left(   right){x_i, y_i}  right)^2 right)^{frac 1 2}$

where $x = left( x_1, x_2, ldots, x_n right), y = left( y_1, y_2, ldots, y_n right) in mathcal A$.

=== Riemannian Manifold ===
Let $x in mathbb R^n$ be a point.

Let $left( x_1, ldots, x_n right)$ be the standard coordinates.

Let $T_x mathbb R^n$ be the tangent space of $mathbb R^n$ at $x$.

Let $T_x mathbb R^n$ be identified with $mathbb R^n$:

:$T_x mathbb R^n cong mathbb R^n$
 

Let $v, w in T_x mathbb R^n$ be vectors such that:

:$ds v = sum_{i mathop = 1}^n v^i left. partial_i rightrvert_{ }x$

:$ds w = sum_{i mathop = 1}^n w^i left. partial_i rightrvert_{ }x$

Let $g$ be a Riemannian metric such that:

:$ds g_x = leftlangle v,   rightrangle w_x = sum_{i mathop = 1}^n v^i w^i$


Then $g$ is called the Euclidean metric.

 ",Definition:Euclidean Metric,,false,"Let M_1' = ( A_1', d_1') and M_2' = ( A_2', d_2') be metric spaces.

Let A_1'× A_2' be the cartesian product of A_1' and A_2'.


The Euclidean metric on A_1'× A_2' is defined as:

:d_2 (   )x, y := ( ( d_1'(   )x_1, y_1)^2 + ( d_2'(   )x_2, y_2)^2 )^1/2

where x = ( x_1, x_2 ), y = ( y_1, y_2 ) ∈ A_1'× A_2'.


=== General Definition ===
Let M_1' = ( A_1', d_1'), M_2' = ( A_2', d_2'), …, M_n' = ( A_n', d_n') be metric spaces.

Let 𝒜 = ∏_i  = 1^n A_i' be the cartesian product of A_1', A_2', …, A_n'.


The Euclidean metric on 𝒜 = ∏_i  = 1^n A_i' is defined as:

:d_2 (   )x, y := ( ∑_i  = 1^n ( d_i'(   )x_i, y_i)^2 )^1/2

where x = ( x_1, x_2, …, x_n ), y = ( y_1, y_2, …, y_n ) ∈𝒜.

=== Riemannian Manifold ===
Let x ∈ℝ^n be a point.

Let ( x_1, …, x_n ) be the standard coordinates.

Let T_x ℝ^n be the tangent space of ℝ^n at x.

Let T_x ℝ^n be identified with ℝ^n:

:T_x ℝ^n ≅ℝ^n
 

Let v, w ∈ T_x ℝ^n be vectors such that:

:v = ∑_i  = 1^n v^i . ∂_i |_x

:w = ∑_i  = 1^n w^i . ∂_i |_x

Let g be a Riemannian metric such that:

:g_x = ⟨ v,   ⟩ w_x = ∑_i  = 1^n v^i w^i


Then g is called the Euclidean metric.

 ",Euclidean
"['Definitions/Euclidean Norms', 'Definitions/Normed Spaces', 'Definitions/Euclidean Space']",Definition:Euclidean,"Let $mathbf v = left( v_1, v_2, ldots, v_n right)$ be a vector in the real Euclidean $n$-space $mathbb R^n$.


The Euclidean norm of $mathbf v$ is defined as:
:$ds leftlVert mathbf v rightrVert = left( sum_{k mathop = 1}^n v_k^2 right)^{1/2}$",Definition:Euclidean Norm,,false,"Let 𝐯 = ( v_1, v_2, …, v_n ) be a vector in the real Euclidean n-space ℝ^n.


The Euclidean norm of 𝐯 is defined as:
:‖𝐯‖ = ( ∑_k  = 1^n v_k^2 )^1/2",Euclidean
"['Definitions/Euclidean Domains', 'Definitions/Integral Domains']",Definition:Euclidean,"Let $left( D, +, circ right)$ be an integral domain.

Let there exist a Euclidean valuation on $D$.

Then $D$ is called a Euclidean domain.


=== Euclidean Valuation ===
Let $left( D, +, circ right)$ be an integral domain with zero $0_D$.

Let there exist a mapping $nu: D setminus leftlbrace 0_D rightrbrace to mathbb N$ such that for all $a in D, b in D_{ne 0_D}$:

 
 
 
 

Then $nu$ is a Euclidean valuation on $D$.",Definition:Euclidean Domain,,false,"Let ( D, +, ∘) be an integral domain.

Let there exist a Euclidean valuation on D.

Then D is called a Euclidean domain.


=== Euclidean Valuation ===
Let ( D, +, ∘) be an integral domain with zero 0_D.

Let there exist a mapping ν: D ∖{ 0_D }→ℕ such that for all a ∈ D, b ∈ D_ 0_D:

 
 
 
 

Then ν is a Euclidean valuation on D.",Euclidean
"['Definitions/Euclidean Relations', 'Definitions/Relation Theory']",Definition:Euclidean,"Let $mathcal R subseteq S times S$ be a relation in $S$.


=== Left-Euclidean ===
Let $mathcal R subseteq S times S$ be a relation in $S$.


$mathcal R$ is left-Euclidean  if and only if :

:$left( x, z right) in mathcal R land left( y, z right) in mathcal R implies left( x, y right) in mathcal R$

=== Right-Euclidean ===
Let $mathcal R subseteq S times S$ be a relation in $S$.


$mathcal R$ is right-Euclidean  if and only if :

:$left( x, y right) in mathcal R land left( x, z right) in mathcal R implies left( y, z right) in mathcal R$

=== Euclidean ===

$mathcal R$ is Euclidean  if and only if  it is both left-Euclidean and right-Euclidean.


",Definition:Euclidean Relation,,false,"Let ℛ⊆ S × S be a relation in S.


=== Left-Euclidean ===
Let ℛ⊆ S × S be a relation in S.


ℛ is left-Euclidean  if and only if :

:( x, z ) ∈ℛ( y, z ) ∈ℛ( x, y ) ∈ℛ

=== Right-Euclidean ===
Let ℛ⊆ S × S be a relation in S.


ℛ is right-Euclidean  if and only if :

:( x, y ) ∈ℛ( x, z ) ∈ℛ( y, z ) ∈ℛ

=== Euclidean ===

ℛ is Euclidean  if and only if  it is both left-Euclidean and right-Euclidean.


",Euclidean
"['Definitions/Euler Characteristic of Finite Graph', 'Definitions/Euler Characteristic', 'Definitions/Graph Theory']",Definition:Euler Characteristic,"Let $G = left( V, E right)$ be a finite graph.

Let $G$ be embedded in a surface.


The Euler characteristic of $G$ is written $chi left(   right)G$ and is defined as:
:$chi left(   right)G = v - e + f$
where:
:$v = leftlvert V rightrvert$ is the number of vertices
:$e = leftlvert E rightrvert$ is the number of edges
:$f$ is the number of faces.",Definition:Euler Characteristic of Finite Graph,,false,"Let G = ( V, E ) be a finite graph.

Let G be embedded in a surface.


The Euler characteristic of G is written χ(   )G and is defined as:
:χ(   )G = v - e + f
where:
:v = | V | is the number of vertices
:e = | E | is the number of edges
:f is the number of faces.",Euler Characteristic
"['Definitions/Euler Characteristic of Surface', 'Definitions/Euler Characteristic', 'Definitions/Graph Theory']",Definition:Euler Characteristic,"Let $S$ be a surface.

Let $T$ be a triangulation of $S$.

The Euler characteristic of $S$ is written $chi left(   right)S$ and is defined as:
:$chi left(   right)S = v - e + f$
where:
:$v = leftlvert V rightrvert$ is the number of vertices of $T$
:$e = leftlvert E rightrvert$ is the number of edges of $T$
:$f$ is the number of faces of $T$.",Definition:Euler Characteristic of Surface,,false,"Let S be a surface.

Let T be a triangulation of S.

The Euler characteristic of S is written χ(   )S and is defined as:
:χ(   )S = v - e + f
where:
:v = | V | is the number of vertices of T
:e = | E | is the number of edges of T
:f is the number of faces of T.",Euler Characteristic
['Definitions/Topology'],Definition:Expansion,"Let $S$ be a set.

Let $tau_1$ and $tau_2$ be topologies on $S$ such that $tau_1 subseteq tau_2$.


Then $tau_2$ is an expansion of $tau_1$.",Definition:Expansion of Topology,,false,"Let S be a set.

Let τ_1 and τ_2 be topologies on S such that τ_1 ⊆τ_2.


Then τ_2 is an expansion of τ_1.",Expansion
"['Definitions/Decimal Expansions', 'Definitions/Decimal Notation', 'Definitions/Basis Expansions', 'Definitions/Decimal']",Definition:Expansion,"Let $x in mathbb R$ be a real number.

The decimal expansion of $x$ is the expansion of $x$ in base $10$.


$x = leftlfloor x rightrfloor + ds sum_{j mathop ge 1} frac {d_j} {10^j}$:
:$left[ s cdotp d_1 d_2 d_3 ldots right]_{10}$
where:
:$s = leftlfloor x rightrfloor$, the floor of $x$
:it is not the case that there exists $m in mathbb N$ such that $d_M = 9$ for all $M ge m$.
(That is, the sequence of digits does not end with an infinite sequence of $9$s.)


=== Decimal Point ===
Let $x in mathbb R$ have a decimal expansion:
:$n. d_1 d_2 d_3 ldots$


The dot that separates the integer part from the fractional part of $x$ is called the decimal point.

That is, it is the radix point when used specifically for a base $10$ representation.

=== Size Less than 1 ===
A number $x$ such that $leftlvert x rightrvert < 0$ has a units digit which is zero.

Such a number may be expressed either with or without the zero, for example:
:$0 cdotp 568$
or:
:$cdotp 568$

While both are commonplace, the form with the zero is less prone to the mistake where decimal point is missed when reading it.

=== Decimal Place ===
Let $x in mathbb R$ be a real number.

Let the decimal expansion of $x$ be:

:$x = left[ s cdotp d_1 d_2 d_3 ldots right]_{10}$

Then $d_k$ is defined as being the digit in the $k$th decimal place.",Definition:Decimal Expansion,,false,"Let x ∈ℝ be a real number.

The decimal expansion of x is the expansion of x in base 10.


x = ⌊ x ⌋ + ∑_j ≥ 1d_j/10^j:
:[ s  d_1 d_2 d_3 …]_10
where:
:s = ⌊ x ⌋, the floor of x
:it is not the case that there exists m ∈ℕ such that d_M = 9 for all M ≥ m.
(That is, the sequence of digits does not end with an infinite sequence of 9s.)


=== Decimal Point ===
Let x ∈ℝ have a decimal expansion:
:n. d_1 d_2 d_3 …


The dot that separates the integer part from the fractional part of x is called the decimal point.

That is, it is the radix point when used specifically for a base 10 representation.

=== Size Less than 1 ===
A number x such that | x | < 0 has a units digit which is zero.

Such a number may be expressed either with or without the zero, for example:
:0  568
or:
:568

While both are commonplace, the form with the zero is less prone to the mistake where decimal point is missed when reading it.

=== Decimal Place ===
Let x ∈ℝ be a real number.

Let the decimal expansion of x be:

:x = [ s  d_1 d_2 d_3 …]_10

Then d_k is defined as being the digit in the kth decimal place.",Expansion
"['Definitions/Number Bases', 'Definitions/Real Numbers', 'Definitions/Basis Expansions']",Definition:Expansion,"=== Positive Real Numbers ===
Let $x in mathbb R$ be a real number such that $x ge 0$.

Let $b in mathbb N: b ge 2$.


Let us define the recursive sequence:
:$forall n in mathbb N: n ge 1: leftlangle f_n rightrangle = begin {cases}
b left( x - leftlfloor x rightrfloor right) & : n = 1 \
b left( f_{n - 1} - leftlfloor f_{n - 1}  rightrfloor  right) & : n > 1
end{cases}$

Then we define:
:$forall n in mathbb N: n ge 1: leftlangle d_n rightrangle = leftlfloor f_n rightrfloor$


It follows from the method of construction and the definition of the floor function that:

:$forall n: 0 le f_n < b$ and hence $forall n: 0 le d_n le b - 1$
:$forall n: f_n = 0 implies f_{n + 1} = 0$ and hence $d_{n + 1} = 0$.


Hence we can express $x = leftlfloor x rightrfloor + displaystyle sum_{j mathop ge 1} frac {d_j} {b^j}$ as:
:$left[ s cdotp d_1 d_2 d_3 ldots right]_b$
where:
:$s = leftlfloor x rightrfloor$
:it is not the case that there exists $m in mathbb N$ such that $d_M = b - 1$ for all $M ge m$.
(That is, the sequence of digits does not end with an infinite sequence of $b - 1$.)


This is called the expansion of $x$ in base $b$.

The generic term for such an expansion is a basis expansion.


It follows from the Division Theorem that for a given $b$ and $x$ this expansion is unique.


=== Termination ===
Let $b in mathbb N: b ge 2$.

Let the basis expansion of $x$ in base $b$ be:

:$left[ s cdotp d_1 d_2 d_3 ldots right]_b$


Let it be the case that:
:$exists m in mathbb N: forall k ge m: d_k = 0$

That is, every digit of $x$ in base $b$ after a certain point is zero.

Then $x$ is said to terminate.

=== Recurrence ===
Let $b in mathbb N: b ge 2$.

Let $x$ be a real number.

Let the basis expansion of $x$ in base $b$ be:

:$left[ s cdotp d_1 d_2 d_3 ldots right]_b$

Let there be a finite sequence of $p$ digits of $x$:
:$left( d_{r + 1} d_{r + 1} ldots d_{r + p}  right)$
such that for all $k in mathbb Z_{ge 0}$ and for all $j in leftlbrace 1, 2, ldots, p rightrbrace$:
:$d_{r + j + k p} = d_{r + j}$

where $p$ is the smallest $p$ to have this property.

That is, let $x$ be of the form:

:$left[ s cdotp d_1 d_2 d_3 ldots d_r d_{r + 1} d_{r + 2} ldots d_{r + p} d_{r + 1} d_{r + 2} ldots d_{r + p} d_{r + 1} d_{r + 2} ldots d_{r + p} d_{r + 1} ldots right]_b$


That is, $left( d_{r + 1} d_{r + 2} ldots d_{r + p}  right)$ repeats from then on, or recurs.

Then $x$ is said to recur.


=== Non-Recurring Part ===
Let $b in mathbb N: b ge 2$.

Let $x$ be a real number.

Let the basis expansion of $x$ in base $b$ be recurring:

:$left[ s cdotp d_1 d_2 d_3 ldots d_r d_{r + 1} d_{r + 2} ldots d_{r + p} d_{r + 1} d_{r + 2} ldots d_{r + p } d_{r + 1} d_{r + 2} ldots d_{r + p} d_{r + 1} ldots right]_b$


The non-recurring part of $x$  is:
:$left[ s cdotp d_1 d_2 d_3 ldots d_r right]$

=== Recurring Part ===
Let $b in mathbb N: b ge 2$.

Let $x$ be a real number.

Let the basis expansion of $x$ in base $b$ be recurring:

:$left[ s cdotp d_1 d_2 d_3 ldots d_r d_{r + 1} d_{r + 2} ldots d_{r + p} d_{r + 1} d_{r + 2} ldots d_{r + p } d_{r + 1} d_{r + 2} ldots d_{r + p} d_{r + 1} ldots right]_b$


The recurring part of $x$  is:
:$left[ d_{r + 1} d_{r + 2} ldots d_{r + p} right]$


=== Period ===
Let $b in mathbb N: b ge 2$.

Let $x$ be a rational number.

Let the basis expansion of $x$ in base $b$ be:

:$left[ s cdotp d_1 d_2 d_3 ldots right]_b$

From Basis Expansion of Rational Number, $x$ either terminates or recurs.


The period of recurrence is the number of digits in the recurring part after which it repeats itself.


Category:Definitions/Basis Expansions

=== Period ===
Let $b in mathbb N: b ge 2$.

Let $x$ be a rational number.

Let the basis expansion of $x$ in base $b$ be:

:$left[ s cdotp d_1 d_2 d_3 ldots right]_b$

From Basis Expansion of Rational Number, $x$ either terminates or recurs.


The period of recurrence is the number of digits in the recurring part after which it repeats itself.


Category:Definitions/Basis Expansions

=== Negative Real Numbers ===
Let $x in mathbb R: x < 0$.

We take the absolute value $y$ of $x$, that is:
:$y = leftlvert x rightrvert$

Then we take the expansion of $y$ in base $b$:
:$leftlvert s . d_1 d_2 d_3 ldots rightrvert_b$
where $s = leftlfloor y rightrfloor$.

Finally, the expansion of $x$ in base $b$ is defined as:
:$-left[ s . d_1 d_2 d_3 ldots right]_b$


=== Termination ===
Let $b in mathbb N: b ge 2$.

Let the basis expansion of $x$ in base $b$ be:

:$left[ s cdotp d_1 d_2 d_3 ldots right]_b$


Let it be the case that:
:$exists m in mathbb N: forall k ge m: d_k = 0$

That is, every digit of $x$ in base $b$ after a certain point is zero.

Then $x$ is said to terminate.

=== Recurrence ===
Let $b in mathbb N: b ge 2$.

Let $x$ be a real number.

Let the basis expansion of $x$ in base $b$ be:

:$left[ s cdotp d_1 d_2 d_3 ldots right]_b$

Let there be a finite sequence of $p$ digits of $x$:
:$left( d_{r + 1} d_{r + 1} ldots d_{r + p}  right)$
such that for all $k in mathbb Z_{ge 0}$ and for all $j in leftlbrace 1, 2, ldots, p rightrbrace$:
:$d_{r + j + k p} = d_{r + j}$

where $p$ is the smallest $p$ to have this property.

That is, let $x$ be of the form:

:$left[ s cdotp d_1 d_2 d_3 ldots d_r d_{r + 1} d_{r + 2} ldots d_{r + p} d_{r + 1} d_{r + 2} ldots d_{r + p} d_{r + 1} d_{r + 2} ldots d_{r + p} d_{r + 1} ldots right]_b$


That is, $left( d_{r + 1} d_{r + 2} ldots d_{r + p}  right)$ repeats from then on, or recurs.

Then $x$ is said to recur.


=== Non-Recurring Part ===
Let $b in mathbb N: b ge 2$.

Let $x$ be a real number.

Let the basis expansion of $x$ in base $b$ be recurring:

:$left[ s cdotp d_1 d_2 d_3 ldots d_r d_{r + 1} d_{r + 2} ldots d_{r + p} d_{r + 1} d_{r + 2} ldots d_{r + p } d_{r + 1} d_{r + 2} ldots d_{r + p} d_{r + 1} ldots right]_b$


The non-recurring part of $x$  is:
:$left[ s cdotp d_1 d_2 d_3 ldots d_r right]$

=== Recurring Part ===
Let $b in mathbb N: b ge 2$.

Let $x$ be a real number.

Let the basis expansion of $x$ in base $b$ be recurring:

:$left[ s cdotp d_1 d_2 d_3 ldots d_r d_{r + 1} d_{r + 2} ldots d_{r + p} d_{r + 1} d_{r + 2} ldots d_{r + p } d_{r + 1} d_{r + 2} ldots d_{r + p} d_{r + 1} ldots right]_b$


The recurring part of $x$  is:
:$left[ d_{r + 1} d_{r + 2} ldots d_{r + p} right]$


=== Period ===
Let $b in mathbb N: b ge 2$.

Let $x$ be a rational number.

Let the basis expansion of $x$ in base $b$ be:

:$left[ s cdotp d_1 d_2 d_3 ldots right]_b$

From Basis Expansion of Rational Number, $x$ either terminates or recurs.


The period of recurrence is the number of digits in the recurring part after which it repeats itself.


Category:Definitions/Basis Expansions

=== Period ===
Let $b in mathbb N: b ge 2$.

Let $x$ be a rational number.

Let the basis expansion of $x$ in base $b$ be:

:$left[ s cdotp d_1 d_2 d_3 ldots right]_b$

From Basis Expansion of Rational Number, $x$ either terminates or recurs.


The period of recurrence is the number of digits in the recurring part after which it repeats itself.


Category:Definitions/Basis Expansions",Definition:Basis Expansion,,false,"=== Positive Real Numbers ===
Let x ∈ℝ be a real number such that x ≥ 0.

Let b ∈ℕ: b ≥ 2.


Let us define the recursive sequence:
:∀ n ∈ℕ: n ≥ 1: ⟨ f_n ⟩ = 
b ( x - ⌊ x ⌋)     : n = 1 

b ( f_n - 1 - ⌊ f_n - 1⌋)     : n > 1

Then we define:
:∀ n ∈ℕ: n ≥ 1: ⟨ d_n ⟩ = ⌊ f_n ⌋


It follows from the method of construction and the definition of the floor function that:

:∀ n: 0 ≤ f_n < b and hence ∀ n: 0 ≤ d_n ≤ b - 1
:∀ n: f_n = 0  f_n + 1 = 0 and hence d_n + 1 = 0.


Hence we can express x = ⌊ x ⌋ + ∑_j ≥ 1d_j/b^j as:
:[ s  d_1 d_2 d_3 …]_b
where:
:s = ⌊ x ⌋
:it is not the case that there exists m ∈ℕ such that d_M = b - 1 for all M ≥ m.
(That is, the sequence of digits does not end with an infinite sequence of b - 1.)


This is called the expansion of x in base b.

The generic term for such an expansion is a basis expansion.


It follows from the Division Theorem that for a given b and x this expansion is unique.


=== Termination ===
Let b ∈ℕ: b ≥ 2.

Let the basis expansion of x in base b be:

:[ s  d_1 d_2 d_3 …]_b


Let it be the case that:
:∃ m ∈ℕ: ∀ k ≥ m: d_k = 0

That is, every digit of x in base b after a certain point is zero.

Then x is said to terminate.

=== Recurrence ===
Let b ∈ℕ: b ≥ 2.

Let x be a real number.

Let the basis expansion of x in base b be:

:[ s  d_1 d_2 d_3 …]_b

Let there be a finite sequence of p digits of x:
:( d_r + 1 d_r + 1… d_r + p)
such that for all k ∈ℤ_≥ 0 and for all j ∈{ 1, 2, …, p }:
:d_r + j + k p = d_r + j

where p is the smallest p to have this property.

That is, let x be of the form:

:[ s  d_1 d_2 d_3 … d_r d_r + 1 d_r + 2… d_r + p d_r + 1 d_r + 2… d_r + p d_r + 1 d_r + 2… d_r + p d_r + 1…]_b


That is, ( d_r + 1 d_r + 2… d_r + p) repeats from then on, or recurs.

Then x is said to recur.


=== Non-Recurring Part ===
Let b ∈ℕ: b ≥ 2.

Let x be a real number.

Let the basis expansion of x in base b be recurring:

:[ s  d_1 d_2 d_3 … d_r d_r + 1 d_r + 2… d_r + p d_r + 1 d_r + 2… d_r + p  d_r + 1 d_r + 2… d_r + p d_r + 1…]_b


The non-recurring part of x  is:
:[ s  d_1 d_2 d_3 … d_r ]

=== Recurring Part ===
Let b ∈ℕ: b ≥ 2.

Let x be a real number.

Let the basis expansion of x in base b be recurring:

:[ s  d_1 d_2 d_3 … d_r d_r + 1 d_r + 2… d_r + p d_r + 1 d_r + 2… d_r + p  d_r + 1 d_r + 2… d_r + p d_r + 1…]_b


The recurring part of x  is:
:[ d_r + 1 d_r + 2… d_r + p]


=== Period ===
Let b ∈ℕ: b ≥ 2.

Let x be a rational number.

Let the basis expansion of x in base b be:

:[ s  d_1 d_2 d_3 …]_b

From Basis Expansion of Rational Number, x either terminates or recurs.


The period of recurrence is the number of digits in the recurring part after which it repeats itself.


Category:Definitions/Basis Expansions

=== Period ===
Let b ∈ℕ: b ≥ 2.

Let x be a rational number.

Let the basis expansion of x in base b be:

:[ s  d_1 d_2 d_3 …]_b

From Basis Expansion of Rational Number, x either terminates or recurs.


The period of recurrence is the number of digits in the recurring part after which it repeats itself.


Category:Definitions/Basis Expansions

=== Negative Real Numbers ===
Let x ∈ℝ: x < 0.

We take the absolute value y of x, that is:
:y = | x |

Then we take the expansion of y in base b:
:| s . d_1 d_2 d_3 …|_b
where s = ⌊ y ⌋.

Finally, the expansion of x in base b is defined as:
:-[ s . d_1 d_2 d_3 …]_b


=== Termination ===
Let b ∈ℕ: b ≥ 2.

Let the basis expansion of x in base b be:

:[ s  d_1 d_2 d_3 …]_b


Let it be the case that:
:∃ m ∈ℕ: ∀ k ≥ m: d_k = 0

That is, every digit of x in base b after a certain point is zero.

Then x is said to terminate.

=== Recurrence ===
Let b ∈ℕ: b ≥ 2.

Let x be a real number.

Let the basis expansion of x in base b be:

:[ s  d_1 d_2 d_3 …]_b

Let there be a finite sequence of p digits of x:
:( d_r + 1 d_r + 1… d_r + p)
such that for all k ∈ℤ_≥ 0 and for all j ∈{ 1, 2, …, p }:
:d_r + j + k p = d_r + j

where p is the smallest p to have this property.

That is, let x be of the form:

:[ s  d_1 d_2 d_3 … d_r d_r + 1 d_r + 2… d_r + p d_r + 1 d_r + 2… d_r + p d_r + 1 d_r + 2… d_r + p d_r + 1…]_b


That is, ( d_r + 1 d_r + 2… d_r + p) repeats from then on, or recurs.

Then x is said to recur.


=== Non-Recurring Part ===
Let b ∈ℕ: b ≥ 2.

Let x be a real number.

Let the basis expansion of x in base b be recurring:

:[ s  d_1 d_2 d_3 … d_r d_r + 1 d_r + 2… d_r + p d_r + 1 d_r + 2… d_r + p  d_r + 1 d_r + 2… d_r + p d_r + 1…]_b


The non-recurring part of x  is:
:[ s  d_1 d_2 d_3 … d_r ]

=== Recurring Part ===
Let b ∈ℕ: b ≥ 2.

Let x be a real number.

Let the basis expansion of x in base b be recurring:

:[ s  d_1 d_2 d_3 … d_r d_r + 1 d_r + 2… d_r + p d_r + 1 d_r + 2… d_r + p  d_r + 1 d_r + 2… d_r + p d_r + 1…]_b


The recurring part of x  is:
:[ d_r + 1 d_r + 2… d_r + p]


=== Period ===
Let b ∈ℕ: b ≥ 2.

Let x be a rational number.

Let the basis expansion of x in base b be:

:[ s  d_1 d_2 d_3 …]_b

From Basis Expansion of Rational Number, x either terminates or recurs.


The period of recurrence is the number of digits in the recurring part after which it repeats itself.


Category:Definitions/Basis Expansions

=== Period ===
Let b ∈ℕ: b ≥ 2.

Let x be a rational number.

Let the basis expansion of x in base b be:

:[ s  d_1 d_2 d_3 …]_b

From Basis Expansion of Rational Number, x either terminates or recurs.


The period of recurrence is the number of digits in the recurring part after which it repeats itself.


Category:Definitions/Basis Expansions",Expansion
"['Definitions/Algebraic Expansions', 'Definitions/Algebra']",Definition:Expansion,"An (algebraic) expansion is a algebraic expression written as the sum of terms.

The word is also used for the actual process of converting the power of a multinomial, in order to obtain such an expression.",Definition:Algebraic Expansion,,false,"An (algebraic) expansion is a algebraic expression written as the sum of terms.

The word is also used for the actual process of converting the power of a multinomial, in order to obtain such an expression.",Expansion
['Definitions/Set Partitions'],Definition:Expansion,"Let $S$ be a set.

Let $Bbb S = leftlbrace S_1, S_2, ldots, S_n rightrbrace$ form a partition of $S$.


Then the representation by such a partition $ds bigcup_{k mathop = 1}^n S_k = S$ is also called a finite expansion of $S$.


The notations:
:$S = S_1 mid S_2 mid cdots mid S_n$
or:
:$Bbb S = leftlbrace S_1 mid S_2 mid cdots mid S_n rightrbrace$
are sometimes seen.


Category:Definitions/Set Partitions",Definition:Set Partition/Finite Expansion,,false,"Let S be a set.

Let S = { S_1, S_2, …, S_n } form a partition of S.


Then the representation by such a partition ⋃_k  = 1^n S_k = S is also called a finite expansion of S.


The notations:
:S = S_1 | S_2 |⋯| S_n
or:
:S = { S_1 | S_2 |⋯| S_n }
are sometimes seen.


Category:Definitions/Set Partitions",Expansion
"['Definitions/Propositional Expansions', 'Definitions/Quantifiers']",Definition:Expansion,"Suppose our universe of discourse consists of the objects $mathbf X_1, mathbf X_2, mathbf X_3, ldots$ and so on.


(There may be an infinite number of objects in this universe.)


=== Universal Quantifier ===
Suppose our universe of discourse consists of the objects $mathbf X_1, mathbf X_2, mathbf X_3, ldots$ and so on.

Let $forall$ be the universal quantifier.

What $forall x: P left(   right)x$ means is:

:$mathbf X_1$ has property $P$, and $mathbf X_2$ has property $P$, and $mathbf X_3$ has property $P$, and ...

This translates into propositional logic as:

:$P left(   right){mathbf X_1} land P left(   right){mathbf X_2} land P left(   right){mathbf X_3} land ldots$


This expression of $forall x$ as a conjunction is known as the propositional expansion of $forall x$. 


The propositional expansion for the universal quantifier can exist in actuality only when the number of objects in the universe is finite.

If the universe is infinite, then the propositional expansion can exist only conceptually, and the universal quantifier cannot be eliminated.


Category:Definitions/Propositional Expansions
Category:Definitions/Universal Quantifier

=== Existential Quantifier ===
Suppose our universe of discourse consists of the objects $mathbf X_1, mathbf X_2, mathbf X_3, ldots$ and so on.

Let $exists$ be the existential quantifier.

What $exists x: P left(   right)x$ means is:

:At least one of $mathbf X_1, mathbf X_2, mathbf X_3, ldots$ has property $P$.

This means:

:Either $mathbf X_1$ has property $P$, or $mathbf X_2$ has property $P$, or $mathbf X_3$ has property $P$, or ...

This translates into propositional logic as:

:$P left(   right){mathbf X_1} lor P left(   right){mathbf X_2} lor P left(   right){mathbf X_3} lor ldots$


This expression of $exists x$ as a disjunction is known as the propositional expansion of $exists x$.


The propositional expansion for the existential quantifier can exist in actuality only when the number of objects in the universe is finite.

If the universe is infinite, then the propositional expansion can exist only conceptually, and the existential quantifier cannot be eliminated.


Category:Definitions/Propositional Expansions
Category:Definitions/Existential Quantifier",Definition:Propositional Expansion,,false,"Suppose our universe of discourse consists of the objects 𝐗_1, 𝐗_2, 𝐗_3, … and so on.


(There may be an infinite number of objects in this universe.)


=== Universal Quantifier ===
Suppose our universe of discourse consists of the objects 𝐗_1, 𝐗_2, 𝐗_3, … and so on.

Let ∀ be the universal quantifier.

What ∀ x: P (   )x means is:

:𝐗_1 has property P, and 𝐗_2 has property P, and 𝐗_3 has property P, and ...

This translates into propositional logic as:

:P (   )𝐗_1 P (   )𝐗_2 P (   )𝐗_3…


This expression of ∀ x as a conjunction is known as the propositional expansion of ∀ x. 


The propositional expansion for the universal quantifier can exist in actuality only when the number of objects in the universe is finite.

If the universe is infinite, then the propositional expansion can exist only conceptually, and the universal quantifier cannot be eliminated.


Category:Definitions/Propositional Expansions
Category:Definitions/Universal Quantifier

=== Existential Quantifier ===
Suppose our universe of discourse consists of the objects 𝐗_1, 𝐗_2, 𝐗_3, … and so on.

Let ∃ be the existential quantifier.

What ∃ x: P (   )x means is:

:At least one of 𝐗_1, 𝐗_2, 𝐗_3, … has property P.

This means:

:Either 𝐗_1 has property P, or 𝐗_2 has property P, or 𝐗_3 has property P, or ...

This translates into propositional logic as:

:P (   )𝐗_1 P (   )𝐗_2 P (   )𝐗_3…


This expression of ∃ x as a disjunction is known as the propositional expansion of ∃ x.


The propositional expansion for the existential quantifier can exist in actuality only when the number of objects in the universe is finite.

If the universe is infinite, then the propositional expansion can exist only conceptually, and the existential quantifier cannot be eliminated.


Category:Definitions/Propositional Expansions
Category:Definitions/Existential Quantifier",Expansion
['Definitions/Relation Theory'],Definition:Extension,"Let:

:$mathcal R_1 subseteq X times Y$ be a relation on $X times Y$
:$mathcal R_2 subseteq S times T$ be a relation on $S times T$
:$X subseteq S$
:$Y subseteq T$
:$mathcal R_2 restriction_{X times Y}$ be the restriction of $mathcal R_2$ to $X times Y$.


Let $mathcal R_2 restriction_{X times Y} = mathcal R_1$.


Then $mathcal R_2$ extends or is an extension of $mathcal R_1$.",Definition:Extension of Relation,,false,"Let:

:ℛ_1 ⊆ X × Y be a relation on X × Y
:ℛ_2 ⊆ S × T be a relation on S × T
:X ⊆ S
:Y ⊆ T
:ℛ_2 _X × Y be the restriction of ℛ_2 to X × Y.


Let ℛ_2 _X × Y = ℛ_1.


Then ℛ_2 extends or is an extension of ℛ_1.",Extension
"['Definitions/Mapping Theory', 'Definitions/Restrictions']",Definition:Extension,"As a mapping is, by definition, also a relation, the definition of an extension of a mapping is the same as that for an extension of a relation:

Let:

:$f_1 subseteq X times Y$ be a mapping on $X times Y$
:$f_2 subseteq S times T$ be a mapping on $S times T$
:$X subseteq S$
:$Y subseteq T$
:$f_2 restriction_{X times Y}$ be the restriction of $f_2$ to $X times Y$.


Let $f_2 restriction_{X times Y} = f_1$.

That is, let $f_1$ be a subset of $f_2$.


Then $f_2$ extends or is an extension of $f_1$.",Definition:Extension of Mapping,,false,"As a mapping is, by definition, also a relation, the definition of an extension of a mapping is the same as that for an extension of a relation:

Let:

:f_1 ⊆ X × Y be a mapping on X × Y
:f_2 ⊆ S × T be a mapping on S × T
:X ⊆ S
:Y ⊆ T
:f_2 _X × Y be the restriction of f_2 to X × Y.


Let f_2 _X × Y = f_1.

That is, let f_1 be a subset of f_2.


Then f_2 extends or is an extension of f_1.",Extension
['Definitions/Abstract Algebra'],Definition:Extension,"Let $left({S, circ}right)$ be a magma.

Let $left({T, circ restriction_T}right)$ be a submagma of $left({S, circ}right)$, where $circ restriction_T$ denotes the restriction of $circ$ to $T$.


Then:
: $left({S, circ}right)$ is an extension of $left({T, circ restriction_T}right)$
or
: $left({S, circ}right)$ extends $left({T, circ restriction_T}right)$


We can use the term directly to the operation itself and say:
: $circ$ is an extension of $circ restriction_T$
or:
: $circ$ extends $circ restriction_T$",Definition:Extension of Operation,,false,"Let (S, ∘) be a magma.

Let (T, ∘_T) be a submagma of (S, ∘), where ∘_T denotes the restriction of ∘ to T.


Then:
: (S, ∘) is an extension of (T, ∘_T)
or
: (S, ∘) extends (T, ∘_T)


We can use the term directly to the operation itself and say:
: ∘ is an extension of ∘_T
or:
: ∘ extends ∘_T",Extension
['Definitions/Sequences'],Definition:Extension,"As a sequence is, by definition, also a mapping, the definition of an extension of a sequence is the same as that for an extension of a mapping:

Let:

: $left langle {a_k} right rangle_{k mathop in A}$ be a sequence on $A$, where $A subseteq mathbb N$.
: $left langle {b_k} right rangle_{k mathop in B}$ be a sequence on $B$, where $B subseteq mathbb N$.
: $A subseteq B$
: $forall k in A: b_k = a_k$.

Then $left langle {b_k} right rangle_{k mathop in B}$ extends or is an extension of $left langle {a_k} right rangle_{k mathop in A}$.",Definition:Extension of Sequence,,false,"As a sequence is, by definition, also a mapping, the definition of an extension of a sequence is the same as that for an extension of a mapping:

Let:

: ⟨a_k⟩_k ∈ A be a sequence on A, where A ⊆ℕ.
: ⟨b_k⟩_k ∈ B be a sequence on B, where B ⊆ℕ.
: A ⊆ B
: ∀ k ∈ A: b_k = a_k.

Then ⟨b_k⟩_k ∈ B extends or is an extension of ⟨a_k⟩_k ∈ A.",Extension
"['Definitions/Class Theory', 'Definitions/Subclasses', 'Definitions/Class Extensions']",Definition:Extension,"Let $A$ and $B$ be classes.

$B$ is an extension of $A$  if and only if :
:$A subseteq B$


=== Immediate Extension ===
Let $A$ and $B$ be classes.

Let $B$ be an extension of $A$.

$B$ is an immediate extension of $A$  if and only if  $B$ contains exactly one more element than $A$.",Definition:Extension of Class,,false,"Let A and B be classes.

B is an extension of A  if and only if :
:A ⊆ B


=== Immediate Extension ===
Let A and B be classes.

Let B be an extension of A.

B is an immediate extension of A  if and only if  B contains exactly one more element than A.",Extension
['Definitions/Class Extensions'],Definition:Extension,"Let $A$ and $B$ be classes.

Let $B$ be an extension of $A$.

$B$ is an immediate extension of $A$  if and only if  $B$ contains exactly one more element than $A$.",Definition:Extension of Class/Immediate,,false,"Let A and B be classes.

Let B be an extension of A.

B is an immediate extension of A  if and only if  B contains exactly one more element than A.",Extension
['Definitions/Field Extensions'],Definition:Extension,"Let $F$ be a field.


A field extension over $F$ is a field $E$ where $F subseteq E$.

That is, such that $F$ is a subfield of $E$.


This can be expressed:
:$E$ is a field extension over a field $F$
or:
:$E$ over $F$ is a field extension 
as:
:$E / F$ is a field extension.  


$E / F$ can be voiced as $E$ over $F$.",Definition:Field Extension,,false,"Let F be a field.


A field extension over F is a field E where F ⊆ E.

That is, such that F is a subfield of E.


This can be expressed:
:E is a field extension over a field F
or:
:E over F is a field extension 
as:
:E / F is a field extension.  


E / F can be voiced as E over F.",Extension
['Definitions/Ring Theory'],Definition:Extension,"Let $R$ and $S$ be commutative rings with unity.

Let $phi : R to S$ be a ring monomorphism.

Then $phi : R to S$ is a ring extension of $R$.


Alternatively, we can define $S$ to be a ring extension of $R$ if $R$ is a subring of $S$ (provided we insist that a subring inherits the multiplicative identity from its parent ring).


These definitions are equivalent up to isomorphism, for if $R subseteq S$ is a subring, then the identity $operatorname{id} : R to S$ is a monomorphism.

Conversely if $phi : R to S$ is a monomorphism, then $operatorname{im}phi subseteq S$ is a subring of $S$.

Moreover by Surgery for Rings, we can find a ring $T$, isomorphic to $S$, that contains $R$ as a subring.

 ",Definition:Ring Extension,,false,"Let R and S be commutative rings with unity.

Let ϕ : R → S be a ring monomorphism.

Then ϕ : R → S is a ring extension of R.


Alternatively, we can define S to be a ring extension of R if R is a subring of S (provided we insist that a subring inherits the multiplicative identity from its parent ring).


These definitions are equivalent up to isomorphism, for if R ⊆ S is a subring, then the identity id : R → S is a monomorphism.

Conversely if ϕ : R → S is a monomorphism, then imϕ⊆ S is a subring of S.

Moreover by Surgery for Rings, we can find a ring T, isomorphic to S, that contains R as a subring.

 ",Extension
['Definitions/Ideal Theory'],Definition:Extension,"Let $A$ and $B$ be commutative ring with unity.

Let $f : A to B$ be a ring homomorphism.

Let $mathfrak a$ be an ideal of $A$.


The extension of $mathfrak a$ by $f$ is the ideal generated by its image under $f$:
:$mathfrak a^e = leftlangle f left[ mathfrak a right] rightrangle$",Definition:Extension of Ideal,,false,"Let A and B be commutative ring with unity.

Let f : A → B be a ring homomorphism.

Let 𝔞 be an ideal of A.


The extension of 𝔞 by f is the ideal generated by its image under f:
:𝔞^e = ⟨ f [ 𝔞] ⟩",Extension
['Definitions/Model Theory for Predicate Logic'],Definition:Extension,"Let $mathcal A, mathcal B$ be structures for a signature $mathcal L$.


Then $mathcal B$ is an extension of $mathcal A$  if and only if  $mathcal A$ is a substructure of $mathcal B$.",Definition:Extension of Structure,,false,"Let 𝒜, ℬ be structures for a signature ℒ.


Then ℬ is an extension of 𝒜  if and only if  𝒜 is a substructure of ℬ.",Extension
['Definitions/Propositional Tableaus'],Definition:Extension,"Let $left({T, mathbf H, Phi}right)$ be a propositional tableau.


=== Definition 1 ===
Let $T$ be a propositional tableau.


A tableau $T'$ is an extension of $T$ if $T'$ can be obtained from $T$ by repeatedly adding nodes to the leaf nodes of $T$ (by means of the tableau extension rules).

=== Definition 2 ===
Let $left({T, mathbf H, Phi}right)$ be a propositional tableau.


An extension of $T$ is a propositional tableau $left({S, mathbf H', Phi'}right)$ such that:

:$S$ is an extension of $T$;
:$mathbf H = mathbf H'$;
:$Phi'$ is an extension of $Phi$.",Definition:Extension of Propositional Tableau,,false,"Let (T, 𝐇, Φ) be a propositional tableau.


=== Definition 1 ===
Let T be a propositional tableau.


A tableau T' is an extension of T if T' can be obtained from T by repeatedly adding nodes to the leaf nodes of T (by means of the tableau extension rules).

=== Definition 2 ===
Let (T, 𝐇, Φ) be a propositional tableau.


An extension of T is a propositional tableau (S, 𝐇', Φ') such that:

:S is an extension of T;
:𝐇 = 𝐇';
:Φ' is an extension of Φ.",Extension
"['Definitions/Set Exteriors', 'Definitions/Topology']",Definition:Exterior,"Let $T$ be a topological space.

Let $H subseteq T$.


=== Definition 1 ===
Let $T$ be a topological space.

Let $H subseteq T$.


The exterior of $H$ is the complement of the closure of $H$ in $T$.

=== Definition 2 ===
Let $T$ be a topological space.

Let $H subseteq T$.


The exterior of $H$ is the interior of the complement of $H$ in $T$.",Definition:Exterior (Topology),,false,"Let T be a topological space.

Let H ⊆ T.


=== Definition 1 ===
Let T be a topological space.

Let H ⊆ T.


The exterior of H is the complement of the closure of H in T.

=== Definition 2 ===
Let T be a topological space.

Let H ⊆ T.


The exterior of H is the interior of the complement of H in T.",Exterior
['Definitions/Differential Forms'],Definition:Exterior,"Let an exact $n$-form $omega$ be given on an $m$-manifold, with local coordinates $x_1, x_2, dots, x_m$.

Let a local coordinate expression for $omega$ be given:

:$omega = f left(   right){x_1, ldots, x_m} ,mathrm d x_{phi left(   right)1} wedge mathrm d x_{phi left(   right)2} wedge cdots wedge mathrm d x_{phi left(   right)n}$

where:
:$phi: leftlbrace 1, ldots, n rightrbrace to leftlbrace 1, ldots, m rightrbrace$ is an injection which determines which coordinate vectors $omega$ acts on.
:$wedge$ denotes the wedge product.


The exterior derivative $mathrm d omega$ is the $left( n + 1 right)$-form defined as:

:$ds mathrm d omega = left( sum_{k mathop = 1}^m frac {partial f} {partial x_k} ,mathrm d x_k right) wedge mathrm d x_{phi left(   right)1} wedge mathrm d x_{phi left(   right)2} wedge dots wedge mathrm d x_{phi left(   right)n}$


For inexact forms:
:$mathrm d left(   right){a + b} = mathrm d a + mathrm d b$",Definition:Exterior Derivative,,false,"Let an exact n-form ω be given on an m-manifold, with local coordinates x_1, x_2, …, x_m.

Let a local coordinate expression for ω be given:

:ω = f (   )x_1, …, x_m d x_ϕ(   )1∧d x_ϕ(   )2∧⋯∧d x_ϕ(   )n

where:
:ϕ: { 1, …, n }→{ 1, …, m } is an injection which determines which coordinate vectors ω acts on.
:∧ denotes the wedge product.


The exterior derivative dω is the ( n + 1 )-form defined as:

:dω = ( ∑_k  = 1^m ∂ f/∂ x_k d x_k ) ∧d x_ϕ(   )1∧d x_ϕ(   )2∧…∧d x_ϕ(   )n


For inexact forms:
:d(   )a + b = d a + d b",Exterior
"['Definitions/External Angles', 'Definitions/Polygons']",Definition:Exterior,"Contrary to intuition, the external angle of a vertex of a polygon is not the size of the angle between the sides forming that vertex, as measured outside the polygon.

An external angle is in fact an angle formed by one side of a polygon and a line produced from an adjacent side.

:

While $angle AFE$ is the internal angle of vertex $F$, the external angle of this vertex is $angle EFG$.",Definition:Polygon/External Angle,,false,"Contrary to intuition, the external angle of a vertex of a polygon is not the size of the angle between the sides forming that vertex, as measured outside the polygon.

An external angle is in fact an angle formed by one side of a polygon and a line produced from an adjacent side.

:

While ∠ AFE is the internal angle of vertex F, the external angle of this vertex is ∠ EFG.",Exterior
['Definitions/Transversals (Geometry)'],Definition:Exterior,":


An exterior angle of a transversal is an angle which is not between the two lines cut by a transversal.

In the above figure, the exterior angles with respect to the transversal $EF$ are:
:$angle AHE$
:$angle CJF$
:$angle BHE$
:$angle DJF$",Definition:Transversal (Geometry)/Exterior Angle,,false,":


An exterior angle of a transversal is an angle which is not between the two lines cut by a transversal.

In the above figure, the exterior angles with respect to the transversal EF are:
:∠ AHE
:∠ CJF
:∠ BHE
:∠ DJF",Exterior
['Definitions/Complex Analysis'],Definition:Exterior,"Let $S subseteq mathbb C$ be a subset of the complex plane.

Let $z_0 in mathbb C$.


=== Definition 1 ===
Let $S subseteq mathbb C$ be a subset of the complex plane.

Let $z_0 in mathbb C$.


$z_0$ is an exterior point of $S$  if and only if  $z_0$ has an $epsilon$-neighborhood which is disjoint from $S$.

=== Definition 2 ===
Let $S subseteq mathbb C$ be a subset of the complex plane.

Let $z_0 in mathbb C$.


$z_0$ is an exterior point of $S$  if and only if :
:$z_0$ is not an interior point of $S$
and:
:$z_0$ is not a boundary point of $S$.",Definition:Exterior Point (Complex Analysis),,false,"Let S ⊆ℂ be a subset of the complex plane.

Let z_0 ∈ℂ.


=== Definition 1 ===
Let S ⊆ℂ be a subset of the complex plane.

Let z_0 ∈ℂ.


z_0 is an exterior point of S  if and only if  z_0 has an ϵ-neighborhood which is disjoint from S.

=== Definition 2 ===
Let S ⊆ℂ be a subset of the complex plane.

Let z_0 ∈ℂ.


z_0 is an exterior point of S  if and only if :
:z_0 is not an interior point of S
and:
:z_0 is not a boundary point of S.",Exterior
['Definitions/Jordan Curves'],Definition:Exterior,"Let $f: left[ 0 ,.,.,   right]1 to mathbb R^2$ be a Jordan curve.


It follows from the Jordan Curve Theorem that $mathbb R^2 setminus mathrm {Img} left( f right)$ is a union of two disjoint connected components, one of which is unbounded.

This unbounded component is called the exterior of $f$, and is denoted as $mathrm {Ext} left( f right)$.",Definition:Jordan Curve/Exterior,,false,"Let f: [ 0  . . ]1 →ℝ^2 be a Jordan curve.


It follows from the Jordan Curve Theorem that ℝ^2 ∖Img( f ) is a union of two disjoint connected components, one of which is unbounded.

This unbounded component is called the exterior of f, and is denoted as Ext( f ).",Exterior
"['Definitions/External Angles', 'Definitions/Polygons']",Definition:Exterior Angle,"Contrary to intuition, the external angle of a vertex of a polygon is not the size of the angle between the sides forming that vertex, as measured outside the polygon.

An external angle is in fact an angle formed by one side of a polygon and a line produced from an adjacent side.

:

While $angle AFE$ is the internal angle of vertex $F$, the external angle of this vertex is $angle EFG$.",Definition:Polygon/External Angle,,false,"Contrary to intuition, the external angle of a vertex of a polygon is not the size of the angle between the sides forming that vertex, as measured outside the polygon.

An external angle is in fact an angle formed by one side of a polygon and a line produced from an adjacent side.

:

While ∠ AFE is the internal angle of vertex F, the external angle of this vertex is ∠ EFG.",Exterior Angle
['Definitions/Transversals (Geometry)'],Definition:Exterior Angle,":


An exterior angle of a transversal is an angle which is not between the two lines cut by a transversal.

In the above figure, the exterior angles with respect to the transversal $EF$ are:
:$angle AHE$
:$angle CJF$
:$angle BHE$
:$angle DJF$",Definition:Transversal (Geometry)/Exterior Angle,,false,":


An exterior angle of a transversal is an angle which is not between the two lines cut by a transversal.

In the above figure, the exterior angles with respect to the transversal EF are:
:∠ AHE
:∠ CJF
:∠ BHE
:∠ DJF",Exterior Angle
"['Definitions/Extraction of Roots', 'Definitions/Roots of Numbers']",Definition:Extraction of Root,The process of evaluating roots of a given real number is referred to as extraction.,Definition:Root of Number/Extraction,,false,The process of evaluating roots of a given real number is referred to as extraction.,Extraction of Root
"['Definitions/Extraction of Roots', 'Definitions/Roots of Equations']",Definition:Extraction of Root,The process of finding roots of a given equation is referred to as extraction.,Definition:Root of Equation/Extraction,,false,The process of finding roots of a given equation is referred to as extraction.,Extraction of Root
"['Definitions/Faces of Graphs', 'Definitions/Planar Graphs', 'Definitions/Faces']",Definition:Face,":

The faces of a planar graph are the areas which are surrounded by edges.

In the above, the faces are $BCEF$, $ABF$, $CFG$, $AFG$ and $ABCDCEG$.


=== Incident ===
:


Let $G = left( V, E right)$ be a planar graph:

Then a face of $G$ is incident to an edge $e$ of $G$ if $e$ is one of those which surrounds the face.

Similarly, a face of $G$ is incident to a vertex $v$ of $G$ if $v$ is at the end of one of those incident edges.


In the above graph, for example, the face $BCEF$ is incident to:
:the edges $BC, CE, EF, FB$
:the vertices $B, C, E, F$.

=== Adjacent ===
:

Let $G = left( V, E right)$ be a planar graph.

Two faces of $G$ are adjacent  if and only if  they are both incident to the same edge (or edges).

In the above diagram, $BCEF$ and $ABF$ are adjacent, but $BCEF$ and $AFG$ are not adjacent.


Note that faces which are both incident to the same vertex are not considered adjacent unless they are also both incident to the same edge.",Definition:Planar Graph/Face,,false,":

The faces of a planar graph are the areas which are surrounded by edges.

In the above, the faces are BCEF, ABF, CFG, AFG and ABCDCEG.


=== Incident ===
:


Let G = ( V, E ) be a planar graph:

Then a face of G is incident to an edge e of G if e is one of those which surrounds the face.

Similarly, a face of G is incident to a vertex v of G if v is at the end of one of those incident edges.


In the above graph, for example, the face BCEF is incident to:
:the edges BC, CE, EF, FB
:the vertices B, C, E, F.

=== Adjacent ===
:

Let G = ( V, E ) be a planar graph.

Two faces of G are adjacent  if and only if  they are both incident to the same edge (or edges).

In the above diagram, BCEF and ABF are adjacent, but BCEF and AFG are not adjacent.


Note that faces which are both incident to the same vertex are not considered adjacent unless they are also both incident to the same edge.",Face
"['Definitions/Faces of Polyhedra', 'Definitions/Polyhedra', 'Definitions/Faces']",Definition:Face,"The faces of a polyhedron are the polygons which contain it.


 ",Definition:Polyhedron/Face,,false,"The faces of a polyhedron are the polygons which contain it.


 ",Face
"['Definitions/Faces of Solid Figures', 'Definitions/Solid Geometry', 'Definitions/Faces']",Definition:Face,"The faces of a three-dimensional figure are the surfaces which form its extremities.


 ",Definition:Geometric Figure/Three-Dimensional Figure/Face,,false,"The faces of a three-dimensional figure are the surfaces which form its extremities.


 ",Face
"['Definitions/Geometry', 'Definitions/Solid Geometry', 'Definitions/Surfaces']",Definition:Face," 
and:
: 
:An extremity of a solid is a surface.
 ''
 

=== Plane Surface ===
 


=== Side ===
From the definition of surface, it follows that a plane locally separates space into two sides.

Thus the sides of a plane are the parts of that space into which the plane separates it.


Category:Definitions/Surfaces

=== The Plane ===
The plane is the term used for the general plane surface which is infinite in all directions.

=== Regular Surface ===
A subset $S subseteq mathbb R^3$ is a regular surface  if and only if  for each $p in S$ there exist:
:a neighborhood $V subseteq mathbb R^3$ of $p$
:an open set $U subseteq mathbb R^2$
:a surjective mapping $mathbf x : U to V cap S$, written as:
::$mathbf x left(   right){u, v} := left( x left(   right){u, v}, y left(   right){u, v}, z left(   right){u, v}  right)$

such that:
:$(1): quad x left(   right){u, v}, y left(   right){u, v}, z left(   right){u, v}$ are smooth
:$(2): quad mathbf x: U to V cap S$ is a homeomorphism
:$(3): quad$ For each $q in U$, the differential $mathrm d_q mathbf x: mathbb R^2 to mathbb R^3$ of $mathbf x$ at $q$ is one-to-one",Definition:Surface (Geometry),,false," 
and:
: 
:An extremity of a solid is a surface.
 ”
 

=== Plane Surface ===
 


=== Side ===
From the definition of surface, it follows that a plane locally separates space into two sides.

Thus the sides of a plane are the parts of that space into which the plane separates it.


Category:Definitions/Surfaces

=== The Plane ===
The plane is the term used for the general plane surface which is infinite in all directions.

=== Regular Surface ===
A subset S ⊆ℝ^3 is a regular surface  if and only if  for each p ∈ S there exist:
:a neighborhood V ⊆ℝ^3 of p
:an open set U ⊆ℝ^2
:a surjective mapping 𝐱 : U → V ∩ S, written as:
::𝐱(   )u, v := ( x (   )u, v, y (   )u, v, z (   )u, v)

such that:
:(1):    x (   )u, v, y (   )u, v, z (   )u, v are smooth
:(2):   𝐱: U → V ∩ S is a homeomorphism
:(3): For each q ∈ U, the differential d_q 𝐱: ℝ^2 →ℝ^3 of 𝐱 at q is one-to-one",Face
"['Definitions/Faces of Polyhedral Angles', 'Definitions/Polyhedral Angles', 'Definitions/Faces']",Definition:Face,Each of the planes bounded by the defining half-lines is known as a face of the polyhedral angle.,Definition:Polyhedral Angle/Face,,false,Each of the planes bounded by the defining half-lines is known as a face of the polyhedral angle.,Face
"['Definitions/Divisors', 'Definitions/Number Theory']",Definition:Factor,"Let $left( mathbb Z, +, times right)$ be the ring of integers.

Let $x, y in mathbb Z$.


Then $x$ divides $y$ is defined as:
:$x mathrel backslash y iff exists t in mathbb Z: y = t times x$


=== Aliquot Part ===
An aliquot part of an integer $n$ is a divisor of $n$ which is strictly less than $n$.

=== Aliquant Part ===
An aliquant part of an integer $n$ is a positive integer which is less than $n$ but is not a divisor of $n$.",Definition:Divisor (Algebra)/Integer,,false,"Let ( ℤ, +, ×) be the ring of integers.

Let x, y ∈ℤ.


Then x divides y is defined as:
:x  y ∃ t ∈ℤ: y = t × x


=== Aliquot Part ===
An aliquot part of an integer n is a divisor of n which is strictly less than n.

=== Aliquant Part ===
An aliquant part of an integer n is a positive integer which is less than n but is not a divisor of n.",Factor
['Definitions/Cartesian Product'],Definition:Factor,"Let $S$ and $T$ be sets.

Let $S times T$ be the cartesian product of $S$ and $T$.


Then the sets $S$ and $T$ are called the factors of $S times T$.",Definition:Cartesian Product/Factors,,false,"Let S and T be sets.

Let S × T be the cartesian product of S and T.


Then the sets S and T are called the factors of S × T.",Factor
['Definitions/Product Topology'],Definition:Factor,"Let $leftlangle left( X_i, tau_i right)  rightrangle_{i mathop in I}$ be an indexed family of topological spaces where $I$ is an arbitrary index set.

Let $left( mathcal X, tau right)$ be the product space of $leftlangle left( x_i, tau_i right)  rightrangle_{i mathop in I}$.


Each of the topological spaces $left( X_i, tau_i right)$ are called the factors of $left( mathcal X, tau right)$, and can be referred to as factor spaces.",Definition:Product Topology/Factor Space,,false,"Let ⟨( X_i, τ_i )  ⟩_i ∈ I be an indexed family of topological spaces where I is an arbitrary index set.

Let ( 𝒳, τ) be the product space of ⟨( x_i, τ_i )  ⟩_i ∈ I.


Each of the topological spaces ( X_i, τ_i ) are called the factors of ( 𝒳, τ), and can be referred to as factor spaces.",Factor
['Definitions/Indexed Families'],Definition:Family,"Let $I$ and $S$ be sets.

Let $x: I to S$ be an indexing function for $S$.

Let $x_i$ denote the image of an element $i in I$ of the domain $I$ of $x$.

Let $leftlangle x_i rightrangle_{i mathop in I}$ denote the set of the images of all the elements $i in I$ under $x$.


The image $mathrm {Img} left( x right)$, consisting of the terms $leftlangle x_i rightrangle_{i mathop in I}$, along with the indexing function $x$ itself, is called a family of elements of $S$ indexed by $I$.",Definition:Indexing Set/Family,,false,"Let I and S be sets.

Let x: I → S be an indexing function for S.

Let x_i denote the image of an element i ∈ I of the domain I of x.

Let ⟨ x_i ⟩_i ∈ I denote the set of the images of all the elements i ∈ I under x.


The image Img( x ), consisting of the terms ⟨ x_i ⟩_i ∈ I, along with the indexing function x itself, is called a family of elements of S indexed by I.",Family
['Definitions/Indexed Families'],Definition:Family,"Let $mathcal S$ be a set of sets.

Let $I$ be an indexing set.

Let $leftlangle S_i rightrangle_{i mathop in I}$ be a family of elements of $mathcal S$ indexed by $I$.


Then $leftlangle S_i rightrangle_{i mathop in I}$ is referred to as an indexed family of sets.",Definition:Indexing Set/Family of Sets,,false,"Let 𝒮 be a set of sets.

Let I be an indexing set.

Let ⟨ S_i ⟩_i ∈ I be a family of elements of 𝒮 indexed by I.


Then ⟨ S_i ⟩_i ∈ I is referred to as an indexed family of sets.",Family
['Definitions/Indexed Families'],Definition:Family,"Let $S$ be a set.

Let $I$ be an indexing set.

For each $i in I$, let $S_i$ be a corresponding subset of $S$.

Let $leftlangle S_i rightrangle_{i mathop in I}$ be a family of subsets of $S$ indexed by $I$.


Then $leftlangle S_i rightrangle_{i mathop in I}$ is referred to as an indexed family of subsets (of $S$ by $I$).",Definition:Indexing Set/Family of Subsets,,false,"Let S be a set.

Let I be an indexing set.

For each i ∈ I, let S_i be a corresponding subset of S.

Let ⟨ S_i ⟩_i ∈ I be a family of subsets of S indexed by I.


Then ⟨ S_i ⟩_i ∈ I is referred to as an indexed family of subsets (of S by I).",Family
['Definitions/Field Theory'],Definition:Field,"A field is a non-trivial division ring whose ring product is commutative.


Thus, let $left( F, +, times right)$ be an algebraic structure.


Then $left( F, +, times right)$ is a field  if and only if :
:$(1): quad$ the algebraic structure $left( F, + right)$ is an abelian group
:$(2): quad$ the algebraic structure $left( F^*, times right)$ is an abelian group where $F^* = F setminus leftlbrace 0_F rightrbrace$
:$(3): quad$ the operation $times$ distributes over $+$.


This definition gives rise to the field axioms, as follows:

=== Field Axioms ===
 

=== Addition ===
The distributand $+$ of a field $left( F, +, times right)$ is referred to as field addition, or just addition.


=== Additive Group ===
Let $left( F, +, times right)$ be a field.


The group $left( F, + right)$ is known as the additive group of $F$.

=== Additive Inverse ===
Let $left( F, +, times right)$ be a field whose addition operation is $+$.

Let $a in R$ be any arbitrary element of $F$.


The additive inverse of $a$ is its inverse under addition, denoted $-a$:

:$a + left( -a right) = 0_F$

where $0_F$ is the zero of $R$.


=== Additive Inverse of Number ===

The concept is often encountered in the context of numbers:
Let $Bbb F$ be one of the standard number systems: $mathbb N$, $mathbb Z$, $mathbb Q$, $mathbb R$, $mathbb C$.

Let $a in Bbb F$ be any arbitrary number.


The additive inverse of $a$ is its inverse under addition, denoted $-a$:

:$a + left( -a right) = 0$

Category:Definitions/Field Theory
Category:Definitions/Addition

=== Product ===
Let $left( F, +, times right)$ be a field.


The distributive operation $times$ in $left( F, +, times right)$ is known as the (field) product.",Definition:Field (Abstract Algebra),division ring,true,"A field is a non-trivial division ring whose ring product is commutative.


Thus, let ( F, +, ×) be an algebraic structure.


Then ( F, +, ×) is a field  if and only if :
:(1): the algebraic structure ( F, + ) is an abelian group
:(2): the algebraic structure ( F^*, ×) is an abelian group where F^* = F ∖{ 0_F }
:(3): the operation × distributes over +.


This definition gives rise to the field axioms, as follows:

=== Field Axioms ===
 

=== Addition ===
The distributand + of a field ( F, +, ×) is referred to as field addition, or just addition.


=== Additive Group ===
Let ( F, +, ×) be a field.


The group ( F, + ) is known as the additive group of F.

=== Additive Inverse ===
Let ( F, +, ×) be a field whose addition operation is +.

Let a ∈ R be any arbitrary element of F.


The additive inverse of a is its inverse under addition, denoted -a:

:a + ( -a ) = 0_F

where 0_F is the zero of R.


=== Additive Inverse of Number ===

The concept is often encountered in the context of numbers:
Let F be one of the standard number systems: ℕ, ℤ, ℚ, ℝ, ℂ.

Let a ∈ F be any arbitrary number.


The additive inverse of a is its inverse under addition, denoted -a:

:a + ( -a ) = 0

Category:Definitions/Field Theory
Category:Definitions/Addition

=== Product ===
Let ( F, +, ×) be a field.


The distributive operation × in ( F, +, ×) is known as the (field) product.",Field
['Definitions/Relation Theory'],Definition:Field,"Let $S$ and $T$ be sets.

Let $mathcal R subseteq S times T$ be a relation.


The field of $mathcal R$ is defined as:
:$mathrm {Field} left( mathcal R right) := leftlbrace x in S: exists t in T: left( x, t right) in mathcal R rightrbrace cup leftlbrace x in T: exists s in S: left( s, x right) in mathcal R rightrbrace$

That is, it is the union of the domain of $mathcal R$ with its image.


=== Class Theory ===
 
Let $V$ be a basic universe.

Let $mathcal R subseteq V times V$ be a relation in $V$.


The field of $mathcal R$ is defined as:
:$mathrm {Field} left( mathcal R right) := leftlbrace x in V: exists y in V: left( x, y right) in mathcal R rightrbrace cup leftlbrace y in V: exists x in V: left( x, y right) in mathcal R rightrbrace$

That is, it is the union of the domain of $mathcal R$ with its image.",Definition:Field of Relation,,false,"Let S and T be sets.

Let ℛ⊆ S × T be a relation.


The field of ℛ is defined as:
:Field( ℛ) := { x ∈ S: ∃ t ∈ T: ( x, t ) ∈ℛ}∪{ x ∈ T: ∃ s ∈ S: ( s, x ) ∈ℛ}

That is, it is the union of the domain of ℛ with its image.


=== Class Theory ===
 
Let V be a basic universe.

Let ℛ⊆ V × V be a relation in V.


The field of ℛ is defined as:
:Field( ℛ) := { x ∈ V: ∃ y ∈ V: ( x, y ) ∈ℛ}∪{ y ∈ V: ∃ x ∈ V: ( x, y ) ∈ℛ}

That is, it is the union of the domain of ℛ with its image.",Field
"['Definitions/Fields (Physics)', 'Definitions/Physics']",Definition:Field,"A field, in the context of physics, is a region of space which is acted on by a point-function.

That is, in which a physical quantity is associated with every point of spacetime.


The physical quantity in question may be either in vector or scalar form.


=== Scalar Field ===
Let $F$ be a field which acts on a region of space $S$.

Let the point-function giving rise to $F$ be a scalar quantity.


Then $F$ is a scalar field.

=== Vector Field ===
Let $F$ be a field which acts on a region of space $S$.

Let the point-function giving rise to $F$ be a vector quantity.


Then $F$ is a vector field.

=== Vector Field on Smooth Manifold ===
Let $M$ be a smooth manifold.

Let $TM$ be the tangent bundle of $M$.

Let $T_p M$ be the tangent space at $p in M$.


Then by the vector field on $M$ we mean the continuous map $X : M to TM$ such that:

:$forall p in M : X_p in T_p M$",Definition:Field (Physics),,false,"A field, in the context of physics, is a region of space which is acted on by a point-function.

That is, in which a physical quantity is associated with every point of spacetime.


The physical quantity in question may be either in vector or scalar form.


=== Scalar Field ===
Let F be a field which acts on a region of space S.

Let the point-function giving rise to F be a scalar quantity.


Then F is a scalar field.

=== Vector Field ===
Let F be a field which acts on a region of space S.

Let the point-function giving rise to F be a vector quantity.


Then F is a vector field.

=== Vector Field on Smooth Manifold ===
Let M be a smooth manifold.

Let TM be the tangent bundle of M.

Let T_p M be the tangent space at p ∈ M.


Then by the vector field on M we mean the continuous map X : M → TM such that:

:∀ p ∈ M : X_p ∈ T_p M",Field
"['Definitions/Vector Fields', 'Definitions/Vectors', 'Definitions/Vector Spaces', 'Definitions/Fields (Physics)']",Definition:Field,"Let $F$ be a field which acts on a region of space $S$.

Let the point-function giving rise to $F$ be a vector quantity.


Then $F$ is a vector field.

=== Vector Field on Smooth Manifold ===
Let $M$ be a smooth manifold.

Let $TM$ be the tangent bundle of $M$.

Let $T_p M$ be the tangent space at $p in M$.


Then by the vector field on $M$ we mean the continuous map $X : M to TM$ such that:

:$forall p in M : X_p in T_p M$",Definition:Vector Field,,false,"Let F be a field which acts on a region of space S.

Let the point-function giving rise to F be a vector quantity.


Then F is a vector field.

=== Vector Field on Smooth Manifold ===
Let M be a smooth manifold.

Let TM be the tangent bundle of M.

Let T_p M be the tangent space at p ∈ M.


Then by the vector field on M we mean the continuous map X : M → TM such that:

:∀ p ∈ M : X_p ∈ T_p M",Field
['Definitions/Operations'],Definition:Finitary,"A finitary operation is an operation which takes a finite number of operands.


Category:Definitions/Operations",Definition:Operation/Arity/Finitary,,false,"A finitary operation is an operation which takes a finite number of operands.


Category:Definitions/Operations",Finitary
"['Definitions/Finitary Arguments', 'Definitions/Logical Arguments']",Definition:Finitary,"A finitary argument is a logical argument which starts with a finite number of axioms, and can be translated into a finite number of statements.",Definition:Logical Argument/Finitary,,false,"A finitary argument is a logical argument which starts with a finite number of axioms, and can be translated into a finite number of statements.",Finitary
['Definitions/Generators of Modules'],Definition:Finite Type,"Let $R$ be a ring.

Let $M$ be a module over $R$.


Then $M$ is finitely generated  if and only if  there is a generator for $M$ which is finite.",Definition:Finitely Generated Module,,false,"Let R be a ring.

Let M be a module over R.


Then M is finitely generated  if and only if  there is a generator for M which is finite.",Finite Type
"['Definitions/Algebraic Geometry', 'Definitions/Schemes']",Definition:Finite Type,"Let $left( X, mathcal O_X right)$ and $left( Y, mathcal O_Y right)$ be schemes.

Let $f : left( X, mathcal O_X right) to left( Y, mathcal O_Y right)$ be a morphism of schemes.


$f$ is of finite type  if and only if  $f$ is locally of finite type and quasi-compact.",Definition:Morphism of Schemes of Finite Type,,false,"Let ( X, 𝒪_X ) and ( Y, 𝒪_Y ) be schemes.

Let f : ( X, 𝒪_X ) →( Y, 𝒪_Y ) be a morphism of schemes.


f is of finite type  if and only if  f is locally of finite type and quasi-compact.",Finite Type
['Definitions/Abstract Algebra'],Definition:Finitely Generated,"Let $left( A, circ right)$ be an algebraic structure.

Let $left( A, circ right)$ have a generator which is finite.


Then $left( A, circ right)$ is finitely generated.",Definition:Finitely Generated Algebraic Structure,,false,"Let ( A, ∘) be an algebraic structure.

Let ( A, ∘) have a generator which is finite.


Then ( A, ∘) is finitely generated.",Finitely Generated
['Definitions/Monoids'],Definition:Finitely Generated,"Let $left( M, circ right)$ be a monoid.

Let $S subseteq M$.

Let $H$ be the smallest submonoid of $M$ such that $S subseteq H$.


Then:
:$S$ is a generator of $left( H, circ right)$
:$S$ generates $left( H, circ right)$
:$left( H, circ right)$ is the submonoid of $left( M, circ right)$ generated by $S$.


This is written $H = {leftlangle S rightrangle}$.


If $S$ is a singleton, for example $S = leftlbrace x rightrbrace$, then we can (and usually do) write $H = {leftlangle x rightrangle}$ for $H = {leftlangle leftlbrace x rightrbrace rightrangle}$.",Definition:Generator of Monoid,,false,"Let ( M, ∘) be a monoid.

Let S ⊆ M.

Let H be the smallest submonoid of M such that S ⊆ H.


Then:
:S is a generator of ( H, ∘)
:S generates ( H, ∘)
:( H, ∘) is the submonoid of ( M, ∘) generated by S.


This is written H = ⟨ S ⟩.


If S is a singleton, for example S = { x }, then we can (and usually do) write H = ⟨ x ⟩ for H = ⟨{ x }⟩.",Finitely Generated
"['Definitions/Generators of Groups', 'Definitions/Group Theory']",Definition:Finitely Generated,"Let $G$ be a group.


$G$ is finitely generated  if and only if  $G$ has a generator which is finite.",Definition:Finitely Generated Group,,false,"Let G be a group.


G is finitely generated  if and only if  G has a generator which is finite.",Finitely Generated
['Definitions/Generators of Modules'],Definition:Finitely Generated,"Let $R$ be a ring.

Let $M$ be a module over $R$.


Then $M$ is finitely generated  if and only if  there is a generator for $M$ which is finite.",Definition:Finitely Generated Module,,false,"Let R be a ring.

Let M be a module over R.


Then M is finitely generated  if and only if  there is a generator for M which is finite.",Finitely Generated
['Definitions/Field Extensions'],Definition:Finitely Generated,"Let $E / F$ be a field extension.


Then $E$ is said to be finitely generated over $F$  if and only if , for some $alpha_1, ldots, alpha_n in E$:

:$E = F left({alpha_1, ldots, alpha_n}right)$ 

where $F left({alpha_1, ldots, alpha_n}right)$ is the field in $E$ generated by $F cup left{{alpha_1, ldots, alpha_n}right}$.",Definition:Finitely Generated Field Extension,,false,"Let E / F be a field extension.


Then E is said to be finitely generated over F  if and only if , for some α_1, …, α_n ∈ E:

:E = F (α_1, …, α_n) 

where F (α_1, …, α_n) is the field in E generated by F ∪{α_1, …, α_n}.",Finitely Generated
['Definitions/Algebras'],Definition:Finitely Generated,"Let $A$ be a commutative ring.

Let $B$ be an $A$-algebra.


Then $B$ is finitely generated  if and only if  $B$ has a generator which is finite.",Definition:Finitely Generated Algebra,,false,"Let A be a commutative ring.

Let B be an A-algebra.


Then B is finitely generated  if and only if  B has a generator which is finite.",Finitely Generated
"['Definitions/Foci of Conic Sections', 'Definitions/Conic Sections', 'Definitions/Foci']",Definition:Focus,"Let $mathcal K$ be a conic section specified in terms of:
:a given straight line $D$
:a given point $F$
:a given constant $epsilon$

where $K$ is the locus of points $P$ such that the distance $p$ from $P$ to $D$ and the distance $q$ from $P$ to $F$ are related by the condition:
:$q = epsilon , p$


The point $F$ is known as the focus of $mathcal K$.",Definition:Conic Section/Focus,,false,"Let 𝒦 be a conic section specified in terms of:
:a given straight line D
:a given point F
:a given constant ϵ

where K is the locus of points P such that the distance p from P to D and the distance q from P to F are related by the condition:
:q = ϵ  p


The point F is known as the focus of 𝒦.",Focus
"['Definitions/Foci of Ellipses', 'Definitions/Foci of Conic Sections', 'Definitions/Ellipses']",Definition:Focus,"Let $K$ be an ellipse specified in terms of:
:a given straight line $D$
:a given point $F$
:a given constant $epsilon$ such that $0 < epsilon < 1$

where $K$ is the locus of points $P$ such that the distance $p$ from $P$ to $D$ and the distance $q$ from $P$ to $F$ are related by the condition:
:$q = epsilon , p$


The point $F$ is known as the focus of the ellipse.",Definition:Ellipse/Focus,,false,"Let K be an ellipse specified in terms of:
:a given straight line D
:a given point F
:a given constant ϵ such that 0 < ϵ < 1

where K is the locus of points P such that the distance p from P to D and the distance q from P to F are related by the condition:
:q = ϵ  p


The point F is known as the focus of the ellipse.",Focus
"['Definitions/Parabolas', 'Definitions/Foci of Conic Sections']",Definition:Focus,":


Let $K$ be a parabola specified in terms of:
:a given straight line $D$
:a given point $F$

where $K$ is the locus of points $P$ such that the distance $p$ from $P$ to $D$ equals the distance $q$ from $P$ to $F$:
:$p = q$


The point $F$ is known as the focus of the parabola.",Definition:Parabola/Focus,,false,":


Let K be a parabola specified in terms of:
:a given straight line D
:a given point F

where K is the locus of points P such that the distance p from P to D equals the distance q from P to F:
:p = q


The point F is known as the focus of the parabola.",Focus
"['Definitions/Hyperbolas', 'Definitions/Foci of Conic Sections']",Definition:Focus,":


Let $K$ be a hyperbola specified in terms of:
:a given straight line $D$
:a given point $F$
:a given constant $epsilon$ such that $epsilon > 1$

where $K$ is the locus of points $P$ such that the distance $p$ from $P$ to $D$ and the distance $q$ from $P$ to $F$ are related by the condition:
:$q = epsilon , p$


The point $F_1$ is known as a focus of the hyperbola.

The symmetrically-positioned point $F_2$ is also a focus of the hyperbola.",Definition:Hyperbola/Focus,,false,":


Let K be a hyperbola specified in terms of:
:a given straight line D
:a given point F
:a given constant ϵ such that ϵ > 1

where K is the locus of points P such that the distance p from P to D and the distance q from P to F are related by the condition:
:q = ϵ  p


The point F_1 is known as a focus of the hyperbola.

The symmetrically-positioned point F_2 is also a focus of the hyperbola.",Focus
['Definitions/Foot (Linear Measure)'],Definition:Foot,"The foot is the FPS base unit and an imperial unit of length.


=== Conversion Factors ===


=== Symbol ===
 ",Definition:FPS/Length/Foot,FPS base unit,true,"The foot is the FPS base unit and an imperial unit of length.


=== Conversion Factors ===


=== Symbol ===
 ",Foot
['Definitions/Triangles'],Definition:Foot,"Let $triangle ABC$ be a triangle.

Let $h_a$ be the altitude of $A$:

:


The point at which $h_a$ meets $BC$ (or its production) is the foot of the altitude $h_a$.


Category:Definitions/Triangles",Definition:Altitude of Triangle/Foot,,false,"Let ABC be a triangle.

Let h_a be the altitude of A:

:


The point at which h_a meets BC (or its production) is the foot of the altitude h_a.


Category:Definitions/Triangles",Foot
['Definitions/Perpendiculars'],Definition:Foot,":


The foot of a perpendicular is the point where it intersects the line to which it is at right angles.

In the above diagram, the point $C$ is the foot of the perpendicular $CD$.",Definition:Right Angle/Perpendicular/Foot,,false,":


The foot of a perpendicular is the point where it intersects the line to which it is at right angles.

In the above diagram, the point C is the foot of the perpendicular CD.",Foot
['Definitions/Formal Languages'],Definition:Formula,"Let $mathcal F$ be a formal language whose alphabet is $mathcal A$.

A well-formed formula is a collation in $mathcal A$ which can be built by using the rules of formation of the formal grammar of $mathcal F$.


That is, a collation in $mathcal A$ is a well-formed formula in $mathcal F$  if and only if  it has a parsing sequence in $mathcal F$.",Definition:Well-Formed Formula,,false,"Let ℱ be a formal language whose alphabet is 𝒜.

A well-formed formula is a collation in 𝒜 which can be built by using the rules of formation of the formal grammar of ℱ.


That is, a collation in 𝒜 is a well-formed formula in ℱ  if and only if  it has a parsing sequence in ℱ.",Formula
['Definitions/Symbolic Logic'],Definition:Formula,"Let $mathcal L$ be a formal language used in the field of symbolic logic.


Then the well-formed formulas of $mathcal L$ are often referred to as logical formulas.

They are symbolic representations of statements, and often of compound statements in particular.",Definition:Logical Formula,,false,"Let ℒ be a formal language used in the field of symbolic logic.


Then the well-formed formulas of ℒ are often referred to as logical formulas.

They are symbolic representations of statements, and often of compound statements in particular.",Formula
"['Definitions/Monoids', 'Definitions/Category of Monoids', 'Definitions/Category of Sets']",Definition:Free,"Let $S$ be a set.


A free monoid over $S$ is a monoid $M$ together with a mapping $i: S to M$, subject to:

:For all monoids $N$, for all mappings $f: S to N$, there is a unique monoid homomorphism $bar f: M to N$, such that:

::$bar f circ i = f$

This condition is called the universal (mapping) property or UMP of the free monoid over $S$.


=== Category-Theoretic Formulation ===

Let $mathbf{Mon}$ be the category of monoids, and let $mathbf{Set}$ be the category of sets.

Let $leftvert{cdot}rightvert$ be the underlying set functor on $mathbf{Mon}$.


Let $M in mathbf{Mon}_0$ be a monoid, and let $i: S to leftvert{M}rightvert$ be a mapping.

Then $left({M, i}right)$ is said to be a free monoid over $S$  if and only if :

:For all $N in mathbf{Mon}_0$ and $f: S to leftvert{N}rightvert in mathbf{Set}_1$, a unique $bar f in mathbf{Mon}_1$ makes the following diagram commute:

::$begin{xy}
<0em,4em>*{mathbf{Mon} :},
<0em,1em>*{mathbf{Set} :},

<4em,4em>*+{M} = ""M"",
<8em,4em>*+{N} = ""N"",
""M"";""N"" **@{.} ?>*@{>} ?*!/_1em/{bar f},

<4em,1em>*+{leftvert{M}rightvert} = ""MM"",
<8em,1em>*+{leftvert{N}rightvert} = ""NN"",
<4em,-3em>*+{S} = ""S"",

""MM"";""NN"" **@{-} ?>*@{>} ?*!/_1em/{leftvert{bar f}rightvert},
""S"";""MM"" **@{-} ?>*@{>} ?*!/_1em/{i},
""S"";""NN"" **@{-} ?>*@{>} ?*!/^1em/{f}
end{xy}$

This condition is called the universal (mapping) property or UMP of the free monoid over $S$.",Definition:Free Monoid,,false,"Let S be a set.


A free monoid over S is a monoid M together with a mapping i: S → M, subject to:

:For all monoids N, for all mappings f: S → N, there is a unique monoid homomorphism f̅: M → N, such that:

::f̅∘ i = f

This condition is called the universal (mapping) property or UMP of the free monoid over S.


=== Category-Theoretic Formulation ===

Let 𝐌𝐨𝐧 be the category of monoids, and let 𝐒𝐞𝐭 be the category of sets.

Let |·| be the underlying set functor on 𝐌𝐨𝐧.


Let M ∈𝐌𝐨𝐧_0 be a monoid, and let i: S →|M| be a mapping.

Then (M, i) is said to be a free monoid over S  if and only if :

:For all N ∈𝐌𝐨𝐧_0 and f: S →|N|∈𝐒𝐞𝐭_1, a unique f̅∈𝐌𝐨𝐧_1 makes the following diagram commute:

::<0em,4em>*𝐌𝐨𝐧 :,
<0em,1em>*𝐒𝐞𝐭 :,

<4em,4em>*+M = ""M"",
<8em,4em>*+N = ""N"",
""M"";""N"" **@. ?>*@> ?*!/_1em/f̅,

<4em,1em>*+|M| = ""MM"",
<8em,1em>*+|N| = ""NN"",
<4em,-3em>*+S = ""S"",

""MM"";""NN"" **@- ?>*@> ?*!/_1em/|f̅|,
""S"";""MM"" **@- ?>*@> ?*!/_1em/i,
""S"";""NN"" **@- ?>*@> ?*!/^1em/f

This condition is called the universal (mapping) property or UMP of the free monoid over S.",Free
"['Definitions/Monoids', 'Definitions/Examples of Monoids', 'Definitions/Polynomial Theory']",Definition:Free,"The free commutative monoid on an indexed set $X = leftlangle X_j: j in J rightrangle$ is the set $M$ of all monomials under the standard multiplication.

 
That is, it is the set $M$ of all finite sequences of $X$.",Definition:Free Commutative Monoid,,false,"The free commutative monoid on an indexed set X = ⟨ X_j: j ∈ J ⟩ is the set M of all monomials under the standard multiplication.

 
That is, it is the set M of all finite sequences of X.",Free
"['Definitions/Group Theory', 'Definitions/Free Groups']",Definition:Free,"=== Definition 1 ===
A group $G$ is a free group  if and only if  it is isomorphic to the free group on some set.

=== Definition 2 ===
A group $G$ is a free group  if and only if  it has a presentation of the form ${leftlangle S rightrangle}$, where $S$ is a set.

That is, it has a presentation without relators.",Definition:Free Group,,false,"=== Definition 1 ===
A group G is a free group  if and only if  it is isomorphic to the free group on some set.

=== Definition 2 ===
A group G is a free group  if and only if  it has a presentation of the form ⟨ S ⟩, where S is a set.

That is, it has a presentation without relators.",Free
"['Definitions/Free Groups', 'Definitions/Group Theory']",Definition:Free,"Let $X$ be a set.


A free group on $X$ is a certain $X$-pointed group, that is, a pair $left( F, iota right)$ where:
:$F$ is a group
:$iota : X to F$ is a mapping
that can be defined as follows:


=== Definition 1 ===
Let $X$ be a set.


A free group on $X$ is an $X$-pointed group $left( F, iota right)$ that satisfies the following universal property:
:For every $X$-pointed group $left( G, kappa right)$ there exists a unique group homomorphism $phi : F to G$ such that:
::$phi circ iota = kappa$
:that is, a morphism of pointed groups $F to G$.

=== Definition 2 ===
Let $X$ be a set.


The free group on $X$ is the pair $left( F, iota right)$ such that:
:$F$ is the group of reduced group words on $X$
:$iota : X to F$ is the canonical injection.",Definition:Free Group on Set,,false,"Let X be a set.


A free group on X is a certain X-pointed group, that is, a pair ( F, ι) where:
:F is a group
:ι : X → F is a mapping
that can be defined as follows:


=== Definition 1 ===
Let X be a set.


A free group on X is an X-pointed group ( F, ι) that satisfies the following universal property:
:For every X-pointed group ( G, κ) there exists a unique group homomorphism ϕ : F → G such that:
::ϕ∘ι = κ
:that is, a morphism of pointed groups F → G.

=== Definition 2 ===
Let X be a set.


The free group on X is the pair ( F, ι) such that:
:F is the group of reduced group words on X
:ι : X → F is the canonical injection.",Free
['Definitions/Abelian Groups'],Definition:Free,"Let $G$ be an abelian group.


$G$ is a free abelian group  if and only if  the $mathbb Z$-module associated with $G$ is a free $mathbb Z$-module.

That is, $G$ is a free abelian group  if and only if  it has a basis over $mathbb Z$.",Definition:Free Abelian Group,,false,"Let G be an abelian group.


G is a free abelian group  if and only if  the ℤ-module associated with G is a free ℤ-module.

That is, G is a free abelian group  if and only if  it has a basis over ℤ.",Free
['Definitions/Abelian Groups'],Definition:Free,"Let $mathbb Z$ be the additive group of integers.

Let $S$ be a set.


The free abelian group on $S$ is the pair $left( mathbb Z^{left( S right)}, iota right)$ where:
* $mathbb Z^{left( S right)}$ is the direct sum of $S$ copies of $mathbb Z$. That is, of the indexed family $S to leftlbrace mathbb Z rightrbrace$
* $iota : S to mathbb Z^{left( S right)}$ is the canonical mapping, which sends $s$ to the mapping $delta_{st} in mathbb Z^{left( S right)}$, where $delta$ denotes Kronecker delta.",Definition:Free Abelian Group on Set,,false,"Let ℤ be the additive group of integers.

Let S be a set.


The free abelian group on S is the pair ( ℤ^( S ), ι) where:
* ℤ^( S ) is the direct sum of S copies of ℤ. That is, of the indexed family S →{ℤ}
* ι : S →ℤ^( S ) is the canonical mapping, which sends s to the mapping δ_st∈ℤ^( S ), where δ denotes Kronecker delta.",Free
['Definitions/Module Theory'],Definition:Free,"Let $R$ be a ring with unity.

Let $G$ be a unitary $R$-module.


Then $G$ is described as free  if and only if  there exists a basis of $G$.",Definition:Free Module,,false,"Let R be a ring with unity.

Let G be a unitary R-module.


Then G is described as free  if and only if  there exists a basis of G.",Free
"['Definitions/Module Theory', 'Definitions/Abstract Algebra']",Definition:Free,"Let $R$ be a ring.

 

Let $I$ be an indexing set.


The free $R$-module on $I$ is the direct sum of $R$ as a module over itself:
:$ds R^{left( I right)} := bigoplus_{i mathop in I} R$
of the family $I to leftlbrace R rightrbrace$ to the singleton $leftlbrace R rightrbrace$.",Definition:Free Module on Set,,false,"Let R be a ring.

 

Let I be an indexing set.


The free R-module on I is the direct sum of R as a module over itself:
:R^( I ) := ⊕_i ∈ I R
of the family I →{ R } to the singleton { R }.",Free
['Definitions/Group Actions'],Definition:Free,"Let $G$ be a group with identity $e$ acting on a set $X$.


The group action is free  if and only if :
:$forall g in G: forall x in X : g * x = x implies g = e$",Definition:Free Group Action,,false,"Let G be a group with identity e acting on a set X.


The group action is free  if and only if :
:∀ g ∈ G: ∀ x ∈ X : g * x = x  g = e",Free
"['Definitions/Physics', 'Definitions/Mechanics', 'Definitions/Applied Mathematics', 'Definitions/Frequency']",Definition:Frequency,"Let $f left(   right)t$ be a periodic function of time $t$.


The frequency of $f$ is the number of periods of $f$ that occur during a unit time interval.


=== Dimension ===
The dimension of measurement of frequency is:
:$mathsf T^{-1}$


Category:Definitions/Dimensions of Measurement
Category:Definitions/Frequency

=== Units ===
The SI unit of measurement of frequency  is the hertz $mathrm {Hz}$:
:$1  mathrm {Hz} = 1  mathrm{s}^{-1}$

that is, $1$ per second of time.",Definition:Frequency (Physics),,false,"Let f (   )t be a periodic function of time t.


The frequency of f is the number of periods of f that occur during a unit time interval.


=== Dimension ===
The dimension of measurement of frequency is:
:𝖳^-1


Category:Definitions/Dimensions of Measurement
Category:Definitions/Frequency

=== Units ===
The SI unit of measurement of frequency  is the hertz Hz:
:1  Hz = 1  s^-1

that is, 1 per second of time.",Frequency
"['Definitions/Frequency of Periodic Real Function', 'Definitions/Periodic Functions']",Definition:Frequency,"Let $f: mathbb R to mathbb R$ be a periodic real function.

The frequency $nu$ of $f$ is the reciprocal of the period $L$ of $f$:
:$nu = dfrac 1 L$

where:
:$forall x in X: f left(   right)x = f left(   right){x + L}$",Definition:Periodic Real Function/Frequency,,false,"Let f: ℝ→ℝ be a periodic real function.

The frequency ν of f is the reciprocal of the period L of f:
:ν =  1 L

where:
:∀ x ∈ X: f (   )x = f (   )x + L",Frequency
"['Definitions/Frequency (Descriptive Statistics)', 'Definitions/Class Intervals', 'Definitions/Qualitative Variables', 'Definitions/Descriptive Statistics']",Definition:Frequency,"Let $S$ be a sample or a population.

Let $omega$ be a qualitative variable, or a class interval of a quantitative variable.


The frequency of $omega$ is the number of individuals in $S$ satisfying $omega$.",Definition:Frequency (Descriptive Statistics),,false,"Let S be a sample or a population.

Let ω be a qualitative variable, or a class interval of a quantitative variable.


The frequency of ω is the number of individuals in S satisfying ω.",Frequency
"['Definitions/Relative Frequency', 'Definitions/Frequency (Descriptive Statistics)']",Definition:Frequency,"Let $S$ be a sample or a finite population.

Let $omega$ be a qualitative variable, or a class interval of a quantitative variable.


The relative frequency of $omega$ is defined as:

:$operatorname {RF}  left(   right)omega := dfrac {f_omega} n$

where:

:$f_omega$ is the (absolute) frequency of $omega$

:$n$ is the number of individuals in $S$.",Definition:Relative Frequency,,false,"Let S be a sample or a finite population.

Let ω be a qualitative variable, or a class interval of a quantitative variable.


The relative frequency of ω is defined as:

:RF(   )ω := f_ω n

where:

:f_ω is the (absolute) frequency of ω

:n is the number of individuals in S.",Frequency
"['Definitions/Cumulative Frequency', 'Definitions/Descriptive Statistics']",Definition:Frequency,"Let $left( Omega, unicode{x3a3}, Pr right)$ be a probability space.

Let $X$ be a discrete random variable on $left( Omega, unicode{x3a3}, Pr right)$.


=== Absolute ===
Let $left( Omega, unicode{x3a3}, Pr right)$ be a probability space.

Let $X$ be a discrete random variable on $left( Omega, unicode{x3a3}, Pr right)$.


The absolute cumulative frequency of $X$ is defined as:
:$forall x in mathrm {Dom} left( X right): text {acf}  left(   right)x = ds sum_{y mathop le x} Omega left(   right)y$

 

=== Relative ===
Let $left( Omega, unicode{x3a3}, Pr right)$ be a probability space.

Let $X$ be a discrete random variable on $left( Omega, unicode{x3a3}, Pr right)$.


The relative cumulative frequency of $X$ is defined as:
:$forall x in mathrm {Dom} left( X right): text {acf}  left(   right)x = dfrac {ds sum_{y mathop le x} Omega left(   right)y} {leftlvert mathrm {Dom} left( X right) rightrvert }$

 ",Definition:Cumulative Frequency,,false,"Let ( Ω, x3a3, ) be a probability space.

Let X be a discrete random variable on ( Ω, x3a3, ).


=== Absolute ===
Let ( Ω, x3a3, ) be a probability space.

Let X be a discrete random variable on ( Ω, x3a3, ).


The absolute cumulative frequency of X is defined as:
:∀ x ∈Dom( X ): acf(   )x = ∑_y ≤ xΩ(   )y

 

=== Relative ===
Let ( Ω, x3a3, ) be a probability space.

Let X be a discrete random variable on ( Ω, x3a3, ).


The relative cumulative frequency of X is defined as:
:∀ x ∈Dom( X ): acf(   )x = ∑_y ≤ xΩ(   )y|Dom( X ) |

 ",Frequency
"['Definitions/Frequency Curves (Statistics)', 'Definitions/Frequency Polygons', 'Definitions/Frequency Curves']",Definition:Frequency Curve,A frequency curve is a smooth curve approximating a frequency polygon for a large data set.,Definition:Frequency Curve (Statistics),curve,true,A frequency curve is a smooth curve approximating a frequency polygon for a large data set.,Frequency Curve
"['Definitions/Frequency Curves (Probability Theory)', 'Definitions/Frequency Functions', 'Definitions/Frequency Curves']",Definition:Frequency Curve,A frequency curve is a curve representing a frequency function.,Definition:Frequency Curve (Probability Theory),curve,true,A frequency curve is a curve representing a frequency function.,Frequency Curve
"['Definitions/T1 Spaces', 'Definitions/Separation Axioms']",Definition:Fréchet Space,"Let $T = left( S, tau right)$ be a topological space.


=== Definition 1 ===

Let $T = left( S, tau right)$ be a topological space.


$left( S, tau right)$ is a Fréchet space or $T_1$ space  if and only if :

:$forall x, y in S$ such that $x ne y$, both:
::$exists U in tau: x in U, y notin U$
:and:
::$exists V in tau: y in V, x notin V$

That is:
:for any two distinct elements $x, y in S$ there exist open sets $U, V in tau$ such that $x$ is in $U$ but not in $V$, and $y$ is in $V$ but not in $U$.


That is:
:$left( S, tau right)$ is $T_1$  if and only if  every two elements of $S$ are separated.

=== Definition 2 ===
Let $T = left( S, tau right)$ be a topological space.


$left( S, tau right)$ is a Fréchet space or $T_1$ space  if and only if  all points of $S$ are closed in $T$.",Definition:Fréchet Space (Topology),,false,"Let T = ( S, τ) be a topological space.


=== Definition 1 ===

Let T = ( S, τ) be a topological space.


( S, τ) is a Fréchet space or T_1 space  if and only if :

:∀ x, y ∈ S such that x  y, both:
::∃ U ∈τ: x ∈ U, y ∉ U
:and:
::∃ V ∈τ: y ∈ V, x ∉ V

That is:
:for any two distinct elements x, y ∈ S there exist open sets U, V ∈τ such that x is in U but not in V, and y is in V but not in U.


That is:
:( S, τ) is T_1  if and only if  every two elements of S are separated.

=== Definition 2 ===
Let T = ( S, τ) be a topological space.


( S, τ) is a Fréchet space or T_1 space  if and only if  all points of S are closed in T.",Fréchet Space
"['Definitions/Fréchet Product Metric', 'Definitions/Examples of Metric Spaces']",Definition:Fréchet Space,"Let $mathbb R^omega$ denote the countable-dimensional real Cartesian space.

Let:
:$x := leftlangle x_i rightrangle_{i mathop in mathbb N} = left( x_0, x_1, x_2, ldots right)$
and:
:$y := leftlangle y_i rightrangle_{i mathop in mathbb N} = left( y_0, y_1, y_2, ldots right)$
denote arbitrary elements of $mathbb R^omega$.


Let the distance function $d: mathbb R^omega times mathbb R^omega to mathbb R$ be applied to $mathbb R^omega$ as:
:$forall x, y in mathbb R^omega: d left(   right){x, y} = ds sum_{i mathop in mathbb N} dfrac {2^{-i} leftlvert x_i - y_i rightrvert } {1 + leftlvert x_i - y_i rightrvert }$


The distance function $d$ is referred to as the Fréchet (product) metric.


The resulting metric space $left( mathbb R^omega, d right)$ is then referred to as the Fréchet (metric) space.",Definition:Fréchet Space (Functional Analysis),,false,"Let ℝ^ω denote the countable-dimensional real Cartesian space.

Let:
:x := ⟨ x_i ⟩_i ∈ℕ = ( x_0, x_1, x_2, …)
and:
:y := ⟨ y_i ⟩_i ∈ℕ = ( y_0, y_1, y_2, …)
denote arbitrary elements of ℝ^ω.


Let the distance function d: ℝ^ω×ℝ^ω→ℝ be applied to ℝ^ω as:
:∀ x, y ∈ℝ^ω: d (   )x, y = ∑_i ∈ℕ2^-i| x_i - y_i |1 + | x_i - y_i |


The distance function d is referred to as the Fréchet (product) metric.


The resulting metric space ( ℝ^ω, d ) is then referred to as the Fréchet (metric) space.",Fréchet Space
"['Definitions/Galois Groups of Field Extensions', 'Definitions/Field Extensions', 'Definitions/Galois Theory', 'Definitions/Examples of Groups', 'Definitions/Galois Groups']",Definition:Galois Group,"Let $L / K$ be a field extension.


The Galois group of $L / K$ is the subgroup of the automorphism group of $L$ consisting of field automorphisms that fix $K$ point-wise:
:$mathrm {Gal} left( L / K right) = leftlbrace sigma in mathrm {Aut} left( L right): forall k in K: sigma left(   right)k = k rightrbrace$


=== Topological Group ===
The notation $mathrm {Gal} left( L / K right)$ is also a shorthand for the topological group:
:$left( mathrm {Gal} left( L / K right), tau right)$
where $tau$ is the Krull topology.",Definition:Galois Group of Field Extension,,false,"Let L / K be a field extension.


The Galois group of L / K is the subgroup of the automorphism group of L consisting of field automorphisms that fix K point-wise:
:Gal( L / K ) = {σ∈Aut( L ): ∀ k ∈ K: σ(   )k = k }


=== Topological Group ===
The notation Gal( L / K ) is also a shorthand for the topological group:
:( Gal( L / K ), τ)
where τ is the Krull topology.",Galois Group
"['Definitions/Galois Groups of Polynomials', 'Definitions/Galois Groups', 'Definitions/Polynomial Theory']",Definition:Galois Group,"Let $P$ be a polynomial.

The Galois group of $P$ is the symmetry group of the roots of $P$.",Definition:Galois Group of Polynomial,,false,"Let P be a polynomial.

The Galois group of P is the symmetry group of the roots of P.",Galois Group
"['Definitions/Abstract Algebra', 'Definitions/Algebraic Structures']",Definition:Generated,"Let $left( A, circ right)$ be an algebraic structure.

Let $G subseteq A$ be any subset of $A$.


The algebraic substructure generated by $G$ is the smallest substructure of $left( A, circ right)$ which contains $G$.


It is written ${leftlangle G rightrangle}$.",Definition:Generated Algebraic Substructure,,false,"Let ( A, ∘) be an algebraic structure.

Let G ⊆ A be any subset of A.


The algebraic substructure generated by G is the smallest substructure of ( A, ∘) which contains G.


It is written ⟨ G ⟩.",Generated
['Definitions/Monoids'],Definition:Generated,"Let $left( M, circ right)$ be a monoid whose identity is $e_M$.

Let $S subseteq M$

Let $H$ be the smallest (with respect to set inclusion) submonoid of $M$ such that $left( S cup leftlbrace e_M rightrbrace  right) subseteq H$.


Then $left( H, circ right)$ is the submonoid of $left( M, circ right)$ generated by $S$.


This is written $H = {leftlangle S rightrangle}$.


If $S$ is a singleton, for example $S = leftlbrace x rightrbrace$, then we can (and usually do) write $H = {leftlangle x rightrangle}$ for $H = {leftlangle leftlbrace x rightrbrace rightrangle}$.


=== Generator ===
Let $left( M, circ right)$ be a monoid

Let $S subseteq M$.

Let $left( H, circ right)$ be the submonoid of $left( M, circ right)$ generated by $S$.

Then $S$ is known as a generator of $left( H, circ right)$.",Definition:Generated Submonoid,,false,"Let ( M, ∘) be a monoid whose identity is e_M.

Let S ⊆ M

Let H be the smallest (with respect to set inclusion) submonoid of M such that ( S ∪{ e_M }) ⊆ H.


Then ( H, ∘) is the submonoid of ( M, ∘) generated by S.


This is written H = ⟨ S ⟩.


If S is a singleton, for example S = { x }, then we can (and usually do) write H = ⟨ x ⟩ for H = ⟨{ x }⟩.


=== Generator ===
Let ( M, ∘) be a monoid

Let S ⊆ M.

Let ( H, ∘) be the submonoid of ( M, ∘) generated by S.

Then S is known as a generator of ( H, ∘).",Generated
"['Definitions/Generated Subgroups', 'Definitions/Subgroups', 'Definitions/Generators of Groups']",Definition:Generated,"Let $G$ be a group.

Let $S subset G$ be a subset.


=== Definition 1 ===
Let $G$ be a group.

Let $S subset G$ be a subset.


The subgroup generated by $S$ is the smallest subgroup containing $S$.

=== Definition 2 ===
Let $G$ be a group.

Let $S subset G$ be a subset.


The subgroup generated by $S$ is the intersection of all subgroups of $G$ containing $S$.

=== Definition 3 ===
Let $G$ be a group.

Let $S subset G$ be a subset.

Let $S^{-1}$ be the set of inverses of $S$.


The subgroup generated by $S$ is the set of words on the union $S cup S^{-1}$.",Definition:Generated Subgroup,,false,"Let G be a group.

Let S ⊂ G be a subset.


=== Definition 1 ===
Let G be a group.

Let S ⊂ G be a subset.


The subgroup generated by S is the smallest subgroup containing S.

=== Definition 2 ===
Let G be a group.

Let S ⊂ G be a subset.


The subgroup generated by S is the intersection of all subgroups of G containing S.

=== Definition 3 ===
Let G be a group.

Let S ⊂ G be a subset.

Let S^-1 be the set of inverses of S.


The subgroup generated by S is the set of words on the union S ∪ S^-1.",Generated
"['Definitions/Generated Normal Subgroups', 'Definitions/Normal Subgroups', 'Definitions/Normality in Groups', 'Definitions/Generated Subgroups', 'Definitions/Generators of Groups', 'Definitions/Subgroups']",Definition:Generated,"Let $G$ be a group.

Let $S subseteq G$ be a subset.


=== Definition 1 ===
Let $G$ be a group. 

Let $S subseteq G$ be a subset.


The normal subgroup generated by $S$, denoted ${leftlangle S^G rightrangle}$,  is the intersection of all normal subgroups of $G$ containing $S$.

=== Definition 2 ===
 

Let $G$ be a group. 

Let $S subseteq G$ be a subset.


The normal subgroup generated by $S$, denoted ${leftlangle S^G rightrangle}$,  is the smallest normal subgroup of $G$ containing $S$:
:${leftlangle S^G rightrangle} = {leftlangle x S x^{-1}: x in G rightrangle}$",Definition:Generated Normal Subgroup,,false,"Let G be a group.

Let S ⊆ G be a subset.


=== Definition 1 ===
Let G be a group. 

Let S ⊆ G be a subset.


The normal subgroup generated by S, denoted ⟨ S^G ⟩,  is the intersection of all normal subgroups of G containing S.

=== Definition 2 ===
 

Let G be a group. 

Let S ⊆ G be a subset.


The normal subgroup generated by S, denoted ⟨ S^G ⟩,  is the smallest normal subgroup of G containing S:
:⟨ S^G ⟩ = ⟨ x S x^-1: x ∈ G ⟩",Generated
['Definitions/Generators of Modules'],Definition:Generated,"Let $R$ be a ring.

Let $M$ be an $R$-module.

Let $Ssubset M$ be a subset of $M$.


=== $R$-module ===
Let $R$ be a ring.

Let $M = left( G, +, circ right)_R$ be an $R$-module.

Let $S subset M$ be a subset of $M$.


The submodule generated by $S$ is the intersection of all submodules of $M$ containing $S$.

=== Unitary $R$-Module ===
Let $R$ be a ring with unity.

Let $M$ be a unitary $R$-module.

Let $Ssubset M$ be a subset.


The submodule generated by $S$ is the set of all linear combinations of elements of $S$.",Definition:Generated Submodule,,false,"Let R be a ring.

Let M be an R-module.

Let S⊂ M be a subset of M.


=== R-module ===
Let R be a ring.

Let M = ( G, +, ∘)_R be an R-module.

Let S ⊂ M be a subset of M.


The submodule generated by S is the intersection of all submodules of M containing S.

=== Unitary R-Module ===
Let R be a ring with unity.

Let M be a unitary R-module.

Let S⊂ M be a subset.


The submodule generated by S is the set of all linear combinations of elements of S.",Generated
"['Definitions/Generators of Ideals', 'Definitions/Ideal Theory']",Definition:Generated,"Let $left( R, +, circ right)$ be a ring with unity.

Let $S subseteq R$ be a subset of $R$.


The ideal generated by $S$ is the smallest ideal of $R$ containing $S$, that is, the intersection of all ideals containing $S$.


=== Unitary Rings ===
Let $left( R, +, circ right)$ be a ring with unity.

Let $S subseteq R$ be a subset of $R$.


The ideal generated by $S$ is the set of all two-sided linear combinations of elements of $S$.

=== Commutative and Unitary Rings ===
Let $left( R, +, circ right)$ be a commutative ring with unity.

Let $S subseteq R$ be a subset of $R$.


The ideal generated by $S$ is the set of all linear combinations of elements of $S$.",Definition:Generated Ideal of Ring,,false,"Let ( R, +, ∘) be a ring with unity.

Let S ⊆ R be a subset of R.


The ideal generated by S is the smallest ideal of R containing S, that is, the intersection of all ideals containing S.


=== Unitary Rings ===
Let ( R, +, ∘) be a ring with unity.

Let S ⊆ R be a subset of R.


The ideal generated by S is the set of all two-sided linear combinations of elements of S.

=== Commutative and Unitary Rings ===
Let ( R, +, ∘) be a commutative ring with unity.

Let S ⊆ R be a subset of R.


The ideal generated by S is the set of all linear combinations of elements of S.",Generated
['Definitions/Ring Theory'],Definition:Generated,"Let $left( R, +, circ right)$ be a ring.

Let $S subseteq R$ be a subset.


The subring generated by $S$ is the smallest subring of $R$ containing $S$; that is, it is the intersection of all subrings of $R$ containing $S$.",Definition:Generated Subring,,false,"Let ( R, +, ∘) be a ring.

Let S ⊆ R be a subset.


The subring generated by S is the smallest subring of R containing S; that is, it is the intersection of all subrings of R containing S.",Generated
['Definitions/Ring Extensions'],Definition:Generated,"Let $S$ be a commutative rings with unity.

Let $R$ be a subring of $S$ with unity such that the unity of $R$ is the unity of $S$.

That is, $S$ is a ring extension of $R$.


Let $T subseteq S$ be a subset of $S$.


=== Definition 1 ===
Let $S$ be a commutative rings with unity.

Let $R$ be a subring of $S$ with unity such that the unity of $R$ is the unity of $S$.

That is, $S$ is a ring extension of $R$.


Let $T subseteq S$ be a subset of $S$.


=== Definition 1 ===
Let $S$ be a commutative rings with unity.

Let $R$ be a subring of $S$ with unity such that the unity of $R$ is the unity of $S$.

That is, $S$ is a ring extension of $R$.


Let $T subseteq S$ be a subset of $S$.


=== Definition 1 ===


=== Definition 2 ===


The ring extension $R left[ T right]$ generated by $T$ is the smallest subring of $S$ containing $T$ and $R$, that is, the intersection of all subrings of $S$ containing $T$ and $R$.

Thus $T$ is a generator of $R left[ T right]$  if and only if  $R left[ T right]$ has no proper subring containing $T$ and $R$.

=== Definition 2 ===
Let $S$ be a commutative rings with unity.

Let $R$ be a subring of $S$ with unity such that the unity of $R$ is the unity of $S$.

That is, $S$ is a ring extension of $R$.


Let $T subseteq S$ be a subset of $S$.


=== Definition 1 ===


=== Definition 2 ===


Let $R left[ leftlbrace X_t rightrbrace  right]$ be the polynomial ring in $T$ variables $X_t$.

Let $operatorname {ev} : R left[ leftlbrace X_t rightrbrace  right] to S$ be the evaluation homomorphism associated with the inclusion $T hookrightarrow S$.


The ring extension $R left[ T right]$ generated by $T$ is $mathrm {Img} left( operatorname {ev}  right)$, the image of $operatorname {ev}$.

$T$ is said to be a generator of $R left[ T right]$.

The ring extension $R left[ T right]$ generated by $T$ is the smallest subring of $S$ containing $T$ and $R$, that is, the intersection of all subrings of $S$ containing $T$ and $R$.

Thus $T$ is a generator of $R left[ T right]$  if and only if  $R left[ T right]$ has no proper subring containing $T$ and $R$.

=== Definition 2 ===
Let $S$ be a commutative rings with unity.

Let $R$ be a subring of $S$ with unity such that the unity of $R$ is the unity of $S$.

That is, $S$ is a ring extension of $R$.


Let $T subseteq S$ be a subset of $S$.


=== Definition 1 ===
Let $S$ be a commutative rings with unity.

Let $R$ be a subring of $S$ with unity such that the unity of $R$ is the unity of $S$.

That is, $S$ is a ring extension of $R$.


Let $T subseteq S$ be a subset of $S$.


=== Definition 1 ===


=== Definition 2 ===


The ring extension $R left[ T right]$ generated by $T$ is the smallest subring of $S$ containing $T$ and $R$, that is, the intersection of all subrings of $S$ containing $T$ and $R$.

Thus $T$ is a generator of $R left[ T right]$  if and only if  $R left[ T right]$ has no proper subring containing $T$ and $R$.

=== Definition 2 ===
Let $S$ be a commutative rings with unity.

Let $R$ be a subring of $S$ with unity such that the unity of $R$ is the unity of $S$.

That is, $S$ is a ring extension of $R$.


Let $T subseteq S$ be a subset of $S$.


=== Definition 1 ===


=== Definition 2 ===


Let $R left[ leftlbrace X_t rightrbrace  right]$ be the polynomial ring in $T$ variables $X_t$.

Let $operatorname {ev} : R left[ leftlbrace X_t rightrbrace  right] to S$ be the evaluation homomorphism associated with the inclusion $T hookrightarrow S$.


The ring extension $R left[ T right]$ generated by $T$ is $mathrm {Img} left( operatorname {ev}  right)$, the image of $operatorname {ev}$.

$T$ is said to be a generator of $R left[ T right]$.

Let $R left[ leftlbrace X_t rightrbrace  right]$ be the polynomial ring in $T$ variables $X_t$.

Let $operatorname {ev} : R left[ leftlbrace X_t rightrbrace  right] to S$ be the evaluation homomorphism associated with the inclusion $T hookrightarrow S$.


The ring extension $R left[ T right]$ generated by $T$ is $mathrm {Img} left( operatorname {ev}  right)$, the image of $operatorname {ev}$.

$T$ is said to be a generator of $R left[ T right]$.",Definition:Generated Ring Extension,,false,"Let S be a commutative rings with unity.

Let R be a subring of S with unity such that the unity of R is the unity of S.

That is, S is a ring extension of R.


Let T ⊆ S be a subset of S.


=== Definition 1 ===
Let S be a commutative rings with unity.

Let R be a subring of S with unity such that the unity of R is the unity of S.

That is, S is a ring extension of R.


Let T ⊆ S be a subset of S.


=== Definition 1 ===
Let S be a commutative rings with unity.

Let R be a subring of S with unity such that the unity of R is the unity of S.

That is, S is a ring extension of R.


Let T ⊆ S be a subset of S.


=== Definition 1 ===


=== Definition 2 ===


The ring extension R [ T ] generated by T is the smallest subring of S containing T and R, that is, the intersection of all subrings of S containing T and R.

Thus T is a generator of R [ T ]  if and only if  R [ T ] has no proper subring containing T and R.

=== Definition 2 ===
Let S be a commutative rings with unity.

Let R be a subring of S with unity such that the unity of R is the unity of S.

That is, S is a ring extension of R.


Let T ⊆ S be a subset of S.


=== Definition 1 ===


=== Definition 2 ===


Let R [ { X_t }] be the polynomial ring in T variables X_t.

Let ev : R [ { X_t }] → S be the evaluation homomorphism associated with the inclusion T ↪ S.


The ring extension R [ T ] generated by T is Img( ev), the image of ev.

T is said to be a generator of R [ T ].

The ring extension R [ T ] generated by T is the smallest subring of S containing T and R, that is, the intersection of all subrings of S containing T and R.

Thus T is a generator of R [ T ]  if and only if  R [ T ] has no proper subring containing T and R.

=== Definition 2 ===
Let S be a commutative rings with unity.

Let R be a subring of S with unity such that the unity of R is the unity of S.

That is, S is a ring extension of R.


Let T ⊆ S be a subset of S.


=== Definition 1 ===
Let S be a commutative rings with unity.

Let R be a subring of S with unity such that the unity of R is the unity of S.

That is, S is a ring extension of R.


Let T ⊆ S be a subset of S.


=== Definition 1 ===


=== Definition 2 ===


The ring extension R [ T ] generated by T is the smallest subring of S containing T and R, that is, the intersection of all subrings of S containing T and R.

Thus T is a generator of R [ T ]  if and only if  R [ T ] has no proper subring containing T and R.

=== Definition 2 ===
Let S be a commutative rings with unity.

Let R be a subring of S with unity such that the unity of R is the unity of S.

That is, S is a ring extension of R.


Let T ⊆ S be a subset of S.


=== Definition 1 ===


=== Definition 2 ===


Let R [ { X_t }] be the polynomial ring in T variables X_t.

Let ev : R [ { X_t }] → S be the evaluation homomorphism associated with the inclusion T ↪ S.


The ring extension R [ T ] generated by T is Img( ev), the image of ev.

T is said to be a generator of R [ T ].

Let R [ { X_t }] be the polynomial ring in T variables X_t.

Let ev : R [ { X_t }] → S be the evaluation homomorphism associated with the inclusion T ↪ S.


The ring extension R [ T ] generated by T is Img( ev), the image of ev.

T is said to be a generator of R [ T ].",Generated
['Definitions/Field Extensions'],Definition:Generated,"Let $E / F$ be a field extension.

Let $S subset E$ be a subset of $E$.


=== Definition 1 ===
Let $E / F$ be a field extension.

Let $S subset E$ be a subset of $E$.


The field extension $F left[ S right]$ generated by $S$ is the smallest subfield extension of $E$ containing $S$, that is, the intersection of all subfields of $E$ containing $S$ and $F$.

Thus $S$ is a generator of $F left[ S right]$  if and only if  $F left[ S right]$ has no proper subfield extension containing $S$.

=== Definition 2 ===
Let $E / F$ be a field extension.

Let $S subset E$ be a subset of $E$.


Let $F left[ leftlbrace X_s rightrbrace  right]$ be the polynomial ring in $S$ variables $X_s$.

Let $operatorname {ev} : F left[ leftlbrace X_s rightrbrace  right] to E$ be the evaluation homomorphism associated to the inclusion $S hookrightarrow E$.


The field extension $F left[ S right]$ generated by $S$ is the set of all elements of $E$ of the form $operatorname {ev}  left(   right)f / operatorname {ev}  left(   right)g$, where $operatorname {ev}  left(   right)g ne 0$.

$S$ is said to be a generator of $F left[ S right]$.",Definition:Generated Field Extension,,false,"Let E / F be a field extension.

Let S ⊂ E be a subset of E.


=== Definition 1 ===
Let E / F be a field extension.

Let S ⊂ E be a subset of E.


The field extension F [ S ] generated by S is the smallest subfield extension of E containing S, that is, the intersection of all subfields of E containing S and F.

Thus S is a generator of F [ S ]  if and only if  F [ S ] has no proper subfield extension containing S.

=== Definition 2 ===
Let E / F be a field extension.

Let S ⊂ E be a subset of E.


Let F [ { X_s }] be the polynomial ring in S variables X_s.

Let ev : F [ { X_s }] → E be the evaluation homomorphism associated to the inclusion S ↪ E.


The field extension F [ S ] generated by S is the set of all elements of E of the form ev(   )f / ev(   )g, where ev(   )g  0.

S is said to be a generator of F [ S ].",Generated
['Definitions/Topology'],Definition:Generated,"Let $S$ be a set.

Let $mathcal B$ be a synthetic basis of $S$.


=== Definition 1 ===
Let $S$ be a set.

Let $mathcal B$ be a synthetic basis of $S$.


The topology on $S$ generated by $mathcal B$ is defined as:
:$tau = leftlbrace bigcup mathcal A: mathcal A subseteq mathcal B rightrbrace$

That is, the set of all unions of sets from $mathcal B$.


Category:Definitions/Topology

=== Definition 2 ===
Let $S$ be a set.

Let $mathcal B$ be a synthetic basis of $S$.


The topology on $S$ generated by $mathcal B$ is defined as:
:$tau = leftlbrace U subseteq S: U = bigcup leftlbrace B in mathcal B: B subseteq U rightrbrace rightrbrace$


Category:Definitions/Topology

=== Definition 3 ===
Let $S$ be a set.

Let $mathcal B$ be a synthetic basis of $S$.


The topology on $S$ generated by $mathcal B$ is defined as:
:$tau = leftlbrace U subseteq S: forall x in U: exists B in mathcal B: x in B subseteq U rightrbrace$


Category:Definitions/Topology",Definition:Topology Generated by Synthetic Basis,,false,"Let S be a set.

Let ℬ be a synthetic basis of S.


=== Definition 1 ===
Let S be a set.

Let ℬ be a synthetic basis of S.


The topology on S generated by ℬ is defined as:
:τ = {⋃𝒜: 𝒜⊆ℬ}

That is, the set of all unions of sets from ℬ.


Category:Definitions/Topology

=== Definition 2 ===
Let S be a set.

Let ℬ be a synthetic basis of S.


The topology on S generated by ℬ is defined as:
:τ = { U ⊆ S: U = ⋃{ B ∈ℬ: B ⊆ U }}


Category:Definitions/Topology

=== Definition 3 ===
Let S be a set.

Let ℬ be a synthetic basis of S.


The topology on S generated by ℬ is defined as:
:τ = { U ⊆ S: ∀ x ∈ U: ∃ B ∈ℬ: x ∈ B ⊆ U }


Category:Definitions/Topology",Generated
"['Definitions/Topology', 'Definitions/Sub-Bases', 'Definitions/Topology Generated by Synthetic Sub-Basis']",Definition:Generated,"Let $X$ be a set.

Let $mathcal S subseteq mathcal P left( X right)$ be a synthetic sub-basis on $X$.


=== Definition 1 ===
Let $S$ be a set.

Let $mathcal S subseteq mathcal P left( S right)$ be a synthetic sub-basis on $S$.


Define:
:$ds mathcal B = leftlbrace bigcap mathcal F: mathcal F subseteq mathcal S, mathcal F text{ is finite}  rightrbrace$

That is, $mathcal B$ is the set of all finite intersections of sets in $mathcal S$.

Note that $mathcal F$ is allowed to be empty in the above definition.


The topology generated by $mathcal S$, denoted $tau left(   right)mathcal S$, is defined as:
:$ds tau left(   right)mathcal S = leftlbrace bigcup mathcal A: mathcal A subseteq mathcal B rightrbrace$


It follows directly from Synthetic Basis formed from Synthetic Sub-Basis and Union from Synthetic Basis is Topology that $tau left(   right)mathcal S$ is a topology on $S$.

=== Definition 2 ===
Let $X$ be a set.

Let $mathcal S subseteq mathcal P left( X right)$ be a synthetic sub-basis on $X$.


The topology generated by $mathcal S$, denoted $tau left(   right)mathcal S$, is defined as the unique topology on $X$ that satisfies the following axioms:
:$(1): quad mathcal S subseteq tau left(   right)mathcal S$
:$(2): quad$ For any topology $mathcal T$ on $X$, the implication $mathcal S subseteq mathcal T implies tau left(   right)mathcal S subseteq mathcal T$ holds.

That is, $tau left(   right)mathcal S$ is the coarsest topology on $X$ for which every element of $mathcal S$ is open.",Definition:Topology Generated by Synthetic Sub-Basis,,false,"Let X be a set.

Let 𝒮⊆𝒫( X ) be a synthetic sub-basis on X.


=== Definition 1 ===
Let S be a set.

Let 𝒮⊆𝒫( S ) be a synthetic sub-basis on S.


Define:
:ℬ = {⋂ℱ: ℱ⊆𝒮, ℱ is finite}

That is, ℬ is the set of all finite intersections of sets in 𝒮.

Note that ℱ is allowed to be empty in the above definition.


The topology generated by 𝒮, denoted τ(   )𝒮, is defined as:
:τ(   )𝒮 = {⋃𝒜: 𝒜⊆ℬ}


It follows directly from Synthetic Basis formed from Synthetic Sub-Basis and Union from Synthetic Basis is Topology that τ(   )𝒮 is a topology on S.

=== Definition 2 ===
Let X be a set.

Let 𝒮⊆𝒫( X ) be a synthetic sub-basis on X.


The topology generated by 𝒮, denoted τ(   )𝒮, is defined as the unique topology on X that satisfies the following axioms:
:(1):   𝒮⊆τ(   )𝒮
:(2): For any topology 𝒯 on X, the implication 𝒮⊆𝒯τ(   )𝒮⊆𝒯 holds.

That is, τ(   )𝒮 is the coarsest topology on X for which every element of 𝒮 is open.",Generated
['Definitions/Dynkin Systems'],Definition:Generated,"Let $X$ be a set.

Let $mathcal G subseteq mathcal P left( X right)$ be a collection of subsets of $X$.


Then the Dynkin system generated by $mathcal G$, denoted $delta left(   right)mathcal G$, is the smallest Dynkin system on $X$ that contains $mathcal G$.

That is, $delta left(   right)mathcal G$ is subject to:

:$(1):quad mathcal G subseteq delta left(   right)mathcal G$
:$(2):quad mathcal G subseteq mathcal D implies delta left(   right)mathcal G subseteq mathcal D$ for any Dynkin system $mathcal D$ on $X$


In fact, $delta left(   right)mathcal G$ always exists, and is unique, as proved on Existence and Uniqueness of Dynkin System Generated by Collection of Subsets.


=== Generator ===

One says that $mathcal G$ is a generator for $delta left(   right)mathcal G$.",Definition:Dynkin System Generated by Collection of Subsets,,false,"Let X be a set.

Let 𝒢⊆𝒫( X ) be a collection of subsets of X.


Then the Dynkin system generated by 𝒢, denoted δ(   )𝒢, is the smallest Dynkin system on X that contains 𝒢.

That is, δ(   )𝒢 is subject to:

:(1):  𝒢⊆δ(   )𝒢
:(2):  𝒢⊆𝒟δ(   )𝒢⊆𝒟 for any Dynkin system 𝒟 on X


In fact, δ(   )𝒢 always exists, and is unique, as proved on Existence and Uniqueness of Dynkin System Generated by Collection of Subsets.


=== Generator ===

One says that 𝒢 is a generator for δ(   )𝒢.",Generated
['Definitions/Sigma-Algebras'],Definition:Generated,"Let $X$ be a set.

Let $mathcal G subseteq mathcal P left( X right)$ be a collection of subsets of $X$.


=== Definition 1 ===
Let $X$ be a set.

Let $mathcal G subseteq mathcal P left( X right)$ be a collection of subsets of $X$.


The $sigma$-algebra generated by $mathcal G$, denoted $sigma left(   right)mathcal G$, is the smallest $sigma$-algebra on $X$ that contains $mathcal G$.

That is, $sigma left(   right)mathcal G$ is subject to:

:$(1): quad mathcal G subseteq sigma left(   right)mathcal G$
:$(2): quad$ for all $sigma$-algebras $unicode{x3a3}$ on $X$: $mathcal G subseteq unicode{x3a3} implies sigma left(   right)mathcal G subseteq unicode{x3a3}$


=== Generator ===
Let $X$ be a set.

Let $mathcal G subseteq mathcal P left( X right)$ be a collection of subsets of $X$.

Let $sigma left(   right){mathcal G}$ be the $sigma$-algebra generated by $mathcal G$.


One says that $mathcal G$ is a generator for $sigma left(   right){mathcal G}$.

Also, elements $G$ of $mathcal G$ may be called generators.

=== Definition 2 ===
Let $X$ be a set.

Let $mathcal G subseteq mathcal P left( X right)$ be a collection of subsets of $X$.


The $sigma$-algebra generated by $mathcal G$, $sigma left(   right)mathcal G$, is the intersection of all $sigma$-algebras on $X$ that contain $mathcal G$.


=== Generator ===
Let $X$ be a set.

Let $mathcal G subseteq mathcal P left( X right)$ be a collection of subsets of $X$.

Let $sigma left(   right){mathcal G}$ be the $sigma$-algebra generated by $mathcal G$.


One says that $mathcal G$ is a generator for $sigma left(   right){mathcal G}$.

Also, elements $G$ of $mathcal G$ may be called generators.

=== Generator ===
Let $X$ be a set.

Let $mathcal G subseteq mathcal P left( X right)$ be a collection of subsets of $X$.

Let $sigma left(   right){mathcal G}$ be the $sigma$-algebra generated by $mathcal G$.


One says that $mathcal G$ is a generator for $sigma left(   right){mathcal G}$.

Also, elements $G$ of $mathcal G$ may be called generators.",Definition:Sigma-Algebra Generated by Collection of Subsets,,false,"Let X be a set.

Let 𝒢⊆𝒫( X ) be a collection of subsets of X.


=== Definition 1 ===
Let X be a set.

Let 𝒢⊆𝒫( X ) be a collection of subsets of X.


The σ-algebra generated by 𝒢, denoted σ(   )𝒢, is the smallest σ-algebra on X that contains 𝒢.

That is, σ(   )𝒢 is subject to:

:(1):   𝒢⊆σ(   )𝒢
:(2): for all σ-algebras x3a3 on X: 𝒢⊆x3a3σ(   )𝒢⊆x3a3


=== Generator ===
Let X be a set.

Let 𝒢⊆𝒫( X ) be a collection of subsets of X.

Let σ(   )𝒢 be the σ-algebra generated by 𝒢.


One says that 𝒢 is a generator for σ(   )𝒢.

Also, elements G of 𝒢 may be called generators.

=== Definition 2 ===
Let X be a set.

Let 𝒢⊆𝒫( X ) be a collection of subsets of X.


The σ-algebra generated by 𝒢, σ(   )𝒢, is the intersection of all σ-algebras on X that contain 𝒢.


=== Generator ===
Let X be a set.

Let 𝒢⊆𝒫( X ) be a collection of subsets of X.

Let σ(   )𝒢 be the σ-algebra generated by 𝒢.


One says that 𝒢 is a generator for σ(   )𝒢.

Also, elements G of 𝒢 may be called generators.

=== Generator ===
Let X be a set.

Let 𝒢⊆𝒫( X ) be a collection of subsets of X.

Let σ(   )𝒢 be the σ-algebra generated by 𝒢.


One says that 𝒢 is a generator for σ(   )𝒢.

Also, elements G of 𝒢 may be called generators.",Generated
['Definitions/Sigma-Algebras'],Definition:Generated,"Let $I$ be an indexing set.

Let $leftlangle left( X_i, unicode{x3a3}_i right)  rightrangle_{i mathop in I}$ be a family of measurable spaces.

Let $X$ be a set.

Let $leftlangle f_i: X to X_i rightrangle_{i mathop in I}$ be a family of mappings.


Then the $sigma$-algebra generated by $leftlangle f_i rightrangle_{i mathop in I}$, $sigma left(   right){f_i: i in I}$, is the smallest $sigma$-algebra on $X$ such that every $f_i$ is $sigma left(   right){f_i: i in I} , / , unicode{x3a3}_i$-measurable.

That is, $sigma left(   right){f_i: i in I}$ is subject to:

:$(1):quad forall i in I: forall E_i in unicode{x3a3}_i: f_i^{-1}  left(   right){E_i} in sigma left(   right){f_i: i in I}$
:$(2):quad sigma left(   right){f_i: i in I} subseteq unicode{x3a3}$ for all $sigma$-algebras $unicode{x3a3}$ on $X$ satisfying $(1)$


In fact, $sigma left(   right){f_i: i in I}$ always exists, and is unique, as proved on Existence and Uniqueness of Sigma-Algebra Generated by Collection of Mappings.",Definition:Sigma-Algebra Generated by Collection of Mappings,,false,"Let I be an indexing set.

Let ⟨( X_i, x3a3_i )  ⟩_i ∈ I be a family of measurable spaces.

Let X be a set.

Let ⟨ f_i: X → X_i ⟩_i ∈ I be a family of mappings.


Then the σ-algebra generated by ⟨ f_i ⟩_i ∈ I, σ(   )f_i: i ∈ I, is the smallest σ-algebra on X such that every f_i is σ(   )f_i: i ∈ I  /  x3a3_i-measurable.

That is, σ(   )f_i: i ∈ I is subject to:

:(1):  ∀ i ∈ I: ∀ E_i ∈x3a3_i: f_i^-1(   )E_i∈σ(   )f_i: i ∈ I
:(2):  σ(   )f_i: i ∈ I⊆x3a3 for all σ-algebras x3a3 on X satisfying (1)


In fact, σ(   )f_i: i ∈ I always exists, and is unique, as proved on Existence and Uniqueness of Sigma-Algebra Generated by Collection of Mappings.",Generated
['Definitions/Set Systems'],Definition:Generated,"Let $X$ be a set.

Let $mathcal G subseteq mathcal P left( X right)$ be a collection of subsets of $X$.


Then the monotone class generated by $mathcal G$, $mathfrak m left(   right)mathcal G$, is the smallest monotone class on $X$ that contains $mathcal G$.

That is, $mathfrak m left(   right)mathcal G$ is subject to:

:$(1): quad mathcal G subseteq mathfrak m left(   right)mathcal G$
:$(2): quad mathcal G subseteq mathcal M implies mathfrak m left(   right)mathcal G subseteq mathcal M$ for any monotone class $mathcal M$ on $X$


=== Generator ===

One says that $mathcal G$ is a generator for $mathfrak m left(   right)mathcal G$.",Definition:Monotone Class Generated by Collection of Subsets,,false,"Let X be a set.

Let 𝒢⊆𝒫( X ) be a collection of subsets of X.


Then the monotone class generated by 𝒢, 𝔪(   )𝒢, is the smallest monotone class on X that contains 𝒢.

That is, 𝔪(   )𝒢 is subject to:

:(1):   𝒢⊆𝔪(   )𝒢
:(2):   𝒢⊆ℳ𝔪(   )𝒢⊆ℳ for any monotone class ℳ on X


=== Generator ===

One says that 𝒢 is a generator for 𝔪(   )𝒢.",Generated
['Definitions/Filter Bases'],Definition:Generated,"Let $S$ be a set.

Let $mathcal P left( S right)$ be the power set of $S$.

Let $mathcal B subset mathcal P left( S right)$ be a filter basis of a filter $mathcal F$ on $S$.


$mathcal F$ is said to be generated by $mathcal B$.",Definition:Filter Basis/Generated Filter,,false,"Let S be a set.

Let 𝒫( S ) be the power set of S.

Let ℬ⊂𝒫( S ) be a filter basis of a filter ℱ on S.


ℱ is said to be generated by ℬ.",Generated
['Definitions/Algebraic Structures'],Definition:Generator,"Let $left( A, circ right)$ be an algebraic structure.

Let $G subset A$ be a subset.


=== Definition 1 ===

The subset $G$ is a generator of $A$  if and only if  $A$ is the algebraic substructure generated by $G$.


=== Definition 2 ===

The subset $G$ is a generator of $A$  if and only if :

:$forall x, y in G: x circ y in A$;
:$forall z in A: exists x, y in W left(   right)G: z = x circ y$
where $W left(   right)G$ is the set of words of $G$.

That is, every element in $A$ can be formed as the product of a finite number of elements of $G$.


If $G$ is such a set, then we can write $A = {leftlangle G rightrangle}$.",Definition:Generator of Algebraic Structure,,false,"Let ( A, ∘) be an algebraic structure.

Let G ⊂ A be a subset.


=== Definition 1 ===

The subset G is a generator of A  if and only if  A is the algebraic substructure generated by G.


=== Definition 2 ===

The subset G is a generator of A  if and only if :

:∀ x, y ∈ G: x ∘ y ∈ A;
:∀ z ∈ A: ∃ x, y ∈ W (   )G: z = x ∘ y
where W (   )G is the set of words of G.

That is, every element in A can be formed as the product of a finite number of elements of G.


If G is such a set, then we can write A = ⟨ G ⟩.",Generator
"['Definitions/Semigroups', 'Definitions/Subsemigroups']",Definition:Generator,"Let $left( S, circ right)$ be a semigroup.

Let $varnothing subset X subseteq S$.

Let $left( T, circ right)$ be the smallest subsemigroup of $left( S, circ right)$ such that $X subseteq T$.


Then:
:$X$ is a generator  of $left( T, circ right)$
:$X$ generates $left( T, circ right)$
:$left( T, circ right)$ is the subsemigroup of $left( S, circ right)$ generated by $X$.


This is written:
:$T = {leftlangle X rightrangle}$",Definition:Generator of Subsemigroup,,false,"Let ( S, ∘) be a semigroup.

Let ∅⊂ X ⊆ S.

Let ( T, ∘) be the smallest subsemigroup of ( S, ∘) such that X ⊆ T.


Then:
:X is a generator  of ( T, ∘)
:X generates ( T, ∘)
:( T, ∘) is the subsemigroup of ( S, ∘) generated by X.


This is written:
:T = ⟨ X ⟩",Generator
['Definitions/Monoids'],Definition:Generator,"Let $left( M, circ right)$ be a monoid.

Let $S subseteq M$.

Let $H$ be the smallest submonoid of $M$ such that $S subseteq H$.


Then:
:$S$ is a generator of $left( H, circ right)$
:$S$ generates $left( H, circ right)$
:$left( H, circ right)$ is the submonoid of $left( M, circ right)$ generated by $S$.


This is written $H = {leftlangle S rightrangle}$.


If $S$ is a singleton, for example $S = leftlbrace x rightrbrace$, then we can (and usually do) write $H = {leftlangle x rightrangle}$ for $H = {leftlangle leftlbrace x rightrbrace rightrangle}$.",Definition:Generator of Monoid,,false,"Let ( M, ∘) be a monoid.

Let S ⊆ M.

Let H be the smallest submonoid of M such that S ⊆ H.


Then:
:S is a generator of ( H, ∘)
:S generates ( H, ∘)
:( H, ∘) is the submonoid of ( M, ∘) generated by S.


This is written H = ⟨ S ⟩.


If S is a singleton, for example S = { x }, then we can (and usually do) write H = ⟨ x ⟩ for H = ⟨{ x }⟩.",Generator
"['Definitions/Generators of Groups', 'Definitions/Group Theory']",Definition:Generator,"Let $left( G, circ right)$ be a group.

Let $S subseteq G$.


Then $S$ is a generator of $G$, denoted $G = {leftlangle S rightrangle}$,  if and only if  $G$ is the subgroup generated by $S$.",Definition:Generator of Group,,false,"Let ( G, ∘) be a group.

Let S ⊆ G.


Then S is a generator of G, denoted G = ⟨ S ⟩,  if and only if  G is the subgroup generated by S.",Generator
['Definitions/Monoids'],Definition:Generator,"Let $left( M, circ right)$ be a monoid

Let $S subseteq M$.

Let $left( H, circ right)$ be the submonoid of $left( M, circ right)$ generated by $S$.

Then $S$ is known as a generator of $left( H, circ right)$.",Definition:Generated Submonoid/Generator,,false,"Let ( M, ∘) be a monoid

Let S ⊆ M.

Let ( H, ∘) be the submonoid of ( M, ∘) generated by S.

Then S is known as a generator of ( H, ∘).",Generator
"['Definitions/Group Theory', 'Definitions/Generators of Groups']",Definition:Generator,"Let $left( G, circ right)$ be a group.

Let $S subseteq G$.

Let $H$ be the subgroup generated by $S$.


Then $S$ is a generator of $H$, denoted $H = {leftlangle S rightrangle}$,  if and only if  $H$ is the subgroup generated by $S$.


=== Definition by Predicate ===
A generator of a subgroup can be defined by a predicate.

For example:

:${leftlangle x in G: x^2 = e rightrangle}$

defines the subgroup of $G$ generated by the elements of $G$ of order $2$.",Definition:Generator of Subgroup,,false,"Let ( G, ∘) be a group.

Let S ⊆ G.

Let H be the subgroup generated by S.


Then S is a generator of H, denoted H = ⟨ S ⟩,  if and only if  H is the subgroup generated by S.


=== Definition by Predicate ===
A generator of a subgroup can be defined by a predicate.

For example:

:⟨ x ∈ G: x^2 = e ⟩

defines the subgroup of G generated by the elements of G of order 2.",Generator
"['Definitions/Cyclic Groups', 'Definitions/Generators of Groups']",Definition:Generator,"Let $G$ be a cyclic group generated by the element $g$.

Let $a in G$ be an element of $G$ such that ${leftlangle a rightrangle} = G$.

Then $a$ is a generator of $G$.",Definition:Cyclic Group/Generator,,false,"Let G be a cyclic group generated by the element g.

Let a ∈ G be an element of G such that ⟨ a ⟩ = G.

Then a is a generator of G.",Generator
"['Definitions/Generators of Ideals', 'Definitions/Ideal Theory', 'Definitions/Ring Theory']",Definition:Generator,"Let $R$ be a commutative ring. 

Let $I subset R$ be an ideal.

Let $S subset I$ be a subset.


Then:
:$S$ is a generator of $I$
 if and only if :
:$I$ is the ideal generated by $S$.",Definition:Generator of Ideal of Ring,,false,"Let R be a commutative ring. 

Let I ⊂ R be an ideal.

Let S ⊂ I be a subset.


Then:
:S is a generator of I
 if and only if :
:I is the ideal generated by S.",Generator
"['Definitions/Generators of Modules', 'Definitions/Module Theory', 'Definitions/Linear Algebra']",Definition:Generator,"Let $R$ be a ring.

Let $M$ be an $R$-module.

Let $S subseteq M$ be a subset.


=== Definition 1 ===
Let $R$ be a ring.

Let $M$ be an $R$-module.

Let $S subseteq M$ be a subset.


$S$ is a generator of $M$  if and only if  $M$ is the submodule generated by $S$.

=== Definition 2 ===
Let $R$ be a ring.

Let $M$ be an $R$-module.

Let $S subseteq M$ be a subset.


$S$ is a generator of $M$  if and only if  $M$ has no proper submodule containing $S$.",Definition:Generator of Module,,false,"Let R be a ring.

Let M be an R-module.

Let S ⊆ M be a subset.


=== Definition 1 ===
Let R be a ring.

Let M be an R-module.

Let S ⊆ M be a subset.


S is a generator of M  if and only if  M is the submodule generated by S.

=== Definition 2 ===
Let R be a ring.

Let M be an R-module.

Let S ⊆ M be a subset.


S is a generator of M  if and only if  M has no proper submodule containing S.",Generator
['Definitions/Ring Theory'],Definition:Generator,"Let $R$ be a ring.

Let $S subset R$ be a subset.


Then $S$ is a generator of $R$  if and only if  $R$ is the subring generated by $S$.",Definition:Generator of Ring,,false,"Let R be a ring.

Let S ⊂ R be a subset.


Then S is a generator of R  if and only if  R is the subring generated by S.",Generator
['Definitions/Division Rings'],Definition:Generator,"Let $left( D, +, circ right)$ be a division ring.

Let $S subseteq D$.


The division subring generated by $S$ is the smallest division subring of $D$ containing $S$.",Definition:Generated Division Subring,,false,"Let ( D, +, ∘) be a division ring.

Let S ⊆ D.


The division subring generated by S is the smallest division subring of D containing S.",Generator
['Definitions/Field Theory'],Definition:Generator,"Let $F$ be a field.

Let $S subseteq F$ be a subset and $K le F$ a subfield.


The field generated by $S$ is the smallest subfield of $F$ containing $S$.

The subring of $F$ generated by $K cup S$, written $K left[ S right]$, is the smallest subring of $F$ containing $K cup S$.

The subfield of $F$ generated by $K cup S$, written $K left(   right)S$, is the smallest subfield of $F$ containing $K cup S$.",Definition:Generator of Field,,false,"Let F be a field.

Let S ⊆ F be a subset and K ≤ F a subfield.


The field generated by S is the smallest subfield of F containing S.

The subring of F generated by K ∪ S, written K [ S ], is the smallest subring of F containing K ∪ S.

The subfield of F generated by K ∪ S, written K (   )S, is the smallest subfield of F containing K ∪ S.",Generator
['Definitions/Field Extensions'],Definition:Generator,"Let $E / F$ be a field extension.

Let $S subset E$ be a subset of $E$.


=== Definition 1 ===
Let $E / F$ be a field extension.

Let $S subset E$ be a subset of $E$.


The field extension $F left[ S right]$ generated by $S$ is the smallest subfield extension of $E$ containing $S$, that is, the intersection of all subfields of $E$ containing $S$ and $F$.

Thus $S$ is a generator of $F left[ S right]$  if and only if  $F left[ S right]$ has no proper subfield extension containing $S$.

=== Definition 2 ===
Let $E / F$ be a field extension.

Let $S subset E$ be a subset of $E$.


Let $F left[ leftlbrace X_s rightrbrace  right]$ be the polynomial ring in $S$ variables $X_s$.

Let $operatorname {ev} : F left[ leftlbrace X_s rightrbrace  right] to E$ be the evaluation homomorphism associated to the inclusion $S hookrightarrow E$.


The field extension $F left[ S right]$ generated by $S$ is the set of all elements of $E$ of the form $operatorname {ev}  left(   right)f / operatorname {ev}  left(   right)g$, where $operatorname {ev}  left(   right)g ne 0$.

$S$ is said to be a generator of $F left[ S right]$.",Definition:Generated Field Extension,,false,"Let E / F be a field extension.

Let S ⊂ E be a subset of E.


=== Definition 1 ===
Let E / F be a field extension.

Let S ⊂ E be a subset of E.


The field extension F [ S ] generated by S is the smallest subfield extension of E containing S, that is, the intersection of all subfields of E containing S and F.

Thus S is a generator of F [ S ]  if and only if  F [ S ] has no proper subfield extension containing S.

=== Definition 2 ===
Let E / F be a field extension.

Let S ⊂ E be a subset of E.


Let F [ { X_s }] be the polynomial ring in S variables X_s.

Let ev : F [ { X_s }] → E be the evaluation homomorphism associated to the inclusion S ↪ E.


The field extension F [ S ] generated by S is the set of all elements of E of the form ev(   )f / ev(   )g, where ev(   )g  0.

S is said to be a generator of F [ S ].",Generator
['Definitions/Algebras'],Definition:Generator,"Let $left( A_R, oplus right)$ be an algebra over a ring $R$.

Let $S subseteq A_R$ be a subset of $A_R$.


The subalgebra generated by $S$ is the smallest subalgebra $B_R$ of $A_R$ which contains $S$.",Definition:Generator of Algebra,,false,"Let ( A_R, ⊕) be an algebra over a ring R.

Let S ⊆ A_R be a subset of A_R.


The subalgebra generated by S is the smallest subalgebra B_R of A_R which contains S.",Generator
"['Definitions/Generators of Vector Spaces', 'Definitions/Vector Spaces', 'Definitions/Linear Algebra']",Definition:Generator,"Let $K$ be a division ring.

Let $mathbf V$ be a vector space over $K$.

Let $S subseteq mathbf V$ be a subset of $mathbf V$.


$S$ is a generator of $mathbf V$  if and only if  every element of $mathbf V$ is a linear combination of elements of $S$.",Definition:Generator of Vector Space,,false,"Let K be a division ring.

Let 𝐕 be a vector space over K.

Let S ⊆𝐕 be a subset of 𝐕.


S is a generator of 𝐕  if and only if  every element of 𝐕 is a linear combination of elements of S.",Generator
"['Definitions/Solid Geometry', 'Definitions/Generatrices']",Definition:Generator,"A generatrix is an element of a set of straight lines which constitute a given surface.


=== Generatrix of Right Circular Cone ===


Let $K$ be a right circular cone.

Let $A$ be the apex of $K$.

Let $B$ be the base of $K$.


Then a line joining the apex of $K$ to its directrix is a generatrix of $K$.

=== Generatrix of Cylinder ===
Each of the parallel straight lines forming a cylindrical surface $S$ which pass through the directrix of $S$ is called a generatrix of $S$.",Definition:Generatrix,,false,"A generatrix is an element of a set of straight lines which constitute a given surface.


=== Generatrix of Right Circular Cone ===


Let K be a right circular cone.

Let A be the apex of K.

Let B be the base of K.


Then a line joining the apex of K to its directrix is a generatrix of K.

=== Generatrix of Cylinder ===
Each of the parallel straight lines forming a cylindrical surface S which pass through the directrix of S is called a generatrix of S.",Generator
"['Definitions/Genera of Surfaces', 'Definitions/Graph Theory', 'Definitions/Genera']",Definition:Genus,"Let $S$ be a surface.

Let $G = left( V, E right)$ be a graph which is embedded in $S$.

Let $G$ be such that each of its faces is a simple closed curve.

Let $chi left(   right)G = v - e + f = 2 - 2 p$ be the Euler characteristic of $G$ where:
:$v = leftlvert V rightrvert$ is the number of vertices
:$e = leftlvert E rightrvert$ is the number of edges
:$f$ is the number of faces.

Then $p$ is known as the genus of $S$.",Definition:Genus of Surface,,false,"Let S be a surface.

Let G = ( V, E ) be a graph which is embedded in S.

Let G be such that each of its faces is a simple closed curve.

Let χ(   )G = v - e + f = 2 - 2 p be the Euler characteristic of G where:
:v = | V | is the number of vertices
:e = | E | is the number of edges
:f is the number of faces.

Then p is known as the genus of S.",Genus
"['Definitions/Genera of Manifolds', 'Definitions/Topological Manifolds', 'Definitions/Genera']",Definition:Genus,The genus of a compact topological manifold is the number of handles it has.,Definition:Genus of Manifold,,false,The genus of a compact topological manifold is the number of handles it has.,Genus
"['Definitions/Genera of Riemann Surfaces', 'Definitions/Riemann Surfaces', 'Definitions/Genera']",Definition:Genus,The genus of a Riemann surface $R$ is the number of linearly independent holomorphic $1$-forms that are defined on $R$.,Definition:Genus of Riemann Surface,,false,The genus of a Riemann surface R is the number of linearly independent holomorphic 1-forms that are defined on R.,Genus
"['Definitions/Genera of Plane Algebraic Curves', 'Definitions/Algebraic Curves', 'Definitions/Genera']",Definition:Genus,"Let $mathcal C$ be a plane algebraic curve with no singular points.

The genus of $mathcal C$ is defined as:
:$dbinom {d - 1} 2$
where $d$ denotes the degree of $mathcal C$.


=== Singular Points ===
Let $mathcal C$ be a plane algebraic curve which has $1$ or more singular points.

The genus of $mathcal C$ is defined as:
:$dbinom {d - 2} 2 - sum delta$
where:
:$d$ denotes the degree of $mathcal C$
:each term of the summation corresponds to one of the singular points of $mathcal C$
:$delta$ is $1$ for a double point, but for more complicated singular points is larger.",Definition:Genus of Plane Algebraic Curve,,false,"Let 𝒞 be a plane algebraic curve with no singular points.

The genus of 𝒞 is defined as:
:d - 1 2
where d denotes the degree of 𝒞.


=== Singular Points ===
Let 𝒞 be a plane algebraic curve which has 1 or more singular points.

The genus of 𝒞 is defined as:
:d - 2 2 - ∑δ
where:
:d denotes the degree of 𝒞
:each term of the summation corresponds to one of the singular points of 𝒞
:δ is 1 for a double point, but for more complicated singular points is larger.",Genus
"['Definitions/Grads', 'Definitions/Angles', 'Definitions/Units of Measurement']",Definition:Grade,"The grad is a measurement of plane angle.

It is defined as $dfrac 1 {100}$ of a right angle.

 
 
 
 ",Definition:Grad,,false,"The grad is a measurement of plane angle.

It is defined as 1 100 of a right angle.

 
 
 
 ",Grade
"['Definitions/Slope', 'Definitions/Geometry', 'Definitions/Analytic Geometry']",Definition:Grade,"=== Straight Line ===
Let $mathcal L$ be a straight line embedded in a Cartesian plane.

The slope of $mathcal L$ is defined as the tangent of the angle that $mathcal L$ makes with the $x$-axis.


=== General Form ===
Let $mathcal L$ be a straight line embedded in a Cartesian plane.

Let $mathcal L$ be given by the equation:

:$l x + m y + n = 0$

The slope of $mathcal L$ is defined by means of the ordered pair $left( -l, m right)$, where:
:for $m ne 0$, $psi = arctan left(   right){-dfrac l m}$
:for $m = 0$, $psi = dfrac pi 2$
where $psi$ is the angle that $mathcal L$ makes with the $x$-axis.


Thus:
:when $m = 0$, the slope of $mathcal L$ is $left( l, 0 right)$ and $mathcal L$ is parallel to the $y$-axis
:when $l = 0$, the slope of $mathcal L$ is $left( 0, m right)$ and $mathcal L$ is parallel to the $x$-axis.

=== Curve ===
Let $P$ be a point on a curve $mathcal C$ embedded in a Cartesian plane.

The slope of $mathcal C$ at $P$ is defined as the slope of the tangent to $mathcal C$ at $P$.",Definition:Slope,,false,"=== Straight Line ===
Let ℒ be a straight line embedded in a Cartesian plane.

The slope of ℒ is defined as the tangent of the angle that ℒ makes with the x-axis.


=== General Form ===
Let ℒ be a straight line embedded in a Cartesian plane.

Let ℒ be given by the equation:

:l x + m y + n = 0

The slope of ℒ is defined by means of the ordered pair ( -l, m ), where:
:for m  0, ψ = arctan(   )- l m
:for m = 0, ψ = π 2
where ψ is the angle that ℒ makes with the x-axis.


Thus:
:when m = 0, the slope of ℒ is ( l, 0 ) and ℒ is parallel to the y-axis
:when l = 0, the slope of ℒ is ( 0, m ) and ℒ is parallel to the x-axis.

=== Curve ===
Let P be a point on a curve 𝒞 embedded in a Cartesian plane.

The slope of 𝒞 at P is defined as the slope of the tangent to 𝒞 at P.",Grade
"['Definitions/Graph Theory', 'Definitions/Topology', 'Definitions/Relation Theory', 'Definitions/Branches of Mathematics']",Definition:Graph,"Graph theory is the branch of mathematics concerned with the structure and properties of graphs.

As a (graph-theoretical) graph has the same conceptual definition as a relation, it follows that there is considerable overlap between the fields of graph theory and relation theory.",Definition:Graph Theory,mathematics,true,"Graph theory is the branch of mathematics concerned with the structure and properties of graphs.

As a (graph-theoretical) graph has the same conceptual definition as a relation, it follows that there is considerable overlap between the fields of graph theory and relation theory.",Graph
"['Definitions/Graphs (Graph Theory)', 'Definitions/Graph Theory']",Definition:Graph,"A graph is intuitively defined as a pair consisting of a set of vertices and a set of edges each with a vertex at each end.


=== Vertex ===
 

Let $G = left( V, E right)$ be a graph.

The vertices (singular: vertex) are the elements of $V$.

Informally, the vertices are the points that are connected by the edges.


In the above, the vertices are the points $A, B, C, D, E, F, G$ which are marked as dots.

=== Edge ===
 


Let $G = left( V, E right)$ be a graph.

The edges are the elements of $E$.


In the above, the edges are $AB, AE, BE, CD, CE, CF, DE, DF, FG$.


=== Join ===
Let $G = left( V, E right)$ be a graph.

Let $u$ and $v$ be vertices of $G$.

Let $e = u v$ be an edge of $G$.


Then $e$ joins the vertices $u$ and $v$.",Definition:Graph (Graph Theory),pair,true,"A graph is intuitively defined as a pair consisting of a set of vertices and a set of edges each with a vertex at each end.


=== Vertex ===
 

Let G = ( V, E ) be a graph.

The vertices (singular: vertex) are the elements of V.

Informally, the vertices are the points that are connected by the edges.


In the above, the vertices are the points A, B, C, D, E, F, G which are marked as dots.

=== Edge ===
 


Let G = ( V, E ) be a graph.

The edges are the elements of E.


In the above, the edges are AB, AE, BE, CD, CE, CF, DE, DF, FG.


=== Join ===
Let G = ( V, E ) be a graph.

Let u and v be vertices of G.

Let e = u v be an edge of G.


Then e joins the vertices u and v.",Graph
['Definitions/Category Theory'],Definition:Graph,"A graph is an interpretation of a metagraph within set theory.


Let $mathfrak U$ be a class of sets.

A metagraph $mathcal G$ is a graph  if and only if :

:$(1): quad$ The objects form a subset $operatorname{vert} mathcal G subseteq mathfrak U$

:$(2): quad$ The morphisms form a subset $operatorname{edge} mathcal G subseteq mathfrak U$

 

If the class $mathfrak U$ is a set, then morphisms are functions, and the domain and codomain in the definition of a morphism are those familiar from set theory.

If $mathfrak U$ is a proper class this is not the case, for example the morphisms of $mathcal C$ need not be functions.",Definition:Graph (Category Theory),interpretation,true,"A graph is an interpretation of a metagraph within set theory.


Let 𝔘 be a class of sets.

A metagraph 𝒢 is a graph  if and only if :

:(1): The objects form a subset vert𝒢⊆𝔘

:(2): The morphisms form a subset edge𝒢⊆𝔘

 

If the class 𝔘 is a set, then morphisms are functions, and the domain and codomain in the definition of a morphism are those familiar from set theory.

If 𝔘 is a proper class this is not the case, for example the morphisms of 𝒞 need not be functions.",Graph
"['Definitions/Graphs of Mappings', 'Definitions/Graphs of Relations', 'Definitions/Real Functions', 'Definitions/Mapping Theory']",Definition:Graph,"Let $S$ and $T$ be sets.

Let $f: S to T$ be a mapping.


The graph of $f$ is the relation $mathcal R subseteq S times T$ defined as $mathcal R = leftlbrace left( x, f left(   right)x right): x in S rightrbrace$


Alternatively, this can be expressed:
:$G_f = leftlbrace left( s, t right) in S times T: f left(   right)s = t rightrbrace$
where $G_f$ is the graph of $f$.


The word is usually used in the context of a diagram:


:

=== Graph of Real Function ===
Let $U subseteq mathbb R^n$ be an open subset of $n$-dimensional Euclidean space.

Let $f : U to mathbb R^k$ be a real function.


The graph $Gamma left(   right)f$ of the function $f$ is the subset of $mathbb R^n times mathbb R^k$ such that:

:$Gamma left(   right)f = leftlbrace left( x, y right) in mathbb R^n times mathbb R^k: x in U subseteq mathbb R^n : f left(   right)x = y rightrbrace$

where $times$ denotes the Cartesian product.

=== Graph of Relation ===

The concept can still be applied when $f$ is a relation, but in this case a vertical line through a point in the graph is not guaranteed to intersect the graph at one and only one point.

Let $S times T$ be the cartesian product of two sets $S$ and $T$.

Let $mathcal R$ be a relation on $S times T$.


The graph of $mathcal R$ is the set of all ordered pairs $left( s, t right)$ of $S times T$ such that $s mathrel mathcal R t$:
:$mathcal T left(   right)mathcal R = leftlbrace left( s, t right): s mathrel mathcal R t rightrbrace$",Definition:Graph of Mapping,,false,"Let S and T be sets.

Let f: S → T be a mapping.


The graph of f is the relation ℛ⊆ S × T defined as ℛ = {( x, f (   )x ): x ∈ S }


Alternatively, this can be expressed:
:G_f = {( s, t ) ∈ S × T: f (   )s = t }
where G_f is the graph of f.


The word is usually used in the context of a diagram:


:

=== Graph of Real Function ===
Let U ⊆ℝ^n be an open subset of n-dimensional Euclidean space.

Let f : U →ℝ^k be a real function.


The graph Γ(   )f of the function f is the subset of ℝ^n ×ℝ^k such that:

:Γ(   )f = {( x, y ) ∈ℝ^n ×ℝ^k: x ∈ U ⊆ℝ^n : f (   )x = y }

where × denotes the Cartesian product.

=== Graph of Relation ===

The concept can still be applied when f is a relation, but in this case a vertical line through a point in the graph is not guaranteed to intersect the graph at one and only one point.

Let S × T be the cartesian product of two sets S and T.

Let ℛ be a relation on S × T.


The graph of ℛ is the set of all ordered pairs ( s, t ) of S × T such that s ℛ t:
:𝒯(   )ℛ = {( s, t ): s ℛ t }",Graph
"['Definitions/Real Functions', 'Definitions/Mapping Theory']",Definition:Graph,"Let $U subseteq mathbb R^n$ be an open subset of $n$-dimensional Euclidean space.

Let $f : U to mathbb R^k$ be a real function.


The graph $Gamma left(   right)f$ of the function $f$ is the subset of $mathbb R^n times mathbb R^k$ such that:

:$Gamma left(   right)f = leftlbrace left( x, y right) in mathbb R^n times mathbb R^k: x in U subseteq mathbb R^n : f left(   right)x = y rightrbrace$

where $times$ denotes the Cartesian product.",Definition:Graph of Real Function,,false,"Let U ⊆ℝ^n be an open subset of n-dimensional Euclidean space.

Let f : U →ℝ^k be a real function.


The graph Γ(   )f of the function f is the subset of ℝ^n ×ℝ^k such that:

:Γ(   )f = {( x, y ) ∈ℝ^n ×ℝ^k: x ∈ U ⊆ℝ^n : f (   )x = y }

where × denotes the Cartesian product.",Graph
"['Definitions/Graphs of Relations', 'Definitions/Relations']",Definition:Graph,"Let $S times T$ be the cartesian product of two sets $S$ and $T$.

Let $mathcal R$ be a relation on $S times T$.


The graph of $mathcal R$ is the set of all ordered pairs $left( s, t right)$ of $S times T$ such that $s mathrel mathcal R t$:
:$mathcal T left(   right)mathcal R = leftlbrace left( s, t right): s mathrel mathcal R t rightrbrace$",Definition:Relation/Graph,,false,"Let S × T be the cartesian product of two sets S and T.

Let ℛ be a relation on S × T.


The graph of ℛ is the set of all ordered pairs ( s, t ) of S × T such that s ℛ t:
:𝒯(   )ℛ = {( s, t ): s ℛ t }",Graph
"['Definitions/Statistics', 'Definitions/Diagrams', 'Definitions/Graphs (Statistics)']",Definition:Graph,"In the context of statistics, a graph is a word that loosely means a pictorial or diagrammatic presentation of a set of data.


=== Bar Chart ===
A bar chart is a form of graph which consists of a finite set of (usually) vertical bars whose length determines the statistic being communicated.


:

=== Pie Chart ===
A pie chart is a form of graph which consists of a circle divided into sectors whose areas represent the proportion of the corresponding statistic relative to the whole.


:

=== Pictograph ===
A pictograph is a variant of a bar chart whose  bars consist of a number of icons or other pictorial symbols arranged in a (usually) line.


:$begin {array} {r|l} text {Happy} & 😊😊😊😊😊😊 \ text {Sad} & 😢😢😢 \ text {Unimpressed} & 😒😒😒😒😒😒😒😒 \ text {Disgusted} & 🤢🤢🤢🤢 \ end {array}$",Definition:Graph (Statistics),word,true,"In the context of statistics, a graph is a word that loosely means a pictorial or diagrammatic presentation of a set of data.


=== Bar Chart ===
A bar chart is a form of graph which consists of a finite set of (usually) vertical bars whose length determines the statistic being communicated.


:

=== Pie Chart ===
A pie chart is a form of graph which consists of a circle divided into sectors whose areas represent the proportion of the corresponding statistic relative to the whole.


:

=== Pictograph ===
A pictograph is a variant of a bar chart whose  bars consist of a number of icons or other pictorial symbols arranged in a (usually) line.


:[       Happy      😊😊😊😊😊😊;         Sad         😢😢😢; Unimpressed    😒😒😒😒😒😒😒😒;   Disgusted        🤢🤢🤢🤢;             ]",Graph
"['Definitions/Harmonic Mean', 'Definitions/Pythagorean Means', 'Definitions/Measures of Central Tendency', 'Definitions/Algebra', 'Definitions/Number Theory', 'Definitions/Analysis']",Definition:Harmonic,"Let $x_1, x_2, ldots, x_n in mathbb R$ be real numbers which are all strictly positive.

The harmonic mean $H_n$ of $x_1, x_2, ldots, x_n$ is defined as:

:$ds dfrac 1 {H_n} := frac 1 n left( sum_{k mathop = 1}^n frac 1 {x_k}  right)$

That is, to find the harmonic mean of a set of $n$ numbers, take the reciprocal of the arithmetic mean of their reciprocals.",Definition:Harmonic Mean,,false,"Let x_1, x_2, …, x_n ∈ℝ be real numbers which are all strictly positive.

The harmonic mean H_n of x_1, x_2, …, x_n is defined as:

:1 H_n := 1/n( ∑_k  = 1^n 1/x_k)

That is, to find the harmonic mean of a set of n numbers, take the reciprocal of the arithmetic mean of their reciprocals.",Harmonic
"['Definitions/Harmonic Numbers', 'Definitions/Discrete Mathematics', 'Definitions/Number Theory', 'Definitions/Real Analysis']",Definition:Harmonic,"The harmonic numbers are denoted $H_n$ and are defined for positive integers $n$:
:$ds forall n in mathbb Z, n ge 0: H_n = sum_{k mathop = 1}^n frac 1 k$

From the definition of vacuous summation it is clear that $H_0 = 0$.

=== General Harmonic Numbers ===
Let $r in mathbb R_{>0}$.

For $n in mathbb N_{> 0}$ the harmonic numbers order $r$ are defined as follows:
:$ds H^{left( r right)}  left(   right)n = sum_{k mathop = 1}^n frac 1 {k^r}$


=== Complex Extension ===
Let $r in mathbb R_{>0}$.

For $z in mathbb C setminus mathbb Z_{< 0}$ the harmonic numbers order $r$ can be extended to the complex plane as:
:$ds H^{left( r right) }  left(   right){ }z = sum_{k mathop = 1}^{infty} left( frac 1 {k^r} - frac 1 {left( k + z right)^r}  right)$

 ",Definition:Harmonic Numbers,,false,"The harmonic numbers are denoted H_n and are defined for positive integers n:
:∀ n ∈ℤ, n ≥ 0: H_n = ∑_k  = 1^n 1/k

From the definition of vacuous summation it is clear that H_0 = 0.

=== General Harmonic Numbers ===
Let r ∈ℝ_>0.

For n ∈ℕ_> 0 the harmonic numbers order r are defined as follows:
:H^( r )(   )n = ∑_k  = 1^n 1/k^r


=== Complex Extension ===
Let r ∈ℝ_>0.

For z ∈ℂ∖ℤ_< 0 the harmonic numbers order r can be extended to the complex plane as:
:H^( r ) (   )z = ∑_k  = 1^∞( 1/k^r - 1/( k + z )^r)

 ",Harmonic
"['Definitions/General Harmonic Numbers', 'Definitions/Harmonic Numbers', 'Definitions/P-Series', 'Definitions/Riemann Zeta Function']",Definition:Harmonic,"Let $r in mathbb R_{>0}$.

For $n in mathbb N_{> 0}$ the harmonic numbers order $r$ are defined as follows:
:$ds H^{left( r right)}  left(   right)n = sum_{k mathop = 1}^n frac 1 {k^r}$


=== Complex Extension ===
Let $r in mathbb R_{>0}$.

For $z in mathbb C setminus mathbb Z_{< 0}$ the harmonic numbers order $r$ can be extended to the complex plane as:
:$ds H^{left( r right) }  left(   right){ }z = sum_{k mathop = 1}^{infty} left( frac 1 {k^r} - frac 1 {left( k + z right)^r}  right)$

 ",Definition:Harmonic Numbers/General Definition,,false,"Let r ∈ℝ_>0.

For n ∈ℕ_> 0 the harmonic numbers order r are defined as follows:
:H^( r )(   )n = ∑_k  = 1^n 1/k^r


=== Complex Extension ===
Let r ∈ℝ_>0.

For z ∈ℂ∖ℤ_< 0 the harmonic numbers order r can be extended to the complex plane as:
:H^( r ) (   )z = ∑_k  = 1^∞( 1/k^r - 1/( k + z )^r)

 ",Harmonic
"['Definitions/Ore Numbers', 'Definitions/Number Theory']",Definition:Harmonic,"Let $n in mathbb Z_{>0}$ be a positive integer.

$n$ is an Ore number  if and only if  the harmonic mean of its divisors is an integer.",Definition:Ore Number,,false,"Let n ∈ℤ_>0 be a positive integer.

n is an Ore number  if and only if  the harmonic mean of its divisors is an integer.",Harmonic
"['Definitions/Binomial Coefficients', 'Definitions/Discrete Mathematics', 'Definitions/Leibniz Harmonic Triangle']",Definition:Harmonic,"The Leibniz Harmonic Triangle is a triangular array where:
:the zeroth element in the $n$th row, counting from $0$, is $dfrac 1 {n + 1}$
:subsequent elements in the $n$th row are the zeroth element divided by the corresponding element of Pascal's triangle:


:$begin{array}{r|rrrrrr}
n & 0 & 1 & 2 & 3 & 4 & 5 \
hline
0  & frac 1 1 \
1  & frac 1 2 & frac 1 2 \
2  & frac 1 3 & frac 1 6 &  frac 1 3 \
3  & frac 1 4 & frac 1 {12} & frac 1 {12} & frac 1 4 \
4  & frac 1 5 & frac 1 {20} & frac 1 {30} & frac 1 {20} & frac 1 5 \
5  & frac 1 6 & frac 1 {30} & frac 1 {60} & frac 1 {60} & frac 1 {30} & frac 1 6 \
end{array}$


=== Row ===
Consider the Leibniz harmonic triangle:
The Leibniz Harmonic Triangle is a triangular array where:
:the zeroth element in the $n$th row, counting from $0$, is $dfrac 1 {n + 1}$
:subsequent elements in the $n$th row are the zeroth element divided by the corresponding element of Pascal's triangle:


:$begin{array}{r|rrrrrr}
n & 0 & 1 & 2 & 3 & 4 & 5 \
hline
0  & frac 1 1 \
1  & frac 1 2 & frac 1 2 \
2  & frac 1 3 & frac 1 6 &  frac 1 3 \
3  & frac 1 4 & frac 1 {12} & frac 1 {12} & frac 1 4 \
4  & frac 1 5 & frac 1 {20} & frac 1 {30} & frac 1 {20} & frac 1 5 \
5  & frac 1 6 & frac 1 {30} & frac 1 {60} & frac 1 {60} & frac 1 {30} & frac 1 6 \
end{array}$


=== Row ===
Consider the Leibniz harmonic triangle:
The Leibniz Harmonic Triangle is a triangular array where:
:the zeroth element in the $n$th row, counting from $0$, is $dfrac 1 {n + 1}$
:subsequent elements in the $n$th row are the zeroth element divided by the corresponding element of Pascal's triangle:


:$begin{array}{r|rrrrrr}
n & 0 & 1 & 2 & 3 & 4 & 5 \
hline
0  & frac 1 1 \
1  & frac 1 2 & frac 1 2 \
2  & frac 1 3 & frac 1 6 &  frac 1 3 \
3  & frac 1 4 & frac 1 {12} & frac 1 {12} & frac 1 4 \
4  & frac 1 5 & frac 1 {20} & frac 1 {30} & frac 1 {20} & frac 1 5 \
5  & frac 1 6 & frac 1 {30} & frac 1 {60} & frac 1 {60} & frac 1 {30} & frac 1 6 \
end{array}$


=== Row ===


=== Column ===


=== Diagonal ===


Each of the horizontal lines of numbers corresponding to a given $n$ is known as the $n$th row of Leibniz harmonic triangle.

Hence the top row, containing a single $1$, is identified as the zeroth row, or row $0$.

=== Column ===
Consider the Leibniz harmonic triangle:
The Leibniz Harmonic Triangle is a triangular array where:
:the zeroth element in the $n$th row, counting from $0$, is $dfrac 1 {n + 1}$
:subsequent elements in the $n$th row are the zeroth element divided by the corresponding element of Pascal's triangle:


:$begin{array}{r|rrrrrr}
n & 0 & 1 & 2 & 3 & 4 & 5 \
hline
0  & frac 1 1 \
1  & frac 1 2 & frac 1 2 \
2  & frac 1 3 & frac 1 6 &  frac 1 3 \
3  & frac 1 4 & frac 1 {12} & frac 1 {12} & frac 1 4 \
4  & frac 1 5 & frac 1 {20} & frac 1 {30} & frac 1 {20} & frac 1 5 \
5  & frac 1 6 & frac 1 {30} & frac 1 {60} & frac 1 {60} & frac 1 {30} & frac 1 6 \
end{array}$


=== Row ===


=== Column ===


=== Diagonal ===


Each of the vertical lines of numbers is known as the $m$th column of Leibniz harmonic triangle.

The leftmost column, containing the reciprocals of the non-negative integers, is identified as the zeroth column, or column $0$.

=== Diagonal ===
Consider the Leibniz harmonic triangle:
The Leibniz Harmonic Triangle is a triangular array where:
:the zeroth element in the $n$th row, counting from $0$, is $dfrac 1 {n + 1}$
:subsequent elements in the $n$th row are the zeroth element divided by the corresponding element of Pascal's triangle:


:$begin{array}{r|rrrrrr}
n & 0 & 1 & 2 & 3 & 4 & 5 \
hline
0  & frac 1 1 \
1  & frac 1 2 & frac 1 2 \
2  & frac 1 3 & frac 1 6 &  frac 1 3 \
3  & frac 1 4 & frac 1 {12} & frac 1 {12} & frac 1 4 \
4  & frac 1 5 & frac 1 {20} & frac 1 {30} & frac 1 {20} & frac 1 5 \
5  & frac 1 6 & frac 1 {30} & frac 1 {60} & frac 1 {60} & frac 1 {30} & frac 1 6 \
end{array}$


=== Row ===


=== Column ===


=== Diagonal ===


The $n$th diagonal of Leibniz harmonic triangle consists of the entries in row $n + m$ and column $m$ for $m ge 0$:
:$left({n, 0}right), left({n + 1, 1}right), left({n + 2, 2}right), ldots$

Hence the diagonal leading down and to the right from $left({0, 0}right)$, containing  the reciprocals of the non-negative integers, is identified as the zeroth diagonal, or diagonal $0$.

Each of the horizontal lines of numbers corresponding to a given $n$ is known as the $n$th row of Leibniz harmonic triangle.

Hence the top row, containing a single $1$, is identified as the zeroth row, or row $0$.

=== Column ===
Consider the Leibniz harmonic triangle:
The Leibniz Harmonic Triangle is a triangular array where:
:the zeroth element in the $n$th row, counting from $0$, is $dfrac 1 {n + 1}$
:subsequent elements in the $n$th row are the zeroth element divided by the corresponding element of Pascal's triangle:


:$begin{array}{r|rrrrrr}
n & 0 & 1 & 2 & 3 & 4 & 5 \
hline
0  & frac 1 1 \
1  & frac 1 2 & frac 1 2 \
2  & frac 1 3 & frac 1 6 &  frac 1 3 \
3  & frac 1 4 & frac 1 {12} & frac 1 {12} & frac 1 4 \
4  & frac 1 5 & frac 1 {20} & frac 1 {30} & frac 1 {20} & frac 1 5 \
5  & frac 1 6 & frac 1 {30} & frac 1 {60} & frac 1 {60} & frac 1 {30} & frac 1 6 \
end{array}$


=== Row ===
Consider the Leibniz harmonic triangle:
The Leibniz Harmonic Triangle is a triangular array where:
:the zeroth element in the $n$th row, counting from $0$, is $dfrac 1 {n + 1}$
:subsequent elements in the $n$th row are the zeroth element divided by the corresponding element of Pascal's triangle:


:$begin{array}{r|rrrrrr}
n & 0 & 1 & 2 & 3 & 4 & 5 \
hline
0  & frac 1 1 \
1  & frac 1 2 & frac 1 2 \
2  & frac 1 3 & frac 1 6 &  frac 1 3 \
3  & frac 1 4 & frac 1 {12} & frac 1 {12} & frac 1 4 \
4  & frac 1 5 & frac 1 {20} & frac 1 {30} & frac 1 {20} & frac 1 5 \
5  & frac 1 6 & frac 1 {30} & frac 1 {60} & frac 1 {60} & frac 1 {30} & frac 1 6 \
end{array}$


=== Row ===


=== Column ===


=== Diagonal ===


Each of the horizontal lines of numbers corresponding to a given $n$ is known as the $n$th row of Leibniz harmonic triangle.

Hence the top row, containing a single $1$, is identified as the zeroth row, or row $0$.

=== Column ===
Consider the Leibniz harmonic triangle:
The Leibniz Harmonic Triangle is a triangular array where:
:the zeroth element in the $n$th row, counting from $0$, is $dfrac 1 {n + 1}$
:subsequent elements in the $n$th row are the zeroth element divided by the corresponding element of Pascal's triangle:


:$begin{array}{r|rrrrrr}
n & 0 & 1 & 2 & 3 & 4 & 5 \
hline
0  & frac 1 1 \
1  & frac 1 2 & frac 1 2 \
2  & frac 1 3 & frac 1 6 &  frac 1 3 \
3  & frac 1 4 & frac 1 {12} & frac 1 {12} & frac 1 4 \
4  & frac 1 5 & frac 1 {20} & frac 1 {30} & frac 1 {20} & frac 1 5 \
5  & frac 1 6 & frac 1 {30} & frac 1 {60} & frac 1 {60} & frac 1 {30} & frac 1 6 \
end{array}$


=== Row ===


=== Column ===


=== Diagonal ===


Each of the vertical lines of numbers is known as the $m$th column of Leibniz harmonic triangle.

The leftmost column, containing the reciprocals of the non-negative integers, is identified as the zeroth column, or column $0$.

=== Diagonal ===
Consider the Leibniz harmonic triangle:
The Leibniz Harmonic Triangle is a triangular array where:
:the zeroth element in the $n$th row, counting from $0$, is $dfrac 1 {n + 1}$
:subsequent elements in the $n$th row are the zeroth element divided by the corresponding element of Pascal's triangle:


:$begin{array}{r|rrrrrr}
n & 0 & 1 & 2 & 3 & 4 & 5 \
hline
0  & frac 1 1 \
1  & frac 1 2 & frac 1 2 \
2  & frac 1 3 & frac 1 6 &  frac 1 3 \
3  & frac 1 4 & frac 1 {12} & frac 1 {12} & frac 1 4 \
4  & frac 1 5 & frac 1 {20} & frac 1 {30} & frac 1 {20} & frac 1 5 \
5  & frac 1 6 & frac 1 {30} & frac 1 {60} & frac 1 {60} & frac 1 {30} & frac 1 6 \
end{array}$


=== Row ===


=== Column ===


=== Diagonal ===


The $n$th diagonal of Leibniz harmonic triangle consists of the entries in row $n + m$ and column $m$ for $m ge 0$:
:$left({n, 0}right), left({n + 1, 1}right), left({n + 2, 2}right), ldots$

Hence the diagonal leading down and to the right from $left({0, 0}right)$, containing  the reciprocals of the non-negative integers, is identified as the zeroth diagonal, or diagonal $0$.

Each of the vertical lines of numbers is known as the $m$th column of Leibniz harmonic triangle.

The leftmost column, containing the reciprocals of the non-negative integers, is identified as the zeroth column, or column $0$.

=== Diagonal ===
Consider the Leibniz harmonic triangle:
The Leibniz Harmonic Triangle is a triangular array where:
:the zeroth element in the $n$th row, counting from $0$, is $dfrac 1 {n + 1}$
:subsequent elements in the $n$th row are the zeroth element divided by the corresponding element of Pascal's triangle:


:$begin{array}{r|rrrrrr}
n & 0 & 1 & 2 & 3 & 4 & 5 \
hline
0  & frac 1 1 \
1  & frac 1 2 & frac 1 2 \
2  & frac 1 3 & frac 1 6 &  frac 1 3 \
3  & frac 1 4 & frac 1 {12} & frac 1 {12} & frac 1 4 \
4  & frac 1 5 & frac 1 {20} & frac 1 {30} & frac 1 {20} & frac 1 5 \
5  & frac 1 6 & frac 1 {30} & frac 1 {60} & frac 1 {60} & frac 1 {30} & frac 1 6 \
end{array}$


=== Row ===
Consider the Leibniz harmonic triangle:
The Leibniz Harmonic Triangle is a triangular array where:
:the zeroth element in the $n$th row, counting from $0$, is $dfrac 1 {n + 1}$
:subsequent elements in the $n$th row are the zeroth element divided by the corresponding element of Pascal's triangle:


:$begin{array}{r|rrrrrr}
n & 0 & 1 & 2 & 3 & 4 & 5 \
hline
0  & frac 1 1 \
1  & frac 1 2 & frac 1 2 \
2  & frac 1 3 & frac 1 6 &  frac 1 3 \
3  & frac 1 4 & frac 1 {12} & frac 1 {12} & frac 1 4 \
4  & frac 1 5 & frac 1 {20} & frac 1 {30} & frac 1 {20} & frac 1 5 \
5  & frac 1 6 & frac 1 {30} & frac 1 {60} & frac 1 {60} & frac 1 {30} & frac 1 6 \
end{array}$


=== Row ===


=== Column ===


=== Diagonal ===


Each of the horizontal lines of numbers corresponding to a given $n$ is known as the $n$th row of Leibniz harmonic triangle.

Hence the top row, containing a single $1$, is identified as the zeroth row, or row $0$.

=== Column ===
Consider the Leibniz harmonic triangle:
The Leibniz Harmonic Triangle is a triangular array where:
:the zeroth element in the $n$th row, counting from $0$, is $dfrac 1 {n + 1}$
:subsequent elements in the $n$th row are the zeroth element divided by the corresponding element of Pascal's triangle:


:$begin{array}{r|rrrrrr}
n & 0 & 1 & 2 & 3 & 4 & 5 \
hline
0  & frac 1 1 \
1  & frac 1 2 & frac 1 2 \
2  & frac 1 3 & frac 1 6 &  frac 1 3 \
3  & frac 1 4 & frac 1 {12} & frac 1 {12} & frac 1 4 \
4  & frac 1 5 & frac 1 {20} & frac 1 {30} & frac 1 {20} & frac 1 5 \
5  & frac 1 6 & frac 1 {30} & frac 1 {60} & frac 1 {60} & frac 1 {30} & frac 1 6 \
end{array}$


=== Row ===


=== Column ===


=== Diagonal ===


Each of the vertical lines of numbers is known as the $m$th column of Leibniz harmonic triangle.

The leftmost column, containing the reciprocals of the non-negative integers, is identified as the zeroth column, or column $0$.

=== Diagonal ===
Consider the Leibniz harmonic triangle:
The Leibniz Harmonic Triangle is a triangular array where:
:the zeroth element in the $n$th row, counting from $0$, is $dfrac 1 {n + 1}$
:subsequent elements in the $n$th row are the zeroth element divided by the corresponding element of Pascal's triangle:


:$begin{array}{r|rrrrrr}
n & 0 & 1 & 2 & 3 & 4 & 5 \
hline
0  & frac 1 1 \
1  & frac 1 2 & frac 1 2 \
2  & frac 1 3 & frac 1 6 &  frac 1 3 \
3  & frac 1 4 & frac 1 {12} & frac 1 {12} & frac 1 4 \
4  & frac 1 5 & frac 1 {20} & frac 1 {30} & frac 1 {20} & frac 1 5 \
5  & frac 1 6 & frac 1 {30} & frac 1 {60} & frac 1 {60} & frac 1 {30} & frac 1 6 \
end{array}$


=== Row ===


=== Column ===


=== Diagonal ===


The $n$th diagonal of Leibniz harmonic triangle consists of the entries in row $n + m$ and column $m$ for $m ge 0$:
:$left({n, 0}right), left({n + 1, 1}right), left({n + 2, 2}right), ldots$

Hence the diagonal leading down and to the right from $left({0, 0}right)$, containing  the reciprocals of the non-negative integers, is identified as the zeroth diagonal, or diagonal $0$.

The $n$th diagonal of Leibniz harmonic triangle consists of the entries in row $n + m$ and column $m$ for $m ge 0$:
:$left({n, 0}right), left({n + 1, 1}right), left({n + 2, 2}right), ldots$

Hence the diagonal leading down and to the right from $left({0, 0}right)$, containing  the reciprocals of the non-negative integers, is identified as the zeroth diagonal, or diagonal $0$.",Definition:Leibniz Harmonic Triangle,,false,"The Leibniz Harmonic Triangle is a triangular array where:
:the zeroth element in the nth row, counting from 0, is 1 n + 1
:subsequent elements in the nth row are the zeroth element divided by the corresponding element of Pascal's triangle:


:[    n    0    1    2    3    4    5;    0  1/1;    1  1/2  1/2;    2  1/3  1/6  1/3;    3  1/4 1/12 1/12  1/4;    4  1/5 1/20 1/30 1/20  1/5;    5  1/6 1/30 1/60 1/60 1/30  1/6;      ]


=== Row ===
Consider the Leibniz harmonic triangle:
The Leibniz Harmonic Triangle is a triangular array where:
:the zeroth element in the nth row, counting from 0, is 1 n + 1
:subsequent elements in the nth row are the zeroth element divided by the corresponding element of Pascal's triangle:


:[    n    0    1    2    3    4    5;    0  1/1;    1  1/2  1/2;    2  1/3  1/6  1/3;    3  1/4 1/12 1/12  1/4;    4  1/5 1/20 1/30 1/20  1/5;    5  1/6 1/30 1/60 1/60 1/30  1/6;      ]


=== Row ===
Consider the Leibniz harmonic triangle:
The Leibniz Harmonic Triangle is a triangular array where:
:the zeroth element in the nth row, counting from 0, is 1 n + 1
:subsequent elements in the nth row are the zeroth element divided by the corresponding element of Pascal's triangle:


:[    n    0    1    2    3    4    5;    0  1/1;    1  1/2  1/2;    2  1/3  1/6  1/3;    3  1/4 1/12 1/12  1/4;    4  1/5 1/20 1/30 1/20  1/5;    5  1/6 1/30 1/60 1/60 1/30  1/6;      ]


=== Row ===


=== Column ===


=== Diagonal ===


Each of the horizontal lines of numbers corresponding to a given n is known as the nth row of Leibniz harmonic triangle.

Hence the top row, containing a single 1, is identified as the zeroth row, or row 0.

=== Column ===
Consider the Leibniz harmonic triangle:
The Leibniz Harmonic Triangle is a triangular array where:
:the zeroth element in the nth row, counting from 0, is 1 n + 1
:subsequent elements in the nth row are the zeroth element divided by the corresponding element of Pascal's triangle:


:[    n    0    1    2    3    4    5;    0  1/1;    1  1/2  1/2;    2  1/3  1/6  1/3;    3  1/4 1/12 1/12  1/4;    4  1/5 1/20 1/30 1/20  1/5;    5  1/6 1/30 1/60 1/60 1/30  1/6;      ]


=== Row ===


=== Column ===


=== Diagonal ===


Each of the vertical lines of numbers is known as the mth column of Leibniz harmonic triangle.

The leftmost column, containing the reciprocals of the non-negative integers, is identified as the zeroth column, or column 0.

=== Diagonal ===
Consider the Leibniz harmonic triangle:
The Leibniz Harmonic Triangle is a triangular array where:
:the zeroth element in the nth row, counting from 0, is 1 n + 1
:subsequent elements in the nth row are the zeroth element divided by the corresponding element of Pascal's triangle:


:[    n    0    1    2    3    4    5;    0  1/1;    1  1/2  1/2;    2  1/3  1/6  1/3;    3  1/4 1/12 1/12  1/4;    4  1/5 1/20 1/30 1/20  1/5;    5  1/6 1/30 1/60 1/60 1/30  1/6;      ]


=== Row ===


=== Column ===


=== Diagonal ===


The nth diagonal of Leibniz harmonic triangle consists of the entries in row n + m and column m for m ≥ 0:
:(n, 0), (n + 1, 1), (n + 2, 2), …

Hence the diagonal leading down and to the right from (0, 0), containing  the reciprocals of the non-negative integers, is identified as the zeroth diagonal, or diagonal 0.

Each of the horizontal lines of numbers corresponding to a given n is known as the nth row of Leibniz harmonic triangle.

Hence the top row, containing a single 1, is identified as the zeroth row, or row 0.

=== Column ===
Consider the Leibniz harmonic triangle:
The Leibniz Harmonic Triangle is a triangular array where:
:the zeroth element in the nth row, counting from 0, is 1 n + 1
:subsequent elements in the nth row are the zeroth element divided by the corresponding element of Pascal's triangle:


:[    n    0    1    2    3    4    5;    0  1/1;    1  1/2  1/2;    2  1/3  1/6  1/3;    3  1/4 1/12 1/12  1/4;    4  1/5 1/20 1/30 1/20  1/5;    5  1/6 1/30 1/60 1/60 1/30  1/6;      ]


=== Row ===
Consider the Leibniz harmonic triangle:
The Leibniz Harmonic Triangle is a triangular array where:
:the zeroth element in the nth row, counting from 0, is 1 n + 1
:subsequent elements in the nth row are the zeroth element divided by the corresponding element of Pascal's triangle:


:[    n    0    1    2    3    4    5;    0  1/1;    1  1/2  1/2;    2  1/3  1/6  1/3;    3  1/4 1/12 1/12  1/4;    4  1/5 1/20 1/30 1/20  1/5;    5  1/6 1/30 1/60 1/60 1/30  1/6;      ]


=== Row ===


=== Column ===


=== Diagonal ===


Each of the horizontal lines of numbers corresponding to a given n is known as the nth row of Leibniz harmonic triangle.

Hence the top row, containing a single 1, is identified as the zeroth row, or row 0.

=== Column ===
Consider the Leibniz harmonic triangle:
The Leibniz Harmonic Triangle is a triangular array where:
:the zeroth element in the nth row, counting from 0, is 1 n + 1
:subsequent elements in the nth row are the zeroth element divided by the corresponding element of Pascal's triangle:


:[    n    0    1    2    3    4    5;    0  1/1;    1  1/2  1/2;    2  1/3  1/6  1/3;    3  1/4 1/12 1/12  1/4;    4  1/5 1/20 1/30 1/20  1/5;    5  1/6 1/30 1/60 1/60 1/30  1/6;      ]


=== Row ===


=== Column ===


=== Diagonal ===


Each of the vertical lines of numbers is known as the mth column of Leibniz harmonic triangle.

The leftmost column, containing the reciprocals of the non-negative integers, is identified as the zeroth column, or column 0.

=== Diagonal ===
Consider the Leibniz harmonic triangle:
The Leibniz Harmonic Triangle is a triangular array where:
:the zeroth element in the nth row, counting from 0, is 1 n + 1
:subsequent elements in the nth row are the zeroth element divided by the corresponding element of Pascal's triangle:


:[    n    0    1    2    3    4    5;    0  1/1;    1  1/2  1/2;    2  1/3  1/6  1/3;    3  1/4 1/12 1/12  1/4;    4  1/5 1/20 1/30 1/20  1/5;    5  1/6 1/30 1/60 1/60 1/30  1/6;      ]


=== Row ===


=== Column ===


=== Diagonal ===


The nth diagonal of Leibniz harmonic triangle consists of the entries in row n + m and column m for m ≥ 0:
:(n, 0), (n + 1, 1), (n + 2, 2), …

Hence the diagonal leading down and to the right from (0, 0), containing  the reciprocals of the non-negative integers, is identified as the zeroth diagonal, or diagonal 0.

Each of the vertical lines of numbers is known as the mth column of Leibniz harmonic triangle.

The leftmost column, containing the reciprocals of the non-negative integers, is identified as the zeroth column, or column 0.

=== Diagonal ===
Consider the Leibniz harmonic triangle:
The Leibniz Harmonic Triangle is a triangular array where:
:the zeroth element in the nth row, counting from 0, is 1 n + 1
:subsequent elements in the nth row are the zeroth element divided by the corresponding element of Pascal's triangle:


:[    n    0    1    2    3    4    5;    0  1/1;    1  1/2  1/2;    2  1/3  1/6  1/3;    3  1/4 1/12 1/12  1/4;    4  1/5 1/20 1/30 1/20  1/5;    5  1/6 1/30 1/60 1/60 1/30  1/6;      ]


=== Row ===
Consider the Leibniz harmonic triangle:
The Leibniz Harmonic Triangle is a triangular array where:
:the zeroth element in the nth row, counting from 0, is 1 n + 1
:subsequent elements in the nth row are the zeroth element divided by the corresponding element of Pascal's triangle:


:[    n    0    1    2    3    4    5;    0  1/1;    1  1/2  1/2;    2  1/3  1/6  1/3;    3  1/4 1/12 1/12  1/4;    4  1/5 1/20 1/30 1/20  1/5;    5  1/6 1/30 1/60 1/60 1/30  1/6;      ]


=== Row ===


=== Column ===


=== Diagonal ===


Each of the horizontal lines of numbers corresponding to a given n is known as the nth row of Leibniz harmonic triangle.

Hence the top row, containing a single 1, is identified as the zeroth row, or row 0.

=== Column ===
Consider the Leibniz harmonic triangle:
The Leibniz Harmonic Triangle is a triangular array where:
:the zeroth element in the nth row, counting from 0, is 1 n + 1
:subsequent elements in the nth row are the zeroth element divided by the corresponding element of Pascal's triangle:


:[    n    0    1    2    3    4    5;    0  1/1;    1  1/2  1/2;    2  1/3  1/6  1/3;    3  1/4 1/12 1/12  1/4;    4  1/5 1/20 1/30 1/20  1/5;    5  1/6 1/30 1/60 1/60 1/30  1/6;      ]


=== Row ===


=== Column ===


=== Diagonal ===


Each of the vertical lines of numbers is known as the mth column of Leibniz harmonic triangle.

The leftmost column, containing the reciprocals of the non-negative integers, is identified as the zeroth column, or column 0.

=== Diagonal ===
Consider the Leibniz harmonic triangle:
The Leibniz Harmonic Triangle is a triangular array where:
:the zeroth element in the nth row, counting from 0, is 1 n + 1
:subsequent elements in the nth row are the zeroth element divided by the corresponding element of Pascal's triangle:


:[    n    0    1    2    3    4    5;    0  1/1;    1  1/2  1/2;    2  1/3  1/6  1/3;    3  1/4 1/12 1/12  1/4;    4  1/5 1/20 1/30 1/20  1/5;    5  1/6 1/30 1/60 1/60 1/30  1/6;      ]


=== Row ===


=== Column ===


=== Diagonal ===


The nth diagonal of Leibniz harmonic triangle consists of the entries in row n + m and column m for m ≥ 0:
:(n, 0), (n + 1, 1), (n + 2, 2), …

Hence the diagonal leading down and to the right from (0, 0), containing  the reciprocals of the non-negative integers, is identified as the zeroth diagonal, or diagonal 0.

The nth diagonal of Leibniz harmonic triangle consists of the entries in row n + m and column m for m ≥ 0:
:(n, 0), (n + 1, 1), (n + 2, 2), …

Hence the diagonal leading down and to the right from (0, 0), containing  the reciprocals of the non-negative integers, is identified as the zeroth diagonal, or diagonal 0.",Harmonic
"['Definitions/Harmonic Functions', 'Definitions/Potential Theory']",Definition:Harmonic,"A harmonic function is a is a twice continuously differentiable function $f: U to mathbb R$ (where $U$ is an open set of $mathbb R^n$) which satisfies Laplace's equation:

:$dfrac {partial^2 f} {partial {x_1}^2} + dfrac {partial^2 f} {partial {x_2}^2} + cdots + dfrac {partial^2 f} {partial {x_n}^2} = 0$

everywhere on $U$.


This is usually written using the $nabla^2$ symbol to denote the Laplacian, as:

:$nabla^2 f = 0$


=== Riemannian Manifold ===
Let $left( M, g right)$ be a compact Riemannian manifold with or without boundary.

Let $C^infty left(   right)M$ be the smooth function space.

Let $u in C^infty left(   right)M$ be a smooth real function on $M$.

Let $nabla^2$ be the Laplace-Beltrami operator.


Then $u$ is said to be harmonic  if and only if :
:$nabla^2 u = 0$",Definition:Harmonic Function,,false,"A harmonic function is a is a twice continuously differentiable function f: U →ℝ (where U is an open set of ℝ^n) which satisfies Laplace's equation:

:∂^2 f∂x_1^2 + ∂^2 f∂x_2^2 + ⋯ + ∂^2 f∂x_n^2 = 0

everywhere on U.


This is usually written using the ∇^2 symbol to denote the Laplacian, as:

:∇^2 f = 0


=== Riemannian Manifold ===
Let ( M, g ) be a compact Riemannian manifold with or without boundary.

Let C^∞(   )M be the smooth function space.

Let u ∈ C^∞(   )M be a smooth real function on M.

Let ∇^2 be the Laplace-Beltrami operator.


Then u is said to be harmonic  if and only if :
:∇^2 u = 0",Harmonic
"['Definitions/Harmonic Analysis', 'Definitions/Analysis']",Definition:Harmonic,Harmonic analysis is the study of functions by expressing them as the sum of series of a family of functions such as sines and cosines.,Definition:Harmonic Analysis,functions,true,Harmonic analysis is the study of functions by expressing them as the sum of series of a family of functions such as sines and cosines.,Harmonic
"['Definitions/Number Theory', 'Definitions/Analytic Number Theory', 'Definitions/Arithmetic', 'Definitions/Algebra', 'Definitions/Harmonic Sequences']",Definition:Harmonic,"A harmonic sequence is a sequence $leftlangle a_k rightrangle$ in $mathbb R$ defined as:
:$h_k = dfrac 1 {a + k d}$
where:
:$k in leftlbrace 0, 1, 2, ldots rightrbrace$
:$-dfrac a d notin leftlbrace 0, 1, 2, ldots rightrbrace$


Thus its general form is:
:$dfrac 1 a, dfrac 1 {a + d}, dfrac 1 {a + 2 d}, dfrac 1 {a + 3 d}, ldots$


=== Initial Term ===
Let $leftlangle h_k rightrangle$ be the harmonic sequence:

:$a_k = dfrac 1 {a + k d}$ for $k = 0, 1, 2, ldots$


The term $a$ is the initial term of $leftlangle a_k rightrangle$.


Category:Definitions/Harmonic Sequences

=== Common Difference ===
Let $leftlangle h_k rightrangle$ be the harmonic sequence:

:$a_k = dfrac 1 {a + k d}$ for $k = 0, 1, 2, ldots$


The term $d$ is the common difference of $leftlangle a_k rightrangle$.


Category:Definitions/Harmonic Sequences",Definition:Harmonic Sequence,,false,"A harmonic sequence is a sequence ⟨ a_k ⟩ in ℝ defined as:
:h_k =  1 a + k d
where:
:k ∈{ 0, 1, 2, …}
:- a d ∉{ 0, 1, 2, …}


Thus its general form is:
:1 a,  1 a + d,  1 a + 2 d,  1 a + 3 d, …


=== Initial Term ===
Let ⟨ h_k ⟩ be the harmonic sequence:

:a_k =  1 a + k d for k = 0, 1, 2, …


The term a is the initial term of ⟨ a_k ⟩.


Category:Definitions/Harmonic Sequences

=== Common Difference ===
Let ⟨ h_k ⟩ be the harmonic sequence:

:a_k =  1 a + k d for k = 0, 1, 2, …


The term d is the common difference of ⟨ a_k ⟩.


Category:Definitions/Harmonic Sequences",Harmonic
"['Definitions/Harmonic Series', 'Definitions/Series', 'Definitions/Harmonic Numbers', 'Definitions/Real Analysis']",Definition:Harmonic,"The series defined as:
:$ds sum_{n mathop = 1}^infty frac 1 n = 1 + frac 1 2 + frac 1 3 + frac 1 4 + cdots$

is known as the harmonic series.


=== General Harmonic Series ===
Let $leftlangle x_n rightrangle$ be a sequence of numbers such that $leftlangle leftlvert x_n rightrvert  rightrangle$ is a harmonic sequence.


Then the series defined as:
:$ds sum_{n mathop = 1}^infty x_n$

is a harmonic series.",Definition:Harmonic Series,,false,"The series defined as:
:∑_n  = 1^∞1/n = 1 + 1/2 + 1/3 + 1/4 + ⋯

is known as the harmonic series.


=== General Harmonic Series ===
Let ⟨ x_n ⟩ be a sequence of numbers such that ⟨| x_n |⟩ is a harmonic sequence.


Then the series defined as:
:∑_n  = 1^∞ x_n

is a harmonic series.",Harmonic
"[""Definitions/Mercator's Constant"", 'Definitions/Harmonic Series', 'Definitions/Specific Numbers']",Definition:Harmonic,"Mercator's constant is the real number:

 
 
 
 
 

 ",Definition:Mercator's Constant,,false,"Mercator's constant is the real number:

 
 
 
 
 

 ",Harmonic
"['Definitions/Harmonic Series', 'Definitions/Series']",Definition:Harmonic,"Let $leftlangle x_n rightrangle$ be a sequence of numbers such that $leftlangle leftlvert x_n rightrvert  rightrangle$ is a harmonic sequence.


Then the series defined as:
:$ds sum_{n mathop = 1}^infty x_n$

is a harmonic series.",Definition:Harmonic Series/General,,false,"Let ⟨ x_n ⟩ be a sequence of numbers such that ⟨| x_n |⟩ is a harmonic sequence.


Then the series defined as:
:∑_n  = 1^∞ x_n

is a harmonic series.",Harmonic
['Definitions/Harmonic Sequences'],Definition:Harmonic,"The term harmonic progression is used to mean one of the following:


=== Harmonic Sequence ===
A harmonic sequence is a sequence $leftlangle a_k rightrangle$ in $mathbb R$ defined as:
:$h_k = dfrac 1 {a + k d}$
where:
:$k in leftlbrace 0, 1, 2, ldots rightrbrace$
:$-dfrac a d notin leftlbrace 0, 1, 2, ldots rightrbrace$


Thus its general form is:
:$dfrac 1 a, dfrac 1 {a + d}, dfrac 1 {a + 2 d}, dfrac 1 {a + 3 d}, ldots$


=== Initial Term ===
Let $leftlangle h_k rightrangle$ be the harmonic sequence:

:$a_k = dfrac 1 {a + k d}$ for $k = 0, 1, 2, ldots$


The term $a$ is the initial term of $leftlangle a_k rightrangle$.


Category:Definitions/Harmonic Sequences

=== Common Difference ===
Let $leftlangle h_k rightrangle$ be the harmonic sequence:

:$a_k = dfrac 1 {a + k d}$ for $k = 0, 1, 2, ldots$


The term $d$ is the common difference of $leftlangle a_k rightrangle$.


Category:Definitions/Harmonic Sequences

=== Harmonic Series ===
The series defined as:
:$ds sum_{n mathop = 1}^infty frac 1 n = 1 + frac 1 2 + frac 1 3 + frac 1 4 + cdots$

is known as the harmonic series.


=== General Harmonic Series ===
Let $leftlangle x_n rightrangle$ be a sequence of numbers such that $leftlangle leftlvert x_n rightrvert  rightrangle$ is a harmonic sequence.


Then the series defined as:
:$ds sum_{n mathop = 1}^infty x_n$

is a harmonic series.",Definition:Harmonic Progression,,false,"The term harmonic progression is used to mean one of the following:


=== Harmonic Sequence ===
A harmonic sequence is a sequence ⟨ a_k ⟩ in ℝ defined as:
:h_k =  1 a + k d
where:
:k ∈{ 0, 1, 2, …}
:- a d ∉{ 0, 1, 2, …}


Thus its general form is:
:1 a,  1 a + d,  1 a + 2 d,  1 a + 3 d, …


=== Initial Term ===
Let ⟨ h_k ⟩ be the harmonic sequence:

:a_k =  1 a + k d for k = 0, 1, 2, …


The term a is the initial term of ⟨ a_k ⟩.


Category:Definitions/Harmonic Sequences

=== Common Difference ===
Let ⟨ h_k ⟩ be the harmonic sequence:

:a_k =  1 a + k d for k = 0, 1, 2, …


The term d is the common difference of ⟨ a_k ⟩.


Category:Definitions/Harmonic Sequences

=== Harmonic Series ===
The series defined as:
:∑_n  = 1^∞1/n = 1 + 1/2 + 1/3 + 1/4 + ⋯

is known as the harmonic series.


=== General Harmonic Series ===
Let ⟨ x_n ⟩ be a sequence of numbers such that ⟨| x_n |⟩ is a harmonic sequence.


Then the series defined as:
:∑_n  = 1^∞ x_n

is a harmonic series.",Harmonic
"['Definitions/Harmonics (Analysis)', ""Definitions/Laplace's Equation""]",Definition:Harmonic,"A harmonic is a solution $phi$ to Laplace's equation in $2$ dimensions:
:$nabla^2 phi = 0$
that is:
:$dfrac {partial^2 phi} {partial x^2} + dfrac {partial^2 phi} {partial y^2} = 0$",Definition:Harmonic (Analysis),solution ϕ,true,"A harmonic is a solution ϕ to Laplace's equation in 2 dimensions:
:∇^2 ϕ = 0
that is:
:∂^2 ϕ∂ x^2 + ∂^2 ϕ∂ y^2 = 0",Harmonic
"['Definitions/Spherical Harmonics', ""Definitions/Laplace's Equation""]",Definition:Harmonic,A spherical harmonic is a solution $phi$ to Laplace's equation in $3$ dimensions when expressed in spherical coordinates.,Definition:Spherical Harmonic,,false,A spherical harmonic is a solution ϕ to Laplace's equation in 3 dimensions when expressed in spherical coordinates.,Harmonic
"['Definitions/Surface Harmonics', 'Definitions/Spherical Harmonics']",Definition:Harmonic,"A surface harmonic is a spherical harmonic:
:$r^n left( a_n P_n left(   right){cos theta} + ds sum_{m mathop = 1}^n left(  {a_n}^m cos m phi + {b_n}^m sin m phi right)  {P_n}^m left(   right){cos theta}  right)$

such that $r = 1$.

That is:
:$a_n P_n left(   right){cos theta} + ds sum_{m mathop = 1}^n left(  {a_n}^m cos m phi + {b_n}^m sin m phi right)  {P_n}^m left(   right){cos theta}$",Definition:Surface Harmonic,,false,"A surface harmonic is a spherical harmonic:
:r^n ( a_n P_n (   )cosθ + ∑_m  = 1^n (  a_n^m cos m ϕ + b_n^m sin m ϕ)  P_n^m (   )cosθ)

such that r = 1.

That is:
:a_n P_n (   )cosθ + ∑_m  = 1^n (  a_n^m cos m ϕ + b_n^m sin m ϕ)  P_n^m (   )cosθ",Harmonic
"['Definitions/Tesseral Harmonics', 'Definitions/Surface Harmonics', 'Definitions/Spherical Harmonics']",Definition:Harmonic,"A tesseral harmonic is a surface harmonic in the form:
:$cos m phi ,  {P_n}^m left(   right){cos theta}$
or:
:$sin m phi ,  {P_n}^m left(   right){cos theta}$

such that $m < n$.",Definition:Tesseral Harmonic,,false,"A tesseral harmonic is a surface harmonic in the form:
:cos m ϕ P_n^m (   )cosθ
or:
:sin m ϕ P_n^m (   )cosθ

such that m < n.",Harmonic
"['Definitions/Sectoral Harmonics', 'Definitions/Surface Harmonics', 'Definitions/Spherical Harmonics']",Definition:Harmonic,"A sectoral harmonic is a surface harmonic in the form:
:$cos m phi ,  {P_n}^m left(   right){cos theta}$
or:
:$sin m phi ,  {P_n}^m left(   right){cos theta}$

such that $m = n$.",Definition:Sectoral Harmonic,,false,"A sectoral harmonic is a surface harmonic in the form:
:cos m ϕ P_n^m (   )cosθ
or:
:sin m ϕ P_n^m (   )cosθ

such that m = n.",Harmonic
"['Definitions/Zonal Harmonics', 'Definitions/Spherical Harmonics']",Definition:Harmonic,"Let $H$ be a spherical harmonic in the form:
:$r^n left( a_n P_n left(   right){cos theta} + ds sum_{m mathop = 1}^n left(  {a_n}^m cos m phi + {b_n}^m sin m phi right)  {P_n}^m left(   right){cos theta}  right)$

The function $P_n left(   right){cos theta}$ is known as a zonal harmonic.",Definition:Zonal Harmonic,,false,"Let H be a spherical harmonic in the form:
:r^n ( a_n P_n (   )cosθ + ∑_m  = 1^n (  a_n^m cos m ϕ + b_n^m sin m ϕ)  P_n^m (   )cosθ)

The function P_n (   )cosθ is known as a zonal harmonic.",Harmonic
"['Definitions/Lagrangian Mechanics', 'Definitions/Physics']",Definition:Harmonic,"Let $P$ be a physical particle.

Let its position $x left(   right)t$ be a real function, where $t$ is time.

Let $k > 0$.


Then the potential energy of the form:

:$U left(   right)x = dfrac 1 2 k x^2$

is called the harmonic potential energy.",Definition:Harmonic Potential Energy,,false,"Let P be a physical particle.

Let its position x (   )t be a real function, where t is time.

Let k > 0.


Then the potential energy of the form:

:U (   )x =  1 2 k x^2

is called the harmonic potential energy.",Harmonic
['Definitions/Physics'],Definition:Harmonic,"Let $P$ be a physical particle.

Let the potential energy of $P$ be that of the harmonic potential.


Then $P$ is called the harmonic oscillator.",Definition:Harmonic Oscillator,,false,"Let P be a physical particle.

Let the potential energy of P be that of the harmonic potential.


Then P is called the harmonic oscillator.",Harmonic
"['Definitions/Simple Harmonic Motion', 'Definitions/Harmonic Motion', 'Definitions/Mechanics']",Definition:Harmonic,"Consider a physical system $S$ whose motion can be expressed in the form of the following equation:
:$x = A sin left(   right){omega t + phi}$
where $A$ and $phi$ are constants.


Then $S$ is in a state of simple harmonic motion.


=== Amplitude ===
Consider a physical system $S$ in a state of simple harmonic motion:
:$x = A sin left(   right){omega t + phi}$


The parameter $A$ is known as the amplitude of the motion.

=== Phase ===
Consider a physical system $S$ in a state of simple harmonic motion:
:$x = A sin left(   right){omega t + phi}$


The expression $omega t + phi$ is known as the phase of the motion.


=== Initial Phase ===
Consider a physical system $S$ in a state of simple harmonic motion:
:$x = A sin left(   right){omega t + phi}$


The parameter $phi$ is known as the initial phase of the motion.

Let $S_1$ and $S_2$ be physical systems in a state of simple harmonic motion described respectively by the equations:

 
 
 
 

=== Out of Phase ===
Let $S_1$ and $S_2$ be physical systems in a state of simple harmonic motion described respectively by the equations:

 
 
 
 

$S_1$ and $S_2$ are out of phase  if and only if  $alpha_1 ne alpha_2$.


=== Phase Difference ===
Let $S_1$ and $S_2$ be physical systems in a state of simple harmonic motion described respectively by the equations:

 
 
 
 

such that $S_1$ and $S_2$ are out of phase.

The phase difference of $S_1$ and $S_2$ is defined as $leftlvert alpha_1 - alpha_2 rightrvert$.

=== In Phase ===
Let $S_1$ and $S_2$ be physical systems in a state of simple harmonic motion described respectively by the equations:

 
 
 
 

$S_1$ and $S_2$ are in phase  if and only if  $alpha_1 = alpha_2$.

=== Period ===
Consider a physical system $S$ in a state of simple harmonic motion:
:$x = A sin left(   right){omega t + phi}$


The period $T$ of the motion of $S$ is the time required for one complete cycle:
:$T = dfrac {2 pi} omega$

=== Frequency ===
Consider a physical system $S$ in a state of simple harmonic motion:
:$x = A sin left(   right){omega t + phi}$


The frequency $nu$ of the motion of $S$ is the number of complete cycles per unit time:
:$nu = dfrac 1 T = dfrac omega {2 pi}$",Definition:Simple Harmonic Motion,,false,"Consider a physical system S whose motion can be expressed in the form of the following equation:
:x = A sin(   )ω t + ϕ
where A and ϕ are constants.


Then S is in a state of simple harmonic motion.


=== Amplitude ===
Consider a physical system S in a state of simple harmonic motion:
:x = A sin(   )ω t + ϕ


The parameter A is known as the amplitude of the motion.

=== Phase ===
Consider a physical system S in a state of simple harmonic motion:
:x = A sin(   )ω t + ϕ


The expression ω t + ϕ is known as the phase of the motion.


=== Initial Phase ===
Consider a physical system S in a state of simple harmonic motion:
:x = A sin(   )ω t + ϕ


The parameter ϕ is known as the initial phase of the motion.

Let S_1 and S_2 be physical systems in a state of simple harmonic motion described respectively by the equations:

 
 
 
 

=== Out of Phase ===
Let S_1 and S_2 be physical systems in a state of simple harmonic motion described respectively by the equations:

 
 
 
 

S_1 and S_2 are out of phase  if and only if  α_1 α_2.


=== Phase Difference ===
Let S_1 and S_2 be physical systems in a state of simple harmonic motion described respectively by the equations:

 
 
 
 

such that S_1 and S_2 are out of phase.

The phase difference of S_1 and S_2 is defined as |α_1 - α_2 |.

=== In Phase ===
Let S_1 and S_2 be physical systems in a state of simple harmonic motion described respectively by the equations:

 
 
 
 

S_1 and S_2 are in phase  if and only if  α_1 = α_2.

=== Period ===
Consider a physical system S in a state of simple harmonic motion:
:x = A sin(   )ω t + ϕ


The period T of the motion of S is the time required for one complete cycle:
:T = 2 πω

=== Frequency ===
Consider a physical system S in a state of simple harmonic motion:
:x = A sin(   )ω t + ϕ


The frequency ν of the motion of S is the number of complete cycles per unit time:
:ν =  1 T = ω2 π",Harmonic
"['Definitions/Harmonic Ratios', 'Definitions/Cross-Ratios']",Definition:Harmonic,"Let $A$, $B$, $C$ and $D$ be points on a straight line.

Let the cross-ratio $leftlbrace A, B; C, D rightrbrace$ of $A$, $B$, $C$ and $D$ be equal to $-1$:
:$dfrac {AC / CB} {AD / DB} = -1$
that is:
:$dfrac {AC cdot DB} {AD cdot CB} = -1$


Then $leftlbrace A, B; C, D rightrbrace$ is known as a harmonic ratio.",Definition:Harmonic Ratio,,false,"Let A, B, C and D be points on a straight line.

Let the cross-ratio { A, B; C, D } of A, B, C and D be equal to -1:
:AC / CBAD / DB = -1
that is:
:AC · DBAD · CB = -1


Then { A, B; C, D } is known as a harmonic ratio.",Harmonic
"['Definitions/Harmonic Ranges', 'Definitions/Straight Lines', 'Definitions/Analytic Geometry']",Definition:Harmonic,"Let $A$ and $B$ be points on a straight line.

Let $P$ and $Q$ lie on $AB$ such that $P$ is on the line segment $AB$ while $Q$ is outside the line segment $AB$.


:


Let $P$ and $Q$ be positioned such that the cross-ratio $leftlbrace A, B; P, Q rightrbrace$ forms a harmonic ratio:
:$dfrac {AP} {PB} = -dfrac {AQ} {QB}$


Then $left( AB, PQ right)$ are said to be a harmonic range.",Definition:Harmonic Range,,false,"Let A and B be points on a straight line.

Let P and Q lie on AB such that P is on the line segment AB while Q is outside the line segment AB.


:


Let P and Q be positioned such that the cross-ratio { A, B; P, Q } forms a harmonic ratio:
:APPB = -AQQB


Then ( AB, PQ ) are said to be a harmonic range.",Harmonic
"['Definitions/Harmonic Conjugates', 'Definitions/Harmonic Ranges']",Definition:Harmonic,"=== Harmonic Range ===
Let $AB$ and $PQ$ be line segments on a straight line such that $left( AB, PQ right)$ is a harmonic range.

Then $P$ and $Q$ are said to be harmonic conjugates with respect to $A$ and $B$.

=== Harmonic Pencil ===
Let $AB$ and $PQ$ be line segments on a straight line such that $left( AB, PQ right)$ is a harmonic range.

Let $O$ be a point which is not on the straight line $AB$.

Let $O left(   right){AB, PQ}$ be the harmonic pencil formed from $O$ and $left( AB, PQ right)$.


:


The rays $OP$ and $OQ$ are said to be harmonic conjugates with respect to $OA$ and $OB$.",Definition:Harmonic Conjugates,,false,"=== Harmonic Range ===
Let AB and PQ be line segments on a straight line such that ( AB, PQ ) is a harmonic range.

Then P and Q are said to be harmonic conjugates with respect to A and B.

=== Harmonic Pencil ===
Let AB and PQ be line segments on a straight line such that ( AB, PQ ) is a harmonic range.

Let O be a point which is not on the straight line AB.

Let O (   )AB, PQ be the harmonic pencil formed from O and ( AB, PQ ).


:


The rays OP and OQ are said to be harmonic conjugates with respect to OA and OB.",Harmonic
"['Definitions/Harmonic Pencils', 'Definitions/Harmonic Ranges']",Definition:Harmonic,"Let $A$ and $B$ be points on a straight line.

Let $P$ and $Q$ lie on $AB$ such that $left( AB, PQ right)$ is a harmonic range.


Let $O$ be a point which is not on the straight line $AB$.


:


Then the pencil $O left(   right){AB, PQ}$ formed by joining $O$ to the four points $A$, $B$, $P$ and $Q$ is said to be a harmonic pencil.",Definition:Harmonic Pencil,,false,"Let A and B be points on a straight line.

Let P and Q lie on AB such that ( AB, PQ ) is a harmonic range.


Let O be a point which is not on the straight line AB.


:


Then the pencil O (   )AB, PQ formed by joining O to the four points A, B, P and Q is said to be a harmonic pencil.",Harmonic
['Definitions/Length'],Definition:Height,"Height, like depth, is used as a term for linear measure in a dimension perpendicular to both length and breadth.


However, whereas depth has connotations of down, height is used for distances up from the plane.


=== Euclidean Definition ===

When discussing the size and shape of a general polygon, the words height and width are often seen.

The height of a polygon is the linear measure going up the page.


 
: 
:The height of any figure is the perpendicular drawn from the vertex to the base.
 ''
 

In contrast, the width is the linear measure going across the page.",Definition:Linear Measure/Height,,false,"Height, like depth, is used as a term for linear measure in a dimension perpendicular to both length and breadth.


However, whereas depth has connotations of down, height is used for distances up from the plane.


=== Euclidean Definition ===

When discussing the size and shape of a general polygon, the words height and width are often seen.

The height of a polygon is the linear measure going up the page.


 
: 
:The height of any figure is the perpendicular drawn from the vertex to the base.
 ”
 

In contrast, the width is the linear measure going across the page.",Height
['Definitions/Triangles'],Definition:Height,"The height of a triangle is the length of a perpendicular from the apex to whichever side has been chosen as its base.


That is, the length of the altitude so defined.


:

Thus the length of the altitude $h_a$ so constructed is called the height of $triangle ABC$.",Definition:Triangle (Geometry)/Height,length,true,"The height of a triangle is the length of a perpendicular from the apex to whichever side has been chosen as its base.


That is, the length of the altitude so defined.


:

Thus the length of the altitude h_a so constructed is called the height of ABC.",Height
['Definitions/Polygons'],Definition:Height,"The height of a polygon is the length of a perpendicular from the base to the vertex most distant from the base.


 
: 
:The height of any figure is the perpendicular drawn from the vertex to the base.
 ''
 ",Definition:Polygon/Height,length,true,"The height of a polygon is the length of a perpendicular from the base to the vertex most distant from the base.


 
: 
:The height of any figure is the perpendicular drawn from the vertex to the base.
 ”
 ",Height
['Definitions/Cones'],Definition:Height,":

Let a perpendicular $AE$ be dropped from the apex of a cone to the plane containing its base.

The length $h$ of the line $AE$ is the height of the cone.",Definition:Cone (Geometry)/Height,,false,":

Let a perpendicular AE be dropped from the apex of a cone to the plane containing its base.

The length h of the line AE is the height of the cone.",Height
"['Definitions/Ring Theory', 'Definitions/Commutative Algebra']",Definition:Height,"Let $A$ be a commutative ring with unity.

Let $mathfrak p$ be a prime ideal in $A$.


The height of $mathfrak p$ is the supremum over all $n$ such that there exists a chain of prime ideals:

:$mathfrak p_0 subsetneqq mathfrak p_1 subsetneqq cdots subsetneqq mathfrak p_n = mathfrak p$


It is denoted by:
:$operatorname {ht}  left(   right){mathfrak p}$

 ",Definition:Height of Prime Ideal,,false,"Let A be a commutative ring with unity.

Let 𝔭 be a prime ideal in A.


The height of 𝔭 is the supremum over all n such that there exists a chain of prime ideals:

:𝔭_0 ⫋𝔭_1 ⫋⋯⫋𝔭_n = 𝔭


It is denoted by:
:ht(   )𝔭

 ",Height
"['Definitions/Ring Theory', 'Definitions/Commutative Algebra']",Definition:Height,"Let $A$ be a commutative ring with unity.

Let $I$ be a proper ideal in $A$.


The height of $I$ is defined as:
:$operatorname {ht}  left(   right)I := inf leftlbrace operatorname {ht}  left(   right){mathfrak p} : mathfrak p in mathrm {Spec} left( A right) text{ s.t. } I subseteq mathfrak p  rightrbrace$
where:
:$operatorname {ht}  left(   right){mathfrak p}$ is the height of $mathfrak p$
:$mathrm {Spec} left( A right)$ is the prime spectrum of $A$",Definition:Height of Proper Ideal,,false,"Let A be a commutative ring with unity.

Let I be a proper ideal in A.


The height of I is defined as:
:ht(   )I := inf{ht(   )𝔭 : 𝔭∈Spec( A )  s.t.  I ⊆𝔭}
where:
:ht(   )𝔭 is the height of 𝔭
:Spec( A ) is the prime spectrum of A",Height
"['Definitions/Homogeneous Expressions', 'Definitions/Expressions', 'Definitions/Algebra', 'Definitions/Homogeneity']",Definition:Homogeneous,"A homogeneous expression is an algebraic expression in which the variables can be replaced throughout by the product of that variable with a given non-zero constant, and the constant can be extracted as a factor of the resulting expression.",Definition:Homogeneous Expression,,false,"A homogeneous expression is an algebraic expression in which the variables can be replaced throughout by the product of that variable with a given non-zero constant, and the constant can be extracted as a factor of the resulting expression.",Homogeneous
"['Definitions/Homogeneous Equations', 'Definitions/Homogeneous Expressions', 'Definitions/Homogeneity']",Definition:Homogeneous,A homogeneous equation is formed when a homogeneous expression is equated to zero.,Definition:Homogeneous Equation,,false,A homogeneous equation is formed when a homogeneous expression is equated to zero.,Homogeneous
['Definitions/Quadratic Equations'],Definition:Homogeneous,"A homogeneous quadratic equation is a quadratic equation in two variables such that each term is of degree $2$:

:$a x^2 + h x y + b y^2 = 0$",Definition:Homogeneous Quadratic Equation,,false,"A homogeneous quadratic equation is a quadratic equation in two variables such that each term is of degree 2:

:a x^2 + h x y + b y^2 = 0",Homogeneous
['Definitions/Analytic Geometry'],Definition:Homogeneous,A straight line or plane is homogeneous  if and only if  it contains the origin.,Definition:Homogeneous (Analytic Geometry),,false,A straight line or plane is homogeneous  if and only if  it contains the origin.,Homogeneous
"['Definitions/Homogeneous Cartesian Coordinates', 'Definitions/Cartesian Coordinate Systems', 'Definitions/Projective Geometry', 'Definitions/Homogeneity']",Definition:Homogeneous,"Let $mathcal C$ denote the Cartesian plane.

Let $P = left( x, y right)$ be an arbitrary point in $mathcal C$.


Let $x$ and $y$ be expressed in the forms:

 
 
 
 

where $Z$ is an arbitrary real number.


$P$ is then determined by the ordered triple $left( X, Y, Z right)$, the terms of which are called its homogeneous Cartesian coordinates.",Definition:Homogeneous Cartesian Coordinates,,false,"Let 𝒞 denote the Cartesian plane.

Let P = ( x, y ) be an arbitrary point in 𝒞.


Let x and y be expressed in the forms:

 
 
 
 

where Z is an arbitrary real number.


P is then determined by the ordered triple ( X, Y, Z ), the terms of which are called its homogeneous Cartesian coordinates.",Homogeneous
"['Definitions/Homogeneous Polynomials', 'Definitions/Homogeneous Expressions', 'Definitions/Polynomial Theory', 'Definitions/Homogeneity']",Definition:Homogeneous,A homogeneous polynomial is a polynomial whose monomials with nonzero coefficients all have the same total degree.,Definition:Homogeneous Polynomial,,false,A homogeneous polynomial is a polynomial whose monomials with nonzero coefficients all have the same total degree.,Homogeneous
"['Definitions/Algebra', 'Definitions/Linear Algebra']",Definition:Homogeneous,"A system of homogeneous linear equations is a set of simultaneous linear equations:

:$ds forall i in left[ 1 ,.,.,   right]m: sum_{j mathop = 1}^n alpha_{i j} x_j = beta_i$

such that all the $beta_i$ are equal to zero:

:$ds forall i in left[ 1 ,.,.,   right]m : sum_{j mathop = 1}^n alpha_{i j} x_j = 0$

That is:

 
 
 
 
 
 


=== Matrix Representation ===
A system of homogeneous linear equations is often expressed as:

:$mathbf A mathbf x = mathbf 0$

where:

:$mathbf A = begin {bmatrix}
a_{11} & a_{12} & cdots & a_{1n} \
a_{21} & a_{22} & cdots & a_{2n} \
vdots & vdots & ddots & vdots \
a_{m1} & a_{m2} & cdots & a_{mn} \
end {bmatrix}$,  $mathbf x = begin{bmatrix} x_1 \ x_2 \ vdots \ x_n end{bmatrix}$, $mathbf 0 = begin {bmatrix} 0 \ 0 \ vdots \ 0 end {bmatrix}$

are matrices.",Definition:Homogeneous Linear Equations,,false,"A system of homogeneous linear equations is a set of simultaneous linear equations:

:∀ i ∈[ 1  . . ]m: ∑_j  = 1^n α_i j x_j = β_i

such that all the β_i are equal to zero:

:∀ i ∈[ 1  . . ]m : ∑_j  = 1^n α_i j x_j = 0

That is:

 
 
 
 
 
 


=== Matrix Representation ===
A system of homogeneous linear equations is often expressed as:

:𝐀𝐱 = 0

where:

:𝐀 = [ a_11 a_12    ⋯ a_1n; a_21 a_22    ⋯ a_2n;    ⋮    ⋮    ⋱    ⋮; a_m1 a_m2    ⋯ a_mn;      ],  𝐱 = [ x_1; x_2;   ⋮; x_n ], 0 = [ 0; 0; ⋮; 0 ]

are matrices.",Homogeneous
"['Definitions/Homogeneous Functions', 'Definitions/Algebra', 'Definitions/Analysis']",Definition:Homogeneous,"Let $V$ and $W$ be two vector spaces over a field $mathbb F$.

Let $f: V to W$ be a function from $V$ to $W$.

Then $f$ is homogeneous of degree $n$  if and only if :
:$f left(   right){alpha mathbf v} = alpha^n f left(   right){mathbf v}$
for all nonzero $mathbf v in V$ and $alpha in mathbb F$.


=== Degree ===
Let $V$ and $W$ be two vector spaces over a field $F$.

Let $f: V to W$ be a homogeneous function of degree $n$ from $V$ to $W$:
:$f left({alpha mathbf v}right) = alpha^n f left({mathbf v}right)$
for all nonzero $mathbf v in V$ and $alpha in F$.


The element $n in mathbb N$ is the degree of $f$.


Category:Definitions/Homogeneous Functions

=== Zero Degree ===

A special case is when $n = 0$:

Let $V$ and $W$ be two vector spaces over a field $F$.

Let $f: V to W$ be a function from $V$ to $W$.


$f$ is a homogeneous function of degree zero  if and only if :
:$f left(   right){alpha mathbf v} = alpha^0 f left(   right){mathbf v} = f left(   right){mathbf v}$


Category:Definitions/Homogeneous Functions",Definition:Homogeneous Function,,false,"Let V and W be two vector spaces over a field 𝔽.

Let f: V → W be a function from V to W.

Then f is homogeneous of degree n  if and only if :
:f (   )α𝐯 = α^n f (   )𝐯
for all nonzero 𝐯∈ V and α∈𝔽.


=== Degree ===
Let V and W be two vector spaces over a field F.

Let f: V → W be a homogeneous function of degree n from V to W:
:f (α𝐯) = α^n f (𝐯)
for all nonzero 𝐯∈ V and α∈ F.


The element n ∈ℕ is the degree of f.


Category:Definitions/Homogeneous Functions

=== Zero Degree ===

A special case is when n = 0:

Let V and W be two vector spaces over a field F.

Let f: V → W be a function from V to W.


f is a homogeneous function of degree zero  if and only if :
:f (   )α𝐯 = α^0 f (   )𝐯 = f (   )𝐯


Category:Definitions/Homogeneous Functions",Homogeneous
"['Definitions/Homogeneous Functions', 'Definitions/Real Analysis']",Definition:Homogeneous,"Let $f: mathbb R^2 to mathbb R$ be a real-valued function of two variables.

$f left(   right){x, y}$ is a homogeneous function  if and only if :
:$exists n in mathbb Z: forall t in mathbb R: f left(   right){t x, t y} = t^n f left(   right){x, y}$


Thus, loosely speaking, a homogeneous function of $x$ and $y$ is one where $x$ and $y$ are both of the same ""power"".


=== Degree ===
Let $f: mathbb R^2 to mathbb R$ be a homogeneous function of two variables:

:$exists n in mathbb Z: forall t in mathbb R: f left(   right){t x, t y} = t^n f left(   right){x, y}$


The integer $n$ is known as the degree of $f$.

=== Zero Degree ===

A special case is when $n = 0$:

Let $f: mathbb R^2 to mathbb R$ be a real-valued function of two variables.


$f left(   right){x, y}$ is a homogeneous function of degree zero or of zero degree  if and only if :
:$forall t in mathbb R: f left(   right){t x, t y} = t^0 f left(   right){x, y} = f left(   right){x, y}$


Category:Definitions/Homogeneous Functions",Definition:Homogeneous Function/Real Space,,false,"Let f: ℝ^2 →ℝ be a real-valued function of two variables.

f (   )x, y is a homogeneous function  if and only if :
:∃ n ∈ℤ: ∀ t ∈ℝ: f (   )t x, t y = t^n f (   )x, y


Thus, loosely speaking, a homogeneous function of x and y is one where x and y are both of the same ""power"".


=== Degree ===
Let f: ℝ^2 →ℝ be a homogeneous function of two variables:

:∃ n ∈ℤ: ∀ t ∈ℝ: f (   )t x, t y = t^n f (   )x, y


The integer n is known as the degree of f.

=== Zero Degree ===

A special case is when n = 0:

Let f: ℝ^2 →ℝ be a real-valued function of two variables.


f (   )x, y is a homogeneous function of degree zero or of zero degree  if and only if :
:∀ t ∈ℝ: f (   )t x, t y = t^0 f (   )x, y = f (   )x, y


Category:Definitions/Homogeneous Functions",Homogeneous
"['Definitions/Homogeneous Differential Equations', 'Definitions/Homogeneous Functions', 'Definitions/First Order ODEs']",Definition:Homogeneous,"A homogeneous differential equation is a first order ordinary differential equation of the form:
:$M left(   right){x, y} + N left(   right){x, y} dfrac {mathrm d y} {mathrm d x} = 0$
where both $M$ and $N$ are homogeneous functions of the same degree.",Definition:Homogeneous Differential Equation,,false,"A homogeneous differential equation is a first order ordinary differential equation of the form:
:M (   )x, y + N (   )x, yd yd x = 0
where both M and N are homogeneous functions of the same degree.",Homogeneous
"['Definitions/Integral Equations of the Second Kind', 'Definitions/Integral Equations', 'Definitions/Second Kind']",Definition:Homogeneous,"An integral equation of the second kind
:$g left(   right)x = f left(   right)x + lambda ds int_{a left(   right)x}^{b left(   right)x} K left(   right){x, y} g left(   right)y ,mathrm d x$
is described as homogeneous  if and only if  $f left(   right)x equiv 0$.


That is, if it is of the form:
:$g left(   right)x = lambda ds int_{a left(   right)x}^{b left(   right)x} K left(   right){x, y} g left(   right)y ,mathrm d x$
where:
:$K left(   right){x, y}$ is a known function
:$a left(   right)x$ and $b left(   right)x$ are known functions of $x$, or constant
:$g left(   right)x$ is an unknown function.


=== Kernel ===
Consider the integral equation:

:of the first kind:
::$f left(   right)x = lambda ds int_{a left(   right)x}^{b left(   right)x} K left(   right){x, y} g left(   right)y ,mathrm d x$

:of the second kind:
::$g left(   right)x = f left(   right)x + lambda ds int_{a left(   right)x}^{b left(   right)x} K left(   right){x, y} g left(   right)y ,mathrm d x$

:of the third kind:
::$u left(   right)x g left(   right)x = f left(   right)x + lambda ds int_{a left(   right)x}^{b left(   right)x} K left(   right){x, y} g left(   right)y ,mathrm d x$


The function $K left(   right){x, y}$ is known as the kernel of the integral equation.

=== Parameter ===
Consider the integral equation:

:of the first kind:
::$f left(   right)x = lambda ds int_{a left(   right)x}^{b left(   right)x} K left(   right){x, y} g left(   right)y ,mathrm d x$

:of the second kind:
::$g left(   right)x = f left(   right)x + lambda ds int_{a left(   right)x}^{b left(   right)x} K left(   right){x, y} g left(   right)y ,mathrm d x$

:of the third kind:
::$u left(   right)x g left(   right)x = f left(   right)x + lambda ds int_{a left(   right)x}^{b left(   right)x} K left(   right){x, y} g left(   right)y ,mathrm d x$


The number $lambda$ is known as the parameter of the integral equation.",Definition:Integral Equation of the Second Kind/Homogeneous,,false,"An integral equation of the second kind
:g (   )x = f (   )x + λ∫_a (   )x^b (   )x K (   )x, y g (   )y  d x
is described as homogeneous  if and only if  f (   )x ≡ 0.


That is, if it is of the form:
:g (   )x = λ∫_a (   )x^b (   )x K (   )x, y g (   )y  d x
where:
:K (   )x, y is a known function
:a (   )x and b (   )x are known functions of x, or constant
:g (   )x is an unknown function.


=== Kernel ===
Consider the integral equation:

:of the first kind:
::f (   )x = λ∫_a (   )x^b (   )x K (   )x, y g (   )y  d x

:of the second kind:
::g (   )x = f (   )x + λ∫_a (   )x^b (   )x K (   )x, y g (   )y  d x

:of the third kind:
::u (   )x g (   )x = f (   )x + λ∫_a (   )x^b (   )x K (   )x, y g (   )y  d x


The function K (   )x, y is known as the kernel of the integral equation.

=== Parameter ===
Consider the integral equation:

:of the first kind:
::f (   )x = λ∫_a (   )x^b (   )x K (   )x, y g (   )y  d x

:of the second kind:
::g (   )x = f (   )x + λ∫_a (   )x^b (   )x K (   )x, y g (   )y  d x

:of the third kind:
::u (   )x g (   )x = f (   )x + λ∫_a (   )x^b (   )x K (   )x, y g (   )y  d x


The number λ is known as the parameter of the integral equation.",Homogeneous
['Definitions/Model Theory for Predicate Logic'],Definition:Homogeneous,"Let $T$ be an $mathcal L$-theory.

Let $kappa$ be an infinite cardinal.


A model $mathcal M$ of $T$ is $kappa$-homogeneous  if and only if  for every subset $A$ and element $b$ in the universe of $mathcal M$ with the cardinality of $A$ strictly less than $kappa$, if $f: A to mathcal M$ is partial elementary, then $f$ extends to an elementary map $f^*: A cup leftlbrace b rightrbrace to mathcal M$.

That is, $mathcal M$ is $kappa$-homogeneous  if and only if  for all $A subseteq mathcal M$ with  $leftlvert A rightrvert < kappa$ and all $b in mathcal M$, every elementary $f: A to mathcal M$ extends to an elementary $f^*: A cup leftlbrace b rightrbrace to mathcal M$.


We say $mathcal M$ is homogeneous  if and only if  it is $kappa$-homogeneous where $kappa$ is the cardinality of the universe of $mathcal M$.",Definition:Homogeneous (Model Theory),,false,"Let T be an ℒ-theory.

Let κ be an infinite cardinal.


A model ℳ of T is κ-homogeneous  if and only if  for every subset A and element b in the universe of ℳ with the cardinality of A strictly less than κ, if f: A →ℳ is partial elementary, then f extends to an elementary map f^*: A ∪{ b }→ℳ.

That is, ℳ is κ-homogeneous  if and only if  for all A ⊆ℳ with  | A | < κ and all b ∈ℳ, every elementary f: A →ℳ extends to an elementary f^*: A ∪{ b }→ℳ.


We say ℳ is homogeneous  if and only if  it is κ-homogeneous where κ is the cardinality of the universe of ℳ.",Homogeneous
"['Definitions/Physics', 'Definitions/Homogeneity']",Definition:Homogeneous,"A body is said to be homogeneous  if and only if  the substance of any part of it is indistinguishable from any other part.


=== Warning ===
Just to specify that a body is made of the same substance throughout is not an adequate definition of homogeneous.

For example, a column of air in the atmosphere is denser at the bottom than at the top.

The fact that it is ""all made of air"" is one thing, but the air at the bottom can be distinguished from that higher up because the densities are different.

Thus a column of air is not homogeneous.


Category:Definitions/Homogeneity",Definition:Homogeneous (Physics),,false,"A body is said to be homogeneous  if and only if  the substance of any part of it is indistinguishable from any other part.


=== Warning ===
Just to specify that a body is made of the same substance throughout is not an adequate definition of homogeneous.

For example, a column of air in the atmosphere is denser at the bottom than at the top.

The fact that it is ""all made of air"" is one thing, but the air at the bottom can be distinguished from that higher up because the densities are different.

Thus a column of air is not homogeneous.


Category:Definitions/Homogeneity",Homogeneous
"['Definitions/Homomorphisms (Abstract Algebra)', 'Definitions/Abstract Algebra', 'Definitions/Mapping Theory', 'Definitions/Homomorphisms']",Definition:Homomorphism,"Let $left( S, circ right)$ and $left( T, * right)$ be algebraic structures.

Let $phi: left( S, circ right) to left( T, * right)$ be a mapping from $left( S, circ right)$ to $left( T, * right)$.

Let $circ$ have the morphism property under $phi$, that is:

:$forall x, y in S: phi left(   right){x circ y} = phi left(   right)x * phi left(   right)y$


Then $phi$ is a homomorphism.


This can be generalised to algebraic structures with more than one operation:

Let:
:$left( S_1, circ_1, circ_2, ldots, circ_n right)$
:$left( T, *_1, *_2, ldots, *_n right)$
be algebraic structures.


Let $phi: left( S_1, circ_1, circ_2, ldots, circ_n right) to left( T, *_1, *_2, ldots, *_n right)$ be a mapping from $left( S_1, circ_1, circ_2, ldots, circ_n right)$ to $left( T, *_1, *_2, ldots, *_n right)$.

Let, $forall k in left[ 1 ,.,.,   right]n$, $circ_k$ have the morphism property under $phi$, that is:

:$forall x, y in S: phi left(   right){x circ_k y} = phi left(   right)x *_k phi left(   right)y$


Then $phi$ is a homomorphism.


=== Semigroup Homomorphism ===
Let $left( S, circ right)$ and $left( T, * right)$ be semigroups.

Let $phi: S to T$ be a mapping such that $circ$ has the morphism property under $phi$.


That is, $forall a, b in S$:
:$phi left(   right){a circ b} = phi left(   right)a * phi left(   right)b$


Then $phi: left( S, circ right) to left( T, * right)$ is a semigroup homomorphism.

=== Monoid Homomorphism ===
Let $left( S, circ right)$ and $left( T, * right)$ be monoids.

Let $phi: S to T$ be a mapping such that $circ$ has the morphism property under $phi$.

That is, $forall a, b in S$:
:$phi left(   right){a circ b} = phi left(   right)a * phi left(   right)b$

Suppose further that $phi$ preserves identities, that is:

:$phi left(   right){e_S} = e_T$


Then $phi: left( S, circ right) to left( T, * right)$ is a monoid homomorphism.

=== Group Homomorphism ===
Let $left( G, circ right)$ and $left( H, * right)$ be groups.

Let $phi: G to H$ be a mapping such that $circ$ has the morphism property under $phi$.


That is, $forall a, b in G$:
:$phi left(   right){a circ b} = phi left(   right)a * phi left(   right)b$


Then $phi: left( G, circ right) to left( H, * right)$ is a group homomorphism.

=== Ring Homomorphism ===
Let $left( R, +, circ right)$ and $left( S, oplus, * right)$ be rings.

Let $phi: R to S$ be a mapping such that both $+$ and $circ$ have the morphism property under $phi$.


That is, $forall a, b in R$:

 
 
 
 


Then $phi: left( R, +, circ right) to left( S, oplus, * right)$ is a ring homomorphism.

=== Field Homomorphism ===
Let $left( F, +, times right)$ and $left( K, oplus, otimes right)$ be fields.

Let $phi: F to K$ be a mapping such that both $+$ and $times$ have the morphism property under $phi$.


That is, $forall a, b in F$:

 
 
 
 


Then $phi: left( F, +, times right) to left( K, oplus, otimes right)$ is a field homomorphism.

=== $F$-Homomorphism ===
Let $R, S$ be rings with unity.

Let $F$ be a subfield of both $R$ and $S$.


Then a ring homomorphism $varphi: R to S$ is called an $F$-homomorphism if:
:$forall a in F: phi left(   right)a = a$


That is, $phi restriction_F = I_F$ where:
:$phi restriction_F$ is the restriction of $phi$ to $F$
:$I_F$ is the identity mapping on $F$.

=== $R$-Algebraic Structure Homomorphism ===
Let $R$ be a ring.

Let $left( S, ast_1, ast_2, ldots, ast_n, circ right)_R$ and $left( T, odot_1, odot_2, ldots, odot_n, otimes right)_R$ be $R$-algebraic structures.

Let $phi: S to T$ be a mapping.


Then $phi$ is an $R$-algebraic structure homomorphism  if and only if :

:$(1): quad forall k in left[ 1 ,.,.,   right]n: forall x, y in S: phi left(   right){x ast_k y} = phi left(   right)x odot_k phi left(   right)y$
:$(2): quad forall x in S: forall lambda in R: phi left(   right){lambda circ x} = lambda otimes phi left(   right)x$
where $left[ 1 ,.,.,   right]n = leftlbrace 1, 2, ldots, n rightrbrace$ denotes an integer interval.


Note that this definition also applies to modules and vector spaces.

=== $G$-Module Homomorphism ===
Let $left( G, cdot right)$ be a group.

Let $left( V, phi right)$ and $left( W, mu right)$ be $G$-modules.


Then a linear transformation $f: V to W$ is called a $G$-module homomorphism  if and only if :
:$forall g in G: forall v in V: f left(   right){phi left(   right){g, v} } = mu left(   right){g, f left(   right)v}$

=== Homomorphism of Complexes ===
Let $left( R, +, cdot right)$ be a ring.

Let:
:$M: quad cdots longrightarrow M_i stackrel {d_i} {longrightarrow} M_{i + 1} stackrel {d_{i + 1} } {longrightarrow} M_{i + 2} stackrel {d_{i + 2} } {longrightarrow} cdots$
and
:$N: quad cdots longrightarrow N_i stackrel {d'_i} {longrightarrow} N_{i + 1} stackrel {d'_{i + 1} } {longrightarrow} N_{i + 2} stackrel {d'_{i + 2} } {longrightarrow} cdots$
be two differential complexes of $R$-modules.

Let $phi = leftlbrace phi_i: i in mathbb Z rightrbrace$ be a family of module homomorphisms $phi_i: M_i to N_i$.


Then $phi$ is a homomorphism of complexes  if and only if  for each $i in mathbb Z$:
:$phi_{i + 1} circ d_i = phi_i circ d'_i$


That is, for each $i in mathbb Z$ we have a commutative diagram:

::$begin{xy}xymatrix@L+2mu@+1em {
 M_i ar[r]^*{d_i}
     ar[d]^*{phi_i} &
M_{i+1} ar[d]^*{phi_{i+1}} \
 N_i ar[r]^*{d'_i} &
 N_{i+1} } end{xy}$",Definition:Homomorphism (Abstract Algebra),,false,"Let ( S, ∘) and ( T, * ) be algebraic structures.

Let ϕ: ( S, ∘) →( T, * ) be a mapping from ( S, ∘) to ( T, * ).

Let ∘ have the morphism property under ϕ, that is:

:∀ x, y ∈ S: ϕ(   )x ∘ y = ϕ(   )x * ϕ(   )y


Then ϕ is a homomorphism.


This can be generalised to algebraic structures with more than one operation:

Let:
:( S_1, ∘_1, ∘_2, …, ∘_n )
:( T, *_1, *_2, …, *_n )
be algebraic structures.


Let ϕ: ( S_1, ∘_1, ∘_2, …, ∘_n ) →( T, *_1, *_2, …, *_n ) be a mapping from ( S_1, ∘_1, ∘_2, …, ∘_n ) to ( T, *_1, *_2, …, *_n ).

Let, ∀ k ∈[ 1  . . ]n, ∘_k have the morphism property under ϕ, that is:

:∀ x, y ∈ S: ϕ(   )x ∘_k y = ϕ(   )x *_k ϕ(   )y


Then ϕ is a homomorphism.


=== Semigroup Homomorphism ===
Let ( S, ∘) and ( T, * ) be semigroups.

Let ϕ: S → T be a mapping such that ∘ has the morphism property under ϕ.


That is, ∀ a, b ∈ S:
:ϕ(   )a ∘ b = ϕ(   )a * ϕ(   )b


Then ϕ: ( S, ∘) →( T, * ) is a semigroup homomorphism.

=== Monoid Homomorphism ===
Let ( S, ∘) and ( T, * ) be monoids.

Let ϕ: S → T be a mapping such that ∘ has the morphism property under ϕ.

That is, ∀ a, b ∈ S:
:ϕ(   )a ∘ b = ϕ(   )a * ϕ(   )b

Suppose further that ϕ preserves identities, that is:

:ϕ(   )e_S = e_T


Then ϕ: ( S, ∘) →( T, * ) is a monoid homomorphism.

=== Group Homomorphism ===
Let ( G, ∘) and ( H, * ) be groups.

Let ϕ: G → H be a mapping such that ∘ has the morphism property under ϕ.


That is, ∀ a, b ∈ G:
:ϕ(   )a ∘ b = ϕ(   )a * ϕ(   )b


Then ϕ: ( G, ∘) →( H, * ) is a group homomorphism.

=== Ring Homomorphism ===
Let ( R, +, ∘) and ( S, ⊕, * ) be rings.

Let ϕ: R → S be a mapping such that both + and ∘ have the morphism property under ϕ.


That is, ∀ a, b ∈ R:

 
 
 
 


Then ϕ: ( R, +, ∘) →( S, ⊕, * ) is a ring homomorphism.

=== Field Homomorphism ===
Let ( F, +, ×) and ( K, ⊕, ⊗) be fields.

Let ϕ: F → K be a mapping such that both + and × have the morphism property under ϕ.


That is, ∀ a, b ∈ F:

 
 
 
 


Then ϕ: ( F, +, ×) →( K, ⊕, ⊗) is a field homomorphism.

=== F-Homomorphism ===
Let R, S be rings with unity.

Let F be a subfield of both R and S.


Then a ring homomorphism φ: R → S is called an F-homomorphism if:
:∀ a ∈ F: ϕ(   )a = a


That is, ϕ_F = I_F where:
:ϕ_F is the restriction of ϕ to F
:I_F is the identity mapping on F.

=== R-Algebraic Structure Homomorphism ===
Let R be a ring.

Let ( S, ∗_1, ∗_2, …, ∗_n, ∘)_R and ( T, ⊙_1, ⊙_2, …, ⊙_n, ⊗)_R be R-algebraic structures.

Let ϕ: S → T be a mapping.


Then ϕ is an R-algebraic structure homomorphism  if and only if :

:(1):   ∀ k ∈[ 1  . . ]n: ∀ x, y ∈ S: ϕ(   )x ∗_k y = ϕ(   )x ⊙_k ϕ(   )y
:(2):   ∀ x ∈ S: ∀λ∈ R: ϕ(   )λ∘ x = λ⊗ϕ(   )x
where [ 1  . . ]n = { 1, 2, …, n } denotes an integer interval.


Note that this definition also applies to modules and vector spaces.

=== G-Module Homomorphism ===
Let ( G, ·) be a group.

Let ( V, ϕ) and ( W, μ) be G-modules.


Then a linear transformation f: V → W is called a G-module homomorphism  if and only if :
:∀ g ∈ G: ∀ v ∈ V: f (   )ϕ(   )g, v = μ(   )g, f (   )v

=== Homomorphism of Complexes ===
Let ( R, +, ·) be a ring.

Let:
:M:   ⋯⟶ M_i d_i⟶ M_i + 1d_i + 1⟶ M_i + 2d_i + 2⟶⋯
and
:N:   ⋯⟶ N_i d'_i⟶ N_i + 1d'_i + 1⟶ N_i + 2d'_i + 2⟶⋯
be two differential complexes of R-modules.

Let ϕ = {ϕ_i: i ∈ℤ} be a family of module homomorphisms ϕ_i: M_i → N_i.


Then ϕ is a homomorphism of complexes  if and only if  for each i ∈ℤ:
:ϕ_i + 1∘ d_i = ϕ_i ∘ d'_i


That is, for each i ∈ℤ we have a commutative diagram:

::@L+2mu@+1em 
 M_i [r]^*d_i[d]^*ϕ_i   
M_i+1[d]^*ϕ_i+1

 N_i [r]^*d'_i   
 N_i+1",Homomorphism
"['Definitions/Group Homomorphisms', 'Definitions/Homomorphisms (Abstract Algebra)', 'Definitions/Group Theory']",Definition:Homomorphism,"Let $left( G, circ right)$ and $left( H, * right)$ be groups.

Let $phi: G to H$ be a mapping such that $circ$ has the morphism property under $phi$.


That is, $forall a, b in G$:
:$phi left(   right){a circ b} = phi left(   right)a * phi left(   right)b$


Then $phi: left( G, circ right) to left( H, * right)$ is a group homomorphism.",Definition:Group Homomorphism,,false,"Let ( G, ∘) and ( H, * ) be groups.

Let ϕ: G → H be a mapping such that ∘ has the morphism property under ϕ.


That is, ∀ a, b ∈ G:
:ϕ(   )a ∘ b = ϕ(   )a * ϕ(   )b


Then ϕ: ( G, ∘) →( H, * ) is a group homomorphism.",Homomorphism
"['Definitions/Ring Homomorphisms', 'Definitions/Homomorphisms (Abstract Algebra)', 'Definitions/Ring Theory']",Definition:Homomorphism,"Let $left( R, +, circ right)$ and $left( S, oplus, * right)$ be rings.

Let $phi: R to S$ be a mapping such that both $+$ and $circ$ have the morphism property under $phi$.


That is, $forall a, b in R$:

 
 
 
 


Then $phi: left( R, +, circ right) to left( S, oplus, * right)$ is a ring homomorphism.",Definition:Ring Homomorphism,,false,"Let ( R, +, ∘) and ( S, ⊕, * ) be rings.

Let ϕ: R → S be a mapping such that both + and ∘ have the morphism property under ϕ.


That is, ∀ a, b ∈ R:

 
 
 
 


Then ϕ: ( R, +, ∘) →( S, ⊕, * ) is a ring homomorphism.",Homomorphism
"['Definitions/Field Homomorphisms', 'Definitions/Ring Homomorphisms', 'Definitions/Homomorphisms (Abstract Algebra)', 'Definitions/Field Theory']",Definition:Homomorphism,"Let $left( F, +, times right)$ and $left( K, oplus, otimes right)$ be fields.

Let $phi: F to K$ be a mapping such that both $+$ and $times$ have the morphism property under $phi$.


That is, $forall a, b in F$:

 
 
 
 


Then $phi: left( F, +, times right) to left( K, oplus, otimes right)$ is a field homomorphism.",Definition:Field Homomorphism,,false,"Let ( F, +, ×) and ( K, ⊕, ⊗) be fields.

Let ϕ: F → K be a mapping such that both + and × have the morphism property under ϕ.


That is, ∀ a, b ∈ F:

 
 
 
 


Then ϕ: ( F, +, ×) →( K, ⊕, ⊗) is a field homomorphism.",Homomorphism
"['Definitions/Ring Homomorphisms', 'Definitions/Field Theory']",Definition:Homomorphism,"Let $R, S$ be rings with unity.

Let $F$ be a subfield of both $R$ and $S$.


Then a ring homomorphism $varphi: R to S$ is called an $F$-homomorphism if:
:$forall a in F: phi left(   right)a = a$


That is, $phi restriction_F = I_F$ where:
:$phi restriction_F$ is the restriction of $phi$ to $F$
:$I_F$ is the identity mapping on $F$.",Definition:F-Homomorphism,,false,"Let R, S be rings with unity.

Let F be a subfield of both R and S.


Then a ring homomorphism φ: R → S is called an F-homomorphism if:
:∀ a ∈ F: ϕ(   )a = a


That is, ϕ_F = I_F where:
:ϕ_F is the restriction of ϕ to F
:I_F is the identity mapping on F.",Homomorphism
"['Definitions/R-Algebraic Structure Homomorphisms', 'Definitions/Homomorphisms (Abstract Algebra)', 'Definitions/Linear Algebra']",Definition:Homomorphism,"Let $R$ be a ring.

Let $left( S, ast_1, ast_2, ldots, ast_n, circ right)_R$ and $left( T, odot_1, odot_2, ldots, odot_n, otimes right)_R$ be $R$-algebraic structures.

Let $phi: S to T$ be a mapping.


Then $phi$ is an $R$-algebraic structure homomorphism  if and only if :

:$(1): quad forall k in left[ 1 ,.,.,   right]n: forall x, y in S: phi left(   right){x ast_k y} = phi left(   right)x odot_k phi left(   right)y$
:$(2): quad forall x in S: forall lambda in R: phi left(   right){lambda circ x} = lambda otimes phi left(   right)x$
where $left[ 1 ,.,.,   right]n = leftlbrace 1, 2, ldots, n rightrbrace$ denotes an integer interval.


Note that this definition also applies to modules and vector spaces.",Definition:R-Algebraic Structure Homomorphism,,false,"Let R be a ring.

Let ( S, ∗_1, ∗_2, …, ∗_n, ∘)_R and ( T, ⊙_1, ⊙_2, …, ⊙_n, ⊗)_R be R-algebraic structures.

Let ϕ: S → T be a mapping.


Then ϕ is an R-algebraic structure homomorphism  if and only if :

:(1):   ∀ k ∈[ 1  . . ]n: ∀ x, y ∈ S: ϕ(   )x ∗_k y = ϕ(   )x ⊙_k ϕ(   )y
:(2):   ∀ x ∈ S: ∀λ∈ R: ϕ(   )λ∘ x = λ⊗ϕ(   )x
where [ 1  . . ]n = { 1, 2, …, n } denotes an integer interval.


Note that this definition also applies to modules and vector spaces.",Homomorphism
"['Definitions/Graph Theory', 'Definitions/Homomorphisms']",Definition:Homomorphism,"Let $G = left( V left(   right)G, E left(   right)G right)$ and $H = left( V left(   right)H, E left(   right)H right)$ be graphs.


Let there exist a mapping $F: V left(   right)G to V left(   right)H$ such that:
:for each edge $leftlbrace u, v rightrbrace in E left(   right)G$
:there exists an edge $leftlbrace F left(   right)u, F left(   right)v rightrbrace in E left(   right)H$.


Then $G$ and $H$ are homomorphic.

The mapping $F$ is called a homomorphism from $G$ to $H$.",Definition:Homomorphism (Graph Theory),,false,"Let G = ( V (   )G, E (   )G ) and H = ( V (   )H, E (   )H ) be graphs.


Let there exist a mapping F: V (   )G → V (   )H such that:
:for each edge { u, v }∈ E (   )G
:there exists an edge { F (   )u, F (   )v }∈ E (   )H.


Then G and H are homomorphic.

The mapping F is called a homomorphism from G to H.",Homomorphism
"['Definitions/Ideals of Rings', 'Definitions/Ring Theory', 'Definitions/Ideal Theory']",Definition:Ideal,"Let $left( R, +, circ right)$ be a ring.

Let $left( J, + right)$ be a subgroup of $left( R, + right)$.


Then $J$ is an ideal of $R$  if and only if :
:$forall j in J: forall r in R: j circ r in J land r circ j in J$

that is,  if and only if :
:$forall r in R: J circ r subseteq J land r circ J subseteq J$


The letter $J$ is frequently used to denote an ideal.


=== Left Ideal ===
Let $left( R, +, circ right)$ be a ring.

Let $left( J, + right)$ be a subgroup of $left( R, + right)$.


$J$ is a left ideal of $R$  if and only if :
:$forall j in J: forall r in R: r circ j in J$

that is,  if and only if :
:$forall r in R: r circ J subseteq J$

=== Right Ideal ===
Let $left( R, +, circ right)$ be a ring.

Let $left( J, + right)$ be a subgroup of $left( R, + right)$.


$J$ is a right ideal of $R$  if and only if :
:$forall j in J: forall r in R: j circ r in J$

that is,  if and only if :
:$forall r in R: J circ r subseteq J$

It follows that in a commutative ring, a left ideal, a right ideal and an ideal are the same thing.


=== Proper Ideal ===
Let $left( R, +, circ right)$ be a ring.


A proper ideal $J$ of $left( R, +, circ right)$ is an ideal of $R$ such that $J$ is a proper subset of $R$.

That is, such that $J subseteq R$ and $J ne R$.",Definition:Ideal of Ring,,false,"Let ( R, +, ∘) be a ring.

Let ( J, + ) be a subgroup of ( R, + ).


Then J is an ideal of R  if and only if :
:∀ j ∈ J: ∀ r ∈ R: j ∘ r ∈ J  r ∘ j ∈ J

that is,  if and only if :
:∀ r ∈ R: J ∘ r ⊆ J  r ∘ J ⊆ J


The letter J is frequently used to denote an ideal.


=== Left Ideal ===
Let ( R, +, ∘) be a ring.

Let ( J, + ) be a subgroup of ( R, + ).


J is a left ideal of R  if and only if :
:∀ j ∈ J: ∀ r ∈ R: r ∘ j ∈ J

that is,  if and only if :
:∀ r ∈ R: r ∘ J ⊆ J

=== Right Ideal ===
Let ( R, +, ∘) be a ring.

Let ( J, + ) be a subgroup of ( R, + ).


J is a right ideal of R  if and only if :
:∀ j ∈ J: ∀ r ∈ R: j ∘ r ∈ J

that is,  if and only if :
:∀ r ∈ R: J ∘ r ⊆ J

It follows that in a commutative ring, a left ideal, a right ideal and an ideal are the same thing.


=== Proper Ideal ===
Let ( R, +, ∘) be a ring.


A proper ideal J of ( R, +, ∘) is an ideal of R such that J is a proper subset of R.

That is, such that J ⊆ R and J  R.",Ideal
['Definitions/Ideal Theory'],Definition:Ideal,"Let $left( R, +, circ right)$ be a ring.

Let $left( J, + right)$ be a subgroup of $left( R, + right)$.


$J$ is a left ideal of $R$  if and only if :
:$forall j in J: forall r in R: r circ j in J$

that is,  if and only if :
:$forall r in R: r circ J subseteq J$",Definition:Ideal of Ring/Left Ideal,,false,"Let ( R, +, ∘) be a ring.

Let ( J, + ) be a subgroup of ( R, + ).


J is a left ideal of R  if and only if :
:∀ j ∈ J: ∀ r ∈ R: r ∘ j ∈ J

that is,  if and only if :
:∀ r ∈ R: r ∘ J ⊆ J",Ideal
['Definitions/Ideal Theory'],Definition:Ideal,"Let $left( R, +, circ right)$ be a ring.

Let $left( J, + right)$ be a subgroup of $left( R, + right)$.


$J$ is a right ideal of $R$  if and only if :
:$forall j in J: forall r in R: j circ r in J$

that is,  if and only if :
:$forall r in R: J circ r subseteq J$",Definition:Ideal of Ring/Right Ideal,,false,"Let ( R, +, ∘) be a ring.

Let ( J, + ) be a subgroup of ( R, + ).


J is a right ideal of R  if and only if :
:∀ j ∈ J: ∀ r ∈ R: j ∘ r ∈ J

that is,  if and only if :
:∀ r ∈ R: J ∘ r ⊆ J",Ideal
['Definitions/Algebras'],Definition:Ideal,"Let $R$ be a ring.

Let $left( A, ast right)$ be an $R$-algebra.

Let $left( J, ast right)$ be a subalgebra of $left( A, ast right)$. 


We say that $J$ is an ideal of $A$  if and only if :
:for each $a in A$ and $x in J$ we have $a ast x in J$ and $x ast a in J$. 


=== Left Ideal ===
Let $R$ be a ring.

Let $left( A, ast right)$ be an $R$-algebra.

Let $left( J, ast right)$ be a subalgebra of $left( A, ast right)$. 


We say that $J$ is a left ideal of $A$  if and only if :
:for each $a in A$ and $x in J$ we have $a ast x in J$.

=== Right Ideal ===
Let $R$ be a ring.

Let $left( A, ast right)$ be an $R$-algebra.

Let $left( J, ast right)$ be a subalgebra of $left( A, ast right)$. 


We say that $J$ is a left ideal of $A$  if and only if :
:for each $a in A$ and $x in J$ we have $x ast a in J$.

=== Proper Ideal ===
Let $R$ be a ring.

Let $left( A, ast right)$ be an $R$-algebra.

Let $left( J, ast right)$ be a subalgebra of $left( A, ast right)$. 


We say that $J$ is a proper ideal of $A$  if and only if :
:$J ne A$",Definition:Ideal of Algebra,,false,"Let R be a ring.

Let ( A, ∗) be an R-algebra.

Let ( J, ∗) be a subalgebra of ( A, ∗). 


We say that J is an ideal of A  if and only if :
:for each a ∈ A and x ∈ J we have a ∗ x ∈ J and x ∗ a ∈ J. 


=== Left Ideal ===
Let R be a ring.

Let ( A, ∗) be an R-algebra.

Let ( J, ∗) be a subalgebra of ( A, ∗). 


We say that J is a left ideal of A  if and only if :
:for each a ∈ A and x ∈ J we have a ∗ x ∈ J.

=== Right Ideal ===
Let R be a ring.

Let ( A, ∗) be an R-algebra.

Let ( J, ∗) be a subalgebra of ( A, ∗). 


We say that J is a left ideal of A  if and only if :
:for each a ∈ A and x ∈ J we have x ∗ a ∈ J.

=== Proper Ideal ===
Let R be a ring.

Let ( A, ∗) be an R-algebra.

Let ( J, ∗) be a subalgebra of ( A, ∗). 


We say that J is a proper ideal of A  if and only if :
:J  A",Ideal
['Definitions/Ideals of Algebras'],Definition:Ideal,"Let $R$ be a ring.

Let $left( A, ast right)$ be an $R$-algebra.

Let $left( J, ast right)$ be a subalgebra of $left( A, ast right)$. 


We say that $J$ is a left ideal of $A$  if and only if :
:for each $a in A$ and $x in J$ we have $a ast x in J$.",Definition:Ideal of Algebra/Left Ideal,,false,"Let R be a ring.

Let ( A, ∗) be an R-algebra.

Let ( J, ∗) be a subalgebra of ( A, ∗). 


We say that J is a left ideal of A  if and only if :
:for each a ∈ A and x ∈ J we have a ∗ x ∈ J.",Ideal
['Definitions/Ideals of Algebras'],Definition:Ideal,"Let $R$ be a ring.

Let $left( A, ast right)$ be an $R$-algebra.

Let $left( J, ast right)$ be a subalgebra of $left( A, ast right)$. 


We say that $J$ is a left ideal of $A$  if and only if :
:for each $a in A$ and $x in J$ we have $x ast a in J$.",Definition:Ideal of Algebra/Right Ideal,,false,"Let R be a ring.

Let ( A, ∗) be an R-algebra.

Let ( J, ∗) be a subalgebra of ( A, ∗). 


We say that J is a left ideal of A  if and only if :
:for each a ∈ A and x ∈ J we have x ∗ a ∈ J.",Ideal
['Definitions/Order Theory'],Definition:Ideal,"Let $left( S, preceq right)$ be an ordered set.

Let $I subseteq S$ be a non-empty subset of $S$.


Then $I$ is an ideal of $S$  if and only if  $I$ satisifies the ideal axioms:
 

=== Proper Ideal ===
Let $left( S, preccurlyeq right)$ be an ordered set.

Let $mathcal I$ be an ideal on $left( S, preccurlyeq right)$.


Then:
:$mathcal I$ is a proper ideal on $S$
 if and only if :
:$mathcal I ne S$

That is,  if and only if  $mathcal I$ is a proper subset of $S$.

=== Join Semilattice ===
Let $left( S, vee, preceq right)$ be a join semilattice.

Let $I subseteq S$ be a non-empty subset of $S$.


Then $I$ is a join semilattice ideal of $S$  if and only if  $I$ satisifies the join semilattice ideal axioms:
 

=== Lattice ===
Let $left( S, vee, wedge, preceq right)$ be a lattice.

Let $I subseteq S$ be a non-empty subset of $S$.



$I$ is a lattice ideal of $S$  if and only if  $I$ satisifes the lattice ideal axioms:
 ",Definition:Ideal (Order Theory),,false,"Let ( S, ≼) be an ordered set.

Let I ⊆ S be a non-empty subset of S.


Then I is an ideal of S  if and only if  I satisifies the ideal axioms:
 

=== Proper Ideal ===
Let ( S, ≼) be an ordered set.

Let ℐ be an ideal on ( S, ≼).


Then:
:ℐ is a proper ideal on S
 if and only if :
:ℐ S

That is,  if and only if  ℐ is a proper subset of S.

=== Join Semilattice ===
Let ( S, ∨, ≼) be a join semilattice.

Let I ⊆ S be a non-empty subset of S.


Then I is a join semilattice ideal of S  if and only if  I satisifies the join semilattice ideal axioms:
 

=== Lattice ===
Let ( S, ∨, ∧, ≼) be a lattice.

Let I ⊆ S be a non-empty subset of S.



I is a lattice ideal of S  if and only if  I satisifes the lattice ideal axioms:
 ",Ideal
['Definitions/Lattice Theory'],Definition:Ideal,"Let $left( S, vee, preceq right)$ be a join semilattice.

Let $I subseteq S$ be a non-empty subset of $S$.


Then $I$ is a join semilattice ideal of $S$  if and only if  $I$ satisifies the join semilattice ideal axioms:
 ",Definition:Join Semilattice Ideal,,false,"Let ( S, ∨, ≼) be a join semilattice.

Let I ⊆ S be a non-empty subset of S.


Then I is a join semilattice ideal of S  if and only if  I satisifies the join semilattice ideal axioms:
 ",Ideal
"['Definitions/Lattice Ideals', 'Definitions/Lattice Theory']",Definition:Ideal,"Let $left( S, vee, wedge, preceq right)$ be a lattice.

Let $I subseteq S$ be a non-empty subset of $S$.


=== Definition 1 ===
Let $left( S, vee, wedge, preceq right)$ be a lattice.

Let $I subseteq S$ be a non-empty subset of $S$.



$I$ is a lattice ideal of $S$  if and only if  $I$ satisifes the lattice ideal axioms:
 

=== Definition 2 ===
Let $left( S, vee, wedge, preceq right)$ be a lattice.

Let $I subseteq S$ be a non-empty subset of $S$.



$I$ is a lattice ideal of $S$  if and only if  $I$ is a join semilattice ideal",Definition:Lattice Ideal,,false,"Let ( S, ∨, ∧, ≼) be a lattice.

Let I ⊆ S be a non-empty subset of S.


=== Definition 1 ===
Let ( S, ∨, ∧, ≼) be a lattice.

Let I ⊆ S be a non-empty subset of S.



I is a lattice ideal of S  if and only if  I satisifes the lattice ideal axioms:
 

=== Definition 2 ===
Let ( S, ∨, ∧, ≼) be a lattice.

Let I ⊆ S be a non-empty subset of S.



I is a lattice ideal of S  if and only if  I is a join semilattice ideal",Ideal
"['Definitions/Physics', 'Definitions/Applied Mathematics', 'Definitions/Ideals in Physics']",Definition:Ideal,An ideal (or idealized) object is one in which certain attributes are approximated to zero or infinity.,Definition:Ideal (Physics),,false,An ideal (or idealized) object is one in which certain attributes are approximated to zero or infinity.,Ideal
"['Definitions/Identities (Equations)', 'Definitions/Equations', 'Definitions/Identities']",Definition:Identity,"An identity is an equation which is true for all values attained by the variables it contains.


=== Symbol ===
",Definition:Identity (Equation),equation,true,"An identity is an equation which is true for all values attained by the variables it contains.


=== Symbol ===
",Identity
"['Definitions/Identity Elements', 'Definitions/Abstract Algebra']",Definition:Identity,"Let $left( S, circ right)$ be an algebraic structure.


=== Left Identity ===
Let $left( S, circ right)$ be an algebraic structure.

An element $e_L in S$ is called a left identity (element)  if and only if :
:$forall x in S: e_L circ x = x$

=== Right Identity ===
Let $left( S, circ right)$ be an algebraic structure.

An element $e_R in S$ is called a right identity (element)  if and only if :
:$forall x in S: x circ e_R = x$

=== Two-Sided Identity ===
Let $left( S, circ right)$ be an algebraic structure.

An element $e in S$ is called an identity (element)  if and only if  it is both a left identity and a right identity:

:$forall x in S: x circ e = x = e circ x$


In Identity is Unique it is established that an identity element, if it exists, is unique within $left( S, circ right)$.

Thus it is justified to refer to it as the identity (of a given algebraic structure).


This identity is often denoted $e_S$, or $e$ if it is clearly understood what structure is being discussed.",Definition:Identity (Abstract Algebra),,false,"Let ( S, ∘) be an algebraic structure.


=== Left Identity ===
Let ( S, ∘) be an algebraic structure.

An element e_L ∈ S is called a left identity (element)  if and only if :
:∀ x ∈ S: e_L ∘ x = x

=== Right Identity ===
Let ( S, ∘) be an algebraic structure.

An element e_R ∈ S is called a right identity (element)  if and only if :
:∀ x ∈ S: x ∘ e_R = x

=== Two-Sided Identity ===
Let ( S, ∘) be an algebraic structure.

An element e ∈ S is called an identity (element)  if and only if  it is both a left identity and a right identity:

:∀ x ∈ S: x ∘ e = x = e ∘ x


In Identity is Unique it is established that an identity element, if it exists, is unique within ( S, ∘).

Thus it is justified to refer to it as the identity (of a given algebraic structure).


This identity is often denoted e_S, or e if it is clearly understood what structure is being discussed.",Identity
['Definitions/Images'],Definition:Image,"Let $f: S to T$ be a mapping.


=== Definition 1 ===
The image of a mapping $f: S to T$ is the set:

:$mathrm {Img} left( f right) = leftlbrace t in T: exists s in S: f left(   right)s = t rightrbrace$

That is, it is the set of values taken by $f$.

=== Definition 2 ===
The image of a mapping $f: S to T$ is the set:

:$mathrm {Img} left( f right) = f left[ S right]$

where $f left[ S right]$ is the image of $S$ under $f$.


=== Class Theory ===

 
Let $V$ be a basic universe.

Let $A subseteq V$ and $B subseteq V$ be classes.

Let $f: A to B$ be a class mapping.

The image of $mathcal R$ is defined and denoted as:
:$mathrm {Img} left( mathcal R right) := leftlbrace y in V: exists x in V: left( x, y right) in mathcal R rightrbrace$

That is, it is the class of all $y$ such that $left( x, y right) in mathcal R$ for at least one $x$.",Definition:Image (Relation Theory)/Mapping/Mapping,,false,"Let f: S → T be a mapping.


=== Definition 1 ===
The image of a mapping f: S → T is the set:

:Img( f ) = { t ∈ T: ∃ s ∈ S: f (   )s = t }

That is, it is the set of values taken by f.

=== Definition 2 ===
The image of a mapping f: S → T is the set:

:Img( f ) = f [ S ]

where f [ S ] is the image of S under f.


=== Class Theory ===

 
Let V be a basic universe.

Let A ⊆ V and B ⊆ V be classes.

Let f: A → B be a class mapping.

The image of ℛ is defined and denoted as:
:Img( ℛ) := { y ∈ V: ∃ x ∈ V: ( x, y ) ∈ℛ}

That is, it is the class of all y such that ( x, y ) ∈ℛ for at least one x.",Image
"['Definitions/Images', 'Definitions/Relations']",Definition:Image,"Let $mathcal R subseteq S times T$ be a relation.


The image of $mathcal R$ is defined as:

:$mathrm {Img} left( mathcal R right) := mathcal R left[ S right] = leftlbrace t in T: exists s in S: left( s, t right) in mathcal R rightrbrace$


=== General Definition ===
Let $ds prod_{i mathop = 1}^n S_i$ be the cartesian product of sets $S_1$ to $S_n$.

Let $ds mathcal R subseteq prod_{i mathop = 1}^n S_i$ be an $n$-ary relation on $ds prod_{i mathop = 1}^n S_i$.

The image of $mathcal R$ is the set defined as:
:$mathrm {Img} left( mathcal R right) := leftlbrace s_n in S_n: exists left( s_1, s_2, ldots, s_{n - 1}  right) in ds prod_{i mathop = 1}^{n - 1} S_i: left( s_1, s_2, ldots, s_n right) in mathcal R rightrbrace$


The concept is usually encountered when $mathcal R$ is an endorelation on $S$:
:$mathrm {Img} left( mathcal R right) := leftlbrace s_n in S: exists left( s_1, s_2, ldots, s_{n - 1}  right) in S^{n - 1}: left( s_1, s_2, ldots, s_n right) in mathcal R rightrbrace$

=== Class Theory ===

 
Let $V$ be a basic universe.

Let $mathcal R subseteq V times V$ be a relation in $V$.

The image of $mathcal R$ is defined and denoted as:
:$mathrm {Img} left( mathcal R right) := leftlbrace y in V: exists x in V: left( x, y right) in mathcal R rightrbrace$

That is, it is the class of all $y$ such that $left( x, y right) in mathcal R$ for at least one $x$.",Definition:Image (Relation Theory)/Relation/Relation,,false,"Let ℛ⊆ S × T be a relation.


The image of ℛ is defined as:

:Img( ℛ) := ℛ[ S ] = { t ∈ T: ∃ s ∈ S: ( s, t ) ∈ℛ}


=== General Definition ===
Let ∏_i  = 1^n S_i be the cartesian product of sets S_1 to S_n.

Let ℛ⊆∏_i  = 1^n S_i be an n-ary relation on ∏_i  = 1^n S_i.

The image of ℛ is the set defined as:
:Img( ℛ) := { s_n ∈ S_n: ∃( s_1, s_2, …, s_n - 1) ∈∏_i  = 1^n - 1 S_i: ( s_1, s_2, …, s_n ) ∈ℛ}


The concept is usually encountered when ℛ is an endorelation on S:
:Img( ℛ) := { s_n ∈ S: ∃( s_1, s_2, …, s_n - 1) ∈ S^n - 1: ( s_1, s_2, …, s_n ) ∈ℛ}

=== Class Theory ===

 
Let V be a basic universe.

Let ℛ⊆ V × V be a relation in V.

The image of ℛ is defined and denoted as:
:Img( ℛ) := { y ∈ V: ∃ x ∈ V: ( x, y ) ∈ℛ}

That is, it is the class of all y such that ( x, y ) ∈ℛ for at least one x.",Image
"['Definitions/Column Space', 'Definitions/Matrix Theory', 'Definitions/Linear Algebra']",Definition:Image,"Let $R$ be a ring.

Let:

:$mathbf A_{m times n} = begin{bmatrix}
a_{1 1} & a_{1 2} & cdots & a_{1 n} \
a_{2 1} & a_{2 2} & cdots & a_{2 n} \
vdots & vdots & ddots & vdots \
a_{m 1} & a_{m 2} & cdots & a_{m n} \
end{bmatrix}$

be a matrix over $R$ such that every column is defined as a vector:

:$forall i: 1 le i le m: begin {bmatrix} a_{1 i} \ a_{2 i} \ vdots \ a_{m i} end {bmatrix} in mathbf V$

where $mathbf V$ is some vector space.


Then the column space of $mathbf A$ is the linear span of all such column vectors:

:$mathrm C left(   right){mathbf A} = mathrm {span} left(   right){begin {bmatrix} a_{1 1} \ a_{2 1} \ vdots \ a_{m 1} end {bmatrix}, begin {bmatrix} a_{1 2} \ a_{2 2} \ vdots \ a_{m 2} end {bmatrix}, cdots, begin {bmatrix} a_{1 n} \ a_{2 n} \ vdots \ a_{m n} end {bmatrix} }$",Definition:Column Space,,false,"Let R be a ring.

Let:

:𝐀_m × n = [ a_1 1 a_1 2     ⋯ a_1 n; a_2 1 a_2 2     ⋯ a_2 n;     ⋮     ⋮     ⋱     ⋮; a_m 1 a_m 2     ⋯ a_m n;       ]

be a matrix over R such that every column is defined as a vector:

:∀ i: 1 ≤ i ≤ m: [ a_1 i; a_2 i;     ⋮; a_m i ]∈𝐕

where 𝐕 is some vector space.


Then the column space of 𝐀 is the linear span of all such column vectors:

:C(   )𝐀 = span(   )[ a_1 1; a_2 1;     ⋮; a_m 1 ], [ a_1 2; a_2 2;     ⋮; a_m 2 ], ⋯, [ a_1 n; a_2 n;     ⋮; a_m n ]",Image
['Definitions/Vector Analysis'],Definition:Image,"=== Real Cartesian Space ===
 

=== Complex Plane ===
Let $C$ be a contour in $mathbb C$ defined by the (finite) sequence $leftlangle C_1, ldots, C_n rightrangle$ of directed smooth curves in $mathbb C$.

Let $C_k$ be parameterized by the smooth path $gamma_k: left[ a_k ,.,.,   right]{b_k} to mathbb C$ for all $k in leftlbrace 1, ldots, n rightrbrace$.


The image of $C$ is defined as:

:$ds mathrm {Img} left( C right) := bigcup_{k mathop = 1}^n mathrm {Img} left( gamma_k right)$

where $mathrm {Img} left( gamma_k right)$ denotes the image of $gamma_k$.


If $mathrm {Img} left( C right) subseteq D$, where $D$ is a subset of $mathbb C$, we say that $C$ is a contour in $D$.

Category:Definitions/Vector Analysis",Definition:Image of Contour,,false,"=== Real Cartesian Space ===
 

=== Complex Plane ===
Let C be a contour in ℂ defined by the (finite) sequence ⟨ C_1, …, C_n ⟩ of directed smooth curves in ℂ.

Let C_k be parameterized by the smooth path γ_k: [ a_k  . . ]b_k→ℂ for all k ∈{ 1, …, n }.


The image of C is defined as:

:Img( C ) := ⋃_k  = 1^n Img( γ_k )

where Img( γ_k ) denotes the image of γ_k.


If Img( C ) ⊆ D, where D is a subset of ℂ, we say that C is a contour in D.

Category:Definitions/Vector Analysis",Image
['Definitions/Images'],Definition:Image Set,"Let $f: S to T$ be a mapping.


=== Definition 1 ===
The image of a mapping $f: S to T$ is the set:

:$mathrm {Img} left( f right) = leftlbrace t in T: exists s in S: f left(   right)s = t rightrbrace$

That is, it is the set of values taken by $f$.

=== Definition 2 ===
The image of a mapping $f: S to T$ is the set:

:$mathrm {Img} left( f right) = f left[ S right]$

where $f left[ S right]$ is the image of $S$ under $f$.


=== Class Theory ===

 
Let $V$ be a basic universe.

Let $A subseteq V$ and $B subseteq V$ be classes.

Let $f: A to B$ be a class mapping.

The image of $mathcal R$ is defined and denoted as:
:$mathrm {Img} left( mathcal R right) := leftlbrace y in V: exists x in V: left( x, y right) in mathcal R rightrbrace$

That is, it is the class of all $y$ such that $left( x, y right) in mathcal R$ for at least one $x$.",Definition:Image (Relation Theory)/Mapping/Mapping,,false,"Let f: S → T be a mapping.


=== Definition 1 ===
The image of a mapping f: S → T is the set:

:Img( f ) = { t ∈ T: ∃ s ∈ S: f (   )s = t }

That is, it is the set of values taken by f.

=== Definition 2 ===
The image of a mapping f: S → T is the set:

:Img( f ) = f [ S ]

where f [ S ] is the image of S under f.


=== Class Theory ===

 
Let V be a basic universe.

Let A ⊆ V and B ⊆ V be classes.

Let f: A → B be a class mapping.

The image of ℛ is defined and denoted as:
:Img( ℛ) := { y ∈ V: ∃ x ∈ V: ( x, y ) ∈ℛ}

That is, it is the class of all y such that ( x, y ) ∈ℛ for at least one x.",Image Set
"['Definitions/Images', 'Definitions/Relations']",Definition:Image Set,"Let $mathcal R subseteq S times T$ be a relation.


The image of $mathcal R$ is defined as:

:$mathrm {Img} left( mathcal R right) := mathcal R left[ S right] = leftlbrace t in T: exists s in S: left( s, t right) in mathcal R rightrbrace$


=== General Definition ===
Let $ds prod_{i mathop = 1}^n S_i$ be the cartesian product of sets $S_1$ to $S_n$.

Let $ds mathcal R subseteq prod_{i mathop = 1}^n S_i$ be an $n$-ary relation on $ds prod_{i mathop = 1}^n S_i$.

The image of $mathcal R$ is the set defined as:
:$mathrm {Img} left( mathcal R right) := leftlbrace s_n in S_n: exists left( s_1, s_2, ldots, s_{n - 1}  right) in ds prod_{i mathop = 1}^{n - 1} S_i: left( s_1, s_2, ldots, s_n right) in mathcal R rightrbrace$


The concept is usually encountered when $mathcal R$ is an endorelation on $S$:
:$mathrm {Img} left( mathcal R right) := leftlbrace s_n in S: exists left( s_1, s_2, ldots, s_{n - 1}  right) in S^{n - 1}: left( s_1, s_2, ldots, s_n right) in mathcal R rightrbrace$

=== Class Theory ===

 
Let $V$ be a basic universe.

Let $mathcal R subseteq V times V$ be a relation in $V$.

The image of $mathcal R$ is defined and denoted as:
:$mathrm {Img} left( mathcal R right) := leftlbrace y in V: exists x in V: left( x, y right) in mathcal R rightrbrace$

That is, it is the class of all $y$ such that $left( x, y right) in mathcal R$ for at least one $x$.",Definition:Image (Relation Theory)/Relation/Relation,,false,"Let ℛ⊆ S × T be a relation.


The image of ℛ is defined as:

:Img( ℛ) := ℛ[ S ] = { t ∈ T: ∃ s ∈ S: ( s, t ) ∈ℛ}


=== General Definition ===
Let ∏_i  = 1^n S_i be the cartesian product of sets S_1 to S_n.

Let ℛ⊆∏_i  = 1^n S_i be an n-ary relation on ∏_i  = 1^n S_i.

The image of ℛ is the set defined as:
:Img( ℛ) := { s_n ∈ S_n: ∃( s_1, s_2, …, s_n - 1) ∈∏_i  = 1^n - 1 S_i: ( s_1, s_2, …, s_n ) ∈ℛ}


The concept is usually encountered when ℛ is an endorelation on S:
:Img( ℛ) := { s_n ∈ S: ∃( s_1, s_2, …, s_n - 1) ∈ S^n - 1: ( s_1, s_2, …, s_n ) ∈ℛ}

=== Class Theory ===

 
Let V be a basic universe.

Let ℛ⊆ V × V be a relation in V.

The image of ℛ is defined and denoted as:
:Img( ℛ) := { y ∈ V: ∃ x ∈ V: ( x, y ) ∈ℛ}

That is, it is the class of all y such that ( x, y ) ∈ℛ for at least one x.",Image Set
"['Definitions/Improper Fractions', 'Definitions/Fractions']",Definition:Improper,"An improper fraction is a fraction representing a rational number whose absolute value is greater than $1$.

Specifically, when expressed in the form $r = dfrac p q$, where $p$ and $q$ are integers such that (the absolute value of) the numerator is greater than (the absolute value of) the denominator: $leftlvert p rightrvert > leftlvert q rightrvert$.",Definition:Fraction/Improper,,false,"An improper fraction is a fraction representing a rational number whose absolute value is greater than 1.

Specifically, when expressed in the form r =  p q, where p and q are integers such that (the absolute value of) the numerator is greater than (the absolute value of) the denominator: | p | > | q |.",Improper
"['Definitions/Improper Integrals', 'Definitions/Definite Integrals', 'Definitions/Integral Calculus']",Definition:Improper,"An improper integral is a definite integral over an interval which is not closed, that is, open or half open, and whose limits of integration are the end points of that interval.

When the end point is not actually in the interval, the conventional definition of the definite integral is not valid.

Therefore we use the technique of limits to specify the integral.


Note: In the below, in all cases the necessary limits must exist in order for the definition to hold.",Definition:Improper Integral,,false,"An improper integral is a definite integral over an interval which is not closed, that is, open or half open, and whose limits of integration are the end points of that interval.

When the end point is not actually in the interval, the conventional definition of the definite integral is not valid.

Therefore we use the technique of limits to specify the integral.


Note: In the below, in all cases the necessary limits must exist in order for the definition to hold.",Improper
"['Definitions/Inconsistent (Logic)', 'Definitions/Proof Systems', 'Definitions/Logic']",Definition:Inconsistent,"Let $mathcal L$ be a logical language.

Let $mathscr P$ be a proof system for $mathcal L$.

=== Definition 1 ===
Let $mathcal L$ be a logical language.

Let $mathscr P$ be a proof system for $mathcal L$.


A set $mathcal F$ of logical formulas is inconsistent for $mathscr P$  if and only if :

:For every logical formula $phi$, $mathcal F vdash_{mathscr P} phi$.

That is, every logical formula $phi$ is a provable consequence of $mathcal F$.

=== Definition 2 ===
Let $mathcal L$ be a logical language.

Let $mathscr P$ be a proof system for $mathcal L$.


A set $mathcal F$ of logical formulas is inconsistent for $mathscr P$  if and only if :

:There exists a logical formula $phi$ such that both
::$mathcal F vdash_{mathscr P} left( phi land neg phi right)$",Definition:Inconsistent (Logic),,false,"Let ℒ be a logical language.

Let 𝒫 be a proof system for ℒ.

=== Definition 1 ===
Let ℒ be a logical language.

Let 𝒫 be a proof system for ℒ.


A set ℱ of logical formulas is inconsistent for 𝒫  if and only if :

:For every logical formula ϕ, ℱ⊢_𝒫ϕ.

That is, every logical formula ϕ is a provable consequence of ℱ.

=== Definition 2 ===
Let ℒ be a logical language.

Let 𝒫 be a proof system for ℒ.


A set ℱ of logical formulas is inconsistent for 𝒫  if and only if :

:There exists a logical formula ϕ such that both
::ℱ⊢_𝒫( ϕϕ)",Inconsistent
"['Definitions/Consistent Simultaneous Equations', 'Definitions/Simultaneous Equations']",Definition:Inconsistent,"A set of equations is described as inconsistent  if and only if  they are not consistent

That is, there exists no set of values for its variables such that all the equations are satisfied.",Definition:Consistent Simultaneous Equations/Inconsistent,,false,"A set of equations is described as inconsistent  if and only if  they are not consistent

That is, there exists no set of values for its variables such that all the equations are satisfied.",Inconsistent
"['Definitions/Independent Variables', 'Definitions/Analysis', 'Definitions/Mapping Theory']",Definition:Independent,"=== Real Function ===
Let $f: mathbb R to mathbb R$ be a real function.

Let $f left(   right)x = y$.

Then $x$ is referred to as an independent variable.

=== Complex Function ===
Let $f: mathbb C to mathbb C$ be a complex function.

Let $f left(   right)z = w$.


Then $z$ is referred to as an independent variable (of $f$).",Definition:Independent Variable,,false,"=== Real Function ===
Let f: ℝ→ℝ be a real function.

Let f (   )x = y.

Then x is referred to as an independent variable.

=== Complex Function ===
Let f: ℂ→ℂ be a complex function.

Let f (   )z = w.


Then z is referred to as an independent variable (of f).",Independent
"['Definitions/Independent Events', 'Definitions/Probability Theory']",Definition:Independent,"Let $mathcal E$ be an experiment with probability space $left( Omega, unicode{x3a3}, Pr right)$.

Let $A, B in unicode{x3a3}$ be events of $mathcal E$ such that $Pr left(   right)A > 0$ and $Pr left(   right)B > 0$.


=== Definition 1 ===
Let $mathcal E$ be an experiment with probability space $left( Omega, unicode{x3a3}, Pr right)$.

Let $A, B in unicode{x3a3}$ be events of $mathcal E$ such that $Pr left(   right)A > 0$ and $Pr left(   right)B > 0$.


The events $A$ and $B$ are defined as independent (of each other)  if and only if  the occurrence of one of them does not affect the probability of the occurrence of the other one.


Formally, $A$ is independent of $B$  if and only if :
:$Pr left( A , middle vert ,   right)B = Pr left(   right)A$
where $Pr left( A , middle vert ,   right)B$ denotes the conditional probability of $A$ given $B$.

=== Definition 2 ===
Let $mathcal E$ be an experiment with probability space $left( Omega, unicode{x3a3}, Pr right)$.

Let $A, B in unicode{x3a3}$ be events of $mathcal E$ such that $Pr left(   right)A > 0$ and $Pr left(   right)B > 0$.


The events $A$ and $B$ are defined as independent (of each other)  if and only if  the occurrence of both of them together has the same probability as the product of the probabilities of each of them occurring on their own.


Formally, $A$ and $B$ are independent  if and only if :
:$Pr left(   right){A cap B} = Pr left(   right)A Pr left(   right)B$",Definition:Independent Events,,false,"Let ℰ be an experiment with probability space ( Ω, x3a3, ).

Let A, B ∈x3a3 be events of ℰ such that (   )A > 0 and (   )B > 0.


=== Definition 1 ===
Let ℰ be an experiment with probability space ( Ω, x3a3, ).

Let A, B ∈x3a3 be events of ℰ such that (   )A > 0 and (   )B > 0.


The events A and B are defined as independent (of each other)  if and only if  the occurrence of one of them does not affect the probability of the occurrence of the other one.


Formally, A is independent of B  if and only if :
:( A  | )B = (   )A
where ( A  | )B denotes the conditional probability of A given B.

=== Definition 2 ===
Let ℰ be an experiment with probability space ( Ω, x3a3, ).

Let A, B ∈x3a3 be events of ℰ such that (   )A > 0 and (   )B > 0.


The events A and B are defined as independent (of each other)  if and only if  the occurrence of both of them together has the same probability as the product of the probabilities of each of them occurring on their own.


Formally, A and B are independent  if and only if :
:(   )A ∩ B = (   )A (   )B",Independent
['Definitions/Group Theory'],Definition:Independent,"Let $G$ be a group whose identity is $e$.

Let $left langle {H_n} right rangle$ be a sequence of subgroups of $G$.


=== Definition 1 ===
Let $G$ be a group whose identity is $e$.

Let $leftlangle H_n rightrangle$ be a sequence of subgroups of $G$.


The subgroups $H_1, H_2, ldots, H_n$ are independent  if and only if :

:$ds prod_{k mathop = 1}^n h_k = e iff forall k in leftlbrace 1, 2, ldots, n rightrbrace: h_k = e$
where $h_k in H_k$ for all $k in leftlbrace 1, 2, ldots, n rightrbrace$.


That is, the product of any elements from different $H_k$ instances forms the identity  if and only if  all of those elements are the identity.

=== Definition 2 ===
Let $G$ be a group whose identity is $e$.

Let $leftlangle H_n rightrangle$ be a sequence of subgroups of $G$.


The subgroups $H_1, H_2, ldots, H_n$ are independent  if and only if :

:$ds forall k in leftlbrace 2, 3, ldots, n rightrbrace: left( prod_{j mathop = 1}^{k - 1} H_j right) cap H_k = leftlbrace e rightrbrace$


That is, the product of any elements from different $H_k$ instances forms the identity  if and only if  all of those elements are the identity.",Definition:Independent Subgroups,,false,"Let G be a group whose identity is e.

Let ⟨H_n⟩ be a sequence of subgroups of G.


=== Definition 1 ===
Let G be a group whose identity is e.

Let ⟨ H_n ⟩ be a sequence of subgroups of G.


The subgroups H_1, H_2, …, H_n are independent  if and only if :

:∏_k  = 1^n h_k = e ∀ k ∈{ 1, 2, …, n }: h_k = e
where h_k ∈ H_k for all k ∈{ 1, 2, …, n }.


That is, the product of any elements from different H_k instances forms the identity  if and only if  all of those elements are the identity.

=== Definition 2 ===
Let G be a group whose identity is e.

Let ⟨ H_n ⟩ be a sequence of subgroups of G.


The subgroups H_1, H_2, …, H_n are independent  if and only if :

:∀ k ∈{ 2, 3, …, n }: ( ∏_j  = 1^k - 1 H_j ) ∩ H_k = { e }


That is, the product of any elements from different H_k instances forms the identity  if and only if  all of those elements are the identity.",Independent
"['Definitions/Linear Independence', 'Definitions/Vector Spaces', 'Definitions/Module Theory', 'Definitions/Linear Algebra', 'Definitions/Linearity']",Definition:Independent,"Let $G$ be an abelian group whose identity is $e$.

Let $R$ be a ring with unity whose zero is $0_R$ and whose unity is $1_R$.


Let $left( G, +_G, circ right)_R$ be a unitary $R$-module.


=== Sequence ===
Let $G$ be an abelian group whose identity is $e$.

Let $R$ be a ring with unity whose zero is $0_R$ and whose unity is $1_R$.


Let $left( G, +_G, circ right)_R$ be a unitary $R$-module.


Let $leftlangle a_n rightrangle$ be a sequence of elements of $G$ such that:
:$ds forall leftlangle lambda_n rightrangle subseteq R: sum_{k mathop = 1}^n lambda_k circ a_k = e implies lambda_1 = lambda_2 = cdots = lambda_n = 0_R$

That is, the only way to make $e$ with a linear combination of $leftlangle a_n rightrangle$ is by making all the terms of $leftlangle lambda_n rightrangle$ equal to $0_R$.


Such a sequence is linearly independent.


=== Linearly Independent Sequence on a Real Vector Space ===
Let $left( mathbb R^n, +, cdot right)_mathbb R$ be a real vector space.

Let $leftlangle mathbf v_n rightrangle$ be a sequence of vectors in $mathbb R^n$.

Then $leftlangle mathbf v_n rightrangle$ is linearly independent  if and only if :

:$ds forall leftlangle lambda_n rightrangle subseteq mathbb R: sum_{k mathop = 1}^n lambda_k mathbf v_k = mathbf 0 implies lambda_1 = lambda_2 = cdots = lambda_n = 0$

where $mathbf 0 in mathbb R^n$ is the zero vector and $0 in mathbb R$ is the zero scalar.

=== Set ===
Let $G$ be an abelian group whose identity is $e$.

Let $R$ be a ring with unity whose zero is $0_R$ and whose unity is $1_R$.


Let $left( G, +_G, circ right)_R$ be a unitary $R$-module.


Let $S subseteq G$.


Then $S$ is a linearly independent set (over $R$)  if and only if  every finite sequence of distinct terms in $S$ is a linearly independent sequence.

That is, such that:
:$ds forall leftlangle lambda_n rightrangle subseteq R: sum_{k mathop = 1}^n lambda_k circ a_k = e implies lambda_1 = lambda_2 = cdots = lambda_n = 0_R$
where $a_1, a_2, ldots, a_k$ are distinct elements of $S$.


=== Linearly Independent Set on a Real Vector Space ===
Let $left( mathbb R^n, +, cdot right)_mathbb R$ be a real vector space.

Let $S subseteq mathbb R^n$.


Then $S$ is a linearly independent set of real vectors  if and only if  every finite sequence of distinct terms in $S$ is a linearly independent sequence.

That is, such that:
:$ds forall leftlbrace lambda_k: 1 le k le n rightrbrace subseteq mathbb R: sum_{k mathop = 1}^n lambda_k mathbf v_k = mathbf 0 implies lambda_1 = lambda_2 = cdots = lambda_n = 0$
where $mathbf v_1, mathbf v_2, ldots, mathbf v_n$ are distinct elements of $S$.

=== Linearly Independent Set on a Complex Vector Space ===
Let $left( mathbb C^n, +, cdot right)_mathbb C$ be a complex vector space.

Let $S subseteq mathbb C^n$.


Then $S$ is a linearly independent set of complex vectors  if and only if  every finite sequence of distinct terms in $S$ is a linearly independent sequence.

That is, such that:
:$ds forall leftlbrace lambda_k: 1 le k le n rightrbrace subseteq mathbb C: sum_{k mathop = 1}^n lambda_k mathbf v_k = mathbf 0 implies lambda_1 = lambda_2 = cdots = lambda_n = 0$
where $mathbf v_1, mathbf v_2, ldots, mathbf v_n$ are distinct elements of $S$.",Definition:Linearly Independent,,false,"Let G be an abelian group whose identity is e.

Let R be a ring with unity whose zero is 0_R and whose unity is 1_R.


Let ( G, +_G, ∘)_R be a unitary R-module.


=== Sequence ===
Let G be an abelian group whose identity is e.

Let R be a ring with unity whose zero is 0_R and whose unity is 1_R.


Let ( G, +_G, ∘)_R be a unitary R-module.


Let ⟨ a_n ⟩ be a sequence of elements of G such that:
:∀⟨λ_n ⟩⊆ R: ∑_k  = 1^n λ_k ∘ a_k = e λ_1 = λ_2 = ⋯ = λ_n = 0_R

That is, the only way to make e with a linear combination of ⟨ a_n ⟩ is by making all the terms of ⟨λ_n ⟩ equal to 0_R.


Such a sequence is linearly independent.


=== Linearly Independent Sequence on a Real Vector Space ===
Let ( ℝ^n, +, ·)_ℝ be a real vector space.

Let ⟨𝐯_n ⟩ be a sequence of vectors in ℝ^n.

Then ⟨𝐯_n ⟩ is linearly independent  if and only if :

:∀⟨λ_n ⟩⊆ℝ: ∑_k  = 1^n λ_k 𝐯_k = 0λ_1 = λ_2 = ⋯ = λ_n = 0

where 0∈ℝ^n is the zero vector and 0 ∈ℝ is the zero scalar.

=== Set ===
Let G be an abelian group whose identity is e.

Let R be a ring with unity whose zero is 0_R and whose unity is 1_R.


Let ( G, +_G, ∘)_R be a unitary R-module.


Let S ⊆ G.


Then S is a linearly independent set (over R)  if and only if  every finite sequence of distinct terms in S is a linearly independent sequence.

That is, such that:
:∀⟨λ_n ⟩⊆ R: ∑_k  = 1^n λ_k ∘ a_k = e λ_1 = λ_2 = ⋯ = λ_n = 0_R
where a_1, a_2, …, a_k are distinct elements of S.


=== Linearly Independent Set on a Real Vector Space ===
Let ( ℝ^n, +, ·)_ℝ be a real vector space.

Let S ⊆ℝ^n.


Then S is a linearly independent set of real vectors  if and only if  every finite sequence of distinct terms in S is a linearly independent sequence.

That is, such that:
:∀{λ_k: 1 ≤ k ≤ n }⊆ℝ: ∑_k  = 1^n λ_k 𝐯_k = 0λ_1 = λ_2 = ⋯ = λ_n = 0
where 𝐯_1, 𝐯_2, …, 𝐯_n are distinct elements of S.

=== Linearly Independent Set on a Complex Vector Space ===
Let ( ℂ^n, +, ·)_ℂ be a complex vector space.

Let S ⊆ℂ^n.


Then S is a linearly independent set of complex vectors  if and only if  every finite sequence of distinct terms in S is a linearly independent sequence.

That is, such that:
:∀{λ_k: 1 ≤ k ≤ n }⊆ℂ: ∑_k  = 1^n λ_k 𝐯_k = 0λ_1 = λ_2 = ⋯ = λ_n = 0
where 𝐯_1, 𝐯_2, …, 𝐯_n are distinct elements of S.",Independent
['Definitions/Logic'],Definition:Independent,"Let $p$ and $q$ be statements.

Let it be the case that:
:$(1): quad p$ and $q$ are not contrary
:$(2): quad p$ and $q$ are not subcontrary
:$(3): quad p$ is not superimplicant to $q$
:$(4): quad p$ is not subimplicant to $q$
:$(5): quad p$ and $q$ are not equivalent
:$(6): quad p$ and $q$ are not contradictory.


Then $p$ and $q$ are independent statements.",Definition:Independent Statements,,false,"Let p and q be statements.

Let it be the case that:
:(1):    p and q are not contrary
:(2):    p and q are not subcontrary
:(3):    p is not superimplicant to q
:(4):    p is not subimplicant to q
:(5):    p and q are not equivalent
:(6):    p and q are not contradictory.


Then p and q are independent statements.",Independent
"['Definitions/Indexed Families', 'Definitions/Indices']",Definition:Index,"Let $I$ and $S$ be sets.

Let $x: I to S$ be a mapping.

Let $x_i$ denote the image of an element $i in I$ of the domain $I$ of $x$.

Let $leftlangle x_i rightrangle_{i mathop in I}$ denote the set of the images of all the element $i in I$ under $x$.


An element of the domain $I$ of $x$ is called an index.",Definition:Indexing Set/Index,,false,"Let I and S be sets.

Let x: I → S be a mapping.

Let x_i denote the image of an element i ∈ I of the domain I of x.

Let ⟨ x_i ⟩_i ∈ I denote the set of the images of all the element i ∈ I under x.


An element of the domain I of x is called an index.",Index
"['Definitions/Indexed Families', 'Definitions/Set Theory', 'Definitions/Mapping Theory']",Definition:Index,"Let $I$ and $S$ be sets.

Let $x: I to S$ be a mapping.

Let $x_i$ denote the image of an element $i in I$ of the domain $I$ of $x$.

Let $leftlangle x_i rightrangle_{i mathop in I}$ denote the set of the images of all the elements $i in I$ under $x$.


When a mapping is used in this context, the domain $I$ of $x$ is called the indexing set of the terms $leftlangle x_i rightrangle_{i mathop in I}$.


=== Index ===
Let $I$ and $S$ be sets.

Let $x: I to S$ be a mapping.

Let $x_i$ denote the image of an element $i in I$ of the domain $I$ of $x$.

Let $leftlangle x_i rightrangle_{i mathop in I}$ denote the set of the images of all the element $i in I$ under $x$.


An element of the domain $I$ of $x$ is called an index.

=== Indexed Set ===
Let $I$ and $S$ be sets.

Let $x: I to S$ be a mapping.

Let $x_i$ denote the image of an element $i in I$ of the domain $I$ of $x$.

Let $leftlangle x_i rightrangle_{i mathop in I}$ denote the set of the images of all the elements $i in I$ under $x$.


The image of $x$, that is, $x left[ I right]$ or $mathrm {Img} left( x right)$, is called an indexed set.

That is, it is the set indexed by $I$.

=== Indexing Function ===
Let $I$ and $S$ be sets.

Let $x: I to S$ be a mapping.

Let $x_i$ denote the image of an element $i in I$ of the domain $I$ of $x$.

Let $leftlangle x_i rightrangle_{i mathop in I}$ denote the set of the images of all the element $i in I$ under $x$.


When used in this context, the mapping $x$ is referred to as an indexing function for $S$.


=== Notation ===
The family of elements $x$ of $S$ indexed by $I$ is often seen with one of the following notations:

:$leftlangle x_i rightrangle_{i mathop in I}$

:$left( x_i right)_{i mathop in I}$

:$leftlbrace x_i rightrbrace_{i mathop in I}$


There is little consistency in the literature, but $left( x_i right)_{i mathop in I}$ is perhaps most common.

The preferred notation on   is $leftlangle x_i rightrangle_{i mathop in I}$.

The subscripted $i in I$ is often left out, if it is obvious in the particular context.


Note the use of $x_i$ to denote the image of the index $i$ under the indexing function $x$.

As $x$ is actually a mapping, one would expect the conventional notation $x left(   right)i$.

However, this is generally not used, and $x_i$ is used instead.


Category:Definitions/Indexed Families

=== Family ===
Let $I$ and $S$ be sets.

Let $x: I to S$ be an indexing function for $S$.

Let $x_i$ denote the image of an element $i in I$ of the domain $I$ of $x$.

Let $leftlangle x_i rightrangle_{i mathop in I}$ denote the set of the images of all the elements $i in I$ under $x$.


The image $mathrm {Img} left( x right)$, consisting of the terms $leftlangle x_i rightrangle_{i mathop in I}$, along with the indexing function $x$ itself, is called a family of elements of $S$ indexed by $I$.

=== Term ===
Let $I$ and $S$ be sets.

Let $x: I to S$ be a mapping.

Let $x_i$ denote the image of an element $i in I$ of the domain $I$ of $x$.

Let $leftlangle x_i rightrangle_{i mathop in I}$ denote the set of the images of all the element $i in I$ under $x$.


The image of $x$ at an index $i$ is referred to as a term of the (indexed) family, and is denoted $x_i$.


=== Notation ===
The family of elements $x$ of $S$ indexed by $I$ is often seen with one of the following notations:

:$leftlangle x_i rightrangle_{i mathop in I}$

:$left( x_i right)_{i mathop in I}$

:$leftlbrace x_i rightrbrace_{i mathop in I}$


There is little consistency in the literature, but $left( x_i right)_{i mathop in I}$ is perhaps most common.

The preferred notation on   is $leftlangle x_i rightrangle_{i mathop in I}$.

The subscripted $i in I$ is often left out, if it is obvious in the particular context.


Note the use of $x_i$ to denote the image of the index $i$ under the indexing function $x$.

As $x$ is actually a mapping, one would expect the conventional notation $x left(   right)i$.

However, this is generally not used, and $x_i$ is used instead.


Category:Definitions/Indexed Families

=== Family of Distinct Elements ===
Let $I$ and $S$ be sets.

Let $x: I to S$ be an indexing function for $S$.

Let $leftlangle x_i rightrangle_{i mathop in I}$ denote the family of elements of $S$ indexed by $x$.


Let $x$ be an injection, that is:
:$forall alpha, beta in I: alpha ne beta implies x_alpha ne x_beta$

Then $leftlangle x_i rightrangle _{i mathop in I}$ is called a family of distinct elements of $S$.

=== Family of Sets ===
Let $mathcal S$ be a set of sets.

Let $I$ be an indexing set.

Let $leftlangle S_i rightrangle_{i mathop in I}$ be a family of elements of $mathcal S$ indexed by $I$.


Then $leftlangle S_i rightrangle_{i mathop in I}$ is referred to as an indexed family of sets.

=== Family of Subsets ===
Let $S$ be a set.

Let $I$ be an indexing set.

For each $i in I$, let $S_i$ be a corresponding subset of $S$.

Let $leftlangle S_i rightrangle_{i mathop in I}$ be a family of subsets of $S$ indexed by $I$.


Then $leftlangle S_i rightrangle_{i mathop in I}$ is referred to as an indexed family of subsets (of $S$ by $I$).",Definition:Indexing Set,,false,"Let I and S be sets.

Let x: I → S be a mapping.

Let x_i denote the image of an element i ∈ I of the domain I of x.

Let ⟨ x_i ⟩_i ∈ I denote the set of the images of all the elements i ∈ I under x.


When a mapping is used in this context, the domain I of x is called the indexing set of the terms ⟨ x_i ⟩_i ∈ I.


=== Index ===
Let I and S be sets.

Let x: I → S be a mapping.

Let x_i denote the image of an element i ∈ I of the domain I of x.

Let ⟨ x_i ⟩_i ∈ I denote the set of the images of all the element i ∈ I under x.


An element of the domain I of x is called an index.

=== Indexed Set ===
Let I and S be sets.

Let x: I → S be a mapping.

Let x_i denote the image of an element i ∈ I of the domain I of x.

Let ⟨ x_i ⟩_i ∈ I denote the set of the images of all the elements i ∈ I under x.


The image of x, that is, x [ I ] or Img( x ), is called an indexed set.

That is, it is the set indexed by I.

=== Indexing Function ===
Let I and S be sets.

Let x: I → S be a mapping.

Let x_i denote the image of an element i ∈ I of the domain I of x.

Let ⟨ x_i ⟩_i ∈ I denote the set of the images of all the element i ∈ I under x.


When used in this context, the mapping x is referred to as an indexing function for S.


=== Notation ===
The family of elements x of S indexed by I is often seen with one of the following notations:

:⟨ x_i ⟩_i ∈ I

:( x_i )_i ∈ I

:{ x_i }_i ∈ I


There is little consistency in the literature, but ( x_i )_i ∈ I is perhaps most common.

The preferred notation on   is ⟨ x_i ⟩_i ∈ I.

The subscripted i ∈ I is often left out, if it is obvious in the particular context.


Note the use of x_i to denote the image of the index i under the indexing function x.

As x is actually a mapping, one would expect the conventional notation x (   )i.

However, this is generally not used, and x_i is used instead.


Category:Definitions/Indexed Families

=== Family ===
Let I and S be sets.

Let x: I → S be an indexing function for S.

Let x_i denote the image of an element i ∈ I of the domain I of x.

Let ⟨ x_i ⟩_i ∈ I denote the set of the images of all the elements i ∈ I under x.


The image Img( x ), consisting of the terms ⟨ x_i ⟩_i ∈ I, along with the indexing function x itself, is called a family of elements of S indexed by I.

=== Term ===
Let I and S be sets.

Let x: I → S be a mapping.

Let x_i denote the image of an element i ∈ I of the domain I of x.

Let ⟨ x_i ⟩_i ∈ I denote the set of the images of all the element i ∈ I under x.


The image of x at an index i is referred to as a term of the (indexed) family, and is denoted x_i.


=== Notation ===
The family of elements x of S indexed by I is often seen with one of the following notations:

:⟨ x_i ⟩_i ∈ I

:( x_i )_i ∈ I

:{ x_i }_i ∈ I


There is little consistency in the literature, but ( x_i )_i ∈ I is perhaps most common.

The preferred notation on   is ⟨ x_i ⟩_i ∈ I.

The subscripted i ∈ I is often left out, if it is obvious in the particular context.


Note the use of x_i to denote the image of the index i under the indexing function x.

As x is actually a mapping, one would expect the conventional notation x (   )i.

However, this is generally not used, and x_i is used instead.


Category:Definitions/Indexed Families

=== Family of Distinct Elements ===
Let I and S be sets.

Let x: I → S be an indexing function for S.

Let ⟨ x_i ⟩_i ∈ I denote the family of elements of S indexed by x.


Let x be an injection, that is:
:∀α, β∈ I: αβ x_α x_β

Then ⟨ x_i ⟩ _i ∈ I is called a family of distinct elements of S.

=== Family of Sets ===
Let 𝒮 be a set of sets.

Let I be an indexing set.

Let ⟨ S_i ⟩_i ∈ I be a family of elements of 𝒮 indexed by I.


Then ⟨ S_i ⟩_i ∈ I is referred to as an indexed family of sets.

=== Family of Subsets ===
Let S be a set.

Let I be an indexing set.

For each i ∈ I, let S_i be a corresponding subset of S.

Let ⟨ S_i ⟩_i ∈ I be a family of subsets of S indexed by I.


Then ⟨ S_i ⟩_i ∈ I is referred to as an indexed family of subsets (of S by I).",Index
"['Definitions/Sequences', 'Definitions/Indices']",Definition:Index,"Let $leftlangle x_n rightrangle$ be a sequence.

Let $x_k$ be the $k$th term of $leftlangle x_n rightrangle$.

Then the integer $k$ is known as the index of $x_k$.",Definition:Term of Sequence/Index,,false,"Let ⟨ x_n ⟩ be a sequence.

Let x_k be the kth term of ⟨ x_n ⟩.

Then the integer k is known as the index of x_k.",Index
"['Definitions/Index of Subgroups', 'Definitions/Group Theory', 'Definitions/Indices']",Definition:Index,"Let $G$ be a group.

Let $H$ be a subgroup of $G$.

The index of $H$ (in $G$), denoted $left[ G :   right]H$, is the cardinality of the left (or right) coset space $G / H$.


=== Finite Index ===
Let $G$ be a group.

Let $H$ be a subgroup of $G$.

Let $left[ G :   right]H$ denote the index of $H$ in $G$, that is, the cardinality of the left (or right) coset space $G / H$.


If $G / H$ is a finite set, then $left[ G :   right]H$ is finite, and $H$ is of finite index in $G$.

=== Infinite Index ===
Let $G$ be a group.

Let $H$ be a subgroup of $G$.

Let $left[ G :   right]H$ denote the index of $H$ in $G$, that is, the cardinality of the left (or right) coset space $G / H$.


If $G / H$ is an infinite set, then $left[ G :   right]H$ is infinite, and $H$ is of infinite index in $G$.",Definition:Index of Subgroup,,false,"Let G be a group.

Let H be a subgroup of G.

The index of H (in G), denoted [ G :   ]H, is the cardinality of the left (or right) coset space G / H.


=== Finite Index ===
Let G be a group.

Let H be a subgroup of G.

Let [ G :   ]H denote the index of H in G, that is, the cardinality of the left (or right) coset space G / H.


If G / H is a finite set, then [ G :   ]H is finite, and H is of finite index in G.

=== Infinite Index ===
Let G be a group.

Let H be a subgroup of G.

Let [ G :   ]H denote the index of H in G, that is, the cardinality of the left (or right) coset space G / H.


If G / H is an infinite set, then [ G :   ]H is infinite, and H is of infinite index in G.",Index
"['Definitions/Matrices', 'Definitions/Indices']",Definition:Index,"Let $mathbf A$ be an $m times n$ matrix.

Let $a_{i j}$ be the element in row $i$ and column $j$ of $mathbf A$.


Then the subscripts $i$ and $j$ are referred to as the indices (singular: index) of $a_{i j}$.",Definition:Matrix/Indices,,false,"Let 𝐀 be an m × n matrix.

Let a_i j be the element in row i and column j of 𝐀.


Then the subscripts i and j are referred to as the indices (singular: index) of a_i j.",Index
"['Definitions/Scientific Notation', 'Definitions/Floating-Point Representation', 'Definitions/Numbers']",Definition:Index,"Scientific notation is an implementation of floating-point representation for representing approximations to (usually large) numbers by presenting them in the form:
:$n approx m times 10^e$

where:
:$m$ is a rational number such that $1 le m < 10$, expressed in decimal notation
:$e$ is an integer.


=== Base ===
Let $n$ be a number expressed in scientific notation as:
:$n approx m times 10^e$


The number $10$, in this context, is referred to as the base.

=== Mantissa ===
Let $n$ be a number expressed in scientific notation as:
:$n approx m times 10^e$


The number $m$ is known as the mantissa.

=== Exponent ===
Let $n$ be a number expressed in scientific notation as:
:$n approx m times 10^e$


The number $e$ is known as the exponent.",Definition:Scientific Notation,,false,"Scientific notation is an implementation of floating-point representation for representing approximations to (usually large) numbers by presenting them in the form:
:n ≈ m × 10^e

where:
:m is a rational number such that 1 ≤ m < 10, expressed in decimal notation
:e is an integer.


=== Base ===
Let n be a number expressed in scientific notation as:
:n ≈ m × 10^e


The number 10, in this context, is referred to as the base.

=== Mantissa ===
Let n be a number expressed in scientific notation as:
:n ≈ m × 10^e


The number m is known as the mantissa.

=== Exponent ===
Let n be a number expressed in scientific notation as:
:n ≈ m × 10^e


The number e is known as the exponent.",Index
"['Definitions/Exponents', 'Definitions/Powers', 'Definitions/Indices']",Definition:Index,"In the power operation $x^r$, the number $r$ is known as the exponent of $x$, particularly for $r in mathbb R$.",Definition:Power (Algebra)/Exponent,,false,"In the power operation x^r, the number r is known as the exponent of x, particularly for r ∈ℝ.",Index
"['Definitions/Roots of Numbers', 'Definitions/Indices']",Definition:Index,"Let $sqrt [n] x$ denote the $n$th root of $x$.

The number $n$ is known as the index of the root.


If $n$ is not specified, that is $sqrt x$ is presented, this means the square root.",Definition:Root of Number/Index,,false,"Let √(x) denote the nth root of x.

The number n is known as the index of the root.


If n is not specified, that is √(x) is presented, this means the square root.",Index
"['Definitions/Summations', 'Definitions/Indices']",Definition:Index,"Consider the summation, in either of the three forms:

:$ds sum_{j mathop = 1}^n a_j qquad sum_{1 mathop le j mathop le n} a_j qquad sum_{R left(   right)j} a_j$


The variable $j$, an example of a bound variable, is known as the index variable of the summation.",Definition:Summation/Index Variable,,false,"Consider the summation, in either of the three forms:

:∑_j  = 1^n a_j     ∑_1 ≤ j ≤ n a_j     ∑_R (   )j a_j


The variable j, an example of a bound variable, is known as the index variable of the summation.",Index
"['Definitions/Index Numbers', 'Definitions/Statistics', 'Definitions/Economics', 'Definitions/Indices']",Definition:Index,"An index number in the context of statistics and economics is a measure of a change in some business activity over time.

It is constructed based on reliable information on relevant components, usually weighted according to importance.

An index number is usually defined relative to a specific base year, at which time the value of the index number would be assigned a round number, usually $100$ but sometimes $1000$.


=== Base Year ===
The base year of an index number is the year from which the index number is first calculated.


The index number for that year is usually assigned a round number, for example $100$ of $1000$.

=== Relative ===
Let $I$ be an index number composed of a weighted mean of a number of other index numbers.

Each of those contributing index numbers are referred to as relatives of $I$",Definition:Index Number,,false,"An index number in the context of statistics and economics is a measure of a change in some business activity over time.

It is constructed based on reliable information on relevant components, usually weighted according to importance.

An index number is usually defined relative to a specific base year, at which time the value of the index number would be assigned a round number, usually 100 but sometimes 1000.


=== Base Year ===
The base year of an index number is the year from which the index number is first calculated.


The index number for that year is usually assigned a round number, for example 100 of 1000.

=== Relative ===
Let I be an index number composed of a weighted mean of a number of other index numbers.

Each of those contributing index numbers are referred to as relatives of I",Index
"['Definitions/Inertia (Physics)', 'Definitions/Physics', 'Definitions/Inertia']",Definition:Inertia,"Inertia is the tendency of a body to maintain the same velocity in the absence of an external force, in accordance with Newton's First Law of Motion.

Equivalently put, inertia is the resistance of a body to a change in its motion.

Inertia is equivalent to mass.",Definition:Inertia (Physics),tendency,true,"Inertia is the tendency of a body to maintain the same velocity in the absence of an external force, in accordance with Newton's First Law of Motion.

Equivalently put, inertia is the resistance of a body to a change in its motion.

Inertia is equivalent to mass.",Inertia
"['Definitions/Inertia of Hermitian Matrices', 'Definitions/Hermitian Matrices', 'Definitions/Inertia']",Definition:Inertia,"Let $mathbf H$ be a Hermitian matrix.

The inertia of $mathbf H$ is an ordered triple of integers comprising:
:the number of positive eigenvalues of $mathbf H$
:the number of negative eigenvalues of $mathbf H$
:the number of zero eigenvalues of $mathbf H$
in that order.",Definition:Inertia of Hermitian Matrix,,false,"Let 𝐇 be a Hermitian matrix.

The inertia of 𝐇 is an ordered triple of integers comprising:
:the number of positive eigenvalues of 𝐇
:the number of negative eigenvalues of 𝐇
:the number of zero eigenvalues of 𝐇
in that order.",Inertia
['Definitions/Infima'],Definition:Infimum,"Let $left( S, preccurlyeq right)$ be an ordered set.

Let $T subseteq S$.


An element $c in S$ is the infimum of $T$ in $S$  if and only if :

:$(1): quad c$ is a lower bound of $T$ in $S$
:$(2): quad d preccurlyeq c$ for all lower bounds $d$ of $T$ in $S$.


If there exists an infimum of $T$ (in $S$), we say that:
:$T$ admits an infimum (in $S$) or
:$T$ has an infimum (in $S$).


=== Subset of Real Numbers ===

The concept is often encountered where $left( S, preccurlyeq right)$ is the set of real numbers under the usual ordering $left( mathbb R, le right)$:

Let $T subseteq mathbb R$.


A real number $c in mathbb R$ is the infimum of $T$ in $mathbb R$  if and only if :

:$(1): quad c$ is a lower bound of $T$ in $mathbb R$
:$(2): quad d le c$ for all lower bounds $d$ of $T$ in $mathbb R$.


If there exists an infimum of $T$ (in $mathbb R$), we say that $T$ admits an infimum (in $mathbb R$).


The infimum of $T$ is denoted $inf T$ or $inf left(   right)T$.

The infimum of $T$ is denoted $inf T$ or $inf left(   right)T$.


=== Finite Infimum ===
Let $left( S, preccurlyeq right)$ be an ordered set.

Let $T subseteq S$ admit a infimum $inf left(   right)T$.


If $T$ is finite, $inf left(   right)T$ is called a finite infimum.",Definition:Infimum of Set,,false,"Let ( S, ≼) be an ordered set.

Let T ⊆ S.


An element c ∈ S is the infimum of T in S  if and only if :

:(1):    c is a lower bound of T in S
:(2):    d ≼ c for all lower bounds d of T in S.


If there exists an infimum of T (in S), we say that:
:T admits an infimum (in S) or
:T has an infimum (in S).


=== Subset of Real Numbers ===

The concept is often encountered where ( S, ≼) is the set of real numbers under the usual ordering ( ℝ, ≤):

Let T ⊆ℝ.


A real number c ∈ℝ is the infimum of T in ℝ  if and only if :

:(1):    c is a lower bound of T in ℝ
:(2):    d ≤ c for all lower bounds d of T in ℝ.


If there exists an infimum of T (in ℝ), we say that T admits an infimum (in ℝ).


The infimum of T is denoted inf T or inf(   )T.

The infimum of T is denoted inf T or inf(   )T.


=== Finite Infimum ===
Let ( S, ≼) be an ordered set.

Let T ⊆ S admit a infimum inf(   )T.


If T is finite, inf(   )T is called a finite infimum.",Infimum
['Definitions/Infima'],Definition:Infimum,"Let $T subseteq mathbb R$.


A real number $c in mathbb R$ is the infimum of $T$ in $mathbb R$  if and only if :

:$(1): quad c$ is a lower bound of $T$ in $mathbb R$
:$(2): quad d le c$ for all lower bounds $d$ of $T$ in $mathbb R$.


If there exists an infimum of $T$ (in $mathbb R$), we say that $T$ admits an infimum (in $mathbb R$).


The infimum of $T$ is denoted $inf T$ or $inf left(   right)T$.",Definition:Infimum of Set/Real Numbers,,false,"Let T ⊆ℝ.


A real number c ∈ℝ is the infimum of T in ℝ  if and only if :

:(1):    c is a lower bound of T in ℝ
:(2):    d ≤ c for all lower bounds d of T in ℝ.


If there exists an infimum of T (in ℝ), we say that T admits an infimum (in ℝ).


The infimum of T is denoted inf T or inf(   )T.",Infimum
['Definitions/Infima'],Definition:Infimum,"Let $S$ be a set.

Let $left( T, preceq right)$ be an ordered set.

Let $f: S to T$ be a mapping from $S$ to $T$.

Let $f left[ S right]$, the image of $f$, admit an infimum.


Then the infimum of $f$ (on $S$) is defined by:
:$ds inf_{x mathop in S} f left(   right)x = inf f left[ S right]$


=== Real-Valued Function ===
Let $f: S to mathbb R$ be a real-valued function.

Let $f$ be bounded below on $S$.


=== Definition 1 ===
Let $f: S to mathbb R$ be a real-valued function.

Let $f$ be bounded below on $S$.


The infimum of $f$ on $S$ is defined by:
:$ds inf_{x mathop in S} f left(   right)x = inf f left[ S right]$
where
:$inf f left[ S right]$ is the infimum in $mathbb R$ of the image of $S$ under $f$.

=== Definition 2 ===
Let $f: S to mathbb R$ be a real-valued function.

Let $f$ be bounded below on $S$.


The infimum of $f$ on $S$ is defined as $ds inf_{x mathop in S} f left(   right)x := k in mathbb R$ such that:

:$(1): quad forall x in S: k le f left(   right)x$
:$(2): quad forall epsilon in mathbb R_{>0}: exists x in S: f left(   right)x < k + epsilon$",Definition:Infimum of Mapping,,false,"Let S be a set.

Let ( T, ≼) be an ordered set.

Let f: S → T be a mapping from S to T.

Let f [ S ], the image of f, admit an infimum.


Then the infimum of f (on S) is defined by:
:inf_x ∈ S f (   )x = inf f [ S ]


=== Real-Valued Function ===
Let f: S →ℝ be a real-valued function.

Let f be bounded below on S.


=== Definition 1 ===
Let f: S →ℝ be a real-valued function.

Let f be bounded below on S.


The infimum of f on S is defined by:
:inf_x ∈ S f (   )x = inf f [ S ]
where
:inf f [ S ] is the infimum in ℝ of the image of S under f.

=== Definition 2 ===
Let f: S →ℝ be a real-valued function.

Let f be bounded below on S.


The infimum of f on S is defined as inf_x ∈ S f (   )x := k ∈ℝ such that:

:(1):   ∀ x ∈ S: k ≤ f (   )x
:(2):   ∀ϵ∈ℝ_>0: ∃ x ∈ S: f (   )x < k + ϵ",Infimum
['Definitions/Infima'],Definition:Infimum,"Let $f: S to mathbb R$ be a real-valued function.

Let $f$ be bounded below on $S$.


=== Definition 1 ===
Let $f: S to mathbb R$ be a real-valued function.

Let $f$ be bounded below on $S$.


The infimum of $f$ on $S$ is defined by:
:$ds inf_{x mathop in S} f left(   right)x = inf f left[ S right]$
where
:$inf f left[ S right]$ is the infimum in $mathbb R$ of the image of $S$ under $f$.

=== Definition 2 ===
Let $f: S to mathbb R$ be a real-valued function.

Let $f$ be bounded below on $S$.


The infimum of $f$ on $S$ is defined as $ds inf_{x mathop in S} f left(   right)x := k in mathbb R$ such that:

:$(1): quad forall x in S: k le f left(   right)x$
:$(2): quad forall epsilon in mathbb R_{>0}: exists x in S: f left(   right)x < k + epsilon$",Definition:Infimum of Mapping/Real-Valued Function,,false,"Let f: S →ℝ be a real-valued function.

Let f be bounded below on S.


=== Definition 1 ===
Let f: S →ℝ be a real-valued function.

Let f be bounded below on S.


The infimum of f on S is defined by:
:inf_x ∈ S f (   )x = inf f [ S ]
where
:inf f [ S ] is the infimum in ℝ of the image of S under f.

=== Definition 2 ===
Let f: S →ℝ be a real-valued function.

Let f be bounded below on S.


The infimum of f on S is defined as inf_x ∈ S f (   )x := k ∈ℝ such that:

:(1):   ∀ x ∈ S: k ≤ f (   )x
:(2):   ∀ϵ∈ℝ_>0: ∃ x ∈ S: f (   )x < k + ϵ",Infimum
"['Definitions/Sequences', 'Definitions/Infima']",Definition:Infimum,"A special case of an infimum of a mapping is an infimum of a sequence, where the domain of the mapping is $mathbb N$.

Let $left( T, preceq right)$ be an ordered set.

Let $leftlangle x_n rightrangle$ be a sequence in $T$.


Let $leftlbrace x_n: n in mathbb N rightrbrace$ admit an infimum.


Then the infimum of $leftlangle x_n rightrangle$) is defined as:
:$inf left(   right){leftlangle x_n rightrangle } = inf left(   right){leftlbrace x_n: n in mathbb N rightrbrace }$",Definition:Infimum of Sequence,,false,"A special case of an infimum of a mapping is an infimum of a sequence, where the domain of the mapping is ℕ.

Let ( T, ≼) be an ordered set.

Let ⟨ x_n ⟩ be a sequence in T.


Let { x_n: n ∈ℕ} admit an infimum.


Then the infimum of ⟨ x_n ⟩) is defined as:
:inf(   )⟨ x_n ⟩ = inf(   ){ x_n: n ∈ℕ}",Infimum
"['Definitions/Sequences', 'Definitions/Infima']",Definition:Infimum,"Let $leftlangle x_n rightrangle$ be a real sequence.


Let $leftlbrace x_n: n in mathbb N rightrbrace$ admit an infimum.


Then the infimum of $leftlangle x_n rightrangle$) is defined as:
:$inf left(   right){leftlangle x_n rightrangle } = inf left(   right){leftlbrace x_n: n in mathbb N rightrbrace }$",Definition:Infimum of Real Sequence,,false,"Let ⟨ x_n ⟩ be a real sequence.


Let { x_n: n ∈ℕ} admit an infimum.


Then the infimum of ⟨ x_n ⟩) is defined as:
:inf(   )⟨ x_n ⟩ = inf(   ){ x_n: n ∈ℕ}",Infimum
"['Definitions/Algebraic Number Theory', 'Definitions/Commutative Algebra']",Definition:Integral,"Let $A$ be an extension of a commutative ring with unity $R$.


Let $C$ be the set of all elements of $A$ that are integral over $R$.

Then $C$ is called the integral closure of $R$ in $A$.",Definition:Integral Closure,,false,"Let A be an extension of a commutative ring with unity R.


Let C be the set of all elements of A that are integral over R.

Then C is called the integral closure of R in A.",Integral
"['Definitions/Algebraic Number Theory', 'Definitions/Commutative Algebra']",Definition:Integral,"=== Ring Extension ===
Let $phi : A hookrightarrow B$ be a ring extension.

Let $C$ be the integral closure of $A$ in $B$.


Then $A$ is integrally closed in $B$  if and only if  $C = phi(A)$.

=== Integral Domain ===
Let $R$ be an integral domain.


Then $R$ is integrally closed  if and only if  it is integrally closed in its field of fractions.",Definition:Integrally Closed,,false,"=== Ring Extension ===
Let ϕ : A ↪ B be a ring extension.

Let C be the integral closure of A in B.


Then A is integrally closed in B  if and only if  C = ϕ(A).

=== Integral Domain ===
Let R be an integral domain.


Then R is integrally closed  if and only if  it is integrally closed in its field of fractions.",Integral
"['Definitions/Algebraic Number Theory', 'Definitions/Commutative Algebra']",Definition:Integral,"Let $A$ be a commutative ring with unity.

Let $R subseteq A$ be a subring.


Then $a in A$ is said to be integral over $R$  if and only if  is is a root of a monic nonzero polynomial over $R$.",Definition:Integral Element of Ring Extension,,false,"Let A be a commutative ring with unity.

Let R ⊆ A be a subring.


Then a ∈ A is said to be integral over R  if and only if  is is a root of a monic nonzero polynomial over R.",Integral
['Definitions/Commutative Algebra'],Definition:Integral,"Let $A$ be a commutative ring with unity.

Let $Rsubset A$ be a subring.


The ring extension $R subseteq A$ is said to be integral  if and only if  for all $a in A$, $a$ is integral over $R$.",Definition:Integral Ring Extension,,false,"Let A be a commutative ring with unity.

Let R⊂ A be a subring.


The ring extension R ⊆ A is said to be integral  if and only if  for all a ∈ A, a is integral over R.",Integral
"['Definitions/Integral Domains', 'Definitions/Ring Theory']",Definition:Integral,"=== Definition 1 ===
An integral domain $left( D, +, circ right)$ is:

:a commutative ring which is non-null
:with a unity
:in which there are no (proper) zero divisors, that is:
::: $forall x, y in D: x circ y = 0_D implies x = 0_D text{ or } y = 0_D$

that is, in which all non-zero elements are cancellable.

=== Definition 2 ===
An integral domain $left( D, +, circ right)$ is a commutative ring such that $left( D^*, circ right)$ is a monoid, all of whose elements are cancellable.

In this context, $D^*$ denotes the ring $D$ without zero: $D setminus leftlbrace 0_D rightrbrace$.

=== Integral Domain Axioms ===
 ",Definition:Integral Domain,,false,"=== Definition 1 ===
An integral domain ( D, +, ∘) is:

:a commutative ring which is non-null
:with a unity
:in which there are no (proper) zero divisors, that is:
::: ∀ x, y ∈ D: x ∘ y = 0_D  x = 0_D  or  y = 0_D

that is, in which all non-zero elements are cancellable.

=== Definition 2 ===
An integral domain ( D, +, ∘) is a commutative ring such that ( D^*, ∘) is a monoid, all of whose elements are cancellable.

In this context, D^* denotes the ring D without zero: D ∖{ 0_D }.

=== Integral Domain Axioms ===
 ",Integral
"['Definitions/Integral Polynomials', 'Definitions/Polynomial Theory', 'Definitions/Integers']",Definition:Integral,An integral polynomial is a polynomial over the ring of integers $mathbb Z$.,Definition:Integral Polynomial,,false,An integral polynomial is a polynomial over the ring of integers ℤ.,Integral
"['Definitions/Discrete Mathematics', 'Definitions/Number Theory', 'Definitions/Field Theory']",Definition:Integral,"=== Rings and Fields ===
Let $left( F, +, times right)$ be a ring or a field whose zero is $0_F$.

Let $a in F$.

Let $n in mathbb Z$ be an integer.

Then $n cdot a$ is an integral multiple of $a$ where $n cdot a$ is defined as:

:$n cdot a := begin {cases}
0_F & : n = 0 \
left( left( n - 1 right) cdot a right) + a & : n > 1 \
leftlvert n rightrvert cdot left( -a right) & : n < 0 \
end {cases}$
where $leftlvert n rightrvert$ is the absolute value of $n$.


Using sum notation:
:$ds n cdot a := sum_{j mathop = 1}^n a$

=== Real Numbers ===

This concept is often seen when $F$ is the set of real numbers $mathbb R$.

Let $x, y in mathbb R$ be real numbers.

Then $x$ is an integral multiple of $y$  if and only if  $x$ is congruent to $0$ modulo $y$:
:$x equiv 0 pmod y$

That is:
:$exists k in mathbb Z: x = 0 + k y$",Definition:Integral Multiple,,false,"=== Rings and Fields ===
Let ( F, +, ×) be a ring or a field whose zero is 0_F.

Let a ∈ F.

Let n ∈ℤ be an integer.

Then n · a is an integral multiple of a where n · a is defined as:

:n · a := 
0_F     : n = 0 
( ( n - 1 ) · a ) + a     : n > 1 
| n |·( -a )     : n < 0
where | n | is the absolute value of n.


Using sum notation:
:n · a := ∑_j  = 1^n a

=== Real Numbers ===

This concept is often seen when F is the set of real numbers ℝ.

Let x, y ∈ℝ be real numbers.

Then x is an integral multiple of y  if and only if  x is congruent to 0 modulo y:
:x ≡ 0  y

That is:
:∃ k ∈ℤ: x = 0 + k y",Integral
"['Definitions/Floor Function', 'Definitions/Real Analysis', 'Definitions/Number Theory', 'Definitions/Discrete Mathematics']",Definition:Integral,"Let $x$ be a real number.

Informally, the floor function of $x$ is the greatest integer less than or equal to $x$.

=== Definition 1 ===
Let $x$ be a real number.


The floor function of $x$ is defined as the supremum of the set of integers no greater than $x$:
:$leftlfloor x rightrfloor := sup leftlbrace m in mathbb Z: m le x rightrbrace$
where $le$ is the usual ordering on the real numbers.

=== Definition 2 ===
Let $x in mathbb R$ be a real number.


The floor function of $x$, denoted $leftlfloor x rightrfloor$, is defined as the greatest element of the set of integers:
:$leftlbrace m in mathbb Z: m le x rightrbrace$
where $le$ is the usual ordering on the real numbers.

=== Definition 3 ===
Let $x$ be a real number.


The floor function of $x$ is the unique integer $leftlfloor x rightrfloor$ such that:
:$leftlfloor x rightrfloor le x < leftlfloor x rightrfloor + 1$",Definition:Floor Function,,false,"Let x be a real number.

Informally, the floor function of x is the greatest integer less than or equal to x.

=== Definition 1 ===
Let x be a real number.


The floor function of x is defined as the supremum of the set of integers no greater than x:
:⌊ x ⌋ := sup{ m ∈ℤ: m ≤ x }
where ≤ is the usual ordering on the real numbers.

=== Definition 2 ===
Let x ∈ℝ be a real number.


The floor function of x, denoted ⌊ x ⌋, is defined as the greatest element of the set of integers:
:{ m ∈ℤ: m ≤ x }
where ≤ is the usual ordering on the real numbers.

=== Definition 3 ===
Let x be a real number.


The floor function of x is the unique integer ⌊ x ⌋ such that:
:⌊ x ⌋≤ x < ⌊ x ⌋ + 1",Integral
"['Definitions/Integral Calculus', 'Definitions/Calculus', 'Definitions/Branches of Mathematics']",Definition:Integral,"Integral calculus is a subfield of calculus which is concerned with the study of the rates at which quantities accumulate.

Equivalently, given the rate of change of a quantity integral calculus provides techniques of providing the quantity itself.

The equivalence of the two uses are demonstrated in the Fundamental Theorem of Calculus.

The technique is also frequently used for the purpose of calculating areas and volumes of curved geometric figures.",Definition:Calculus/Integral,,false,"Integral calculus is a subfield of calculus which is concerned with the study of the rates at which quantities accumulate.

Equivalently, given the rate of change of a quantity integral calculus provides techniques of providing the quantity itself.

The equivalence of the two uses are demonstrated in the Fundamental Theorem of Calculus.

The technique is also frequently used for the purpose of calculating areas and volumes of curved geometric figures.",Integral
['Definitions/Integral Calculus'],Definition:Integral,"=== Indefinite Integral ===
Let $F$ be a real function which is continuous on the closed interval $left[ a ,.,.,   right]b$ and differentiable on the open interval $left( a ,.,.,   right)b$.

Let $f$ be a real function which is continuous on the open interval $left( a ,.,.,   right)b$.


Let:
:$forall x in left( a ,.,.,   right)b: F' left(   right)x = f left(   right)x$
where $F'$ denotes the derivative of $F$   $x$.


Then $F$ is a primitive of $f$, and is denoted:
:$ds F = int f left(   right)x ,mathrm d x$

=== Definite Integral ===
Let $left[ a ,.,.,   right]b$ be a closed real interval.

Let $f: left[ a ,.,.,   right]b to mathbb R$ be a real function.

Let $Delta$ be a finite subdivision of $left[ a ,.,.,   right]b$, $Delta = leftlbrace x_0, ldots, x_n rightrbrace$, $x_0 = a$ and $x_n = b$.

Let there for $Delta$ be a corresponding sequence $C$ of sample points $c_i$, $C = left( c_1, ldots, c_n right)$, where $c_i in left[ x_{i - 1}  ,.,.,   right]{x_i}$ for every $i in leftlbrace 1, ldots, n rightrbrace$.

Let $S left(   right){f; Delta, C}$ denote the Riemann sum of $f$ for the subdivision $Delta$ and the sample point sequence $C$.


Then $f$ is said to be (properly) Riemann integrable on $left[ a ,.,.,   right]b$  if and only if :
:$exists L in mathbb R: forall epsilon in mathbb R_{>0}: exists delta in mathbb R_{>0}: forall$ finite subdivisions $Delta$ of $left[ a ,.,.,   right]b: forall$ sample point sequences $C$ of $Delta: leftlVert Delta rightrVert < delta implies leftlvert S left(   right){f; Delta, C} - L rightrvert < epsilon$
where $leftlVert Delta rightrVert$ denotes the norm of $Delta$.


The real number $L$ is called the Riemann integral of $f$ over $left[ a ,.,.,   right]b$ and is denoted:
:$ds int_a^b f left(   right)x ,mathrm d x$


More usually (and informally), we say:
:$f$ is (Riemann) integrable over $left[ a ,.,.,   right]b$.


=== Riemann Integral as Integral Operator ===
Let $C left[ a ,.,.,   right]b$ be the space of continuous functions.

Let $x in C left[ a ,.,.,   right]b$ be a Riemann integrable function.

Let $mathbb R$ be the set of real numbers.


The Riemann integral operator, denoted by $I$, is the mapping $I : C left[ a ,.,.,   right]b to mathbb R$ such that:

:$ds I left(   right)x := int_a^b x left(   right)t ,mathrm d t$

where $ds int_a^b x left(   right)t ,mathrm d t$ is the Riemann integral.",Definition:Integral (Calculus),,false,"=== Indefinite Integral ===
Let F be a real function which is continuous on the closed interval [ a  . . ]b and differentiable on the open interval ( a  . . )b.

Let f be a real function which is continuous on the open interval ( a  . . )b.


Let:
:∀ x ∈( a  . . )b: F' (   )x = f (   )x
where F' denotes the derivative of F   x.


Then F is a primitive of f, and is denoted:
:F = ∫ f (   )x  d x

=== Definite Integral ===
Let [ a  . . ]b be a closed real interval.

Let f: [ a  . . ]b →ℝ be a real function.

Let Δ be a finite subdivision of [ a  . . ]b, Δ = { x_0, …, x_n }, x_0 = a and x_n = b.

Let there for Δ be a corresponding sequence C of sample points c_i, C = ( c_1, …, c_n ), where c_i ∈[ x_i - 1 . . ]x_i for every i ∈{ 1, …, n }.

Let S (   )f; Δ, C denote the Riemann sum of f for the subdivision Δ and the sample point sequence C.


Then f is said to be (properly) Riemann integrable on [ a  . . ]b  if and only if :
:∃ L ∈ℝ: ∀ϵ∈ℝ_>0: ∃δ∈ℝ_>0: ∀ finite subdivisions Δ of [ a  . . ]b: ∀ sample point sequences C of Δ: ‖Δ‖ < δ| S (   )f; Δ, C - L | < ϵ
where ‖Δ‖ denotes the norm of Δ.


The real number L is called the Riemann integral of f over [ a  . . ]b and is denoted:
:∫_a^b f (   )x  d x


More usually (and informally), we say:
:f is (Riemann) integrable over [ a  . . ]b.


=== Riemann Integral as Integral Operator ===
Let C [ a  . . ]b be the space of continuous functions.

Let x ∈ C [ a  . . ]b be a Riemann integrable function.

Let ℝ be the set of real numbers.


The Riemann integral operator, denoted by I, is the mapping I : C [ a  . . ]b →ℝ such that:

:I (   )x := ∫_a^b x (   )t  d t

where ∫_a^b x (   )t  d t is the Riemann integral.",Integral
"['Definitions/Definite Integrals', 'Definitions/Integral Calculus']",Definition:Integral,"Let $left[ a ,.,.,   right]b$ be a closed real interval.

Let $f: left[ a ,.,.,   right]b to mathbb R$ be a real function.


=== Riemann Integral ===
Let $left[ a ,.,.,   right]b$ be a closed real interval.

Let $f: left[ a ,.,.,   right]b to mathbb R$ be a real function.

Let $Delta$ be a finite subdivision of $left[ a ,.,.,   right]b$, $Delta = leftlbrace x_0, ldots, x_n rightrbrace$, $x_0 = a$ and $x_n = b$.

Let there for $Delta$ be a corresponding sequence $C$ of sample points $c_i$, $C = left( c_1, ldots, c_n right)$, where $c_i in left[ x_{i - 1}  ,.,.,   right]{x_i}$ for every $i in leftlbrace 1, ldots, n rightrbrace$.

Let $S left(   right){f; Delta, C}$ denote the Riemann sum of $f$ for the subdivision $Delta$ and the sample point sequence $C$.


Then $f$ is said to be (properly) Riemann integrable on $left[ a ,.,.,   right]b$  if and only if :
:$exists L in mathbb R: forall epsilon in mathbb R_{>0}: exists delta in mathbb R_{>0}: forall$ finite subdivisions $Delta$ of $left[ a ,.,.,   right]b: forall$ sample point sequences $C$ of $Delta: leftlVert Delta rightrVert < delta implies leftlvert S left(   right){f; Delta, C} - L rightrvert < epsilon$
where $leftlVert Delta rightrVert$ denotes the norm of $Delta$.


The real number $L$ is called the Riemann integral of $f$ over $left[ a ,.,.,   right]b$ and is denoted:
:$ds int_a^b f left(   right)x ,mathrm d x$


More usually (and informally), we say:
:$f$ is (Riemann) integrable over $left[ a ,.,.,   right]b$.


=== Riemann Integral as Integral Operator ===
Let $C left[ a ,.,.,   right]b$ be the space of continuous functions.

Let $x in C left[ a ,.,.,   right]b$ be a Riemann integrable function.

Let $mathbb R$ be the set of real numbers.


The Riemann integral operator, denoted by $I$, is the mapping $I : C left[ a ,.,.,   right]b to mathbb R$ such that:

:$ds I left(   right)x := int_a^b x left(   right)t ,mathrm d t$

where $ds int_a^b x left(   right)t ,mathrm d t$ is the Riemann integral.

=== Darboux Integral ===
Let $left[ a ,.,.,   right]b$ be a closed real interval.

Let $f: left[ a ,.,.,   right]b to mathbb R$ be a real function.

Let $f$ be bounded on $left[ a ,.,.,   right]b$.


Suppose that:
:$ds underline {int_a^b} f left(   right)x ,mathrm d x = overline {int_a^b} f left(   right)x ,mathrm d x$
where $ds underline {int_a^b}$ and $ds overline {int_a^b}$ denote the lower Darboux integral and upper Darboux integral, respectively.


Then the definite (Darboux) integral of $f$ over $left[ a ,.,.,   right]b$ is defined as:
:$ds int_a^b f left(   right)x ,mathrm d x = underline {int_a^b} f left(   right)x ,mathrm d x = overline {int_a^b} f left(   right)x ,mathrm d x$


$f$ is formally defined as (properly) integrable over $left[ a ,.,.,   right]b$ in the sense of Darboux, or (properly) Darboux integrable over $left[ a ,.,.,   right]b$.


More usually (and informally), we say:
:$f$ is (Darboux) integrable over $left[ a ,.,.,   right]b$.",Definition:Definite Integral,,false,"Let [ a  . . ]b be a closed real interval.

Let f: [ a  . . ]b →ℝ be a real function.


=== Riemann Integral ===
Let [ a  . . ]b be a closed real interval.

Let f: [ a  . . ]b →ℝ be a real function.

Let Δ be a finite subdivision of [ a  . . ]b, Δ = { x_0, …, x_n }, x_0 = a and x_n = b.

Let there for Δ be a corresponding sequence C of sample points c_i, C = ( c_1, …, c_n ), where c_i ∈[ x_i - 1 . . ]x_i for every i ∈{ 1, …, n }.

Let S (   )f; Δ, C denote the Riemann sum of f for the subdivision Δ and the sample point sequence C.


Then f is said to be (properly) Riemann integrable on [ a  . . ]b  if and only if :
:∃ L ∈ℝ: ∀ϵ∈ℝ_>0: ∃δ∈ℝ_>0: ∀ finite subdivisions Δ of [ a  . . ]b: ∀ sample point sequences C of Δ: ‖Δ‖ < δ| S (   )f; Δ, C - L | < ϵ
where ‖Δ‖ denotes the norm of Δ.


The real number L is called the Riemann integral of f over [ a  . . ]b and is denoted:
:∫_a^b f (   )x  d x


More usually (and informally), we say:
:f is (Riemann) integrable over [ a  . . ]b.


=== Riemann Integral as Integral Operator ===
Let C [ a  . . ]b be the space of continuous functions.

Let x ∈ C [ a  . . ]b be a Riemann integrable function.

Let ℝ be the set of real numbers.


The Riemann integral operator, denoted by I, is the mapping I : C [ a  . . ]b →ℝ such that:

:I (   )x := ∫_a^b x (   )t  d t

where ∫_a^b x (   )t  d t is the Riemann integral.

=== Darboux Integral ===
Let [ a  . . ]b be a closed real interval.

Let f: [ a  . . ]b →ℝ be a real function.

Let f be bounded on [ a  . . ]b.


Suppose that:
:∫_a^b f (   )x  d x = ∫_a^b f (   )x  d x
where ∫_a^b and ∫_a^b denote the lower Darboux integral and upper Darboux integral, respectively.


Then the definite (Darboux) integral of f over [ a  . . ]b is defined as:
:∫_a^b f (   )x  d x = ∫_a^b f (   )x  d x = ∫_a^b f (   )x  d x


f is formally defined as (properly) integrable over [ a  . . ]b in the sense of Darboux, or (properly) Darboux integrable over [ a  . . ]b.


More usually (and informally), we say:
:f is (Darboux) integrable over [ a  . . ]b.",Integral
"['Definitions/Primitives', 'Definitions/Integral Calculus']",Definition:Integral,"=== Primitive of Real Function ===
Let $F$ be a real function which is continuous on the closed interval $left[ a ,.,.,   right]b$ and differentiable on the open interval $left( a ,.,.,   right)b$.

Let $f$ be a real function which is continuous on the open interval $left( a ,.,.,   right)b$.


Let:
:$forall x in left( a ,.,.,   right)b: F' left(   right)x = f left(   right)x$
where $F'$ denotes the derivative of $F$   $x$.


Then $F$ is a primitive of $f$, and is denoted:
:$ds F = int f left(   right)x ,mathrm d x$

=== Primitive of Complex Function ===
Let $F: D to mathbb C$ be a complex function which is complex-differentiable on a connected domain $D$.

Let $f: D to mathbb C$ be a continuous complex function.


Let:
:$forall z in D: F' left(   right)z = f left(   right)z$
where $F'$ denotes the derivative of $F$   $z$.


Then $F$ is a primitive of $f$, and is denoted:
:$ds F = int f left(   right)z ,mathrm d z$

=== Primitive of Vector-Valued Function ===
Let $U subset mathbb R$ be an open set in $mathbb R$.

Let $mathbf f: U to mathbb R^n$ be a vector-valued function on $U$:

:$forall x in U: mathbf f left(   right)x = ds sum_{k mathop = 1}^n f_k left(   right)x mathbf e_k$

where:
:$f_1, f_2, ldots, f_n$ are real functions from $U$ to $mathbb R$
:$left( mathbf e_1, mathbf e_2, ldots, mathbf e_k right)$ denotes the standard ordered basis on $mathbb R^n$.

Let $mathbf f$ be differentiable on $U$.


Let $mathbf g left(   right)x := dfrac mathrm d {mathrm d x} mathbf f left(   right)x$ be the derivative of $mathbf f$   $x$.


The primitive of $mathbf g$   $x$ is defined as:

:$ds int mathbf g left(   right)x ,mathrm d x := mathbf f left(   right)x + mathbf c$

where $mathbf c$ is a arbitrary constant vector.",Definition:Primitive (Calculus),,false,"=== Primitive of Real Function ===
Let F be a real function which is continuous on the closed interval [ a  . . ]b and differentiable on the open interval ( a  . . )b.

Let f be a real function which is continuous on the open interval ( a  . . )b.


Let:
:∀ x ∈( a  . . )b: F' (   )x = f (   )x
where F' denotes the derivative of F   x.


Then F is a primitive of f, and is denoted:
:F = ∫ f (   )x  d x

=== Primitive of Complex Function ===
Let F: D →ℂ be a complex function which is complex-differentiable on a connected domain D.

Let f: D →ℂ be a continuous complex function.


Let:
:∀ z ∈ D: F' (   )z = f (   )z
where F' denotes the derivative of F   z.


Then F is a primitive of f, and is denoted:
:F = ∫ f (   )z  d z

=== Primitive of Vector-Valued Function ===
Let U ⊂ℝ be an open set in ℝ.

Let 𝐟: U →ℝ^n be a vector-valued function on U:

:∀ x ∈ U: 𝐟(   )x = ∑_k  = 1^n f_k (   )x 𝐞_k

where:
:f_1, f_2, …, f_n are real functions from U to ℝ
:( 𝐞_1, 𝐞_2, …, 𝐞_k ) denotes the standard ordered basis on ℝ^n.

Let 𝐟 be differentiable on U.


Let 𝐠(   )x := dd x𝐟(   )x be the derivative of 𝐟   x.


The primitive of 𝐠   x is defined as:

:∫𝐠(   )x  d x := 𝐟(   )x + 𝐜

where 𝐜 is a arbitrary constant vector.",Integral
"['Definitions/Solutions to Differential Equations', 'Definitions/Differential Equations']",Definition:Integral,"Let $Phi$ be a differential equation defined on a domain $D$.

Let $phi$ be a function which satisfies $Phi$ on the whole of $D$.


Then $phi$ is known as a solution of $Phi$.


Note that, in general, there may be more than one solution to a given differential equation.

On the other hand, there may be none at all.


=== General Solution ===
Let $Phi$ be a differential equation.

The general solution to $Phi$ is the set of all functions $phi$ that satisfy $Phi$.


 

=== Particular Solution ===
Let $Phi$ be a differential equation.

Let $S$ denote the solution set of $Phi$.

A particular solution of $Phi$ is the element of $S$, or subset of $S$, which satisfies a particular boundary condition of $Phi$.

=== Weak Solution ===
A weak solution is a solution to a non-standard formulation of a differential equation.

 ",Definition:Differential Equation/Solution,,false,"Let Φ be a differential equation defined on a domain D.

Let ϕ be a function which satisfies Φ on the whole of D.


Then ϕ is known as a solution of Φ.


Note that, in general, there may be more than one solution to a given differential equation.

On the other hand, there may be none at all.


=== General Solution ===
Let Φ be a differential equation.

The general solution to Φ is the set of all functions ϕ that satisfy Φ.


 

=== Particular Solution ===
Let Φ be a differential equation.

Let S denote the solution set of Φ.

A particular solution of Φ is the element of S, or subset of S, which satisfies a particular boundary condition of Φ.

=== Weak Solution ===
A weak solution is a solution to a non-standard formulation of a differential equation.

 ",Integral
"['Definitions/Set Interiors', 'Definitions/Topology']",Definition:Interior,"Let $T = left( S, tau right)$ be a topological space.

Let $H subseteq S$.


=== Definition 1 ===
Let $T = left( S, tau right)$ be a topological space.

Let $H subseteq S$.


The interior of $H$ is the union of all subsets of $H$ which are open in $T$.


That is, the interior of $H$ is defined as:
:$ds H^circ := bigcup_{K mathop in mathbb K} K$
where $mathbb K = leftlbrace K in tau: K subseteq H rightrbrace$.

=== Definition 2 ===
Let $T = left( S, tau right)$ be a topological space.

Let $H subseteq S$.


The interior of $H$ is defined as the largest open set of $T$ which is contained in $H$.

=== Definition 3 ===
Let $T = left( S, tau right)$ be a topological space.

Let $H subseteq S$.


The interior of $H$ is the set of all interior points of $H$.",Definition:Interior (Topology),,false,"Let T = ( S, τ) be a topological space.

Let H ⊆ S.


=== Definition 1 ===
Let T = ( S, τ) be a topological space.

Let H ⊆ S.


The interior of H is the union of all subsets of H which are open in T.


That is, the interior of H is defined as:
:H^∘ := ⋃_K ∈𝕂 K
where 𝕂 = { K ∈τ: K ⊆ H }.

=== Definition 2 ===
Let T = ( S, τ) be a topological space.

Let H ⊆ S.


The interior of H is defined as the largest open set of T which is contained in H.

=== Definition 3 ===
Let T = ( S, τ) be a topological space.

Let H ⊆ S.


The interior of H is the set of all interior points of H.",Interior
['Definitions/Complex Analysis'],Definition:Interior,"Let $H subseteq mathbb C$ be a subset of the complex plane.

The interior of $H$ is the subset of $H$ which consists of the interior points of $H$.",Definition:Interior (Complex Analysis),,false,"Let H ⊆ℂ be a subset of the complex plane.

The interior of H is the subset of H which consists of the interior points of H.",Interior
['Definitions/Complex Analysis'],Definition:Interior,"Let $S subseteq mathbb C$ be a subset of the complex plane.

Let $z in S$.


$z$ is an interior point of $S$  if and only if  $z$ has an $epsilon$-neighborhood $N_epsilon left(   right)z$ such that $N_epsilon left(   right)z subseteq S$.",Definition:Interior Point (Complex Analysis),,false,"Let S ⊆ℂ be a subset of the complex plane.

Let z ∈ S.


z is an interior point of S  if and only if  z has an ϵ-neighborhood N_ϵ(   )z such that N_ϵ(   )z ⊆ S.",Interior
"['Definitions/Regions', 'Definitions/Metric Spaces', 'Definitions/Geometry', 'Definitions/Analysis']",Definition:Interior,"=== Metric Space ===
Let $M = left( A, d right)$ be a metric space.

A region of $M$ is a subset $U$ of $M$ such that $U$ is:

:$(1): quad$ non-empty
:$(2): quad$ path-connected.

=== Complex ===
Let $D subseteq mathbb C$ be a subset of the set of complex numbers.

$D$ is a region of $mathbb C$  if and only if :
:$(1): quad$ $D$ is non-empty
:$(2): quad$ $D$ is path-connected.

=== Region in the Plane ===

The usual usage of region is in the real number plane or complex plane.

A point set $R$ in the plane is a region  if and only if :

:$(1): quad$ Each point of $R$ is the center of a circle all of whose elements consist of points of $R$
:$(2): quad$ Each point of $R$ can be joined by a curve consisting entirely of points of $R$.

==== Interior ====

The boundary of a region separates its interior from the exterior.

The interior consists of the points of the plane which are the elements of the region.

Such points are called interior points of the region.


It is ""usual"" that the interior is the ""smaller bit"" which is visually apparently on the inside as it appears on the page or screen, but this is of course not necessarily the case.


Also see the definition of interior and boundary from a topological perspective.


==== Bounded ====

A region in the the plane is bounded if there is a circle in the plane which encloses it.


Also see the definition of bounded in the context of a metric space.",Definition:Region,,false,"=== Metric Space ===
Let M = ( A, d ) be a metric space.

A region of M is a subset U of M such that U is:

:(1): non-empty
:(2): path-connected.

=== Complex ===
Let D ⊆ℂ be a subset of the set of complex numbers.

D is a region of ℂ  if and only if :
:(1): D is non-empty
:(2): D is path-connected.

=== Region in the Plane ===

The usual usage of region is in the real number plane or complex plane.

A point set R in the plane is a region  if and only if :

:(1): Each point of R is the center of a circle all of whose elements consist of points of R
:(2): Each point of R can be joined by a curve consisting entirely of points of R.

==== Interior ====

The boundary of a region separates its interior from the exterior.

The interior consists of the points of the plane which are the elements of the region.

Such points are called interior points of the region.


It is ""usual"" that the interior is the ""smaller bit"" which is visually apparently on the inside as it appears on the page or screen, but this is of course not necessarily the case.


Also see the definition of interior and boundary from a topological perspective.


==== Bounded ====

A region in the the plane is bounded if there is a circle in the plane which encloses it.


Also see the definition of bounded in the context of a metric space.",Interior
['Definitions/Transversals (Geometry)'],Definition:Interior,":


An interior angle of a transversal is an angle which is between the two lines cut by that transversal.

In the above figure, the interior angles with respect to the transversal $EF$ are:
:$angle AHJ$
:$angle CJH$
:$angle BHJ$
:$angle DJH$",Definition:Transversal (Geometry)/Interior Angle,,false,":


An interior angle of a transversal is an angle which is between the two lines cut by that transversal.

In the above figure, the interior angles with respect to the transversal EF are:
:∠ AHJ
:∠ CJH
:∠ BHJ
:∠ DJH",Interior
"['Definitions/Internal Angles', 'Definitions/Polygons']",Definition:Interior,"The internal angle of a vertex of a polygon is the size of the angle between the sides adjacent to that vertex, as measured inside the polygon.",Definition:Polygon/Internal Angle,,false,"The internal angle of a vertex of a polygon is the size of the angle between the sides adjacent to that vertex, as measured inside the polygon.",Interior
['Definitions/Jordan Curves'],Definition:Interior,"Let $f: left[ 0 ,.,.,   right]1 to mathbb R^2$ be a Jordan curve.


It follows from the Jordan Curve Theorem that $mathbb R^2 setminus mathrm {Img} left( f right)$ is a union of two disjoint connected components, one of which is bounded.

This bounded component is called the interior of $f$, and is denoted as $mathrm {Int} left( f right)$.",Definition:Jordan Curve/Interior,,false,"Let f: [ 0  . . ]1 →ℝ^2 be a Jordan curve.


It follows from the Jordan Curve Theorem that ℝ^2 ∖Img( f ) is a union of two disjoint connected components, one of which is bounded.

This bounded component is called the interior of f, and is denoted as Int( f ).",Interior
"['Definitions/Internal Angles', 'Definitions/Polygons']",Definition:Interior Angle,"The internal angle of a vertex of a polygon is the size of the angle between the sides adjacent to that vertex, as measured inside the polygon.",Definition:Polygon/Internal Angle,,false,"The internal angle of a vertex of a polygon is the size of the angle between the sides adjacent to that vertex, as measured inside the polygon.",Interior Angle
['Definitions/Transversals (Geometry)'],Definition:Interior Angle,":


An interior angle of a transversal is an angle which is between the two lines cut by that transversal.

In the above figure, the interior angles with respect to the transversal $EF$ are:
:$angle AHJ$
:$angle CJH$
:$angle BHJ$
:$angle DJH$",Definition:Transversal (Geometry)/Interior Angle,,false,":


An interior angle of a transversal is an angle which is between the two lines cut by that transversal.

In the above figure, the interior angles with respect to the transversal EF are:
:∠ AHJ
:∠ CJH
:∠ BHJ
:∠ DJH",Interior Angle
"['Definitions/Set Theory', 'Definitions/Set Intersection']",Definition:Intersection,"Let $S$ and $T$ be sets.


The (set) intersection of $S$ and $T$ is written $S cap T$.

It means the set which consists of all the elements which are contained in both of $S$ and $T$:

:$x in S cap T iff x in S land x in T$

or, more formally:

:$A = S cap T iff forall z: left( z in A iff z in S land z in T right)$


We can write:

:$S cap T := leftlbrace x: x in S land x in T rightrbrace$

and can voice it $S$ intersect $T$.


It can be seen that, in this form, $cap$ is a binary operation which acts on sets.


One often says that two sets intersect  if and only if  they have non-empty intersection.


=== Set of Sets ===
Let $Bbb S$ be a set of sets.

The intersection of $Bbb S$ is:
:$bigcap Bbb S := leftlbrace x: forall S in Bbb S: x in S rightrbrace$
That is, the set of all objects that are elements of all the elements of $Bbb S$.


Thus:
:$bigcap leftlbrace S, T rightrbrace := S cap T$

=== Family of Sets ===
Let $I$ be an indexing set.

Let $leftlangle S_i rightrangle_{i mathop in I}$ be a family of sets indexed by $I$.


Then the intersection of $leftlangle S_i rightrangle$ is defined as:

:$ds bigcap_{i mathop in I} S_i := leftlbrace x: forall i in I: x in S_i rightrbrace$


=== In the context of the Universal Set ===

In treatments of set theory in which the concept of the universal set is recognised, this can be expressed as follows.

Let $mathbb U$ be a universal set.

Let $I$ be an indexing set.

Let $leftlangle S_i rightrangle_{i mathop in I}$ be an indexed family of subsets of $mathbb U$.


Then the intersection of $leftlangle S_i rightrangle$ is defined and denoted as:

:$ds bigcap_{i mathop in I} S_i := leftlbrace x in mathbb U: forall i in I: x in S_i rightrbrace$

=== Subsets of General Set ===
This definition is the same when the universal set $mathbb U$ is replaced by any set $X$, which may or may not be a universal set:

Let $I$ be an indexing set.

Let $leftlangle S_i rightrangle_{i mathop in I}$ be an indexed family of subsets of a set $X$.


Then the intersection of $leftlangle S_i rightrangle$ is defined as:

:$ds bigcap_{i mathop in I} S_i := leftlbrace x in X: exists i in I: x in S_i rightrbrace$
where $i$ is a bound variable.

=== Countable Intersection ===
Let $mathbb S$ be a set of sets.

Let $leftlangle S_n rightrangle_{n mathop in mathbb N}$ be a sequence in $mathbb S$.

Let $S$ be the intersection of $leftlangle S_n rightrangle_{n mathop in mathbb N}$:
:$ds S = bigcap_{n mathop in mathbb N} S_n$


Then $S$ is a countable intersection of sets in $mathbb S$.

=== Finite Intersection ===
Let $S = S_1 cap S_2 cap ldots cap S_n$.

Then:
:$ds S = bigcap_{i mathop in mathbb N^*_n} S_i := leftlbrace x: forall i in mathbb N^*_n: x in S_i rightrbrace$
where $mathbb N^*_n = leftlbrace 1, 2, 3, ldots, n rightrbrace$.


If it is clear from the context that $i in mathbb N^*_n$, we can also write $ds bigcap_{mathbb N^*_n} S_i$.",Definition:Set Intersection,,false,"Let S and T be sets.


The (set) intersection of S and T is written S ∩ T.

It means the set which consists of all the elements which are contained in both of S and T:

:x ∈ S ∩ T  x ∈ S  x ∈ T

or, more formally:

:A = S ∩ T ∀ z: ( z ∈ A  z ∈ S  z ∈ T )


We can write:

:S ∩ T := { x: x ∈ S  x ∈ T }

and can voice it S intersect T.


It can be seen that, in this form, ∩ is a binary operation which acts on sets.


One often says that two sets intersect  if and only if  they have non-empty intersection.


=== Set of Sets ===
Let S be a set of sets.

The intersection of S is:
:⋂ S := { x: ∀ S ∈ S: x ∈ S }
That is, the set of all objects that are elements of all the elements of S.


Thus:
:⋂{ S, T } := S ∩ T

=== Family of Sets ===
Let I be an indexing set.

Let ⟨ S_i ⟩_i ∈ I be a family of sets indexed by I.


Then the intersection of ⟨ S_i ⟩ is defined as:

:⋂_i ∈ I S_i := { x: ∀ i ∈ I: x ∈ S_i }


=== In the context of the Universal Set ===

In treatments of set theory in which the concept of the universal set is recognised, this can be expressed as follows.

Let 𝕌 be a universal set.

Let I be an indexing set.

Let ⟨ S_i ⟩_i ∈ I be an indexed family of subsets of 𝕌.


Then the intersection of ⟨ S_i ⟩ is defined and denoted as:

:⋂_i ∈ I S_i := { x ∈𝕌: ∀ i ∈ I: x ∈ S_i }

=== Subsets of General Set ===
This definition is the same when the universal set 𝕌 is replaced by any set X, which may or may not be a universal set:

Let I be an indexing set.

Let ⟨ S_i ⟩_i ∈ I be an indexed family of subsets of a set X.


Then the intersection of ⟨ S_i ⟩ is defined as:

:⋂_i ∈ I S_i := { x ∈ X: ∃ i ∈ I: x ∈ S_i }
where i is a bound variable.

=== Countable Intersection ===
Let 𝕊 be a set of sets.

Let ⟨ S_n ⟩_n ∈ℕ be a sequence in 𝕊.

Let S be the intersection of ⟨ S_n ⟩_n ∈ℕ:
:S = ⋂_n ∈ℕ S_n


Then S is a countable intersection of sets in 𝕊.

=== Finite Intersection ===
Let S = S_1 ∩ S_2 ∩…∩ S_n.

Then:
:S = ⋂_i ∈ℕ^*_n S_i := { x: ∀ i ∈ℕ^*_n: x ∈ S_i }
where ℕ^*_n = { 1, 2, 3, …, n }.


If it is clear from the context that i ∈ℕ^*_n, we can also write ⋂_ℕ^*_n S_i.",Intersection
['Definitions/Geometry'],Definition:Intersection,"The intersection of two lines $AB$ and $CD$ is denoted by $AB cap CD$.

The intersection of two geometric figures is the set of points shared by both figures.


Note that this use of $cap$ is consistent with that of its more usual context of set intersection.


When two lines intersect, they are said to cut each other.",Definition:Intersection (Geometry),,false,"The intersection of two lines AB and CD is denoted by AB ∩ CD.

The intersection of two geometric figures is the set of points shared by both figures.


Note that this use of ∩ is consistent with that of its more usual context of set intersection.


When two lines intersect, they are said to cut each other.",Intersection
['Definitions/Conditional'],Definition:Inverse,"The inverse of the conditional:
: $p implies q$
is the statement:
:$neg p implies neg q$",Definition:Inverse Statement,,false,"The inverse of the conditional:
: p  q
is the statement:
:p  q",Inverse
['Definitions/Inverse Elements'],Definition:Inverse,"Let $left( S, circ right)$ be an algebraic structure with an identity element $e_S$.

Let $x, y in S$ be elements.


The element $y$ is an inverse of $x$  if and only if :
:$y circ x = e_S = x circ y$
that is,  if and only if  $y$ is both:
:a left inverse of $x$
and:
:a right inverse of $x$.",Definition:Inverse (Abstract Algebra)/Inverse,,false,"Let ( S, ∘) be an algebraic structure with an identity element e_S.

Let x, y ∈ S be elements.


The element y is an inverse of x  if and only if :
:y ∘ x = e_S = x ∘ y
that is,  if and only if  y is both:
:a left inverse of x
and:
:a right inverse of x.",Inverse
['Definitions/Ring Theory'],Definition:Inverse,"Let $left( R, +, circ right)$ be a ring with unity.

Let $U_R$ denotes the group of units of $R$.

The inverse of $x in U_R$ by $circ$ is called the (ring) product inverse of $x$.


The usual means of denoting the product inverse of an element $x$ is by $x^{-1}$.

Thus it is distinguished from the additive inverse of $x$, that is, the (ring) negative of $x$, which is usually denoted $-x$.",Definition:Product Inverse,,false,"Let ( R, +, ∘) be a ring with unity.

Let U_R denotes the group of units of R.

The inverse of x ∈ U_R by ∘ is called the (ring) product inverse of x.


The usual means of denoting the product inverse of an element x is by x^-1.

Thus it is distinguished from the additive inverse of x, that is, the (ring) negative of x, which is usually denoted -x.",Inverse
"['Definitions/Algebraic Structures', 'Definitions/Semigroups', 'Definitions/Inverse Semigroups']",Definition:Inverse,"An inverse semigroup is a semigroup $left( S, circ right)$ such that:

:$forall a in S: exists! b in S: a = a circ b circ a, b = b circ a circ b$


=== Inverse ===
Let $(S, circ)$ be an inverse semigroup.

Let $ain S$.


The inverse of $a$ is the unique element $bin S$ such that:
:$a = a circ b circ a$ and $b = b circ a circ b$


Category:Definitions/Inverse Semigroups",Definition:Inverse Semigroup,,false,"An inverse semigroup is a semigroup ( S, ∘) such that:

:∀ a ∈ S: ∃! b ∈ S: a = a ∘ b ∘ a, b = b ∘ a ∘ b


=== Inverse ===
Let (S, ∘) be an inverse semigroup.

Let a∈ S.


The inverse of a is the unique element b∈ S such that:
:a = a ∘ b ∘ a and b = b ∘ a ∘ b


Category:Definitions/Inverse Semigroups",Inverse
['Definitions/Inverse Semigroups'],Definition:Inverse,"Let $(S, circ)$ be an inverse semigroup.

Let $ain S$.


The inverse of $a$ is the unique element $bin S$ such that:
:$a = a circ b circ a$ and $b = b circ a circ b$


Category:Definitions/Inverse Semigroups",Definition:Inverse in Inverse Semigroup,,false,"Let (S, ∘) be an inverse semigroup.

Let a∈ S.


The inverse of a is the unique element b∈ S such that:
:a = a ∘ b ∘ a and b = b ∘ a ∘ b


Category:Definitions/Inverse Semigroups",Inverse
['Definitions/Linear Transformations'],Definition:Inverse,"Let $V$ and $U$ be vector spaces.

Let $A : V to U$ be an invertible (in the sense of a mapping) linear transformation with inverse mapping $A^{-1} : U to V$.


We say that $A^{-1}$ is the inverse linear transformation of $A$.",Definition:Inverse Linear Transformation,,false,"Let V and U be vector spaces.

Let A : V → U be an invertible (in the sense of a mapping) linear transformation with inverse mapping A^-1 : U → V.


We say that A^-1 is the inverse linear transformation of A.",Inverse
['Definitions/Linear Operators'],Definition:Inverse,"Let $X$ be a vector space.

Let $A : X to X$ be an invertible (in the sense of a mapping) linear transformation with inverse mapping $A^{-1} : X to X$.


We say that $A^{-1}$ is the inverse linear operator of $A$.",Definition:Inverse Linear Operator,,false,"Let X be a vector space.

Let A : X → X be an invertible (in the sense of a mapping) linear transformation with inverse mapping A^-1 : X → X.


We say that A^-1 is the inverse linear operator of A.",Inverse
"['Definitions/Inverse Relations', 'Definitions/Relation Theory']",Definition:Inverse,"Let $mathcal R subseteq S times T$ be a relation.


The inverse relation to (or of) $mathcal R$ is defined as:

:$mathcal R^{-1} := leftlbrace left( t, s right): left( s, t right) in mathcal R rightrbrace$


That is, $mathcal R^{-1} subseteq T times S$ is the relation which satisfies:

:$forall s in S: forall t in T: left( t, s right) in mathcal R^{-1} iff left( s, t right) in mathcal R$


=== Class Theoretical Definition ===
 
Let $V$ be a basic universe.

Let $A$ and $B$ be subclasses of $V$.

Let $mathcal R subseteq A times B$ be a relation on $A times B$.


The inverse relation to (or of) $mathcal R$ is defined as the class of all ordered pairs $left( b, a right)$ such that $left( a, b right) in mathcal R$:

:$mathcal R^{-1} := leftlbrace left( b, a right): left( a, b right) in mathcal R rightrbrace$",Definition:Inverse Relation,,false,"Let ℛ⊆ S × T be a relation.


The inverse relation to (or of) ℛ is defined as:

:ℛ^-1 := {( t, s ): ( s, t ) ∈ℛ}


That is, ℛ^-1⊆ T × S is the relation which satisfies:

:∀ s ∈ S: ∀ t ∈ T: ( t, s ) ∈ℛ^-1( s, t ) ∈ℛ


=== Class Theoretical Definition ===
 
Let V be a basic universe.

Let A and B be subclasses of V.

Let ℛ⊆ A × B be a relation on A × B.


The inverse relation to (or of) ℛ is defined as the class of all ordered pairs ( b, a ) such that ( a, b ) ∈ℛ:

:ℛ^-1 := {( b, a ): ( a, b ) ∈ℛ}",Inverse
"['Definitions/Inverse Mappings', 'Definitions/Inverses of Mappings', 'Definitions/Mapping Theory', 'Definitions/Inverses']",Definition:Inverse,"=== Definition 1 ===
Let $f: S to T$ be a mapping.

Let $f^{-1} subseteq T times S$ be the inverse of $f$:
:$f^{-1} := leftlbrace left( t, s right): f left(   right)s = t rightrbrace$


Let $f^{-1}$ itself be a mapping:
:$forall y in T: left( y, x_1 right) in f^{-1} land left( y, x_2 right) in f^{-1} implies x_1 = x_2$
and
:$forall y in T: exists x in S: left( y, x right) in f$


Then $f^{-1}$ is called the inverse mapping of $f$.

=== Definition 2 ===
Let $f: S to T$ and $g: T to S$ be mappings.

Let:
:$g circ f = I_S$
:$f circ g = I_T$
where:
:$g circ f$ and $f circ g$ denotes the composition of $f$ with $g$ in either order
:$I_S$ and $I_T$ denote the identity mappings on $S$ and $T$ respectively.

That is, $f$ and $g$ are both left inverse mappings and right inverse mappings of each other.


Then:
:$g$ is the inverse (mapping) of $f$
:$f$ is the inverse (mapping) of $g$.",Definition:Inverse Mapping,,false,"=== Definition 1 ===
Let f: S → T be a mapping.

Let f^-1⊆ T × S be the inverse of f:
:f^-1 := {( t, s ): f (   )s = t }


Let f^-1 itself be a mapping:
:∀ y ∈ T: ( y, x_1 ) ∈ f^-1( y, x_2 ) ∈ f^-1 x_1 = x_2
and
:∀ y ∈ T: ∃ x ∈ S: ( y, x ) ∈ f


Then f^-1 is called the inverse mapping of f.

=== Definition 2 ===
Let f: S → T and g: T → S be mappings.

Let:
:g ∘ f = I_S
:f ∘ g = I_T
where:
:g ∘ f and f ∘ g denotes the composition of f with g in either order
:I_S and I_T denote the identity mappings on S and T respectively.

That is, f and g are both left inverse mappings and right inverse mappings of each other.


Then:
:g is the inverse (mapping) of f
:f is the inverse (mapping) of g.",Inverse
"['Definitions/Inverses of Mappings', 'Definitions/Inverse Relations', 'Definitions/Mapping Theory', 'Definitions/Relation Theory', 'Definitions/Inverses']",Definition:Inverse,"Let $f: S to T$ be a mapping.


The inverse of $f$ is its inverse relation, defined as:
:$f^{-1} := leftlbrace left( t, s right): f left(   right)s = t rightrbrace$

That is:
:$f^{-1} := leftlbrace left( t, s right): left( s, t right) in f rightrbrace$


That is, $f^{-1} subseteq T times S$ is the relation which satisfies:

:$forall s in S: forall t in T: left( t, s right) in f^{-1} iff left( s, t right) in f$",Definition:Inverse of Mapping,,false,"Let f: S → T be a mapping.


The inverse of f is its inverse relation, defined as:
:f^-1 := {( t, s ): f (   )s = t }

That is:
:f^-1 := {( t, s ): ( s, t ) ∈ f }


That is, f^-1⊆ T × S is the relation which satisfies:

:∀ s ∈ S: ∀ t ∈ T: ( t, s ) ∈ f^-1( s, t ) ∈ f",Inverse
"['Definitions/Inverse Matrices', 'Definitions/Matrix Theory']",Definition:Inverse,"Let $n in mathbb Z_{>0}$ be a (strictly) positive integer.

Let $mathbf A$ be a square matrix of order $n$.


Let there exist a square matrix $mathbf B$ of order $n$ such that:
:$mathbf A mathbf B = mathbf I_n = mathbf B mathbf A$

where $mathbf I_n$ denotes the unit matrix of order $n$.


Then $mathbf B$ is called the inverse of $mathbf A$ and is usually denoted $mathbf A^{-1}$.


=== Left Inverse Matrix ===
Let $m, n in mathbb Z_{>0}$ be a (strictly) positive integer.


Let $mathbf A = left[ a right]_{m n}$ be a matrix of order $m times n$.

Let $mathbf B = left[ b right]_{n m}$ be a matrix of order $n times m$ such that:
:$mathbf B mathbf A = mathbf I_n$

where $mathbf I_n$ denotes the unit matrix of order $n$.


Then $mathbf B$ is known as a left inverse (matrix) of $mathbf A$.

=== Right Inverse Matrix ===
Let $m, n in mathbb Z_{>0}$ be a (strictly) positive integer.


Let $mathbf A = left[ a right]_{m n}$ be a matrix of order $m times n$.

Let $mathbf B = left[ b right]_{n m}$ be a matrix of order $n times m$ such that:
:$mathbf A mathbf B = mathbf I_m$

where $mathbf I_m$ denotes the unit matrix of order $m$.


Then $mathbf B$ is known as a right inverse (matrix) of $mathbf A$.",Definition:Inverse Matrix,,false,"Let n ∈ℤ_>0 be a (strictly) positive integer.

Let 𝐀 be a square matrix of order n.


Let there exist a square matrix 𝐁 of order n such that:
:𝐀𝐁 = 𝐈_n = 𝐁𝐀

where 𝐈_n denotes the unit matrix of order n.


Then 𝐁 is called the inverse of 𝐀 and is usually denoted 𝐀^-1.


=== Left Inverse Matrix ===
Let m, n ∈ℤ_>0 be a (strictly) positive integer.


Let 𝐀 = [ a ]_m n be a matrix of order m × n.

Let 𝐁 = [ b ]_n m be a matrix of order n × m such that:
:𝐁𝐀 = 𝐈_n

where 𝐈_n denotes the unit matrix of order n.


Then 𝐁 is known as a left inverse (matrix) of 𝐀.

=== Right Inverse Matrix ===
Let m, n ∈ℤ_>0 be a (strictly) positive integer.


Let 𝐀 = [ a ]_m n be a matrix of order m × n.

Let 𝐁 = [ b ]_n m be a matrix of order n × m such that:
:𝐀𝐁 = 𝐈_m

where 𝐈_m denotes the unit matrix of order m.


Then 𝐁 is known as a right inverse (matrix) of 𝐀.",Inverse
"['Definitions/Induced Mappings', 'Definitions/Preimages', 'Definitions/Inverse Image Mappings']",Definition:Inverse Image,"Let $S$ and $T$ be sets.

Let $mathcal P left( S right)$ and $mathcal P left( T right)$ be their power sets.


=== Relation ===

Let $mathcal R subseteq S times T$ be a relation on $S times T$.

Let $S$ and $T$ be sets.

Let $mathcal P left( S right)$ and $mathcal P left( T right)$ be their power sets.

Let $mathcal R subseteq S times T$ be a relation on $S times T$.


The inverse image mapping of $mathcal R$ is the mapping $mathcal R^gets: mathcal P left( T right) to mathcal P left( S right)$ that sends a subset $Y subseteq T$ to its preimage $mathcal R^{-1}  left(   right)Y$ under $mathcal R$:

:$forall Y in mathcal P left( T right): mathcal R^gets left(   right)Y = begin {cases} leftlbrace s in S: exists t in Y: left( t, s right) in mathcal R^{-1}  rightrbrace & : mathrm {Img} left( mathcal R right) cap Y ne varnothing \ varnothing & : mathrm {Img} left( mathcal R right) cap Y = varnothing end {cases}$

=== Mapping ===

Let $f: S to T$ be a mapping.

Let $S$ and $T$ be sets.

Let $mathcal P left( S right)$ and $mathcal P left( T right)$ be their power sets.

Let $f: S to T$ be a mapping.


The inverse image mapping of $f$ is the mapping $f^gets: mathcal P left( T right) to mathcal P left( S right)$ that sends a subset $Y subseteq T$ to its preimage $f^{-1} left( T right)$ under $f$:

:$forall Y in mathcal P left( T right): f^gets left(   right)Y = begin {cases} leftlbrace s in S: exists t in Y: f left(   right)s = t rightrbrace & : mathrm {Img} left( f right) cap Y ne varnothing \ varnothing & : mathrm {Img} left( f right) cap Y = varnothing end {cases}$",Definition:Inverse Image Mapping,,false,"Let S and T be sets.

Let 𝒫( S ) and 𝒫( T ) be their power sets.


=== Relation ===

Let ℛ⊆ S × T be a relation on S × T.

Let S and T be sets.

Let 𝒫( S ) and 𝒫( T ) be their power sets.

Let ℛ⊆ S × T be a relation on S × T.


The inverse image mapping of ℛ is the mapping ℛ^: 𝒫( T ) →𝒫( S ) that sends a subset Y ⊆ T to its preimage ℛ^-1(   )Y under ℛ:

:∀ Y ∈𝒫( T ): ℛ^(   )Y = { s ∈ S: ∃ t ∈ Y: ( t, s ) ∈ℛ^-1}    : Img( ℛ) ∩ Y ∅
∅    : Img( ℛ) ∩ Y = ∅

=== Mapping ===

Let f: S → T be a mapping.

Let S and T be sets.

Let 𝒫( S ) and 𝒫( T ) be their power sets.

Let f: S → T be a mapping.


The inverse image mapping of f is the mapping f^: 𝒫( T ) →𝒫( S ) that sends a subset Y ⊆ T to its preimage f^-1( T ) under f:

:∀ Y ∈𝒫( T ): f^(   )Y = { s ∈ S: ∃ t ∈ Y: f (   )s = t }    : Img( f ) ∩ Y ∅
∅    : Img( f ) ∩ Y = ∅",Inverse Image
['Definitions/Inverse Image Mappings'],Definition:Inverse Image,"Let $S$ and $T$ be sets.

Let $mathcal P left( S right)$ and $mathcal P left( T right)$ be their power sets.

Let $f: S to T$ be a mapping.


=== Definition 1 ===
Let $S$ and $T$ be sets.

Let $mathcal P left( S right)$ and $mathcal P left( T right)$ be their power sets.

Let $f: S to T$ be a mapping.


The inverse image mapping of $f$ is the mapping $f^gets: mathcal P left( T right) to mathcal P left( S right)$ that sends a subset $Y subseteq T$ to its preimage $f^{-1} left( T right)$ under $f$:

:$forall Y in mathcal P left( T right): f^gets left(   right)Y = begin {cases} leftlbrace s in S: exists t in Y: f left(   right)s = t rightrbrace & : mathrm {Img} left( f right) cap Y ne varnothing \ varnothing & : mathrm {Img} left( f right) cap Y = varnothing end {cases}$

=== Definition 2 ===
Let $S$ and $T$ be sets.

Let $mathcal P left( S right)$ and $mathcal P left( T right)$ be their power sets.

Let $f: S to T$ be a mapping.


The inverse image mapping of $f$ is the direct image mapping of the inverse $f^{-1}$ of $f$:
:$f^gets = left( f^{-1}  right)^to: mathcal P left( T right) to mathcal P left( S right)$:

That is:
:$forall Y in mathcal P left( T right): f^gets left(   right)Y = leftlbrace s in S: exists t in Y: f left(   right)s = t rightrbrace$",Definition:Inverse Image Mapping/Mapping,,false,"Let S and T be sets.

Let 𝒫( S ) and 𝒫( T ) be their power sets.

Let f: S → T be a mapping.


=== Definition 1 ===
Let S and T be sets.

Let 𝒫( S ) and 𝒫( T ) be their power sets.

Let f: S → T be a mapping.


The inverse image mapping of f is the mapping f^: 𝒫( T ) →𝒫( S ) that sends a subset Y ⊆ T to its preimage f^-1( T ) under f:

:∀ Y ∈𝒫( T ): f^(   )Y = { s ∈ S: ∃ t ∈ Y: f (   )s = t }    : Img( f ) ∩ Y ∅
∅    : Img( f ) ∩ Y = ∅

=== Definition 2 ===
Let S and T be sets.

Let 𝒫( S ) and 𝒫( T ) be their power sets.

Let f: S → T be a mapping.


The inverse image mapping of f is the direct image mapping of the inverse f^-1 of f:
:f^ = ( f^-1)^→: 𝒫( T ) →𝒫( S ):

That is:
:∀ Y ∈𝒫( T ): f^(   )Y = { s ∈ S: ∃ t ∈ Y: f (   )s = t }",Inverse Image
['Definitions/Inverse Image Mappings'],Definition:Inverse Image,"Let $S$ and $T$ be sets.

Let $mathcal P left( S right)$ and $mathcal P left( T right)$ be their power sets.

Let $mathcal R subseteq S times T$ be a relation on $S times T$.


=== Definition 1 ===
Let $S$ and $T$ be sets.

Let $mathcal P left( S right)$ and $mathcal P left( T right)$ be their power sets.

Let $mathcal R subseteq S times T$ be a relation on $S times T$.


The inverse image mapping of $mathcal R$ is the mapping $mathcal R^gets: mathcal P left( T right) to mathcal P left( S right)$ that sends a subset $Y subseteq T$ to its preimage $mathcal R^{-1}  left(   right)Y$ under $mathcal R$:

:$forall Y in mathcal P left( T right): mathcal R^gets left(   right)Y = begin {cases} leftlbrace s in S: exists t in Y: left( t, s right) in mathcal R^{-1}  rightrbrace & : mathrm {Img} left( mathcal R right) cap Y ne varnothing \ varnothing & : mathrm {Img} left( mathcal R right) cap Y = varnothing end {cases}$

=== Definition 2 ===
Let $S$ and $T$ be sets.

Let $mathcal P left( S right)$ and $mathcal P left( T right)$ be their power sets.

Let $mathcal R subseteq S times T$ be a relation on $S times T$.


The inverse image mapping of $mathcal R$ is the direct image mapping of the inverse $mathcal R^{-1}$ of $mathcal R$:
:$mathcal R^gets = left( mathcal R^{-1}  right)^to: mathcal P left( T right) to mathcal P left( S right)$

That is:
:$forall Y in mathcal P left( T right): mathcal R^gets left(   right)Y = leftlbrace s in S: exists t in Y: left( t, s right) in mathcal R^{-1}  rightrbrace$",Definition:Inverse Image Mapping/Relation,,false,"Let S and T be sets.

Let 𝒫( S ) and 𝒫( T ) be their power sets.

Let ℛ⊆ S × T be a relation on S × T.


=== Definition 1 ===
Let S and T be sets.

Let 𝒫( S ) and 𝒫( T ) be their power sets.

Let ℛ⊆ S × T be a relation on S × T.


The inverse image mapping of ℛ is the mapping ℛ^: 𝒫( T ) →𝒫( S ) that sends a subset Y ⊆ T to its preimage ℛ^-1(   )Y under ℛ:

:∀ Y ∈𝒫( T ): ℛ^(   )Y = { s ∈ S: ∃ t ∈ Y: ( t, s ) ∈ℛ^-1}    : Img( ℛ) ∩ Y ∅
∅    : Img( ℛ) ∩ Y = ∅

=== Definition 2 ===
Let S and T be sets.

Let 𝒫( S ) and 𝒫( T ) be their power sets.

Let ℛ⊆ S × T be a relation on S × T.


The inverse image mapping of ℛ is the direct image mapping of the inverse ℛ^-1 of ℛ:
:ℛ^ = ( ℛ^-1)^→: 𝒫( T ) →𝒫( S )

That is:
:∀ Y ∈𝒫( T ): ℛ^(   )Y = { s ∈ S: ∃ t ∈ Y: ( t, s ) ∈ℛ^-1}",Inverse Image
['Definitions/Sheaf Theory'],Definition:Inverse Image,"Let $T_1 = left( S_1, tau_1 right)$ and $T_2 = left( S_2, tau_2 right)$ be topological spaces.

Let $f: T_1 to T_2$ be continuous.

Let $mathbf C$ be a category which has all small inductive limits.

Let $mathcal F$ be a $mathbf C$-valued presheaf on $T_2$.


The inverse image presheaf of $mathcal F$ via $f$ is the presheaf $f^{-1}_{operatorname {Psh} } mathcal F$ on $T_1$ with:
:$left( f^{-1}_{operatorname{Psh} } mathcal F right)  left(   right)U = ds varinjlim_{V mathop supseteq f left(   right)U} mathcal F left(   right)V$ where the inductive limit goes over open $V subseteq Y$
:$operatorname {res}^U_W$ is the induced map on the inductive limit of the subset $leftlbrace V: V supseteq f left(   right)U rightrbrace subseteq leftlbrace V : V supseteq f left(   right)W rightrbrace$",Definition:Inverse Image Presheaf,,false,"Let T_1 = ( S_1, τ_1 ) and T_2 = ( S_2, τ_2 ) be topological spaces.

Let f: T_1 → T_2 be continuous.

Let 𝐂 be a category which has all small inductive limits.

Let ℱ be a 𝐂-valued presheaf on T_2.


The inverse image presheaf of ℱ via f is the presheaf f^-1_Pshℱ on T_1 with:
:( f^-1_Pshℱ)  (   )U = _V ⊇ f (   )Uℱ(   )V where the inductive limit goes over open V ⊆ Y
:res^U_W is the induced map on the inductive limit of the subset { V: V ⊇ f (   )U }⊆{ V : V ⊇ f (   )W }",Inverse Image
"['Definitions/Inverse Mappings', 'Definitions/Inverses of Mappings', 'Definitions/Mapping Theory', 'Definitions/Inverses']",Definition:Invertible,"=== Definition 1 ===
Let $f: S to T$ be a mapping.

Let $f^{-1} subseteq T times S$ be the inverse of $f$:
:$f^{-1} := leftlbrace left( t, s right): f left(   right)s = t rightrbrace$


Let $f^{-1}$ itself be a mapping:
:$forall y in T: left( y, x_1 right) in f^{-1} land left( y, x_2 right) in f^{-1} implies x_1 = x_2$
and
:$forall y in T: exists x in S: left( y, x right) in f$


Then $f^{-1}$ is called the inverse mapping of $f$.

=== Definition 2 ===
Let $f: S to T$ and $g: T to S$ be mappings.

Let:
:$g circ f = I_S$
:$f circ g = I_T$
where:
:$g circ f$ and $f circ g$ denotes the composition of $f$ with $g$ in either order
:$I_S$ and $I_T$ denote the identity mappings on $S$ and $T$ respectively.

That is, $f$ and $g$ are both left inverse mappings and right inverse mappings of each other.


Then:
:$g$ is the inverse (mapping) of $f$
:$f$ is the inverse (mapping) of $g$.",Definition:Inverse Mapping,,false,"=== Definition 1 ===
Let f: S → T be a mapping.

Let f^-1⊆ T × S be the inverse of f:
:f^-1 := {( t, s ): f (   )s = t }


Let f^-1 itself be a mapping:
:∀ y ∈ T: ( y, x_1 ) ∈ f^-1( y, x_2 ) ∈ f^-1 x_1 = x_2
and
:∀ y ∈ T: ∃ x ∈ S: ( y, x ) ∈ f


Then f^-1 is called the inverse mapping of f.

=== Definition 2 ===
Let f: S → T and g: T → S be mappings.

Let:
:g ∘ f = I_S
:f ∘ g = I_T
where:
:g ∘ f and f ∘ g denotes the composition of f with g in either order
:I_S and I_T denote the identity mappings on S and T respectively.

That is, f and g are both left inverse mappings and right inverse mappings of each other.


Then:
:g is the inverse (mapping) of f
:f is the inverse (mapping) of g.",Invertible
['Definitions/Abstract Algebra'],Definition:Invertible,"Let $left( S, circ right)$ be an algebraic structure which has an identity $e_S$.

If $x in S$ has an inverse, then $x$ is said to be invertible for $circ$.


That is, $x$ is invertible  if and only if :

:$exists y in S: x circ y = e_S = y circ x$


=== Invertible Operation ===
Let $left({S, circ}right)$ be an algebraic structure.


The operation $circ$ is invertible  if and only if :
:$forall a, b in S: exists r, s in S: a circ r = b = s circ a$",Definition:Invertible Element,,false,"Let ( S, ∘) be an algebraic structure which has an identity e_S.

If x ∈ S has an inverse, then x is said to be invertible for ∘.


That is, x is invertible  if and only if :

:∃ y ∈ S: x ∘ y = e_S = y ∘ x


=== Invertible Operation ===
Let (S, ∘) be an algebraic structure.


The operation ∘ is invertible  if and only if :
:∀ a, b ∈ S: ∃ r, s ∈ S: a ∘ r = b = s ∘ a",Invertible
"['Definitions/Inverse Matrices', 'Definitions/Matrices']",Definition:Invertible,"Let $left( R, +, circ right)$ be a ring with unity.

Let $n in mathbb Z_{>0}$ be a (strictly) positive integer.

Let $mathbf A$ be an element of the ring of square matrices $left( mathcal M_R left(   right)n, +, times right)$.


Then $mathbf A$ is invertible  if and only if :
:$exists mathbf B in left( mathcal M_R left(   right)n, +, times right): mathbf A mathbf B = mathbf I_n = mathbf B mathbf A$
where $mathbf I_n$ denotes the unit matrix of order $n$.


Such a $mathbf B$ is the inverse of $mathbf A$.

It is usually denoted $mathbf A^{-1}$.",Definition:Invertible Matrix,,false,"Let ( R, +, ∘) be a ring with unity.

Let n ∈ℤ_>0 be a (strictly) positive integer.

Let 𝐀 be an element of the ring of square matrices ( ℳ_R (   )n, +, ×).


Then 𝐀 is invertible  if and only if :
:∃𝐁∈( ℳ_R (   )n, +, ×): 𝐀𝐁 = 𝐈_n = 𝐁𝐀
where 𝐈_n denotes the unit matrix of order n.


Such a 𝐁 is the inverse of 𝐀.

It is usually denoted 𝐀^-1.",Invertible
"['Definitions/Invertible Bounded Linear Operator', 'Definitions/Invertible Bounded Linear Operators', 'Definitions/Bounded Linear Operators', 'Definitions/Invertible Bounded Linear Operators']",Definition:Invertible,"=== Normed Vector Space ===
Let $left( X, leftlVert cdot rightrVert right)$ be a normed vector space.

Let $T : X to X$ be an invertible bounded linear transformation.


We say that $A$ is a bounded linear operator. 

=== Inner Product Space ===
Let $left( X, leftlangle cdot,   rightranglecdot right)$ be an inner product  space.

Let $T : X to X$ be an invertible bounded linear transformation.


We say that $A$ is a bounded linear operator.",Definition:Invertible Bounded Linear Operator,,false,"=== Normed Vector Space ===
Let ( X, ‖·‖) be a normed vector space.

Let T : X → X be an invertible bounded linear transformation.


We say that A is a bounded linear operator. 

=== Inner Product Space ===
Let ( X, ⟨·,   ⟩·) be an inner product  space.

Let T : X → X be an invertible bounded linear transformation.


We say that A is a bounded linear operator.",Invertible
"['Definitions/Bounded Linear Transformations', 'Definitions/Invertible Bounded Linear Transformations']",Definition:Invertible,"=== Normed Vector Space ===
Let $left( V, leftlVert cdot rightrVert_V right)$ and $left( U, leftlVert cdot rightrVert_U right)$ be normed vector spaces.

Let $A : V to U$ be a bounded linear transformation.


We say that $A$ is invertible as a bounded linear transformation  if and only if :

:$A$ has an inverse mapping that is a bounded linear transformation.


That is:

:there exists a bounded linear transformation $B : U to V$ such that: 

::$A circ B = I_U$
::$B circ A = I_V$

where $I_U$ and $I_V$ are the identity mappings on $U$ and $V$ respectively.

We say that $B$ is the inverse of $A$ and write $B = A^{-1}$. 

The process of finding an $A^{-1}$ given $A$ is called inverting.

=== Inner Product Space ===
Let $left( V, leftlangle cdot,   rightranglecdot right)$ and $left( U, leftlangle cdot,   rightranglecdot right)$ be inner product spaces.

Let $A : V to U$ be a bounded linear transformation.


We say that $A$ is invertible as a bounded linear transformation  if and only if :

:$A$ has an inverse mapping that is a bounded linear transformation.


That is:

:there exists a bounded linear transformation $B : U to V$ such that: 

::$A circ B = I_U$
::$B circ A = I_V$

where $I_U$ and $I_V$ are the identity mappings on $U$ and $V$ respectively.

We say that $B$ is the inverse of $A$ and write $B = A^{-1}$. 

The process of finding an $A^{-1}$ given $A$ is called inverting.",Definition:Invertible Bounded Linear Transformation,,false,"=== Normed Vector Space ===
Let ( V, ‖·‖_V ) and ( U, ‖·‖_U ) be normed vector spaces.

Let A : V → U be a bounded linear transformation.


We say that A is invertible as a bounded linear transformation  if and only if :

:A has an inverse mapping that is a bounded linear transformation.


That is:

:there exists a bounded linear transformation B : U → V such that: 

::A ∘ B = I_U
::B ∘ A = I_V

where I_U and I_V are the identity mappings on U and V respectively.

We say that B is the inverse of A and write B = A^-1. 

The process of finding an A^-1 given A is called inverting.

=== Inner Product Space ===
Let ( V, ⟨·,   ⟩·) and ( U, ⟨·,   ⟩·) be inner product spaces.

Let A : V → U be a bounded linear transformation.


We say that A is invertible as a bounded linear transformation  if and only if :

:A has an inverse mapping that is a bounded linear transformation.


That is:

:there exists a bounded linear transformation B : U → V such that: 

::A ∘ B = I_U
::B ∘ A = I_V

where I_U and I_V are the identity mappings on U and V respectively.

We say that B is the inverse of A and write B = A^-1. 

The process of finding an A^-1 given A is called inverting.",Invertible
"['Definitions/Powers', 'Definitions/Algebra', 'Definitions/Numbers', 'Definitions/Real Analysis', 'Definitions/Complex Analysis', 'Definitions/Involution']",Definition:Involution,"=== Natural Numbers ===
Let $mathbb N$ denote the natural numbers.


For each $m in mathbb N$, recursively define $e_m: mathbb N to mathbb N$ to be the mapping:
:$e_m left({n}right) = begin{cases}
1 & : n = 0 \
m times e_m left({x}right) & : n = x + 1
end{cases}$
where:
: $+$ denotes natural number addition.
: $times$ denotes natural number multiplication.


$e_m left({n}right)$ is then expressed as a binary operation in the form:
:$m^n := e_m left({n}right)$

and is called $m$ to the power of $n$.

=== Integers ===
Let $x in mathbb R$ be a real number.

Let $n in mathbb Z$ be an integer.

The expression $x^n$ is called $x$ to the power of $n$.

$x^n$ is defined recursively as:


:$x^n = begin {cases} 1 & : n = 0 \ & \ x times x^{n - 1} & : n > 0 \ & \ dfrac {x^{n + 1} } x & : n < 0 end {cases}$

where $dfrac {x^{n + 1} } x$ denotes division.


=== Even Power ===
Let $x in mathbb R$ be a real number.

Let $n in mathbb Z$ be an even integer.


Then $x^n$ is called an even power of $x$.


Category:Definitions/Integer Powers
Category:Definitions/Even Integers

=== Odd Power ===
Let $x in mathbb R$ be a real number.

Let $n in mathbb Z$ be an odd integer.


Then $x^n$ is called an odd power of $x$


Category:Definitions/Integer Powers
Category:Definitions/Odd Integers

=== Rational Numbers ===
Let $x in mathbb R$ be a real number such that $x > 0$.

Let $m in mathbb Z$ be an integer.

Let $y = sqrt [m] x$ be the $m$th root of $x$.


Then we can write $y = x^{1/m}$ which means the same thing as $y = sqrt [m] x$.


Thus we can define the power to a positive rational number:

Let $r = dfrac p q in mathbb Q$ be a positive rational number where $p in mathbb Z_{ge 0}, q in mathbb Z_{> 0}$.

Then $x^r$ is defined as:
:$x^r = x^{p/q} = left( sqrt [q] x right)^p = sqrt [q] {left( x^p right) }$


When $r = dfrac {-p} q in mathbb Q: r < 0$ we define:
:$x^r = x^{-p/q} = dfrac 1 {x^{p/q} }$
analogously for the negative integer definition.

=== Real Numbers ===
Let $x in mathbb R_{>0}$ be a (strictly) positive real number.

Let $r in mathbb R$ be a real number.


We define $x^r$ as:

:$x^r := exp left(   right){r ln x}$
where $exp$ denotes the exponential function.


This definition is an extension of the definition for rational $r$.

This follows from Logarithms of Powers and Exponential of Natural Logarithm: it can be seen that:
:$forall r in mathbb Q: exp left(   right){r ln x} = exp left(   right){ln left(   right){x^r} } = x^r$

 

=== Complex Numbers ===
Let $z, k in mathbb C$ be complex numbers.


$z$ to the power of $k$ is defined as the multifunction:

:$z^k := e^{k ln left( z right)}$

where:
:$e^z$ is the exponential function
:$ln$ is the  natural logarithm multifunction.


=== Principal Branch ===
The principal branch of a complex number raised to a complex power is defined as:

:$z^k = e^{k operatorname {Ln} z}$

where $operatorname {Ln} z$ is the principal branch of the natural logarithm.


=== Positive Real Base ===
Let $t > 0$ be a real number and let $k$ be a complex number.


The principal branch of a positive real number raised to a complex power is defined as:

:$t^k = e^{k ln t}$

where $ln$ is the natural logarithm of a positive real number.


Category:Definitions/Complex Powers

Category:Definitions/Complex Powers

=== Multiindices ===
Let $k = left langle {k_j}right rangle_{j = 1, ldots, n}$ be a multiindex indexed by $leftlbrace 1, ldots, n rightrbrace$.

Let $x = left( x_1, ldots, x_n right) in mathbb R^n$ be an ordered tuple of real numbers.


Then $x^k$ is defined as:

:$ds x^k := prod_{j mathop = 1}^n x_j^{k_j}$
where the powers on the   are integer powers.


Category:Definitions/Analysis

=== Power of Zero ===
Let $r in mathbb R$ be a real number.

(This includes the situation where $r in mathbb Z$ or $r in mathbb Q$.)

When $x=0$, $x^r$ is defined as follows:

:$0^r = begin{cases}
1 & : r = 0 \
0 & : r > 0 \
text{Undefined} & : r < 0 \
end{cases}$

This takes account of the awkward case $0^0$: it is ""generally accepted"" that $0^0 = 1$ as this convention agrees with certain general results which would otherwise need a special case.",Definition:Power (Algebra),,false,"=== Natural Numbers ===
Let ℕ denote the natural numbers.


For each m ∈ℕ, recursively define e_m: ℕ→ℕ to be the mapping:
:e_m (n) = 
1     : n = 0 

m × e_m (x)     : n = x + 1
where:
: + denotes natural number addition.
: × denotes natural number multiplication.


e_m (n) is then expressed as a binary operation in the form:
:m^n := e_m (n)

and is called m to the power of n.

=== Integers ===
Let x ∈ℝ be a real number.

Let n ∈ℤ be an integer.

The expression x^n is called x to the power of n.

x^n is defined recursively as:


:x^n =  1     : n = 0 
   
 x × x^n - 1    : n > 0 
   
x^n + 1 x     : n < 0

where x^n + 1 x denotes division.


=== Even Power ===
Let x ∈ℝ be a real number.

Let n ∈ℤ be an even integer.


Then x^n is called an even power of x.


Category:Definitions/Integer Powers
Category:Definitions/Even Integers

=== Odd Power ===
Let x ∈ℝ be a real number.

Let n ∈ℤ be an odd integer.


Then x^n is called an odd power of x


Category:Definitions/Integer Powers
Category:Definitions/Odd Integers

=== Rational Numbers ===
Let x ∈ℝ be a real number such that x > 0.

Let m ∈ℤ be an integer.

Let y = √(x) be the mth root of x.


Then we can write y = x^1/m which means the same thing as y = √(x).


Thus we can define the power to a positive rational number:

Let r =  p q ∈ℚ be a positive rational number where p ∈ℤ_≥ 0, q ∈ℤ_> 0.

Then x^r is defined as:
:x^r = x^p/q = ( √(x))^p = √(( x^p ) )


When r = -p q ∈ℚ: r < 0 we define:
:x^r = x^-p/q =  1 x^p/q
analogously for the negative integer definition.

=== Real Numbers ===
Let x ∈ℝ_>0 be a (strictly) positive real number.

Let r ∈ℝ be a real number.


We define x^r as:

:x^r := exp(   )r ln x
where exp denotes the exponential function.


This definition is an extension of the definition for rational r.

This follows from Logarithms of Powers and Exponential of Natural Logarithm: it can be seen that:
:∀ r ∈ℚ: exp(   )r ln x = exp(   )ln(   )x^r = x^r

 

=== Complex Numbers ===
Let z, k ∈ℂ be complex numbers.


z to the power of k is defined as the multifunction:

:z^k := e^k ln( z )

where:
:e^z is the exponential function
:ln is the  natural logarithm multifunction.


=== Principal Branch ===
The principal branch of a complex number raised to a complex power is defined as:

:z^k = e^k Ln z

where Ln z is the principal branch of the natural logarithm.


=== Positive Real Base ===
Let t > 0 be a real number and let k be a complex number.


The principal branch of a positive real number raised to a complex power is defined as:

:t^k = e^k ln t

where ln is the natural logarithm of a positive real number.


Category:Definitions/Complex Powers

Category:Definitions/Complex Powers

=== Multiindices ===
Let k = ⟨k_j⟩_j = 1, …, n be a multiindex indexed by { 1, …, n }.

Let x = ( x_1, …, x_n ) ∈ℝ^n be an ordered tuple of real numbers.


Then x^k is defined as:

:x^k := ∏_j  = 1^n x_j^k_j
where the powers on the   are integer powers.


Category:Definitions/Analysis

=== Power of Zero ===
Let r ∈ℝ be a real number.

(This includes the situation where r ∈ℤ or r ∈ℚ.)

When x=0, x^r is defined as follows:

:0^r = 
1     : r = 0 

0     : r > 0 
Undefined    : r < 0

This takes account of the awkward case 0^0: it is ""generally accepted"" that 0^0 = 1 as this convention agrees with certain general results which would otherwise need a special case.",Involution
"['Definitions/Mapping Theory', 'Definitions/Involutions', 'Definitions/Involution']",Definition:Involution,"Let $A$ be a set.

Let $f: A to A$ be a mapping on $A$.


=== Definition 1 ===
Let $A$ be a set.

Let $f: A to A$ be a mapping on $A$.


$f$ is an involution  if and only if :
:$forall x in A: f left(   right){f left(   right)x} = x$

That is:
:$f circ f = I_A$
where $I_A$ denotes the identity mapping on $A$.

=== Definition 2 ===
Let $A$ be a set.

Let $f: A to A$ be a mapping on $A$.


$f$ is an involution  if and only if :
:$forall x, y in A: f left(   right)x = y implies f left(   right)y = x$

=== Definition 3 ===
Let $A$ be a set.

Let $f: A to A$ be a mapping on $A$.


$f$ is an involution  if and only if  $f$ is both a bijection and a symmetric relation.

That is,  if and only if  $f$ is a bijection such that:
:$f = f^{-1}$",Definition:Involution (Mapping),,false,"Let A be a set.

Let f: A → A be a mapping on A.


=== Definition 1 ===
Let A be a set.

Let f: A → A be a mapping on A.


f is an involution  if and only if :
:∀ x ∈ A: f (   )f (   )x = x

That is:
:f ∘ f = I_A
where I_A denotes the identity mapping on A.

=== Definition 2 ===
Let A be a set.

Let f: A → A be a mapping on A.


f is an involution  if and only if :
:∀ x, y ∈ A: f (   )x = y  f (   )y = x

=== Definition 3 ===
Let A be a set.

Let f: A → A be a mapping on A.


f is an involution  if and only if  f is both a bijection and a symmetric relation.

That is,  if and only if  f is a bijection such that:
:f = f^-1",Involution
"['Definitions/Factorization', 'Definitions/Ring Theory', 'Definitions/Irreducible Elements of Rings']",Definition:Irreducible,"Let $left( D, +, circ right)$ be an integral domain whose zero is $0_D$.

Let $left( U_D, circ right)$ be the group of units of $left( D, +, circ right)$.


Let $x in D: x notin U_D, x ne 0_D$, that is, $x$ is non-zero and not a unit.


=== Definition 1 ===
Let $left( D, +, circ right)$ be an integral domain whose zero is $0_D$.

Let $left( U_D, circ right)$ be the group of units of $left( D, +, circ right)$.

Let $x in D: x notin U_D, x ne 0_D$, that is, $x$ is non-zero and not a unit.


$x$ is defined as irreducible  if and only if  it has no non-trivial factorization in $D$.

That is,  if and only if  $x$ cannot be written as a product of two non-units.

=== Definition 2 ===
Let $left( D, +, circ right)$ be an integral domain whose zero is $0_D$.

Let $left( U_D, circ right)$ be the group of units of $left( D, +, circ right)$.

Let $x in D: x notin U_D, x ne 0_D$, that is, $x$ is non-zero and not a unit.


$x$ is defined as irreducible  if and only if  the only divisors of $x$ are its associates and the units of $D$.

That is,  if and only if  $x$ has no proper divisors.",Definition:Irreducible Element of Ring,,false,"Let ( D, +, ∘) be an integral domain whose zero is 0_D.

Let ( U_D, ∘) be the group of units of ( D, +, ∘).


Let x ∈ D: x ∉ U_D, x  0_D, that is, x is non-zero and not a unit.


=== Definition 1 ===
Let ( D, +, ∘) be an integral domain whose zero is 0_D.

Let ( U_D, ∘) be the group of units of ( D, +, ∘).

Let x ∈ D: x ∉ U_D, x  0_D, that is, x is non-zero and not a unit.


x is defined as irreducible  if and only if  it has no non-trivial factorization in D.

That is,  if and only if  x cannot be written as a product of two non-units.

=== Definition 2 ===
Let ( D, +, ∘) be an integral domain whose zero is 0_D.

Let ( U_D, ∘) be the group of units of ( D, +, ∘).

Let x ∈ D: x ∉ U_D, x  0_D, that is, x is non-zero and not a unit.


x is defined as irreducible  if and only if  the only divisors of x are its associates and the units of D.

That is,  if and only if  x has no proper divisors.",Irreducible
"['Definitions/Irreducible Polynomials', 'Definitions/Factorization', 'Definitions/Polynomial Theory']",Definition:Irreducible,"An irreducible polynomial is a polynomial which is not reducible.


=== Definition 1 ===
Let $R$ be an integral domain.


An irreducible polynomial over $R$ is an irreducible element of the polynomial ring $R left[ X right]$.

=== Definition 2: for fields ===
Let $K$ be a field.


An irreducible polynomial over $K$ is a nonconstant polynomial over $K$ that is not the product of two polynomials of smaller degree.

=== Definition 3: for fields ===
Let $K$ be a field.


An irreducible polynomial over $K$ is a polynomial over $K$ that is not the product of two nonconstant polynomials.",Definition:Irreducible Polynomial,,false,"An irreducible polynomial is a polynomial which is not reducible.


=== Definition 1 ===
Let R be an integral domain.


An irreducible polynomial over R is an irreducible element of the polynomial ring R [ X ].

=== Definition 2: for fields ===
Let K be a field.


An irreducible polynomial over K is a nonconstant polynomial over K that is not the product of two polynomials of smaller degree.

=== Definition 3: for fields ===
Let K be a field.


An irreducible polynomial over K is a polynomial over K that is not the product of two nonconstant polynomials.",Irreducible
"['Definitions/Topology', 'Definitions/Connected Spaces', 'Definitions/Irreducible Spaces']",Definition:Irreducible,"=== Definition 1 ===
A topological space $T = left( S, tau right)$ is irreducible  if and only if  there exists no cover of $T$ by two proper closed sets of $T$.

=== Definition 2 ===
A topological space $T = left( S, tau right)$ is irreducible  if and only if  there is no finite cover of $T$ by proper closed sets of $T$.

=== Definition 3 ===
A topological space $T = left( S, tau right)$ is irreducible  if and only if  every two non-empty open sets of $T$ have non-empty intersection:

:$forall U, V in tau: U, V ne varnothing implies U cap V ne varnothing$

=== Definition 4 ===
A topological space $T = left( S, tau right)$ is irreducible  if and only if  every non-empty open set of $T$ is (everywhere) dense in $T$.

=== Definition 5 ===
A topological space $T = left( S, tau right)$ is irreducible  if and only if  the interior of every proper closed set of $T$ is empty.

=== Definition 6 ===
A topological space $T = left( S, tau right)$ is irreducible  if and only if  the closure of every non-empty open set is the whole space:
:$forall U in tau: U ne varnothing implies U^- = S$

=== Definition 7 ===
A topological space $T = left( S, tau right)$ is irreducible  if and only if  every open set of $T$ is connected.",Definition:Irreducible Space,,false,"=== Definition 1 ===
A topological space T = ( S, τ) is irreducible  if and only if  there exists no cover of T by two proper closed sets of T.

=== Definition 2 ===
A topological space T = ( S, τ) is irreducible  if and only if  there is no finite cover of T by proper closed sets of T.

=== Definition 3 ===
A topological space T = ( S, τ) is irreducible  if and only if  every two non-empty open sets of T have non-empty intersection:

:∀ U, V ∈τ: U, V ∅ U ∩ V ∅

=== Definition 4 ===
A topological space T = ( S, τ) is irreducible  if and only if  every non-empty open set of T is (everywhere) dense in T.

=== Definition 5 ===
A topological space T = ( S, τ) is irreducible  if and only if  the interior of every proper closed set of T is empty.

=== Definition 6 ===
A topological space T = ( S, τ) is irreducible  if and only if  the closure of every non-empty open set is the whole space:
:∀ U ∈τ: U ∅ U^- = S

=== Definition 7 ===
A topological space T = ( S, τ) is irreducible  if and only if  every open set of T is connected.",Irreducible
['Definitions/Topology'],Definition:Irreducible,"Let $T = left( S, tau right)$ be a topological space.

Let $A$ be a subset of $S$.


Then $A$ is irreducible (subset)  if and only if 
:$A$ is non-empty and closed and for all closed subsets $B, C$ of $S$: $A = B cup C implies A = B$ or $A = C$",Definition:Irreducible Subset (Topology),,false,"Let T = ( S, τ) be a topological space.

Let A be a subset of S.


Then A is irreducible (subset)  if and only if 
:A is non-empty and closed and for all closed subsets B, C of S: A = B ∪ C  A = B or A = C",Irreducible
['Definitions/Representation Theory'],Definition:Irreducible,"Let $rho: G to mathrm {GL} left( V right)$ be a linear representation.

Then $rho$ is irreducible  if and only if  it is not reducible.


That is,  if and only if  there exists no non-trivial proper vector subspace $W$ of $V$ such that:
: $forall g in G: rho left(   right)g left(   right)W subseteq W$


Category:Definitions/Representation Theory",Definition:Irreducible (Representation Theory)/Linear Representation,,false,"Let ρ: G →GL( V ) be a linear representation.

Then ρ is irreducible  if and only if  it is not reducible.


That is,  if and only if  there exists no non-trivial proper vector subspace W of V such that:
: ∀ g ∈ G: ρ(   )g (   )W ⊆ W


Category:Definitions/Representation Theory",Irreducible
['Definitions/Representation Theory'],Definition:Irreducible,"=== Linear Representation ===
Let $rho: G to mathrm {GL} left( V right)$ be a linear representation.

Then $rho$ is irreducible  if and only if  it is not reducible.


That is,  if and only if  there exists no non-trivial proper vector subspace $W$ of $V$ such that:
: $forall g in G: rho left(   right)g left(   right)W subseteq W$


Category:Definitions/Representation Theory

=== G-Module ===
A $G$-module is irreducible  if and only if  the corresponding linear representation is irreducible.


That is, any proper $G$-submodule is trivial.",Definition:Irreducible (Representation Theory),,false,"=== Linear Representation ===
Let ρ: G →GL( V ) be a linear representation.

Then ρ is irreducible  if and only if  it is not reducible.


That is,  if and only if  there exists no non-trivial proper vector subspace W of V such that:
: ∀ g ∈ G: ρ(   )g (   )W ⊆ W


Category:Definitions/Representation Theory

=== G-Module ===
A G-module is irreducible  if and only if  the corresponding linear representation is irreducible.


That is, any proper G-submodule is trivial.",Irreducible
['Definitions/Order Theory'],Definition:Irreducible,"Let $left({S, wedge, preceq}right)$ be a meet semilattice.

Let $g in S$.


Then $g$ is meet irreducible  if and only if :
:$forall x, y in S: g = x wedge y implies g = x$ or $g = y$",Definition:Meet Irreducible,,false,"Let (S, ∧, ≼) be a meet semilattice.

Let g ∈ S.


Then g is meet irreducible  if and only if :
:∀ x, y ∈ S: g = x ∧ y  g = x or g = y",Irreducible
['Definitions/Order Theory'],Definition:Irreducible,"Let $left( S, preceq right)$ be an ordered set.

Let $p in S$.

An element $p$ is completely irreducible  if and only if 
:$p^succeq setminus leftlbrace p rightrbrace$ admits a minimum element
where $p^succeq$ denotes the upper closure of $p$.",Definition:Completely Irreducible,,false,"Let ( S, ≼) be an ordered set.

Let p ∈ S.

An element p is completely irreducible  if and only if 
:p^≽∖{ p } admits a minimum element
where p^≽ denotes the upper closure of p.",Irreducible
['Definitions/Schemes'],Definition:Irreducible,"Let $left( X, mathcal O_X right)$ be a scheme.


Then $left( X, mathcal O_X right)$ is irreducible  if and only if  $X$ is an irreducible space.",Definition:Irreducible Scheme,,false,"Let ( X, 𝒪_X ) be a scheme.


Then ( X, 𝒪_X ) is irreducible  if and only if  X is an irreducible space.",Irreducible
"['Definitions/Isometries (Metric Spaces)', 'Definitions/Metric Spaces', 'Definitions/Pseudometric Spaces', 'Definitions/Isometries']",Definition:Isometry,"=== Definition 1 ===
Let $M_1 = left( A_1, d_1 right)$ and $M_2 = left( A_2, d_2 right)$ be metric spaces or pseudometric spaces.


Let $phi: A_1 to A_2$ be a bijection such that:
:$forall a, b in A_1: d_1 left(   right){a, b} = d_2 left(   right){phi left(   right)a, phi left(   right)b}$


Then $phi$ is called an isometry.

That is, an isometry is a distance-preserving bijection.

=== Definition 2 ===
Let $M_1 = left( A_1, d_1 right)$ and $M_2 = left( A_2, d_2 right)$ be metric spaces or pseudometric spaces.


:$M_1$ and $M_2$ are isometric  if and only if  there exist inverse mappings $phi: A_1 to A_2$ and $phi^{-1}: A_2 to A_1$ such that:

::$forall a, b in A_1: d_1 left(   right){a, b} = d_2 left(   right){phi left(   right)a, phi left(   right)b}$
:and:
::$forall u, v in A_2: d_2 left(   right){u, v} = d_1 left(   right){phi^{-1}  left(   right)u, phi^{-1}  left(   right)v}$

Such metric spaces $M_1$ and $M_2$ are defined as being isometric.


=== Isometry Into ===
Let $M_1 = left( A_1, d_1 right)$ and $M_2 = left( A_2, d_2 right)$ be metric spaces or pseudometric spaces.


Let $phi: A_1 to A_2$ be an injection such that:
:$forall a, b in A_1: d_1 left(   right){a, b} = d_2 left(   right){phi left(   right)a, phi left(   right)b}$


Then $phi$ is called an isometry (from $M_1$) into $M_2$.


That is, an isometry (from $M_1$) into $M_2$ is an isometry which is not actually a surjection, but satisfies the other conditions for being an isometry.",Definition:Isometry (Metric Spaces),,false,"=== Definition 1 ===
Let M_1 = ( A_1, d_1 ) and M_2 = ( A_2, d_2 ) be metric spaces or pseudometric spaces.


Let ϕ: A_1 → A_2 be a bijection such that:
:∀ a, b ∈ A_1: d_1 (   )a, b = d_2 (   )ϕ(   )a, ϕ(   )b


Then ϕ is called an isometry.

That is, an isometry is a distance-preserving bijection.

=== Definition 2 ===
Let M_1 = ( A_1, d_1 ) and M_2 = ( A_2, d_2 ) be metric spaces or pseudometric spaces.


:M_1 and M_2 are isometric  if and only if  there exist inverse mappings ϕ: A_1 → A_2 and ϕ^-1: A_2 → A_1 such that:

::∀ a, b ∈ A_1: d_1 (   )a, b = d_2 (   )ϕ(   )a, ϕ(   )b
:and:
::∀ u, v ∈ A_2: d_2 (   )u, v = d_1 (   )ϕ^-1(   )u, ϕ^-1(   )v

Such metric spaces M_1 and M_2 are defined as being isometric.


=== Isometry Into ===
Let M_1 = ( A_1, d_1 ) and M_2 = ( A_2, d_2 ) be metric spaces or pseudometric spaces.


Let ϕ: A_1 → A_2 be an injection such that:
:∀ a, b ∈ A_1: d_1 (   )a, b = d_2 (   )ϕ(   )a, ϕ(   )b


Then ϕ is called an isometry (from M_1) into M_2.


That is, an isometry (from M_1) into M_2 is an isometry which is not actually a surjection, but satisfies the other conditions for being an isometry.",Isometry
"['Definitions/Isometries (Euclidean Geometry)', 'Definitions/Euclidean Geometry', 'Definitions/Isometries']",Definition:Isometry,"Let $mathcal E$ be a real Euclidean space.


Let $phi: mathcal E to mathcal E$ be a bijection such that:
:$forall P, Q in mathcal E: PQ = P'Q'$
where:
:$P$ and $Q$ are arbitrary points in $mathcal E$
:$P'$ and $Q'$ are the images of $P$ and $Q$ respectively
:$PQ$ and $P'Q'$ denote the lengths of the straight line segments $PQ$ and $P'Q'$ respectively.


Then $phi$ is an isometry.


That is, an isometry is a bijection which preserves distance between points.


=== Context ===
An isometry is defined usually for either:
:$n = 2$, representing the plane
or:
:$n = 3$, representing ordinary space.",Definition:Isometry (Euclidean Geometry),,false,"Let ℰ be a real Euclidean space.


Let ϕ: ℰ→ℰ be a bijection such that:
:∀ P, Q ∈ℰ: PQ = P'Q'
where:
:P and Q are arbitrary points in ℰ
:P' and Q' are the images of P and Q respectively
:PQ and P'Q' denote the lengths of the straight line segments PQ and P'Q' respectively.


Then ϕ is an isometry.


That is, an isometry is a bijection which preserves distance between points.


=== Context ===
An isometry is defined usually for either:
:n = 2, representing the plane
or:
:n = 3, representing ordinary space.",Isometry
"['Definitions/Isometries (Inner Product Spaces)', 'Definitions/Inner Product Spaces', 'Definitions/Isometries']",Definition:Isometry,"Let $V$ and $W$ be inner product spaces.

Let their inner products be $leftlangle cdot,   rightranglecdot_V$ and $leftlangle cdot,   rightranglecdot_W$ respectively.

Let the mapping $F : V to W$ be a vector space isomorphism that preserves inner products:

:$forall v_1, v_2 in V : leftlangle v_1,   rightrangle{v_2}_V = leftlangle F left(   right){v_1},   rightrangle{F left(   right){v_2}}_W$


Then $F$ is called a (linear) isometry.


=== Hilbert Spaces ===
Let $H$ and $K$ be Hilbert spaces.

Let their inner products be $leftlangle cdot,   rightranglecdot_H$ and $leftlangle cdot,   rightranglecdot_K$ respectively.


A linear map $U: H to K$ is called an isometry  if and only if :

:$forall g,h in H: leftlangle g,   rightrangle h_H = leftlangle U g,   rightrangle{U h}_K$


 ",Definition:Isometry (Inner Product Spaces),,false,"Let V and W be inner product spaces.

Let their inner products be ⟨·,   ⟩·_V and ⟨·,   ⟩·_W respectively.

Let the mapping F : V → W be a vector space isomorphism that preserves inner products:

:∀ v_1, v_2 ∈ V : ⟨ v_1,   ⟩v_2_V = ⟨ F (   )v_1,   ⟩F (   )v_2_W


Then F is called a (linear) isometry.


=== Hilbert Spaces ===
Let H and K be Hilbert spaces.

Let their inner products be ⟨·,   ⟩·_H and ⟨·,   ⟩·_K respectively.


A linear map U: H → K is called an isometry  if and only if :

:∀ g,h ∈ H: ⟨ g,   ⟩ h_H = ⟨ U g,   ⟩U h_K


 ",Isometry
"['Definitions/Isometries (Riemannian Manifolds)', 'Definitions/Riemannian Manifolds', 'Definitions/Isometries']",Definition:Isometry,"Let $left( M, g right)$ and $left( tilde M, tilde g right)$ be Riemannian manifolds with Riemannian metrics $g$ and $tilde g$ respectively.

Let the mapping $phi : M to tilde M$ be a diffeomorphism such that:

:$phi^* tilde g = g$


Then $phi$ is called an isometry from $left( M, g right)$ to $left( tilde M, tilde g right)$.",Definition:Isometry (Riemannian Manifolds),,false,"Let ( M, g ) and ( M̃, g̃) be Riemannian manifolds with Riemannian metrics g and g̃ respectively.

Let the mapping ϕ : M →M̃ be a diffeomorphism such that:

:ϕ^* g̃ = g


Then ϕ is called an isometry from ( M, g ) to ( M̃, g̃).",Isometry
"['Definitions/Isomorphisms (Abstract Algebra)', 'Definitions/Bijections', 'Definitions/Epimorphisms (Abstract Algebra)', 'Definitions/Monomorphisms (Abstract Algebra)', 'Definitions/Homomorphisms (Abstract Algebra)', 'Definitions/Abstract Algebra', 'Definitions/Isomorphisms']",Definition:Isomorphism,"An isomorphism is a homomorphism which is a bijection.

That is, it is a mapping which is both a monomorphism and an epimorphism.


An algebraic structure $left( S, circ right)$ is isomorphic to another algebraic structure $left( T, * right)$  if and only if  there exists an isomorphism from $left( S, circ right)$ to $left( T, * right)$, and we can write $S cong T$ (although notation may vary).


=== Semigroup Isomorphism ===
Let $left( S, circ right)$ and $left( T, * right)$ be semigroups.

Let $phi: S to T$ be a (semigroup) homomorphism.


Then $phi$ is a semigroup isomorphism  if and only if  $phi$ is a bijection.

That is, $phi$ is a semigroup isomorphism  if and only if  $phi$ is both a monomorphism and an epimorphism.


If $S$ is isomorphic to $T$, then the notation $S cong T$ can be used (although notation varies).

=== Monoid Isomorphism ===
Let $left( S, circ right)$ and $left( T, * right)$ be monoids.

Let $phi: S to T$ be a (monoid) homomorphism.


Then $phi$ is a monoid isomorphism  if and only if  $phi$ is a bijection.


That is, $phi$ is a monoid isomorphism  if and only if  $phi$ is both a monomorphism and an epimorphism.


If $S$ is isomorphic to $T$, then the notation $S cong T$ can be used (although notation varies).

=== Group Isomorphism ===
Let $left( G, circ right)$ and $left( H, * right)$ be groups.

Let $phi: G to H$ be a (group) homomorphism.


Then $phi$ is a group isomorphism  if and only if  $phi$ is a bijection.


That is, $phi$ is a group isomorphism  if and only if  $phi$ is both a monomorphism and an epimorphism.


If $G$ is isomorphic to $H$, then the notation $G cong H$ can be used (although notation varies).

=== Ring Isomorphism ===
Let $left( R, +, circ right)$ and $left( S, oplus, * right)$ be rings.

Let $phi: R to S$ be a (ring) homomorphism.


Then $phi$ is a ring isomorphism  if and only if  $phi$ is a bijection.

That is, $phi$ is a ring isomorphism  if and only if  $phi$ is both a monomorphism and an epimorphism.

=== $F$-Isomorphism ===
Let $R, S$ be rings with unity.

Let $F$ be a subfield of both $R$ and $S$.


Let $phi: R to S$ be an $F$-homomorphism such that $phi$ is bijective.


Then $phi$ is an $F$-isomorphism.


The relationship between $R$ and $S$ is denoted $R cong_F S$.

=== Field Isomorphism ===
Let $left( F, +, circ right)$ and $left( K, oplus, * right)$ be fields.

Let $phi: F to K$ be a (field) homomorphism.


Then $phi$ is a field isomorphism  if and only if  $phi$ is a bijection.

That is, $phi$ is a field isomorphism  if and only if  $phi$ is both a monomorphism and an epimorphism.

=== $R$-Algebraic Structure Isomorphism ===
Let $left( S, ast_1, ast_2, ldots, ast_n, circ right)_R$ and $left( T, odot_1, odot_2, ldots, odot_n, otimes right)_R$ be $R$-algebraic structures.

Let $phi: S to T$ be an $R$-algebraic structure homomorphism.


Then $phi$ is an $R$-algebraic structure isomorphism  if and only if  $phi$ is a bijection.


=== Module Isomorphism ===
Let $R$ be a ring.

Let $left( G, +_G, circ right)_R$ and $left( H, +_H, circ right)_R$ be $R$-modules.

Let $phi: G to H$ be a module homomorphism.


Then $phi$ is a module isomorphism  if and only if  $phi$ is a bijection.

=== Vector Space Isomorphism ===
Let $left( V, +, circ right)$ and $left( W, +', circ' right)$ be $K$-vector spaces.

Then $phi: V to W$ is a vector space isomorphism  if and only if :

:$(1): quad phi$ is a bijection
:$(2): quad forall mathbf x, mathbf y in V: phi left(   right){mathbf x + mathbf y} = phi left(   right){mathbf x} +' phi left(   right){mathbf y}$
:$(3): quad forall mathbf x in V: forall lambda in K: phi left(   right){lambda mathbf x} = lambda phi left(   right){mathbf x}$


Category:Definitions/Isomorphisms (Abstract Algebra)
Category:Definitions/R-Algebraic Structure Isomorphisms
Category:Definitions/Linear Algebra

=== Ordered Structure Isomorphism ===
An ordered structure isomorphism from an ordered structure $left( S, circ, preceq right)$ to another $left( T, *, preccurlyeq right)$ is a mapping $phi: S to T$ that is both:

:$(1): quad$ An isomorphism, that is a bijective homomorphism, from the structure $left( S, circ right)$ to the structure $left( T, * right)$
:$(2): quad$ An order isomorphism from the ordered set $left( S, preceq right)$ to the ordered set $left( T, preccurlyeq right)$.


=== Ordered Semigroup Isomorphism ===
Let $left( S, circ, preceq right)$ and $left( T, *, preccurlyeq right)$ be ordered semigroups.


An ordered semigroup isomorphism from $left( S, circ, preceq right)$ to $left( T, *, preccurlyeq right)$ is a mapping $phi: S to T$ that is both:

:$(1): quad$ A semigroup isomorphism from the semigroup $left( S, circ right)$ to the semigroup $left( T, * right)$

:$(2): quad$ An order isomorphism from the ordered set $left( S, preceq right)$ to the ordered set $left( T, preccurlyeq right)$.

=== Ordered Group Isomorphism ===
Let $left({S, circ, preceq}right)$ and $left({T, *, preccurlyeq}right)$ be ordered groups.


An ordered group isomorphism from $left({S, circ, preceq}right)$ to $left({T, *, preccurlyeq}right)$ is a mapping $phi: S to T$ that is both:

:$(1): quad$ A group isomorphism from the group $left({S, circ}right)$ to the group $left({T, *}right)$

:$(2): quad$ An order isomorphism from the ordered set $left({S, preceq}right)$ to the ordered set $left({T, preccurlyeq}right)$.

=== Ordered Ring Isomorphism ===
Let $left( S, +, circ, preceq right)$ and $left( T, oplus, *, preccurlyeq right)$ be ordered rings.


An ordered ring isomorphism from $left( S, +, circ, preceq right)$ to $left( T, oplus, *, preccurlyeq right)$ is a mapping $phi: S to T$ that is both:

:$(1): quad$ An ordered group isomorphism from the ordered group $left( S, +, preceq right)$ to the ordered group $left( T, oplus, preccurlyeq right)$

:$(2): quad$ A semigroup isomorphism from the semigroup $left( S, circ right)$ to the semigroup $left( T, * right)$.

=== Ordered Field Isomorphism ===
Let $left( S, +, circ, preceq right)$ and $left( T, oplus, *, preccurlyeq right)$ be ordered fields.


An ordered field isomorphism from $left( S, +, circ, preceq right)$ to $left( T, oplus, *, preccurlyeq right)$ is a mapping $phi: S to T$ that is both:

:$(1): quad$ An ordered group isomorphism from the ordered group $left( S, +, preceq right)$ to the ordered group $left( T, oplus, preccurlyeq right)$

:$(2): quad$ A group isomorphism from the group $left( S_{ne 0}, circ right)$ to the semigroup $left( T_{ne 0}, * right)$

where $S_{ne 0}$ and $T_{ne 0}$ denote the sets $S$ and $T$ without the zeros of $S$ and $T$ respectively.",Definition:Isomorphism (Abstract Algebra),homomorphism,true,"An isomorphism is a homomorphism which is a bijection.

That is, it is a mapping which is both a monomorphism and an epimorphism.


An algebraic structure ( S, ∘) is isomorphic to another algebraic structure ( T, * )  if and only if  there exists an isomorphism from ( S, ∘) to ( T, * ), and we can write S ≅ T (although notation may vary).


=== Semigroup Isomorphism ===
Let ( S, ∘) and ( T, * ) be semigroups.

Let ϕ: S → T be a (semigroup) homomorphism.


Then ϕ is a semigroup isomorphism  if and only if  ϕ is a bijection.

That is, ϕ is a semigroup isomorphism  if and only if  ϕ is both a monomorphism and an epimorphism.


If S is isomorphic to T, then the notation S ≅ T can be used (although notation varies).

=== Monoid Isomorphism ===
Let ( S, ∘) and ( T, * ) be monoids.

Let ϕ: S → T be a (monoid) homomorphism.


Then ϕ is a monoid isomorphism  if and only if  ϕ is a bijection.


That is, ϕ is a monoid isomorphism  if and only if  ϕ is both a monomorphism and an epimorphism.


If S is isomorphic to T, then the notation S ≅ T can be used (although notation varies).

=== Group Isomorphism ===
Let ( G, ∘) and ( H, * ) be groups.

Let ϕ: G → H be a (group) homomorphism.


Then ϕ is a group isomorphism  if and only if  ϕ is a bijection.


That is, ϕ is a group isomorphism  if and only if  ϕ is both a monomorphism and an epimorphism.


If G is isomorphic to H, then the notation G ≅ H can be used (although notation varies).

=== Ring Isomorphism ===
Let ( R, +, ∘) and ( S, ⊕, * ) be rings.

Let ϕ: R → S be a (ring) homomorphism.


Then ϕ is a ring isomorphism  if and only if  ϕ is a bijection.

That is, ϕ is a ring isomorphism  if and only if  ϕ is both a monomorphism and an epimorphism.

=== F-Isomorphism ===
Let R, S be rings with unity.

Let F be a subfield of both R and S.


Let ϕ: R → S be an F-homomorphism such that ϕ is bijective.


Then ϕ is an F-isomorphism.


The relationship between R and S is denoted R ≅_F S.

=== Field Isomorphism ===
Let ( F, +, ∘) and ( K, ⊕, * ) be fields.

Let ϕ: F → K be a (field) homomorphism.


Then ϕ is a field isomorphism  if and only if  ϕ is a bijection.

That is, ϕ is a field isomorphism  if and only if  ϕ is both a monomorphism and an epimorphism.

=== R-Algebraic Structure Isomorphism ===
Let ( S, ∗_1, ∗_2, …, ∗_n, ∘)_R and ( T, ⊙_1, ⊙_2, …, ⊙_n, ⊗)_R be R-algebraic structures.

Let ϕ: S → T be an R-algebraic structure homomorphism.


Then ϕ is an R-algebraic structure isomorphism  if and only if  ϕ is a bijection.


=== Module Isomorphism ===
Let R be a ring.

Let ( G, +_G, ∘)_R and ( H, +_H, ∘)_R be R-modules.

Let ϕ: G → H be a module homomorphism.


Then ϕ is a module isomorphism  if and only if  ϕ is a bijection.

=== Vector Space Isomorphism ===
Let ( V, +, ∘) and ( W, +', ∘' ) be K-vector spaces.

Then ϕ: V → W is a vector space isomorphism  if and only if :

:(1):   ϕ is a bijection
:(2):   ∀𝐱, 𝐲∈ V: ϕ(   )𝐱 + 𝐲 = ϕ(   )𝐱 +' ϕ(   )𝐲
:(3):   ∀𝐱∈ V: ∀λ∈ K: ϕ(   )λ𝐱 = λϕ(   )𝐱


Category:Definitions/Isomorphisms (Abstract Algebra)
Category:Definitions/R-Algebraic Structure Isomorphisms
Category:Definitions/Linear Algebra

=== Ordered Structure Isomorphism ===
An ordered structure isomorphism from an ordered structure ( S, ∘, ≼) to another ( T, *, ≼) is a mapping ϕ: S → T that is both:

:(1): An isomorphism, that is a bijective homomorphism, from the structure ( S, ∘) to the structure ( T, * )
:(2): An order isomorphism from the ordered set ( S, ≼) to the ordered set ( T, ≼).


=== Ordered Semigroup Isomorphism ===
Let ( S, ∘, ≼) and ( T, *, ≼) be ordered semigroups.


An ordered semigroup isomorphism from ( S, ∘, ≼) to ( T, *, ≼) is a mapping ϕ: S → T that is both:

:(1): A semigroup isomorphism from the semigroup ( S, ∘) to the semigroup ( T, * )

:(2): An order isomorphism from the ordered set ( S, ≼) to the ordered set ( T, ≼).

=== Ordered Group Isomorphism ===
Let (S, ∘, ≼) and (T, *, ≼) be ordered groups.


An ordered group isomorphism from (S, ∘, ≼) to (T, *, ≼) is a mapping ϕ: S → T that is both:

:(1): A group isomorphism from the group (S, ∘) to the group (T, *)

:(2): An order isomorphism from the ordered set (S, ≼) to the ordered set (T, ≼).

=== Ordered Ring Isomorphism ===
Let ( S, +, ∘, ≼) and ( T, ⊕, *, ≼) be ordered rings.


An ordered ring isomorphism from ( S, +, ∘, ≼) to ( T, ⊕, *, ≼) is a mapping ϕ: S → T that is both:

:(1): An ordered group isomorphism from the ordered group ( S, +, ≼) to the ordered group ( T, ⊕, ≼)

:(2): A semigroup isomorphism from the semigroup ( S, ∘) to the semigroup ( T, * ).

=== Ordered Field Isomorphism ===
Let ( S, +, ∘, ≼) and ( T, ⊕, *, ≼) be ordered fields.


An ordered field isomorphism from ( S, +, ∘, ≼) to ( T, ⊕, *, ≼) is a mapping ϕ: S → T that is both:

:(1): An ordered group isomorphism from the ordered group ( S, +, ≼) to the ordered group ( T, ⊕, ≼)

:(2): A group isomorphism from the group ( S_ 0, ∘) to the semigroup ( T_ 0, * )

where S_ 0 and T_ 0 denote the sets S and T without the zeros of S and T respectively.",Isomorphism
['Definitions/Group Homomorphisms'],Definition:Isomorphism,"Let $left( G, circ right)$ and $left( H, * right)$ be groups.

Let $phi: G to H$ be a (group) homomorphism.


Then $phi$ is a group isomorphism  if and only if  $phi$ is a bijection.


That is, $phi$ is a group isomorphism  if and only if  $phi$ is both a monomorphism and an epimorphism.


If $G$ is isomorphic to $H$, then the notation $G cong H$ can be used (although notation varies).",Definition:Isomorphism (Abstract Algebra)/Group Isomorphism,,false,"Let ( G, ∘) and ( H, * ) be groups.

Let ϕ: G → H be a (group) homomorphism.


Then ϕ is a group isomorphism  if and only if  ϕ is a bijection.


That is, ϕ is a group isomorphism  if and only if  ϕ is both a monomorphism and an epimorphism.


If G is isomorphic to H, then the notation G ≅ H can be used (although notation varies).",Isomorphism
"['Definitions/Isomorphisms (Abstract Algebra)', 'Definitions/Ring Homomorphisms']",Definition:Isomorphism,"Let $left( R, +, circ right)$ and $left( S, oplus, * right)$ be rings.

Let $phi: R to S$ be a (ring) homomorphism.


Then $phi$ is a ring isomorphism  if and only if  $phi$ is a bijection.

That is, $phi$ is a ring isomorphism  if and only if  $phi$ is both a monomorphism and an epimorphism.",Definition:Isomorphism (Abstract Algebra)/Ring Isomorphism,,false,"Let ( R, +, ∘) and ( S, ⊕, * ) be rings.

Let ϕ: R → S be a (ring) homomorphism.


Then ϕ is a ring isomorphism  if and only if  ϕ is a bijection.

That is, ϕ is a ring isomorphism  if and only if  ϕ is both a monomorphism and an epimorphism.",Isomorphism
"['Definitions/R-Algebraic Structure Isomorphisms', 'Definitions/R-Algebraic Structure Homomorphisms', 'Definitions/Isomorphisms (Abstract Algebra)', 'Definitions/Linear Algebra']",Definition:Isomorphism,"Let $left( S, ast_1, ast_2, ldots, ast_n, circ right)_R$ and $left( T, odot_1, odot_2, ldots, odot_n, otimes right)_R$ be $R$-algebraic structures.

Let $phi: S to T$ be an $R$-algebraic structure homomorphism.


Then $phi$ is an $R$-algebraic structure isomorphism  if and only if  $phi$ is a bijection.


=== Module Isomorphism ===
Let $R$ be a ring.

Let $left( G, +_G, circ right)_R$ and $left( H, +_H, circ right)_R$ be $R$-modules.

Let $phi: G to H$ be a module homomorphism.


Then $phi$ is a module isomorphism  if and only if  $phi$ is a bijection.

=== Vector Space Isomorphism ===
Let $left( V, +, circ right)$ and $left( W, +', circ' right)$ be $K$-vector spaces.

Then $phi: V to W$ is a vector space isomorphism  if and only if :

:$(1): quad phi$ is a bijection
:$(2): quad forall mathbf x, mathbf y in V: phi left(   right){mathbf x + mathbf y} = phi left(   right){mathbf x} +' phi left(   right){mathbf y}$
:$(3): quad forall mathbf x in V: forall lambda in K: phi left(   right){lambda mathbf x} = lambda phi left(   right){mathbf x}$


Category:Definitions/Isomorphisms (Abstract Algebra)
Category:Definitions/R-Algebraic Structure Isomorphisms
Category:Definitions/Linear Algebra",Definition:Isomorphism (Abstract Algebra)/R-Algebraic Structure Isomorphism,,false,"Let ( S, ∗_1, ∗_2, …, ∗_n, ∘)_R and ( T, ⊙_1, ⊙_2, …, ⊙_n, ⊗)_R be R-algebraic structures.

Let ϕ: S → T be an R-algebraic structure homomorphism.


Then ϕ is an R-algebraic structure isomorphism  if and only if  ϕ is a bijection.


=== Module Isomorphism ===
Let R be a ring.

Let ( G, +_G, ∘)_R and ( H, +_H, ∘)_R be R-modules.

Let ϕ: G → H be a module homomorphism.


Then ϕ is a module isomorphism  if and only if  ϕ is a bijection.

=== Vector Space Isomorphism ===
Let ( V, +, ∘) and ( W, +', ∘' ) be K-vector spaces.

Then ϕ: V → W is a vector space isomorphism  if and only if :

:(1):   ϕ is a bijection
:(2):   ∀𝐱, 𝐲∈ V: ϕ(   )𝐱 + 𝐲 = ϕ(   )𝐱 +' ϕ(   )𝐲
:(3):   ∀𝐱∈ V: ∀λ∈ K: ϕ(   )λ𝐱 = λϕ(   )𝐱


Category:Definitions/Isomorphisms (Abstract Algebra)
Category:Definitions/R-Algebraic Structure Isomorphisms
Category:Definitions/Linear Algebra",Isomorphism
"['Definitions/Relational Structures', 'Definitions/Mapping Theory']",Definition:Isomorphism,"Let $left( S_1, mathcal R_1 right)$ and $left( S_2, mathcal R_2 right)$ be relational structures.

Let there exist a bijection $phi: S_1 to S_2$ such that:
:$(1): quad forall left( s_1, t_1 right) in mathcal R_1: left( phi left(   right){s_1}, phi left(   right){t_1}  right) in mathcal R_2$
:$(2): quad forall left( s_2, t_2 right) in mathcal R_2: left( phi^{-1}  left(   right){s_2}, phi^{-1}  left(   right){t_2}  right) in mathcal R_1$


Then $left( S_1, mathcal R_1 right)$ and $left( S_2, mathcal R_2 right)$ are isomorphic, and this is denoted $S_1 cong S_2$.


The function $phi$ is called a relation isomorphism, or just an isomorphism, from $left( S_1, mathcal R_1 right)$ to $left( S_2, mathcal R_2 right)$.",Definition:Relation Isomorphism,,false,"Let ( S_1, ℛ_1 ) and ( S_2, ℛ_2 ) be relational structures.

Let there exist a bijection ϕ: S_1 → S_2 such that:
:(1):   ∀( s_1, t_1 ) ∈ℛ_1: ( ϕ(   )s_1, ϕ(   )t_1) ∈ℛ_2
:(2):   ∀( s_2, t_2 ) ∈ℛ_2: ( ϕ^-1(   )s_2, ϕ^-1(   )t_2) ∈ℛ_1


Then ( S_1, ℛ_1 ) and ( S_2, ℛ_2 ) are isomorphic, and this is denoted S_1 ≅ S_2.


The function ϕ is called a relation isomorphism, or just an isomorphism, from ( S_1, ℛ_1 ) to ( S_2, ℛ_2 ).",Isomorphism
"['Definitions/Order Theory', 'Definitions/Order Isomorphisms']",Definition:Isomorphism,"Let $left( S, preceq_1 right)$ and $left( T, preceq_2 right)$ be ordered sets.

=== Definition 1 ===
Let $left( S, preceq_1 right)$ and $left( T, preceq_2 right)$ be ordered sets.


Let $phi: S to T$ be a bijection such that:

:$phi: S to T$ is order-preserving
:$phi^{-1}: T to S$ is order-preserving.

Then $phi$ is an order isomorphism.


That is, $phi$ is an order isomorphism  if and only if :

:$phi$ is bijective
:$forall x, y in S: x preceq_1 y implies phi left(   right)x preceq_2 phi left(   right)y$
:$forall p, q in T: p preceq_2 q implies phi^{-1}  left(   right)p preceq_1 phi^{-1}  left(   right)q$

So an order isomorphism can be described as a bijection that preserves ordering in both directions.

=== Definition 2 ===
Let $left( S, preceq_1 right)$ and $left( T, preceq_2 right)$ be ordered sets.


Let $phi: S to T$ be a surjective order embedding.

Then $phi$ is an order isomorphism.


That is, $phi$ is an order isomorphism  if and only if :

:$(1): quad phi$ is surjective
:$(2): quad forall x, y in S: x preceq_1 y iff phi left(   right)x preceq_2 phi left(   right)y$

=== Definition 3 ===
Let $left( S, preceq_1 right)$ and $left( T, preceq_2 right)$ be ordered sets.


Let $phi: S to T$ be a bijection such that:
:$forall x, y in S: x preceq_1 y iff phi left(   right)x preceq_2 phi left(   right)y$

Then $phi$ is an order isomorphism.

=== Well-Orderings ===

When $preceq_1$ and $preceq_2$ are well-orderings, the condition on the order preservation can be relaxed:

Let $left( S, preceq_1 right)$ and $left( T, preceq_2 right)$ be well-ordered sets.

Let $phi: S to T$ be a bijection such that $phi: S to T$ is order-preserving:
:$forall x, y in S: x preceq_1 y implies phi left(   right)x preceq_2 phi left(   right)y$


Then $phi$ is an order isomorphism.


Two well-ordered sets $left( S, preceq_1 right)$ and $left( T, preceq_2 right)$ are (order) isomorphic  if and only if  there exists such an order isomorphism between them.

Thus $left( S, preceq_1 right)$ is described as (order) isomorphic to (or with) $left( T, preceq_2 right)$, and vice versa.

This may be written $left( S, preceq_1 right) cong left( T, preceq_2 right)$.


Where no confusion is possible, it may be abbreviated to $S cong T$.


=== Class-Theoretical Definition ===
 
Let $left( A, preccurlyeq_1 right)$ and $left( B, preccurlyeq_2 right)$ be well-ordered classes.

Let $phi: A to B$ be a bijection such that $phi: A to B$ is order-preserving:
:$forall x, y in S: x preccurlyeq_1 y implies phi left(   right)x preccurlyeq_2 phi left(   right)y$


Then $phi$ is an order isomorphism.",Definition:Order Isomorphism,,false,"Let ( S, ≼_1 ) and ( T, ≼_2 ) be ordered sets.

=== Definition 1 ===
Let ( S, ≼_1 ) and ( T, ≼_2 ) be ordered sets.


Let ϕ: S → T be a bijection such that:

:ϕ: S → T is order-preserving
:ϕ^-1: T → S is order-preserving.

Then ϕ is an order isomorphism.


That is, ϕ is an order isomorphism  if and only if :

:ϕ is bijective
:∀ x, y ∈ S: x ≼_1 y ϕ(   )x ≼_2 ϕ(   )y
:∀ p, q ∈ T: p ≼_2 q ϕ^-1(   )p ≼_1 ϕ^-1(   )q

So an order isomorphism can be described as a bijection that preserves ordering in both directions.

=== Definition 2 ===
Let ( S, ≼_1 ) and ( T, ≼_2 ) be ordered sets.


Let ϕ: S → T be a surjective order embedding.

Then ϕ is an order isomorphism.


That is, ϕ is an order isomorphism  if and only if :

:(1):   ϕ is surjective
:(2):   ∀ x, y ∈ S: x ≼_1 y ϕ(   )x ≼_2 ϕ(   )y

=== Definition 3 ===
Let ( S, ≼_1 ) and ( T, ≼_2 ) be ordered sets.


Let ϕ: S → T be a bijection such that:
:∀ x, y ∈ S: x ≼_1 y ϕ(   )x ≼_2 ϕ(   )y

Then ϕ is an order isomorphism.

=== Well-Orderings ===

When ≼_1 and ≼_2 are well-orderings, the condition on the order preservation can be relaxed:

Let ( S, ≼_1 ) and ( T, ≼_2 ) be well-ordered sets.

Let ϕ: S → T be a bijection such that ϕ: S → T is order-preserving:
:∀ x, y ∈ S: x ≼_1 y ϕ(   )x ≼_2 ϕ(   )y


Then ϕ is an order isomorphism.


Two well-ordered sets ( S, ≼_1 ) and ( T, ≼_2 ) are (order) isomorphic  if and only if  there exists such an order isomorphism between them.

Thus ( S, ≼_1 ) is described as (order) isomorphic to (or with) ( T, ≼_2 ), and vice versa.

This may be written ( S, ≼_1 ) ≅( T, ≼_2 ).


Where no confusion is possible, it may be abbreviated to S ≅ T.


=== Class-Theoretical Definition ===
 
Let ( A, ≼_1 ) and ( B, ≼_2 ) be well-ordered classes.

Let ϕ: A → B be a bijection such that ϕ: A → B is order-preserving:
:∀ x, y ∈ S: x ≼_1 y ϕ(   )x ≼_2 ϕ(   )y


Then ϕ is an order isomorphism.",Isomorphism
"['Definitions/Order Isomorphisms', 'Definitions/Ordered Structures']",Definition:Isomorphism,"An ordered structure isomorphism from an ordered structure $left( S, circ, preceq right)$ to another $left( T, *, preccurlyeq right)$ is a mapping $phi: S to T$ that is both:

:$(1): quad$ An isomorphism, that is a bijective homomorphism, from the structure $left( S, circ right)$ to the structure $left( T, * right)$
:$(2): quad$ An order isomorphism from the ordered set $left( S, preceq right)$ to the ordered set $left( T, preccurlyeq right)$.


=== Ordered Semigroup Isomorphism ===
Let $left( S, circ, preceq right)$ and $left( T, *, preccurlyeq right)$ be ordered semigroups.


An ordered semigroup isomorphism from $left( S, circ, preceq right)$ to $left( T, *, preccurlyeq right)$ is a mapping $phi: S to T$ that is both:

:$(1): quad$ A semigroup isomorphism from the semigroup $left( S, circ right)$ to the semigroup $left( T, * right)$

:$(2): quad$ An order isomorphism from the ordered set $left( S, preceq right)$ to the ordered set $left( T, preccurlyeq right)$.

=== Ordered Group Isomorphism ===
Let $left({S, circ, preceq}right)$ and $left({T, *, preccurlyeq}right)$ be ordered groups.


An ordered group isomorphism from $left({S, circ, preceq}right)$ to $left({T, *, preccurlyeq}right)$ is a mapping $phi: S to T$ that is both:

:$(1): quad$ A group isomorphism from the group $left({S, circ}right)$ to the group $left({T, *}right)$

:$(2): quad$ An order isomorphism from the ordered set $left({S, preceq}right)$ to the ordered set $left({T, preccurlyeq}right)$.

=== Ordered Ring Isomorphism ===
Let $left( S, +, circ, preceq right)$ and $left( T, oplus, *, preccurlyeq right)$ be ordered rings.


An ordered ring isomorphism from $left( S, +, circ, preceq right)$ to $left( T, oplus, *, preccurlyeq right)$ is a mapping $phi: S to T$ that is both:

:$(1): quad$ An ordered group isomorphism from the ordered group $left( S, +, preceq right)$ to the ordered group $left( T, oplus, preccurlyeq right)$

:$(2): quad$ A semigroup isomorphism from the semigroup $left( S, circ right)$ to the semigroup $left( T, * right)$.

=== Ordered Field Isomorphism ===
Let $left( S, +, circ, preceq right)$ and $left( T, oplus, *, preccurlyeq right)$ be ordered fields.


An ordered field isomorphism from $left( S, +, circ, preceq right)$ to $left( T, oplus, *, preccurlyeq right)$ is a mapping $phi: S to T$ that is both:

:$(1): quad$ An ordered group isomorphism from the ordered group $left( S, +, preceq right)$ to the ordered group $left( T, oplus, preccurlyeq right)$

:$(2): quad$ A group isomorphism from the group $left( S_{ne 0}, circ right)$ to the semigroup $left( T_{ne 0}, * right)$

where $S_{ne 0}$ and $T_{ne 0}$ denote the sets $S$ and $T$ without the zeros of $S$ and $T$ respectively.",Definition:Isomorphism (Abstract Algebra)/Ordered Structure Isomorphism,,false,"An ordered structure isomorphism from an ordered structure ( S, ∘, ≼) to another ( T, *, ≼) is a mapping ϕ: S → T that is both:

:(1): An isomorphism, that is a bijective homomorphism, from the structure ( S, ∘) to the structure ( T, * )
:(2): An order isomorphism from the ordered set ( S, ≼) to the ordered set ( T, ≼).


=== Ordered Semigroup Isomorphism ===
Let ( S, ∘, ≼) and ( T, *, ≼) be ordered semigroups.


An ordered semigroup isomorphism from ( S, ∘, ≼) to ( T, *, ≼) is a mapping ϕ: S → T that is both:

:(1): A semigroup isomorphism from the semigroup ( S, ∘) to the semigroup ( T, * )

:(2): An order isomorphism from the ordered set ( S, ≼) to the ordered set ( T, ≼).

=== Ordered Group Isomorphism ===
Let (S, ∘, ≼) and (T, *, ≼) be ordered groups.


An ordered group isomorphism from (S, ∘, ≼) to (T, *, ≼) is a mapping ϕ: S → T that is both:

:(1): A group isomorphism from the group (S, ∘) to the group (T, *)

:(2): An order isomorphism from the ordered set (S, ≼) to the ordered set (T, ≼).

=== Ordered Ring Isomorphism ===
Let ( S, +, ∘, ≼) and ( T, ⊕, *, ≼) be ordered rings.


An ordered ring isomorphism from ( S, +, ∘, ≼) to ( T, ⊕, *, ≼) is a mapping ϕ: S → T that is both:

:(1): An ordered group isomorphism from the ordered group ( S, +, ≼) to the ordered group ( T, ⊕, ≼)

:(2): A semigroup isomorphism from the semigroup ( S, ∘) to the semigroup ( T, * ).

=== Ordered Field Isomorphism ===
Let ( S, +, ∘, ≼) and ( T, ⊕, *, ≼) be ordered fields.


An ordered field isomorphism from ( S, +, ∘, ≼) to ( T, ⊕, *, ≼) is a mapping ϕ: S → T that is both:

:(1): An ordered group isomorphism from the ordered group ( S, +, ≼) to the ordered group ( T, ⊕, ≼)

:(2): A group isomorphism from the group ( S_ 0, ∘) to the semigroup ( T_ 0, * )

where S_ 0 and T_ 0 denote the sets S and T without the zeros of S and T respectively.",Isomorphism
['Definitions/Category Theory'],Definition:Isomorphism,"Let $mathbf C$ be a category, and let $X, Y$ be objects of $mathbf C$.


A morphism $f: X to Y$ is an isomorphism  if and only if  there exists a morphism $g: Y to X$ such that:

:$g circ f = I_X$
:$f circ g = I_Y$

where $I_X$ denotes the identity morphism on $X$.

It can be seen that this is equivalent to $g$ being both a retraction and a section of $f$.


=== Inverse Morphism ===
Let $mathbf C$ be a metacategory.

Let $f: X to Y$ be a morphism of $mathbf C$.


A morphism $g: Y to X$ is said to be an inverse (morphism) for $f$  if and only if :

:$g circ f = I_X$
:$f circ g = I_Y$

where $I_X$ denotes the identity morphism on $X$.


It follows that $f$ is an isomorphism  if and only if  it has an inverse morphism.",Definition:Isomorphism (Category Theory),,false,"Let 𝐂 be a category, and let X, Y be objects of 𝐂.


A morphism f: X → Y is an isomorphism  if and only if  there exists a morphism g: Y → X such that:

:g ∘ f = I_X
:f ∘ g = I_Y

where I_X denotes the identity morphism on X.

It can be seen that this is equivalent to g being both a retraction and a section of f.


=== Inverse Morphism ===
Let 𝐂 be a metacategory.

Let f: X → Y be a morphism of 𝐂.


A morphism g: Y → X is said to be an inverse (morphism) for f  if and only if :

:g ∘ f = I_X
:f ∘ g = I_Y

where I_X denotes the identity morphism on X.


It follows that f is an isomorphism  if and only if  it has an inverse morphism.",Isomorphism
"['Definitions/Category Theory', 'Definitions/Isomorphisms']",Definition:Isomorphism,"Let $mathbf C$ and $mathbf D$ be metacategories.

Let $F: mathbf C to mathbf D$ be a functor.


Then $F$ is an isomorphism (of categories)  if and only if  there exists a functor $G: mathbf C to mathbf D$ such that:

:$G F: mathbf C to mathbf C$ is the identity functor $I_{mathbf C}$
:$F G: mathbf D to mathbf D$ is the identity functor $I_{mathbf D}$


=== Isomorphic Categories ===
Let $mathbf C$ and $mathbf D$ be metacategories.

Let $F: mathbf C to mathbf D$ be an isomorphism of categories.


Then $mathbf C$ and $mathbf D$ are said to be isomorphic, and we write $mathbf C cong mathbf D$.",Definition:Isomorphism of Categories,,false,"Let 𝐂 and 𝐃 be metacategories.

Let F: 𝐂→𝐃 be a functor.


Then F is an isomorphism (of categories)  if and only if  there exists a functor G: 𝐂→𝐃 such that:

:G F: 𝐂→𝐂 is the identity functor I_𝐂
:F G: 𝐃→𝐃 is the identity functor I_𝐃


=== Isomorphic Categories ===
Let 𝐂 and 𝐃 be metacategories.

Let F: 𝐂→𝐃 be an isomorphism of categories.


Then 𝐂 and 𝐃 are said to be isomorphic, and we write 𝐂≅𝐃.",Isomorphism
['Definitions/Graph Theory'],Definition:Isomorphism,"Let $G = left( V left(   right)G, E left(   right)G right)$ and $H = left( V left(   right)H, E left(   right)H right)$ be graphs.

Let there exist a bijection $F: V left(   right)G to V left(   right)H$ such that:
:for each edge $leftlbrace u, v rightrbrace in E left(   right)G$, there exists an edge $leftlbrace F left(   right)u, F left(   right)v rightrbrace in E left(   right)H$.


That is, that:
:$F: V left(   right)G to V left(   right)H$ is a homomorphism, and

:$F^{-1}: V left(   right)H to V left(   right)G$ is a homomorphism.


Then $G$ and $H$ are isomorphic, and this is denoted $G cong H$.

The function $F$ is called an isomorphism from $G$ to $H$.",Definition:Isomorphism (Graph Theory),,false,"Let G = ( V (   )G, E (   )G ) and H = ( V (   )H, E (   )H ) be graphs.

Let there exist a bijection F: V (   )G → V (   )H such that:
:for each edge { u, v }∈ E (   )G, there exists an edge { F (   )u, F (   )v }∈ E (   )H.


That is, that:
:F: V (   )G → V (   )H is a homomorphism, and

:F^-1: V (   )H → V (   )G is a homomorphism.


Then G and H are isomorphic, and this is denoted G ≅ H.

The function F is called an isomorphism from G to H.",Isomorphism
['Definitions/Hilbert Spaces'],Definition:Isomorphism,"Let $H, K$ be Hilbert spaces.

Denote by $leftlangle cdot,   rightranglecdot_H$ and $leftlangle cdot,   rightranglecdot_K$ their respective inner products.

An isomorphism between $H$ and $K$ is a map $U: H to K$, such that:

:$(1): quad U$ is a linear map
:$(2): quad U$ is surjective
:$(3): quad forall g, h in H: leftlangle g,   rightrangle h_H = leftlangle U g,   rightrangle{U h}_K$

These three requirements may be summarized by stating that $U$ be a surjective isometry.

Furthermore, Surjection that Preserves Inner Product is Linear shows that requirement $(1)$ is superfluous.


If such an isomorphism $U$ exists, $H$ and $K$ are said to be isomorphic.


As the name isomorphism suggests, Hilbert Space Isomorphism is Equivalence Relation.",Definition:Isomorphism (Hilbert Spaces),,false,"Let H, K be Hilbert spaces.

Denote by ⟨·,   ⟩·_H and ⟨·,   ⟩·_K their respective inner products.

An isomorphism between H and K is a map U: H → K, such that:

:(1):    U is a linear map
:(2):    U is surjective
:(3):   ∀ g, h ∈ H: ⟨ g,   ⟩ h_H = ⟨ U g,   ⟩U h_K

These three requirements may be summarized by stating that U be a surjective isometry.

Furthermore, Surjection that Preserves Inner Product is Linear shows that requirement (1) is superfluous.


If such an isomorphism U exists, H and K are said to be isomorphic.


As the name isomorphism suggests, Hilbert Space Isomorphism is Equivalence Relation.",Isomorphism
"['Definitions/Homeomorphisms (Topological Spaces)', 'Definitions/Homeomorphisms', 'Definitions/Bijections', 'Definitions/Continuous Mappings', 'Definitions/Topology']",Definition:Isomorphism,"Let $T_alpha = left( S_alpha, tau_alpha right)$ and $T_beta = left( S_beta, tau_beta right)$ be topological spaces.

Let $f: T_alpha to T_beta$ be a bijection.


=== Definition 1 ===
Let $T_alpha = left( S_alpha, tau_alpha right)$ and $T_beta = left( S_beta, tau_beta right)$ be topological spaces.

Let $f: T_alpha to T_beta$ be a bijection.


$f$ is a homeomorphism  if and only if  both $f$ and $f^{-1}$ are continuous.

=== Definition 2 ===
Let $T_alpha = left( S_alpha, tau_alpha right)$ and $T_beta = left( S_beta, tau_beta right)$ be topological spaces.

Let $f: T_alpha to T_beta$ be a bijection.


$f$ is a homeomorphism  if and only if :
:$forall U subseteq S_alpha: U in tau_alpha iff f left[ U right] in tau_beta$


That is, $f$ is a homeomorphism  if and only if :
:for all subsets $U$ of $S_alpha$, $U$ is open in $T_alpha$  if and only if  $f left[ U right]$ is open in $T_beta$.

=== Definition 3 ===
Let $T_alpha = left( S_alpha, tau_alpha right)$ and $T_beta = left( S_beta, tau_beta right)$ be topological spaces.

Let $f: T_alpha to T_beta$ be a bijection.


$f$ is a homeomorphism  if and only if  $f$ is both an open mapping and a continuous mapping.

=== Definition 4 ===
Let $T_alpha = left( S_alpha, tau_alpha right)$ and $T_beta = left( S_beta, tau_beta right)$ be topological spaces.

Let $f: T_alpha to T_beta$ be a bijection.


$f$ is a homeomorphism  if and only if  $f$ is both a closed mapping and a continuous mapping.",Definition:Homeomorphism/Topological Spaces,,false,"Let T_α = ( S_α, τ_α) and T_β = ( S_β, τ_β) be topological spaces.

Let f: T_α→ T_β be a bijection.


=== Definition 1 ===
Let T_α = ( S_α, τ_α) and T_β = ( S_β, τ_β) be topological spaces.

Let f: T_α→ T_β be a bijection.


f is a homeomorphism  if and only if  both f and f^-1 are continuous.

=== Definition 2 ===
Let T_α = ( S_α, τ_α) and T_β = ( S_β, τ_β) be topological spaces.

Let f: T_α→ T_β be a bijection.


f is a homeomorphism  if and only if :
:∀ U ⊆ S_α: U ∈τ_α f [ U ] ∈τ_β


That is, f is a homeomorphism  if and only if :
:for all subsets U of S_α, U is open in T_α  if and only if  f [ U ] is open in T_β.

=== Definition 3 ===
Let T_α = ( S_α, τ_α) and T_β = ( S_β, τ_β) be topological spaces.

Let f: T_α→ T_β be a bijection.


f is a homeomorphism  if and only if  f is both an open mapping and a continuous mapping.

=== Definition 4 ===
Let T_α = ( S_α, τ_α) and T_β = ( S_β, τ_β) be topological spaces.

Let f: T_α→ T_β be a bijection.


f is a homeomorphism  if and only if  f is both a closed mapping and a continuous mapping.",Isomorphism
['Definitions/Physics'],Definition:Isotropy,"Isotropy is a property of a physical system such that measurements of that system are independent of the direction in which those measurements are taken.

 ",Definition:Isotropy (Physics),property,true,"Isotropy is a property of a physical system such that measurements of that system are independent of the direction in which those measurements are taken.

 ",Isotropy
['Definitions/Riemannian Geometry'],Definition:Isotropy,"Let $left( M, g right)$ be a Riemannian manifold.

Let $text {Iso} left(   right){M, g}$ be the set of all isometries from $M$ to itself.

Let $text {Iso}_p left(   right){M, g}$ be the isotropy subgroup at $p in M$.

That is, let $text {Iso}_p left(   right){M, g}$ be the subgroup of $text {Iso} left(   right){M, g}$ consisting of isometries that fix $p in M$.

Let $T_p M$ be the tangent space of $M$ at $p in M$.

For each $phi in text {Iso}_p left(   right){M, g}$ let $,mathrm d phi_p$ be a linear mapping such that:

:$,mathrm d phi_p : T_p M to T_p M$

Let $mathrm {GL} left( T_p M right)$ be the general linear group over $T_p M$.

Let $I_p : text {Iso}_p left(   right){M, g} to mathrm {GL} left( T_p M right)$ be a mapping such that $I_p left(   right)phi = ,mathrm d phi_p$.


Then the mapping $I_p$ is a representation of $text {Iso}_p left(   right){M, g}$ and is called the isotropy representation.",Definition:Isotropy Representation,,false,"Let ( M, g ) be a Riemannian manifold.

Let Iso(   )M, g be the set of all isometries from M to itself.

Let Iso_p (   )M, g be the isotropy subgroup at p ∈ M.

That is, let Iso_p (   )M, g be the subgroup of Iso(   )M, g consisting of isometries that fix p ∈ M.

Let T_p M be the tangent space of M at p ∈ M.

For each ϕ∈Iso_p (   )M, g let dϕ_p be a linear mapping such that:

:dϕ_p : T_p M → T_p M

Let GL( T_p M ) be the general linear group over T_p M.

Let I_p : Iso_p (   )M, g→GL( T_p M ) be a mapping such that I_p (   )ϕ =  dϕ_p.


Then the mapping I_p is a representation of Iso_p (   )M, g and is called the isotropy representation.",Isotropy
['Definitions/Group Theory'],Definition:Join,"Let $left( G, circ right)$ be a group.

Let $A$ and $B$ be subgroups of $G$.


The join of $A$ and $B$ is written and defined as:
:$A vee B := {leftlangle A cup B rightrangle}$
where ${leftlangle A cup B rightrangle}$ is the subgroup generated by $A cup B$.


By the definition of subgroup generator, this can alternatively be written:

:$ds A vee B := bigcap leftlbrace T: T text { is a subgroup of } G: A cup B subseteq T rightrbrace$


=== General Definition ===
Let $left( G, circ right)$ be a group.

Let $H_1, H_2, ldots, H_n$ be subgroups of $G$.


Then the join of $H_1, H_2, ldots, H_n$ is defined as:
:$ds bigvee_{k mathop = 1}^n H_k := {leftlangle bigcup_{k mathop = 1}^n H_k rightrangle}$
or:
:$ds bigvee_{k mathop = 1}^n H_k := bigcap leftlbrace T: T text { is a subgroup of } G: bigcup_{k mathop = 1}^n H_k subseteq T rightrbrace$",Definition:Join of Subgroups,,false,"Let ( G, ∘) be a group.

Let A and B be subgroups of G.


The join of A and B is written and defined as:
:A ∨ B := ⟨ A ∪ B ⟩
where ⟨ A ∪ B ⟩ is the subgroup generated by A ∪ B.


By the definition of subgroup generator, this can alternatively be written:

:A ∨ B := ⋂{ T: T  is a subgroup of  G: A ∪ B ⊆ T }


=== General Definition ===
Let ( G, ∘) be a group.

Let H_1, H_2, …, H_n be subgroups of G.


Then the join of H_1, H_2, …, H_n is defined as:
:⋁_k  = 1^n H_k := ⟨⋃_k  = 1^n H_k ⟩
or:
:⋁_k  = 1^n H_k := ⋂{ T: T  is a subgroup of  G: ⋃_k  = 1^n H_k ⊆ T }",Join
['Definitions/Boolean Algebras'],Definition:Join,"Consider the Boolean algebra $left( S, vee, wedge, neg right)$


The operation $vee$ is called join.",Definition:Boolean Algebra/Join,,false,"Consider the Boolean algebra ( S, ∨, ∧, )


The operation ∨ is called join.",Join
"['Definitions/Order Theory', 'Definitions/Lattice Theory']",Definition:Join,"Let $left( S, preceq right)$ be an ordered set.

Let $a, b in S$.

Let their supremum $sup leftlbrace a, b rightrbrace$ exist in $S$.


Then the join of $a$ and $b$ is defined as:

:$a vee b = sup leftlbrace a, b rightrbrace$


Expanding the definition of supremum, one sees that $c = a vee b$  if and only if :

:$(1): quad a preceq c$ and $b preceq c$
:$(2): quad forall s in S: a preceq s$ and $b preceq s implies c preceq s$",Definition:Join (Order Theory),,false,"Let ( S, ≼) be an ordered set.

Let a, b ∈ S.

Let their supremum sup{ a, b } exist in S.


Then the join of a and b is defined as:

:a ∨ b = sup{ a, b }


Expanding the definition of supremum, one sees that c = a ∨ b  if and only if :

:(1):    a ≼ c and b ≼ c
:(2):   ∀ s ∈ S: a ≼ s and b ≼ s  c ≼ s",Join
"['Definitions/Edges of Graphs', 'Definitions/Vertices of Graphs']",Definition:Join,"Let $G = left( V, E right)$ be a graph.

Let $u$ and $v$ be vertices of $G$.

Let $e = u v$ be an edge of $G$.


Then $e$ joins the vertices $u$ and $v$.",Definition:Graph (Graph Theory)/Edge/Join,,false,"Let G = ( V, E ) be a graph.

Let u and v be vertices of G.

Let e = u v be an edge of G.


Then e joins the vertices u and v.",Join
"['Definitions/Kernels (Abstract Algebra)', 'Definitions/Abstract Algebra']",Definition:Kernel,"=== Kernel of Magma Homomorphism ===
Let $left( S, circ right)$ be a magma.

Let $left( T, * right)$ be an algebraic structure with an identity element $e$.

Let $phi: left( S, circ right) to left( T, * right)$ be a homomorphism.


The kernel of $phi$ is the subset of the domain of $phi$ defined as:
:$ker left(   right)phi = leftlbrace x in S: phi left(   right)x = e rightrbrace$


That is, $ker left(   right)phi$ is the subset of $S$ that maps to the identity of $T$.

=== Kernel of Group Homomorphism ===
Let $left( G, circ right)$ and $left( H, * right)$ be groups.

Let $phi: left( G, circ right) to left( H, * right)$ be a group homomorphism.


The kernel of $phi$ is the subset of the domain of $phi$ defined as:
:$ker left(   right)phi := phi^{-1} left[ e_H right] = leftlbrace x in G: phi left(   right)x = e_H rightrbrace$
where $e_H$ is the identity of $H$.


That is, $ker left(   right)phi$ is the subset of $G$ that maps to the identity of $H$.

=== Kernel of Ring Homomorphism ===
Let $left( R_1, +_1, circ_1 right)$ and $left( R_2, +_2, circ_2 right)$ be rings.

Let $phi: left( R_1, +_1, circ_1 right) to left( R_2, +_2, circ_2 right)$ be a ring homomorphism.


The kernel of $phi$ is the subset of the domain of $phi$ defined as:
:$ker left(   right)phi = leftlbrace x in R_1: phi left(   right)x = 0_{R_2}  rightrbrace$
where $0_{R_2}$ is the zero of $R_2$.


That is, $ker left(   right)phi$ is the subset of $R_1$ that maps to the zero of $R_2$.


From Ring Homomorphism Preserves Zero it follows that $0_{R_1} in ker left(   right)phi$ where $0_{R_1}$ is the zero of $R_1$.

=== Kernel of Linear Transformation ===
Let $phi: G to H$ be a linear transformation where $G$ and $H$ are $R$-modules.

Let $e_H$ be the identity of $H$.


The kernel of $phi$ is defined as:

:$ker left(   right)phi := phi^{-1} left[ leftlbrace e_H rightrbrace  right]$

where $phi^{-1} left[ S right]$ denotes the preimage of $S$ under $phi$.


=== In Vector Space ===
Let $left( mathbf V, +, times right)$ be a vector space.

Let $left( mathbf V', +, times right)$ be a vector space whose zero vector is $mathbf 0'$.

Let $T: mathbf V to mathbf V'$ be a linear transformation.


Then the kernel of $T$ is defined as:

:$ker left(   right)T := T^{-1} left[ leftlbrace mathbf 0' rightrbrace  right] = leftlbrace mathbf x in mathbf V: T left(   right){mathbf x} = mathbf 0' rightrbrace$",Definition:Kernel (Abstract Algebra),,false,"=== Kernel of Magma Homomorphism ===
Let ( S, ∘) be a magma.

Let ( T, * ) be an algebraic structure with an identity element e.

Let ϕ: ( S, ∘) →( T, * ) be a homomorphism.


The kernel of ϕ is the subset of the domain of ϕ defined as:
:(   )ϕ = { x ∈ S: ϕ(   )x = e }


That is, (   )ϕ is the subset of S that maps to the identity of T.

=== Kernel of Group Homomorphism ===
Let ( G, ∘) and ( H, * ) be groups.

Let ϕ: ( G, ∘) →( H, * ) be a group homomorphism.


The kernel of ϕ is the subset of the domain of ϕ defined as:
:(   )ϕ := ϕ^-1[ e_H ] = { x ∈ G: ϕ(   )x = e_H }
where e_H is the identity of H.


That is, (   )ϕ is the subset of G that maps to the identity of H.

=== Kernel of Ring Homomorphism ===
Let ( R_1, +_1, ∘_1 ) and ( R_2, +_2, ∘_2 ) be rings.

Let ϕ: ( R_1, +_1, ∘_1 ) →( R_2, +_2, ∘_2 ) be a ring homomorphism.


The kernel of ϕ is the subset of the domain of ϕ defined as:
:(   )ϕ = { x ∈ R_1: ϕ(   )x = 0_R_2}
where 0_R_2 is the zero of R_2.


That is, (   )ϕ is the subset of R_1 that maps to the zero of R_2.


From Ring Homomorphism Preserves Zero it follows that 0_R_1∈(   )ϕ where 0_R_1 is the zero of R_1.

=== Kernel of Linear Transformation ===
Let ϕ: G → H be a linear transformation where G and H are R-modules.

Let e_H be the identity of H.


The kernel of ϕ is defined as:

:(   )ϕ := ϕ^-1[ { e_H }]

where ϕ^-1[ S ] denotes the preimage of S under ϕ.


=== In Vector Space ===
Let ( 𝐕, +, ×) be a vector space.

Let ( 𝐕', +, ×) be a vector space whose zero vector is 0'.

Let T: 𝐕→𝐕' be a linear transformation.


Then the kernel of T is defined as:

:(   )T := T^-1[ {0' }] = {𝐱∈𝐕: T (   )𝐱 = 0' }",Kernel
['Definitions/Group Actions'],Definition:Kernel,"Let $G$ be a group with identity $e$.

Let $X$ be a set.

Let $* : Gtimes Xto X$ be a group action.


=== Definition 1 ===
Let $G$ be a group with identity $e$.

Let $X$ be a set.

Let $* : Gtimes Xto X$ be a group action.


The kernel of the group action is the set:
:$G_0 = leftlbrace g in G: forall x in X: g * x = x rightrbrace$

=== Definition 2 ===
Let $G$ be a group with identity $e$.

Let $X$ be a set.

Let $* : G times X to X$ be a group action.


The kernel of the group action is the kernel of its permutation representation.",Definition:Kernel of Group Action,,false,"Let G be a group with identity e.

Let X be a set.

Let * : G× X→ X be a group action.


=== Definition 1 ===
Let G be a group with identity e.

Let X be a set.

Let * : G× X→ X be a group action.


The kernel of the group action is the set:
:G_0 = { g ∈ G: ∀ x ∈ X: g * x = x }

=== Definition 2 ===
Let G be a group with identity e.

Let X be a set.

Let * : G × X → X be a group action.


The kernel of the group action is the kernel of its permutation representation.",Kernel
"['Definitions/Kernels of Linear Transformations', 'Definitions/Linear Algebra', 'Definitions/Kernels (Abstract Algebra)']",Definition:Kernel,"Let $phi: G to H$ be a linear transformation where $G$ and $H$ are $R$-modules.

Let $e_H$ be the identity of $H$.


The kernel of $phi$ is defined as:

:$ker left(   right)phi := phi^{-1} left[ leftlbrace e_H rightrbrace  right]$

where $phi^{-1} left[ S right]$ denotes the preimage of $S$ under $phi$.


=== In Vector Space ===
Let $left( mathbf V, +, times right)$ be a vector space.

Let $left( mathbf V', +, times right)$ be a vector space whose zero vector is $mathbf 0'$.

Let $T: mathbf V to mathbf V'$ be a linear transformation.


Then the kernel of $T$ is defined as:

:$ker left(   right)T := T^{-1} left[ leftlbrace mathbf 0' rightrbrace  right] = leftlbrace mathbf x in mathbf V: T left(   right){mathbf x} = mathbf 0' rightrbrace$",Definition:Kernel of Linear Transformation,,false,"Let ϕ: G → H be a linear transformation where G and H are R-modules.

Let e_H be the identity of H.


The kernel of ϕ is defined as:

:(   )ϕ := ϕ^-1[ { e_H }]

where ϕ^-1[ S ] denotes the preimage of S under ϕ.


=== In Vector Space ===
Let ( 𝐕, +, ×) be a vector space.

Let ( 𝐕', +, ×) be a vector space whose zero vector is 0'.

Let T: 𝐕→𝐕' be a linear transformation.


Then the kernel of T is defined as:

:(   )T := T^-1[ {0' }] = {𝐱∈𝐕: T (   )𝐱 = 0' }",Kernel
['Definitions/Homological Algebra'],Definition:Kernel,"Let $left( R, +, cdot right)$ be a ring.

Let:
:$M: quad cdots longrightarrow M_i stackrel {d_i} longrightarrow M_{i + 1} stackrel {d_{i + 1} } longrightarrow M_{i + 2} stackrel {d_{i + 2} } longrightarrow cdots$
and
:$N: quad cdots longrightarrow N_i stackrel {d'_i} longrightarrow N_{i + 1} stackrel {d'_{i + 1} } longrightarrow N_{i + 2} stackrel {d'_{i + 2} } longrightarrow cdots$
be two differential complexes of $R$-modules.

Let $phi = leftlbrace phi_i : i in mathbb Z rightrbrace$ be a homomorphism $M to N$.

For each $i in mathbb Z$ let $K_i$ be the kernel of $phi_i$.

For each $i in mathbb Z$ let $f_i$ be the restriction of $d_i$ to $K_i$.


Then the kernel of $phi$ is:

:$ker phi : quad cdots longrightarrow K_i stackrel {f_i} longrightarrow K_{i + 1} stackrel {f_{i + 1} } longrightarrow K_{i + 2} stackrel {f_{i + 2} } longrightarrow cdots$",Definition:Kernel of Homomorphism of Differential Complexes,,false,"Let ( R, +, ·) be a ring.

Let:
:M:   ⋯⟶ M_i d_i⟶ M_i + 1d_i + 1⟶ M_i + 2d_i + 2⟶⋯
and
:N:   ⋯⟶ N_i d'_i⟶ N_i + 1d'_i + 1⟶ N_i + 2d'_i + 2⟶⋯
be two differential complexes of R-modules.

Let ϕ = {ϕ_i : i ∈ℤ} be a homomorphism M → N.

For each i ∈ℤ let K_i be the kernel of ϕ_i.

For each i ∈ℤ let f_i be the restriction of d_i to K_i.


Then the kernel of ϕ is:

:ϕ :   ⋯⟶ K_i f_i⟶ K_i + 1f_i + 1⟶ K_i + 2f_i + 2⟶⋯",Kernel
"['Definitions/Integral Transforms', 'Definitions/Kernels']",Definition:Kernel,"Let $F left(   right)p$ be an integral transform:

:$F left(   right)p = ds int_a^b f left(   right)x K left(   right){p, x} ,mathrm d x$


The function $K left(   right){p, x}$ is the kernel of $F left(   right)p$.",Definition:Integral Transform/Kernel,,false,"Let F (   )p be an integral transform:

:F (   )p = ∫_a^b f (   )x K (   )p, x d x


The function K (   )p, x is the kernel of F (   )p.",Kernel
"['Definitions/Integral Equations', 'Definitions/Kernels']",Definition:Kernel,"Consider the integral equation:

:of the first kind:
::$f left(   right)x = lambda ds int_{a left(   right)x}^{b left(   right)x} K left(   right){x, y} g left(   right)y ,mathrm d x$

:of the second kind:
::$g left(   right)x = f left(   right)x + lambda ds int_{a left(   right)x}^{b left(   right)x} K left(   right){x, y} g left(   right)y ,mathrm d x$

:of the third kind:
::$u left(   right)x g left(   right)x = f left(   right)x + lambda ds int_{a left(   right)x}^{b left(   right)x} K left(   right){x, y} g left(   right)y ,mathrm d x$


The function $K left(   right){x, y}$ is known as the kernel of the integral equation.",Definition:Integral Equation/Kernel,,false,"Consider the integral equation:

:of the first kind:
::f (   )x = λ∫_a (   )x^b (   )x K (   )x, y g (   )y  d x

:of the second kind:
::g (   )x = f (   )x + λ∫_a (   )x^b (   )x K (   )x, y g (   )y  d x

:of the third kind:
::u (   )x g (   )x = f (   )x + λ∫_a (   )x^b (   )x K (   )x, y g (   )y  d x


The function K (   )x, y is known as the kernel of the integral equation.",Kernel
['Definitions/Measure Theory'],Definition:Kernel,"Let $left( X, unicode{x3a3} right)$ be a measurable space.

Let $overline mathbb R_{ge 0}$ be the set of positive extended real numbers.


A kernel is a mapping $N: X times unicode{x3a3} to overline{mathbb R}_{ge0}$ such that:

:$(1): quad forall x in X: N_x: unicode{x3a3} to overline mathbb R_{ge 0}, E mapsto N left(   right){x, E}$ is a measure
:$(2): quad forall E in unicode{x3a3}: N_E: X to overline mathbb R_{ge 0}, x mapsto N left(   right){x, E}$ is a positive $unicode{x3a3}$-measurable function

 ",Definition:Kernel (Measure Theory),,false,"Let ( X, x3a3) be a measurable space.

Let R_≥ 0 be the set of positive extended real numbers.


A kernel is a mapping N: X ×x3a3→ℝ_≥0 such that:

:(1):   ∀ x ∈ X: N_x: x3a3→R_≥ 0, E ↦ N (   )x, E is a measure
:(2):   ∀ E ∈x3a3: N_E: X →R_≥ 0, x ↦ N (   )x, E is a positive x3a3-measurable function

 ",Kernel
['Definitions/Measure Theory'],Definition:Kernel,"Let $left( X, unicode{x3a3}, mu right)$ be a measure space.

Let $N: X times unicode{x3a3} to overline {mathbb R_{ge 0} }$ be a kernel.


The transformation of $mu$ by $N$ is the mapping $mu N: unicode{x3a3} to overline mathbb R$ defined by:

:$ds forall E in unicode{x3a3}: mu N left(   right)E := int N_E left(   right)x ,mathrm d mu left(   right)x$

where $N_E left(   right)x = N left(   right){x, E}$.",Definition:Kernel Transformation of Measure,,false,"Let ( X, x3a3, μ) be a measure space.

Let N: X ×x3a3→ℝ_≥ 0 be a kernel.


The transformation of μ by N is the mapping μ N: x3a3→R defined by:

:∀ E ∈x3a3: μ N (   )E := ∫ N_E (   )x  dμ(   )x

where N_E (   )x = N (   )x, E.",Kernel
['Definitions/Measure Theory'],Definition:Kernel,"Let $left( X, unicode{x3a3}, mu right)$ be a measure space.

Let $N: X times unicode{x3a3} to overline mathbb R_{ge 0}$ be a kernel.

Let $f: X to overline mathbb R$ be a positive measurable function.


The transformation of $f$ by $N$ is the mapping $N f: X to overline mathbb R$ defined by:

:$forall x in X: N f left(   right)x := ds int f ,mathrm d N_x$

where $N_x$ is the measure $E mapsto N left(   right){x, E}$.",Definition:Kernel Transformation of Positive Measurable Function,,false,"Let ( X, x3a3, μ) be a measure space.

Let N: X ×x3a3→R_≥ 0 be a kernel.

Let f: X →R be a positive measurable function.


The transformation of f by N is the mapping N f: X →R defined by:

:∀ x ∈ X: N f (   )x := ∫ f  d N_x

where N_x is the measure E ↦ N (   )x, E.",Kernel
['Definitions/Category Theory'],Definition:Kernel,"Let $mathbf C$ be a category.

Let $A$ and $B$ be objects of $mathbf C$.

Let $f : A to B $ be a morphism in $mathbf C$.


=== Definition 1: for categories with initial objects ===
Let $mathbf C$ be a category.

Let $A$ and $B$ be objects of $mathbf C$.

Let $f : A to B $ be a morphism in $mathbf C$.

Let $mathbf C$ have an initial object $0$.


A kernel of $f$ is a morphism $ker left(   right)f to A$ which is a pullback of the unique morphism $0 to B$ via $f$ to $A$.



=== Uniqueness ===


=== Definition 2: for categories with zero objects ===
Let $mathbf C$ be a category.

Let $A$ and $B$ be objects of $mathbf C$.

Let $f: A to B $ be a morphism in $mathbf C$.

Let $mathbf C$ have a zero object $0$.


A kernel of $f$ is a morphism $ker(f) to A$, which is an equalizer of $f$ and the zero morphism $0: A to B$.


=== Uniqueness ===


=== Uniqueness ===
",Definition:Kernel (Category Theory),,false,"Let 𝐂 be a category.

Let A and B be objects of 𝐂.

Let f : A → B be a morphism in 𝐂.


=== Definition 1: for categories with initial objects ===
Let 𝐂 be a category.

Let A and B be objects of 𝐂.

Let f : A → B be a morphism in 𝐂.

Let 𝐂 have an initial object 0.


A kernel of f is a morphism (   )f → A which is a pullback of the unique morphism 0 → B via f to A.



=== Uniqueness ===


=== Definition 2: for categories with zero objects ===
Let 𝐂 be a category.

Let A and B be objects of 𝐂.

Let f: A → B be a morphism in 𝐂.

Let 𝐂 have a zero object 0.


A kernel of f is a morphism (f) → A, which is an equalizer of f and the zero morphism 0: A → B.


=== Uniqueness ===


=== Uniqueness ===
",Kernel
"['Definitions/Knot Theory', 'Definitions/Topology', 'Definitions/Geometry', 'Definitions/Branches of Mathematics']",Definition:Knot,Knot theory is the branch of geometry which studies the embedding of knots in $3$-dimensional space.,Definition:Knot Theory,geometry,true,Knot theory is the branch of geometry which studies the embedding of knots in 3-dimensional space.,Knot
['Definitions/Knot Theory'],Definition:Knot,"Let $Y$ be a manifold and $X subset Y$ a submanifold of $Y$.

Let $i: X to Y$ be an inclusion, that is a mapping such that $i left[ X right] = X$.


Then a knotted embedding is an embedding $phi: X to Y$ (or the image of such an embedding) such that $phi left[ X right]$ is not freely homotopic to $i left[ X right]$.


=== Sphere Knot ===
A knotted $n$-sphere is a knotted embedding:
: $phi: Bbb S^n to mathbb R^{n + 2}$


Category:Definitions/Knot Theory

=== Circle Knot ===
The description of the sphere is dropped for $Bbb S^1$ and the term knot is used without qualification for knotted embeddings $phi: Bbb S^1 to mathbb R^3$.

 


Category:Definitions/Knot Theory

=== Elementary Knot ===
Circle knots can often be quite wild and unwieldy - most of modern knot theory concerns itself with a specific kind of knot.

These knots are described as a finite set of points in $mathbb R^3$ called $leftlbrace x_1, x_2, dots, x_n rightrbrace$, together with line segments from $x_i$ to $x_{i + 1}$ and a line segment from $x_n$ to $x_1$.

The union of all these line segments is clearly a circle knot, or an unknot, an embedding of the circle which is homotopic to a circle.


 

Category:Definitions/Knot Theory",Definition:Knot (Knot Theory),,false,"Let Y be a manifold and X ⊂ Y a submanifold of Y.

Let i: X → Y be an inclusion, that is a mapping such that i [ X ] = X.


Then a knotted embedding is an embedding ϕ: X → Y (or the image of such an embedding) such that ϕ[ X ] is not freely homotopic to i [ X ].


=== Sphere Knot ===
A knotted n-sphere is a knotted embedding:
: ϕ:  S^n →ℝ^n + 2


Category:Definitions/Knot Theory

=== Circle Knot ===
The description of the sphere is dropped for S^1 and the term knot is used without qualification for knotted embeddings ϕ:  S^1 →ℝ^3.

 


Category:Definitions/Knot Theory

=== Elementary Knot ===
Circle knots can often be quite wild and unwieldy - most of modern knot theory concerns itself with a specific kind of knot.

These knots are described as a finite set of points in ℝ^3 called { x_1, x_2, …, x_n }, together with line segments from x_i to x_i + 1 and a line segment from x_n to x_1.

The union of all these line segments is clearly a circle knot, or an unknot, an embedding of the circle which is homotopic to a circle.


 

Category:Definitions/Knot Theory",Knot
"['Definitions/Knot (Unit of Measurement)', 'Definitions/Velocity', 'Definitions/Units of Measurement']",Definition:Knot,"The knot is a unit of speed which is used for air and sea navigation.

It is defined as $1$ nautical mile per hour.

It is now defined as exactly $1 , 852$ metres per hour.


=== Conversion Factors ===
",Definition:Knot (Unit of Measurement),unit of speed,true,"The knot is a unit of speed which is used for air and sea navigation.

It is defined as 1 nautical mile per hour.

It is now defined as exactly 1   852 metres per hour.


=== Conversion Factors ===
",Knot
"['Definitions/Knots of Splines', 'Definitions/Splines']",Definition:Knot,"Let $left[ a ,.,.,   right]b$ be a closed real interval.

Let $T := leftlbrace a = t_0, t_1, t_2, ldots, t_{n - 1}, t_n = b rightrbrace$ form a subdivision of $left[ a ,.,.,   right]b$.

Let $S: left[ a ,.,.,   right]b to mathbb R$ be a spline function on $left[ a ,.,.,   right]b$ on $T$.


The points $T := leftlbrace t_0, t_1, t_2, ldots, t_{n - 1}, t_n rightrbrace$ of $S$ are known as the knots.


=== Knot Vector ===
Let $left[ a ,.,.,   right]b$ be a closed real interval.

Let $T := leftlbrace a = t_0, t_1, t_2, ldots, t_{n - 1}, t_n = b rightrbrace$ form a subdivision of $left[ a ,.,.,   right]b$.

Let $S: left[ a ,.,.,   right]b to mathbb R$ be a spline function on $left[ a ,.,.,   right]b$ on $T$.


The ordered $n + 1$-tuple $mathbf t := left( t_0, t_1, t_2, ldots, t_{n - 1}, t_n right)$ of $S$ is known as the knot vector.


Category:Definitions/Knots of Splines",Definition:Spline Function/Knot,,false,"Let [ a  . . ]b be a closed real interval.

Let T := { a = t_0, t_1, t_2, …, t_n - 1, t_n = b } form a subdivision of [ a  . . ]b.

Let S: [ a  . . ]b →ℝ be a spline function on [ a  . . ]b on T.


The points T := { t_0, t_1, t_2, …, t_n - 1, t_n } of S are known as the knots.


=== Knot Vector ===
Let [ a  . . ]b be a closed real interval.

Let T := { a = t_0, t_1, t_2, …, t_n - 1, t_n = b } form a subdivision of [ a  . . ]b.

Let S: [ a  . . ]b →ℝ be a spline function on [ a  . . ]b on T.


The ordered n + 1-tuple 𝐭 := ( t_0, t_1, t_2, …, t_n - 1, t_n ) of S is known as the knot vector.


Category:Definitions/Knots of Splines",Knot
"['Definitions/Lattices (Order Theory)', 'Definitions/Lattice Theory', 'Definitions/Order Theory', 'Definitions/Ordered Structures']",Definition:Lattice,"=== Definition 1 ===
Let $left( S, preceq right)$ be an ordered set.

Suppose that $S$ admits all finite non-empty suprema and finite non-empty infima.

Denote with $vee$ and $wedge$ the join and meet operations on $S$, respectively.


Then the ordered structure $left( S, vee, wedge, preceq right)$ is called a lattice.

=== Definition 2 ===
Let $left( S, vee, wedge, preceq right)$ be an ordered structure.


Then $left( S, vee, wedge, preceq right)$ is called a lattice  if and only if :

:$(1): quad left( S, vee, preceq right)$ is a join semilattice
and:
:$(2): quad left( S, wedge, preceq right)$ is a meet semilattice.


That is, for all $a, b in S$:

:$a vee b$ is the supremum of $leftlbrace a, b rightrbrace$
and:
:$a wedge b$ is the infimum of $leftlbrace a, b rightrbrace$

=== Definition 3 ===
Let $left( S, vee right)$ and $left( S, wedge right)$ be semilattices on a set $S$.

Suppose that $vee$ and $wedge$ satisfy the absorption laws, that is, for all $a, b in S$:

:$a vee left( a wedge b right) = a$
:$a wedge left( a vee b right) = a$

Let $preceq$ be the ordering on $S$ defined by:

:$forall a, b in S: a preceq b$  if and only if  $a vee b = b$

as on Semilattice Induces Ordering.


Then the ordered structure $left( S, vee, wedge, preceq right)$ is called a lattice.


Thus $left( S, vee, wedge, preceq right)$ is called a lattice  if and only if  the lattice axioms are satisfied and $preceq$ is defined as above:
 ",Definition:Lattice (Order Theory),,false,"=== Definition 1 ===
Let ( S, ≼) be an ordered set.

Suppose that S admits all finite non-empty suprema and finite non-empty infima.

Denote with ∨ and ∧ the join and meet operations on S, respectively.


Then the ordered structure ( S, ∨, ∧, ≼) is called a lattice.

=== Definition 2 ===
Let ( S, ∨, ∧, ≼) be an ordered structure.


Then ( S, ∨, ∧, ≼) is called a lattice  if and only if :

:(1):   ( S, ∨, ≼) is a join semilattice
and:
:(2):   ( S, ∧, ≼) is a meet semilattice.


That is, for all a, b ∈ S:

:a ∨ b is the supremum of { a, b }
and:
:a ∧ b is the infimum of { a, b }

=== Definition 3 ===
Let ( S, ∨) and ( S, ∧) be semilattices on a set S.

Suppose that ∨ and ∧ satisfy the absorption laws, that is, for all a, b ∈ S:

:a ∨( a ∧ b ) = a
:a ∧( a ∨ b ) = a

Let ≼ be the ordering on S defined by:

:∀ a, b ∈ S: a ≼ b  if and only if  a ∨ b = b

as on Semilattice Induces Ordering.


Then the ordered structure ( S, ∨, ∧, ≼) is called a lattice.


Thus ( S, ∨, ∧, ≼) is called a lattice  if and only if  the lattice axioms are satisfied and ≼ is defined as above:
 ",Lattice
"['Definitions/Point Lattices', 'Definitions/Group Theory', 'Definitions/Lattice Theory']",Definition:Lattice,"=== Definition 1 ===
A point lattice is a discrete subgroup of $mathbb R^m$ under vector addition.

=== Definition 2 ===
Let $mathbb R^m$ be the $m$-dimensional real Euclidean space.

Let $leftlbrace mathbf v_1, mathbf v_2, ldots, mathbf v_n rightrbrace$ be a linearly independent set of vectors of $mathbb R^m$.

A point lattice in $mathbb R^m$ is the set of all integer linear combinations of such vectors.


That is:
:$ds mathcal L left(   right){mathbf v_1, mathbf v_2, ldots, mathbf v_n} = leftlbrace sum_{i mathop = 1}^n a_i mathbf v_i : a_i in mathbb Z rightrbrace$",Definition:Point Lattice,,false,"=== Definition 1 ===
A point lattice is a discrete subgroup of ℝ^m under vector addition.

=== Definition 2 ===
Let ℝ^m be the m-dimensional real Euclidean space.

Let {𝐯_1, 𝐯_2, …, 𝐯_n } be a linearly independent set of vectors of ℝ^m.

A point lattice in ℝ^m is the set of all integer linear combinations of such vectors.


That is:
:ℒ(   )𝐯_1, 𝐯_2, …, 𝐯_n = {∑_i  = 1^n a_i 𝐯_i : a_i ∈ℤ}",Lattice
['Definitions/Polynomial Theory'],Definition:Leading Coefficient,"Let $R$ be a commutative ring with unity.

Let $P in R left[ X right]$ be a nonzero polynomial over $R$.

Let $n$ be the degree of $P$.


The leading coefficient of $P$ is the coefficient of $x^n$ in $P$.

 

Let $left( R, +, circ right)$ be a ring.

Let $left( S, +, circ right)$ be a subring of $R$.

Let $ds f = sum_{k mathop = 0}^n a_k circ x^k$ be a polynomial in $x$ over $S$.


The coefficient $a_n ne 0_R$ is called the leading coefficient of $f$.


=== Polynomial Form ===
Let $R$ be a commutative ring with unity.

Let $f = a_0 + a_1 X + cdots + a_{r-1} X^{r-1} + a_r X^r$ be a polynomial form in the single indeterminate $X$ over $R$.


Then the ring element $a_r$ is called the leading coefficient of $f$.",Definition:Leading Coefficient of Polynomial,,false,"Let R be a commutative ring with unity.

Let P ∈ R [ X ] be a nonzero polynomial over R.

Let n be the degree of P.


The leading coefficient of P is the coefficient of x^n in P.

 

Let ( R, +, ∘) be a ring.

Let ( S, +, ∘) be a subring of R.

Let f = ∑_k  = 0^n a_k ∘ x^k be a polynomial in x over S.


The coefficient a_n  0_R is called the leading coefficient of f.


=== Polynomial Form ===
Let R be a commutative ring with unity.

Let f = a_0 + a_1 X + ⋯ + a_r-1 X^r-1 + a_r X^r be a polynomial form in the single indeterminate X over R.


Then the ring element a_r is called the leading coefficient of f.",Leading Coefficient
['Definitions/Matrix Theory'],Definition:Leading Coefficient,"Let $mathbf A = left[ a right]_{m n}$ be an $m times n$ matrix.

The leading coefficient of each row of $mathbf A$ is the leftmost non-zero element of that row.


A zero row has no leading coefficient.",Definition:Leading Coefficient of Matrix,,false,"Let 𝐀 = [ a ]_m n be an m × n matrix.

The leading coefficient of each row of 𝐀 is the leftmost non-zero element of that row.


A zero row has no leading coefficient.",Leading Coefficient
['Definitions/Language Definitions'],Definition:Left,"The direction left is that way:
:$gets$",Definition:Left (Direction),,false,"The direction left is that way:
:",Left
['Definitions/Language Definitions'],Definition:Left,"In an equation:
:$text {Expression $1$} = text {Expression $2$}$
the term $text {Expression $1$}$ is the left hand side.",Definition:Left Hand Side,,false,"In an equation:
:Expression 1 = Expression 2
the term Expression 1 is the left hand side.",Left
"['Definitions/Relation Theory', 'Definitions/Left-Total Relations']",Definition:Left,"Let $mathcal R subseteq S times T$ be a relation.


Then $mathcal R$ is left-total  if and only if :
:$forall s in S: exists t in T: left( s, t right) in mathcal R$


That is,  if and only if  every element of $S$ relates to some element of $T$.",Definition:Left-Total Relation,,false,"Let ℛ⊆ S × T be a relation.


Then ℛ is left-total  if and only if :
:∀ s ∈ S: ∃ t ∈ T: ( s, t ) ∈ℛ


That is,  if and only if  every element of S relates to some element of T.",Left
"['Definitions/Reflexive Relations', 'Definitions/Quasi-Reflexive Relations', 'Definitions/Left Quasi-Reflexive Relations']",Definition:Left,"Let $mathcal R subseteq S times S$ be a relation in $S$.


=== Definition 1 ===
Let $mathcal R subseteq S times S$ be a relation in $S$.


$mathcal R$ is left quasi-reflexive  if and only if :

:$forall x, y in S: left( x, y right) in mathcal R implies left( x, x right) in mathcal R$

=== Definition 2 ===
Let $mathcal R subseteq S times S$ be a relation in $S$.


$mathcal R$ is left quasi-reflexive  if and only if :

:$forall x in mathrm {Dom} left( mathcal R right): left( x, x right) in mathcal R$

where $mathrm {Dom} left( mathcal R right)$ denotes the domain of $mathcal R$.",Definition:Left Quasi-Reflexive Relation,,false,"Let ℛ⊆ S × S be a relation in S.


=== Definition 1 ===
Let ℛ⊆ S × S be a relation in S.


ℛ is left quasi-reflexive  if and only if :

:∀ x, y ∈ S: ( x, y ) ∈ℛ( x, x ) ∈ℛ

=== Definition 2 ===
Let ℛ⊆ S × S be a relation in S.


ℛ is left quasi-reflexive  if and only if :

:∀ x ∈Dom( ℛ): ( x, x ) ∈ℛ

where Dom( ℛ) denotes the domain of ℛ.",Left
['Definitions/Euclidean Relations'],Definition:Left,"Let $mathcal R subseteq S times S$ be a relation in $S$.


$mathcal R$ is left-Euclidean  if and only if :

:$left( x, z right) in mathcal R land left( y, z right) in mathcal R implies left( x, y right) in mathcal R$",Definition:Euclidean Relation/Left-Euclidean,,false,"Let ℛ⊆ S × S be a relation in S.


ℛ is left-Euclidean  if and only if :

:( x, z ) ∈ℛ( y, z ) ∈ℛ( x, y ) ∈ℛ",Left
['Definitions/Relations'],Definition:Left,"Let $A$ be a class.

Let $mathcal R$ be a relation on $A$.


An element $x$ of $A$ is left normal with respect to $mathcal R$  if and only if :
:$forall y in A: mathcal R left(   right){x, y}$ holds.",Definition:Left Normal Element of Relation,,false,"Let A be a class.

Let ℛ be a relation on A.


An element x of A is left normal with respect to ℛ  if and only if :
:∀ y ∈ A: ℛ(   )x, y holds.",Left
"['Definitions/Mapping Theory', 'Definitions/Cancellability']",Definition:Left,"A mapping $f: Y to Z$ is left cancellable (or left-cancellable)  if and only if :

:$forall X: forall left( g_1, g_2: X to Y right): f circ g_1 = f circ g_2 implies g_1 = g_2$

That is, for any set $X$, if $g_1$ and $g_2$ are mappings from $X$ to $Y$:
:If $f circ g_1 = f circ g_2$
:then $g_1 = g_2$.",Definition:Left Cancellable Mapping,,false,"A mapping f: Y → Z is left cancellable (or left-cancellable)  if and only if :

:∀ X: ∀( g_1, g_2: X → Y ): f ∘ g_1 = f ∘ g_2  g_1 = g_2

That is, for any set X, if g_1 and g_2 are mappings from X to Y:
:If f ∘ g_1 = f ∘ g_2
:then g_1 = g_2.",Left
['Definitions/Inverse Mappings'],Definition:Left,"Let $S, T$ be sets where $S ne varnothing$, that is, $S$ is not empty.

Let $f: S to T$ be a mapping.


Let $g: T to S$ be a mapping such that:
:$g circ f = I_S$
where:
:$g circ f$ denotes the composite mapping $f$ followed by $g$;
:$I_S$ is the identity mapping on $S$.


Then $g: T to S$ is called a left inverse (mapping).",Definition:Left Inverse Mapping,,false,"Let S, T be sets where S ∅, that is, S is not empty.

Let f: S → T be a mapping.


Let g: T → S be a mapping such that:
:g ∘ f = I_S
where:
:g ∘ f denotes the composite mapping f followed by g;
:I_S is the identity mapping on S.


Then g: T → S is called a left inverse (mapping).",Left
['Definitions/Intervals'],Definition:Left,"Let $left( S, preccurlyeq right)$ be an ordered set.

Let $a, b in S$.


The left half-open interval between $a$ and $b$ is the set:

:$left( a ,.,.,   right]b := a^succ cap b^preccurlyeq = leftlbrace s in S: left( a prec s right) land left( s preccurlyeq b right)  rightrbrace$

where:
:$a^succ$ denotes the strict upper closure of $a$
:$b^preccurlyeq$ denotes the lower closure of $b$.",Definition:Interval/Ordered Set/Left Half-Open,,false,"Let ( S, ≼) be an ordered set.

Let a, b ∈ S.


The left half-open interval between a and b is the set:

:( a  . . ]b := a^≻∩ b^≼ = { s ∈ S: ( a ≺ s ) ( s ≼ b )  }

where:
:a^≻ denotes the strict upper closure of a
:b^≼ denotes the lower closure of b.",Left
['Definitions/Derivatives'],Definition:Left,"Let $B$ be a Banach space over the set of real numbers $mathbb R$.

Let $f: mathbb R to B$ be a mapping from $mathbb R$ to $B$.


The left-hand derivative of $f$ is defined as the left-hand limit:
:$ds f'_- left(   right)x = lim_{h mathop to 0^-} frac {f left(   right){x + h} - f left(   right)x} h$

If the left-hand derivative exists, then $f$ is said to be left-hand differentiable at $x$.


=== Real Functions ===
Let $f: mathbb R to mathbb R$ be a real function.


The left-hand derivative of $f$ is defined as the left-hand limit:
:$ds f'_- left(   right)x = lim_{h mathop to 0^-} frac {f left(   right){x + h} - f left(   right)x} h$

If the left-hand derivative exists, then $f$ is said to be left-hand differentiable at $x$.",Definition:Left-Hand Derivative,,false,"Let B be a Banach space over the set of real numbers ℝ.

Let f: ℝ→ B be a mapping from ℝ to B.


The left-hand derivative of f is defined as the left-hand limit:
:f'_- (   )x = lim_h → 0^-f (   )x + h - f (   )x/h

If the left-hand derivative exists, then f is said to be left-hand differentiable at x.


=== Real Functions ===
Let f: ℝ→ℝ be a real function.


The left-hand derivative of f is defined as the left-hand limit:
:f'_- (   )x = lim_h → 0^-f (   )x + h - f (   )x/h

If the left-hand derivative exists, then f is said to be left-hand differentiable at x.",Left
['Definitions/Limits of Real Functions'],Definition:Left,"Let $left( a ,.,.,   right)b$ be an open real interval.

Let $f: left( a ,.,.,   right)b to mathbb R$ be a real function.

Let $L in mathbb R$.


Suppose that:
:$forall epsilon in mathbb R_{>0}: exists delta in mathbb R_{>0}: forall x in mathbb R: b - delta < x < b implies leftlvert f left(   right)x - L rightrvert < epsilon$

where $mathbb R_{>0}$ denotes the set of strictly positive real numbers.

That is, for every real strictly positive $epsilon$ there exists a real strictly positive $delta$ such that every real number in the domain of $f$, less than $b$ but within $delta$ of $b$, has an image within $epsilon$ of $L$.


:

Then $f left(   right)x$ is said to tend to the limit $L$ as $x$ tends to $b$ from the left, and we write:
:$f left(   right)x to L$ as $x to b^-$
or
:$ds lim_{x mathop to b^-} f left(   right)x = L$


This is voiced:
:the limit of $f left(   right)x$ as $x$ tends to $b$ from the left
and such an $L$ is called:
:a limit from the left.",Definition:Limit of Real Function/Left,,false,"Let ( a  . . )b be an open real interval.

Let f: ( a  . . )b →ℝ be a real function.

Let L ∈ℝ.


Suppose that:
:∀ϵ∈ℝ_>0: ∃δ∈ℝ_>0: ∀ x ∈ℝ: b - δ < x < b | f (   )x - L | < ϵ

where ℝ_>0 denotes the set of strictly positive real numbers.

That is, for every real strictly positive ϵ there exists a real strictly positive δ such that every real number in the domain of f, less than b but within δ of b, has an image within ϵ of L.


:

Then f (   )x is said to tend to the limit L as x tends to b from the left, and we write:
:f (   )x → L as x → b^-
or
:lim_x → b^- f (   )x = L


This is voiced:
:the limit of f (   )x as x tends to b from the left
and such an L is called:
:a limit from the left.",Left
['Definitions/Difference Quotients'],Definition:Left,"Let $V$ be a vector space over the real numbers $mathbb R$.

Let $f: mathbb R to V$ be a function.


A left difference quotient is an expression of the form:
:$dfrac {f left(   right){x + h} - f left(   right)x} h$
where $h < 0$ is a strictly negative real number.",Definition:Difference Quotient/Left,,false,"Let V be a vector space over the real numbers ℝ.

Let f: ℝ→ V be a function.


A left difference quotient is an expression of the form:
:f (   )x + h - f (   )x h
where h < 0 is a strictly negative real number.",Left
['Definitions/Continuous Real Functions'],Definition:Left,"Let $A subseteq mathbb R$ be an open subset of the real numbers $mathbb R$.

Let $f: A to mathbb R$ be a real function.


Let $x_0 in A$. 

Then $f$ is said to be left-continuous at $x_0$  if and only if  the limit from the left of $f left(   right)x$ as $x to x_0$ exists and:

:$ds lim_{substack {x mathop to x_0^- \ x_0 mathop in A} } f left(   right)x = f left(   right){x_0}$

where $ds lim_{x mathop to x_0^-}$ is a limit from the left.


Furthermore, $f$ is said to be left-continuous  if and only if :

:$forall x_0 in A$, $f$ is left-continuous at $x_0$",Definition:Continuous Real Function/Left-Continuous,,false,"Let A ⊆ℝ be an open subset of the real numbers ℝ.

Let f: A →ℝ be a real function.


Let x_0 ∈ A. 

Then f is said to be left-continuous at x_0  if and only if  the limit from the left of f (   )x as x → x_0 exists and:

:lim_x → x_0^- 
 x_0 ∈ A f (   )x = f (   )x_0

where lim_x → x_0^- is a limit from the left.


Furthermore, f is said to be left-continuous  if and only if :

:∀ x_0 ∈ A, f is left-continuous at x_0",Left
['Definitions/Derivatives'],Definition:Left,"Let $f: mathbb R to mathbb R$ be a real function.


The left-hand derivative of $f$ is defined as the left-hand limit:
:$ds f'_- left(   right)x = lim_{h mathop to 0^-} frac {f left(   right){x + h} - f left(   right)x} h$

If the left-hand derivative exists, then $f$ is said to be left-hand differentiable at $x$.",Definition:Left-Hand Derivative/Real Function,,false,"Let f: ℝ→ℝ be a real function.


The left-hand derivative of f is defined as the left-hand limit:
:f'_- (   )x = lim_h → 0^-f (   )x + h - f (   )x/h

If the left-hand derivative exists, then f is said to be left-hand differentiable at x.",Left
[],Definition:Left,"Let $a, b in mathbb R$ be real numbers.


The left half-open (real) interval from $a$ to $b$ is the subset:
:$left( a ,.,.,   right]b := leftlbrace x in mathbb R: a < x le b rightrbrace$",Definition:Real Interval/Half-Open/Left,,false,"Let a, b ∈ℝ be real numbers.


The left half-open (real) interval from a to b is the subset:
:( a  . . ]b := { x ∈ℝ: a < x ≤ b }",Left
['Definitions/Real Intervals'],Definition:Left,"There are two unbounded closed intervals involving a real number $a in mathbb R$, defined as:

 
 
 
 ",Definition:Real Interval/Unbounded Closed,,false,"There are two unbounded closed intervals involving a real number a ∈ℝ, defined as:

 
 
 
 ",Left
['Definitions/Real Intervals'],Definition:Left,"There are two unbounded open intervals involving a real number $a in mathbb R$, defined as:

 
 
 
 ",Definition:Real Interval/Unbounded Open,,false,"There are two unbounded open intervals involving a real number a ∈ℝ, defined as:

 
 
 
 ",Left
['Definitions/Zero Elements'],Definition:Left,"Let $left( S, circ right)$ be an algebraic structure.

An element $z_L in S$ is called a left zero element (or just left zero)  if and only if :
:$forall x in S: z_L circ x = z_L$",Definition:Left Zero,,false,"Let ( S, ∘) be an algebraic structure.

An element z_L ∈ S is called a left zero element (or just left zero)  if and only if :
:∀ x ∈ S: z_L ∘ x = z_L",Left
['Definitions/Identity Elements'],Definition:Left,"Let $left( S, circ right)$ be an algebraic structure.

An element $e_L in S$ is called a left identity (element)  if and only if :
:$forall x in S: e_L circ x = x$",Definition:Identity (Abstract Algebra)/Left Identity,,false,"Let ( S, ∘) be an algebraic structure.

An element e_L ∈ S is called a left identity (element)  if and only if :
:∀ x ∈ S: e_L ∘ x = x",Left
"['Definitions/Abstract Algebra', 'Definitions/Left Operation']",Definition:Left,"Let $S$ be a set.

For any $x, y in S$, the left operation on $S$ is the binary operation defined as:
:$forall x, y in S: x gets y = x$",Definition:Left Operation,,false,"Let S be a set.

For any x, y ∈ S, the left operation on S is the binary operation defined as:
:∀ x, y ∈ S: x  y = x",Left
['Definitions/Cancellability'],Definition:Left,"Let $left( S, circ right)$ be an algebraic structure.


An element $x in left( S, circ right)$ is left cancellable  if and only if :

:$forall a, b in S: x circ a = x circ b implies a = b$",Definition:Cancellable Element/Left Cancellable,,false,"Let ( S, ∘) be an algebraic structure.


An element x ∈( S, ∘) is left cancellable  if and only if :

:∀ a, b ∈ S: x ∘ a = x ∘ b  a = b",Left
['Definitions/Cancellability'],Definition:Left,"Let $left( S, circ right)$ be an algebraic structure.


The operation $circ$ in $left( S, circ right)$ is left cancellable  if and only if :
:$forall a, b, c in S: a circ b = a circ c implies b = c$

That is,  if and only if  all elements of $left( S, circ right)$ are left cancellable.",Definition:Left Cancellable Operation,,false,"Let ( S, ∘) be an algebraic structure.


The operation ∘ in ( S, ∘) is left cancellable  if and only if :
:∀ a, b, c ∈ S: a ∘ b = a ∘ c  b = c

That is,  if and only if  all elements of ( S, ∘) are left cancellable.",Left
['Definitions/Distributive Operations'],Definition:Left,"Let $S$ be a set on which is defined two binary operations, defined on all the elements of $S times S$, denoted here as $circ$ and $*$.

The operation $circ$ is left distributive over the operation $*$  if and only if :

:$forall a, b, c in S: a circ left( b * c right) = left( a circ b right) * left( a circ c right)$",Definition:Distributive Operation/Left,,false,"Let S be a set on which is defined two binary operations, defined on all the elements of S × S, denoted here as ∘ and *.

The operation ∘ is left distributive over the operation *  if and only if :

:∀ a, b, c ∈ S: a ∘( b * c ) = ( a ∘ b ) * ( a ∘ c )",Left
['Definitions/Inverse Elements'],Definition:Left,"Let $left( S, circ right)$ be a monoid whose identity is $e_S$.

An element $x_L in S$ is called a left inverse of $x$  if and only if :
:$x_L circ x = e_S$",Definition:Inverse (Abstract Algebra)/Left Inverse,,false,"Let ( S, ∘) be a monoid whose identity is e_S.

An element x_L ∈ S is called a left inverse of x  if and only if :
:x_L ∘ x = e_S",Left
['Definitions/Quasigroups'],Definition:Left,"Let $left( S, circ right)$ be a magma. 


$left( S, circ right)$ is a left quasigroup  if and only if :
:for all $a in S$, the left regular representation $lambda_a$ is a permutation on $S$.

That is:
:$forall a, b in S: exists ! x in S: a circ x = b$",Definition:Quasigroup/Left Quasigroup,,false,"Let ( S, ∘) be a magma. 


( S, ∘) is a left quasigroup  if and only if :
:for all a ∈ S, the left regular representation λ_a is a permutation on S.

That is:
:∀ a, b ∈ S: ∃ ! x ∈ S: a ∘ x = b",Left
"['Definitions/Left Regular Representation', 'Definitions/Regular Representations']",Definition:Left,"Let $left( S, circ right)$ be a magma.

The mapping $lambda_a: S to S$ is defined as:

:$forall x in S: lambda_a left(   right)x = a circ x$


This is known as the left regular representation of $left( S, circ right)$ with respect to $a$.",Definition:Regular Representations/Left Regular Representation,,false,"Let ( S, ∘) be a magma.

The mapping λ_a: S → S is defined as:

:∀ x ∈ S: λ_a (   )x = a ∘ x


This is known as the left regular representation of ( S, ∘) with respect to a.",Left
['Definitions/Operations'],Definition:Left,"Let $x$ and $y$ be elements which are operated on by a given operation $circ$.

The left-hand product of $x$ by $y$ is the product $y circ x$.",Definition:Operation/Binary Operation/Product/Left,,false,"Let x and y be elements which are operated on by a given operation ∘.

The left-hand product of x by y is the product y ∘ x.",Left
['Definitions/Naturally Ordered Semigroup'],Definition:Left,"Let $left( S, circ, preceq right)$ be a positively totally ordered semigroup.


Then $left( S, circ, preceq right)$ is a left naturally totally ordered semigroup  if and only if :

:$a prec b implies exists x in S: b = x circ a$",Definition:Left Naturally Totally Ordered Semigroup,,false,"Let ( S, ∘, ≼) be a positively totally ordered semigroup.


Then ( S, ∘, ≼) is a left naturally totally ordered semigroup  if and only if :

:a ≺ b ∃ x ∈ S: b = x ∘ a",Left
['Definitions/Cosets'],Definition:Left,"Let $left( S, circ right)$ be an algebraic structure.

Let $left( H, circ right)$ be a subgroup of $left( S, circ right)$.


The left coset of $x$ modulo $H$, or left coset of $H$ by $x$, is:

:$x circ H = leftlbrace y in S: exists h in H: y = x circ h rightrbrace$


That is, it is the subset product with singleton:

:$x circ H = leftlbrace x rightrbrace circ H$",Definition:Coset/Left Coset,,false,"Let ( S, ∘) be an algebraic structure.

Let ( H, ∘) be a subgroup of ( S, ∘).


The left coset of x modulo H, or left coset of H by x, is:

:x ∘ H = { y ∈ S: ∃ h ∈ H: y = x ∘ h }


That is, it is the subset product with singleton:

:x ∘ H = { x }∘ H",Left
['Definitions/Cosets'],Definition:Left,"Let $G$ be a group.

Let $H$ be a subgroup of $G$.


The left coset space (of $G$ modulo $H$) is the quotient set of $G$ by left congruence modulo $H$, denoted $G / H^l$.

It is the set of all the left cosets of $H$ in $G$.

 ",Definition:Coset Space/Left Coset Space,,false,"Let G be a group.

Let H be a subgroup of G.


The left coset space (of G modulo H) is the quotient set of G by left congruence modulo H, denoted G / H^l.

It is the set of all the left cosets of H in G.

 ",Left
['Definitions/Transversals (Group Theory)'],Definition:Left,"Let $G$ be a group.

Let $H$ be a subgroup of $G$.

Let $S subseteq G$ be a subset of $G$.


$S$ is a left transversal for $H$ in $G$  if and only if  every left coset of $H$ contains exactly one element of $S$.",Definition:Transversal (Group Theory)/Left Transversal,,false,"Let G be a group.

Let H be a subgroup of G.

Let S ⊆ G be a subset of G.


S is a left transversal for H in G  if and only if  every left coset of H contains exactly one element of S.",Left
['Definitions/Group Actions'],Definition:Left,"Let $X$ be a set.

Let $left( G, circ right)$ be a group whose identity is $e$.


A (left) group action is an operation $phi: G times X to X$ such that:

:$forall left( g, x right) in G times X: g * x := phi left(   right){g, x} in X$

in such a way that the group action axioms are satisfied:
 ",Definition:Group Action/Left Group Action,,false,"Let X be a set.

Let ( G, ∘) be a group whose identity is e.


A (left) group action is an operation ϕ: G × X → X such that:

:∀( g, x ) ∈ G × X: g * x := ϕ(   )g, x∈ X

in such a way that the group action axioms are satisfied:
 ",Left
['Definitions/Subset Product Action'],Definition:Left,"Let $left( G, circ right)$ be a group.

Let $mathcal P left( G right)$ be the power set of $G$.


The (left) subset product action of $G$ is the group action $*: G times mathcal P left( G right) to mathcal P left( G right)$:
:$forall g in G, S in mathcal P left( G right): g * S = g circ S$",Definition:Subset Product Action/Left,,false,"Let ( G, ∘) be a group.

Let 𝒫( G ) be the power set of G.


The (left) subset product action of G is the group action *: G ×𝒫( G ) →𝒫( G ):
:∀ g ∈ G, S ∈𝒫( G ): g * S = g ∘ S",Left
['Definitions/Congruence Modulo Subgroup'],Definition:Left,"Let $G$ be a group.

Let $H$ be a subgroup of $G$.


We can use $H$ to define a relation on $G$ as follows:

:$mathcal R^l_H := leftlbrace left( x, y right) in G times G: x^{-1} y in H rightrbrace$

This is called left congruence modulo $H$.",Definition:Congruence Modulo Subgroup/Left Congruence,,false,"Let G be a group.

Let H be a subgroup of G.


We can use H to define a relation on G as follows:

:ℛ^l_H := {( x, y ) ∈ G × G: x^-1 y ∈ H }

This is called left congruence modulo H.",Left
['Definitions/Zero Divisors'],Definition:Left,"Let $left( R, +, circ right)$ be a ring.


A left zero divisor (in $R$) is an element $x in R$ such that:
:$exists y in R^*: x circ y = 0_R$

where $R^*$ is defined as $R setminus leftlbrace 0_R rightrbrace$.",Definition:Left Zero Divisor,,false,"Let ( R, +, ∘) be a ring.


A left zero divisor (in R) is an element x ∈ R such that:
:∃ y ∈ R^*: x ∘ y = 0_R

where R^* is defined as R ∖{ 0_R }.",Left
['Definitions/Linear Ring Actions'],Definition:Left,"Let $R$ be a ring.

Let $M$ be an abelian group.

Let $circ : R times M to M$ be a mapping from the cartesian product $R times M$.


$circ$ is a left linear ring action of $R$ on $M$  if and only if  $circ$ satisfies the left ring action axioms:
 ",Definition:Linear Ring Action/Left,,false,"Let R be a ring.

Let M be an abelian group.

Let ∘ : R × M → M be a mapping from the cartesian product R × M.


∘ is a left linear ring action of R on M  if and only if  ∘ satisfies the left ring action axioms:
 ",Left
['Definitions/Ideal Theory'],Definition:Left,"Let $left( R, +, circ right)$ be a ring.

Let $left( J, + right)$ be a subgroup of $left( R, + right)$.


$J$ is a left ideal of $R$  if and only if :
:$forall j in J: forall r in R: r circ j in J$

that is,  if and only if :
:$forall r in R: r circ J subseteq J$",Definition:Ideal of Ring/Left Ideal,,false,"Let ( R, +, ∘) be a ring.

Let ( J, + ) be a subgroup of ( R, + ).


J is a left ideal of R  if and only if :
:∀ j ∈ J: ∀ r ∈ R: r ∘ j ∈ J

that is,  if and only if :
:∀ r ∈ R: r ∘ J ⊆ J",Left
['Definitions/Maximal Ideals of Rings'],Definition:Left,"Let $R$ be a ring.


A left ideal $J$ of $R$ is a maximal left ideal  if and only if :

:$(1): quad J subsetneq R$
:$(2): quad$ There is no left ideal $K$ of $R$ such that $J subsetneq K subsetneq R$.


Category:Definitions/Maximal Ideals of Rings",Definition:Maximal Ideal of Ring/Left,,false,"Let R be a ring.


A left ideal J of R is a maximal left ideal  if and only if :

:(1):    J ⊊ R
:(2): There is no left ideal K of R such that J ⊊ K ⊊ R.


Category:Definitions/Maximal Ideals of Rings",Left
"['Definitions/Left Modules', 'Definitions/Module Theory']",Definition:Left,"Let $left( R, +_R, times_R right)$ be a ring.

Let $left( G, +_G right)$ be an abelian group.


A left module over $R$ is an $R$-algebraic structure $left( G, +_G, circ right)_R$ with one operation $circ$, the (left) ring action, which satisfies the left module axioms:
 ",Definition:Left Module,,false,"Let ( R, +_R, ×_R ) be a ring.

Let ( G, +_G ) be an abelian group.


A left module over R is an R-algebraic structure ( G, +_G, ∘)_R with one operation ∘, the (left) ring action, which satisfies the left module axioms:
 ",Left
"['Definitions/Linear Algebra', 'Definitions/Null Spaces']",Definition:Left,"Let $R$ be a ring.

Let $mathbf A$ be a matrix in the matrix space $mathcal M_{m, n}  left(   right)R$.

Let $mathbf A^intercal$ be the transpose of $mathbf A$.

The left null space $mathbf A$ is defined as the null space of $mathbf A^intercal$.",Definition:Left Null Space,,false,"Let R be a ring.

Let 𝐀 be a matrix in the matrix space ℳ_m, n(   )R.

Let 𝐀^⊺ be the transpose of 𝐀.

The left null space 𝐀 is defined as the null space of 𝐀^⊺.",Left
"['Definitions/Prime Numbers', 'Definitions/Recreational Mathematics', 'Definitions/Left-Truncatable Primes']",Definition:Left,"A left-truncatable prime is a prime number which remains prime when any number of digits are removed from the left hand end.

Zeroes are excluded, in order to eliminate, for example, prime numbers of the form $10^n + 3$ for arbitrarily large $n$.


=== Sequence ===
 ",Definition:Left-Truncatable Prime,,false,"A left-truncatable prime is a prime number which remains prime when any number of digits are removed from the left hand end.

Zeroes are excluded, in order to eliminate, for example, prime numbers of the form 10^n + 3 for arbitrarily large n.


=== Sequence ===
 ",Left
"['Definitions/Examples of Categories', 'Definitions/Module Theory']",Definition:Left,"Let $R$ be a ring.


The category of left $R$-modules is the category $mathbf {R-Mod}$ with:

 ",Definition:Category of Left Modules,,false,"Let R be a ring.


The category of left R-modules is the category 𝐑-𝐌𝐨𝐝 with:

 ",Left
['Definitions/Orientation (Coordinate Axes)'],Definition:Left,"A Cartesian plane is defined as being left-handed if it has the following property:

Let a left hand be placed, with palm uppermost, such that the thumb points along the $x$-axis in the positive direction, such that the thumb and index finger are at right-angles to each other.

Then the index finger is pointed along the $y$-axis in the positive direction.


:",Definition:Orientation of Coordinate Axes/Cartesian Plane/Left-Handed,,false,"A Cartesian plane is defined as being left-handed if it has the following property:

Let a left hand be placed, with palm uppermost, such that the thumb points along the x-axis in the positive direction, such that the thumb and index finger are at right-angles to each other.

Then the index finger is pointed along the y-axis in the positive direction.


:",Left
['Definitions/Orientation (Coordinate Axes)'],Definition:Left,"A Cartesian $3$-Space is defined as being left-handed if it has the following property:

Let a left hand be placed such that:
:the thumb and index finger are at right-angles to each other
:the $3$rd finger is at right-angles to the thumb and index finger, upwards from the palm
:the thumb points along the $x$-axis in the positive direction
:the index finger points along the $y$-axis in the positive direction.

Then the $3$rd finger is pointed along the $z$-axis in the positive direction.


:",Definition:Orientation of Coordinate Axes/Cartesian 3-Space/Left-Handed,,false,"A Cartesian 3-Space is defined as being left-handed if it has the following property:

Let a left hand be placed such that:
:the thumb and index finger are at right-angles to each other
:the 3rd finger is at right-angles to the thumb and index finger, upwards from the palm
:the thumb points along the x-axis in the positive direction
:the index finger points along the y-axis in the positive direction.

Then the 3rd finger is pointed along the z-axis in the positive direction.


:",Left
['Definitions/Cancellability'],Definition:Left Cancellable,"Let $left( S, circ right)$ be an algebraic structure.


An element $x in left( S, circ right)$ is left cancellable  if and only if :

:$forall a, b in S: x circ a = x circ b implies a = b$",Definition:Cancellable Element/Left Cancellable,,false,"Let ( S, ∘) be an algebraic structure.


An element x ∈( S, ∘) is left cancellable  if and only if :

:∀ a, b ∈ S: x ∘ a = x ∘ b  a = b",Left Cancellable
['Definitions/Cancellability'],Definition:Left Cancellable,"Let $left( S, circ right)$ be an algebraic structure.


The operation $circ$ in $left( S, circ right)$ is left cancellable  if and only if :
:$forall a, b, c in S: a circ b = a circ c implies b = c$

That is,  if and only if  all elements of $left( S, circ right)$ are left cancellable.",Definition:Left Cancellable Operation,,false,"Let ( S, ∘) be an algebraic structure.


The operation ∘ in ( S, ∘) is left cancellable  if and only if :
:∀ a, b, c ∈ S: a ∘ b = a ∘ c  b = c

That is,  if and only if  all elements of ( S, ∘) are left cancellable.",Left Cancellable
"['Definitions/Mapping Theory', 'Definitions/Cancellability']",Definition:Left Cancellable,"A mapping $f: Y to Z$ is left cancellable (or left-cancellable)  if and only if :

:$forall X: forall left( g_1, g_2: X to Y right): f circ g_1 = f circ g_2 implies g_1 = g_2$

That is, for any set $X$, if $g_1$ and $g_2$ are mappings from $X$ to $Y$:
:If $f circ g_1 = f circ g_2$
:then $g_1 = g_2$.",Definition:Left Cancellable Mapping,,false,"A mapping f: Y → Z is left cancellable (or left-cancellable)  if and only if :

:∀ X: ∀( g_1, g_2: X → Y ): f ∘ g_1 = f ∘ g_2  g_1 = g_2

That is, for any set X, if g_1 and g_2 are mappings from X to Y:
:If f ∘ g_1 = f ∘ g_2
:then g_1 = g_2.",Left Cancellable
['Definitions/Inverse Mappings'],Definition:Left Inverse,"Let $S, T$ be sets where $S ne varnothing$, that is, $S$ is not empty.

Let $f: S to T$ be a mapping.


Let $g: T to S$ be a mapping such that:
:$g circ f = I_S$
where:
:$g circ f$ denotes the composite mapping $f$ followed by $g$;
:$I_S$ is the identity mapping on $S$.


Then $g: T to S$ is called a left inverse (mapping).",Definition:Left Inverse Mapping,,false,"Let S, T be sets where S ∅, that is, S is not empty.

Let f: S → T be a mapping.


Let g: T → S be a mapping such that:
:g ∘ f = I_S
where:
:g ∘ f denotes the composite mapping f followed by g;
:I_S is the identity mapping on S.


Then g: T → S is called a left inverse (mapping).",Left Inverse
['Definitions/Inverse Elements'],Definition:Left Inverse,"Let $left( S, circ right)$ be a monoid whose identity is $e_S$.

An element $x_L in S$ is called a left inverse of $x$  if and only if :
:$x_L circ x = e_S$",Definition:Inverse (Abstract Algebra)/Left Inverse,,false,"Let ( S, ∘) be a monoid whose identity is e_S.

An element x_L ∈ S is called a left inverse of x  if and only if :
:x_L ∘ x = e_S",Left Inverse
['Definitions/Inverse Matrices'],Definition:Left Inverse,"Let $m, n in mathbb Z_{>0}$ be a (strictly) positive integer.


Let $mathbf A = left[ a right]_{m n}$ be a matrix of order $m times n$.

Let $mathbf B = left[ b right]_{n m}$ be a matrix of order $n times m$ such that:
:$mathbf B mathbf A = mathbf I_n$

where $mathbf I_n$ denotes the unit matrix of order $n$.


Then $mathbf B$ is known as a left inverse (matrix) of $mathbf A$.",Definition:Inverse Matrix/Left,,false,"Let m, n ∈ℤ_>0 be a (strictly) positive integer.


Let 𝐀 = [ a ]_m n be a matrix of order m × n.

Let 𝐁 = [ b ]_n m be a matrix of order n × m such that:
:𝐁𝐀 = 𝐈_n

where 𝐈_n denotes the unit matrix of order n.


Then 𝐁 is known as a left inverse (matrix) of 𝐀.",Left Inverse
"['Definitions/Length', 'Definitions/Geometry', 'Definitions/Fundamental Dimensions']",Definition:Length,"Length is linear measure taken in a particular direction.

Usually, in multi-dimensional figures, the dimension in which the linear measure is greatest is referred to as length.

It is the most widely used term for linear measure, as it is the standard term used when only one dimension is under consideration.


Length is the fundamental notion of Euclidean geometry, never defined but regarded as an intuitive concept at the basis of every geometrical theorem.


=== Vector Definition ===
Let $A$ and $B$ be points in a real Euclidean space $mathbb R^n$.

Let $mathbf a$ and $mathbf b$ be the position vectors of $A$ and $B$ respectively.


The length of the straight line $AB$ is given by:
:$L left(   right){AB} := leftlvert mathbf a - mathbf b rightrvert$
where:
:$mathbf a - mathbf b$ denotes the vector subtraction operation
:$leftlvert mathbf a - mathbf b rightrvert$ denotes the magnitude of $mathbf a - mathbf b$.",Definition:Linear Measure/Length,measure,true,"Length is linear measure taken in a particular direction.

Usually, in multi-dimensional figures, the dimension in which the linear measure is greatest is referred to as length.

It is the most widely used term for linear measure, as it is the standard term used when only one dimension is under consideration.


Length is the fundamental notion of Euclidean geometry, never defined but regarded as an intuitive concept at the basis of every geometrical theorem.


=== Vector Definition ===
Let A and B be points in a real Euclidean space ℝ^n.

Let 𝐚 and 𝐛 be the position vectors of A and B respectively.


The length of the straight line AB is given by:
:L (   )AB := |𝐚 - 𝐛|
where:
:𝐚 - 𝐛 denotes the vector subtraction operation
:|𝐚 - 𝐛| denotes the magnitude of 𝐚 - 𝐛.",Length
['Definitions/Real Intervals'],Definition:Length,"Let:
:$left[ a ,.,.,   right]b$
or
:$left[ a ,.,.,   right)b$
or
:$left( a ,.,.,   right]b$
or
:$left( a ,.,.,   right)b$
be a real interval.


The difference $b - a$ between the endpoints is called the length of the interval.",Definition:Real Interval/Length,,false,"Let:
:[ a  . . ]b
or
:[ a  . . )b
or
:( a  . . ]b
or
:( a  . . )b
be a real interval.


The difference b - a between the endpoints is called the length of the interval.",Length
['Definitions/Analytic Geometry'],Definition:Length,The length of a curve is defined as the limit of the length of a polygonal line inscribed within the curve as the maximum length of the chords which form that polygonal line tends to zero.,Definition:Length of Curve,limit,true,The length of a curve is defined as the limit of the length of a polygonal line inscribed within the curve as the maximum length of the chords which form that polygonal line tends to zero.,Length
['Definitions/Sequences'],Definition:Length,"The length of a finite sequence is the number of terms it contains, or equivalently, the cardinality of its domain.


=== Sequence of $n$ Terms ===
A sequence of $n$ terms is a (finite) sequence whose length is $n$.",Definition:Length of Sequence,,false,"The length of a finite sequence is the number of terms it contains, or equivalently, the cardinality of its domain.


=== Sequence of n Terms ===
A sequence of n terms is a (finite) sequence whose length is n.",Length
['Definitions/Complex Analysis'],Definition:Length,"Let $C$ be a contour in $mathbb C$ defined by the (finite) sequence $leftlangle C_1, ldots, C_n rightrangle$ of directed smooth curves in $mathbb C$.

Let $C_k$ be parameterized by the smooth path $gamma_k: left[ a_k ,.,.,   right]{b_k} to mathbb C$ for all $k in leftlbrace 1, ldots, n rightrbrace$.


The length of $C$ is defined as:

:$ds L left(   right)C := sum_{k mathop = 1}^n int_{a_k}^{b_k} leftlvert gamma_k' left(   right)t rightrvert ,mathrm d t$


It follows from Length of Contour is Well-Defined that $L left(   right)C$ is defined and independent of the parameterizations of $C_1, ldots, C_n$.",Definition:Contour/Length/Complex Plane,,false,"Let C be a contour in ℂ defined by the (finite) sequence ⟨ C_1, …, C_n ⟩ of directed smooth curves in ℂ.

Let C_k be parameterized by the smooth path γ_k: [ a_k  . . ]b_k→ℂ for all k ∈{ 1, …, n }.


The length of C is defined as:

:L (   )C := ∑_k  = 1^n ∫_a_k^b_k|γ_k' (   )t | d t


It follows from Length of Contour is Well-Defined that L (   )C is defined and independent of the parameterizations of C_1, …, C_n.",Length
"['Definitions/Vectors', 'Definitions/Vector Length']",Definition:Length,"The length of a vector $mathbf v$ in a normed vector space $left( V, leftlVert , cdot , rightrVert  right)$ is defined as $leftlVert mathbf v rightrVert$, the norm of $mathbf v$.


=== Arrow Representation ===
Let $mathbf v$ be a vector quantity represented as an arrow in a real vector space $mathbb R^n$.

The length of $mathbf v$ is the length of the line segment representing $mathbf v$ in $mathbb R^n$.",Definition:Vector Length,,false,"The length of a vector 𝐯 in a normed vector space ( V, ‖ · ‖) is defined as ‖𝐯‖, the norm of 𝐯.


=== Arrow Representation ===
Let 𝐯 be a vector quantity represented as an arrow in a real vector space ℝ^n.

The length of 𝐯 is the length of the line segment representing 𝐯 in ℝ^n.",Length
['Definitions/Chains (Order Theory)'],Definition:Length,"Let $left( S, preceq right)$ be an ordered set.

Let $T$ be a chain in $S$.

Let $T$ be finite and non-empty.


The length of the chain $T$ is its cardinality minus $1$.


Category:Definitions/Chains (Order Theory)",Definition:Length of Chain,,false,"Let ( S, ≼) be an ordered set.

Let T be a chain in S.

Let T be finite and non-empty.


The length of the chain T is its cardinality minus 1.


Category:Definitions/Chains (Order Theory)",Length
['Definitions/Ordinal Sequences'],Definition:Length,"Let $alpha$ be an ordinal.

Let $theta$ be an ordinal sequence whose domain is $alpha$.


Then $alpha$ can be referred to as the length of $theta$.


The length of $theta$ can be denoted $leftlvert theta rightrvert$.",Definition:Ordinal Sequence/Length,,false,"Let α be an ordinal.

Let θ be an ordinal sequence whose domain is α.


Then α can be referred to as the length of θ.


The length of θ can be denoted |θ|.",Length
['Definitions/Continued Fractions'],Definition:Length,The number of partial denominators in a cycle of a periodic continued fraction is called the cycle length.,Definition:Periodic Continued Fraction/Cycle/Length,,false,The number of partial denominators in a cycle of a periodic continued fraction is called the cycle length.,Length
['Definitions/Continued Fractions'],Definition:Length,"Let $k$ be a field.

Let $C$ be a continued fraction in $k$, either finite or infinite.


The length of $C$ is an extended natural number equal to:
*$infty$ if $C$ is an infinite continued fraction.
*$n$ if $C$ is a finite continued fraction with domain the integer interval $left[0 ,.,., nright]$.",Definition:Length of Continued Fraction,,false,"Let k be a field.

Let C be a continued fraction in k, either finite or infinite.


The length of C is an extended natural number equal to:
*∞ if C is an infinite continued fraction.
*n if C is a finite continued fraction with domain the integer interval [0  . .  n].",Length
['Definitions/Walks'],Definition:Length,"The length of a walk (or a path, or a trail) is the number of edges it has, counting repeated edges as many times as they appear.


A walk is said to be of infinite length  if and only if  it has infinitely many edges.


=== Zero Length Walk ===
A zero length walk is a walk which consists of one vertex.


Category:Definitions/Walks",Definition:Walk (Graph Theory)/Length,,false,"The length of a walk (or a path, or a trail) is the number of edges it has, counting repeated edges as many times as they appear.


A walk is said to be of infinite length  if and only if  it has infinitely many edges.


=== Zero Length Walk ===
A zero length walk is a walk which consists of one vertex.


Category:Definitions/Walks",Length
['Definitions/Rooted Trees'],Definition:Length,"Let $T$ be a rooted tree with root node $r_T$.

Let $Gamma$ be a finite branch of $T$.


The length of $Gamma$ is defined as the number of ancestors of the leaf at the end of that branch.",Definition:Rooted Tree/Branch/Length,,false,"Let T be a rooted tree with root node r_T.

Let Γ be a finite branch of T.


The length of Γ is defined as the number of ancestors of the leaf at the end of that branch.",Length
['Definitions/Collations'],Definition:Length,"The length of a finite string in a formal language is the number of symbols it contains.


The length of a string $S$ can be denoted $mathrm {len} left(   right)S$ or $leftlvert S rightrvert$.",Definition:Length of String,,false,"The length of a finite string in a formal language is the number of symbols it contains.


The length of a string S can be denoted len(   )S or | S |.",Length
['Definitions/Proof Systems'],Definition:Length,"The length of a tableau proof is the number of lines it has.


Category:Definitions/Proof Systems",Definition:Tableau Proof (Formal Systems)/Length,,false,"The length of a tableau proof is the number of lines it has.


Category:Definitions/Proof Systems",Length
['Definitions/Group Actions'],Definition:Length,"Let $G$ be a group acting on a set $X$.

Let $x in X$.


Let $mathrm {Orb} left( x right)$ be the orbit of $x$.

The length of the orbit $mathrm {Orb} left( x right)$ of $x$ is the number of elements of $X$ it contains:
:$leftlvert mathrm {Orb} left( x right) rightrvert$


Category:Definitions/Group Actions",Definition:Orbit (Group Theory)/Length,,false,"Let G be a group acting on a set X.

Let x ∈ X.


Let Orb( x ) be the orbit of x.

The length of the orbit Orb( x ) of x is the number of elements of X it contains:
:|Orb( x ) |


Category:Definitions/Group Actions",Length
['Definitions/Normal Series'],Definition:Length,"Let $G$ be a group whose identity is $e$.

Let $leftlangle G_i rightrangle_{i mathop in left[ 0 ,.,.,   right]n}$ be a normal series for $G$:
:$leftlangle G_i rightrangle_{i mathop in left[ 0 ,.,.,   right]n} = left( leftlbrace e rightrbrace = G_0 lhd G_1 lhd cdots lhd G_{n-1} lhd G_n = G right)$


The length of $leftlangle G_i rightrangle_{i mathop in left[ 0 ,.,.,   right]n}$ is the number of (normal) subgroups which make it.

In this context, the length of $leftlangle G_i rightrangle_{i mathop in left[ 0 ,.,.,   right]n}$ is $n$.


If such a normal series is infinite, then its length is not defined.


Category:Definitions/Normal Series",Definition:Normal Series/Length,,false,"Let G be a group whose identity is e.

Let ⟨ G_i ⟩_i ∈[ 0  . . ]n be a normal series for G:
:⟨ G_i ⟩_i ∈[ 0  . . ]n = ( { e } = G_0  G_1 ⋯ G_n-1 G_n = G )


The length of ⟨ G_i ⟩_i ∈[ 0  . . ]n is the number of (normal) subgroups which make it.

In this context, the length of ⟨ G_i ⟩_i ∈[ 0  . . ]n is n.


If such a normal series is infinite, then its length is not defined.


Category:Definitions/Normal Series",Length
"['Definitions/Composition Series', 'Definitions/Group Theory']",Definition:Length,"Let $G$ be a finite group.

The length of $G$ is the length of a composition series for $G$.

That is, the length of $G$ is the number of factors in a composition series for $G$ (not including $G$ itself).


The length of $G$ can be denoted $l left(   right)G$.


By the Jordan-Hölder Theorem, all composition series for $G$ have the same length.

Therefore, the length of a finite group $G$ is well-defined.",Definition:Length of Group,,false,"Let G be a finite group.

The length of G is the length of a composition series for G.

That is, the length of G is the number of factors in a composition series for G (not including G itself).


The length of G can be denoted l (   )G.


By the Jordan-Hölder Theorem, all composition series for G have the same length.

Therefore, the length of a finite group G is well-defined.",Length
"['Definitions/Cyclic Permutations', 'Definitions/Permutation Theory']",Definition:Length,"Let $S_n$ denote the symmetric group on $n$ letters.

Let $rho in S_n$ be a permutation on $S$.


Then $rho$ is a cyclic permutation of length $k$  if and only if  there exists $k in mathbb Z: k > 0$ and $i in mathbb Z$ such that:
:$(1): quad k$ is the smallest such that $rho^k left(   right)i = i$

:$(2): quad rho$ fixes each $j$ not in $leftlbrace i, rho left(   right)i, ldots, rho^{k - 1}  left(   right)i rightrbrace$.


$rho$ is usually denoted using cycle notation as:
:$begin{pmatrix} i & rho left(   right)i & ldots & rho^{k - 1}  left(   right)i end{pmatrix}$

but some sources introduce it using two-row notation:

:$begin{pmatrix} a_1 & a_2 & cdots & a_k & cdots & i & cdots \ a_2 & a_3 & cdots & a_1 & cdots & i & cdots end{pmatrix}$",Definition:Cyclic Permutation,,false,"Let S_n denote the symmetric group on n letters.

Let ρ∈ S_n be a permutation on S.


Then ρ is a cyclic permutation of length k  if and only if  there exists k ∈ℤ: k > 0 and i ∈ℤ such that:
:(1):    k is the smallest such that ρ^k (   )i = i

:(2):   ρ fixes each j not in { i, ρ(   )i, …, ρ^k - 1(   )i }.


ρ is usually denoted using cycle notation as:
:[             i       ρ(   )i             … ρ^k - 1(   )i ]

but some sources introduce it using two-row notation:

:[ a_1 a_2   ⋯ a_k   ⋯   i   ⋯; a_2 a_3   ⋯ a_1   ⋯   i   ⋯ ]",Length
['Definitions/Vectors'],Definition:Like,"Let $mathbf a$ and $mathbf b$ be vector quantities.

Then $mathbf a$ and $mathbf b$ are known as like vector quantities  if and only if  they have the same direction.",Definition:Like Vector Quantities,,false,"Let 𝐚 and 𝐛 be vector quantities.

Then 𝐚 and 𝐛 are known as like vector quantities  if and only if  they have the same direction.",Like
['Definitions/Electric Charge'],Definition:Like,"If $2$ electric charges are of the same polarity, they are referred to as being like (electric) charges.


The usage is archaic; the word one would expect is alike.",Definition:Electric Charge/Polarity/Like,,false,"If 2 electric charges are of the same polarity, they are referred to as being like (electric) charges.


The usage is archaic; the word one would expect is alike.",Like
['Definitions/Limits of Sequences'],Definition:Limit,"=== Topological Space ===
Let $T = left( S, tau right)$ be a topological space.

Let $A subseteq S$.

Let $leftlangle x_n rightrangle$ be a sequence in $A$.

Let $leftlangle x_n rightrangle$ converge to a value $alpha in S$.


Then $alpha$ is known as a limit (point) of $leftlangle x_n rightrangle$ (as $n$ tends to infinity).

=== Metric Space ===
Let $M = left( A, d right)$ be a metric space or pseudometric space.

Let $leftlangle x_n rightrangle$ be a sequence in $M$.

Let $leftlangle x_n rightrangle$ converge to a value $l in A$.


Then $l$ is a limit of $leftlangle x_n rightrangle$ as $n$ tends to infinity.


If $M$ is a metric space, this is usually written:
:$ds l = lim_{n mathop to infty} x_n$

=== Normed Division Ring ===
Let $left( R, leftlVert , cdot , rightrVert  right)$ be a normed division ring.

Let $leftlangle x_n rightrangle $ be a sequence in $R$.

Let $leftlangle x_n rightrangle$ converge to $x in R$.

Then $x$ is a limit of $leftlangle x_n rightrangle$ as $n$ tends to infinity which is usually written:
:$ds x = lim_{n mathop to infty} x_n$

=== Normed Vector Space ===
Let $M = left( X, leftlVert , cdot , rightrVert right)$ be a normed vector space.

Let $L in X$.

Let $leftlangle x_n rightrangle_{n mathop in mathbb N}$ be a sequence in $X$.

Let $leftlangle x_n rightrangle_{n mathop in mathbb N}$ converge to $L$. 


Then $L$ is a limit of  $leftlangle x_n rightrangle_{n mathop in mathbb N}$ as $n$ tends to infinity which is usually written:
:$ds L = lim_{n mathop to infty} x_n$

=== Test Function Space ===
Let $mathcal D left(   right){mathbb R^d}$ be the test function space.

Let $phi in mathcal D left(   right){mathbb R^d}$ be a test function.

Let $leftlangle phi_n rightrangle_{n mathop in mathbb N}$ be a sequence of test functions in $mathcal D left(   right){mathbb R^d}$.

Let $leftlangle phi_n rightrangle_{n mathop in mathbb N}$ converge to $phi$ in $mathcal D left(   right){mathbb R^d}$. 


Then $phi$ is a limit of  $leftlangle phi_n rightrangle_{n mathop in mathbb N}$ in $mathcal D left(   right){mathbb R^d}$ as $n$ tends to infinity which is usually written:
:$phi_n stackrel mathcal D {longrightarrow} phi$",Definition:Limit of Sequence,,false,"=== Topological Space ===
Let T = ( S, τ) be a topological space.

Let A ⊆ S.

Let ⟨ x_n ⟩ be a sequence in A.

Let ⟨ x_n ⟩ converge to a value α∈ S.


Then α is known as a limit (point) of ⟨ x_n ⟩ (as n tends to infinity).

=== Metric Space ===
Let M = ( A, d ) be a metric space or pseudometric space.

Let ⟨ x_n ⟩ be a sequence in M.

Let ⟨ x_n ⟩ converge to a value l ∈ A.


Then l is a limit of ⟨ x_n ⟩ as n tends to infinity.


If M is a metric space, this is usually written:
:l = lim_n →∞ x_n

=== Normed Division Ring ===
Let ( R, ‖ · ‖) be a normed division ring.

Let ⟨ x_n ⟩ be a sequence in R.

Let ⟨ x_n ⟩ converge to x ∈ R.

Then x is a limit of ⟨ x_n ⟩ as n tends to infinity which is usually written:
:x = lim_n →∞ x_n

=== Normed Vector Space ===
Let M = ( X, ‖ · ‖) be a normed vector space.

Let L ∈ X.

Let ⟨ x_n ⟩_n ∈ℕ be a sequence in X.

Let ⟨ x_n ⟩_n ∈ℕ converge to L. 


Then L is a limit of  ⟨ x_n ⟩_n ∈ℕ as n tends to infinity which is usually written:
:L = lim_n →∞ x_n

=== Test Function Space ===
Let 𝒟(   )ℝ^d be the test function space.

Let ϕ∈𝒟(   )ℝ^d be a test function.

Let ⟨ϕ_n ⟩_n ∈ℕ be a sequence of test functions in 𝒟(   )ℝ^d.

Let ⟨ϕ_n ⟩_n ∈ℕ converge to ϕ in 𝒟(   )ℝ^d. 


Then ϕ is a limit of  ⟨ϕ_n ⟩_n ∈ℕ in 𝒟(   )ℝ^d as n tends to infinity which is usually written:
:ϕ_n 𝒟⟶ϕ",Limit
"['Definitions/Limits of Mappings between Metric Spaces', 'Definitions/Limits of Mappings', 'Definitions/Metric Spaces']",Definition:Limit,"Let $M_1 = left( A_1, d_1 right)$ and $M_2 = left( A_2, d_2 right)$ be metric spaces.

Let $c$ be a limit point of $M_1$.

Let $f: A_1 to A_2$ be a mapping from $A_1$ to $A_2$ defined everywhere on $A_1$ except possibly at $c$.


Let $L in M_2$.


$f left(   right)x$ is said to tend to the limit $L$ as $x$ tends to $c$ and is written:
:$f left(   right)x to L$ as $x to c$
or:
:$ds lim_{x mathop to c} f left(   right)x = L$

 if and only if  the following equivalent conditions hold:


=== $epsilon$-$delta$ Condition ===
Let $M_1 = left( A_1, d_1 right)$ and $M_2 = left( A_2, d_2 right)$ be metric spaces.

Let $c$ be a limit point of $M_1$.

Let $f: A_1 to A_2$ be a mapping from $A_1$ to $A_2$ defined everywhere on $A_1$ except possibly at $c$.


Let $L in M_2$.


$f left(   right)x$ is said to tend to the limit $L$ as $x$ tends to $c$ and is written:
:$f left(   right)x to L$ as $x to c$
or
:$ds lim_{x mathop to c} f left(   right)x = L$

 if and only if :

:$forall epsilon in mathbb R_{>0}: exists delta in mathbb R_{>0}: 0 < d_1 left(   right){x, c} < delta implies d_2 left(   right){f left(   right)x, L} < epsilon$

That is, for every real positive $epsilon$ there exists a real positive $delta$ such that every point in the domain of $f$ within $delta$ of $c$ has an image within $epsilon$ of some point $L$ in the codomain of $f$.


This is voiced:
:the limit of $f left(   right)x$ as $x$ tends to $c$.

=== $epsilon$-Ball Condition ===
Let $M_1 = left( A_1, d_1 right)$ and $M_2 = left( A_2, d_2 right)$ be metric spaces.

Let $c$ be a limit point of $M_1$.

Let $f: A_1 to A_2$ be a mapping from $A_1$ to $A_2$ defined everywhere on $A_1$ except possibly at $c$.


Let $L in M_2$.


$f left(   right)x$ is said to tend to the limit $L$ as $x$ tends to $c$ and is written:
:$f left(   right)x to L$ as $x to c$
or
:$ds lim_{x mathop to c} f left(   right)x = L$

 if and only if :

:$forall epsilon in mathbb R_{>0}: exists delta in mathbb R_{>0}: f left[ B_delta left(   right){c; d_1} setminus leftlbrace c rightrbrace right] subseteq B_epsilon left(   right){L; d_2}$

where:
:$B_delta left(   right){c; d_1} setminus leftlbrace c rightrbrace$ is the deleted $delta $-neighborhood of $c$ in $M_1$
:$B_epsilon left(   right){L; d_2}$ is the open $epsilon$-ball of $L$ in $M_2$.


That is, for every open $epsilon$-ball of $L$ in $M_2$, there exists a deleted $delta $-neighborhood of $c$ in $M_1$ whose image is a subset of that open $epsilon$-ball.


This is voiced:
: the limit of $f left(   right)x$ as $x$ tends to $c$.

This is voiced:
: the limit of $f left(   right)x$ as $x$ tends to $c$.",Definition:Limit of Mapping between Metric Spaces,,false,"Let M_1 = ( A_1, d_1 ) and M_2 = ( A_2, d_2 ) be metric spaces.

Let c be a limit point of M_1.

Let f: A_1 → A_2 be a mapping from A_1 to A_2 defined everywhere on A_1 except possibly at c.


Let L ∈ M_2.


f (   )x is said to tend to the limit L as x tends to c and is written:
:f (   )x → L as x → c
or:
:lim_x → c f (   )x = L

 if and only if  the following equivalent conditions hold:


=== ϵ-δ Condition ===
Let M_1 = ( A_1, d_1 ) and M_2 = ( A_2, d_2 ) be metric spaces.

Let c be a limit point of M_1.

Let f: A_1 → A_2 be a mapping from A_1 to A_2 defined everywhere on A_1 except possibly at c.


Let L ∈ M_2.


f (   )x is said to tend to the limit L as x tends to c and is written:
:f (   )x → L as x → c
or
:lim_x → c f (   )x = L

 if and only if :

:∀ϵ∈ℝ_>0: ∃δ∈ℝ_>0: 0 < d_1 (   )x, c < δ d_2 (   )f (   )x, L < ϵ

That is, for every real positive ϵ there exists a real positive δ such that every point in the domain of f within δ of c has an image within ϵ of some point L in the codomain of f.


This is voiced:
:the limit of f (   )x as x tends to c.

=== ϵ-Ball Condition ===
Let M_1 = ( A_1, d_1 ) and M_2 = ( A_2, d_2 ) be metric spaces.

Let c be a limit point of M_1.

Let f: A_1 → A_2 be a mapping from A_1 to A_2 defined everywhere on A_1 except possibly at c.


Let L ∈ M_2.


f (   )x is said to tend to the limit L as x tends to c and is written:
:f (   )x → L as x → c
or
:lim_x → c f (   )x = L

 if and only if :

:∀ϵ∈ℝ_>0: ∃δ∈ℝ_>0: f [ B_δ(   )c; d_1∖{ c }] ⊆ B_ϵ(   )L; d_2

where:
:B_δ(   )c; d_1∖{ c } is the deleted δ-neighborhood of c in M_1
:B_ϵ(   )L; d_2 is the open ϵ-ball of L in M_2.


That is, for every open ϵ-ball of L in M_2, there exists a deleted δ-neighborhood of c in M_1 whose image is a subset of that open ϵ-ball.


This is voiced:
: the limit of f (   )x as x tends to c.

This is voiced:
: the limit of f (   )x as x tends to c.",Limit
"['Definitions/Limits of Real Functions', 'Definitions/Limits of Mappings', 'Definitions/Limits', 'Definitions/Real Analysis']",Definition:Limit,"Let $left( a ,.,.,   right)b$ be an open real interval.

Let $c in left( a ,.,.,   right)b$.

Let $f: left( a ,.,.,   right)b setminus leftlbrace c rightrbrace to mathbb R$ be a real function.

Let $L in mathbb R$.


=== Definition 1 ===
Let $left( a ,.,.,   right)b$ be an open real interval.

Let $c in left( a ,.,.,   right)b$.

Let $f: left( a ,.,.,   right)b setminus leftlbrace c rightrbrace to mathbb R$ be a real function.

Let $L in mathbb R$.


$f left(   right)x$ tends to the limit $L$ as $x$ tends to $c$  if and only if :

:$forall epsilon in mathbb R_{>0}: exists delta in mathbb R_{>0}: forall x in mathbb R: 0 < leftlvert x - c rightrvert < delta implies leftlvert f left(   right)x - L rightrvert < epsilon$

where $mathbb R_{>0}$ denotes the set of strictly positive real numbers.

=== Definition 2 ===
Let $left( a ,.,.,   right)b$ be an open real interval.

Let $c in left( a ,.,.,   right)b$.

Let $f: left( a ,.,.,   right)b setminus leftlbrace c rightrbrace to mathbb R$ be a real function.

Let $L in mathbb R$.


$f left(   right)x$ tends to the limit $L$ as $x$ tends to $c$  if and only if :
:$forall epsilon in mathbb R_{>0}: exists delta in mathbb R_{>0}: x in N_delta left(   right)c setminus leftlbrace c rightrbrace implies f left(   right)x in N_epsilon left(   right)L$
where:
:$N_epsilon left(   right)L$ denotes the $epsilon$-neighborhood of $L$
:$N_delta left(   right)c setminus leftlbrace c rightrbrace$ denotes the deleted $delta$-neighborhood of $c$
:$mathbb R_{>0}$ denotes the set of strictly positive real numbers.


That is:
:For every (strictly) positive real number $epsilon$, there exists a (strictly) positive real number $delta$ such that every real number $x ne c$ in the domain of $f$ within $delta$ of $c$ has an image within $epsilon$ of $L$.


$epsilon$ is usually considered as having the connotation of being ""small"" in magnitude, but this is a misunderstanding of its intent: the point is that (in this context) $epsilon$ can be made arbitrarily small.


:

It can directly be seen that this definition is the same as that for a general metric space.",Definition:Limit of Real Function,,false,"Let ( a  . . )b be an open real interval.

Let c ∈( a  . . )b.

Let f: ( a  . . )b ∖{ c }→ℝ be a real function.

Let L ∈ℝ.


=== Definition 1 ===
Let ( a  . . )b be an open real interval.

Let c ∈( a  . . )b.

Let f: ( a  . . )b ∖{ c }→ℝ be a real function.

Let L ∈ℝ.


f (   )x tends to the limit L as x tends to c  if and only if :

:∀ϵ∈ℝ_>0: ∃δ∈ℝ_>0: ∀ x ∈ℝ: 0 < | x - c | < δ| f (   )x - L | < ϵ

where ℝ_>0 denotes the set of strictly positive real numbers.

=== Definition 2 ===
Let ( a  . . )b be an open real interval.

Let c ∈( a  . . )b.

Let f: ( a  . . )b ∖{ c }→ℝ be a real function.

Let L ∈ℝ.


f (   )x tends to the limit L as x tends to c  if and only if :
:∀ϵ∈ℝ_>0: ∃δ∈ℝ_>0: x ∈ N_δ(   )c ∖{ c } f (   )x ∈ N_ϵ(   )L
where:
:N_ϵ(   )L denotes the ϵ-neighborhood of L
:N_δ(   )c ∖{ c } denotes the deleted δ-neighborhood of c
:ℝ_>0 denotes the set of strictly positive real numbers.


That is:
:For every (strictly) positive real number ϵ, there exists a (strictly) positive real number δ such that every real number x  c in the domain of f within δ of c has an image within ϵ of L.


ϵ is usually considered as having the connotation of being ""small"" in magnitude, but this is a misunderstanding of its intent: the point is that (in this context) ϵ can be made arbitrarily small.


:

It can directly be seen that this definition is the same as that for a general metric space.",Limit
"['Definitions/Limits of Complex Functions', 'Definitions/Limits of Mappings', 'Definitions/Limits', 'Definitions/Complex Analysis']",Definition:Limit,"The definition for the limit of a complex function is exactly the same as that for the general metric space.


Let $A_1, A_2 subseteq mathbb C$ be subsets of the complex plane.

Let $c$ be a limit point of $A_1$.

Let $f: A_1 to A_2$ be a complex function from $A_1$ to $A_2$ defined everywhere on $A_1$ except possibly at $c$.


Let $L in A_2$.


Then $f left(   right)z$ is said to tend to the limit $L$ as $z$ tends to $c$, and we write:
:$f left(   right)z to L$ as $z to c$

or
:$ds lim_{z mathop to c} f left(   right)z = L$

if the following equivalent conditions hold.


This is voiced:
: the limit of $f left(   right)z$ as $z$ tends to $c$.


=== Epsilon-Delta Condition ===

:$forall epsilon in mathbb R_{>0}: exists delta in mathbb R_{>0}: forall z in A_1: 0 < leftlvert z - c rightrvert < delta implies leftlvert f left(   right)z - L rightrvert < epsilon$


That is, for every real positive $epsilon$ there exists a real positive $delta$ such that every point in the domain of $f$ within $delta$ of $c$ has an image within $epsilon$ of some point $L$ in the codomain of $f$.


=== Epsilon-Neighborhood Condition ===

:$forall N_epsilon left(   right)L: exists N_delta left(   right)c setminus leftlbrace c rightrbrace: f left(   right){N_delta left(   right)c setminus leftlbrace c rightrbrace} subseteq N_epsilon left(   right)L$

where:
:$N_delta left(   right)c setminus leftlbrace c rightrbrace$ is the deleted $delta $-neighborhood of $c$ in $M_1$;
:$N_epsilon left(   right)L$ is the $epsilon$-neighborhood of $L$ in $M_2$.


That is, for every $epsilon$-neighborhood of $L$ in $A_2$, there exists a deleted $delta$-neighborhood of $c$ in $A_1$ whose image is a subset of that $epsilon$-neighborhood.",Definition:Limit of Complex Function,,false,"The definition for the limit of a complex function is exactly the same as that for the general metric space.


Let A_1, A_2 ⊆ℂ be subsets of the complex plane.

Let c be a limit point of A_1.

Let f: A_1 → A_2 be a complex function from A_1 to A_2 defined everywhere on A_1 except possibly at c.


Let L ∈ A_2.


Then f (   )z is said to tend to the limit L as z tends to c, and we write:
:f (   )z → L as z → c

or
:lim_z → c f (   )z = L

if the following equivalent conditions hold.


This is voiced:
: the limit of f (   )z as z tends to c.


=== Epsilon-Delta Condition ===

:∀ϵ∈ℝ_>0: ∃δ∈ℝ_>0: ∀ z ∈ A_1: 0 < | z - c | < δ| f (   )z - L | < ϵ


That is, for every real positive ϵ there exists a real positive δ such that every point in the domain of f within δ of c has an image within ϵ of some point L in the codomain of f.


=== Epsilon-Neighborhood Condition ===

:∀ N_ϵ(   )L: ∃ N_δ(   )c ∖{ c }: f (   )N_δ(   )c ∖{ c }⊆ N_ϵ(   )L

where:
:N_δ(   )c ∖{ c } is the deleted δ-neighborhood of c in M_1;
:N_ϵ(   )L is the ϵ-neighborhood of L in M_2.


That is, for every ϵ-neighborhood of L in A_2, there exists a deleted δ-neighborhood of c in A_1 whose image is a subset of that ϵ-neighborhood.",Limit
"['Definitions/Limits Superior', 'Definitions/Real Analysis']",Definition:Limit,"Let $leftlangle x_n rightrangle$ be a bounded sequence in $mathbb R$.


=== Definition 1 ===
Let $leftlangle x_n rightrangle$ be a bounded sequence in $mathbb R$.

Let $L$ be the set of all real numbers which are the limit of some subsequence of $leftlangle x_n rightrangle$.


From Existence of Maximum and Minimum of Bounded Sequence, $L$ has a maximum.

This maximum is called the limit superior.

It can be denoted:
:$ds limsup_{n mathop to infty}  left(   right){x_n} = overline l$

=== Definition 2 ===
Let $leftlangle x_n rightrangle$ be a bounded sequence in $mathbb R$.


The limit superior of $leftlangle x_n rightrangle$ is defined and denoted as:
:$ds limsup_{n mathop to infty}  left(   right){x_n} = inf leftlbrace sup_{m mathop ge n} x_m: n in mathbb N rightrbrace$",Definition:Limit Superior,,false,"Let ⟨ x_n ⟩ be a bounded sequence in ℝ.


=== Definition 1 ===
Let ⟨ x_n ⟩ be a bounded sequence in ℝ.

Let L be the set of all real numbers which are the limit of some subsequence of ⟨ x_n ⟩.


From Existence of Maximum and Minimum of Bounded Sequence, L has a maximum.

This maximum is called the limit superior.

It can be denoted:
:lim sup_n →∞(   )x_n = l

=== Definition 2 ===
Let ⟨ x_n ⟩ be a bounded sequence in ℝ.


The limit superior of ⟨ x_n ⟩ is defined and denoted as:
:lim sup_n →∞(   )x_n = inf{sup_m ≥ n x_m: n ∈ℕ}",Limit
"['Definitions/Limits Inferior', 'Definitions/Real Analysis']",Definition:Limit,"Let $leftlangle x_n rightrangle$ be a bounded sequence in $mathbb R$.


=== Definition 1 ===
Let $leftlangle x_n rightrangle$ be a bounded sequence in $mathbb R$.

Let $L$ be the set of all real numbers which are the limit of some subsequence of $leftlangle x_n rightrangle$.


From Existence of Maximum and Minimum of Bounded Sequence, $L$ has a minimum.

This minimum is called the limit inferior.

It can be denoted:
:$ds liminf_{n mathop to infty}  left(   right){x_n} = underline l$

=== Definition 2 ===
Let $leftlangle x_n rightrangle$ be a bounded sequence in $mathbb R$.


The limit inferior of $leftlangle x_n rightrangle$ is defined and denoted as:
:$ds liminf_{n mathop to infty}  left(   right){x_n} = sup leftlbrace inf_{m mathop ge n} x_m: n in mathbb N rightrbrace$",Definition:Limit Inferior,,false,"Let ⟨ x_n ⟩ be a bounded sequence in ℝ.


=== Definition 1 ===
Let ⟨ x_n ⟩ be a bounded sequence in ℝ.

Let L be the set of all real numbers which are the limit of some subsequence of ⟨ x_n ⟩.


From Existence of Maximum and Minimum of Bounded Sequence, L has a minimum.

This minimum is called the limit inferior.

It can be denoted:
:lim inf_n →∞(   )x_n = l

=== Definition 2 ===
Let ⟨ x_n ⟩ be a bounded sequence in ℝ.


The limit inferior of ⟨ x_n ⟩ is defined and denoted as:
:lim inf_n →∞(   )x_n = sup{inf_m ≥ n x_m: n ∈ℕ}",Limit
"['Definitions/Convergence', 'Definitions/Power Series', 'Definitions/Real Analysis']",Definition:Limit,"Let $xi in mathbb R$ be a real number.

Let $ds S left(   right)x = sum_{n mathop = 0}^infty a_n left( x - xi right)^n$ be a power series about $xi$.

Let $I$ be the interval of convergence of $S left(   right)x$.

Let the endpoints of $I$ be $xi - R$ and $xi + R$.

(This follows from the fact that $xi$ is the midpoint of $I$.)


Then $R$ is called the radius of convergence of $S left(   right)x$.


If $S left(   right)x$ is convergent over the whole of $mathbb R$, then $I = mathbb R$ and thus the radius of convergence is infinite.",Definition:Radius of Convergence/Real Domain,,false,"Let ξ∈ℝ be a real number.

Let S (   )x = ∑_n  = 0^∞ a_n ( x - ξ)^n be a power series about ξ.

Let I be the interval of convergence of S (   )x.

Let the endpoints of I be ξ - R and ξ + R.

(This follows from the fact that ξ is the midpoint of I.)


Then R is called the radius of convergence of S (   )x.


If S (   )x is convergent over the whole of ℝ, then I = ℝ and thus the radius of convergence is infinite.",Limit
['Definitions/Measure Theory'],Definition:Limit,"Let $Bbb S = leftlbrace E_n : n in mathbb N rightrbrace$ be a sequence of sets.

Let the limit superior of $Bbb S$ be equal to the limit inferior of $Bbb S$.


Then the limit of $Bbb S$, denoted $ds lim_{n mathop to infty} E_n$, is defined as:

:$ds lim_{n mathop to infty} E_n := limsup_{n mathop to infty} E_n$
and so also:
:$ds lim_{n mathop to infty} E_n := liminf_{n mathop to infty} E_n$

and $Bbb S$ converges to the limit.",Definition:Limit of Sets,,false,"Let S = { E_n : n ∈ℕ} be a sequence of sets.

Let the limit superior of S be equal to the limit inferior of S.


Then the limit of S, denoted lim_n →∞ E_n, is defined as:

:lim_n →∞ E_n := lim sup_n →∞ E_n
and so also:
:lim_n →∞ E_n := lim inf_n →∞ E_n

and S converges to the limit.",Limit
"['Definitions/Category Theory', 'Definitions/Limits and Colimits']",Definition:Limit,"Let $mathbf C$ be a metacategory.

Let $D: mathbf J to mathbf C$ be a $mathbf J$-diagram in $mathbf C$.

Let $mathbf{Cone} left({D}right)$ be the category of cones to $D$.


A limit for $D$ is a terminal object in $mathbf{Cone} left({D}right)$.
 

It is denoted by $varprojlim_j D_j$; the associated morphisms $p_i: varprojlim_j D_j to D_i$ are usually left implicit.


=== Finite Limit ===
Let $mathbf C$ be a metacategory.

Let $D: mathbf J to mathbf C$ be a $mathbf J$-diagram in $mathbf C$.

Let $varprojlim_j D_j$ be a limit for $D$.


Then $varprojlim_j D_j$ is called a finite limit  if and only if  $mathbf J$ is a finite category.",Definition:Limit (Category Theory),,false,"Let 𝐂 be a metacategory.

Let D: 𝐉→𝐂 be a 𝐉-diagram in 𝐂.

Let 𝐂𝐨𝐧𝐞(D) be the category of cones to D.


A limit for D is a terminal object in 𝐂𝐨𝐧𝐞(D).
 

It is denoted by _j D_j; the associated morphisms p_i: _j D_j → D_i are usually left implicit.


=== Finite Limit ===
Let 𝐂 be a metacategory.

Let D: 𝐉→𝐂 be a 𝐉-diagram in 𝐂.

Let _j D_j be a limit for D.


Then _j D_j is called a finite limit  if and only if  𝐉 is a finite category.",Limit
['Definitions/Continued Fractions'],Definition:Limit,"Let $left( F, leftlVert ,cdot, rightrVert  right)$ be a valued field.

Let $C = leftlangle a_n rightrangle_{n mathop ge 0}$ be a infinite continued fraction in $F$.


Let $C$ converge to $x in F$:

Then $x$ is the value of $C$.",Definition:Value of Continued Fraction/Infinite,,false,"Let ( F, ‖ · ‖) be a valued field.

Let C = ⟨ a_n ⟩_n ≥ 0 be a infinite continued fraction in F.


Let C converge to x ∈ F:

Then x is the value of C.",Limit
"['Definitions/Ordinals', 'Definitions/Limit Ordinals']",Definition:Limit,"=== Definition 1 ===
An ordinal $lambda$ is a limit ordinal  if and only if  it is a limit element in the well-ordering on the class of all ordinals $operatorname {On}$ that is the subset relation.

=== Definition 2 ===
An ordinal $lambda$ is a limit ordinal  if and only if  it is neither the zero ordinal nor a successor ordinal.",Definition:Limit Ordinal,,false,"=== Definition 1 ===
An ordinal λ is a limit ordinal  if and only if  it is a limit element in the well-ordering on the class of all ordinals On that is the subset relation.

=== Definition 2 ===
An ordinal λ is a limit ordinal  if and only if  it is neither the zero ordinal nor a successor ordinal.",Limit
['Definitions/Well-Orderings'],Definition:Limit,"Let $A$ be a class.

Let $preccurlyeq$ be a well-ordering on $A$.

Let $x$ be neither the smallest element of $A$ nor an immediate successor of any element of $A$.


Then $x$ is a limit element of $A$ (under $preccurlyeq$).",Definition:Limit Element under Well-Ordering,,false,"Let A be a class.

Let ≼ be a well-ordering on A.

Let x be neither the smallest element of A nor an immediate successor of any element of A.


Then x is a limit element of A (under ≼).",Limit
"['Definitions/Linear Algebra', 'Definitions/Algebra', 'Definitions/Linearity', 'Definitions/Branches of Mathematics']",Definition:Linear Algebra,Linear algebra is the branch of algebra which studies vector spaces and linear transformations between them.,Definition:Linear Algebra (Mathematical Branch),algebra,true,Linear algebra is the branch of algebra which studies vector spaces and linear transformations between them.,Linear Algebra
"['Definitions/Algebras over Fields', 'Definitions/Algebras', 'Definitions/Field Theory']",Definition:Linear Algebra,"Let $F$ be a field.


An algebra over $F$ is an ordered pair $left( A, * right)$ where:
:$A$ is a vector space over $F$
:$* : A^2 to A$ is a bilinear mapping


That is, it is an algebra $left( A, * right)$ over the ring $F$ where:
:$F$ is a field
:the $F$-module $A$ is a vector space.


The symbol $A$ is often used for such an algebra, more so as the level of abstraction increases.


=== Multiplication ===
Let $F$ be a field.

Let $left( A, * right)$ be an algebra over $F$ such that:
:$A$ is a vector space over $F$
:$* : A^2 to A$ is a bilinear mapping.


The bilinear mapping $*$ is often referred to as multiplication.",Definition:Algebra over Field,,false,"Let F be a field.


An algebra over F is an ordered pair ( A, * ) where:
:A is a vector space over F
:* : A^2 → A is a bilinear mapping


That is, it is an algebra ( A, * ) over the ring F where:
:F is a field
:the F-module A is a vector space.


The symbol A is often used for such an algebra, more so as the level of abstraction increases.


=== Multiplication ===
Let F be a field.

Let ( A, * ) be an algebra over F such that:
:A is a vector space over F
:* : A^2 → A is a bilinear mapping.


The bilinear mapping * is often referred to as multiplication.",Linear Algebra
"['Definitions/Linear Forms (Linear Algebra)', 'Definitions/Linear Algebra', 'Definitions/Linear Transformations', 'Definitions/Linear Forms', 'Definitions/Linearity']",Definition:Linear Form,"Let $left( R, +, times right)$ be a commutative ring.

Let $left( R, +_R, circ right)_R$ denote the $R$-module $R$.

Let $left( G, +_G, circ right)_R$ be a module over $R$.


Let $phi: left( G, +_G, circ right)_R to left( R, +_R, circ right)_R$ be a linear transformation from $G$ to the $R$-module $R$.


$phi$ is called a linear form on $G$.",Definition:Linear Form (Linear Algebra),,false,"Let ( R, +, ×) be a commutative ring.

Let ( R, +_R, ∘)_R denote the R-module R.

Let ( G, +_G, ∘)_R be a module over R.


Let ϕ: ( G, +_G, ∘)_R →( R, +_R, ∘)_R be a linear transformation from G to the R-module R.


ϕ is called a linear form on G.",Linear Form
"['Definitions/Linear Forms (Polynomial Theory)', 'Definitions/Forms', 'Definitions/Polynomial Theory', 'Definitions/Linear Forms', 'Definitions/Linearity']",Definition:Linear Form,A linear form is a form whose variables are of degree $1$.,Definition:Linear Form (Polynomial Theory),,false,A linear form is a form whose variables are of degree 1.,Linear Form
['Definitions/Topology'],Definition:Locally Finite,"Let $T = left( S, tau right)$ be a topological space.

Let $mathcal F$ be a set of subsets of $S$.


Then $mathcal F$ is locally finite  if and only if  each element of $S$ has a neighborhood which intersects a finite number of sets in $mathcal F$.",Definition:Locally Finite Set of Subsets,,false,"Let T = ( S, τ) be a topological space.

Let ℱ be a set of subsets of S.


Then ℱ is locally finite  if and only if  each element of S has a neighborhood which intersects a finite number of sets in ℱ.",Locally Finite
['Definitions/Covers'],Definition:Locally Finite,"Let $T = left( S, tau right)$ be a topological space.

Let $mathcal C$ be a cover of $S$.


Then $mathcal C$ is locally finite  if and only if  each element of $S$ has a neighborhood which intersects a finite number of sets in $mathcal C$.",Definition:Locally Finite Cover,,false,"Let T = ( S, τ) be a topological space.

Let 𝒞 be a cover of S.


Then 𝒞 is locally finite  if and only if  each element of S has a neighborhood which intersects a finite number of sets in 𝒞.",Locally Finite
['Definitions/Graph Theory'],Definition:Locally Finite,A locally finite graph $G$ is an infinite graph where every vertex of $G$ has finite degree.,Definition:Locally Finite Graph,,false,A locally finite graph G is an infinite graph where every vertex of G has finite degree.,Locally Finite
"['Definitions/Loops (Plane Geometry)', 'Definitions/Plane Curves']",Definition:Loop,"A loop is a part of a plane curve that intersects itself.

Hence it encloses a bounded set of points.",Definition:Loop (Plane Geometry),,false,"A loop is a part of a plane curve that intersects itself.

Hence it encloses a bounded set of points.",Loop
"['Definitions/Loops (Graph Theory)', 'Definitions/Loop-Graphs']",Definition:Loop,"Let $G = left( V, E right)$ be a loop-graph.

A loop is an edge $e$ of $G$ whose endvertices are the same vertex.


Thus a loop $e$ on the vertex $v$ would be written:
:$e = vv$",Definition:Loop (Graph Theory),,false,"Let G = ( V, E ) be a loop-graph.

A loop is an edge e of G whose endvertices are the same vertex.


Thus a loop e on the vertex v would be written:
:e = vv",Loop
['Definitions/Abstract Algebra'],Definition:Loop,"An algebra loop $left( S, circ right)$ is a quasigroup with an identity element.
:$exists e in S: forall x in S: x circ e = x = e circ x$",Definition:Algebra Loop,,false,"An algebra loop ( S, ∘) is a quasigroup with an identity element.
:∃ e ∈ S: ∀ x ∈ S: x ∘ e = x = e ∘ x",Loop
['Definitions/Relation Theory'],Definition:Loop,"Let $mathcal R$ be a relation on a set $S$.

Let $a_1, a_2, ldots a_n$ be elements of $S$.


A relational loop on $S$ takes the form:

:$left( a_1 mathrel mathcal R a_2 land a_2 mathrel mathcal R a_3 dots land a_{n - 1} mathrel mathcal R a_n land a_n mathrel mathcal R a_1 right)$

That is, it is a subset of $mathcal R$ of the form:
:$leftlbrace left( a_1, a_2 right), left( a_2, a_3 right), ldots, left( a_{n - 1}, a_n right), left( a_n, a_1 right)  rightrbrace$",Definition:Relational Loop,,false,"Let ℛ be a relation on a set S.

Let a_1, a_2, … a_n be elements of S.


A relational loop on S takes the form:

:( a_1 ℛ a_2  a_2 ℛ a_3 … a_n - 1ℛ a_n  a_n ℛ a_1 )

That is, it is a subset of ℛ of the form:
:{( a_1, a_2 ), ( a_2, a_3 ), …, ( a_n - 1, a_n ), ( a_n, a_1 )  }",Loop
"['Definitions/Loops (Topology)', 'Definitions/Topology']",Definition:Loop,"Let $T = left( S, tau right)$ be a topological space.

Let $gamma: left[ 0 ,.,.,   right]1 to S$ be a path in $T$.

Let $gamma left(   right)0 = gamma left(   right)1$.


Then $gamma$ is called a loop (in $T$).


=== Simple Loop ===
Let $T = left( S, tau right)$ be a topological space.

Let $gamma: left[ 0 ,.,.,   right]1 to S$ be a path in $T$.


$gamma$ is a simple loop (in $T$)  if and only if :
:$gamma left(   right){t_1} ne gamma left(   right){t_2}$ for all $t_1 ,t_2 in left[ 0 ,.,.,   right)1$ with $t_1 ne t_2$
:$gamma left(   right)0 = gamma left(   right)1$

=== Base Point ===
Let $X$ be a topological space.

Let $gamma: left[ 0 ,.,.,   right]1 to X$ be a loop.


The base point of $gamma$ is $gamma left(   right)0$.

In other words, $gamma$ is said to be based at $gamma left(   right)0$.

=== Set of All Loops ===
Let $T$ be a topological space.


The set of all loops based at $p in T$ is denoted by $Omega left(   right){T, p}$.

=== Constant Loop ===
Let $T$ be a topological space.

Let $p in T$.

Let $Omega left(   right){T, p}$ denote the set of all loops based at $p$.


A constant loop $c_p$ is the loop $c_p in Omega left(   right){T, p}$ such that:

:$forall t in left[ 0 ,.,.,   right]1 : c_p left(   right)t = p$

=== Null-Homotopic Loop ===
Let $T = left( S, tau right)$ be a topological space.

Let $gamma$ be a loop in $T$.

Suppose $gamma$ is path-homotopic to a constant loop.


Then $gamma$ is said to be null-homotopic.

=== Circle Representative of Loop ===
Let $T = left( S, tau right)$ be a topological space.

Let $gamma: left[ 0 ,.,.,   right]1 to S$ be a loop in $T$.

Let $Bbb S^1 subseteq mathbb C$ be the unit circle in $mathbb C$:

:$Bbb S^1 = leftlbrace z in mathbb C : leftlvert z rightrvert = 1 rightrbrace$

Suppose $omega : left[ 0 ,.,.,   right]1 to Bbb S^1$ such that $omega left(   right)s = exp left(   right){2 pi i s}$.


Then the unique map $tilde f : Bbb S^1 to T$ such that $tilde f circ omega = f$ is called the circle representative of $f$.

=== Loop in Topological Manifold ===
Let $M$ be a topological manifold.

Let $sigma : left[ 0 ,.,.,   right]1 to M$ be a continuous path.

Let $sigma left(   right)0 = sigma left(   right)1$.


Then $sigma$ is called a loop.",Definition:Loop (Topology),,false,"Let T = ( S, τ) be a topological space.

Let γ: [ 0  . . ]1 → S be a path in T.

Let γ(   )0 = γ(   )1.


Then γ is called a loop (in T).


=== Simple Loop ===
Let T = ( S, τ) be a topological space.

Let γ: [ 0  . . ]1 → S be a path in T.


γ is a simple loop (in T)  if and only if :
:γ(   )t_1γ(   )t_2 for all t_1 ,t_2 ∈[ 0  . . )1 with t_1  t_2
:γ(   )0 = γ(   )1

=== Base Point ===
Let X be a topological space.

Let γ: [ 0  . . ]1 → X be a loop.


The base point of γ is γ(   )0.

In other words, γ is said to be based at γ(   )0.

=== Set of All Loops ===
Let T be a topological space.


The set of all loops based at p ∈ T is denoted by Ω(   )T, p.

=== Constant Loop ===
Let T be a topological space.

Let p ∈ T.

Let Ω(   )T, p denote the set of all loops based at p.


A constant loop c_p is the loop c_p ∈Ω(   )T, p such that:

:∀ t ∈[ 0  . . ]1 : c_p (   )t = p

=== Null-Homotopic Loop ===
Let T = ( S, τ) be a topological space.

Let γ be a loop in T.

Suppose γ is path-homotopic to a constant loop.


Then γ is said to be null-homotopic.

=== Circle Representative of Loop ===
Let T = ( S, τ) be a topological space.

Let γ: [ 0  . . ]1 → S be a loop in T.

Let S^1 ⊆ℂ be the unit circle in ℂ:

:S^1 = { z ∈ℂ : | z | = 1 }

Suppose ω : [ 0  . . ]1 → S^1 such that ω(   )s = exp(   )2 π i s.


Then the unique map f̃ :  S^1 → T such that f̃∘ω = f is called the circle representative of f.

=== Loop in Topological Manifold ===
Let M be a topological manifold.

Let σ : [ 0  . . ]1 → M be a continuous path.

Let σ(   )0 = σ(   )1.


Then σ is called a loop.",Loop
['Definitions/Matroid Theory'],Definition:Loop,"Let $M = left( S, mathscr I right)$ be a matroid.


A loop of $M$ is an element $x$ of $S$ such that $leftlbrace x rightrbrace$ is a dependent subset of $S$.

That is, $x in S$ is a loop  if and only if  $leftlbrace x rightrbrace not in mathscr I$.",Definition:Loop (Matroid),,false,"Let M = ( S, ℐ) be a matroid.


A loop of M is an element x of S such that { x } is a dependent subset of S.

That is, x ∈ S is a loop  if and only if  { x }∉ℐ.",Loop
['Definitions/Boundedness'],Definition:Lower Bound,"Let $left( S, preceq right)$ be an ordered set.

Let $T$ be a subset of $S$.


A lower bound for $T$ (in $S$) is an element $m in S$ such that:
:$forall t in T: m preceq t$

That is, $m$ precedes every element of $T$.


=== Subset of Real Numbers ===

The concept is usually encountered where $left( S, preceq right)$ is the set of real numbers under the usual ordering $left( mathbb R, le right)$:

Let $mathbb R$ be the set of real numbers.

Let $T$ be a subset of $S$.


A lower bound for $T$ (in $mathbb R$) is an element $m in mathbb R$ such that:
:$forall t in T: m le t$",Definition:Lower Bound of Set,,false,"Let ( S, ≼) be an ordered set.

Let T be a subset of S.


A lower bound for T (in S) is an element m ∈ S such that:
:∀ t ∈ T: m ≼ t

That is, m precedes every element of T.


=== Subset of Real Numbers ===

The concept is usually encountered where ( S, ≼) is the set of real numbers under the usual ordering ( ℝ, ≤):

Let ℝ be the set of real numbers.

Let T be a subset of S.


A lower bound for T (in ℝ) is an element m ∈ℝ such that:
:∀ t ∈ T: m ≤ t",Lower Bound
['Definitions/Boundedness'],Definition:Lower Bound,"Let $mathbb R$ be the set of real numbers.

Let $T$ be a subset of $S$.


A lower bound for $T$ (in $mathbb R$) is an element $m in mathbb R$ such that:
:$forall t in T: m le t$",Definition:Lower Bound of Set/Real Numbers,,false,"Let ℝ be the set of real numbers.

Let T be a subset of S.


A lower bound for T (in ℝ) is an element m ∈ℝ such that:
:∀ t ∈ T: m ≤ t",Lower Bound
['Definitions/Boundedness'],Definition:Lower Bound,"Let $f: S to T$ be a mapping whose codomain is an ordered set $left( T, preceq right)$.


Let $f$ be bounded below in $T$ by $H in T$.


Then $H$ is a lower bound of $f$.


=== Real-Valued Function ===

The concept is usually encountered where $left( T, preceq right)$ is the set of real numbers under the usual ordering $left( mathbb R, le right)$:

Let $f: S to mathbb R$ be a real-valued function.


Let $f$ be bounded below in $T$ by $L in T$.


Then $L$ is a lower bound of $f$.


=== Lower Bound of Number ===
When considering the lower bound of a set of numbers, it is commonplace to ignore the set and instead refer just to the number itself.

Thus the construction:

:The set of numbers which fulfil the propositional function $P left({n}right)$ is bounded below with the lower bound $N$

would be reported as:

:The number $n$ such that $P left({n}right)$ has the lower bound $N$.


This construct obscures the details of what is actually being stated. Its use on   is considered an abuse of notation and so discouraged.


This also applies in the case where it is the lower bound of a mapping which is under discussion.


Category:Definitions/Numbers",Definition:Lower Bound of Mapping,,false,"Let f: S → T be a mapping whose codomain is an ordered set ( T, ≼).


Let f be bounded below in T by H ∈ T.


Then H is a lower bound of f.


=== Real-Valued Function ===

The concept is usually encountered where ( T, ≼) is the set of real numbers under the usual ordering ( ℝ, ≤):

Let f: S →ℝ be a real-valued function.


Let f be bounded below in T by L ∈ T.


Then L is a lower bound of f.


=== Lower Bound of Number ===
When considering the lower bound of a set of numbers, it is commonplace to ignore the set and instead refer just to the number itself.

Thus the construction:

:The set of numbers which fulfil the propositional function P (n) is bounded below with the lower bound N

would be reported as:

:The number n such that P (n) has the lower bound N.


This construct obscures the details of what is actually being stated. Its use on   is considered an abuse of notation and so discouraged.


This also applies in the case where it is the lower bound of a mapping which is under discussion.


Category:Definitions/Numbers",Lower Bound
['Definitions/Boundedness'],Definition:Lower Bound,"Let $f: S to mathbb R$ be a real-valued function.


Let $f$ be bounded below in $T$ by $L in T$.


Then $L$ is a lower bound of $f$.


=== Lower Bound of Number ===
When considering the lower bound of a set of numbers, it is commonplace to ignore the set and instead refer just to the number itself.

Thus the construction:

:The set of numbers which fulfil the propositional function $P left({n}right)$ is bounded below with the lower bound $N$

would be reported as:

:The number $n$ such that $P left({n}right)$ has the lower bound $N$.


This construct obscures the details of what is actually being stated. Its use on   is considered an abuse of notation and so discouraged.


This also applies in the case where it is the lower bound of a mapping which is under discussion.


Category:Definitions/Numbers",Definition:Lower Bound of Mapping/Real-Valued,,false,"Let f: S →ℝ be a real-valued function.


Let f be bounded below in T by L ∈ T.


Then L is a lower bound of f.


=== Lower Bound of Number ===
When considering the lower bound of a set of numbers, it is commonplace to ignore the set and instead refer just to the number itself.

Thus the construction:

:The set of numbers which fulfil the propositional function P (n) is bounded below with the lower bound N

would be reported as:

:The number n such that P (n) has the lower bound N.


This construct obscures the details of what is actually being stated. Its use on   is considered an abuse of notation and so discouraged.


This also applies in the case where it is the lower bound of a mapping which is under discussion.


Category:Definitions/Numbers",Lower Bound
['Definitions/Boundedness'],Definition:Lower Bound,"A special case of a lower bound of a mapping is a lower bound of a sequence, where the domain of the mapping is $mathbb N$.

Let $left( T, preceq right)$ be an ordered set.

Let $leftlangle x_n rightrangle$ be a sequence in $T$.


Let $leftlangle x_n rightrangle$ be bounded below in $T$ by $L in T$.


Then $L$ is a lower bound of $leftlangle x_n rightrangle$.


=== Real Sequence ===

The concept is usually encountered where $left( T, preceq right)$ is the set of real numbers under the usual ordering $left( mathbb R, le right)$:

Let $leftlangle x_n rightrangle$ be a real sequence.


Let $leftlangle x_n rightrangle$ be bounded below in $T$ by $L in mathbb R$.


Then $L$ is a lower bound of $leftlangle x_n rightrangle$.


=== Lower Bound of Number ===
When considering the lower bound of a set of numbers, it is commonplace to ignore the set and instead refer just to the number itself.

Thus the construction:

:The set of numbers which fulfil the propositional function $P left({n}right)$ is bounded below with the lower bound $N$

would be reported as:

:The number $n$ such that $P left({n}right)$ has the lower bound $N$.


This construct obscures the details of what is actually being stated. Its use on   is considered an abuse of notation and so discouraged.


This also applies in the case where it is the lower bound of a mapping which is under discussion.


Category:Definitions/Numbers",Definition:Lower Bound of Sequence,,false,"A special case of a lower bound of a mapping is a lower bound of a sequence, where the domain of the mapping is ℕ.

Let ( T, ≼) be an ordered set.

Let ⟨ x_n ⟩ be a sequence in T.


Let ⟨ x_n ⟩ be bounded below in T by L ∈ T.


Then L is a lower bound of ⟨ x_n ⟩.


=== Real Sequence ===

The concept is usually encountered where ( T, ≼) is the set of real numbers under the usual ordering ( ℝ, ≤):

Let ⟨ x_n ⟩ be a real sequence.


Let ⟨ x_n ⟩ be bounded below in T by L ∈ℝ.


Then L is a lower bound of ⟨ x_n ⟩.


=== Lower Bound of Number ===
When considering the lower bound of a set of numbers, it is commonplace to ignore the set and instead refer just to the number itself.

Thus the construction:

:The set of numbers which fulfil the propositional function P (n) is bounded below with the lower bound N

would be reported as:

:The number n such that P (n) has the lower bound N.


This construct obscures the details of what is actually being stated. Its use on   is considered an abuse of notation and so discouraged.


This also applies in the case where it is the lower bound of a mapping which is under discussion.


Category:Definitions/Numbers",Lower Bound
"['Definitions/Boundedness', 'Definitions/Sequences']",Definition:Lower Bound,"Let $leftlangle x_n rightrangle$ be a real sequence.


Let $leftlangle x_n rightrangle$ be bounded below in $T$ by $L in mathbb R$.


Then $L$ is a lower bound of $leftlangle x_n rightrangle$.


=== Lower Bound of Number ===
When considering the lower bound of a set of numbers, it is commonplace to ignore the set and instead refer just to the number itself.

Thus the construction:

:The set of numbers which fulfil the propositional function $P left({n}right)$ is bounded below with the lower bound $N$

would be reported as:

:The number $n$ such that $P left({n}right)$ has the lower bound $N$.


This construct obscures the details of what is actually being stated. Its use on   is considered an abuse of notation and so discouraged.


This also applies in the case where it is the lower bound of a mapping which is under discussion.


Category:Definitions/Numbers",Definition:Lower Bound of Sequence/Real,,false,"Let ⟨ x_n ⟩ be a real sequence.


Let ⟨ x_n ⟩ be bounded below in T by L ∈ℝ.


Then L is a lower bound of ⟨ x_n ⟩.


=== Lower Bound of Number ===
When considering the lower bound of a set of numbers, it is commonplace to ignore the set and instead refer just to the number itself.

Thus the construction:

:The set of numbers which fulfil the propositional function P (n) is bounded below with the lower bound N

would be reported as:

:The number n such that P (n) has the lower bound N.


This construct obscures the details of what is actually being stated. Its use on   is considered an abuse of notation and so discouraged.


This also applies in the case where it is the lower bound of a mapping which is under discussion.


Category:Definitions/Numbers",Lower Bound
['Definitions/Machines'],Definition:Machine,"A machine, in the context of mechanics, is a physical artefact which takes energy in a particular form and converts it into energy in another form for a specific purpose.",Definition:Machine (Mechanics),,false,"A machine, in the context of mechanics, is a physical artefact which takes energy in a particular form and converts it into energy in another form for a specific purpose.",Machine
['Definitions/Abstract Machines'],Definition:Machine,"A finite state machine is an ordered tuple:

: $F = left({ S, A, I, unicode{x3a3}, T }right)$

where:

: $S$ is the (finite) set of states
: $A subseteq S$ is the set of accepting states
: $I in S$ is the initial state
: $unicode{x3a3}$ is the alphabet of symbols that can be fed into the machine
: $T : left({ S times unicode{x3a3} }right) rightarrow S$ is the transition function.


A finite state machine operates as follows:

:$(1): quad$ At the beginning, the current state $s$ of the finite state machine is $I$.
:$(2): quad$ One by one, the input (a sequence of symbols from $unicode{x3a3}$) is fed into the machine.
:$(3): quad$ After each input symbol $sigma$, the current state $s$ is set to the result of $Tleft({s, sigma}right)$.


If, at the end of processing an input word $w$, $s in A$, the finite state machine is said to accept $w$, otherwise to reject it.


The set of words $w$ accepted by the machine $F$ is called the accepted language $Lleft({F}right)$.

Category:Definitions/Abstract Machines",Definition:Finite State Machine,,false,"A finite state machine is an ordered tuple:

: F = ( S, A, I, x3a3, T )

where:

: S is the (finite) set of states
: A ⊆ S is the set of accepting states
: I ∈ S is the initial state
: x3a3 is the alphabet of symbols that can be fed into the machine
: T : ( S ×x3a3) → S is the transition function.


A finite state machine operates as follows:

:(1): At the beginning, the current state s of the finite state machine is I.
:(2): One by one, the input (a sequence of symbols from x3a3) is fed into the machine.
:(3): After each input symbol σ, the current state s is set to the result of T(s, σ).


If, at the end of processing an input word w, s ∈ A, the finite state machine is said to accept w, otherwise to reject it.


The set of words w accepted by the machine F is called the accepted language L(F).

Category:Definitions/Abstract Machines",Machine
"['Definitions/Examples of Abstract Machines', 'Definitions/Unlimited Register Machines']",Definition:Machine,"An unlimited register machine, abbreviated URM, is an abstract machine with the following characteristics:


=== Registers ===
A URM has a sequence of registers which can store natural numbers: $leftlbrace 0, 1, 2, ldots rightrbrace$.

Any given URM program may make use of only a finite number of these registers.


Registers are usually referred to by the subscripted uppercase letters $R_1, R_2, R_3, ldots$.

The number held at any one time by a register is usually referred to by the corresponding lowercase letter $r_1, r_2, r_3, ldots$.


The registers are unlimited in the following two senses:
:$(1): quad$ Although a URM program may make use of only a finite number of registers, there is no actual upper bound on how many a particular URM program can actually use.
:$(2): quad$ There is no upper bound on the size of the natural numbers that may be stored in any register.


=== Index of Register ===
The subscript (which is a natural number) appended to a URM register is called the index of that register.

Hence, for example, the index of register $R_5$ is $5$.

=== Program ===
The numbers held in the registers of a URM are manipulated according to a program.

A URM program is a finite sequence of basic instructions.


=== Basic Instruction ===
The basic instructions of a URM program form a finite sequence and hence can be considered a set indexed by the (positive) integer $1, 2, 3, ldots$.


The basic instructions are as follows:


Name     	Notation          	Effect                     	Description
Zero     	$Z left(   right)n$        	$0 to R_n$                	Replace the number in $R_n$ by $0$.
Successor	$S left(   right)n$        	$r_n + 1 to R_n$          	Add $1$ to the number in $R_n$.
Copy     	$C left(   right){m, n}$   	$r_m to R_n$              	Replace the number in $R_n$ by the number in $R_m$ (leaving the one in $R_m$ as it was).
Jump     	$J left(   right){m, n, q}$	$r_m = r_n ? Rightarrow q$	If the numbers in $R_m$ and $R_n$ are equal, go to instruction number $q$, otherwise go to the next instruction.



=== Execution ===
The operation of carrying out a basic URM instruction is referred to as execution.

=== Execution ===
The operation of carrying out a basic URM instruction is referred to as execution.

=== Line Number ===
For historical reasons, the index of an instruction in a given URM program is called its line number.

We can refer either to the line of the program or the line in the URM.

=== Set of All URM Programs ===
It is convenient to use $Bbb U$ to stand for the set of all URM programs.

=== Length of Program ===
Let $Bbb U$ denote the set of all URM programs.

Let $P in Bbb U$ be a URM program.

By definition, $P$ is a finite sequence of basic instructions.


We define the function $lambda: Bbb U to mathbb N$ as follows:
:$forall P in Bbb U: lambda left(   right)P = $ the number of basic instructions that comprise $P$

Thus $lambda left(   right)P$ is referred to as the length of $P$.

=== Highest Register ===
Let $Bbb U$ denote the set of all URM programs.

Let $P in Bbb U$ be a URM program.

By definition, $P$ uses a finite number of registers.


We define the function $rho: Bbb U to mathbb N$ as follows:
:$forall P in Bbb U: rho left(   right)P =$ the highest register number used by $P$

That is, in any URM program $P$, no instruction refers to any register with index greater than $rho left(   right)P$.

=== Highest Register ===
Let $Bbb U$ denote the set of all URM programs.

Let $P in Bbb U$ be a URM program.

By definition, $P$ uses a finite number of registers.


We define the function $rho: Bbb U to mathbb N$ as follows:
:$forall P in Bbb U: rho left(   right)P =$ the highest register number used by $P$

That is, in any URM program $P$, no instruction refers to any register with index greater than $rho left(   right)P$.

=== Termination ===
A URM program terminates when there are no more instructions to execute.

This can happen in either of two ways:
:$(1): quad$ If the program executes the last instruction, and this does not involve a Jump to an earlier instruction, the program will stop.
:$(2): quad$ If the program executes a Jump instruction to a non-existent instruction, the program will stop.


=== Exit Jump ===
 

Such a Jump instruction is known as an exit jump .

=== Exit Line ===
 

The line on which a particular run of a URM program stops is called the exit line.


=== Endless Loop ===
 

If a URM program, when running, never reaches a state where it terminates, then it is said to be in an endless loop and will never terminate.

 

Note that whether a program terminates or not may depend on its input.

It may terminate perfectly well for one input, but go into an endless loop on another.

=== Input ===

The input to a URM program is:
:either an ordered $k$-tuple $left( n_1, n_2, ldots, n_k right) in mathbb N^k$
:or a natural number $n in mathbb N$.


In the latter case, it is convenient to consider a single natural number as an ordered $1$-tuple $left( n_1 right) in mathbb N^1 = mathbb N$.

Hence we can discuss inputs to URM programs solely as instances of tuples, and not be concerned with cumbersome repetition for the cases where $k = 1$ and otherwise.


The convention usually used is for a URM program $P$ to start computation with:
:the input $left({n_1, n_2, ldots, n_k}right)$ in registers $R_1, R_2, ldots, R_k$
:$0$ in all other registers used by $P$.


That is, the initial state of the URM is:
:$forall i in left[ 1 ,.,.,   right]k: r_i = n_i$
:$forall i > k: r_i = 0$.


It is usual for the input (either all or part) to be overwritten during the course of the operation of a program. That is, at the end of a program, $R_1, R_2, ldots, R_k$ are not guaranteed still to contain $n_1, n_2, ldots, n_k$ unless the program has been explicitly written so as to ensure that this is the case.


=== Output ===

At the end of the running of a URM program, the output will be found in register $R_1$.

=== Operation ===
When a URM runs a program, it always starts by executing the first instruction of the program.

When it has executed an instruction, it moves to the next instruction and executes that one, unless required otherwise by a Jump instruction.


=== Execution ===
The operation of carrying out a basic URM instruction is referred to as execution.

=== Instruction Pointer ===
The line number of the instruction which is currently about to be executed is known as the instruction pointer.

It can be imagined as a special-purpose register in the URM whose purpose is to hold that line number.

=== Stage of Computation ===
The stage of computation (or just stage) of a URM program is the count of how many instructions have been executed.

Thus each stage corresponds to the processing of one instruction.

=== State ===
The state of a URM program at a particular stage is defined as:
:$(1): quad$ the value of the instruction pointer
:$(2): quad$ the values contained by each of the registers that are used by the URM program.

=== Instruction Pointer ===
The line number of the instruction which is currently about to be executed is known as the instruction pointer.

It can be imagined as a special-purpose register in the URM whose purpose is to hold that line number.

=== Stage of Computation ===
The stage of computation (or just stage) of a URM program is the count of how many instructions have been executed.

Thus each stage corresponds to the processing of one instruction.

=== State ===
The state of a URM program at a particular stage is defined as:
:$(1): quad$ the value of the instruction pointer
:$(2): quad$ the values contained by each of the registers that are used by the URM program.

=== Input ===

The input to a URM program is:
:either an ordered $k$-tuple $left( n_1, n_2, ldots, n_k right) in mathbb N^k$
:or a natural number $n in mathbb N$.


In the latter case, it is convenient to consider a single natural number as an ordered $1$-tuple $left( n_1 right) in mathbb N^1 = mathbb N$.

Hence we can discuss inputs to URM programs solely as instances of tuples, and not be concerned with cumbersome repetition for the cases where $k = 1$ and otherwise.


The convention usually used is for a URM program $P$ to start computation with:
:the input $left({n_1, n_2, ldots, n_k}right)$ in registers $R_1, R_2, ldots, R_k$
:$0$ in all other registers used by $P$.


That is, the initial state of the URM is:
:$forall i in left[ 1 ,.,.,   right]k: r_i = n_i$
:$forall i > k: r_i = 0$.


It is usual for the input (either all or part) to be overwritten during the course of the operation of a program. That is, at the end of a program, $R_1, R_2, ldots, R_k$ are not guaranteed still to contain $n_1, n_2, ldots, n_k$ unless the program has been explicitly written so as to ensure that this is the case.


=== Output ===

At the end of the running of a URM program, the output will be found in register $R_1$.


=== Null Program ===
The null URM program is a URM program which contains no instructions.

That is, a URM program whose length is zero.",Definition:Unlimited Register Machine,,false,"An unlimited register machine, abbreviated URM, is an abstract machine with the following characteristics:


=== Registers ===
A URM has a sequence of registers which can store natural numbers: { 0, 1, 2, …}.

Any given URM program may make use of only a finite number of these registers.


Registers are usually referred to by the subscripted uppercase letters R_1, R_2, R_3, ….

The number held at any one time by a register is usually referred to by the corresponding lowercase letter r_1, r_2, r_3, ….


The registers are unlimited in the following two senses:
:(1): Although a URM program may make use of only a finite number of registers, there is no actual upper bound on how many a particular URM program can actually use.
:(2): There is no upper bound on the size of the natural numbers that may be stored in any register.


=== Index of Register ===
The subscript (which is a natural number) appended to a URM register is called the index of that register.

Hence, for example, the index of register R_5 is 5.

=== Program ===
The numbers held in the registers of a URM are manipulated according to a program.

A URM program is a finite sequence of basic instructions.


=== Basic Instruction ===
The basic instructions of a URM program form a finite sequence and hence can be considered a set indexed by the (positive) integer 1, 2, 3, ….


The basic instructions are as follows:


Name     	Notation          	Effect                     	Description
Zero     	Z (   )n        	0 → R_n                	Replace the number in R_n by 0.
Successor	S (   )n        	r_n + 1 → R_n          	Add 1 to the number in R_n.
Copy     	C (   )m, n   	r_m → R_n              	Replace the number in R_n by the number in R_m (leaving the one in R_m as it was).
Jump     	J (   )m, n, q	r_m = r_n ? ⇒ q	If the numbers in R_m and R_n are equal, go to instruction number q, otherwise go to the next instruction.



=== Execution ===
The operation of carrying out a basic URM instruction is referred to as execution.

=== Execution ===
The operation of carrying out a basic URM instruction is referred to as execution.

=== Line Number ===
For historical reasons, the index of an instruction in a given URM program is called its line number.

We can refer either to the line of the program or the line in the URM.

=== Set of All URM Programs ===
It is convenient to use U to stand for the set of all URM programs.

=== Length of Program ===
Let U denote the set of all URM programs.

Let P ∈ U be a URM program.

By definition, P is a finite sequence of basic instructions.


We define the function λ:  U →ℕ as follows:
:∀ P ∈ U: λ(   )P = the number of basic instructions that comprise P

Thus λ(   )P is referred to as the length of P.

=== Highest Register ===
Let U denote the set of all URM programs.

Let P ∈ U be a URM program.

By definition, P uses a finite number of registers.


We define the function ρ:  U →ℕ as follows:
:∀ P ∈ U: ρ(   )P = the highest register number used by P

That is, in any URM program P, no instruction refers to any register with index greater than ρ(   )P.

=== Highest Register ===
Let U denote the set of all URM programs.

Let P ∈ U be a URM program.

By definition, P uses a finite number of registers.


We define the function ρ:  U →ℕ as follows:
:∀ P ∈ U: ρ(   )P = the highest register number used by P

That is, in any URM program P, no instruction refers to any register with index greater than ρ(   )P.

=== Termination ===
A URM program terminates when there are no more instructions to execute.

This can happen in either of two ways:
:(1): If the program executes the last instruction, and this does not involve a Jump to an earlier instruction, the program will stop.
:(2): If the program executes a Jump instruction to a non-existent instruction, the program will stop.


=== Exit Jump ===
 

Such a Jump instruction is known as an exit jump .

=== Exit Line ===
 

The line on which a particular run of a URM program stops is called the exit line.


=== Endless Loop ===
 

If a URM program, when running, never reaches a state where it terminates, then it is said to be in an endless loop and will never terminate.

 

Note that whether a program terminates or not may depend on its input.

It may terminate perfectly well for one input, but go into an endless loop on another.

=== Input ===

The input to a URM program is:
:either an ordered k-tuple ( n_1, n_2, …, n_k ) ∈ℕ^k
:or a natural number n ∈ℕ.


In the latter case, it is convenient to consider a single natural number as an ordered 1-tuple ( n_1 ) ∈ℕ^1 = ℕ.

Hence we can discuss inputs to URM programs solely as instances of tuples, and not be concerned with cumbersome repetition for the cases where k = 1 and otherwise.


The convention usually used is for a URM program P to start computation with:
:the input (n_1, n_2, …, n_k) in registers R_1, R_2, …, R_k
:0 in all other registers used by P.


That is, the initial state of the URM is:
:∀ i ∈[ 1  . . ]k: r_i = n_i
:∀ i > k: r_i = 0.


It is usual for the input (either all or part) to be overwritten during the course of the operation of a program. That is, at the end of a program, R_1, R_2, …, R_k are not guaranteed still to contain n_1, n_2, …, n_k unless the program has been explicitly written so as to ensure that this is the case.


=== Output ===

At the end of the running of a URM program, the output will be found in register R_1.

=== Operation ===
When a URM runs a program, it always starts by executing the first instruction of the program.

When it has executed an instruction, it moves to the next instruction and executes that one, unless required otherwise by a Jump instruction.


=== Execution ===
The operation of carrying out a basic URM instruction is referred to as execution.

=== Instruction Pointer ===
The line number of the instruction which is currently about to be executed is known as the instruction pointer.

It can be imagined as a special-purpose register in the URM whose purpose is to hold that line number.

=== Stage of Computation ===
The stage of computation (or just stage) of a URM program is the count of how many instructions have been executed.

Thus each stage corresponds to the processing of one instruction.

=== State ===
The state of a URM program at a particular stage is defined as:
:(1): the value of the instruction pointer
:(2): the values contained by each of the registers that are used by the URM program.

=== Instruction Pointer ===
The line number of the instruction which is currently about to be executed is known as the instruction pointer.

It can be imagined as a special-purpose register in the URM whose purpose is to hold that line number.

=== Stage of Computation ===
The stage of computation (or just stage) of a URM program is the count of how many instructions have been executed.

Thus each stage corresponds to the processing of one instruction.

=== State ===
The state of a URM program at a particular stage is defined as:
:(1): the value of the instruction pointer
:(2): the values contained by each of the registers that are used by the URM program.

=== Input ===

The input to a URM program is:
:either an ordered k-tuple ( n_1, n_2, …, n_k ) ∈ℕ^k
:or a natural number n ∈ℕ.


In the latter case, it is convenient to consider a single natural number as an ordered 1-tuple ( n_1 ) ∈ℕ^1 = ℕ.

Hence we can discuss inputs to URM programs solely as instances of tuples, and not be concerned with cumbersome repetition for the cases where k = 1 and otherwise.


The convention usually used is for a URM program P to start computation with:
:the input (n_1, n_2, …, n_k) in registers R_1, R_2, …, R_k
:0 in all other registers used by P.


That is, the initial state of the URM is:
:∀ i ∈[ 1  . . ]k: r_i = n_i
:∀ i > k: r_i = 0.


It is usual for the input (either all or part) to be overwritten during the course of the operation of a program. That is, at the end of a program, R_1, R_2, …, R_k are not guaranteed still to contain n_1, n_2, …, n_k unless the program has been explicitly written so as to ensure that this is the case.


=== Output ===

At the end of the running of a URM program, the output will be found in register R_1.


=== Null Program ===
The null URM program is a URM program which contains no instructions.

That is, a URM program whose length is zero.",Machine
"['Definitions/Turing Machines', 'Definitions/Examples of Abstract Machines']",Definition:Machine,"A Turing machine is an abstract machine which works by manipulating symbols on an imaginary piece of paper by means of a specific set of algorithmic rules.

To simplify things, the piece of paper being worked on is in the form of a series of boxes on a one-dimensional ""tape"" divided into squares.

Each square can be either blank or can contain a symbol taken from a finite set, e.g. $s_1, s_2, ldots, s_alpha$.


The machine examines one square at a time, and carries out an action determined by both:
:$(1): quad$ the symbol in the square
:$(2): quad$ the current internal state of the machine.

The internal state of the machine is a way of providing a device that can keep track of the symbols in other squares.

There can be only a finite set of these states, say $q_1, q_2, ldots, q_beta$.


The actions that the machine can take are as follows:
:$(1): quad$ Replace the symbol in the square with another symbol
:$(2): quad$ Move to examine the square in the immediate left of the current square being looked at
:$(3): quad$ Move to examine the square in the immediate right of the current square being looked at.

After carrying out an action, the machine may change to a different internal state.


The program for the machine is a set of instructions which specify:
:$(1): quad$ what action to take in some possible combinations of the internal state and symbol in the square currently being read
:$(2): quad$ which internal state the machine moves into after carrying out that action.

Thus the instructions have the following form:
:$q_i quad s_j quad A quad q_t$
which is interpreted as:

""If:
:* the machine is in internal state $q_i$
: the symbol in the square currently being examined is $s_j$
then:
: Carry out action $A$
: Move into internal state $q_t$.


The actions can be abbreviated to:
: $L$: Move one square to the left
: $R$: Move one square to the right
: $s_k$: Replace the symbol in the square currently being read with symbol $s_k$.


The computation stops when there is no instruction which specifies what should be done in the current combination of internal state and symbol being read.

=== Formal Definition ===
 

A Turing machine is a 7-tuple $left( Q, unicode{x3a3}, Gamma, delta, q_0, B, F right)$ that satisfies the following:
* $Q$ is a finite set, the states of the machine.
* $unicode{x3a3}$ is a finite set, the input symbols.
* $Gamma supsetneq unicode{x3a3}$ is a finite superset of the input symbols, called the tape symbols.
** For convenience, we also require that $Gamma$ and $Q$ are disjoint.
* $delta : Q times Gamma to Q times Gamma times leftlbrace L, R rightrbrace$ is a partial mapping, the transition function.
** $L$ and $R$ are arbitrary constants called directions.
* $q_0 in Q$ is a distinguished state called the start state.
* $B in Gamma$ is a distinguished tape symbol called the blank symbol. $B$ must not be an element of $unicode{x3a3}$.
* $F subset Q$ be a designated subset of the states called accepting states.


An instantaneous description of a Turing machine is a finite sequence of elements of $Gamma cup Q$, subject to the following conditions:
* There is exactly one element of $Q$ in the sequence.
* The first entry in the sequence is not $B$.
* The last entry in the sequence is not in $Q$.
* If the last entry in the sequence is $B$, then the second-to-last is in $Q$.

By this definition, an instantaneous description can always be written as:
:$X_m X_{m-1} dotsm X_2 X_1 q Y Z_1 Z_2 dotsm Z_{n-1} Z_n$
where $m$ or $n$ may be $0$; $X_i$, $Y$, and $Z_j$ are all elements of $Gamma$; and $q$ is an element of $Q$.

Additionally, $X_m$ and $Z_n$ are not $B$ if they exist; that is, if $m$ and $n$ are not $0$, respectively.


A move reduces one instantaneous description into another by applying the transition function.

We write $A vdash B$ if a machine with instantaneous description $A$ has, after a single move, instantaneous description $B$.

Let $delta left(   right){q, Y} = left( q', Y', d right)$.

Then there are seven cases to consider:

* If $d = L$ and $m > 0$, and either $n > 0$ or $Y' neq B$ then:
:$X_m dotsm X_2 X_1 q Y Z_1 dotsm Z_n vdash X_m dotsm X_2 q' X_1 Y' Z_1 dotsm Z_n$

* If $d = L$ and $m > 0$, but $n = 0$ and $Y' = B$ then:
:$X_m dotsm X_2 X_1 q Y vdash X_m dotsm X_2 q' X_1$

* If $d = L$ but $m = 0$, and either $n > 0$ or $Y' neq B$ then:
:$q Y Z_1 dotsm Z_n vdash q' B Y' Z_1 dotsm Z_n$

* If $d = R$ and $n > 0$, and either $m > 0$ or $Y' neq B$ then:
:$X_m dotsm X_1 q Y Z_1 Z_2 dotsm Z_n vdash X_m dotsm X_1 Y' q' Z_1 Z_2 dotsm Z_n$

* If $d = R$ and $n > 0$, but $m = 0$ and $Y' = B$ then:
:$q Y Z_1 Z_2 dotsm Z_n vdash q' Z_1 Z_2 dotsm Z_n$

* If $d = R$ but $n = 0$, and either $m > 0$ or $Y' neq B$ then:
:$X_m dotsm X_1 q Y vdash X_m dotsm X_1 Y' q' B$

* If $m = 0$, $n = 0$, and $Y' = B$ then regardless of $d$:
:$q Y vdash q' B$


Let $A vdash^* B$ indicate that there exists a finite sequence $leftlangle A_i rightrangle_{0 leq i leq n}$ such that:
:$A = A_0 vdash A_1 vdash A_2 vdash dotso vdash A_n = B$


The language $L left(   right)M$ accepted by the machine $M$ is the set of strings $w in unicode{x3a3}^*$ for which, for some $alpha, beta in Gamma^*$ and $p in F$:
:$q_0 w vdash^* alpha p beta$

As a special case, the null string is in the language  if and only if  the above holds for $w = B$.


A machine $M$ halts on an input $w$, using the same special case for the null string as above,  if and only if  for some $alpha, beta in Gamma^*$, $X in Gamma$, and $q in Q$:
:$q_0 w vdash^* alpha q X beta$
where $delta left(   right){q, X}$ is undefined.",Definition:Turing Machine,,false,"A Turing machine is an abstract machine which works by manipulating symbols on an imaginary piece of paper by means of a specific set of algorithmic rules.

To simplify things, the piece of paper being worked on is in the form of a series of boxes on a one-dimensional ""tape"" divided into squares.

Each square can be either blank or can contain a symbol taken from a finite set, e.g. s_1, s_2, …, s_α.


The machine examines one square at a time, and carries out an action determined by both:
:(1): the symbol in the square
:(2): the current internal state of the machine.

The internal state of the machine is a way of providing a device that can keep track of the symbols in other squares.

There can be only a finite set of these states, say q_1, q_2, …, q_β.


The actions that the machine can take are as follows:
:(1): Replace the symbol in the square with another symbol
:(2): Move to examine the square in the immediate left of the current square being looked at
:(3): Move to examine the square in the immediate right of the current square being looked at.

After carrying out an action, the machine may change to a different internal state.


The program for the machine is a set of instructions which specify:
:(1): what action to take in some possible combinations of the internal state and symbol in the square currently being read
:(2): which internal state the machine moves into after carrying out that action.

Thus the instructions have the following form:
:q_i    s_j    A    q_t
which is interpreted as:

""If:
:* the machine is in internal state q_i
: the symbol in the square currently being examined is s_j
then:
: Carry out action A
: Move into internal state q_t.


The actions can be abbreviated to:
: L: Move one square to the left
: R: Move one square to the right
: s_k: Replace the symbol in the square currently being read with symbol s_k.


The computation stops when there is no instruction which specifies what should be done in the current combination of internal state and symbol being read.

=== Formal Definition ===
 

A Turing machine is a 7-tuple ( Q, x3a3, Γ, δ, q_0, B, F ) that satisfies the following:
* Q is a finite set, the states of the machine.
* x3a3 is a finite set, the input symbols.
* Γ⊋x3a3 is a finite superset of the input symbols, called the tape symbols.
** For convenience, we also require that Γ and Q are disjoint.
* δ : Q ×Γ→ Q ×Γ×{ L, R } is a partial mapping, the transition function.
** L and R are arbitrary constants called directions.
* q_0 ∈ Q is a distinguished state called the start state.
* B ∈Γ is a distinguished tape symbol called the blank symbol. B must not be an element of x3a3.
* F ⊂ Q be a designated subset of the states called accepting states.


An instantaneous description of a Turing machine is a finite sequence of elements of Γ∪ Q, subject to the following conditions:
* There is exactly one element of Q in the sequence.
* The first entry in the sequence is not B.
* The last entry in the sequence is not in Q.
* If the last entry in the sequence is B, then the second-to-last is in Q.

By this definition, an instantaneous description can always be written as:
:X_m X_m-1… X_2 X_1 q Y Z_1 Z_2 … Z_n-1 Z_n
where m or n may be 0; X_i, Y, and Z_j are all elements of Γ; and q is an element of Q.

Additionally, X_m and Z_n are not B if they exist; that is, if m and n are not 0, respectively.


A move reduces one instantaneous description into another by applying the transition function.

We write A ⊢ B if a machine with instantaneous description A has, after a single move, instantaneous description B.

Let δ(   )q, Y = ( q', Y', d ).

Then there are seven cases to consider:

* If d = L and m > 0, and either n > 0 or Y' ≠ B then:
:X_m … X_2 X_1 q Y Z_1 … Z_n ⊢ X_m … X_2 q' X_1 Y' Z_1 … Z_n

* If d = L and m > 0, but n = 0 and Y' = B then:
:X_m … X_2 X_1 q Y ⊢ X_m … X_2 q' X_1

* If d = L but m = 0, and either n > 0 or Y' ≠ B then:
:q Y Z_1 … Z_n ⊢ q' B Y' Z_1 … Z_n

* If d = R and n > 0, and either m > 0 or Y' ≠ B then:
:X_m … X_1 q Y Z_1 Z_2 … Z_n ⊢ X_m … X_1 Y' q' Z_1 Z_2 … Z_n

* If d = R and n > 0, but m = 0 and Y' = B then:
:q Y Z_1 Z_2 … Z_n ⊢ q' Z_1 Z_2 … Z_n

* If d = R but n = 0, and either m > 0 or Y' ≠ B then:
:X_m … X_1 q Y ⊢ X_m … X_1 Y' q' B

* If m = 0, n = 0, and Y' = B then regardless of d:
:q Y ⊢ q' B


Let A ⊢^* B indicate that there exists a finite sequence ⟨ A_i ⟩_0 ≤ i ≤ n such that:
:A = A_0 ⊢ A_1 ⊢ A_2 ⊢…⊢ A_n = B


The language L (   )M accepted by the machine M is the set of strings w ∈x3a3^* for which, for some α, β∈Γ^* and p ∈ F:
:q_0 w ⊢^* α p β

As a special case, the null string is in the language  if and only if  the above holds for w = B.


A machine M halts on an input w, using the same special case for the null string as above,  if and only if  for some α, β∈Γ^*, X ∈Γ, and q ∈ Q:
:q_0 w ⊢^* α q X β
where δ(   )q, X is undefined.",Machine
['Definitions/Turing Machines'],Definition:Machine,"A Nondeterministic (or Non-deterministic) Turing Machine (NTM) is a variation of the classical Turing machine that relaxes the restriction that all the steps in the machine must be definite.

That is, given a single internal state and a single character being read on the tape the machine may have more than one possible response.

If any sequence of choices puts the machine into a halting state, then the machine stops after $n$ steps, where $n$ is the minimum number of steps needed to put the machine into a halting state.

 

=== Formal Definition ===

A nondeterministic Turing machine is a 7-tuple $left( Q, unicode{x3a3}, Gamma, delta, q_0, B, F right)$ that satisfies the following:
* $Q$ is a finite set, the states of the machine.
* $unicode{x3a3}$ is a finite set, the input symbols.
* $Gamma supsetneq unicode{x3a3}$ is a finite superset of the input symbols, called the tape symbols.
** For convenience, we also require that $Gamma$ and $Q$ are disjoint.
* $delta : Q times Gamma to mathcal P left( Q times Gamma times leftlbrace L, R rightrbrace  right)$ is a mapping, the transition function.
** $L$ and $R$ are arbitrary constants called directions.
* $q_0 in Q$ is a distinguished state called the start state.
* $B in Gamma$ is a distinguished tape symbol called the blank symbol. $B$ must not be an element of $unicode{x3a3}$.
* $F subset Q$ be a designated subset of the states called accepting states.


An instantaneous description of a nondeterministic Turing machine is a finite sequence of elements of $Gamma cup Q$, subject to the following conditions:
* There is exactly one element of $Q$ in the sequence.
* The first entry in the sequence is not $B$.
* The last entry in the sequence is not in $Q$.
* If the last entry in the sequence is $B$, then the second-to-last is in $Q$.

By this definition, an instantaneous description can always be written as:
:$X_m X_{m-1} dotsm X_2 X_1 q Y Z_1 Z_2 dotsm Z_{n-1} Z_n$
where $m$ or $n$ may be $0$; $X_i$, $Y$, and $Z_j$ are all elements of $Gamma$; and $q$ is an element of $Q$.

Additionally, $X_m$ and $Z_n$ are not $B$ if they exist; that is, if $m$ and $n$ are not $0$, respectively.


A move reduces one instantaneous description into another by applying the transition function. There may be many possible moves from any given instantaneous description.

We write $A vdash B$ if a machine with instantaneous description $A$ may have, after a single move, instantaneous description $B$.

Let $left( q', Y', d right) in delta left(   right){q, Y}$.

Then there are seven cases to consider:

* If $d = L$ and $m > 0$, and either $n > 0$ or $Y' neq B$ then:
:$X_m dotsm X_2 X_1 q Y Z_1 dotsm Z_n vdash X_m dotsm X_2 q' X_1 Y' Z_1 dotsm Z_n$

* If $d = L$ and $m > 0$, but $n = 0$ and $Y' = B$ then:
:$X_m dotsm X_2 X_1 q Y vdash X_m dotsm X_2 q' X_1$

* If $d = L$ but $m = 0$, and either $n > 0$ or $Y' neq B$ then:
:$q Y Z_1 dotsm Z_n vdash q' B Y' Z_1 dotsm Z_n$

* If $d = R$ and $n > 0$, and either $m > 0$ or $Y' neq B$ then:
:$X_m dotsm X_1 q Y Z_1 Z_2 dotsm Z_n vdash X_m dotsm X_1 Y' q' Z_1 Z_2 dotsm Z_n$

* If $d = R$ and $n > 0$, but $m = 0$ and $Y' = B$ then:
:$q Y Z_1 Z_2 dotsm Z_n vdash q' Z_1 Z_2 dotsm Z_n$

* If $d = R$ but $n = 0$, and either $m > 0$ or $Y' neq B$ then:
:$X_m dotsm X_1 q Y vdash X_m dotsm X_1 Y' q' B$

* If $m = 0$, $n = 0$, and $Y' = B$ then regardless of $d$:
:$q Y vdash q' B$


Let $A vdash^* B$ indicate that there exists a finite sequence $leftlangle A_i rightrangle_{0 leq i leq n}$ such that:
:$A = A_0 vdash A_1 vdash A_2 vdash dotso vdash A_n = B$


The language $L left(   right)M$ accepted by the machine $M$ is the set of strings $w in unicode{x3a3}^*$ for which, for some $alpha, beta in Gamma^*$ and $p in F$:
:$q_0 w vdash^* alpha p beta$

As a special case, the null string is in the language  if and only if  the above holds for $w = B$.",Definition:Nondeterministic Turing Machine,,false,"A Nondeterministic (or Non-deterministic) Turing Machine (NTM) is a variation of the classical Turing machine that relaxes the restriction that all the steps in the machine must be definite.

That is, given a single internal state and a single character being read on the tape the machine may have more than one possible response.

If any sequence of choices puts the machine into a halting state, then the machine stops after n steps, where n is the minimum number of steps needed to put the machine into a halting state.

 

=== Formal Definition ===

A nondeterministic Turing machine is a 7-tuple ( Q, x3a3, Γ, δ, q_0, B, F ) that satisfies the following:
* Q is a finite set, the states of the machine.
* x3a3 is a finite set, the input symbols.
* Γ⊋x3a3 is a finite superset of the input symbols, called the tape symbols.
** For convenience, we also require that Γ and Q are disjoint.
* δ : Q ×Γ→𝒫( Q ×Γ×{ L, R }) is a mapping, the transition function.
** L and R are arbitrary constants called directions.
* q_0 ∈ Q is a distinguished state called the start state.
* B ∈Γ is a distinguished tape symbol called the blank symbol. B must not be an element of x3a3.
* F ⊂ Q be a designated subset of the states called accepting states.


An instantaneous description of a nondeterministic Turing machine is a finite sequence of elements of Γ∪ Q, subject to the following conditions:
* There is exactly one element of Q in the sequence.
* The first entry in the sequence is not B.
* The last entry in the sequence is not in Q.
* If the last entry in the sequence is B, then the second-to-last is in Q.

By this definition, an instantaneous description can always be written as:
:X_m X_m-1… X_2 X_1 q Y Z_1 Z_2 … Z_n-1 Z_n
where m or n may be 0; X_i, Y, and Z_j are all elements of Γ; and q is an element of Q.

Additionally, X_m and Z_n are not B if they exist; that is, if m and n are not 0, respectively.


A move reduces one instantaneous description into another by applying the transition function. There may be many possible moves from any given instantaneous description.

We write A ⊢ B if a machine with instantaneous description A may have, after a single move, instantaneous description B.

Let ( q', Y', d ) ∈δ(   )q, Y.

Then there are seven cases to consider:

* If d = L and m > 0, and either n > 0 or Y' ≠ B then:
:X_m … X_2 X_1 q Y Z_1 … Z_n ⊢ X_m … X_2 q' X_1 Y' Z_1 … Z_n

* If d = L and m > 0, but n = 0 and Y' = B then:
:X_m … X_2 X_1 q Y ⊢ X_m … X_2 q' X_1

* If d = L but m = 0, and either n > 0 or Y' ≠ B then:
:q Y Z_1 … Z_n ⊢ q' B Y' Z_1 … Z_n

* If d = R and n > 0, and either m > 0 or Y' ≠ B then:
:X_m … X_1 q Y Z_1 Z_2 … Z_n ⊢ X_m … X_1 Y' q' Z_1 Z_2 … Z_n

* If d = R and n > 0, but m = 0 and Y' = B then:
:q Y Z_1 Z_2 … Z_n ⊢ q' Z_1 Z_2 … Z_n

* If d = R but n = 0, and either m > 0 or Y' ≠ B then:
:X_m … X_1 q Y ⊢ X_m … X_1 Y' q' B

* If m = 0, n = 0, and Y' = B then regardless of d:
:q Y ⊢ q' B


Let A ⊢^* B indicate that there exists a finite sequence ⟨ A_i ⟩_0 ≤ i ≤ n such that:
:A = A_0 ⊢ A_1 ⊢ A_2 ⊢…⊢ A_n = B


The language L (   )M accepted by the machine M is the set of strings w ∈x3a3^* for which, for some α, β∈Γ^* and p ∈ F:
:q_0 w ⊢^* α p β

As a special case, the null string is in the language  if and only if  the above holds for w = B.",Machine
['Definitions/Euclidean Number Theory'],Definition:Major,"Let $a, b in mathbb R_{>0}$ in the forms:
:$a = dfrac rho {sqrt 2} sqrt {1 + dfrac k {sqrt {1 + k^2} } }$
:$b = dfrac rho {sqrt 2} sqrt {1 - dfrac k {sqrt {1 + k^2} } }$

where:
: $rho$ is a rational number
: $k$ is a rational number whose square root is irrational.


Then $a + b$ is a major.


 ",Definition:Major (Euclidean),,false,"Let a, b ∈ℝ_>0 in the forms:
:a = ρ√(2)√(1 +  k √(1 + k^2))
:b = ρ√(2)√(1 -  k √(1 + k^2))

where:
: ρ is a rational number
: k is a rational number whose square root is irrational.


Then a + b is a major.


 ",Major
"['Definitions/Major Axis of Ellipse', 'Definitions/Major Axis', 'Definitions/Ellipses']",Definition:Major,":


Consider an ellipse $K$ whose foci are $F_1$ and $F_2$.


=== Definition $1$ ===
:


Consider an ellipse $K$ whose foci are $F_1$ and $F_2$.

The major axis of $K$ is the line segment passing through both $F_1$ and $F_2$ whose endpoints are where it intersects $K$.


In the above diagram, $V_1 V_2$ is the major axis of $K$.

=== Definition $2$ ===
:


Consider an ellipse $K$ whose foci are $F_1$ and $F_2$.

The major axis of $K$ is the diameter of $K$ which has the greatest length.


In the above diagram, $V_1 V_2$ is the major axis of $K$.

In the above diagram, $V_1 V_2$ is the major axis of $K$.


=== Semi-Major Axis ===
:

Consider an ellipse $K$.


A semi-major axis of $K$ is either half of the major axis of $K$ from its midpoint to its endpoint.


In the above diagram, $O V_1$ and $O V_2$ are the semi-major axes of $K$.",Definition:Ellipse/Major Axis,,false,":


Consider an ellipse K whose foci are F_1 and F_2.


=== Definition 1 ===
:


Consider an ellipse K whose foci are F_1 and F_2.

The major axis of K is the line segment passing through both F_1 and F_2 whose endpoints are where it intersects K.


In the above diagram, V_1 V_2 is the major axis of K.

=== Definition 2 ===
:


Consider an ellipse K whose foci are F_1 and F_2.

The major axis of K is the diameter of K which has the greatest length.


In the above diagram, V_1 V_2 is the major axis of K.

In the above diagram, V_1 V_2 is the major axis of K.


=== Semi-Major Axis ===
:

Consider an ellipse K.


A semi-major axis of K is either half of the major axis of K from its midpoint to its endpoint.


In the above diagram, O V_1 and O V_2 are the semi-major axes of K.",Major
"['Definitions/Semi-Major Axis of Ellipse', 'Definitions/Major Axis', 'Definitions/Ellipses']",Definition:Major,":

Consider an ellipse $K$.


A semi-major axis of $K$ is either half of the major axis of $K$ from its midpoint to its endpoint.


In the above diagram, $O V_1$ and $O V_2$ are the semi-major axes of $K$.",Definition:Ellipse/Major Axis/Semi-Major Axis,,false,":

Consider an ellipse K.


A semi-major axis of K is either half of the major axis of K from its midpoint to its endpoint.


In the above diagram, O V_1 and O V_2 are the semi-major axes of K.",Major
['Definitions/Lemniscate of Bernoulli'],Definition:Major,"Consider the lemniscate of Bernoulli defined as the locus $M$ described by the equation:
:$P_1 M times P_2 M = left( dfrac {P_1 P_2} 2 right)^2$


:


The line $P_1 P_2$ is the major axis of the lemniscate.


Category:Definitions/Lemniscate of Bernoulli",Definition:Lemniscate of Bernoulli/Major Axis,,false,"Consider the lemniscate of Bernoulli defined as the locus M described by the equation:
:P_1 M × P_2 M = ( P_1 P_2 2 )^2


:


The line P_1 P_2 is the major axis of the lemniscate.


Category:Definitions/Lemniscate of Bernoulli",Major
['Definitions/Lemniscate of Bernoulli'],Definition:Major,"Consider the lemniscate of Bernoulli defined as the locus $M$ described by the equation:
:$P_1 M times P_2 M = left( dfrac {P_1 P_2} 2 right)^2$
where $O$ is the point at the center where the branches cross.


:


Each of the lines $O P_1$ and $O P_2$ is a major semiaxis of the lemniscate.


Category:Definitions/Lemniscate of Bernoulli",Definition:Lemniscate of Bernoulli/Major Semiaxis,,false,"Consider the lemniscate of Bernoulli defined as the locus M described by the equation:
:P_1 M × P_2 M = ( P_1 P_2 2 )^2
where O is the point at the center where the branches cross.


:


Each of the lines O P_1 and O P_2 is a major semiaxis of the lemniscate.


Category:Definitions/Lemniscate of Bernoulli",Major
['Definitions/Categorical Syllogisms'],Definition:Major,"The major premise of a categorical syllogism is conventionally stated first.

It is a categorical statement which expresses the logical relationship between the primary term and the middle term of the syllogism.",Definition:Categorical Syllogism/Premises/Major Premise,,false,"The major premise of a categorical syllogism is conventionally stated first.

It is a categorical statement which expresses the logical relationship between the primary term and the middle term of the syllogism.",Major
"['Definitions/Topological Manifolds', 'Definitions/Manifolds', 'Definitions/Topology']",Definition:Manifold,"Let $M$ be a Hausdorff second-countable locally Euclidean space of dimension $d$. 


Then $M$ is a topological manifold of dimension $d$.


=== Differentiable Manifold ===
Let $M$ be a second-countable locally Euclidean space of dimension $d$. 

Let $mathscr F$ be a $d$-dimensional differentiable structure on $M$ of class $mathcal C^k$, where $k ge 1$.


Then $left( M, mathscr F right)$ is a differentiable manifold of class $mathcal C^k$ and dimension $d$.

=== Smooth Manifold ===
Let $M$ be a second-countable locally Euclidean space of dimension $d$. 

Let $mathscr F$ be a smooth differentiable structure on $M$.


Then $left( M, mathscr F right)$ is called a smooth manifold of dimension $d$.

=== Complex Manifold ===
Let $M$ be a second-countable, complex locally Euclidean space of dimension $d$. 

Let $mathscr F$ be a complex analytic differentiable structure on $M$.


Then $left( M, mathscr F right)$ is called a complex manifold of dimension $d$.",Definition:Topological Manifold,,false,"Let M be a Hausdorff second-countable locally Euclidean space of dimension d. 


Then M is a topological manifold of dimension d.


=== Differentiable Manifold ===
Let M be a second-countable locally Euclidean space of dimension d. 

Let ℱ be a d-dimensional differentiable structure on M of class 𝒞^k, where k ≥ 1.


Then ( M, ℱ) is a differentiable manifold of class 𝒞^k and dimension d.

=== Smooth Manifold ===
Let M be a second-countable locally Euclidean space of dimension d. 

Let ℱ be a smooth differentiable structure on M.


Then ( M, ℱ) is called a smooth manifold of dimension d.

=== Complex Manifold ===
Let M be a second-countable, complex locally Euclidean space of dimension d. 

Let ℱ be a complex analytic differentiable structure on M.


Then ( M, ℱ) is called a complex manifold of dimension d.",Manifold
"['Definitions/Topological Manifolds', 'Definitions/Differentiable Manifolds']",Definition:Manifold,"Let $M$ be a second-countable locally Euclidean space of dimension $d$. 

Let $mathscr F$ be a $d$-dimensional differentiable structure on $M$ of class $mathcal C^k$, where $k ge 1$.


Then $left( M, mathscr F right)$ is a differentiable manifold of class $mathcal C^k$ and dimension $d$.",Definition:Topological Manifold/Differentiable Manifold,,false,"Let M be a second-countable locally Euclidean space of dimension d. 

Let ℱ be a d-dimensional differentiable structure on M of class 𝒞^k, where k ≥ 1.


Then ( M, ℱ) is a differentiable manifold of class 𝒞^k and dimension d.",Manifold
"['Definitions/Smooth Manifolds', 'Definitions/Topological Manifolds', 'Definitions/Differentiable Manifolds']",Definition:Manifold,"Let $M$ be a second-countable locally Euclidean space of dimension $d$. 

Let $mathscr F$ be a smooth differentiable structure on $M$.


Then $left( M, mathscr F right)$ is called a smooth manifold of dimension $d$.",Definition:Topological Manifold/Smooth Manifold,,false,"Let M be a second-countable locally Euclidean space of dimension d. 

Let ℱ be a smooth differentiable structure on M.


Then ( M, ℱ) is called a smooth manifold of dimension d.",Manifold
['Definitions/Topological Manifolds'],Definition:Manifold,"Let $M$ be a second-countable, complex locally Euclidean space of dimension $d$. 

Let $mathscr F$ be a complex analytic differentiable structure on $M$.


Then $left( M, mathscr F right)$ is called a complex manifold of dimension $d$.",Definition:Topological Manifold/Complex Manifold,,false,"Let M be a second-countable, complex locally Euclidean space of dimension d. 

Let ℱ be a complex analytic differentiable structure on M.


Then ( M, ℱ) is called a complex manifold of dimension d.",Manifold
"['Definitions/Linear Algebra', 'Definitions/Hilbert Spaces']",Definition:Manifold,"Let $K$ be a division ring.

Let $left({S, +, circ}right)_K$ be a $K$-algebraic structure with one operation.


Let $T$ be a closed subset of $S$.

Let $left({T, +_T, circ_T}right)_K$ be a $K$-vector space where:
: $+_T$ is the restriction of $+$ to $T times T$ and
: $circ_T$ is the restriction of $circ$ to $K times T$.


Then $left({T, +_T, circ_T}right)_K$ is a (vector) subspace of $left({S, +, circ}right)_K$.


When considering Hilbert spaces, one wants to deal with projections onto subspaces.

These projections however require the linear subspace to be closed in topological sense in order to be well-defined.

Therefore, in treatises of Hilbert spaces, one encounters the terminology linear manifold for the concept of vector subspace defined above.

The adapted definition of linear subspace is then that it is a topologically closed linear manifold.",Definition:Vector Subspace/Hilbert Spaces,,false,"Let K be a division ring.

Let (S, +, ∘)_K be a K-algebraic structure with one operation.


Let T be a closed subset of S.

Let (T, +_T, ∘_T)_K be a K-vector space where:
: +_T is the restriction of + to T × T and
: ∘_T is the restriction of ∘ to K × T.


Then (T, +_T, ∘_T)_K is a (vector) subspace of (S, +, ∘)_K.


When considering Hilbert spaces, one wants to deal with projections onto subspaces.

These projections however require the linear subspace to be closed in topological sense in order to be well-defined.

Therefore, in treatises of Hilbert spaces, one encounters the terminology linear manifold for the concept of vector subspace defined above.

The adapted definition of linear subspace is then that it is a topologically closed linear manifold.",Manifold
"['Definitions/Ideal Theory', 'Definitions/Maximal Ideals of Rings']",Definition:Maximal Ideal,"Let $R$ be a ring.


An ideal $J$ of $R$ is maximal  if and only if :

:$(1): quad J subsetneq R$
:$(2): quad$ There is no ideal $K$ of $R$ such that $J subsetneq K subsetneq R$.


That is,  if and only if  $J$ is a maximal element of the set of all proper ideals of $R$ ordered by the subset relation.


=== Maximal Left Ideal ===
Let $R$ be a ring.


A left ideal $J$ of $R$ is a maximal left ideal  if and only if :

:$(1): quad J subsetneq R$
:$(2): quad$ There is no left ideal $K$ of $R$ such that $J subsetneq K subsetneq R$.


Category:Definitions/Maximal Ideals of Rings

=== Maximal Right Ideal ===
Let $R$ be a ring.


A right ideal $J$ of $R$ is a maximal right ideal  if and only if :

:$(1): quad J subsetneq R$
:$(2): quad$ There is no right ideal $K$ of $R$ such that $J subsetneq K subsetneq R$.


Category:Definitions/Maximal Ideals of Rings

It follows that in a commutative ring, a maximal left ideal, a maximal right ideal and a maximal ideal are the same thing.",Definition:Maximal Ideal of Ring,,false,"Let R be a ring.


An ideal J of R is maximal  if and only if :

:(1):    J ⊊ R
:(2): There is no ideal K of R such that J ⊊ K ⊊ R.


That is,  if and only if  J is a maximal element of the set of all proper ideals of R ordered by the subset relation.


=== Maximal Left Ideal ===
Let R be a ring.


A left ideal J of R is a maximal left ideal  if and only if :

:(1):    J ⊊ R
:(2): There is no left ideal K of R such that J ⊊ K ⊊ R.


Category:Definitions/Maximal Ideals of Rings

=== Maximal Right Ideal ===
Let R be a ring.


A right ideal J of R is a maximal right ideal  if and only if :

:(1):    J ⊊ R
:(2): There is no right ideal K of R such that J ⊊ K ⊊ R.


Category:Definitions/Maximal Ideals of Rings

It follows that in a commutative ring, a maximal left ideal, a maximal right ideal and a maximal ideal are the same thing.",Maximal Ideal
['Definitions/Algebras'],Definition:Maximal Ideal,"Let $R$ be a ring.

Let $left( A, ast right)$ be an $R$-algebra.

Let $left( J, ast right)$ be a proper ideal of $left( A, ast right)$. 


We say that $J$ is a maximal ideal of $A$  if and only if :
:there is no ideal $I$ such that $J subsetneq I$. 


=== Maximal Left Ideal ===
Let $R$ be a ring.

Let $left( A, ast right)$ be an $R$-algebra.

Let $left( J, ast right)$ be a proper left ideal of $left( A, ast right)$. 


We say that $J$ is a maximal left ideal of $A$  if and only if :
:there is no left ideal $I$ such that $J subsetneq I$.

=== Maximal Right Ideal ===
Let $R$ be a ring.

Let $left( A, ast right)$ be an $R$-algebra.

Let $left( J, ast right)$ be a proper right ideal of $left( A, ast right)$. 


We say that $J$ is a maximal right ideal of $A$  if and only if :
:there is no right ideal $I$ such that $J subsetneq I$.",Definition:Maximal Ideal of Algebra,,false,"Let R be a ring.

Let ( A, ∗) be an R-algebra.

Let ( J, ∗) be a proper ideal of ( A, ∗). 


We say that J is a maximal ideal of A  if and only if :
:there is no ideal I such that J ⊊ I. 


=== Maximal Left Ideal ===
Let R be a ring.

Let ( A, ∗) be an R-algebra.

Let ( J, ∗) be a proper left ideal of ( A, ∗). 


We say that J is a maximal left ideal of A  if and only if :
:there is no left ideal I such that J ⊊ I.

=== Maximal Right Ideal ===
Let R be a ring.

Let ( A, ∗) be an R-algebra.

Let ( J, ∗) be a proper right ideal of ( A, ∗). 


We say that J is a maximal right ideal of A  if and only if :
:there is no right ideal I such that J ⊊ I.",Maximal Ideal
"['Definitions/Arithmetic Mean', 'Definitions/Pythagorean Means', 'Definitions/Algebra', 'Definitions/Measures of Central Tendency']",Definition:Mean,"Let $x_1, x_2, ldots, x_n in mathbb R$ be real numbers.

The arithmetic mean of $x_1, x_2, ldots, x_n$ is defined as:

:$ds A_n := dfrac 1 n sum_{k mathop = 1}^n x_k$

That is, to find out the arithmetic mean of a set of numbers, add them all up and divide by how many there are.",Definition:Arithmetic Mean,,false,"Let x_1, x_2, …, x_n ∈ℝ be real numbers.

The arithmetic mean of x_1, x_2, …, x_n is defined as:

:A_n :=  1 n ∑_k  = 1^n x_k

That is, to find out the arithmetic mean of a set of numbers, add them all up and divide by how many there are.",Mean
"['Definitions/Geometric Mean', 'Definitions/Pythagorean Means', 'Definitions/Algebra', 'Definitions/Measures of Central Tendency']",Definition:Mean,"Let $x_1, x_2, ldots, x_n in mathbb R_{>0}$ be (strictly) positive real numbers.

The geometric mean of $x_1, x_2, ldots, x_n$ is defined as:

:$ds G_n := left( prod_{k mathop = 1}^n x_k right)^{1/n}$


That is, to find out the geometric mean of a set of $n$ numbers, multiply them together and take the $n$th root.


=== Mean Proportional ===
In the language of  , the geometric mean of two magnitudes is called the mean proportional.

Thus the mean proportional of $a$ and $b$ is defined as that magnitude $c$ such that:
:$a : c = c : b$
where $a : c$ denotes the ratio between $a$ and $c$.


From the definition of ratio it is seen that $dfrac a c = dfrac c b$ from which it follows that $c = sqrt {a b}$ demonstrating that the definitions are logically equivalent.


=== General Definition ===
In the language of  , the terms of a (finite) geometric sequence of positive integers between (and not including) the first and last terms are called mean proportionals.",Definition:Geometric Mean,,false,"Let x_1, x_2, …, x_n ∈ℝ_>0 be (strictly) positive real numbers.

The geometric mean of x_1, x_2, …, x_n is defined as:

:G_n := ( ∏_k  = 1^n x_k )^1/n


That is, to find out the geometric mean of a set of n numbers, multiply them together and take the nth root.


=== Mean Proportional ===
In the language of  , the geometric mean of two magnitudes is called the mean proportional.

Thus the mean proportional of a and b is defined as that magnitude c such that:
:a : c = c : b
where a : c denotes the ratio between a and c.


From the definition of ratio it is seen that a c =  c b from which it follows that c = √(a b) demonstrating that the definitions are logically equivalent.


=== General Definition ===
In the language of  , the terms of a (finite) geometric sequence of positive integers between (and not including) the first and last terms are called mean proportionals.",Mean
"['Definitions/Arithmetic-Geometric Mean', 'Definitions/Measures of Central Tendency']",Definition:Mean,"The arithmetic-geometric mean of two numbers $a$ and $b$ is the limit of the sequences obtained by the arithmetic-geometric mean iteration.

This is denoted $M left(   right){a, b}$.


=== Arithmetic-Geometric Mean Iteration ===
Let $a$ and $b$ be numbers.

Let $leftlangle a_n rightrangle$ and $leftlangle b_n rightrangle$ be defined as the recursive sequences:

 
 
 
 
 

where:

 
 
 
 

The above process is known as the arithmetic-geometric mean iteration.",Definition:Arithmetic-Geometric Mean,,false,"The arithmetic-geometric mean of two numbers a and b is the limit of the sequences obtained by the arithmetic-geometric mean iteration.

This is denoted M (   )a, b.


=== Arithmetic-Geometric Mean Iteration ===
Let a and b be numbers.

Let ⟨ a_n ⟩ and ⟨ b_n ⟩ be defined as the recursive sequences:

 
 
 
 
 

where:

 
 
 
 

The above process is known as the arithmetic-geometric mean iteration.",Mean
"['Definitions/Harmonic Mean', 'Definitions/Pythagorean Means', 'Definitions/Measures of Central Tendency', 'Definitions/Algebra', 'Definitions/Number Theory', 'Definitions/Analysis']",Definition:Mean,"Let $x_1, x_2, ldots, x_n in mathbb R$ be real numbers which are all strictly positive.

The harmonic mean $H_n$ of $x_1, x_2, ldots, x_n$ is defined as:

:$ds dfrac 1 {H_n} := frac 1 n left( sum_{k mathop = 1}^n frac 1 {x_k}  right)$

That is, to find the harmonic mean of a set of $n$ numbers, take the reciprocal of the arithmetic mean of their reciprocals.",Definition:Harmonic Mean,,false,"Let x_1, x_2, …, x_n ∈ℝ be real numbers which are all strictly positive.

The harmonic mean H_n of x_1, x_2, …, x_n is defined as:

:1 H_n := 1/n( ∑_k  = 1^n 1/x_k)

That is, to find the harmonic mean of a set of n numbers, take the reciprocal of the arithmetic mean of their reciprocals.",Mean
"['Definitions/Algebra', 'Definitions/Measures of Central Tendency']",Definition:Mean,"Let $x_1, x_2, ldots, x_n in mathbb R$ be real numbers.

The quadratic mean of $x_1, x_2, ldots, x_n$ is defined as:

:$Q_n := ds sqrt {frac 1 n sum_{k mathop = 1}^n x_k^2}$",Definition:Quadratic Mean,,false,"Let x_1, x_2, …, x_n ∈ℝ be real numbers.

The quadratic mean of x_1, x_2, …, x_n is defined as:

:Q_n := √(1/n∑_k  = 1^n x_k^2)",Mean
"['Definitions/Weighted Means', 'Definitions/Measures of Central Tendency', 'Definitions/Algebra']",Definition:Mean,"Let $S = leftlangle x_1, x_2, ldots, x_n rightrangle$ be a sequence of real numbers.

Let $W$ be a weight function to be applied to the terms of $S$.


The weighted mean of $S$   $W$ is defined as:
:$bar x := dfrac {ds sum_{i mathop = 1}^n W left(   right){x_i} x_i} {ds sum_{i mathop = 1}^n W left(   right){x_i} }$

This means that elements of $S$ with a larger weight contribute more to the weighted mean than those with a smaller weight.


If we write:
:$forall i: 1 le i le n: w_i = W left(   right){x_i}$
we can write this weighted mean as:
:$bar x := dfrac {w_1 x_1 + w_2 x_2 + cdots + w_n x_n} {w_1 + w_2 + cdots + w_n}$


From the definition of the weight function, none of the weights can be negative.

While some of the weights may be zero, not all of them can, otherwise we would be dividing by zero.


=== Normalized Weighted Mean ===
Let $S = leftlangle x_1, x_2, ldots, x_n rightrangle$ be a sequence of real numbers.

Let $W left(   right)x$ be a weight function to be applied to the terms of $S$.

Let the weights be normalized.

Then the weighted mean of $S$   $W$ can be expressed in the form:
:$ds bar x := sum_{i mathop = 1}^n W left(   right){x_i} x_i$

as by definition of normalized weight function all the weights add up to $1$.

This weighted mean is known as a  normalized weighted mean.


 ",Definition:Weighted Mean,,false,"Let S = ⟨ x_1, x_2, …, x_n ⟩ be a sequence of real numbers.

Let W be a weight function to be applied to the terms of S.


The weighted mean of S   W is defined as:
:x̅ := ∑_i  = 1^n W (   )x_i x_i∑_i  = 1^n W (   )x_i

This means that elements of S with a larger weight contribute more to the weighted mean than those with a smaller weight.


If we write:
:∀ i: 1 ≤ i ≤ n: w_i = W (   )x_i
we can write this weighted mean as:
:x̅ := w_1 x_1 + w_2 x_2 + ⋯ + w_n x_nw_1 + w_2 + ⋯ + w_n


From the definition of the weight function, none of the weights can be negative.

While some of the weights may be zero, not all of them can, otherwise we would be dividing by zero.


=== Normalized Weighted Mean ===
Let S = ⟨ x_1, x_2, …, x_n ⟩ be a sequence of real numbers.

Let W (   )x be a weight function to be applied to the terms of S.

Let the weights be normalized.

Then the weighted mean of S   W can be expressed in the form:
:x̅ := ∑_i  = 1^n W (   )x_i x_i

as by definition of normalized weight function all the weights add up to 1.

This weighted mean is known as a  normalized weighted mean.


 ",Mean
['Definitions/Algebra'],Definition:Mean,"The Heronian mean of two numbers $x$ and $y$ is defined as:

:$H = dfrac {x + sqrt {x y} + y} 3$


It can also be defined as:

:$H = dfrac 2 3 left( dfrac {x + y} 2 right) + dfrac 1 3 sqrt {x y}$


Thus it is seen to be a weighted mean of their arithmetic mean and geometric mean.

 ",Definition:Heronian Mean,,false,"The Heronian mean of two numbers x and y is defined as:

:H = x + √(x y) + y 3


It can also be defined as:

:H =  2 3 ( x + y 2 ) +  1 3 √(x y)


Thus it is seen to be a weighted mean of their arithmetic mean and geometric mean.

 ",Mean
"['Definitions/Hölder Mean', 'Definitions/Measures of Central Tendency', 'Definitions/Real Analysis', 'Definitions/Algebra']",Definition:Mean,"Let $x_1, x_2, ldots, x_n in mathbb R_{ge 0}$ be positive real numbers.

Let $p$ be an extended real number.


The Hölder mean with exponent $p$ of $x_1, x_2, ldots, x_n$ is denoted $M_p left(   right){x_1, x_2, ldots, x_n}$.


=== Non-Zero Exponent ===
Let $x_1, x_2, ldots, x_n in mathbb R_{ge 0}$ be positive real numbers.

Let $p$ be an extended real number.


Let $M_p left(   right){x_1, x_2, ldots, x_n}$ denote the Hölder mean with exponent $p$ of $x_1, x_2, ldots, x_n$.


For $p in mathbb R_{ne 0}$, the Hölder mean is defined as:
:$ds M_p left(   right){x_1, x_2, ldots, x_n} = left( frac 1 n sum_{k mathop = 1}^n {x_k}^p right)^{1 / p}$
whenever the above expression is defined.

=== Negative Exponent with Zero Parameter ===
Let $x_1, x_2, ldots, x_n in mathbb R_{ge 0}$ be positive real numbers.

Let $p$ be an extended real number.


Let $M_p left(   right){x_1, x_2, ldots, x_n}$ denote the Hölder mean with exponent $p$ of $x_1, x_2, ldots, x_n$.


For $p < 0$ and at least one $a_k = 0$, the Hölder mean is defined as:
:$ds M_p left(   right){x_1, x_2, ldots, x_n} = 0$

=== Zero Exponent ===
Let $x_1, x_2, ldots, x_n in mathbb R_{ge 0}$ be positive real numbers.

Let $p$ be an extended real number.


Let $M_p left(   right){x_1, x_2, ldots, x_n}$ denote the Hölder mean with exponent $p$ of $x_1, x_2, ldots, x_n$.


For $p = 0$, the Hölder mean is defined as:
:$M_0 left(   right){x_1, x_2, ldots, x_n} = left( x_1 x_2 cdots x_n right)^{1 / n}$
which is the geometric mean of $x_1, x_2, ldots, x_n$.

=== Positive Infinite Exponent ===
Let $x_1, x_2, ldots, x_n in mathbb R_{ge 0}$ be positive real numbers.

Let $p$ be an extended real number.


Let $M_p left(   right){x_1, x_2, ldots, x_n}$ denote the Hölder mean with exponent $p$ of $x_1, x_2, ldots, x_n$.


For $p = infty$, the Hölder mean is defined as:
:$M_infty left(   right){x_1, x_2, ldots, x_n} = max leftlbrace x_1, x_2, ldots, x_n rightrbrace$

=== Negative Infinite Exponent ===
Let $x_1, x_2, ldots, x_n in mathbb R_{ge 0}$ be positive real numbers.

Let $p$ be an extended real number.


Let $M_p left(   right){x_1, x_2, ldots, x_n}$ denote the Hölder mean with exponent $p$ of $x_1, x_2, ldots, x_n$.


For $p = -infty$, the Hölder mean is defined as:
:$M_{-infty}  left(   right){x_1, x_2, ldots, x_n} = min leftlbrace x_1, x_2, ldots, x_n rightrbrace$",Definition:Hölder Mean,,false,"Let x_1, x_2, …, x_n ∈ℝ_≥ 0 be positive real numbers.

Let p be an extended real number.


The Hölder mean with exponent p of x_1, x_2, …, x_n is denoted M_p (   )x_1, x_2, …, x_n.


=== Non-Zero Exponent ===
Let x_1, x_2, …, x_n ∈ℝ_≥ 0 be positive real numbers.

Let p be an extended real number.


Let M_p (   )x_1, x_2, …, x_n denote the Hölder mean with exponent p of x_1, x_2, …, x_n.


For p ∈ℝ_ 0, the Hölder mean is defined as:
:M_p (   )x_1, x_2, …, x_n = ( 1/n∑_k  = 1^n x_k^p )^1 / p
whenever the above expression is defined.

=== Negative Exponent with Zero Parameter ===
Let x_1, x_2, …, x_n ∈ℝ_≥ 0 be positive real numbers.

Let p be an extended real number.


Let M_p (   )x_1, x_2, …, x_n denote the Hölder mean with exponent p of x_1, x_2, …, x_n.


For p < 0 and at least one a_k = 0, the Hölder mean is defined as:
:M_p (   )x_1, x_2, …, x_n = 0

=== Zero Exponent ===
Let x_1, x_2, …, x_n ∈ℝ_≥ 0 be positive real numbers.

Let p be an extended real number.


Let M_p (   )x_1, x_2, …, x_n denote the Hölder mean with exponent p of x_1, x_2, …, x_n.


For p = 0, the Hölder mean is defined as:
:M_0 (   )x_1, x_2, …, x_n = ( x_1 x_2 ⋯ x_n )^1 / n
which is the geometric mean of x_1, x_2, …, x_n.

=== Positive Infinite Exponent ===
Let x_1, x_2, …, x_n ∈ℝ_≥ 0 be positive real numbers.

Let p be an extended real number.


Let M_p (   )x_1, x_2, …, x_n denote the Hölder mean with exponent p of x_1, x_2, …, x_n.


For p = ∞, the Hölder mean is defined as:
:M_∞(   )x_1, x_2, …, x_n = max{ x_1, x_2, …, x_n }

=== Negative Infinite Exponent ===
Let x_1, x_2, …, x_n ∈ℝ_≥ 0 be positive real numbers.

Let p be an extended real number.


Let M_p (   )x_1, x_2, …, x_n denote the Hölder mean with exponent p of x_1, x_2, …, x_n.


For p = -∞, the Hölder mean is defined as:
:M_-∞(   )x_1, x_2, …, x_n = min{ x_1, x_2, …, x_n }",Mean
"['Definitions/Mean Value of Function', 'Definitions/Integral Calculus', 'Definitions/Measures of Central Tendency']",Definition:Mean,"Let $f$ be an integrable function on some closed interval $left[ a ,.,.,   right]b$.

The mean value of $f$ on $left[ a ,.,.,   right]b$ is defined as:

:$ds frac 1 {b - a} int_a^b f left(   right)x ,mathrm d x$",Definition:Mean Value of Function,,false,"Let f be an integrable function on some closed interval [ a  . . ]b.

The mean value of f on [ a  . . ]b is defined as:

:1/b - a∫_a^b f (   )x  d x",Mean
"['Definitions/Golden Mean', 'Definitions/Fibonacci Numbers', 'Definitions/Real Analysis', 'Definitions/Number Theory', 'Definitions/Algebra', 'Definitions/Geometry', 'Definitions/Specific Numbers']",Definition:Mean,"=== Definition 1 ===
Let a line segment $AB$ be divided at $C$ such that:
:$AB : AC = AC : BC$

Then the golden mean $phi$ is defined as:
:$phi := dfrac {AB} {AC}$

=== Definition 2 ===
The golden mean is the unique positive real number $phi$ satisfying:
:$phi = dfrac {1 + sqrt 5} 2$

=== Definition 3 ===
The golden mean is the unique positive real number $phi$ satisfying:
:$phi = dfrac 1 {phi - 1}$",Definition:Golden Mean,,false,"=== Definition 1 ===
Let a line segment AB be divided at C such that:
:AB : AC = AC : BC

Then the golden mean ϕ is defined as:
:ϕ := ABAC

=== Definition 2 ===
The golden mean is the unique positive real number ϕ satisfying:
:ϕ = 1 + √(5) 2

=== Definition 3 ===
The golden mean is the unique positive real number ϕ satisfying:
:ϕ =  1 ϕ - 1",Mean
"['Definitions/Centroids', 'Definitions/Physics', 'Definitions/Applied Mathematics']",Definition:Mean,"=== Centroid of Set of Points ===
Let $S = leftlbrace A_1, A_2, ldots, A_n rightrbrace$ be a set of $n$ points in Euclidean space.


=== Definition 1 ===
Let $S = leftlbrace A_1, A_2, ldots, A_n rightrbrace$ be a set of $n$ points in Euclidean space.

Let the position vectors of the elements of $S$ be given by $mathbf a_1, mathbf a_2, dotsc, mathbf a_n$ respectively.

Let $G$ be the point whose position vector is given by:

:$vec {OG} = dfrac 1 n left( mathbf a_1 + mathbf a_2 + dotsb + mathbf a_n right)$


Then $G$ is known as the centroid of $S$.

=== Definition 2 ===
Let $S = leftlbrace A_1, A_2, ldots, A_n rightrbrace$ be a set of $n$ points in Euclidean space.

Let the Cartesian coordinates of the elements of $S$ be $left( x_j, y_j, z_j right)$ for each $j in leftlbrace 1, 2, ldots, n rightrbrace$.

Let $G$ be the point whose Cartesian coordinates are given by:

:$G = left( dfrac 1 n ds sum_{j mathop = 1}^n x_j, dfrac 1 n ds sum_{j mathop = 1}^n y_j, dfrac 1 n ds sum_{j mathop = 1}^n z_j right)$

That is, the arithmetic mean of the Cartesian coordinates of the elements of $S$


Then $G$ is known as the centroid of $S$.

=== Centroid of Weighted Set of Points ===
Let $S = leftlbrace A_1, A_2, ldots, A_n rightrbrace$ be a set of $n$ points in Euclidean space whose position vectors are given by $mathbf a_1, mathbf a_2, dotsc, mathbf a_n$ repectively.

Let $W: S to mathbb R$ be a weight function on $S$.

Let $G$ be the point whose position vector is given by:

:$vec {OG} = dfrac {w_1 mathbf a_1 + w_2 mathbf a_2 + dotsb + w_n mathbf a_n} {w_1 + w_2 + dotsb + w_n}$

where $w_i = W left(   right){A_i}$ for each $i$.


Then $G$ is known as the centroid of $S$ with weights $w_i, w_2, dotsc, w_n$.

=== Centroid of Surface ===
Let $S$ be a surface.

Let $S$ be divided into a large number $n$ of small elements.

Consider one point of each of these elements.

Let a weight function be associated with this set of points.

Let $G$ be the centroid of each of these weighted points.

Let $n$ increase indefinitely, such that each element of $S$ converges to a point.

Then the limiting position of $G$ is the centroid of $S$.

=== Centroid of Solid Figure ===
Let $F$ be a solid figure.

Let $F$ be divided into a large number $n$ of small elements.

Consider one point of each of these elements.

Let a weight function be associated with this set of points.

Let $G$ be the centroid of each of these weighted points.

Let $n$ increase indefinitely, such that each element of $F$ converges to a point.

Then the limiting position of $G$ is the centroid of $F$.

=== Centroid of Triangle ===
Let $triangle ABC$ be a triangle.


The centroid of $triangle ABC$ is the point $G$ where its three medians $AL$, $MB$ and $CN$ meet.


:",Definition:Centroid,,false,"=== Centroid of Set of Points ===
Let S = { A_1, A_2, …, A_n } be a set of n points in Euclidean space.


=== Definition 1 ===
Let S = { A_1, A_2, …, A_n } be a set of n points in Euclidean space.

Let the position vectors of the elements of S be given by 𝐚_1, 𝐚_2, …, 𝐚_n respectively.

Let G be the point whose position vector is given by:

:O⃗G⃗ =  1 n ( 𝐚_1 + 𝐚_2 + … + 𝐚_n )


Then G is known as the centroid of S.

=== Definition 2 ===
Let S = { A_1, A_2, …, A_n } be a set of n points in Euclidean space.

Let the Cartesian coordinates of the elements of S be ( x_j, y_j, z_j ) for each j ∈{ 1, 2, …, n }.

Let G be the point whose Cartesian coordinates are given by:

:G = (  1 n ∑_j  = 1^n x_j,  1 n ∑_j  = 1^n y_j,  1 n ∑_j  = 1^n z_j )

That is, the arithmetic mean of the Cartesian coordinates of the elements of S


Then G is known as the centroid of S.

=== Centroid of Weighted Set of Points ===
Let S = { A_1, A_2, …, A_n } be a set of n points in Euclidean space whose position vectors are given by 𝐚_1, 𝐚_2, …, 𝐚_n repectively.

Let W: S →ℝ be a weight function on S.

Let G be the point whose position vector is given by:

:O⃗G⃗ = w_1 𝐚_1 + w_2 𝐚_2 + … + w_n 𝐚_nw_1 + w_2 + … + w_n

where w_i = W (   )A_i for each i.


Then G is known as the centroid of S with weights w_i, w_2, …, w_n.

=== Centroid of Surface ===
Let S be a surface.

Let S be divided into a large number n of small elements.

Consider one point of each of these elements.

Let a weight function be associated with this set of points.

Let G be the centroid of each of these weighted points.

Let n increase indefinitely, such that each element of S converges to a point.

Then the limiting position of G is the centroid of S.

=== Centroid of Solid Figure ===
Let F be a solid figure.

Let F be divided into a large number n of small elements.

Consider one point of each of these elements.

Let a weight function be associated with this set of points.

Let G be the centroid of each of these weighted points.

Let n increase indefinitely, such that each element of F converges to a point.

Then the limiting position of G is the centroid of F.

=== Centroid of Triangle ===
Let ABC be a triangle.


The centroid of ABC is the point G where its three medians AL, MB and CN meet.


:",Mean
"['Definitions/Mean Deviation', 'Definitions/Deviations']",Definition:Mean,"Let $S = leftlbrace x_1, x_2, ldots, x_n rightrbrace$ be a set of observations.

Let $bar x$ denote a measure of central tendency of $S$.


The mean deviation   $bar x$ of $S$ is defined as the arithmetic mean of the deviation of the elements of $S$ from $bar x$:

:$ds sum_{i mathop = 1}^n dfrac 1 n left( x_i - bar x right)$


=== Discrete Random Variable ===
Let $X$ be a discrete random variable.

Let $bar x$ denote a measure of central tendency of $X$.


The mean deviation of $X$ is the first moment of $X$ about $bar x$.

=== Continuous Random Variable ===
Let $X$ be a continuous random variable.

Let $m$ denote the median of $X$.

Let the frequency function of $X$ be $f$.


The mean deviation of $X$ is defined as:
:$ds int_{-infty}^{+infty} left( x - m right) f left(   right)x ,mathrm d x$",Definition:Mean Deviation,,false,"Let S = { x_1, x_2, …, x_n } be a set of observations.

Let x̅ denote a measure of central tendency of S.


The mean deviation   x̅ of S is defined as the arithmetic mean of the deviation of the elements of S from x̅:

:∑_i  = 1^n  1 n ( x_i - x̅)


=== Discrete Random Variable ===
Let X be a discrete random variable.

Let x̅ denote a measure of central tendency of X.


The mean deviation of X is the first moment of X about x̅.

=== Continuous Random Variable ===
Let X be a continuous random variable.

Let m denote the median of X.

Let the frequency function of X be f.


The mean deviation of X is defined as:
:∫_-∞^+∞( x - m ) f (   )x  d x",Mean
"['Definitions/Mean Absolute Deviation', 'Definitions/Deviations']",Definition:Mean,"Let $S = leftlbrace x_1, x_2, ldots, x_n rightrbrace$ be a set of observations.

Let $bar x$ denote a measure of central tendency of $S$.


The mean absolute deviation   $bar x$ of $S$ is defined as the arithmetic mean of the absolute values of the deviation of the elements of $S$ from $bar x$ :

:$ds sum_{i mathop = 1}^n dfrac 1 n leftlvert x_i - bar x rightrvert$


=== Discrete Random Variable ===
Let $X$ be a discrete random variable.

Let $bar x$ denote a measure of central tendency of $X$.


The mean absolute deviation of $X$ is the first absolute moment of $X$ about $bar x$.

=== Continuous Random Variable ===
Let $X$ be a continuous random variable.

Let $m$ denote the median of $X$.

Let the frequency function of $X$ be $f$.


The mean absolute deviation of $X$ is defined as:
:$ds int_{-infty}^{+infty} leftlvert x - m rightrvert f left(   right)x ,mathrm d x$",Definition:Mean Absolute Deviation,,false,"Let S = { x_1, x_2, …, x_n } be a set of observations.

Let x̅ denote a measure of central tendency of S.


The mean absolute deviation   x̅ of S is defined as the arithmetic mean of the absolute values of the deviation of the elements of S from x̅ :

:∑_i  = 1^n  1 n | x_i - x̅|


=== Discrete Random Variable ===
Let X be a discrete random variable.

Let x̅ denote a measure of central tendency of X.


The mean absolute deviation of X is the first absolute moment of X about x̅.

=== Continuous Random Variable ===
Let X be a continuous random variable.

Let m denote the median of X.

Let the frequency function of X be f.


The mean absolute deviation of X is defined as:
:∫_-∞^+∞| x - m | f (   )x  d x",Mean
"['Definitions/Mean Square Deviation', 'Definitions/Deviations']",Definition:Mean,"Let $S = leftlbrace x_1, x_2, ldots, x_n rightrbrace$ be a set of observations.

Let $bar x$ denote a measure of central tendency of $S$.


The mean square deviation   $bar x$ of $S$ is defined as the arithmetic mean of the square of the deviation of the elements of $S$ from $bar x$:

:$ds sum_{i mathop = 1}^n dfrac 1 n left( x_i - bar x right)^2$


=== Discrete Random Variable ===
Let $X$ be a discrete random variable.

Let $bar x$ denote a measure of central tendency of $X$.


The mean square deviation of $X$ is the second moment of $X$ about $bar x$.

=== Continuous Random Variable ===
Let $X$ be a continuous random variable.

Let $m$ denote the median of $X$.

Let the frequency function of $X$ be $f$.


The mean square deviation of $X$ is defined as:
:$ds int_{-infty}^{+infty} left( x - m right)^2 f left(   right)x ,mathrm d x$",Definition:Mean Square Deviation,,false,"Let S = { x_1, x_2, …, x_n } be a set of observations.

Let x̅ denote a measure of central tendency of S.


The mean square deviation   x̅ of S is defined as the arithmetic mean of the square of the deviation of the elements of S from x̅:

:∑_i  = 1^n  1 n ( x_i - x̅)^2


=== Discrete Random Variable ===
Let X be a discrete random variable.

Let x̅ denote a measure of central tendency of X.


The mean square deviation of X is the second moment of X about x̅.

=== Continuous Random Variable ===
Let X be a continuous random variable.

Let m denote the median of X.

Let the frequency function of X be f.


The mean square deviation of X is defined as:
:∫_-∞^+∞( x - m )^2 f (   )x  d x",Mean
"['Definitions/Mean Squared Error', 'Definitions/Variance', 'Definitions/Statistics']",Definition:Mean,The mean squared error is the expected value of the square of the difference between an estimator $T$ and the true parameter value $theta$.,Definition:Mean Squared Error,,false,The mean squared error is the expected value of the square of the difference between an estimator T and the true parameter value θ.,Mean
"['Definitions/Branches of Mathematics', 'Definitions/Measure Theory', 'Definitions/Analysis']",Definition:Measure,"Measure theory is the subfield of analysis concerned with the properties of measures, particularly the Lebesgue measure.",Definition:Measure Theory,analysis,true,"Measure theory is the subfield of analysis concerned with the properties of measures, particularly the Lebesgue measure.",Measure
"['Definitions/Measures', 'Definitions/Measure Theory']",Definition:Measure,"Let $left( X, unicode{x3a3} right)$ be a measurable space.

Let $mu: unicode{x3a3} to overline mathbb R$ be a mapping, where $overline mathbb R$ denotes the set of extended real numbers.


=== Definition 1 ===
Let $left( X, unicode{x3a3} right)$ be a measurable space.

Let $mu: unicode{x3a3} to overline mathbb R$ be a mapping, where $overline mathbb R$ denotes the set of extended real numbers.


$mu$ is called a measure on $unicode{x3a3}$  if and only if  $mu$ fulfils the following axioms:
 

=== Definition 2 ===
Let $left( X, unicode{x3a3} right)$ be a measurable space.

Let $mu: unicode{x3a3} to overline mathbb R$ be a mapping, where $overline mathbb R$ denotes the set of extended real numbers.


$mu$ is called a measure on $unicode{x3a3}$  if and only if  $mu$ fulfils the following axioms:
 

=== Definition 3 ===
Let $left( X, unicode{x3a3} right)$ be a measurable space.

Let $mu: unicode{x3a3} to overline mathbb R$ be a mapping, where $overline mathbb R$ denotes the set of extended real numbers.


$mu$ is called a measure on $unicode{x3a3}$  if and only if  $mu$ fulfils the following axioms:
 ",Definition:Measure (Measure Theory),,false,"Let ( X, x3a3) be a measurable space.

Let μ: x3a3→R be a mapping, where R denotes the set of extended real numbers.


=== Definition 1 ===
Let ( X, x3a3) be a measurable space.

Let μ: x3a3→R be a mapping, where R denotes the set of extended real numbers.


μ is called a measure on x3a3  if and only if  μ fulfils the following axioms:
 

=== Definition 2 ===
Let ( X, x3a3) be a measurable space.

Let μ: x3a3→R be a mapping, where R denotes the set of extended real numbers.


μ is called a measure on x3a3  if and only if  μ fulfils the following axioms:
 

=== Definition 3 ===
Let ( X, x3a3) be a measurable space.

Let μ: x3a3→R be a mapping, where R denotes the set of extended real numbers.


μ is called a measure on x3a3  if and only if  μ fulfils the following axioms:
 ",Measure
"['Definitions/Lebesgue Measures', 'Definitions/Measure Theory']",Definition:Measure,"Let $mathcal J_{ho}^n$ be the set of half-open $n$-rectangles.

Let $mathcal B left(   right){mathbb R^n}$ be the Borel $sigma$-algebra on $mathbb R^n$.


Let $lambda^n$ be the $n$-dimensional Lebesgue pre-measure on $mathcal J_{ho}^n$.

A measure $mu$ extending $lambda^n$ to $mathcal B left(   right){mathbb R^n}$ is called $n$-dimensional Lebesgue measure.

That is, $mu$ is an $n$-dimensional Lebesgue measure  if and only if  it satisfies:

:$mu restriction_{mathcal J_{ho}^n} = lambda^n$

where $restriction$ denotes restriction.


Usually, this measure is also denoted by $lambda^n$, even though this may be considered abuse of notation.


=== Lebesgue Measure on the Reals ===
For a given set $S subseteq mathbb R$, let $leftlbrace I_n rightrbrace$ be a countable set of open intervals such that:

:$S subseteq bigcup leftlbrace I_n rightrbrace$

For the power set $mathcal P left( mathbb R right)$ of the real numbers $mathbb R$, construct a function $mu^*: mathcal P left( mathbb R right) to mathbb R_{>0}$ as:

:$ds mu^* left(   right)S = inf leftlbrace sum_{n mathop in mathbb N} l left(   right){I_n} : leftlbrace I_n rightrbrace : S subseteq bigcup_{n mathop in mathbb N} I_n rightrbrace$

where:
:the infimum ranges over all such sets $leftlbrace I_n rightrbrace$
:$l left(   right){I_n}$ is the length of the interval $I_n$.

Then $mu^*$ is known as the Lebesgue outer measure and can be shown to be an outer measure.",Definition:Lebesgue Measure,,false,"Let 𝒥_ho^n be the set of half-open n-rectangles.

Let ℬ(   )ℝ^n be the Borel σ-algebra on ℝ^n.


Let λ^n be the n-dimensional Lebesgue pre-measure on 𝒥_ho^n.

A measure μ extending λ^n to ℬ(   )ℝ^n is called n-dimensional Lebesgue measure.

That is, μ is an n-dimensional Lebesgue measure  if and only if  it satisfies:

:μ_𝒥_ho^n = λ^n

where  denotes restriction.


Usually, this measure is also denoted by λ^n, even though this may be considered abuse of notation.


=== Lebesgue Measure on the Reals ===
For a given set S ⊆ℝ, let { I_n } be a countable set of open intervals such that:

:S ⊆⋃{ I_n }

For the power set 𝒫( ℝ) of the real numbers ℝ, construct a function μ^*: 𝒫( ℝ) →ℝ_>0 as:

:μ^* (   )S = inf{∑_n ∈ℕ l (   )I_n : { I_n } : S ⊆⋃_n ∈ℕ I_n }

where:
:the infimum ranges over all such sets { I_n }
:l (   )I_n is the length of the interval I_n.

Then μ^* is known as the Lebesgue outer measure and can be shown to be an outer measure.",Measure
['Definitions/Geometry'],Definition:Measure,"A geometric quantity $A$ is said to measure another quantity $B$ when the size of $A$ is a divisor of the size of $B$.

Category:Definitions/Geometry",Definition:Measure (Geometry),,false,"A geometric quantity A is said to measure another quantity B when the size of A is a divisor of the size of B.

Category:Definitions/Geometry",Measure
"['Definitions/Measurement', 'Definitions/Measurable Properties']",Definition:Measure,"Measurement is the process of determining the quantity of a measurable property according to some given scale of measurement.

A measurement is reported as a (real) number multiplied by a unit of measurement for that quantity.",Definition:Measurable Property/Measurement,,false,"Measurement is the process of determining the quantity of a measurable property according to some given scale of measurement.

A measurement is reported as a (real) number multiplied by a unit of measurement for that quantity.",Measure
"['Definitions/Medians of Triangles', 'Definitions/Cevians', 'Definitions/Triangles']",Definition:Median,"Let $triangle ABC$ be a triangle.

:

A median is a cevian which bisects the opposite.


In the above diagram, $CD$ is a median.",Definition:Median of Triangle,,false,"Let ABC be a triangle.

:

A median is a cevian which bisects the opposite.


In the above diagram, CD is a median.",Median
"['Definitions/Medians of Trapezia', 'Definitions/Trapezia']",Definition:Median,"Let $Box ABCD$ be a trapezium.

:

The median of $Box ABCD$ is the straight line through the midpoints of the legs of $Box ABCD$.


In the above diagram, $EF$ is the median of the trapezium $Box ABCD$.",Definition:Median of Trapezium,,false,"Let ABCD be a trapezium.

:

The median of ABCD is the straight line through the midpoints of the legs of ABCD.


In the above diagram, EF is the median of the trapezium ABCD.",Median
"['Definitions/Medians', 'Definitions/Measures of Central Tendency']",Definition:Median,"Let $S$ be a set of quantitative data.

Let $S$ be arranged in order of size.

The median is the element of $S$ that is in the middle of that ordered set.


Suppose there are an odd number of elements of $S$ such that $S$ has cardinality $2 n - 1$.

The median of $S$ in that case is the $n$th element of $S$.


Suppose there are an even number of elements of $S$ such that $S$ has cardinality $2 n$.

Then the middle of $S$ is not well-defined, and so the median of $S$ in that case is the arithmetic mean of the $n$th and $n + 1$th elements of $S$.


=== Continuous Random Variable ===
Let $X$ be a continuous random variable on a probability space $left( Omega, unicode{x3a3}, Pr right)$.

Let $X$ have probability density function $f_X$. 

A median of $X$ is defined as a real number $m_X$ such that: 

:$ds Pr left(   right){X < m_X} = int_{-infty}^{m_X} f_X left(   right)x ,mathrm d x = frac 1 2$

That is, $m_X$ is the first $2$-quantile of $X$.",Definition:Median (Statistics),,false,"Let S be a set of quantitative data.

Let S be arranged in order of size.

The median is the element of S that is in the middle of that ordered set.


Suppose there are an odd number of elements of S such that S has cardinality 2 n - 1.

The median of S in that case is the nth element of S.


Suppose there are an even number of elements of S such that S has cardinality 2 n.

Then the middle of S is not well-defined, and so the median of S in that case is the arithmetic mean of the nth and n + 1th elements of S.


=== Continuous Random Variable ===
Let X be a continuous random variable on a probability space ( Ω, x3a3, ).

Let X have probability density function f_X. 

A median of X is defined as a real number m_X such that: 

:(   )X < m_X = ∫_-∞^m_X f_X (   )x  d x = 1/2

That is, m_X is the first 2-quantile of X.",Median
['Definitions/Medians'],Definition:Median,"Let $X$ be a continuous random variable on a probability space $left( Omega, unicode{x3a3}, Pr right)$.

Let $X$ have probability density function $f_X$. 

A median of $X$ is defined as a real number $m_X$ such that: 

:$ds Pr left(   right){X < m_X} = int_{-infty}^{m_X} f_X left(   right)x ,mathrm d x = frac 1 2$

That is, $m_X$ is the first $2$-quantile of $X$.",Definition:Median of Continuous Random Variable,,false,"Let X be a continuous random variable on a probability space ( Ω, x3a3, ).

Let X have probability density function f_X. 

A median of X is defined as a real number m_X such that: 

:(   )X < m_X = ∫_-∞^m_X f_X (   )x  d x = 1/2

That is, m_X is the first 2-quantile of X.",Median
"['Definitions/Order Theory', 'Definitions/Lattice Theory']",Definition:Meet,"Let $left( S, preceq right)$ be an ordered set.

Let $a, b in S$, and suppose that their infimum $inf leftlbrace a, b rightrbrace$ exists in $S$.


Then $a wedge b$, the meet of $a$ and $b$, is defined as:

:$a wedge b = inf leftlbrace a, b rightrbrace$


Expanding the definition of infimum, one sees that $c = a wedge b$  if and only if :

:$(1): quad c preceq a$ and $c preceq b$
:$(2): quad forall s in S: s preceq a$ and $s preceq b implies s preceq c$",Definition:Meet (Order Theory),,false,"Let ( S, ≼) be an ordered set.

Let a, b ∈ S, and suppose that their infimum inf{ a, b } exists in S.


Then a ∧ b, the meet of a and b, is defined as:

:a ∧ b = inf{ a, b }


Expanding the definition of infimum, one sees that c = a ∧ b  if and only if :

:(1):    c ≼ a and c ≼ b
:(2):   ∀ s ∈ S: s ≼ a and s ≼ b  s ≼ c",Meet
['Definitions/Boolean Algebras'],Definition:Meet,"Consider the Boolean algebra $left( S, vee, wedge, neg right)$


The operation $wedge$ is called meet.",Definition:Boolean Algebra/Meet,,false,"Consider the Boolean algebra ( S, ∨, ∧, )


The operation ∧ is called meet.",Meet
['Definitions/Set Intersection'],Definition:Meet,"Let $leftlangle S_i rightrangle_{i mathop in I}$ be an family of sets indexed by some  indexing set $I$.

The sets in $leftlangle S_i rightrangle$ are said to meet  if and only if  their intersection is not empty.


That is,  if and only if :
:$ds bigcap_{i mathop in I} leftlangle S_i rightrangle ne varnothing$


That is,  if and only if  $leftlangle S_i rightrangle_{i mathop in I}$ is not disjoint.",Definition:Set Meeting Set,,false,"Let ⟨ S_i ⟩_i ∈ I be an family of sets indexed by some  indexing set I.

The sets in ⟨ S_i ⟩ are said to meet  if and only if  their intersection is not empty.


That is,  if and only if :
:⋂_i ∈ I⟨ S_i ⟩∅


That is,  if and only if  ⟨ S_i ⟩_i ∈ I is not disjoint.",Meet
"['Definitions/Metrizable Topologies', 'Definitions/Topology', 'Definitions/Metric Spaces', 'Definitions/Metrizable']",Definition:Metrizable,"Let $T = left( S, tau right)$ be a topological space.

=== Definition 1 ===
Let $T = left( S, tau right)$ be a topological space.


$T$ is said to be metrizable  if and only if  there exists a metric $d$ on $S$ such that:
:$tau$ is the topology induced by $d$ on $S$.

=== Definition 2 ===
Let $T = left( S, tau right)$ be a topological space.


$T$ is said to be metrizable  if and only if  there exists a metric space $M = left( A, d right)$ such that:
:$T$ is homeomorphic to the topological space $left( A, tau_d right)$ 
where $tau_d$ is the topology induced by $d$ on $A$.",Definition:Metrizable Topology,,false,"Let T = ( S, τ) be a topological space.

=== Definition 1 ===
Let T = ( S, τ) be a topological space.


T is said to be metrizable  if and only if  there exists a metric d on S such that:
:τ is the topology induced by d on S.

=== Definition 2 ===
Let T = ( S, τ) be a topological space.


T is said to be metrizable  if and only if  there exists a metric space M = ( A, d ) such that:
:T is homeomorphic to the topological space ( A, τ_d ) 
where τ_d is the topology induced by d on A.",Metrizable
"['Definitions/Metric Spaces', 'Definitions/Uniformities', 'Definitions/Metrizable']",Definition:Metrizable,"Let $M = left( A, d right)$ be a metric space.

Let $mathcal U$ be the uniformity on $X$ defined as:
:$mathcal U := leftlbrace u_epsilon: epsilon in mathbb R_{>0}  rightrbrace$
where:
:$mathbb R_{>0}$ is the set of strictly positive real numbers
:$u_epsilon$ is defined as:
::$u_epsilon := leftlbrace left( x, y right): d left(   right){x, y} < epsilon rightrbrace$


Then $mathcal U$ is defined as metrizable.",Definition:Metrizable Uniformity,,false,"Let M = ( A, d ) be a metric space.

Let 𝒰 be the uniformity on X defined as:
:𝒰 := { u_ϵ: ϵ∈ℝ_>0}
where:
:ℝ_>0 is the set of strictly positive real numbers
:u_ϵ is defined as:
::u_ϵ := {( x, y ): d (   )x, y < ϵ}


Then 𝒰 is defined as metrizable.",Metrizable
['Definitions/Euclidean Number Theory'],Definition:Minor,"Let $a, b in mathbb R_{>0}$ in the forms:
:$a = dfrac rho {sqrt 2} sqrt {1 + dfrac k {sqrt {1 + k^2} } }$
:$b = dfrac rho {sqrt 2} sqrt {1 - dfrac k {sqrt {1 + k^2} } }$

where:
:$rho$ is a rational number
:$k$ is a rational number whose square root is irrational.


Then $a - b$ is a minor.


 ",Definition:Minor (Euclidean),,false,"Let a, b ∈ℝ_>0 in the forms:
:a = ρ√(2)√(1 +  k √(1 + k^2))
:b = ρ√(2)√(1 -  k √(1 + k^2))

where:
:ρ is a rational number
:k is a rational number whose square root is irrational.


Then a - b is a minor.


 ",Minor
"['Definitions/Minor Axis of Ellipse', 'Definitions/Minor Axis', 'Definitions/Ellipses']",Definition:Minor,":

Consider an ellipse $K$ whose foci are $F_1$ and $F_2$.


=== Definition $1$ ===
:


Consider an ellipse $K$ whose foci are $F_1$ and $F_2$.

The minor axis of $K$ is the line segment through the center of $K$ perpendicular to the major axis of $K$ such that its endpoints are the points of intersection with $K$.


In the above diagram, $C_1 C_2$ is the minor axis of $K$.

=== Definition $2$ ===
:


Consider an ellipse $K$ whose foci are $F_1$ and $F_2$.

The minor axis of $K$ is the diameter of $K$ which has the smallest length.


In the above diagram, $C_1 C_2$ is the minor axis of $K$.

In the above diagram, $C_1 C_2$ is the minor axis of $K$.


=== Semi-Minor Axis ===
:

Consider an ellipse $K$.


A semi-minor axis of $K$ is either half of the minor axis of $K$ from its midpoint to its endpoint.


In the above diagram, $O C_1$ and $O C_2$ are the semi-minor axes of $K$.",Definition:Ellipse/Minor Axis,,false,":

Consider an ellipse K whose foci are F_1 and F_2.


=== Definition 1 ===
:


Consider an ellipse K whose foci are F_1 and F_2.

The minor axis of K is the line segment through the center of K perpendicular to the major axis of K such that its endpoints are the points of intersection with K.


In the above diagram, C_1 C_2 is the minor axis of K.

=== Definition 2 ===
:


Consider an ellipse K whose foci are F_1 and F_2.

The minor axis of K is the diameter of K which has the smallest length.


In the above diagram, C_1 C_2 is the minor axis of K.

In the above diagram, C_1 C_2 is the minor axis of K.


=== Semi-Minor Axis ===
:

Consider an ellipse K.


A semi-minor axis of K is either half of the minor axis of K from its midpoint to its endpoint.


In the above diagram, O C_1 and O C_2 are the semi-minor axes of K.",Minor
"['Definitions/Semi-Minor Axis of Ellipse', 'Definitions/Minor Axis', 'Definitions/Ellipses']",Definition:Minor,":

Consider an ellipse $K$.


A semi-minor axis of $K$ is either half of the minor axis of $K$ from its midpoint to its endpoint.


In the above diagram, $O C_1$ and $O C_2$ are the semi-minor axes of $K$.",Definition:Ellipse/Minor Axis/Semi-Minor Axis,,false,":

Consider an ellipse K.


A semi-minor axis of K is either half of the minor axis of K from its midpoint to its endpoint.


In the above diagram, O C_1 and O C_2 are the semi-minor axes of K.",Minor
['Definitions/Categorical Syllogisms'],Definition:Minor,"The minor premise of a categorical syllogism is conventionally stated second.

It is a categorical statement which expresses the logical relationship between the secondary term and the middle term of the syllogism.",Definition:Categorical Syllogism/Premises/Minor Premise,,false,"The minor premise of a categorical syllogism is conventionally stated second.

It is a categorical statement which expresses the logical relationship between the secondary term and the middle term of the syllogism.",Minor
['Definitions/Determinants'],Definition:Minor,"Let $mathbf A = left[ a right]_n$ be a square matrix of order $n$.

Consider the order $k$ square submatrix $mathbf B$ obtained by deleting $n - k$ rows and $n - k$ columns from $mathbf A$.


Let $det left(   right){mathbf B}$ denote the determinant of $mathbf B$.

Then $det left(   right){mathbf B}$ is an order-$k$ minor of $det left(   right){mathbf A}$.


Thus a minor is a determinant formed from the elements (in the same relative order) of $k$ specified rows and columns.",Definition:Minor of Determinant,,false,"Let 𝐀 = [ a ]_n be a square matrix of order n.

Consider the order k square submatrix 𝐁 obtained by deleting n - k rows and n - k columns from 𝐀.


Let (   )𝐁 denote the determinant of 𝐁.

Then (   )𝐁 is an order-k minor of (   )𝐀.


Thus a minor is a determinant formed from the elements (in the same relative order) of k specified rows and columns.",Minor
"['Definitions/Model Theory', 'Definitions/Symbolic Logic', 'Definitions/Formal Semantics']",Definition:Model,"Let $mathscr M$ be a formal semantics for a logical language $mathcal L$.

Let $mathcal M$ be a structure of $mathscr M$.


=== Model of Logical Formula ===
Let $mathscr M$ be a formal semantics for a logical language $mathcal L$.

Let $mathcal M$ be a structure of $mathscr M$.


Let $phi$ be a logical formula of $mathcal L$.

Then $mathcal M$ is a model of $phi$  if and only if :

:$mathcal M models_{mathscr M} phi$

that is,  if and only if  $phi$ is valid in $mathcal M$.


 

Category:Definitions/Model Theory
Category:Definitions/Formal Semantics

=== Model of Set of Logical Formulas ===
Let $mathscr M$ be a formal semantics for a logical language $mathcal L$.

Let $mathcal M$ be a structure of $mathscr M$.


Let $mathcal F$ be a set of logical formulas of $mathcal L$.

Then $mathcal M$ is a model of $mathcal F$  if and only if :

:$mathcal M models_{mathscr M} phi$ for every $phi in mathcal F$

that is,  if and only if  it is a model of every logical formula $phi in mathcal F$.",Definition:Model (Logic),,false,"Let ℳ be a formal semantics for a logical language ℒ.

Let ℳ be a structure of ℳ.


=== Model of Logical Formula ===
Let ℳ be a formal semantics for a logical language ℒ.

Let ℳ be a structure of ℳ.


Let ϕ be a logical formula of ℒ.

Then ℳ is a model of ϕ  if and only if :

:ℳ_ℳϕ

that is,  if and only if  ϕ is valid in ℳ.


 

Category:Definitions/Model Theory
Category:Definitions/Formal Semantics

=== Model of Set of Logical Formulas ===
Let ℳ be a formal semantics for a logical language ℒ.

Let ℳ be a structure of ℳ.


Let ℱ be a set of logical formulas of ℒ.

Then ℳ is a model of ℱ  if and only if :

:ℳ_ℳϕ for every ϕ∈ℱ

that is,  if and only if  it is a model of every logical formula ϕ∈ℱ.",Model
['Definitions/Model Theory for Predicate Logic'],Definition:Model,"Let $mathcal L_1$ be the language of predicate logic.

Let $mathcal A$ be a structure for predicate logic.


Then $mathcal A$ models a sentence $mathbf A$  if and only if :

:$operatorname{val}_mathcal A left(   right){mathbf A} = mathrm T$

where $operatorname{val}_mathcal A left(   right){mathbf A}$ denotes the value of $mathbf A$ in $mathcal A$.


This relationship is denoted:

:$mathcal A models_{mathrm{PL} } mathbf A$


When pertaining to a collection of sentences $mathcal F$, one says $mathcal A$ models $mathcal F$  if and only if :

:$forall mathbf A in mathcal F: mathcal A models_{mathrm{PL} } mathbf A$

that is,  if and only if  it models all elements of $mathcal F$.

This can be expressed symbolically as:

:$mathcal A models_{mathrm {PL} } mathcal F$",Definition:Model (Predicate Logic),,false,"Let ℒ_1 be the language of predicate logic.

Let 𝒜 be a structure for predicate logic.


Then 𝒜 models a sentence 𝐀  if and only if :

:val_𝒜(   )𝐀 = T

where val_𝒜(   )𝐀 denotes the value of 𝐀 in 𝒜.


This relationship is denoted:

:𝒜_PL𝐀


When pertaining to a collection of sentences ℱ, one says 𝒜 models ℱ  if and only if :

:∀𝐀∈ℱ: 𝒜_PL𝐀

that is,  if and only if  it models all elements of ℱ.

This can be expressed symbolically as:

:𝒜_PLℱ",Model
"['Definitions/Applied Mathematics', 'Definitions/Statistics', 'Definitions/Mathematical Models']",Definition:Model,"A mathematical model is an equation, or a system of equations, whose purpose is to provide an approximation to the behavior of a real-world phenomenon.


=== Constant ===
Let $P$ be a stochastic process which is being modelled by means of a model $M$.

A constant of $M$ is a number which implements some aspect of $P$ which is not changed during the evolution of $P$.

=== Parameter ===
Let $P$ be a stochastic process which is being modelled by means of a model $M$.

A parameter of $M$ is a number which implements some aspect of $P$ which is designed so as to be able to be modified during the evolution of $P$ as a result of analysis of the behaviour of $P$ over time.",Definition:Mathematical Model,,false,"A mathematical model is an equation, or a system of equations, whose purpose is to provide an approximation to the behavior of a real-world phenomenon.


=== Constant ===
Let P be a stochastic process which is being modelled by means of a model M.

A constant of M is a number which implements some aspect of P which is not changed during the evolution of P.

=== Parameter ===
Let P be a stochastic process which is being modelled by means of a model M.

A parameter of M is a number which implements some aspect of P which is designed so as to be able to be modified during the evolution of P as a result of analysis of the behaviour of P over time.",Model
['Definitions/Polynomial Theory'],Definition:Modulus,"Let $k = leftlangle k_j rightrangle_{j mathop in J}$ be a multiindex.


The modulus of such a multiindex $k$  is defined by:
:$ds leftlvert k rightrvert = sum_{j mathop in J} k_j$


 

Note that, since by definition all but finitely many of the $k_j$ are zero, this summation is convergent.",Definition:Multiindex/Modulus,,false,"Let k = ⟨ k_j ⟩_j ∈ J be a multiindex.


The modulus of such a multiindex k  is defined by:
:| k | = ∑_j ∈ J k_j


 

Note that, since by definition all but finitely many of the k_j are zero, this summation is convergent.",Modulus
"['Definitions/Complex Modulus', 'Definitions/Complex Numbers', 'Definitions/Complex Analysis', 'Definitions/Polar Form of Complex Number', 'Definitions/Examples of Norms']",Definition:Modulus,"Let $z = a + i b$ be a complex number, where $a, b in mathbb R$.


The (complex) modulus of $z$ is written $leftlvert z rightrvert$, and is defined as the square root of the sum of the squares of the real and imaginary parts:

:$leftlvert z rightrvert := sqrt {a^2 + b^2}$


The (complex) modulus is a real-valued function, and, as and when appropriate, can be referred to as the (complex) modulus function.",Definition:Complex Modulus,,false,"Let z = a + i b be a complex number, where a, b ∈ℝ.


The (complex) modulus of z is written | z |, and is defined as the square root of the sum of the squares of the real and imaginary parts:

:| z | := √(a^2 + b^2)


The (complex) modulus is a real-valued function, and, as and when appropriate, can be referred to as the (complex) modulus function.",Modulus
['Definitions/Complex Analysis'],Definition:Modulus,"Let $f: S to mathbb C$ be a complex-valued function.


Then the (complex) modulus of $f$ is written $leftlvert f rightrvert : S to mathbb R$ and is the real-valued function defined as:

:$forall z in S: leftlvert f rightrvert left(   right)z = leftlvert f left(   right)z rightrvert$",Definition:Modulus of Complex-Valued Function,,false,"Let f: S →ℂ be a complex-valued function.


Then the (complex) modulus of f is written | f | : S →ℝ and is the real-valued function defined as:

:∀ z ∈ S: | f |(   )z = | f (   )z |",Modulus
['Definitions/Geometric Function Theory'],Definition:Modulus,"In geometric function theory, the term modulus is used to denote certain conformal invariants of configurations or curve families.

More precisely, the modulus of a curve family $Gamma$ is the reciprocal of its extremal length:
:$mod Gamma := dfrac 1 {lambda left(   right)Gamma}$


=== Modulus of a Quadrilateral ===

Consider a quadrilateral; that is, a Jordan domain $Q$ in the complex plane (or some other Riemann surface), together with two disjoint closed boundary arcs $alpha$ and $alpha'$.

Then the modulus of the quadrilateral $Q left(   right){alpha, alpha'}$ is the extremal length of the family of curves in $Q$ that connect $alpha$ and $alpha'$.

Equivalently, there exists a rectangle $R = leftlbrace x + i y: leftlvert x rightrvert < a, leftlvert y rightrvert < b rightrbrace$ and a conformal isomorphism between $Q$ and $R$ under which $alpha$ and $alpha'$ correspond to the vertical sides of $R$.

Then the modulus of $Q left(   right){alpha, alpha'}$ is equal to the ratio $a/b$.


See Modulus of a Quadrilateral.


=== Modulus of an Annulus ===

Consider an annulus $A$; that is, a domain whose boundary consists of two Jordan curves.

Then the modulus $mod A$ is the extremal length of the family of curves in $A$ that connect the two boundary components of $A$.

Equivalently, there is a round annulus $tilde A = leftlbrace z in mathbb C: r < leftlvert z rightrvert < R rightrbrace$ that is conformally equivalent to $A$.

Then:
:$mod A := dfrac 1 {2 pi} ln left(   right){dfrac R r}$

The modulus of $A$ can also be denoted $M left(   right)R$.",Definition:Modulus (Geometric Function Theory),,false,"In geometric function theory, the term modulus is used to denote certain conformal invariants of configurations or curve families.

More precisely, the modulus of a curve family Γ is the reciprocal of its extremal length:
:Γ :=  1 λ(   )Γ


=== Modulus of a Quadrilateral ===

Consider a quadrilateral; that is, a Jordan domain Q in the complex plane (or some other Riemann surface), together with two disjoint closed boundary arcs α and α'.

Then the modulus of the quadrilateral Q (   )α, α' is the extremal length of the family of curves in Q that connect α and α'.

Equivalently, there exists a rectangle R = { x + i y: | x | < a, | y | < b } and a conformal isomorphism between Q and R under which α and α' correspond to the vertical sides of R.

Then the modulus of Q (   )α, α' is equal to the ratio a/b.


See Modulus of a Quadrilateral.


=== Modulus of an Annulus ===

Consider an annulus A; that is, a domain whose boundary consists of two Jordan curves.

Then the modulus A is the extremal length of the family of curves in A that connect the two boundary components of A.

Equivalently, there is a round annulus Ã = { z ∈ℂ: r < | z | < R } that is conformally equivalent to A.

Then:
:A :=  1 2 πln(   ) R r

The modulus of A can also be denoted M (   )R.",Modulus
['Definitions/Congruence (Number Theory)'],Definition:Modulus,"Let $x$ be congruent to $y$ modulo $m$.

The number $m$ in this congruence is known as the modulus of the congruence.",Definition:Congruence (Number Theory)/Modulus,,false,"Let x be congruent to y modulo m.

The number m in this congruence is known as the modulus of the congruence.",Modulus
"['Definitions/Elastic Moduli', 'Definitions/Elasticity']",Definition:Modulus,"An elastic modulus is a ratio of stress to strain for a body or a substance which obeys Hooke's Law.

Hence an elastic modulus is the slope of the linear region of a stress-strain diagram.

The type of elastic modulus is dependent on the type of strain under discussion.


=== Young's Modulus ===
 

=== Bulk Modulus ===
Let $B$ be an elastic body.

The bulk modulus of $B$ is a physical property of $B$ which measures its resistance to change in volume without change of shape.

It is defined as the ratio of compressive stress per surface area of $B$ to its change of volume per unit volume associated with this stress, assuming uniform pressure over the surface of $B$.

=== Rigidity Modulus ===
 ",Definition:Elastic Modulus,,false,"An elastic modulus is a ratio of stress to strain for a body or a substance which obeys Hooke's Law.

Hence an elastic modulus is the slope of the linear region of a stress-strain diagram.

The type of elastic modulus is dependent on the type of strain under discussion.


=== Young's Modulus ===
 

=== Bulk Modulus ===
Let B be an elastic body.

The bulk modulus of B is a physical property of B which measures its resistance to change in volume without change of shape.

It is defined as the ratio of compressive stress per surface area of B to its change of volume per unit volume associated with this stress, assuming uniform pressure over the surface of B.

=== Rigidity Modulus ===
 ",Modulus
"['Definitions/Bulk Modulus', 'Definitions/Physical Quantities']",Definition:Modulus,"Let $B$ be an elastic body.

The bulk modulus of $B$ is a physical property of $B$ which measures its resistance to change in volume without change of shape.

It is defined as the ratio of compressive stress per surface area of $B$ to its change of volume per unit volume associated with this stress, assuming uniform pressure over the surface of $B$.",Definition:Bulk Modulus,,false,"Let B be an elastic body.

The bulk modulus of B is a physical property of B which measures its resistance to change in volume without change of shape.

It is defined as the ratio of compressive stress per surface area of B to its change of volume per unit volume associated with this stress, assuming uniform pressure over the surface of B.",Modulus
"['Definitions/Monomorphisms (Abstract Algebra)', 'Definitions/Injections', 'Definitions/Homomorphisms (Abstract Algebra)', 'Definitions/Monomorphisms']",Definition:Monomorphism,"A homomorphism which is an injection is descibed as monic, or called a monomorphism.


=== Semigroup Monomorphism ===
Let $left( S, circ right)$ and $left( T, * right)$ be semigroups.

Let $phi: S to T$ be a (semigroup) homomorphism.


Then $phi$ is a semigroup monomorphism  if and only if  $phi$ is an injection.

=== Group Monomorphism ===
Let $left( G, circ right)$ and $left( H, * right)$ be groups.

Let $phi: G to H$ be a (group) homomorphism.


Then $phi$ is a group monomorphism  if and only if  $phi$ is an injection.

=== Ring Monomorphism ===
Let $left( R, +, circ right)$ and $left( S, oplus, * right)$ be rings.

Let $phi: R to S$ be a (ring) homomorphism.


Then $phi$ is a ring monomorphism  if and only if  $phi$ is an injection.

=== Field Monomorphism ===
Let $left( F, +, circ right)$ and $left( K, oplus, * right)$ be fields.

Let $phi: F to K$ be a (field) homomorphism.


Then $phi$ is a field monomorphism  if and only if  $phi$ is an injection.

=== $R$-Algebraic Structure Monomorphism ===
Let $left( S, ast_1, ast_2, ldots, ast_n, circ right)_R$ and $left( T, odot_1, odot_2, ldots, odot_n, otimes right)_R$ be $R$-algebraic structures.

Then $phi: S to T$ is an $R$-algebraic structure monomorphism  if and only if :

:$(1): quad phi$ is an injection
:$(2): quad forall k: k in left[ 1 ,.,.,   right]n: forall x, y in S: phi left(   right){x ast_k y} = phi left(   right)x odot_k phi left(   right)y$
:$(3): quad forall x in S: forall lambda in R: phi left(   right){lambda circ x} = lambda otimes phi left(   right)x$.


That is,  if and only if :

:$(1): quad phi$ is an injection
:$(2): quad phi$ is an $R$-algebraic structure homomorphism.


This definition continues to apply when $S$ and $T$ are modules, and also when they are vector spaces.


=== Vector Space Monomorphism ===
Let $V$ and $W$ be $K$-vector spaces.

Then $phi: V to W$ is a vector space monomorphism  if and only if :

:$(1): quad phi$ is an injection
:$(2): quad forall mathbf x, mathbf y in V: phi left(   right){mathbf x + mathbf y} = phi left(   right){mathbf x} + phi left(   right){mathbf y}$
:$(3): quad forall mathbf x in V: forall lambda in K: phi left(   right){lambda mathbf x} = lambda phi left(   right){mathbf x}$

=== Ordered Structure Monomorphism ===
Let $left( S, circ, preceq right)$ and $left( T, *, preccurlyeq right)$ be ordered structures.


An ordered structure monomorphism from $left( S, circ, preceq right)$ to $left( T, *, preccurlyeq right)$ is a mapping $phi: S to T$ that is both:

:$(1): quad$ A monomorphism, i.e. an injective homomorphism, from the structure $left( S, circ right)$ to the structure $left( T, * right)$

:$(2): quad$ An order embedding from the ordered set $left( S, preceq right)$ to the ordered set $left( T, preccurlyeq right)$.


=== Ordered Semigroup Monomorphism ===
Let $left( S, circ, preceq right)$ and $left( T, *, preccurlyeq right)$ be ordered semigroups.


An ordered semigroup monomorphism from $left( S, circ, preceq right)$ to $left( T, *, preccurlyeq right)$ is a mapping $phi: S to T$ that is both:

:$(1): quad$ A (semigroup) monomorphism from the semigroup $left( S, circ right)$ to the semigroup $left( T, * right)$

:$(2): quad$ An order embedding from the ordered set $left( S, preceq right)$ to the ordered set $left( T, preccurlyeq right)$.

=== Ordered Group Monomorphism ===
Let $left( S, circ, preceq right)$ and $left( T, *, preccurlyeq right)$ be ordered groups.


An ordered group monomorphism from $left( S, circ, preceq right)$ to $left( T, *, preccurlyeq right)$ is a mapping $phi: S to T$ that is both:

:$(1): quad$ A group monomorphism from the group $left( S, circ right)$ to the group $left( T, * right)$

:$(2): quad$ An order embedding from the ordered set $left( S, preceq right)$ to the ordered set $left( T, preccurlyeq right)$.

=== Ordered Ring Monomorphism ===
Let $left( S, +, circ, preceq right)$ and $left( T, oplus, *, preccurlyeq right)$ be ordered rings.


An ordered ring monomorphism from $left( S, +, circ, preceq right)$ to $left( T, oplus, *, preccurlyeq right)$ is a mapping $phi: S to T$ that is both:

:$(1): quad$ An ordered group monomorphism from the ordered group $left( S, +, preceq right)$ to the ordered group $left( T, oplus, preccurlyeq right)$

:$(2): quad$ A semigroup monomorphism from the semigroup $left( S, circ right)$ to the semigroup $left( T, * right)$.",Definition:Monomorphism (Abstract Algebra),,false,"A homomorphism which is an injection is descibed as monic, or called a monomorphism.


=== Semigroup Monomorphism ===
Let ( S, ∘) and ( T, * ) be semigroups.

Let ϕ: S → T be a (semigroup) homomorphism.


Then ϕ is a semigroup monomorphism  if and only if  ϕ is an injection.

=== Group Monomorphism ===
Let ( G, ∘) and ( H, * ) be groups.

Let ϕ: G → H be a (group) homomorphism.


Then ϕ is a group monomorphism  if and only if  ϕ is an injection.

=== Ring Monomorphism ===
Let ( R, +, ∘) and ( S, ⊕, * ) be rings.

Let ϕ: R → S be a (ring) homomorphism.


Then ϕ is a ring monomorphism  if and only if  ϕ is an injection.

=== Field Monomorphism ===
Let ( F, +, ∘) and ( K, ⊕, * ) be fields.

Let ϕ: F → K be a (field) homomorphism.


Then ϕ is a field monomorphism  if and only if  ϕ is an injection.

=== R-Algebraic Structure Monomorphism ===
Let ( S, ∗_1, ∗_2, …, ∗_n, ∘)_R and ( T, ⊙_1, ⊙_2, …, ⊙_n, ⊗)_R be R-algebraic structures.

Then ϕ: S → T is an R-algebraic structure monomorphism  if and only if :

:(1):   ϕ is an injection
:(2):   ∀ k: k ∈[ 1  . . ]n: ∀ x, y ∈ S: ϕ(   )x ∗_k y = ϕ(   )x ⊙_k ϕ(   )y
:(3):   ∀ x ∈ S: ∀λ∈ R: ϕ(   )λ∘ x = λ⊗ϕ(   )x.


That is,  if and only if :

:(1):   ϕ is an injection
:(2):   ϕ is an R-algebraic structure homomorphism.


This definition continues to apply when S and T are modules, and also when they are vector spaces.


=== Vector Space Monomorphism ===
Let V and W be K-vector spaces.

Then ϕ: V → W is a vector space monomorphism  if and only if :

:(1):   ϕ is an injection
:(2):   ∀𝐱, 𝐲∈ V: ϕ(   )𝐱 + 𝐲 = ϕ(   )𝐱 + ϕ(   )𝐲
:(3):   ∀𝐱∈ V: ∀λ∈ K: ϕ(   )λ𝐱 = λϕ(   )𝐱

=== Ordered Structure Monomorphism ===
Let ( S, ∘, ≼) and ( T, *, ≼) be ordered structures.


An ordered structure monomorphism from ( S, ∘, ≼) to ( T, *, ≼) is a mapping ϕ: S → T that is both:

:(1): A monomorphism, i.e. an injective homomorphism, from the structure ( S, ∘) to the structure ( T, * )

:(2): An order embedding from the ordered set ( S, ≼) to the ordered set ( T, ≼).


=== Ordered Semigroup Monomorphism ===
Let ( S, ∘, ≼) and ( T, *, ≼) be ordered semigroups.


An ordered semigroup monomorphism from ( S, ∘, ≼) to ( T, *, ≼) is a mapping ϕ: S → T that is both:

:(1): A (semigroup) monomorphism from the semigroup ( S, ∘) to the semigroup ( T, * )

:(2): An order embedding from the ordered set ( S, ≼) to the ordered set ( T, ≼).

=== Ordered Group Monomorphism ===
Let ( S, ∘, ≼) and ( T, *, ≼) be ordered groups.


An ordered group monomorphism from ( S, ∘, ≼) to ( T, *, ≼) is a mapping ϕ: S → T that is both:

:(1): A group monomorphism from the group ( S, ∘) to the group ( T, * )

:(2): An order embedding from the ordered set ( S, ≼) to the ordered set ( T, ≼).

=== Ordered Ring Monomorphism ===
Let ( S, +, ∘, ≼) and ( T, ⊕, *, ≼) be ordered rings.


An ordered ring monomorphism from ( S, +, ∘, ≼) to ( T, ⊕, *, ≼) is a mapping ϕ: S → T that is both:

:(1): An ordered group monomorphism from the ordered group ( S, +, ≼) to the ordered group ( T, ⊕, ≼)

:(2): A semigroup monomorphism from the semigroup ( S, ∘) to the semigroup ( T, * ).",Monomorphism
"['Definitions/Category Theory', 'Definitions/Monomorphisms']",Definition:Monomorphism,"Let $mathbf C$ be a metacategory.

A monomorphism is a morphism $f in mathbf C_1$ such that:

:$f circ g = f circ h implies g = h$

for all morphisms $g, h in mathbf C_1$ for which these compositions are defined.


That is, a monomorphism is a morphism which is left cancellable. 


One writes $f: C rightarrowtail D$ to denote that $f$ is a monomorphism.",Definition:Monomorphism (Category Theory),,false,"Let 𝐂 be a metacategory.

A monomorphism is a morphism f ∈𝐂_1 such that:

:f ∘ g = f ∘ h  g = h

for all morphisms g, h ∈𝐂_1 for which these compositions are defined.


That is, a monomorphism is a morphism which is left cancellable. 


One writes f: C ↣ D to denote that f is a monomorphism.",Monomorphism
['Definitions/Order Theory'],Definition:Monotone,"=== Ordered Sets ===
Let $left( S, preceq_1 right)$ and $left( T, preceq_2 right)$ be ordered sets.

Let $phi: left( S, preceq_1 right) to left( T, preceq_2 right)$ be a mapping.


Then $phi$ is monotone  if and only if  it is either increasing or decreasing.


Note that this definition also holds if $S = T$.

=== Real Functions ===
This definition continues to hold when $S = T = mathbb R$.

Thus, let $f$ be a real function.

Then $f$ is monotone  if and only if  it is either increasing or decreasing.

=== Sequences ===
Let $left( S, preceq right)$ be a totally ordered set.


A sequence $leftlangle a_k rightrangle_{k mathop in A}$ of elements of $S$ is monotone  if and only if  it is either increasing or decreasing.


=== Real Sequence ===

The above definition for sequences is usually applied to real number sequences:

Let $leftlangle x_n rightrangle$ be a sequence in $mathbb R$.


Then $leftlangle x_n rightrangle$ is monotone  if and only if  it is either increasing or decreasing.",Definition:Monotone (Order Theory),,false,"=== Ordered Sets ===
Let ( S, ≼_1 ) and ( T, ≼_2 ) be ordered sets.

Let ϕ: ( S, ≼_1 ) →( T, ≼_2 ) be a mapping.


Then ϕ is monotone  if and only if  it is either increasing or decreasing.


Note that this definition also holds if S = T.

=== Real Functions ===
This definition continues to hold when S = T = ℝ.

Thus, let f be a real function.

Then f is monotone  if and only if  it is either increasing or decreasing.

=== Sequences ===
Let ( S, ≼) be a totally ordered set.


A sequence ⟨ a_k ⟩_k ∈ A of elements of S is monotone  if and only if  it is either increasing or decreasing.


=== Real Sequence ===

The above definition for sequences is usually applied to real number sequences:

Let ⟨ x_n ⟩ be a sequence in ℝ.


Then ⟨ x_n ⟩ is monotone  if and only if  it is either increasing or decreasing.",Monotone
['Definitions/Set Systems'],Definition:Monotone,"Let $mathcal S$ be an algebra of sets.

Let $f: mathcal S to overline mathbb R$ be an extended real-valued function, where $overline mathbb R$ denotes the set of extended real numbers.


Then $f$ is defined as monotone or monotonic  if and only if :
:$forall A, B in mathcal S: A subseteq B iff f left(   right)A le f left(   right)B$

Category:Definitions/Set Systems",Definition:Monotone (Measure Theory),,false,"Let 𝒮 be an algebra of sets.

Let f: 𝒮→R be an extended real-valued function, where R denotes the set of extended real numbers.


Then f is defined as monotone or monotonic  if and only if :
:∀ A, B ∈𝒮: A ⊆ B  f (   )A ≤ f (   )B

Category:Definitions/Set Systems",Monotone
['Definitions/Set Systems'],Definition:Monotone,"Let $X$ be a set, and let $mathcal P left( X right)$ be its power set.

Let $mathcal M subseteq mathcal P left( X right)$ be a collection of subsets of $X$.


Then $mathcal M$ is said to be a monotone class (on $X$)  if and only if  for every countable, nonempty, index set $I$, it holds that:

:$ds leftlangle A_i rightrangle_{i mathop in I} in mathcal M implies bigcup_{i mathop in I} A_i in mathcal M$
:$ds leftlangle A_i rightrangle_{i mathop in I} in mathcal M implies bigcap_{i mathop in I} A_i in mathcal M$

that is,  if and only if  $mathcal M$ is closed under countable unions and countable intersections.",Definition:Monotone Class,,false,"Let X be a set, and let 𝒫( X ) be its power set.

Let ℳ⊆𝒫( X ) be a collection of subsets of X.


Then ℳ is said to be a monotone class (on X)  if and only if  for every countable, nonempty, index set I, it holds that:

:⟨ A_i ⟩_i ∈ I∈ℳ⋃_i ∈ I A_i ∈ℳ
:⟨ A_i ⟩_i ∈ I∈ℳ⋂_i ∈ I A_i ∈ℳ

that is,  if and only if  ℳ is closed under countable unions and countable intersections.",Monotone
"['Definitions/Multiplicative Functions', 'Definitions/Field Theory', 'Definitions/Number Theory']",Definition:Multiplicative Function,"Let $K$ be a field.

Let $f: K to K$ be a function on $K$.


Then $f$ is described as completely multiplicative  if and only if :

:$forall m, n in K: f left(   right){m n} = f left(   right)m f left(   right)n$


That is, a completely multiplicative function is one where the value of a product of two numbers equals the product of the value of each one individually.",Definition:Completely Multiplicative Function,,false,"Let K be a field.

Let f: K → K be a function on K.


Then f is described as completely multiplicative  if and only if :

:∀ m, n ∈ K: f (   )m n = f (   )m f (   )n


That is, a completely multiplicative function is one where the value of a product of two numbers equals the product of the value of each one individually.",Multiplicative Function
"['Definitions/Multiplicative Functions', 'Definitions/Ring Theory', 'Definitions/Number Theory']",Definition:Multiplicative Function,"Let $left( R, +, circ right)$ be a ring.

Let $f: R to mathbb R$ be a (real-valued) function on $R$.


$f$ is a multiplicative function on $R$  if and only if :

:$forall x, y in R: f left(   right){x circ y} = f left(   right)x times f left(   right)y$


That is, a multiplicative function on $R$ is one where the value of the product of two elements of $R$ equals the product of their values.",Definition:Multiplicative Function on Ring,,false,"Let ( R, +, ∘) be a ring.

Let f: R →ℝ be a (real-valued) function on R.


f is a multiplicative function on R  if and only if :

:∀ x, y ∈ R: f (   )x ∘ y = f (   )x × f (   )y


That is, a multiplicative function on R is one where the value of the product of two elements of R equals the product of their values.",Multiplicative Function
['Definitions/Ring Theory'],Definition:Multiplicative Function,"Let $R$ be a unique factorization domain.

Let $f : R to mathbb C$ be a complex-valued function.


Then $f$ is multiplicative  if and only if :
:For all coprime $x, yin R$: $f left({x y}right) = f left({x}right) f left({y}right)$


=== Arithmetic Function ===
Let $f : mathbb N to mathbb C$ be an arithmetic function.


Then $f$ is multiplicative  if and only if :
:$m perp n implies f left(   right){m n} = f left(   right)m f left(   right)n$
where $m perp n$ denotes that $m$ is coprime to $n$.


That is, a multiplicative arithmetic function is one where the value of a product of two coprime numbers equals the product of the value of each one individually.",Definition:Multiplicative Function on UFD,,false,"Let R be a unique factorization domain.

Let f : R →ℂ be a complex-valued function.


Then f is multiplicative  if and only if :
:For all coprime x, y∈ R: f (x y) = f (x) f (y)


=== Arithmetic Function ===
Let f : ℕ→ℂ be an arithmetic function.


Then f is multiplicative  if and only if :
:m ⊥ n  f (   )m n = f (   )m f (   )n
where m ⊥ n denotes that m is coprime to n.


That is, a multiplicative arithmetic function is one where the value of a product of two coprime numbers equals the product of the value of each one individually.",Multiplicative Function
['Definitions/Multiple Roots'],Definition:Multiplicity,The multiplicity of a multiple root is the number of times it appears.,Definition:Multiple Root/Multiplicity,,false,The multiplicity of a multiple root is the number of times it appears.,Multiplicity
['Definitions/Complex Analysis'],Definition:Multiplicity,"Let $f: mathbb C to mathbb C$ be a function.

Suppose there is $a in mathbb C$ such that $f left(   right)a = 0$.

Then $a$ is said to be a zero of multiplicity $k$  if and only if  there exists non-zero $L in mathbb R$ such that:

:$ds lim_{z mathop to a} dfrac {leftlvert f left(   right)z rightrvert } {leftlvert z - a rightrvert^k} = L$

 

Category:Definitions/Complex Analysis",Definition:Multiplicity (Complex Analysis),,false,"Let f: ℂ→ℂ be a function.

Suppose there is a ∈ℂ such that f (   )a = 0.

Then a is said to be a zero of multiplicity k  if and only if  there exists non-zero L ∈ℝ such that:

:lim_z → a| f (   )z || z - a |^k = L

 

Category:Definitions/Complex Analysis",Multiplicity
['Definitions/Multigraphs'],Definition:Multiplicity,"Let $G = left( V, E right)$ be a multigraph.

The multiplicity of an edge is the number of edges having the same pair of endvertices.


For example, simple edges have multiplicity $1$.

Thus, an edge is a multiple edge  if and only if  its multiplicity exceeds $1$.

Category:Definitions/Multigraphs",Definition:Multiple Edge/Multiplicity,,false,"Let G = ( V, E ) be a multigraph.

The multiplicity of an edge is the number of edges having the same pair of endvertices.


For example, simple edges have multiplicity 1.

Thus, an edge is a multiple edge  if and only if  its multiplicity exceeds 1.

Category:Definitions/Multigraphs",Multiplicity
['Definitions/Multigraphs'],Definition:Multiplicity,"The multiplicity of a multigraph is the maximum multiplicity of its (multiple) edges.


Category:Definitions/Multigraphs",Definition:Multigraph/Multiplicity,maximum multiplicity,true,"The multiplicity of a multigraph is the maximum multiplicity of its (multiple) edges.


Category:Definitions/Multigraphs",Multiplicity
['Definitions/Prime Decompositions'],Definition:Multiplicity,"Let $n > 1 in mathbb Z$.

Let:
:$n = p_1^{k_1} p_2^{k_2} cdots p_r^{k_r}$
be the prime decomposition of $n$, where:
:$p_1 < p_2 < cdots < p_r$ are distinct primes
:$k_1, k_2, ldots, k_r$ are (strictly) positive integers.


For each $p_j in leftlbrace p_1, p_2, ldots, p_r rightrbrace$, its power $k_j$ is known as the multiplicity of $p_j$.",Definition:Prime Decomposition/Multiplicity,,false,"Let n > 1 ∈ℤ.

Let:
:n = p_1^k_1 p_2^k_2⋯ p_r^k_r
be the prime decomposition of n, where:
:p_1 < p_2 < ⋯ < p_r are distinct primes
:k_1, k_2, …, k_r are (strictly) positive integers.


For each p_j ∈{ p_1, p_2, …, p_r }, its power k_j is known as the multiplicity of p_j.",Multiplicity
"['Definitions/Logical Negation', 'Definitions/Propositional Logic']",Definition:Negation,"The logical not or (logical) negation operator is a unary connective whose action is to reverse the truth value of the statement on which it operates.

:$neg p$ is defined as:
:$p$ is not true
:It is not the case that $p$ is true
:It is false that $p$
:$p$ is false.


Thus the statement $neg p$ is called the negation of $p$.


$neg p$ is voiced not $p$.


=== Truth Function ===
The logical not connective defines the truth function $f^neg$ as follows:

 
 
 
 

=== Truth Table ===
The characteristic truth table of the negation operator $neg p$ is as follows:

:$begin {array} {|c||c|} hline p & neg p \ hline mathrm F & mathrm T \ mathrm T & mathrm F \ hline end {array}$

=== Boolean Interpretation ===
Let $mathbf A$ be a propositional formula.

Let $neg$ denote the negation operator.


The truth value of $neg mathbf A$ under a boolean interpretation $v$ is given by:

:$v left(   right){neg mathbf A} = begin {cases} mathrm T & : v left(   right){mathbf A} = mathrm F \ mathrm F & : v left(   right){mathbf A} = mathrm T end {cases}$",Definition:Logical Not,,false,"The logical not or (logical) negation operator is a unary connective whose action is to reverse the truth value of the statement on which it operates.

:p is defined as:
:p is not true
:It is not the case that p is true
:It is false that p
:p is false.


Thus the statement p is called the negation of p.


p is voiced not p.


=== Truth Function ===
The logical not connective defines the truth function f^ as follows:

 
 
 
 

=== Truth Table ===
The characteristic truth table of the negation operator p is as follows:

:[ p p; F T; T F;   ]

=== Boolean Interpretation ===
Let 𝐀 be a propositional formula.

Let  denote the negation operator.


The truth value of 𝐀 under a boolean interpretation v is given by:

:v (   )𝐀 = T    : v (   )𝐀 = F
F    : v (   )𝐀 = T",Negation
"['Definitions/Numbers', 'Definitions/Negation Functions']",Definition:Negation,"The negation function is the function defined on the various standard number systems as follows:


=== Integer Negation Function ===
The negation function $h: mathbb Z to mathbb Z$ is defined on the set of integers as:
:$forall n in mathbb Z: h left(   right)n = -n$


Category:Definitions/Negation Functions
Category:Definitions/Integers

=== Rational Negation Function ===
The negation function $h: mathbb Q to mathbb Q$ is defined on the set of rational numbers as:
:$forall x in mathbb Q: h left(   right)x = -x$


Category:Definitions/Negation Functions
Category:Definitions/Rational Numbers

=== Real Negation Function ===
The negation function $h: mathbb R to mathbb R$ is defined on the set of real numbers as:
:$forall x in mathbb R: h left(   right)x = -x$


Category:Definitions/Negation Functions
Category:Definitions/Real Numbers

=== Complex Negation Function ===
The negation function $h: mathbb R to mathbb R$ is defined on the set of complex numbers as:
:$forall z = x + i y in mathbb C: h left(   right)z = -x - i y$


Category:Definitions/Negation Functions
Category:Definitions/Complex Numbers

Category:Definitions/Numbers
Category:Definitions/Negation Functions",Definition:Negation Function,,false,"The negation function is the function defined on the various standard number systems as follows:


=== Integer Negation Function ===
The negation function h: ℤ→ℤ is defined on the set of integers as:
:∀ n ∈ℤ: h (   )n = -n


Category:Definitions/Negation Functions
Category:Definitions/Integers

=== Rational Negation Function ===
The negation function h: ℚ→ℚ is defined on the set of rational numbers as:
:∀ x ∈ℚ: h (   )x = -x


Category:Definitions/Negation Functions
Category:Definitions/Rational Numbers

=== Real Negation Function ===
The negation function h: ℝ→ℝ is defined on the set of real numbers as:
:∀ x ∈ℝ: h (   )x = -x


Category:Definitions/Negation Functions
Category:Definitions/Real Numbers

=== Complex Negation Function ===
The negation function h: ℝ→ℝ is defined on the set of complex numbers as:
:∀ z = x + i y ∈ℂ: h (   )z = -x - i y


Category:Definitions/Negation Functions
Category:Definitions/Complex Numbers

Category:Definitions/Numbers
Category:Definitions/Negation Functions",Negation
['Definitions/Nets (Topology)'],Definition:Net,"Let $X$ be a nonempty set.

Let $left( Lambda, precsim right)$ be a preordered set.

Let $F: Lambda to X$ be a mapping.


Then $F$ is referred to as a net.",Definition:Net (Preordered Set),,false,"Let X be a nonempty set.

Let ( Λ, ≾) be a preordered set.

Let F: Λ→ X be a mapping.


Then F is referred to as a net.",Net
['Definitions/Metric Spaces'],Definition:Net,"Let $M = left( A, d right)$ be a metric space.

Let $epsilon in mathbb R_{>0}$ be a strictly positive real number.


An $epsilon$-net for $M$ is a subset $S subseteq A$ such that:
:$ds A subseteq bigcup_{x mathop in S} B_epsilon left(   right)x$
where $B_epsilon left(   right)x$ denotes the open $epsilon$-ball of $x$ in $M$.


That is, it is a subset of $A$ such that the set of all open $epsilon$-balls of elements of that set forms a cover for $A$.


=== Finite Net ===
Let $M$ be a metric space.

Let $epsilon in mathbb R_{>0}$ be a strictly positive real number.


A finite $epsilon$-net for $M$ is an $epsilon$-net for $M$ which is a finite set.",Definition:Net (Metric Space),,false,"Let M = ( A, d ) be a metric space.

Let ϵ∈ℝ_>0 be a strictly positive real number.


An ϵ-net for M is a subset S ⊆ A such that:
:A ⊆⋃_x ∈ S B_ϵ(   )x
where B_ϵ(   )x denotes the open ϵ-ball of x in M.


That is, it is a subset of A such that the set of all open ϵ-balls of elements of that set forms a cover for A.


=== Finite Net ===
Let M be a metric space.

Let ϵ∈ℝ_>0 be a strictly positive real number.


A finite ϵ-net for M is an ϵ-net for M which is a finite set.",Net
"['Definitions/Net (Accounting)', 'Definitions/Accounting']",Definition:Net,"Net means what is left after deductions.


=== Net Profit ===
Net profit is profit after subtracting the operating costs.

=== Net Weight ===
The net weight of an object is the weight remaining after subtracting the tare for the weight of whatever wrapper, container or shipping means in which the object is contained when its weight is measured.",Definition:Net (Accounting),,false,"Net means what is left after deductions.


=== Net Profit ===
Net profit is profit after subtracting the operating costs.

=== Net Weight ===
The net weight of an object is the weight remaining after subtracting the tare for the weight of whatever wrapper, container or shipping means in which the object is contained when its weight is measured.",Net
['Definitions/Electric Charge'],Definition:Neutral,A body which has no electric charge on it is described as (electrically) neutral.,Definition:Electric Charge/Neutral,,false,A body which has no electric charge on it is described as (electrically) neutral.,Neutral
"['Definitions/Neutral Equilibrium', 'Definitions/Equilibrium (Mechanics)']",Definition:Neutral,"Let $B$ be a particle, a system of particles, or a body which is in equilibrium.

Let $B$ be such that if a small displacement is applied, $B$ remains in its new position.


$B$ is then said to be in neutral equilibrium.",Definition:Equilibrium (Mechanics)/Neutral,,false,"Let B be a particle, a system of particles, or a body which is in equilibrium.

Let B be such that if a small displacement is applied, B remains in its new position.


B is then said to be in neutral equilibrium.",Neutral
"['Definitions/Identity Elements', 'Definitions/Identities']",Definition:Neutral,"Let $left( S, circ right)$ be an algebraic structure.

An element $e in S$ is called an identity (element)  if and only if  it is both a left identity and a right identity:

:$forall x in S: x circ e = x = e circ x$


In Identity is Unique it is established that an identity element, if it exists, is unique within $left( S, circ right)$.

Thus it is justified to refer to it as the identity (of a given algebraic structure).


This identity is often denoted $e_S$, or $e$ if it is clearly understood what structure is being discussed.",Definition:Identity (Abstract Algebra)/Two-Sided Identity,,false,"Let ( S, ∘) be an algebraic structure.

An element e ∈ S is called an identity (element)  if and only if  it is both a left identity and a right identity:

:∀ x ∈ S: x ∘ e = x = e ∘ x


In Identity is Unique it is established that an identity element, if it exists, is unique within ( S, ∘).

Thus it is justified to refer to it as the identity (of a given algebraic structure).


This identity is often denoted e_S, or e if it is clearly understood what structure is being discussed.",Neutral
"['Definitions/Tree Theory', 'Definitions/Graph Theory']",Definition:Node,"=== Vertex ===

For graphs which are not trees, the term vertex is generally used instead:

 

Let $G = left( V, E right)$ be a graph.

The vertices (singular: vertex) are the elements of $V$.

Informally, the vertices are the points that are connected by the edges.


In the above, the vertices are the points $A, B, C, D, E, F, G$ which are marked as dots.

=== Node ===
The vertices of a tree are called its nodes.",Definition:Node (Graph Theory),,false,"=== Vertex ===

For graphs which are not trees, the term vertex is generally used instead:

 

Let G = ( V, E ) be a graph.

The vertices (singular: vertex) are the elements of V.

Informally, the vertices are the points that are connected by the edges.


In the above, the vertices are the points A, B, C, D, E, F, G which are marked as dots.

=== Node ===
The vertices of a tree are called its nodes.",Node
"['Definitions/Acnodes', 'Definitions/Analytic Geometry']",Definition:Node,An acnode is a singular point of the locus of an equation describing a curve which is not actually on that curve.,Definition:Acnode,,false,An acnode is a singular point of the locus of an equation describing a curve which is not actually on that curve.,Node
"['Definitions/Crunodes', 'Definitions/Double Points', 'Definitions/Analytic Geometry']",Definition:Node,A crunode is a double point $P$ of the locus of an equation describing a curve which intersects itself in such a way that there are $2$ distinct tangents at $P$.,Definition:Crunode,,false,A crunode is a double point P of the locus of an equation describing a curve which intersects itself in such a way that there are 2 distinct tangents at P.,Node
"['Definitions/Cusps', 'Definitions/Double Points', 'Definitions/Analytic Geometry']",Definition:Node,"A cusp is a singular point on a curve at which there are two different tangents which coincide.

Thus a cusp is a special case of a double point in which the tangents are coincident.


=== Cusp of the First Kind ===
A cusp of the first kind is a cusp in which both parts of the curve lie on opposite sides of the coincident tangents defining that cusp.


=== Single Cusp of the First Kind ===
A single cusp of the first kind is a single cusp in which both parts of the curve lie on opposite sides of the coincident tangents defining that single cusp.
:

=== Double Cusp of the First Kind ===
A double cusp of the first kind is a double cusp in which both parts of the curve lie on opposite sides of the coincident tangents defining that double cusp.
:

=== Cusp of the Second Kind ===
A cusp of the second kind is a cusp in which both parts of the curve lie on the same side of the coincident tangents defining that cusp.


=== Single Cusp of the Second Kind ===
A single cusp of the second kind is a single cusp in which both parts of the curve lie on the same side of the coincident tangents defining that single cusp.
:

=== Double Cusp of the Second Kind ===
A double cusp of the second kind is a double cusp in which both parts of the curve lie on the same side of the coincident tangents defining that double cusp.
:",Definition:Cusp,,false,"A cusp is a singular point on a curve at which there are two different tangents which coincide.

Thus a cusp is a special case of a double point in which the tangents are coincident.


=== Cusp of the First Kind ===
A cusp of the first kind is a cusp in which both parts of the curve lie on opposite sides of the coincident tangents defining that cusp.


=== Single Cusp of the First Kind ===
A single cusp of the first kind is a single cusp in which both parts of the curve lie on opposite sides of the coincident tangents defining that single cusp.
:

=== Double Cusp of the First Kind ===
A double cusp of the first kind is a double cusp in which both parts of the curve lie on opposite sides of the coincident tangents defining that double cusp.
:

=== Cusp of the Second Kind ===
A cusp of the second kind is a cusp in which both parts of the curve lie on the same side of the coincident tangents defining that cusp.


=== Single Cusp of the Second Kind ===
A single cusp of the second kind is a single cusp in which both parts of the curve lie on the same side of the coincident tangents defining that single cusp.
:

=== Double Cusp of the Second Kind ===
A double cusp of the second kind is a double cusp in which both parts of the curve lie on the same side of the coincident tangents defining that double cusp.
:",Node
"['Definitions/Double Cusps', 'Definitions/Cusps']",Definition:Node,"A double cusp is a cusp in which $2$ or more parts of the curve are both continuous through the point.

That is, the curve is tangential to itself.


=== Double Cusp of the First Kind ===
A double cusp of the first kind is a double cusp in which both parts of the curve lie on opposite sides of the coincident tangents defining that double cusp.
:

=== Double Cusp of the Second Kind ===
A double cusp of the second kind is a double cusp in which both parts of the curve lie on the same side of the coincident tangents defining that double cusp.
:

=== Point of Osculoinflection ===
A point of osculoinflection is a double cusp in which one or both parts of the curve has a point of inflection at the point of contact with the tangents defining that double cusp.
:",Definition:Double Cusp,,false,"A double cusp is a cusp in which 2 or more parts of the curve are both continuous through the point.

That is, the curve is tangential to itself.


=== Double Cusp of the First Kind ===
A double cusp of the first kind is a double cusp in which both parts of the curve lie on opposite sides of the coincident tangents defining that double cusp.
:

=== Double Cusp of the Second Kind ===
A double cusp of the second kind is a double cusp in which both parts of the curve lie on the same side of the coincident tangents defining that double cusp.
:

=== Point of Osculoinflection ===
A point of osculoinflection is a double cusp in which one or both parts of the curve has a point of inflection at the point of contact with the tangents defining that double cusp.
:",Node
['Definitions/Category Theory'],Definition:Node,"A metagraph $mathcal G$ consists of:

* objects $X, Y, Z, ldots$
* morphisms $f, g, h, ldots$ between its objects

These are subjected to the following two axioms:

 
 
 
 

 
A metagraph is purely axiomatic, and does not use set theory.

For example, the objects are not ""elements of the set of objects"", because these axioms are (without further interpretation) unfounded in set theory.",Definition:Metagraph,,false,"A metagraph 𝒢 consists of:

* objects X, Y, Z, …
* morphisms f, g, h, … between its objects

These are subjected to the following two axioms:

 
 
 
 

 
A metagraph is purely axiomatic, and does not use set theory.

For example, the objects are not ""elements of the set of objects"", because these axioms are (without further interpretation) unfounded in set theory.",Node
"['Definitions/Knots of Splines', 'Definitions/Splines']",Definition:Node,"Let $left[ a ,.,.,   right]b$ be a closed real interval.

Let $T := leftlbrace a = t_0, t_1, t_2, ldots, t_{n - 1}, t_n = b rightrbrace$ form a subdivision of $left[ a ,.,.,   right]b$.

Let $S: left[ a ,.,.,   right]b to mathbb R$ be a spline function on $left[ a ,.,.,   right]b$ on $T$.


The points $T := leftlbrace t_0, t_1, t_2, ldots, t_{n - 1}, t_n rightrbrace$ of $S$ are known as the knots.


=== Knot Vector ===
Let $left[ a ,.,.,   right]b$ be a closed real interval.

Let $T := leftlbrace a = t_0, t_1, t_2, ldots, t_{n - 1}, t_n = b rightrbrace$ form a subdivision of $left[ a ,.,.,   right]b$.

Let $S: left[ a ,.,.,   right]b to mathbb R$ be a spline function on $left[ a ,.,.,   right]b$ on $T$.


The ordered $n + 1$-tuple $mathbf t := left( t_0, t_1, t_2, ldots, t_{n - 1}, t_n right)$ of $S$ is known as the knot vector.


Category:Definitions/Knots of Splines",Definition:Spline Function/Knot,,false,"Let [ a  . . ]b be a closed real interval.

Let T := { a = t_0, t_1, t_2, …, t_n - 1, t_n = b } form a subdivision of [ a  . . ]b.

Let S: [ a  . . ]b →ℝ be a spline function on [ a  . . ]b on T.


The points T := { t_0, t_1, t_2, …, t_n - 1, t_n } of S are known as the knots.


=== Knot Vector ===
Let [ a  . . ]b be a closed real interval.

Let T := { a = t_0, t_1, t_2, …, t_n - 1, t_n = b } form a subdivision of [ a  . . ]b.

Let S: [ a  . . ]b →ℝ be a spline function on [ a  . . ]b on T.


The ordered n + 1-tuple 𝐭 := ( t_0, t_1, t_2, …, t_n - 1, t_n ) of S is known as the knot vector.


Category:Definitions/Knots of Splines",Node
"['Definitions/Noetherian Modules', 'Definitions/Examples of Modules']",Definition:Noetherian,"Let $A$ be a commutative ring with unity.

Let $M$ be an $A$-module.


=== Definition 1 ===
Let $A$ be a commutative ring with unity.

Let $M$ be an $A$-module.


$M$ is a Noetherian module  if and only if  every submodule of $M$ is finitely generated.

=== Definition 2 ===
Let $A$ be a commutative ring with unity.

Let $M$ be an $A$-module.


$M$ is a Noetherian module  if and only if  it satisfies the ascending chain condition on submodules.

=== Definition 3 ===
Let $A$ be a commutative ring with unity.

Let $M$ be an $A$-module.


$M$ is a Noetherian module  if and only if  it satisfies the maximal condition on submodules.",Definition:Noetherian Module,,false,"Let A be a commutative ring with unity.

Let M be an A-module.


=== Definition 1 ===
Let A be a commutative ring with unity.

Let M be an A-module.


M is a Noetherian module  if and only if  every submodule of M is finitely generated.

=== Definition 2 ===
Let A be a commutative ring with unity.

Let M be an A-module.


M is a Noetherian module  if and only if  it satisfies the ascending chain condition on submodules.

=== Definition 3 ===
Let A be a commutative ring with unity.

Let M be an A-module.


M is a Noetherian module  if and only if  it satisfies the maximal condition on submodules.",Noetherian
"['Definitions/Noetherian Rings', 'Definitions/Ring Theory']",Definition:Noetherian,"=== Definition 1 ===
A commutative ring with unity $A$ is Noetherian  if and only if  every ideal of $A$ is finitely generated.

=== Definition 2 ===
A commutative ring with unity $A$ is Noetherian  if and only if  it satisfies the ascending chain condition on ideals.

=== Definition 3 ===
A commutative ring with unity $A$ is Noetherian  if and only if  it satisfies the maximal condition on ideals.

=== Definition 4 ===
A commutative ring with unity $A$ is Noetherian  if and only if  it is Noetherian as an $A$-module.",Definition:Noetherian Ring,,false,"=== Definition 1 ===
A commutative ring with unity A is Noetherian  if and only if  every ideal of A is finitely generated.

=== Definition 2 ===
A commutative ring with unity A is Noetherian  if and only if  it satisfies the ascending chain condition on ideals.

=== Definition 3 ===
A commutative ring with unity A is Noetherian  if and only if  it satisfies the maximal condition on ideals.

=== Definition 4 ===
A commutative ring with unity A is Noetherian  if and only if  it is Noetherian as an A-module.",Noetherian
"['Definitions/Noetherian Topological Spaces', 'Definitions/Topology', 'Definitions/Algebraic Geometry']",Definition:Noetherian,"=== Definition 1 ===
A topological space $T = left( S, tau right)$ is Noetherian  if and only if  its set of closed sets, ordered by the subset relation, satisfies the descending chain condition.

=== Definition 2 ===
A topological space $T = left( S, tau right)$ is Noetherian  if and only if  its set of open sets, ordered by the subset relation, satisfies the ascending chain condition.

=== Definition 3 ===
A topological space $T = left( S, tau right)$ is Noetherian  if and only if  each non-empty set of closed sets has a minimal element with respect to the subset relation.

=== Definition 4 ===
A topological space $T = left( S, tau right)$ is Noetherian  if and only if  each non-empty set of open sets has a maximal element   the subset relation.",Definition:Noetherian Topological Space,,false,"=== Definition 1 ===
A topological space T = ( S, τ) is Noetherian  if and only if  its set of closed sets, ordered by the subset relation, satisfies the descending chain condition.

=== Definition 2 ===
A topological space T = ( S, τ) is Noetherian  if and only if  its set of open sets, ordered by the subset relation, satisfies the ascending chain condition.

=== Definition 3 ===
A topological space T = ( S, τ) is Noetherian  if and only if  each non-empty set of closed sets has a minimal element with respect to the subset relation.

=== Definition 4 ===
A topological space T = ( S, τ) is Noetherian  if and only if  each non-empty set of open sets has a maximal element   the subset relation.",Noetherian
"['Definitions/Algebraic Geometry', 'Definitions/Schemes']",Definition:Noetherian,"Let $left( X, mathcal O_X right)$ be a scheme.


Then $left( X, mathcal O_X right)$ is Noetherian  if and only if  $left( X, mathcal O_X right)$ is locally Noetherian and quasi-compact.

 ",Definition:Noetherian Scheme,,false,"Let ( X, 𝒪_X ) be a scheme.


Then ( X, 𝒪_X ) is Noetherian  if and only if  ( X, 𝒪_X ) is locally Noetherian and quasi-compact.

 ",Noetherian
['Definitions/Metric Spaces'],Definition:Normal,"Let $X = left( M_1, d_1 right)$ and $Y = left( M_2, d_2 right)$ be complete metric spaces.

Let $mathcal F = leftlangle f_i rightrangle_{i mathop in I}$ be a family of continuous mappings $f_i: X to Y$.


Then $mathcal F$ is a normal family  if and only if :
:every sequence of mappings in $mathcal F$ contains a subsequence which converges uniformly on compact subsets of $X$ to a continuous function $f: X to Y$.",Definition:Normal Family,,false,"Let X = ( M_1, d_1 ) and Y = ( M_2, d_2 ) be complete metric spaces.

Let ℱ = ⟨ f_i ⟩_i ∈ I be a family of continuous mappings f_i: X → Y.


Then ℱ is a normal family  if and only if :
:every sequence of mappings in ℱ contains a subsequence which converges uniformly on compact subsets of X to a continuous function f: X → Y.",Normal
"['Definitions/Adjoints', 'Definitions/Linear Operators', 'Definitions/Linear Transformations on Hilbert Spaces']",Definition:Normal,"Let $mathcal H$ be a Hilbert space.

Let $mathbf T: mathcal H to mathcal H$ be a bounded linear operator.


Then $mathbf T$ is said to be normal  if and only if :

:$mathbf T^* mathbf T = mathbf T mathbf T^*$

where $mathbf T^*$ denotes the adjoint of $mathbf T$.",Definition:Normal Operator,,false,"Let ℋ be a Hilbert space.

Let 𝐓: ℋ→ℋ be a bounded linear operator.


Then 𝐓 is said to be normal  if and only if :

:𝐓^* 𝐓 = 𝐓𝐓^*

where 𝐓^* denotes the adjoint of 𝐓.",Normal
['Definitions/Analytic Geometry'],Definition:Normal,"Let $C$ be a curve embedded in the plane.

The normal to $C$ at a point $P$ is defined as the straight line which lies perpendicular to the tangent at $P$ and in the same plane as $P$.",Definition:Normal to Curve,,false,"Let C be a curve embedded in the plane.

The normal to C at a point P is defined as the straight line which lies perpendicular to the tangent at P and in the same plane as P.",Normal
"['Definitions/Normal Vectors', 'Definitions/Vectors']",Definition:Normal,"Let $S$ be a surface in ordinary $3$-space.

Let $P$ be a point of $S$.


Let $mathbf n$ be a vector whose initial point is at $P$ such that $mathbf n$ is perpendicular to $S$ at $P$.


Then $mathbf n$ is a normal vector to $S$ at $P$.",Definition:Normal Vector,,false,"Let S be a surface in ordinary 3-space.

Let P be a point of S.


Let 𝐧 be a vector whose initial point is at P such that 𝐧 is perpendicular to S at P.


Then 𝐧 is a normal vector to S at P.",Normal
['Definitions/Topology'],Definition:Normal,"Let $M$ be a differentiable manifold.

Let $p in M$ be a point in $M$.

Let $N_p M$ be the normal space at $p$.


The normal bundle of $M$ is the disjoint union of all the normal spaces of $M$:

:$ds N M = coprod_{p mathop in M} N_p M$",Definition:Normal Bundle,,false,"Let M be a differentiable manifold.

Let p ∈ M be a point in M.

Let N_p M be the normal space at p.


The normal bundle of M is the disjoint union of all the normal spaces of M:

:N M = ∐_p ∈ M N_p M",Normal
"['Definitions/Normal Subgroups', 'Definitions/Normal Subsets', 'Definitions/Normality in Groups', 'Definitions/Subgroups']",Definition:Normal,"Let $G$ be a group.

Let $N$ be a subgroup of $G$.


$N$ is a normal subgroup of $G$  if and only if :


=== Definition 1 ===
Let $G$ be a group.

Let $N$ be a subgroup of $G$.


$N$ is a normal subgroup of $G$  if and only if :

:$forall g in G: g circ N = N circ g$

where $g circ N$ denotes the subset product of $g$ with $N$.

=== Definition 2 ===
Let $G$ be a group.

Let $N$ be a subgroup of $G$.


$N$ is a normal subgroup of $G$  if and only if :

: Every right coset of $N$ in $G$ is a left coset
that is:
: The right coset space of $N$ in $G$ equals its left coset space.

=== Definition 3 ===
Let $G$ be a group.

Let $N$ be a subgroup of $G$.


$N$ is a normal subgroup of $G$  if and only if :

 
 
 
 

where $g circ N$ etc. denotes the subset product of $g$ with $N$.

=== Definition 4 ===
Let $G$ be a group.

Let $N$ be a subgroup of $G$.


$N$ is a normal subgroup of $G$  if and only if :

 
 
 
 

where $g circ N$ etc. denotes the subset product of $g$ with $N$.

=== Definition 5 ===
Let $G$ be a group.

Let $N$ be a subgroup of $G$.


$N$ is a normal subgroup of $G$  if and only if :

 
 
 
 

where $g circ N$ etc. denotes the subset product of $g$ with $N$.


That is:
:$forall g in G: kappa_g left(   right)N = N$

where $kappa_g left(   right)N$ denotes the inner automorphism of $N$ by $g$.

=== Definition 6 ===
Let $G$ be a group.

Let $N$ be a subgroup of $G$.


$N$ is a normal subgroup of $G$  if and only if :

 
 
 
 

=== Definition 7 ===
Let $G$ be a group.

Let $N$ be a subgroup of $G$.

$N$ is a normal subgroup of $G$  if and only if :

:$N$ is a normal subset of $G$.",Definition:Normal Subgroup,,false,"Let G be a group.

Let N be a subgroup of G.


N is a normal subgroup of G  if and only if :


=== Definition 1 ===
Let G be a group.

Let N be a subgroup of G.


N is a normal subgroup of G  if and only if :

:∀ g ∈ G: g ∘ N = N ∘ g

where g ∘ N denotes the subset product of g with N.

=== Definition 2 ===
Let G be a group.

Let N be a subgroup of G.


N is a normal subgroup of G  if and only if :

: Every right coset of N in G is a left coset
that is:
: The right coset space of N in G equals its left coset space.

=== Definition 3 ===
Let G be a group.

Let N be a subgroup of G.


N is a normal subgroup of G  if and only if :

 
 
 
 

where g ∘ N etc. denotes the subset product of g with N.

=== Definition 4 ===
Let G be a group.

Let N be a subgroup of G.


N is a normal subgroup of G  if and only if :

 
 
 
 

where g ∘ N etc. denotes the subset product of g with N.

=== Definition 5 ===
Let G be a group.

Let N be a subgroup of G.


N is a normal subgroup of G  if and only if :

 
 
 
 

where g ∘ N etc. denotes the subset product of g with N.


That is:
:∀ g ∈ G: κ_g (   )N = N

where κ_g (   )N denotes the inner automorphism of N by g.

=== Definition 6 ===
Let G be a group.

Let N be a subgroup of G.


N is a normal subgroup of G  if and only if :

 
 
 
 

=== Definition 7 ===
Let G be a group.

Let N be a subgroup of G.

N is a normal subgroup of G  if and only if :

:N is a normal subset of G.",Normal
"['Definitions/Normal Subsets', 'Definitions/Normality in Groups', 'Definitions/Subsets']",Definition:Normal,"Let $left( G, circ right)$ be a group.

Let $S subseteq G$ be a general subset of $G$.


Then $S$ is a normal subset of $G$  if and only if :


=== Definition 1===
Let $left( G, circ right)$ be a group.

Let $S subseteq G$ be a general subset of $G$.


Then $S$ is a normal subset of $G$  if and only if :

:$forall g in G: g circ S = S circ g$

=== Definition 2===
Let $left( G, circ right)$ be a group.

Let $S subseteq G$ be a general subset of $G$.


Then $S$ is a normal subset of $G$  if and only if :

:$forall g in G: g circ S circ g^{-1} = S$
or, equivalently:
:$forall g in G: g^{-1} circ S circ g = S$

=== Definition 3===
Let $left( G, circ right)$ be a group.

Let $S subseteq G$ be a general subset of $G$.


Then $S$ is a normal subset of $G$  if and only if :

:$forall g in G: g circ S circ g^{-1} subseteq S$
or, equivalently:
:$forall g in G: g^{-1} circ S circ g subseteq S$

=== Definition 4===
Let $left( G, circ right)$ be a group.

Let $S subseteq G$ be a general subset of $G$.


Then $S$ is a normal subset of $G$  if and only if :

: $forall g in G: S subseteq g circ S circ g^{-1}$
or, equivalently:
: $forall g in G: S subseteq g^{-1} circ S circ g$

=== Definition 5 ===
Let $left( G, circ right)$ be a group.

Let $S subseteq G$ be a general subset of $G$.


Then $S$ is a normal subset of $G$  if and only if :

:$forall x, y in G: x circ y in S implies y circ x in S$

=== Definition 6 ===
Let $left( G, circ right)$ be a group.

Let $S subseteq G$ be a general subset of $G$.


Then $S$ is a normal subset of $G$  if and only if :

:$N_G left(   right)S = G$
where $N_G left(   right)S$ denotes the normalizer of $S$ in $G$.

=== Definition 7 ===
Let $left( G, circ right)$ be a group.

Let $S subseteq G$ be a general subset of $G$.


Then $S$ is a normal subset of $G$  if and only if :

:$forall g in G: g circ S subseteq S circ g$
or:
:$forall g in G: S circ g subseteq g circ S$",Definition:Normal Subset,,false,"Let ( G, ∘) be a group.

Let S ⊆ G be a general subset of G.


Then S is a normal subset of G  if and only if :


=== Definition 1===
Let ( G, ∘) be a group.

Let S ⊆ G be a general subset of G.


Then S is a normal subset of G  if and only if :

:∀ g ∈ G: g ∘ S = S ∘ g

=== Definition 2===
Let ( G, ∘) be a group.

Let S ⊆ G be a general subset of G.


Then S is a normal subset of G  if and only if :

:∀ g ∈ G: g ∘ S ∘ g^-1 = S
or, equivalently:
:∀ g ∈ G: g^-1∘ S ∘ g = S

=== Definition 3===
Let ( G, ∘) be a group.

Let S ⊆ G be a general subset of G.


Then S is a normal subset of G  if and only if :

:∀ g ∈ G: g ∘ S ∘ g^-1⊆ S
or, equivalently:
:∀ g ∈ G: g^-1∘ S ∘ g ⊆ S

=== Definition 4===
Let ( G, ∘) be a group.

Let S ⊆ G be a general subset of G.


Then S is a normal subset of G  if and only if :

: ∀ g ∈ G: S ⊆ g ∘ S ∘ g^-1
or, equivalently:
: ∀ g ∈ G: S ⊆ g^-1∘ S ∘ g

=== Definition 5 ===
Let ( G, ∘) be a group.

Let S ⊆ G be a general subset of G.


Then S is a normal subset of G  if and only if :

:∀ x, y ∈ G: x ∘ y ∈ S  y ∘ x ∈ S

=== Definition 6 ===
Let ( G, ∘) be a group.

Let S ⊆ G be a general subset of G.


Then S is a normal subset of G  if and only if :

:N_G (   )S = G
where N_G (   )S denotes the normalizer of S in G.

=== Definition 7 ===
Let ( G, ∘) be a group.

Let S ⊆ G be a general subset of G.


Then S is a normal subset of G  if and only if :

:∀ g ∈ G: g ∘ S ⊆ S ∘ g
or:
:∀ g ∈ G: S ∘ g ⊆ g ∘ S",Normal
"['Definitions/Normal Series', 'Definitions/Normality in Groups']",Definition:Normal,"Let $G$ be a group whose identity is $e$.


A normal series for $G$ is a sequence of (normal) subgroups of $G$:
:$leftlbrace e rightrbrace = G_0 lhd G_1 lhd cdots lhd G_n = G$

where $G_{i - 1} lhd G_i$ denotes that $G_{i - 1}$ is a proper normal subgroup of $G_i$.


=== Factors ===
Let $G$ be a group whose identity is $e$.

Let $leftlangle G_i rightrangle_{i mathop in left[ 0 ,.,.,   right]n}$ be a normal series for $G$:
:$leftlangle G_i rightrangle_{i mathop in left[ 0 ,.,.,   right]n} = left( leftlbrace e rightrbrace = G_0 lhd G_1 lhd cdots lhd G_{n - 1} lhd G_n = G right)$


The factor groups of $leftlangle G_i rightrangle_{i mathop in left[ 0 ,.,.,   right]n}$:
:$leftlbrace e rightrbrace = G_0 lhd G_1 lhd cdots lhd G_n = G$

are the quotient groups:
:$G_1 / G_0, G_2 / G_1, ldots, G_i / G_{i - 1}, ldots, G_n / G_{n-1}$

=== Normal Series as Sequence of Homomorphisms ===
Let $G$ be a group whose identity is $e$.

Let $leftlangle G_i rightrangle_{i mathop in left[ 0 ,.,.,   right]n}$ be a normal series for $G$:
:$leftlangle G_i rightrangle_{i mathop in left[ 0 ,.,.,   right]n} = left( leftlbrace e rightrbrace = G_0 lhd G_1 lhd cdots lhd G_{n - 1} lhd G_n = G right)$
whose factor groups are:
:$H_1 = G_1 / G_0, H_2 = G_2 / G_1, ldots, H_i = G_i / G_{i - 1}, ldots, H_n = G_n / G_{n - 1}$


By Kernel of Group Homomorphism Corresponds with Normal Subgroup of Domain, such a series can also be expressed as a sequence $phi_1, ldots, phi_n$ of group homomorphisms:

:$leftlbrace e rightrbrace stackrel {phi_1} {to} H_1 stackrel {phi_2} {to} H_2 stackrel {phi_3} {to} cdots stackrel {phi_n} {to} H_n$

=== Infinite Normal Series ===
A normal series may or may not terminate at either end:
:$cdots stackrel {phi_{i - 1} } {longrightarrow} H_{i - 1} stackrel {phi_i} {longrightarrow} H_i stackrel {phi_{i + 1} } {longrightarrow} H_{i + 1} stackrel {phi_{i + 2} } {longrightarrow} cdots$

Such a series is referred to as an infinite normal series.

The context will determine which end, if either, it terminates.

=== Length ===
Let $G$ be a group whose identity is $e$.

Let $leftlangle G_i rightrangle_{i mathop in left[ 0 ,.,.,   right]n}$ be a normal series for $G$:
:$leftlangle G_i rightrangle_{i mathop in left[ 0 ,.,.,   right]n} = left( leftlbrace e rightrbrace = G_0 lhd G_1 lhd cdots lhd G_{n-1} lhd G_n = G right)$


The length of $leftlangle G_i rightrangle_{i mathop in left[ 0 ,.,.,   right]n}$ is the number of (normal) subgroups which make it.

In this context, the length of $leftlangle G_i rightrangle_{i mathop in left[ 0 ,.,.,   right]n}$ is $n$.


If such a normal series is infinite, then its length is not defined.


Category:Definitions/Normal Series",Definition:Normal Series,,false,"Let G be a group whose identity is e.


A normal series for G is a sequence of (normal) subgroups of G:
:{ e } = G_0  G_1 ⋯ G_n = G

where G_i - 1 G_i denotes that G_i - 1 is a proper normal subgroup of G_i.


=== Factors ===
Let G be a group whose identity is e.

Let ⟨ G_i ⟩_i ∈[ 0  . . ]n be a normal series for G:
:⟨ G_i ⟩_i ∈[ 0  . . ]n = ( { e } = G_0  G_1 ⋯ G_n - 1 G_n = G )


The factor groups of ⟨ G_i ⟩_i ∈[ 0  . . ]n:
:{ e } = G_0  G_1 ⋯ G_n = G

are the quotient groups:
:G_1 / G_0, G_2 / G_1, …, G_i / G_i - 1, …, G_n / G_n-1

=== Normal Series as Sequence of Homomorphisms ===
Let G be a group whose identity is e.

Let ⟨ G_i ⟩_i ∈[ 0  . . ]n be a normal series for G:
:⟨ G_i ⟩_i ∈[ 0  . . ]n = ( { e } = G_0  G_1 ⋯ G_n - 1 G_n = G )
whose factor groups are:
:H_1 = G_1 / G_0, H_2 = G_2 / G_1, …, H_i = G_i / G_i - 1, …, H_n = G_n / G_n - 1


By Kernel of Group Homomorphism Corresponds with Normal Subgroup of Domain, such a series can also be expressed as a sequence ϕ_1, …, ϕ_n of group homomorphisms:

:{ e }ϕ_1→ H_1 ϕ_2→ H_2 ϕ_3→⋯ϕ_n→ H_n

=== Infinite Normal Series ===
A normal series may or may not terminate at either end:
:⋯ϕ_i - 1⟶ H_i - 1ϕ_i⟶ H_i ϕ_i + 1⟶ H_i + 1ϕ_i + 2⟶⋯

Such a series is referred to as an infinite normal series.

The context will determine which end, if either, it terminates.

=== Length ===
Let G be a group whose identity is e.

Let ⟨ G_i ⟩_i ∈[ 0  . . ]n be a normal series for G:
:⟨ G_i ⟩_i ∈[ 0  . . ]n = ( { e } = G_0  G_1 ⋯ G_n-1 G_n = G )


The length of ⟨ G_i ⟩_i ∈[ 0  . . ]n is the number of (normal) subgroups which make it.

In this context, the length of ⟨ G_i ⟩_i ∈[ 0  . . ]n is n.


If such a normal series is infinite, then its length is not defined.


Category:Definitions/Normal Series",Normal
"['Definitions/Generated Normal Subgroups', 'Definitions/Normal Subgroups', 'Definitions/Normality in Groups', 'Definitions/Generated Subgroups', 'Definitions/Generators of Groups', 'Definitions/Subgroups']",Definition:Normal,"Let $G$ be a group.

Let $S subseteq G$ be a subset.


=== Definition 1 ===
Let $G$ be a group. 

Let $S subseteq G$ be a subset.


The normal subgroup generated by $S$, denoted ${leftlangle S^G rightrangle}$,  is the intersection of all normal subgroups of $G$ containing $S$.

=== Definition 2 ===
 

Let $G$ be a group. 

Let $S subseteq G$ be a subset.


The normal subgroup generated by $S$, denoted ${leftlangle S^G rightrangle}$,  is the smallest normal subgroup of $G$ containing $S$:
:${leftlangle S^G rightrangle} = {leftlangle x S x^{-1}: x in G rightrangle}$",Definition:Generated Normal Subgroup,,false,"Let G be a group.

Let S ⊆ G be a subset.


=== Definition 1 ===
Let G be a group. 

Let S ⊆ G be a subset.


The normal subgroup generated by S, denoted ⟨ S^G ⟩,  is the intersection of all normal subgroups of G containing S.

=== Definition 2 ===
 

Let G be a group. 

Let S ⊆ G be a subset.


The normal subgroup generated by S, denoted ⟨ S^G ⟩,  is the smallest normal subgroup of G containing S:
:⟨ S^G ⟩ = ⟨ x S x^-1: x ∈ G ⟩",Normal
['Definitions/Field Extensions'],Definition:Normal,"=== Definition 1 ===
Let $L / K$ be a field extension.

Then $L / K$ is a normal extension  if and only if :
:for every irreducible polynomial $f in K left[ x right]$ with at least one root in $L$, $f$ splits completely in $L$.

=== Definition 2 ===
Let $L / K$ be a field extension.

Let $overline K$ be the algebraic closure of $K$.

Let $mathrm {Gal} left( L / K right)$ denote the set of embeddings of $L$ in $overline K$ which fix $K$ pointwise.


Then $L / K$ is a normal extension  if and only if :
: $sigma left(   right)L = L$
for each $sigma in mathrm {Gal} left( L / K right)$.",Definition:Normal Extension,,false,"=== Definition 1 ===
Let L / K be a field extension.

Then L / K is a normal extension  if and only if :
:for every irreducible polynomial f ∈ K [ x ] with at least one root in L, f splits completely in L.

=== Definition 2 ===
Let L / K be a field extension.

Let K be the algebraic closure of K.

Let Gal( L / K ) denote the set of embeddings of L in K which fix K pointwise.


Then L / K is a normal extension  if and only if :
: σ(   )L = L
for each σ∈Gal( L / K ).",Normal
['Definitions/Ordinal Arithmetic'],Definition:Normal,"Let $x$ be an ordinal.


The Cantor normal form of $x$ is an ordinal summation:

:$x = omega^{a_1} n_1 + dots + omega^{a_k} n_k$

where:

:$k in mathbb N$ is a natural number
:$omega$ is the minimally inductive set
:$leftlangle a_i rightrangle$ is a strictly decreasing finite sequence of ordinals.
:$leftlangle n_i rightrangle$ is a finite sequence of finite ordinals


In summation notation:

:$x = ds sum_{i mathop = 1}^k omega^{a_i} n_i$

 ",Definition:Cantor Normal Form,,false,"Let x be an ordinal.


The Cantor normal form of x is an ordinal summation:

:x = ω^a_1 n_1 + … + ω^a_k n_k

where:

:k ∈ℕ is a natural number
:ω is the minimally inductive set
:⟨ a_i ⟩ is a strictly decreasing finite sequence of ordinals.
:⟨ n_i ⟩ is a finite sequence of finite ordinals


In summation notation:

:x = ∑_i  = 1^k ω^a_i n_i

 ",Normal
"['Definitions/Normal Numbers', 'Definitions/Numbers']",Definition:Normal,"A real number $r$ is normal with respect to a number base $b$  if and only if  its basis expansion in number base $b$ is such that:

:no finite sequence of digits of $r$ of length $n$ occurs more frequently than any other such finite sequence of length $n$.


In particular, for number base $b$, all digits of $r$ have the same natural density in the basis expansion of $r$.",Definition:Normal Number,,false,"A real number r is normal with respect to a number base b  if and only if  its basis expansion in number base b is such that:

:no finite sequence of digits of r of length n occurs more frequently than any other such finite sequence of length n.


In particular, for number base b, all digits of r have the same natural density in the basis expansion of r.",Normal
"['Definitions/Gaussian Distribution', 'Definitions/Examples of Probability Distributions']",Definition:Normal,"Let $X$ be a continuous random variable on a probability space $left( Omega, unicode{x3a3}, Pr right)$.


Then $X$ has a Gaussian distribution  if and only if  the probability density function of $X$ is:

:$f_X left(   right)x = dfrac 1 {sigma sqrt {2 pi} } exp left(   right){-dfrac {left( x - mu right)^2} {2 sigma^2} }$

for $mu in mathbb R, sigma in mathbb R_{> 0}$.


This is written: 

:$X sim N left( mu,   right){sigma^2}$",Definition:Gaussian Distribution,,false,"Let X be a continuous random variable on a probability space ( Ω, x3a3, ).


Then X has a Gaussian distribution  if and only if  the probability density function of X is:

:f_X (   )x =  1 σ√(2 π)exp(   )-( x - μ)^22 σ^2

for μ∈ℝ, σ∈ℝ_> 0.


This is written: 

:X ∼ N ( μ,   )σ^2",Normal
"['Definitions/Conjunctive Normal Form', 'Definitions/Conjunction', 'Definitions/Propositional Logic']",Definition:Normal,"A propositional formula $P$ is in conjunctive normal form  if and only if  it consists of a conjunction of:
:$(1): quad$ disjunctions of literals
and/or:
:$(2): quad$ literals.",Definition:Conjunctive Normal Form,,false,"A propositional formula P is in conjunctive normal form  if and only if  it consists of a conjunction of:
:(1): disjunctions of literals
and/or:
:(2): literals.",Normal
"['Definitions/Disjunctive Normal Form', 'Definitions/Propositional Logic']",Definition:Normal,"A propositional formula $P$ is in disjunctive normal form  if and only if  it consists of a disjunction of:
:$(1): quad$ conjunctions of literals
and/or:
:$(2): quad$ literals.",Definition:Disjunctive Normal Form,,false,"A propositional formula P is in disjunctive normal form  if and only if  it consists of a disjunction of:
:(1): conjunctions of literals
and/or:
:(2): literals.",Normal
['Definitions/Propositional Logic'],Definition:Normal,"A propositional formula $P$ is in negation normal form (NNF)  if and only if :
* The only logical connectives connecting substatements of $P$ are Not, And and Or, that is, elements of the set $left{{neg, land, lor}right}$;
* The Not sign $neg$ appears only in front of atomic statements.

That is $P$ is in negation normal form iff it consists of literals, conjunctions and disjunctions.",Definition:Negation Normal Form,,false,"A propositional formula P is in negation normal form (NNF)  if and only if :
* The only logical connectives connecting substatements of P are Not, And and Or, that is, elements of the set {, , };
* The Not sign  appears only in front of atomic statements.

That is P is in negation normal form iff it consists of literals, conjunctions and disjunctions.",Normal
"['Definitions/Normal Spaces', 'Definitions/T4 Spaces', 'Definitions/T1 Spaces', 'Definitions/Separation Axioms']",Definition:Normal,"Let $T = left( S, tau right)$ be a topological space.


$left( S, tau right)$ is a normal space  if and only if :
:$left( S, tau right)$ is a $T_4$ space
:$left( S, tau right)$ is a $T_1$ (Fréchet) space.


That is:
:$forall A, B in complement left(   right)tau, A cap B = varnothing: exists U, V in tau: A subseteq U, B subseteq V, U cap V = varnothing$ 

:$forall x, y in S$, both:
::$exists U in tau: x in U, y notin U$
::$exists V in tau: y in V, x notin V$

 

This space is also referred to as normal Hausdorff.",Definition:Normal Space,,false,"Let T = ( S, τ) be a topological space.


( S, τ) is a normal space  if and only if :
:( S, τ) is a T_4 space
:( S, τ) is a T_1 (Fréchet) space.


That is:
:∀ A, B ∈∁(   )τ, A ∩ B = ∅: ∃ U, V ∈τ: A ⊆ U, B ⊆ V, U ∩ V = ∅ 

:∀ x, y ∈ S, both:
::∃ U ∈τ: x ∈ U, y ∉ U
::∃ V ∈τ: y ∈ V, x ∉ V

 

This space is also referred to as normal Hausdorff.",Normal
"['Definitions/Completely Normal Spaces', 'Definitions/Normal Spaces', 'Definitions/T5 Spaces', 'Definitions/T1 Spaces', 'Definitions/Separation Axioms']",Definition:Normal,"Let $T = left( S, tau right)$ be a topological space.


$left( S, tau right)$ is a completely normal space  if and only if :
:$left( S, tau right)$ is a $T_5$ space
:$left( S, tau right)$ is a $T_1$ (Fréchet) space.


That is:

:$forall A, B subseteq S, A^- cap B = A cap B^- = varnothing: exists U, V in tau: A subseteq U, B subseteq V, U cap V = varnothing$ 

:$forall x, y in S$, both:
::$exists U in tau: x in U, y notin U$
::$exists V in tau: y in V, x notin V$

 ",Definition:Completely Normal Space,,false,"Let T = ( S, τ) be a topological space.


( S, τ) is a completely normal space  if and only if :
:( S, τ) is a T_5 space
:( S, τ) is a T_1 (Fréchet) space.


That is:

:∀ A, B ⊆ S, A^- ∩ B = A ∩ B^- = ∅: ∃ U, V ∈τ: A ⊆ U, B ⊆ V, U ∩ V = ∅ 

:∀ x, y ∈ S, both:
::∃ U ∈τ: x ∈ U, y ∉ U
::∃ V ∈τ: y ∈ V, x ∉ V

 ",Normal
"['Definitions/Fully Normal Spaces', 'Definitions/Fully T4 Spaces', 'Definitions/T1 Spaces', 'Definitions/Compact Spaces', 'Definitions/Separation Axioms']",Definition:Normal,"Let $T = left( S, tau right)$ be a topological space.


$T$ is fully normal  if and only if :
:Every open cover of $S$ has a star refinement
:All points of $T$ are closed.


That is, $T$ is fully normal  if and only if :
:$T$ is fully $T_4$
:$T$ is a $T_1$ (Fréchet) space.

 ",Definition:Fully Normal Space,,false,"Let T = ( S, τ) be a topological space.


T is fully normal  if and only if :
:Every open cover of S has a star refinement
:All points of T are closed.


That is, T is fully normal  if and only if :
:T is fully T_4
:T is a T_1 (Fréchet) space.

 ",Normal
"['Definitions/Perfectly Normal Spaces', 'Definitions/Normal Spaces', 'Definitions/Perfectly T4 Spaces', 'Definitions/T1 Spaces', 'Definitions/Separation Axioms']",Definition:Normal,"Let $T = left( S, tau right)$ be a topological space.


$left( S, tau right)$ is a perfectly normal space  if and only if :
:$left( S, tau right)$ is a perfectly $T_4$ space
:$left( S, tau right)$ is a $T_1$ (Fréchet) space.


That is:

:Every closed set in $T$ is a $G_delta$ set.

:$forall x, y in S$, both:
::$exists U in tau: x in U, y notin U$
::$exists V in tau: y in V, x notin V$",Definition:Perfectly Normal Space,,false,"Let T = ( S, τ) be a topological space.


( S, τ) is a perfectly normal space  if and only if :
:( S, τ) is a perfectly T_4 space
:( S, τ) is a T_1 (Fréchet) space.


That is:

:Every closed set in T is a G_δ set.

:∀ x, y ∈ S, both:
::∃ U ∈τ: x ∈ U, y ∉ U
::∃ V ∈τ: y ∈ V, x ∉ V",Normal
['Definitions/Set Theory'],Definition:Normal,"Let $S$ be a set.

Then $S$ is an ordinary set  if and only if :
:$S notin S$

That is,  if and only if  $S$ is not an element of itself.",Definition:Ordinary Set,,false,"Let S be a set.

Then S is an ordinary set  if and only if :
:S ∉ S

That is,  if and only if  S is not an element of itself.",Normal
['Definitions/Relations'],Definition:Normal,"Let $A$ be a class.

Let $mathcal R$ be a relation on $A$.


An element $x$ of $A$ is left normal with respect to $mathcal R$  if and only if :
:$forall y in A: mathcal R left(   right){x, y}$ holds.",Definition:Left Normal Element of Relation,,false,"Let A be a class.

Let ℛ be a relation on A.


An element x of A is left normal with respect to ℛ  if and only if :
:∀ y ∈ A: ℛ(   )x, y holds.",Normal
['Definitions/Relations'],Definition:Normal,"Let $A$ be a class.

Let $mathcal R$ be a relation on $A$.


An element $x$ of $A$ is right normal with respect to $mathcal R$  if and only if :
:$forall y in A: mathcal R left(   right){y, x}$ holds.",Definition:Right Normal Element of Relation,,false,"Let A be a class.

Let ℛ be a relation on A.


An element x of A is right normal with respect to ℛ  if and only if :
:∀ y ∈ A: ℛ(   )y, x holds.",Normal
['Definitions/Normality in Groups'],Definition:Normal,"Let $G$ be a group.

Let $S$ be a subset of $G$.


Then the normalizer of $S$ in $G$ is the set $N_G left(   right)S$ defined as:
:$N_G left(   right)S := leftlbrace a in G: S^a = S rightrbrace$

where $S^a$ is the $G$-conjugate of $S$ by $a$.


If $S$ is a singleton such that $S = leftlbrace s rightrbrace$, we may also write $N_G left(   right)s$ for $N_G left(   right)S = N_G left(   right){leftlbrace s rightrbrace}$, as long as there is no possibility of confusion.",Definition:Normalizer,,false,"Let G be a group.

Let S be a subset of G.


Then the normalizer of S in G is the set N_G (   )S defined as:
:N_G (   )S := { a ∈ G: S^a = S }

where S^a is the G-conjugate of S by a.


If S is a singleton such that S = { s }, we may also write N_G (   )s for N_G (   )S = N_G (   ){ s }, as long as there is no possibility of confusion.",Normal
"['Definitions/Null Sets', 'Definitions/Measure Theory', 'Definitions/Topology']",Definition:Null,"Let $left( X, unicode{x3a3}, mu right)$ be a measure space.

A set $N in unicode{x3a3}$ is called a ($mu$-)null set  if and only if  $mu left(   right)N = 0$.


 

=== Family of Null Sets ===

The family of $mu$-null sets, $leftlbrace N in unicode{x3a3}: mu left(   right)N = 0 rightrbrace$, is denoted $mathcal N_mu$. 


=== Signed Measure ===
Let $left( X, unicode{x3a3} right)$ be a measurable space.

Let $mu$ be a signed measure on $left( X, unicode{x3a3} right)$. 

Let $N in unicode{x3a3}$.


We say that $N$ is a $mu$-null set  if and only if :

:for each $A in unicode{x3a3}$ with $A subseteq N$, we have $mu left(   right)A = 0$


Category:Definitions/Signed Measures",Definition:Null Set,,false,"Let ( X, x3a3, μ) be a measure space.

A set N ∈x3a3 is called a (μ-)null set  if and only if  μ(   )N = 0.


 

=== Family of Null Sets ===

The family of μ-null sets, { N ∈x3a3: μ(   )N = 0 }, is denoted 𝒩_μ. 


=== Signed Measure ===
Let ( X, x3a3) be a measurable space.

Let μ be a signed measure on ( X, x3a3). 

Let N ∈x3a3.


We say that N is a μ-null set  if and only if :

:for each A ∈x3a3 with A ⊆ N, we have μ(   )A = 0


Category:Definitions/Signed Measures",Null
"['Definitions/Null Relation', 'Definitions/Empty Set', 'Definitions/Examples of Relations']",Definition:Null,"The null relation is a relation $mathcal R$ in $S$ to $T$ such that $mathcal R$ is the empty set:
:$mathcal R subseteq S times T: mathcal R = varnothing$


That is, no element of $S$ relates to any element in $T$:
:$mathcal R: S times T: forall left( s, t right) in S times T: neg s mathrel mathcal R t$",Definition:Null Relation,,false,"The null relation is a relation ℛ in S to T such that ℛ is the empty set:
:ℛ⊆ S × T: ℛ = ∅


That is, no element of S relates to any element in T:
:ℛ: S × T: ∀( s, t ) ∈ S × T:  s ℛ t",Null
"['Definitions/Ring Theory', 'Definitions/Examples of Rings']",Definition:Null,"A ring with one element is called the null ring.

That is, the null ring is $left( leftlbrace 0_R rightrbrace, +, circ right)$, where ring addition and the ring product are defined as:

 
 
 
 ",Definition:Null Ring,,false,"A ring with one element is called the null ring.

That is, the null ring is ( { 0_R }, +, ∘), where ring addition and the ring product are defined as:

 
 
 
 ",Null
['Definitions/Module Theory'],Definition:Null,"Let $left({R, +_R, circ_R}right)$ be a ring.

Let $G$ be the trivial group.


Then the $R$-module $left({G, +_G, circ}right)_R$ is known as the null module.",Definition:Null Module,,false,"Let (R, +_R, ∘_R) be a ring.

Let G be the trivial group.


Then the R-module (G, +_G, ∘)_R is known as the null module.",Null
"['Definitions/Null Measure', 'Definitions/Measures']",Definition:Null,"Let $left( X, unicode{x3a3} right)$ be a measurable space.


Then the null measure is the measure defined by:

:$mu: unicode{x3a3} to overline mathbb R: mu left(   right)E := 0$

where $overline mathbb R$ denotes the extended real numbers.",Definition:Null Measure,,false,"Let ( X, x3a3) be a measurable space.


Then the null measure is the measure defined by:

:μ: x3a3→R: μ(   )E := 0

where R denotes the extended real numbers.",Null
"['Definitions/Null Sequences', 'Definitions/Convergence', 'Definitions/Sequences', 'Definitions/Analysis', 'Definitions/Real Analysis', 'Definitions/Complex Analysis', 'Definitions/Normed Division Rings']",Definition:Null,"A null sequence is a sequence which converges to zero.


=== Normed Division Ring ===
Let $left( R, leftlVert ,cdot, rightrVert  right)$ be a normed division ring with zero $0_R$.

Let $leftlangle x_n rightrangle$ be a sequence in $R$ which converges to the limit $0_R$:

:$ds lim_{n mathop to infty} x_n = 0_R$


Then $leftlangle x_n rightrangle$ is called a null sequence.",Definition:Null Sequence,,false,"A null sequence is a sequence which converges to zero.


=== Normed Division Ring ===
Let ( R, ‖ · ‖) be a normed division ring with zero 0_R.

Let ⟨ x_n ⟩ be a sequence in R which converges to the limit 0_R:

:lim_n →∞ x_n = 0_R


Then ⟨ x_n ⟩ is called a null sequence.",Null
"['Definitions/Null URM Program', 'Definitions/URM Programs', 'Definitions/Unlimited Register Machines']",Definition:Null,"The null URM program is a URM program which contains no instructions.

That is, a URM program whose length is zero.",Definition:Unlimited Register Machine/Null Program,,false,"The null URM program is a URM program which contains no instructions.

That is, a URM program whose length is zero.",Null
"['Definitions/Null Graph', 'Definitions/Graph Theory']",Definition:Null,"The null graph is the graph which has no vertices.

That is, the null graph is the graph of order zero.


It is called the null graph because, from Empty Set is Unique, there is only one such entity.",Definition:Null Graph,,false,"The null graph is the graph which has no vertices.

That is, the null graph is the graph of order zero.


It is called the null graph because, from Empty Set is Unique, there is only one such entity.",Null
['Definitions/Collations'],Definition:Null,"A null string is a string with no symbols in it.

In particular, the null string is a word.


The null string can be denoted $epsilon$.",Definition:Null String,,false,"A null string is a string with no symbols in it.

In particular, the null string is a word.


The null string can be denoted ϵ.",Null
"['Definitions/Null Spaces', 'Definitions/Linear Algebra']",Definition:Null,"Let:
$quad mathbf A_{m times n} = begin {bmatrix}
a_{11} & a_{12} & cdots & a_{1n} \
a_{21} & a_{22} & cdots & a_{2n} \
vdots & vdots & ddots & vdots \
a_{m1} & a_{m2} & cdots & a_{mn} \
end {bmatrix}$,  $mathbf x_{n times 1} = begin {bmatrix} x_1 \ x_2 \ vdots \ x_n end {bmatrix}$, $mathbf 0_{m times 1} = begin {bmatrix} 0 \ 0 \ vdots \ 0 end {bmatrix}$

be matrices where each column is a member of a real vector space.

The set of all solutions to $mathbf A mathbf x = mathbf 0$:

:$mathrm N left(   right){mathbf A} = leftlbrace mathbf x in mathbb R^n : mathbf {A x} = mathbf 0 rightrbrace$

is called the null space of $mathbf A$.


 ",Definition:Null Space,,false,"Let:
𝐀_m × n = [ a_11 a_12    ⋯ a_1n; a_21 a_22    ⋯ a_2n;    ⋮    ⋮    ⋱    ⋮; a_m1 a_m2    ⋯ a_mn;      ],  𝐱_n × 1 = [ x_1; x_2;   ⋮; x_n ], 0_m × 1 = [ 0; 0; ⋮; 0 ]

be matrices where each column is a member of a real vector space.

The set of all solutions to 𝐀𝐱 = 0:

:N(   )𝐀 = {𝐱∈ℝ^n : 𝐀 𝐱 = 0}

is called the null space of 𝐀.


 ",Null
['Definitions/Ideal Theory'],Definition:Null,"Let $left( R, +, circ right)$ be a ring whose zero is $0_R$.


The null ring $left( leftlbrace 0_R rightrbrace, +, circ right)$ is called the null ideal of $R$.",Definition:Null Ideal,,false,"Let ( R, +, ∘) be a ring whose zero is 0_R.


The null ring ( { 0_R }, +, ∘) is called the null ideal of R.",Null
['Definitions/Category Theory'],Definition:Null,"Let $mathbf C$ be a category.


A zero object of $mathbf C$ is an object which is initial and terminal.",Definition:Zero Object,,false,"Let 𝐂 be a category.


A zero object of 𝐂 is an object which is initial and terminal.",Null
"['Definitions/Mapping Theory', 'Definitions/Injections']",Definition:One-to-One,"=== Definition 1 ===
A mapping $f$ is an injection, or injective  if and only if :
:$forall x_1, x_2 in mathrm {Dom} left( f right): f left(   right){x_1} = f left(   right){x_2} implies x_1 = x_2$


That is, an injection is a mapping such that the output uniquely determines its input.


=== Definition 1 a ===

This can otherwise be put:
A mapping $f$ is an injection, or injective  if and only if :

:$forall x_1, x_2 in mathrm {Dom} left( f right): x_1 ne x_2 implies f left(   right){x_1} ne f left(   right){x_2}$


That is, distinct elements of the domain are mapped to distinct elements of the image.

=== Definition 2 ===
An injection is a relation which is both one-to-one and left-total.


Thus, a relation $f$ is an injection  if and only if :

 
 
 
 
 

=== Definition 3 ===
Let $f$ be a mapping.

Then $f$ is an injection  if and only if :
:$f^{-1} {restriction_{mathrm {Img} left( f right)} }: mathrm {Img} left( f right) to mathrm {Dom} left( f right)$ is a mapping
where $f^{-1} {restriction_{mathrm {Img} left( f right)} }$ is the restriction of the inverse of $f$ to the image set of $f$.

=== Definition 4 ===
Let $f$ be a mapping.

$f$ is an injection  if and only if :
:$forall y in mathrm {Img} left( f right): leftlvert f^{-1}  left(   right)y rightrvert = leftlvert leftlbrace f^{-1} left[ leftlbrace y rightrbrace right]  rightrbrace  rightrvert = 1$
where:
:$mathrm {Img} left( f right)$ denotes the image set of $f$
:$leftlvert , cdot , rightrvert$ denotes the cardinality of a set
:$f^{-1}  left(   right)y$ is the preimage of $y$
:$f^{-1} left[ leftlbrace y rightrbrace right]$ is the preimage of the subset $leftlbrace y rightrbrace subseteq mathrm {Img} left( f right)$.


That is,  if and only if  the preimage of $y$ is a singleton for all $y$ in the image set of $f$.

=== Definition 5 ===
Let $f: S to T$ be a mapping where $S ne varnothing$.

Then $f$ is an injection  if and only if :
:$exists g: T to S: g circ f = I_S$
where $g$ is a mapping.


That is,  if and only if  $f$ has a left inverse.

=== Definition 6 ===
Let $f: S to T$ be a mapping where $S ne varnothing$.

Then $f$ is an injection  if and only if  $f$ is left cancellable:
:$forall X: forall g_1, g_2: X to S: f circ g_1 = f circ g_2 implies g_1 = g_2$
where $g_1$ and $g_2$ are arbitrary mappings from an arbitrary set $X$ to the domain $S$ of $f$.",Definition:Injection,,false,"=== Definition 1 ===
A mapping f is an injection, or injective  if and only if :
:∀ x_1, x_2 ∈Dom( f ): f (   )x_1 = f (   )x_2 x_1 = x_2


That is, an injection is a mapping such that the output uniquely determines its input.


=== Definition 1 a ===

This can otherwise be put:
A mapping f is an injection, or injective  if and only if :

:∀ x_1, x_2 ∈Dom( f ): x_1  x_2  f (   )x_1 f (   )x_2


That is, distinct elements of the domain are mapped to distinct elements of the image.

=== Definition 2 ===
An injection is a relation which is both one-to-one and left-total.


Thus, a relation f is an injection  if and only if :

 
 
 
 
 

=== Definition 3 ===
Let f be a mapping.

Then f is an injection  if and only if :
:f^-1_Img( f ): Img( f ) →Dom( f ) is a mapping
where f^-1_Img( f ) is the restriction of the inverse of f to the image set of f.

=== Definition 4 ===
Let f be a mapping.

f is an injection  if and only if :
:∀ y ∈Img( f ): | f^-1(   )y | = |{ f^-1[ { y }]  }| = 1
where:
:Img( f ) denotes the image set of f
:| · | denotes the cardinality of a set
:f^-1(   )y is the preimage of y
:f^-1[ { y }] is the preimage of the subset { y }⊆Img( f ).


That is,  if and only if  the preimage of y is a singleton for all y in the image set of f.

=== Definition 5 ===
Let f: S → T be a mapping where S ∅.

Then f is an injection  if and only if :
:∃ g: T → S: g ∘ f = I_S
where g is a mapping.


That is,  if and only if  f has a left inverse.

=== Definition 6 ===
Let f: S → T be a mapping where S ∅.

Then f is an injection  if and only if  f is left cancellable:
:∀ X: ∀ g_1, g_2: X → S: f ∘ g_1 = f ∘ g_2  g_1 = g_2
where g_1 and g_2 are arbitrary mappings from an arbitrary set X to the domain S of f.",One-to-One
['Definitions/Relation Theory'],Definition:One-to-One,"A relation $mathcal R subseteq S times T$ is one-to-one if it is both many-to-one and one-to-many.


That is, every element of the domain of $mathcal R$ relates to no more than one element of its codomain, and every element of the image is related to by exactly one element of its domain.",Definition:One-to-One Relation,,false,"A relation ℛ⊆ S × T is one-to-one if it is both many-to-one and one-to-many.


That is, every element of the domain of ℛ relates to no more than one element of its codomain, and every element of the image is related to by exactly one element of its domain.",One-to-One
['Definitions/Polygons'],Definition:Opposite,"When a polygon has an even number of sides, each side has an opposite side, and each vertex likewise has an opposite vertex.

When a polygon has an odd number of sides, each side has an opposite vertex.


The opposite side (or opposite vertex) to a given side (or vertex) is that side (or vertex) which has the same number of sides between it and the side (or vertex) in question.",Definition:Polygon/Opposite,,false,"When a polygon has an even number of sides, each side has an opposite side, and each vertex likewise has an opposite vertex.

When a polygon has an odd number of sides, each side has an opposite vertex.


The opposite side (or opposite vertex) to a given side (or vertex) is that side (or vertex) which has the same number of sides between it and the side (or vertex) in question.",Opposite
['Definitions/Triangles'],Definition:Opposite,"The side of a triangle which is not one of the sides adjacent to a particular vertex is referred to as its opposite.

Thus, each vertex has an opposite side, and each side has an opposite vertex.",Definition:Triangle (Geometry)/Opposite,,false,"The side of a triangle which is not one of the sides adjacent to a particular vertex is referred to as its opposite.

Thus, each vertex has an opposite side, and each side has an opposite vertex.",Opposite
['Definitions/Parallelepipeds'],Definition:Opposite,":

The opposite face of the face $F$ of a parallelepiped $P$ is the face of $P$ which is parallel to $F$.

In the above example, the pairs of parallel planes are:

:Face $ABCD$ is opposite $HGFE$
:Face $ADEH$ is opposite $BCFG$
:Face $ABGH$ is opposite $DCFE$",Definition:Parallelepiped/Opposite Face,,false,":

The opposite face of the face F of a parallelepiped P is the face of P which is parallel to F.

In the above example, the pairs of parallel planes are:

:Face ABCD is opposite HGFE
:Face ADEH is opposite BCFG
:Face ABGH is opposite DCFE",Opposite
['Definitions/Examples of Rings'],Definition:Opposite,"Let $left( R, +, times right)$ be a ring.


Let $* : R times R to R$ be the binary operation on $S$ defined by:
:$forall x, y in S: x * y = y times x$

The opposite ring of $R$ is the algebraic structure $left( R, +, * right)$.",Definition:Opposite Ring,,false,"Let ( R, +, ×) be a ring.


Let * : R × R → R be the binary operation on S defined by:
:∀ x, y ∈ S: x * y = y × x

The opposite ring of R is the algebraic structure ( R, +, * ).",Opposite
"['Definitions/Category Theory', 'Definitions/Examples of Categories']",Definition:Opposite,"Let $mathbf C$ be a metacategory.


Its dual category, denoted $mathbf C^{text{op} }$, is defined as follows:

 

It can be seen that this comes down to the metacategory obtained by reversing the direction of all morphisms of $mathbf C$.",Definition:Dual Category,,false,"Let 𝐂 be a metacategory.


Its dual category, denoted 𝐂^op, is defined as follows:

 

It can be seen that this comes down to the metacategory obtained by reversing the direction of all morphisms of 𝐂.",Opposite
['Definitions/Vectors'],Definition:Opposite,"Let $mathbf v$ and $mathbf w$ be vectors in space.

We say that $mathbf v$ is in the opposite direction to $mathbf w$  if and only if :

:the lines of action of $mathbf v$ and $mathbf w$ are parallel
but at the same time:
:the lines of action of $mathbf v$ and $mathbf w$ are not the same.

Category:Definitions/Vectors",Definition:Opposite Direction,,false,"Let 𝐯 and 𝐰 be vectors in space.

We say that 𝐯 is in the opposite direction to 𝐰  if and only if :

:the lines of action of 𝐯 and 𝐰 are parallel
but at the same time:
:the lines of action of 𝐯 and 𝐰 are not the same.

Category:Definitions/Vectors",Opposite
"['Definitions/Set Theory', 'Definitions/Cardinality']",Definition:Order,"Two sets (either finite or infinite) which are equivalent are said to have the same cardinality.

The cardinality of a set $S$ is written $leftlvert S rightrvert$.


=== Cardinality of Finite Set ===
Let $S$ be a finite set.

The cardinality $leftlvert S rightrvert$ of $S$ is the number of elements in $S$.

That is, if:
:$S sim mathbb N_{< n}$

where:
:$sim$ denotes set equivalence
:$mathbb N_{

Also note that from the definition of finite:
:$exists n in mathbb N: leftlvert S rightrvert = n iff S$ is finite.

=== Cardinality of Infinite Set ===
Let $S$ be an infinite set.

The cardinality $leftlvert S rightrvert$ of $S$ can be indicated as:
:$leftlvert S rightrvert = infty$

However, it needs to be noted that this just means that the cardinality of $S$ cannot be assigned a number $n in mathbb N$.


It means that $leftlvert S rightrvert$ is at least $aleph_0$ (aleph null).",Definition:Cardinality,,false,"Two sets (either finite or infinite) which are equivalent are said to have the same cardinality.

The cardinality of a set S is written | S |.


=== Cardinality of Finite Set ===
Let S be a finite set.

The cardinality | S | of S is the number of elements in S.

That is, if:
:S ∼ℕ_< n

where:
:∼ denotes set equivalence
:ℕ_

Also note that from the definition of finite:
:∃ n ∈ℕ: | S | = n  S is finite.

=== Cardinality of Infinite Set ===
Let S be an infinite set.

The cardinality | S | of S can be indicated as:
:| S | = ∞

However, it needs to be noted that this just means that the cardinality of S cannot be assigned a number n ∈ℕ.


It means that | S | is at least ℵ_0 (aleph null).",Order
"['Definitions/Order of Groups', 'Definitions/Group Theory', 'Definitions/Abstract Algebra']",Definition:Order,"The order of an algebraic structure $left( S, circ right)$ is the cardinality of its underlying set, and is denoted $leftlvert S rightrvert$.


Thus, for a finite set $S$, the order of $left( S, circ right)$ is the number of elements in $S$.


=== Infinite Structure ===
Let $left( S, circ right)$ be an algebraic structure.

Let the underlying set $S$ of $left( S, circ right)$ be infinite.

Then $left( S, circ right)$ is an infinite structure.


That is, $left( S, circ right)$ is an infinite structure  if and only if  $left( S, circ right)$ is not a finite structure.

=== Finite Structure ===
Let $left( S, circ right)$ be an algebraic structure.

Let the underlying set $S$ of $left( S, circ right)$ be finite.

Then $left( S, circ right)$ a finite structure.


That is, $left( S, circ right)$ is a finite structure  if and only if  $left( S, circ right)$ is not an infinite structure.",Definition:Order of Structure,,false,"The order of an algebraic structure ( S, ∘) is the cardinality of its underlying set, and is denoted | S |.


Thus, for a finite set S, the order of ( S, ∘) is the number of elements in S.


=== Infinite Structure ===
Let ( S, ∘) be an algebraic structure.

Let the underlying set S of ( S, ∘) be infinite.

Then ( S, ∘) is an infinite structure.


That is, ( S, ∘) is an infinite structure  if and only if  ( S, ∘) is not a finite structure.

=== Finite Structure ===
Let ( S, ∘) be an algebraic structure.

Let the underlying set S of ( S, ∘) be finite.

Then ( S, ∘) a finite structure.


That is, ( S, ∘) is a finite structure  if and only if  ( S, ∘) is not an infinite structure.",Order
"['Definitions/Group Theory', 'Definitions/Order of Group Elements']",Definition:Order,"Let $G$ be a group whose identity is $e_G$.

Let $x in G$ be an element of $G$.


=== Definition 1 ===
Let $G$ be a group whose identity is $e_G$.

Let $x in G$ be an element of $G$.


The order of $x$ (in $G$), denoted $leftlvert x rightrvert$, is the smallest $k in mathbb Z_{> 0}$ such that $x^k = e_G$.

=== Definition 2 ===
Let $G$ be a group whose identity is $e_G$.

Let $x in G$ be an element of $G$.


The order of $x$ (in $G$), denoted $leftlvert x rightrvert$, is the order of the group generated by $x$:
:$leftlvert x rightrvert := leftlvert {leftlangle x rightrangle} rightrvert$

=== Definition 3 ===
Let $G$ be a group whose identity is $e_G$.

Let $x in G$ be an element of $G$.


The order of $x$ (in $G$), denoted $leftvert{x}rightvert$, is the largest $k in mathbb Z_{gt 0}$ such that:
:$forall i, j in mathbb Z: 0 le i < j < k implies x^i ne x^j$",Definition:Order of Group Element,,false,"Let G be a group whose identity is e_G.

Let x ∈ G be an element of G.


=== Definition 1 ===
Let G be a group whose identity is e_G.

Let x ∈ G be an element of G.


The order of x (in G), denoted | x |, is the smallest k ∈ℤ_> 0 such that x^k = e_G.

=== Definition 2 ===
Let G be a group whose identity is e_G.

Let x ∈ G be an element of G.


The order of x (in G), denoted | x |, is the order of the group generated by x:
:| x | := |⟨ x ⟩|

=== Definition 3 ===
Let G be a group whose identity is e_G.

Let x ∈ G be an element of G.


The order of x (in G), denoted |x|, is the largest k ∈ℤ_ 0 such that:
:∀ i, j ∈ℤ: 0 ≤ i < j < k  x^i  x^j",Order
"['Definitions/Orders of Matrices', 'Definitions/Matrices']",Definition:Order,"Let $left[ a right]_{m n}$ be an $m times n$ matrix.

Then the parameters $m$ and $n$ are known as the order of the matrix.


=== Square Matrix ===
Let $mathbf A$ be an $n times n$ square matrix.

That is, let $mathbf A$ have $n$ rows (and by definition $n$ columns).


Then the order of $mathbf A$ is defined as being $n$.

=== Column Matrix ===
Let $mathbf A$ be an $n times 1$ column matrix.

Then the order of $mathbf A$ is defined as being $n$.


Category:Definitions/Orders of Matrices
Category:Definitions/Column Matrices

=== Row Matrix ===
Let $mathbf A$ be a $1 times n$ row matrix.

Then the order of $mathbf A$ is defined as being $n$.


Category:Definitions/Orders of Matrices
Category:Definitions/Row Matrices",Definition:Matrix/Order,,false,"Let [ a ]_m n be an m × n matrix.

Then the parameters m and n are known as the order of the matrix.


=== Square Matrix ===
Let 𝐀 be an n × n square matrix.

That is, let 𝐀 have n rows (and by definition n columns).


Then the order of 𝐀 is defined as being n.

=== Column Matrix ===
Let 𝐀 be an n × 1 column matrix.

Then the order of 𝐀 is defined as being n.


Category:Definitions/Orders of Matrices
Category:Definitions/Column Matrices

=== Row Matrix ===
Let 𝐀 be a 1 × n row matrix.

Then the order of 𝐀 is defined as being n.


Category:Definitions/Orders of Matrices
Category:Definitions/Row Matrices",Order
"['Definitions/Orders of Matrices', 'Definitions/Square Matrices']",Definition:Order,"Let $mathbf A$ be an $n times n$ square matrix.

That is, let $mathbf A$ have $n$ rows (and by definition $n$ columns).


Then the order of $mathbf A$ is defined as being $n$.",Definition:Matrix/Square Matrix/Order,,false,"Let 𝐀 be an n × n square matrix.

That is, let 𝐀 have n rows (and by definition n columns).


Then the order of 𝐀 is defined as being n.",Order
['Definitions/Determinants'],Definition:Order,The order of a determinant is defined as the order of the square matrix on which it is defined.,Definition:Determinant/Matrix/Order,defined,true,The order of a determinant is defined as the order of the square matrix on which it is defined.,Order
['Definitions/Number Theory'],Definition:Order,"Let $a$ and $n$ be integers.

Let there exist a positive integer $c$ such that:

:$a^c equiv 1 pmod n$


Then the least such integer is called order of $a$ modulo $n$.",Definition:Multiplicative Order of Integer,,false,"Let a and n be integers.

Let there exist a positive integer c such that:

:a^c ≡ 1  n


Then the least such integer is called order of a modulo n.",Order
['Definitions/Sociable Numbers'],Definition:Order,"Let $m$ be a positive integer.

Let $s left({m}right)$ be the aliquot sum of $m$.


Let a sequence $leftlangle{a_k}rightrangle$ be a sociable chain.

The order of $a_k$ is the smallest $r in mathbb Z_{>0}$ such that
:$a_r = a_0$


Category:Definitions/Sociable Numbers",Definition:Sociable Chain/Order,,false,"Let m be a positive integer.

Let s (m) be the aliquot sum of m.


Let a sequence ⟨a_k⟩ be a sociable chain.

The order of a_k is the smallest r ∈ℤ_>0 such that
:a_r = a_0


Category:Definitions/Sociable Numbers",Order
"['Definitions/Order of Derivative', 'Definitions/Higher Derivatives']",Definition:Order,"The order of a derivative is the number of times it has been differentiated.

For example:
:a first derivative is of first order, or order $1$
:a second derivative is of second order, or order $2$

and so on.",Definition:Derivative/Higher Derivatives/Order of Derivative,number,true,"The order of a derivative is the number of times it has been differentiated.

For example:
:a first derivative is of first order, or order 1
:a second derivative is of second order, or order 2

and so on.",Order
"['Definitions/Order of Entire Function', 'Definitions/Entire Functions']",Definition:Order,"Let $f: mathbb C to mathbb C$ be an entire function.

Let $ln$ denote the natural logarithm.


=== Definition 1 ===
Let $f: mathbb C to mathbb C$ be an entire function.


The order $alpha in left[ 0 ,.,.,   right]{+infty}$ of $f$ is the infimum of the $beta ge 0$ for which:
:$f left(   right)z = mathcal O left(   right){exp left(   right){leftlvert z rightrvert^beta} }$
or $infty$ if no such $beta$ exists, where $mathcal O$ denotes big-$mathcal O$ notation.

=== Definition 2 ===
Let $f: mathbb C to mathbb C$ be an entire function.

Let $f$ be not identically zero.


The order $alpha in left[ 0 ,.,.,   right]{+infty}$ of $f$ is the infimum of the $beta ge 0$ for which:
:$ds ln left(   right){max_{leftlvert z rightrvert mathop le R} leftlvert f left(   right)z rightrvert } = mathcal O left(   right){R^beta}$

or $infty$ if no such $beta$ exists, where $mathcal O$ denotes big-$mathcal O$ notation

The order of $0$ is $0$.

=== Definition 3 ===
Let $f: mathbb C to mathbb C$ be an entire function.

Let $f$ be non-constant.


The order $alpha in left[ 0 ,.,.,   right]{+infty}$ of $f$ is the limit superior:
:$ds limsup_{R mathop to infty} frac {ds ln ln max_{leftlvert z rightrvert mathop le R} leftlvert f rightrvert} {ln R}$

The order of a constant function is $0$.",Definition:Order of Entire Function,,false,"Let f: ℂ→ℂ be an entire function.

Let ln denote the natural logarithm.


=== Definition 1 ===
Let f: ℂ→ℂ be an entire function.


The order α∈[ 0  . . ]+∞ of f is the infimum of the β≥ 0 for which:
:f (   )z = 𝒪(   )exp(   )| z |^β
or ∞ if no such β exists, where 𝒪 denotes big-𝒪 notation.

=== Definition 2 ===
Let f: ℂ→ℂ be an entire function.

Let f be not identically zero.


The order α∈[ 0  . . ]+∞ of f is the infimum of the β≥ 0 for which:
:ln(   )max_| z |≤ R| f (   )z | = 𝒪(   )R^β

or ∞ if no such β exists, where 𝒪 denotes big-𝒪 notation

The order of 0 is 0.

=== Definition 3 ===
Let f: ℂ→ℂ be an entire function.

Let f be non-constant.


The order α∈[ 0  . . ]+∞ of f is the limit superior:
:lim sup_R →∞lnlnmax_| z |≤ R| f |/ln R

The order of a constant function is 0.",Order
"['Definitions/Real Analysis', 'Definitions/Complex Analysis']",Definition:Order,"Let $f: mathbb R to mathrm F$ be a function, where $mathrm F in leftlbrace mathbb R, mathbb C rightrbrace$.

Let $f$ be continuous on the real interval $left[ 0 ,.,.,   right)to$, except possibly for some finite number of discontinuities of the first kind in every finite subinterval of $left[ 0 ,.,.,   right)to$.

 


Then $f$ is said to be of exponential order, denoted $f in mathcal E$,  if and only if  it is of exponential order $a$ for some $a > 0$.


=== Exponential Order $a$ ===
Let $f: mathbb R to mathbb F$ be a function, where $mathbb F in leftlbrace mathbb R, mathbb C rightrbrace$.

Let $f$ be continuous on the real interval $left[ 0 ,.,.,   right)to$, except possibly for some finite number of discontinuities of the first kind in every finite subinterval of $left[ 0 ,.,.,   right)to$.

 

Let  $leftlvert , cdot , rightrvert$ be the absolute value if $f$ is real-valued, or the modulus if $f$ is complex-valued.

Let $e^{a t}$ be the exponential function, where $a in mathbb R$ is constant.


Then $f left(   right)t$ is said to be of exponential order $a$, denoted $f in mathcal E_a$,  if and only if  there exist strictly positive real numbers $M, K$ such that:

:$forall t ge M: leftlvert f left(   right)t rightrvert < K e^{a t}$",Definition:Exponential Order,,false,"Let f: ℝ→F be a function, where F∈{ℝ, ℂ}.

Let f be continuous on the real interval [ 0  . . )→, except possibly for some finite number of discontinuities of the first kind in every finite subinterval of [ 0  . . )→.

 


Then f is said to be of exponential order, denoted f ∈ℰ,  if and only if  it is of exponential order a for some a > 0.


=== Exponential Order a ===
Let f: ℝ→𝔽 be a function, where 𝔽∈{ℝ, ℂ}.

Let f be continuous on the real interval [ 0  . . )→, except possibly for some finite number of discontinuities of the first kind in every finite subinterval of [ 0  . . )→.

 

Let  | · | be the absolute value if f is real-valued, or the modulus if f is complex-valued.

Let e^a t be the exponential function, where a ∈ℝ is constant.


Then f (   )t is said to be of exponential order a, denoted f ∈ℰ_a,  if and only if  there exist strictly positive real numbers M, K such that:

:∀ t ≥ M: | f (   )t | < K e^a t",Order
"['Definitions/Order of Differential Equation', 'Definitions/Differential Equations']",Definition:Order,The order of a differential equation is defined as being the order of the highest order derivative that is present in the equation.,Definition:Differential Equation/Order,,false,The order of a differential equation is defined as being the order of the highest order derivative that is present in the equation.,Order
['Definitions/Graphs (Graph Theory)'],Definition:Order,"Let $G = left( V, E right)$ be a graph.

The order of $G$ is the cardinality of its vertex set.


That is, the order of $G$ is $leftlvert V rightrvert$.",Definition:Graph (Graph Theory)/Order,,false,"Let G = ( V, E ) be a graph.

The order of G is the cardinality of its vertex set.


That is, the order of G is | V |.",Order
['Definitions/Latin Squares'],Definition:Order,"Let $mathbf L$ be an $n times n$ Latin square.

The order of $mathbf L$ is $n$.


Category:Definitions/Latin Squares",Definition:Latin Square/Order,,false,"Let 𝐋 be an n × n Latin square.

The order of 𝐋 is n.


Category:Definitions/Latin Squares",Order
['Definitions/Splines'],Definition:Order,"Let $left[ a ,.,.,   right]b$ be a closed real interval.

Let $T := leftlbrace a = t_0, t_1, t_2, ldots, t_{n - 1}, t_n = b rightrbrace$ form a subdivision of $left[ a ,.,.,   right]b$.

Let $S: left[ a ,.,.,   right]b to mathbb R$ be a spline function on $left[ a ,.,.,   right]b$ on $T$.


Some sources, instead of referring to the degree of a spline, use the order.

Let the maximum degree of the polynomials $P_k$ fitted between $t_k$ and $t_{k + 1}$ be $n$.

The order of $S$ is then $n + 1$.


Category:Definitions/Splines",Definition:Spline Function/Order,,false,"Let [ a  . . ]b be a closed real interval.

Let T := { a = t_0, t_1, t_2, …, t_n - 1, t_n = b } form a subdivision of [ a  . . ]b.

Let S: [ a  . . ]b →ℝ be a spline function on [ a  . . ]b on T.


Some sources, instead of referring to the degree of a spline, use the order.

Let the maximum degree of the polynomials P_k fitted between t_k and t_k + 1 be n.

The order of S is then n + 1.


Category:Definitions/Splines",Order
"['Definitions/Order Theory', 'Definitions/Orderings']",Definition:Order,"Let $S$ be a set.

=== Definition 1 ===
Let $S$ be a set.

Let $mathcal R$ be a relation $mathcal R$ on $S$.


$mathcal R$ is an ordering on $S$  if and only if  $mathcal R$ satisfies the ordering axioms:
 

=== Definition 2 ===
Let $S$ be a set.

Let $mathcal R$ be a relation $mathcal R$ on $S$.


$mathcal R$ is an ordering on $S$  if and only if  $mathcal R$ satisfies the ordering axioms:
 

=== Class Theory ===
 
Let $V$ be a basic universe.

Let $mathcal R subseteq V times V$ be a relation on $V$.


$mathcal R$ is an ordering in $V$  if and only if  $mathcal R$ satisfies the ordering axioms:
 ",Definition:Ordering,,false,"Let S be a set.

=== Definition 1 ===
Let S be a set.

Let ℛ be a relation ℛ on S.


ℛ is an ordering on S  if and only if  ℛ satisfies the ordering axioms:
 

=== Definition 2 ===
Let S be a set.

Let ℛ be a relation ℛ on S.


ℛ is an ordering on S  if and only if  ℛ satisfies the ordering axioms:
 

=== Class Theory ===
 
Let V be a basic universe.

Let ℛ⊆ V × V be a relation on V.


ℛ is an ordering in V  if and only if  ℛ satisfies the ordering axioms:
 ",Order
['Definitions/Digraphs'],Definition:Orientation,"Let $G = left( V, E right)$ be a simple graph.

Let $H = left( V, A right)$ be a digraph.


Then $H$ is an orientation of $G$  if and only if  both of the following hold:

:$(1): quad H$ is a simple digraph. That is, $A$ is antisymmetric.
:$(2): quad forall x, y in V: left( leftlbrace x, y rightrbrace in E iff left( x, y right) in A lor left( y, x right) in A right)$


That is, $H$ is formed from $G$ by replacing every edge of $G$ with an arc.",Definition:Orientation (Graph Theory),,false,"Let G = ( V, E ) be a simple graph.

Let H = ( V, A ) be a digraph.


Then H is an orientation of G  if and only if  both of the following hold:

:(1):    H is a simple digraph. That is, A is antisymmetric.
:(2):   ∀ x, y ∈ V: ( { x, y }∈ E ( x, y ) ∈ A ( y, x ) ∈ A )


That is, H is formed from G by replacing every edge of G with an arc.",Orientation
"['Definitions/Analytic Geometry', 'Definitions/Orientation (Coordinate Axes)']",Definition:Orientation,"The orientation of a coordinate system is the disposition of the coordinate axes relative to each other.


=== Cartesian Plane ===
There are $2$ different orientations of a Cartesian plane:


: $qquad qquad qquad$ 


=== Right-Handed ===
A Cartesian plane is defined as being right-handed if it has the following property:

Let a right hand be placed, with palm uppermost, such that the thumb points along the $x$-axis in the positive direction, such that the thumb and index finger are at right-angles to each other.

Then the index finger is pointed along the $y$-axis in the positive direction.


:

=== Left-Handed ===
A Cartesian plane is defined as being left-handed if it has the following property:

Let a left hand be placed, with palm uppermost, such that the thumb points along the $x$-axis in the positive direction, such that the thumb and index finger are at right-angles to each other.

Then the index finger is pointed along the $y$-axis in the positive direction.


:

=== Cartesian $3$-Space ===
There are $2$ different orientations of a Cartesian $3$-space:


: $qquad qquad qquad$ 


=== Right-Handed ===
A Cartesian $3$-Space is defined as being right-handed if it has the following property:

Let a right hand be placed such that:
:the thumb and index finger are at right-angles to each other
:the $3$rd finger is at right-angles to the thumb and index finger, upwards from the palm
:the thumb points along the $x$-axis in the positive direction
:the index finger points along the $y$-axis in the positive direction.

Then the $3$rd finger is pointed along the $z$-axis in the positive direction.


:

=== Left-Handed ===
A Cartesian $3$-Space is defined as being left-handed if it has the following property:

Let a left hand be placed such that:
:the thumb and index finger are at right-angles to each other
:the $3$rd finger is at right-angles to the thumb and index finger, upwards from the palm
:the thumb points along the $x$-axis in the positive direction
:the index finger points along the $y$-axis in the positive direction.

Then the $3$rd finger is pointed along the $z$-axis in the positive direction.


:",Definition:Orientation of Coordinate Axes,,false,"The orientation of a coordinate system is the disposition of the coordinate axes relative to each other.


=== Cartesian Plane ===
There are 2 different orientations of a Cartesian plane:


:  


=== Right-Handed ===
A Cartesian plane is defined as being right-handed if it has the following property:

Let a right hand be placed, with palm uppermost, such that the thumb points along the x-axis in the positive direction, such that the thumb and index finger are at right-angles to each other.

Then the index finger is pointed along the y-axis in the positive direction.


:

=== Left-Handed ===
A Cartesian plane is defined as being left-handed if it has the following property:

Let a left hand be placed, with palm uppermost, such that the thumb points along the x-axis in the positive direction, such that the thumb and index finger are at right-angles to each other.

Then the index finger is pointed along the y-axis in the positive direction.


:

=== Cartesian 3-Space ===
There are 2 different orientations of a Cartesian 3-space:


:  


=== Right-Handed ===
A Cartesian 3-Space is defined as being right-handed if it has the following property:

Let a right hand be placed such that:
:the thumb and index finger are at right-angles to each other
:the 3rd finger is at right-angles to the thumb and index finger, upwards from the palm
:the thumb points along the x-axis in the positive direction
:the index finger points along the y-axis in the positive direction.

Then the 3rd finger is pointed along the z-axis in the positive direction.


:

=== Left-Handed ===
A Cartesian 3-Space is defined as being left-handed if it has the following property:

Let a left hand be placed such that:
:the thumb and index finger are at right-angles to each other
:the 3rd finger is at right-angles to the thumb and index finger, upwards from the palm
:the thumb points along the x-axis in the positive direction
:the index finger points along the y-axis in the positive direction.

Then the 3rd finger is pointed along the z-axis in the positive direction.


:",Orientation
['Definitions/Orthogonality (Geometry)'],Definition:Orthogonal,"=== Orthogonal Curves ===
Two curves are orthogonal if they intersect at right angles.

The term perpendicular can also be used, but the latter term is usual when the intersecting lines are straight.


=== Orthogonal Circles ===
Two circles are orthogonal if their angle of intersection is a right angle.


:

=== Orthogonal Surfaces ===
 ",Definition:Orthogonal (Analytic Geometry),,false,"=== Orthogonal Curves ===
Two curves are orthogonal if they intersect at right angles.

The term perpendicular can also be used, but the latter term is usual when the intersecting lines are straight.


=== Orthogonal Circles ===
Two circles are orthogonal if their angle of intersection is a right angle.


:

=== Orthogonal Surfaces ===
 ",Orthogonal
"['Definitions/Orthogonal Curves', 'Definitions/Orthogonality (Geometry)', 'Definitions/Analytic Geometry']",Definition:Orthogonal,"Two curves are orthogonal if they intersect at right angles.

The term perpendicular can also be used, but the latter term is usual when the intersecting lines are straight.


=== Orthogonal Circles ===
Two circles are orthogonal if their angle of intersection is a right angle.


:",Definition:Orthogonal Curves,,false,"Two curves are orthogonal if they intersect at right angles.

The term perpendicular can also be used, but the latter term is usual when the intersecting lines are straight.


=== Orthogonal Circles ===
Two circles are orthogonal if their angle of intersection is a right angle.


:",Orthogonal
"['Definitions/Orthogonal Coordinate Systems', 'Definitions/Coordinate Systems']",Definition:Orthogonal,"An orthogonal coordinate system is a coordinate system in which the coordinate axes are pairwise perpendicular.


=== Orthogonal Curvilinear Coordinates ===
Let $mathcal K$ be a curvilinear coordinate system in $3$-space.

Let $mathcal Q_1$, $mathcal Q_2$ and $mathcal Q_3$ denote the one-parameter families that define the curvilinear coordinates.



Let the relation between those curvilinear coordinates and Cartesian coordinates be expressed as:

 
 
 
 
 

where:
:$left( x, y, z right)$ denotes the Cartesian coordinates of an arbitrary point $P$
:$left( q_1, q_2, q_3 right)$ denotes the curvilinear coordinates of $P$.


Let these equations have the property that the metric of $mathcal K$ between coordinate surfaces of $mathcal Q_i$ and $mathcal Q_j$ is zero where $i ne j$.


That is, for every point $P$ expressible as $left( x, y, z right)$ and $left( q_1, q_2, q_3 right)$:

:$dfrac {partial x} {partial q_i} dfrac {partial x} {partial q_j} + dfrac {partial y} {partial q_i} dfrac {partial y} {partial q_j} + dfrac {partial z} {partial q_i} dfrac {partial z} {partial q_j} = 0$

wherever $i ne j$.


Then $mathcal K$ is an orthogonal curvilinear coordinate system.

=== Rectangular Coordinate System ===
A rectangular coordinate system is a Cartesian coordinate system in which each of the coordinate axes are perpendicular to each other.",Definition:Orthogonal Coordinate System,,false,"An orthogonal coordinate system is a coordinate system in which the coordinate axes are pairwise perpendicular.


=== Orthogonal Curvilinear Coordinates ===
Let 𝒦 be a curvilinear coordinate system in 3-space.

Let 𝒬_1, 𝒬_2 and 𝒬_3 denote the one-parameter families that define the curvilinear coordinates.



Let the relation between those curvilinear coordinates and Cartesian coordinates be expressed as:

 
 
 
 
 

where:
:( x, y, z ) denotes the Cartesian coordinates of an arbitrary point P
:( q_1, q_2, q_3 ) denotes the curvilinear coordinates of P.


Let these equations have the property that the metric of 𝒦 between coordinate surfaces of 𝒬_i and 𝒬_j is zero where i  j.


That is, for every point P expressible as ( x, y, z ) and ( q_1, q_2, q_3 ):

:∂ x∂ q_i∂ x∂ q_j + ∂ y∂ q_i∂ y∂ q_j + ∂ z∂ q_i∂ z∂ q_j = 0

wherever i  j.


Then 𝒦 is an orthogonal curvilinear coordinate system.

=== Rectangular Coordinate System ===
A rectangular coordinate system is a Cartesian coordinate system in which each of the coordinate axes are perpendicular to each other.",Orthogonal
"['Definitions/Orthogonal Curvilinear Coordinates', 'Definitions/Orthogonal Coordinate Systems', 'Definitions/Curvilinear Coordinates']",Definition:Orthogonal,"Let $mathcal K$ be a curvilinear coordinate system in $3$-space.

Let $mathcal Q_1$, $mathcal Q_2$ and $mathcal Q_3$ denote the one-parameter families that define the curvilinear coordinates.

=== Definition 1 ===
Let $mathcal K$ be a curvilinear coordinate system in $3$-space.

Let $mathcal Q_1$, $mathcal Q_2$ and $mathcal Q_3$ denote the one-parameter families that define the curvilinear coordinates.



Let the relation between those curvilinear coordinates and Cartesian coordinates be expressed as:

 
 
 
 
 

where:
:$left( x, y, z right)$ denotes the Cartesian coordinates of an arbitrary point $P$
:$left( q_1, q_2, q_3 right)$ denotes the curvilinear coordinates of $P$.


Let these equations have the property that the metric of $mathcal K$ between coordinate surfaces of $mathcal Q_i$ and $mathcal Q_j$ is zero where $i ne j$.


That is, for every point $P$ expressible as $left( x, y, z right)$ and $left( q_1, q_2, q_3 right)$:

:$dfrac {partial x} {partial q_i} dfrac {partial x} {partial q_j} + dfrac {partial y} {partial q_i} dfrac {partial y} {partial q_j} + dfrac {partial z} {partial q_i} dfrac {partial z} {partial q_j} = 0$

wherever $i ne j$.


Then $mathcal K$ is an orthogonal curvilinear coordinate system.

=== Definition 2 ===
Let $mathcal K$ be a curvilinear coordinate system in $3$-space.

Let $mathcal Q_1$, $mathcal Q_2$ and $mathcal Q_3$ denote the one-parameter families that define the curvilinear coordinates.

Let $left( q_1, q_2, q_3 right)$ denote a set of curvilinear coordinates.

Let $mathcal K$ have the property that for every arbitrary pair of coordinate surfaces $q_i in mathcal Q_i$ and $q_j in mathcal Q_j$ where $i ne j$:

:$q_i$ and $q_j$ are orthogonal.


Then $mathcal K$ is an orthogonal curvilinear coordinate system.

=== Scale Factor ===
 ",Definition:Orthogonal Curvilinear Coordinates,,false,"Let 𝒦 be a curvilinear coordinate system in 3-space.

Let 𝒬_1, 𝒬_2 and 𝒬_3 denote the one-parameter families that define the curvilinear coordinates.

=== Definition 1 ===
Let 𝒦 be a curvilinear coordinate system in 3-space.

Let 𝒬_1, 𝒬_2 and 𝒬_3 denote the one-parameter families that define the curvilinear coordinates.



Let the relation between those curvilinear coordinates and Cartesian coordinates be expressed as:

 
 
 
 
 

where:
:( x, y, z ) denotes the Cartesian coordinates of an arbitrary point P
:( q_1, q_2, q_3 ) denotes the curvilinear coordinates of P.


Let these equations have the property that the metric of 𝒦 between coordinate surfaces of 𝒬_i and 𝒬_j is zero where i  j.


That is, for every point P expressible as ( x, y, z ) and ( q_1, q_2, q_3 ):

:∂ x∂ q_i∂ x∂ q_j + ∂ y∂ q_i∂ y∂ q_j + ∂ z∂ q_i∂ z∂ q_j = 0

wherever i  j.


Then 𝒦 is an orthogonal curvilinear coordinate system.

=== Definition 2 ===
Let 𝒦 be a curvilinear coordinate system in 3-space.

Let 𝒬_1, 𝒬_2 and 𝒬_3 denote the one-parameter families that define the curvilinear coordinates.

Let ( q_1, q_2, q_3 ) denote a set of curvilinear coordinates.

Let 𝒦 have the property that for every arbitrary pair of coordinate surfaces q_i ∈𝒬_i and q_j ∈𝒬_j where i  j:

:q_i and q_j are orthogonal.


Then 𝒦 is an orthogonal curvilinear coordinate system.

=== Scale Factor ===
 ",Orthogonal
"['Definitions/Orthogonal Trajectories', 'Definitions/Orthogonal Curves']",Definition:Orthogonal,"Let $f left(   right){x, y, c}$ define a one-parameter family of curves $F$.

Let $g left(   right){x, y, c}$ also define a one-parameter family of curves $G$, with the property that:

:Every curve in $F$ is orthogonal to every curve in $G$.


Then $F$ is a family of (reciprocal) orthogonal trajectories of $G$, and contrariwise.",Definition:Orthogonal Trajectories,,false,"Let f (   )x, y, c define a one-parameter family of curves F.

Let g (   )x, y, c also define a one-parameter family of curves G, with the property that:

:Every curve in F is orthogonal to every curve in G.


Then F is a family of (reciprocal) orthogonal trajectories of G, and contrariwise.",Orthogonal
"['Definitions/Vector Algebra', 'Definitions/Linear Algebra', 'Definitions/Inner Product Spaces', 'Definitions/Orthogonality (Linear Algebra)']",Definition:Orthogonal,"Let $left( V, leftlangle cdot,   rightranglecdot right)$ be an inner product space.

Let $u, v in V$.


We say that $u$ and $v$ are orthogonal  if and only if :
:$leftlangle u,   rightrangle v = 0$

We denote this: 

:$u perp v$


=== Orthogonal Set ===
Let $left( V, leftlangle cdot,   rightranglecdot right)$ be an inner product space.

Let $S = leftlbrace u_1, ldots, u_n rightrbrace$ be a subset of $V$.


Then $S$ is an orthogonal set  if and only if  its elements are pairwise orthogonal:

:$forall i ne j: leftlangle u_i,   rightrangle{u_j} = 0$

=== Orthogonality of Sets ===
Let $left( V, leftlangle cdot,   rightranglecdot right)$ be an inner product space.

Let $A, B subseteq V$.

We say that $A$ and $B$ are orthogonal  if and only if :

:$forall a in A, b in B: a perp b$

That is, if $a$ and $b$ are orthogonal elements of $A$ and $B$ for all $a in A$ and $b in B$.


We write: 

:$A perp B$

=== Orthogonal Complement ===
Let $mathbb K$ be a field.

Let $V$ be a vector space over $mathbb K$.

Let $left( V, leftlangle cdot,   rightranglecdot right)$ be an inner product space.

Let $Ssubseteq V$ be a subset.


We define the orthogonal complement of $S$ (with respect to $leftlangle cdot,   rightranglecdot$), written $S^perp$ as the set of all $v in V$ which are orthogonal to all $s in S$.

That is: 

:$S^perp = leftlbrace v in V : leftlangle v,   rightrangle s = 0 text { for all } s in S rightrbrace$


If $S = leftlbrace v rightrbrace$ is a singleton, we may write $S^perp$ as $v^perp$.

=== Vectors in $mathbb R^n$ ===
Let $mathbf u$, $mathbf v$ be vectors in $mathbb R^n$.


Then $mathbf u$ and $mathbf v$ are said to be orthogonal  if and only if  their dot product is zero:

:$mathbf u cdot mathbf v = 0$


As Dot Product is Inner Product, this is a special case of the definition of orthogonal vectors.",Definition:Orthogonal (Linear Algebra),,false,"Let ( V, ⟨·,   ⟩·) be an inner product space.

Let u, v ∈ V.


We say that u and v are orthogonal  if and only if :
:⟨ u,   ⟩ v = 0

We denote this: 

:u ⊥ v


=== Orthogonal Set ===
Let ( V, ⟨·,   ⟩·) be an inner product space.

Let S = { u_1, …, u_n } be a subset of V.


Then S is an orthogonal set  if and only if  its elements are pairwise orthogonal:

:∀ i  j: ⟨ u_i,   ⟩u_j = 0

=== Orthogonality of Sets ===
Let ( V, ⟨·,   ⟩·) be an inner product space.

Let A, B ⊆ V.

We say that A and B are orthogonal  if and only if :

:∀ a ∈ A, b ∈ B: a ⊥ b

That is, if a and b are orthogonal elements of A and B for all a ∈ A and b ∈ B.


We write: 

:A ⊥ B

=== Orthogonal Complement ===
Let 𝕂 be a field.

Let V be a vector space over 𝕂.

Let ( V, ⟨·,   ⟩·) be an inner product space.

Let S⊆ V be a subset.


We define the orthogonal complement of S (with respect to ⟨·,   ⟩·), written S^⊥ as the set of all v ∈ V which are orthogonal to all s ∈ S.

That is: 

:S^⊥ = { v ∈ V : ⟨ v,   ⟩ s = 0  for all  s ∈ S }


If S = { v } is a singleton, we may write S^⊥ as v^⊥.

=== Vectors in ℝ^n ===
Let 𝐮, 𝐯 be vectors in ℝ^n.


Then 𝐮 and 𝐯 are said to be orthogonal  if and only if  their dot product is zero:

:𝐮·𝐯 = 0


As Dot Product is Inner Product, this is a special case of the definition of orthogonal vectors.",Orthogonal
['Definitions/Bilinear Forms (Linear Algebra)'],Definition:Orthogonal,"Let $mathbb K$ be a field.

Let $V$ be a vector space over $mathbb K$.

Let $b: V times V to mathbb K$ be a reflexive bilinear form on $V$.

Let $v,win V$.


Then $v$ and $w$ are orthogonal (with respect to $b$)  if and only if  $b left(   right){v, w} = b left(   right){w, v} = 0$


This is denoted: $v perp w$.


=== Orthogonal Subsets ===
Let $mathbb K$ be a field.

Let $V$ be a vector space over $mathbb K$.

Let $b: V times V to mathbb K$ be a reflexive bilinear form on $V$.

Let $S, T subset V$ be subsets.


Then $S$ and $T$ are orthogonal  if and only if  for all $s in S$ and $t in T$, $s$ and $t$ are orthogonal: $s perp t$.

=== Orthogonal Complement ===
Let $mathbb K$ be a field.

Let $V$ be a vector space over $mathbb K$.

Let $b : Vtimes V to mathbb K$ be a reflexive bilinear form on $V$.

Let $Ssubset V$ be a subset.


The orthogonal complement of $S$ (with respect to $b$) is the set of all $v in V$ which are orthogonal to all $s in S$.


This is denoted: $S^perp$.

If $S = leftlbrace v rightrbrace$ is a singleton, we also write $v^perp$.

=== Radical ===
Let $mathbb K$ be a field.

Let $V$ be a vector space over $mathbb K$.

Let $b : Vtimes V to mathbb K$ be a reflexive bilinear form on $V$.


The radical of $V$ is the orthogonal complement of $V$:
:$operatorname {rad}  left(   right)V = V^perp$",Definition:Orthogonal (Bilinear Form),,false,"Let 𝕂 be a field.

Let V be a vector space over 𝕂.

Let b: V × V →𝕂 be a reflexive bilinear form on V.

Let v,w∈ V.


Then v and w are orthogonal (with respect to b)  if and only if  b (   )v, w = b (   )w, v = 0


This is denoted: v ⊥ w.


=== Orthogonal Subsets ===
Let 𝕂 be a field.

Let V be a vector space over 𝕂.

Let b: V × V →𝕂 be a reflexive bilinear form on V.

Let S, T ⊂ V be subsets.


Then S and T are orthogonal  if and only if  for all s ∈ S and t ∈ T, s and t are orthogonal: s ⊥ t.

=== Orthogonal Complement ===
Let 𝕂 be a field.

Let V be a vector space over 𝕂.

Let b : V× V →𝕂 be a reflexive bilinear form on V.

Let S⊂ V be a subset.


The orthogonal complement of S (with respect to b) is the set of all v ∈ V which are orthogonal to all s ∈ S.


This is denoted: S^⊥.

If S = { v } is a singleton, we also write v^⊥.

=== Radical ===
Let 𝕂 be a field.

Let V be a vector space over 𝕂.

Let b : V× V →𝕂 be a reflexive bilinear form on V.


The radical of V is the orthogonal complement of V:
:rad(   )V = V^⊥",Orthogonal
['Definitions/Hilbert Spaces'],Definition:Orthogonal,"Let $H$ be a Hilbert space.

Let $M, N$ be closed linear subspaces of $H$.


Then the orthogonal difference of $M$ and $N$, denoted $M ominus N$, is the set $M cap N^perp$.

 ",Definition:Orthogonal Difference,,false,"Let H be a Hilbert space.

Let M, N be closed linear subspaces of H.


Then the orthogonal difference of M and N, denoted M ⊖ N, is the set M ∩ N^⊥.

 ",Orthogonal
"['Definitions/Orthogonal Matrices', 'Definitions/Matrix Algebra']",Definition:Orthogonal,"Let $R$ be a ring with unity.

Let $mathbf Q$ be an invertible square matrix over $R$.


=== Definition 1 ===
Let $R$ be a ring with unity.

Let $mathbf Q$ be an invertible square matrix over $R$.


Then $mathbf Q$ is orthogonal  if and only if :
:$mathbf Q^{-1} = mathbf Q^intercal$
where:
:$mathbf Q^{-1}$ is the inverse of $mathbf Q$
:$mathbf Q^intercal$ is the transpose of $mathbf Q$

=== Definition 2 ===
Let $R$ be a ring with unity.

Let $mathbf Q$ be an invertible square matrix over $R$.


Then $mathbf Q$ is orthogonal  if and only if :
:$mathbf Q^intercal mathbf Q = mathbf I$
where:
:$mathbf Q^intercal$ is the transpose of $mathbf Q$
:$mathbf I$ is the identity matrix of the same order as $mathbf Q$.

=== Definition 3 ===
Let $R$ be a ring with unity.

Let $mathbf Q$ be an invertible square matrix over $R$.


Then $mathbf Q$ is orthogonal  if and only if :
:$mathbf Q = left( mathbf Q^intercal right)^{-1}$
where:
:$mathbf Q^intercal$ is the transpose of $mathbf Q$
:$left( mathbf Q^intercal right)^{-1}$ is the inverse of $mathbf Q^intercal$.",Definition:Orthogonal Matrix,,false,"Let R be a ring with unity.

Let 𝐐 be an invertible square matrix over R.


=== Definition 1 ===
Let R be a ring with unity.

Let 𝐐 be an invertible square matrix over R.


Then 𝐐 is orthogonal  if and only if :
:𝐐^-1 = 𝐐^⊺
where:
:𝐐^-1 is the inverse of 𝐐
:𝐐^⊺ is the transpose of 𝐐

=== Definition 2 ===
Let R be a ring with unity.

Let 𝐐 be an invertible square matrix over R.


Then 𝐐 is orthogonal  if and only if :
:𝐐^⊺𝐐 = 𝐈
where:
:𝐐^⊺ is the transpose of 𝐐
:𝐈 is the identity matrix of the same order as 𝐐.

=== Definition 3 ===
Let R be a ring with unity.

Let 𝐐 be an invertible square matrix over R.


Then 𝐐 is orthogonal  if and only if :
:𝐐 = ( 𝐐^⊺)^-1
where:
:𝐐^⊺ is the transpose of 𝐐
:( 𝐐^⊺)^-1 is the inverse of 𝐐^⊺.",Orthogonal
['Definitions/Matrix Groups'],Definition:Orthogonal,"Let $k$ be a field.


The ($n$th) orthogonal group (on $k$), denoted $mathrm O left(   right){n, k}$, is the following subset of the general linear group $mathrm {GL} left( n, k right)$:

:$mathrm O left(   right){n, k} := leftlbrace M in mathrm {GL} left( n, k right): M^intercal = M^{-1}  rightrbrace$

where $M^intercal$ denotes the transpose of $M$.


Further, $mathrm O left(   right){n, k}$ is considered to be endowed with conventional matrix multiplication.


That is, the ($n$th) orthogonal group (on $k$) is the set of all orthogonal order-$n$ square matrices over $k$ under (conventional) matrix multiplication.


=== Orthogonal Group of Bilinear Form ===

Let $V$ be a vector space over a field $mathbb K$.

Let $B: V times V to mathbb K$ be a nondegenerate bilinear form.


Its orthogonal group $mathrm O left(   right)B$ is the group of invertible linear transformations $g in mathrm {GL} left( V right)$ such that:
:$forall v, w in V : B left(   right){g v, g w} = B left(   right){v, w}$


=== Orthogonal Group of Inner Product Space ===

Let $V$ be an inner product space.


Its orthogonal group $mathrm O left(   right)V$ is the group of invertible linear transformations $g in mathrm {GL} left( V right)$ such that:
:$forall v, w in V: leftlangle g v,   rightrangle{g w} = leftlangle v,   rightrangle w$

That is, it is the orthogonal group of its inner product.",Definition:Orthogonal Group,,false,"Let k be a field.


The (nth) orthogonal group (on k), denoted O(   )n, k, is the following subset of the general linear group GL( n, k ):

:O(   )n, k := { M ∈GL( n, k ): M^⊺ = M^-1}

where M^⊺ denotes the transpose of M.


Further, O(   )n, k is considered to be endowed with conventional matrix multiplication.


That is, the (nth) orthogonal group (on k) is the set of all orthogonal order-n square matrices over k under (conventional) matrix multiplication.


=== Orthogonal Group of Bilinear Form ===

Let V be a vector space over a field 𝕂.

Let B: V × V →𝕂 be a nondegenerate bilinear form.


Its orthogonal group O(   )B is the group of invertible linear transformations g ∈GL( V ) such that:
:∀ v, w ∈ V : B (   )g v, g w = B (   )v, w


=== Orthogonal Group of Inner Product Space ===

Let V be an inner product space.


Its orthogonal group O(   )V is the group of invertible linear transformations g ∈GL( V ) such that:
:∀ v, w ∈ V: ⟨ g v,   ⟩g w = ⟨ v,   ⟩ w

That is, it is the orthogonal group of its inner product.",Orthogonal
['Definitions/Orthogonality (Linear Algebra)'],Definition:Orthogonal Complement,"Let $mathbb K$ be a field.

Let $V$ be a vector space over $mathbb K$.

Let $left( V, leftlangle cdot,   rightranglecdot right)$ be an inner product space.

Let $Ssubseteq V$ be a subset.


We define the orthogonal complement of $S$ (with respect to $leftlangle cdot,   rightranglecdot$), written $S^perp$ as the set of all $v in V$ which are orthogonal to all $s in S$.

That is: 

:$S^perp = leftlbrace v in V : leftlangle v,   rightrangle s = 0 text { for all } s in S rightrbrace$


If $S = leftlbrace v rightrbrace$ is a singleton, we may write $S^perp$ as $v^perp$.",Definition:Orthogonal (Linear Algebra)/Orthogonal Complement,,false,"Let 𝕂 be a field.

Let V be a vector space over 𝕂.

Let ( V, ⟨·,   ⟩·) be an inner product space.

Let S⊆ V be a subset.


We define the orthogonal complement of S (with respect to ⟨·,   ⟩·), written S^⊥ as the set of all v ∈ V which are orthogonal to all s ∈ S.

That is: 

:S^⊥ = { v ∈ V : ⟨ v,   ⟩ s = 0  for all  s ∈ S }


If S = { v } is a singleton, we may write S^⊥ as v^⊥.",Orthogonal Complement
['Definitions/Bilinear Forms (Linear Algebra)'],Definition:Orthogonal Complement,"Let $mathbb K$ be a field.

Let $V$ be a vector space over $mathbb K$.

Let $b : Vtimes V to mathbb K$ be a reflexive bilinear form on $V$.

Let $Ssubset V$ be a subset.


The orthogonal complement of $S$ (with respect to $b$) is the set of all $v in V$ which are orthogonal to all $s in S$.


This is denoted: $S^perp$.

If $S = leftlbrace v rightrbrace$ is a singleton, we also write $v^perp$.",Definition:Orthogonal (Bilinear Form)/Orthogonal Complement,,false,"Let 𝕂 be a field.

Let V be a vector space over 𝕂.

Let b : V× V →𝕂 be a reflexive bilinear form on V.

Let S⊂ V be a subset.


The orthogonal complement of S (with respect to b) is the set of all v ∈ V which are orthogonal to all s ∈ S.


This is denoted: S^⊥.

If S = { v } is a singleton, we also write v^⊥.",Orthogonal Complement
"['Definitions/Set Theory', 'Definitions/Cartesian Product', 'Definitions/Ordered Pairs', 'Definitions/Ordered Tuples']",Definition:Pair,"The definition of a set does not take any account of the order in which the elements are listed.

That is, $leftlbrace a, b rightrbrace = leftlbrace b, a rightrbrace$, and the elements $a$ and $b$ have the same status - neither is distinguished above the other as being more ""important"".


=== Informal Definition ===
The definition of a set does not take any account of the order in which the elements are listed.

That is, $leftlbrace a, b rightrbrace = leftlbrace b, a rightrbrace$, and the elements $a$ and $b$ have the same status -- neither is distinguished above the other as being more ""important"".


An ordered pair is a two-element set together with an ordering.

In other words, one of the elements is distinguished above the other - it comes first.

Such a structure is written:
:$left( a, b right)$
and it means:
:first $a$, then $b$.


=== Coordinates ===
Let $left( a, b right)$ be an ordered pair.

The following terminology is used:
:$a$ is called the first coordinate
:$b$ is called the second coordinate.

This definition is compatible with the equivalent definition in the context of Cartesian coordinate systems.

=== Kuratowski Formalization ===
The definition of a set does not take any account of the order in which the elements are listed.

That is, $leftlbrace a, b rightrbrace = leftlbrace b, a rightrbrace$, and the elements $a$ and $b$ have the same status - neither is distinguished above the other as being more ""important"".


The concept of an ordered pair can be formalized by the definition:

:$left( a, b right) := leftlbrace leftlbrace a rightrbrace, leftlbrace a, b rightrbrace  rightrbrace$

This formalization justifies the existence of ordered pairs in Zermelo-Fraenkel set theory.


=== Coordinates ===
Let $left( a, b right)$ be an ordered pair.

The following terminology is used:
:$a$ is called the first coordinate
:$b$ is called the second coordinate.

This definition is compatible with the equivalent definition in the context of Cartesian coordinate systems.

=== Empty Set Formalization ===
The concept of an ordered pair can be formalized by the definition:

:$left( a, b right) := leftlbrace leftlbrace varnothing, a rightrbrace, leftlbrace leftlbrace varnothing rightrbrace, b rightrbrace  rightrbrace$

=== Wiener Formalization ===
The concept of an ordered pair can be formalized by the definition:

:$left( a, b right) := leftlbrace leftlbrace varnothing, leftlbrace a rightrbrace rightrbrace, leftlbrace leftlbrace b rightrbrace rightrbrace  rightrbrace$",Definition:Ordered Pair,,false,"The definition of a set does not take any account of the order in which the elements are listed.

That is, { a, b } = { b, a }, and the elements a and b have the same status - neither is distinguished above the other as being more ""important"".


=== Informal Definition ===
The definition of a set does not take any account of the order in which the elements are listed.

That is, { a, b } = { b, a }, and the elements a and b have the same status – neither is distinguished above the other as being more ""important"".


An ordered pair is a two-element set together with an ordering.

In other words, one of the elements is distinguished above the other - it comes first.

Such a structure is written:
:( a, b )
and it means:
:first a, then b.


=== Coordinates ===
Let ( a, b ) be an ordered pair.

The following terminology is used:
:a is called the first coordinate
:b is called the second coordinate.

This definition is compatible with the equivalent definition in the context of Cartesian coordinate systems.

=== Kuratowski Formalization ===
The definition of a set does not take any account of the order in which the elements are listed.

That is, { a, b } = { b, a }, and the elements a and b have the same status - neither is distinguished above the other as being more ""important"".


The concept of an ordered pair can be formalized by the definition:

:( a, b ) := {{ a }, { a, b }}

This formalization justifies the existence of ordered pairs in Zermelo-Fraenkel set theory.


=== Coordinates ===
Let ( a, b ) be an ordered pair.

The following terminology is used:
:a is called the first coordinate
:b is called the second coordinate.

This definition is compatible with the equivalent definition in the context of Cartesian coordinate systems.

=== Empty Set Formalization ===
The concept of an ordered pair can be formalized by the definition:

:( a, b ) := {{∅, a }, {{∅}, b }}

=== Wiener Formalization ===
The concept of an ordered pair can be formalized by the definition:

:( a, b ) := {{∅, { a }}, {{ b }}}",Pair
"['Definitions/Set Theory', 'Definitions/Doubletons']",Definition:Pair,"A doubleton is a set that contains exactly two elements.


The doubleton containing the distinct elements $a$ and $b$ can be written $leftlbrace a, b rightrbrace$.


The set $leftlbrace a, b rightrbrace$ is known as the doubleton of $a$ and $b$.


=== Class Theory Definition ===
Let $a$ and $b$ be sets.

The class $leftlbrace a, b rightrbrace$ is a doubleton (class).

It is defined as the class of all $x$ such that $x = a$ or $x = b$:

:$leftlbrace a, b rightrbrace = leftlbrace x: x = a lor x = b: a ne b rightrbrace$",Definition:Doubleton,,false,"A doubleton is a set that contains exactly two elements.


The doubleton containing the distinct elements a and b can be written { a, b }.


The set { a, b } is known as the doubleton of a and b.


=== Class Theory Definition ===
Let a and b be sets.

The class { a, b } is a doubleton (class).

It is defined as the class of all x such that x = a or x = b:

:{ a, b } = { x: x = a  x = b: a  b }",Pair
"['Definitions/Euclidean Geometry', 'Definitions/Parallel']",Definition:Parallel,"=== Lines ===
 
: 
:Parallel straight lines are straight lines which, being in the same plane and being produced indefinitely in either direction, do not meet one another in either direction.
 ''
 

The contemporary interpretation of the concept of parallelism declares that a straight line is parallel to itself.

=== Planes ===
Two planes are parallel  if and only if , when produced indefinitely, do not intersect at any point.


 


The contemporary interpretation of the concept of parallelism declares that a plane is parallel to itself.

=== Line Parallel to Plane ===
Let $L$ be a straight line.

Let $P$ be a plane.

Then $L$ and $P$ are parallel  if and only if , when produced indefinitely, they do not intersect at any point.


Category:Definitions/Parallel

=== Surfaces ===
Let $S_1$ and $S_2$ be surfaces in ordinary space.

Let $S_1$ and $S_2$ have the property that:

:for every point $P$ on $S_1$, a normal vector passing through $P$ is also a normal vector to $S_2$
and:
:for every point $Q$ on $S_2$, a normal vector passing through $Q$ is also a normal vector to $S_1$.


Then $S_1$ and $S_2$ are parallel.


 ",Definition:Parallel (Geometry),,false,"=== Lines ===
 
: 
:Parallel straight lines are straight lines which, being in the same plane and being produced indefinitely in either direction, do not meet one another in either direction.
 ”
 

The contemporary interpretation of the concept of parallelism declares that a straight line is parallel to itself.

=== Planes ===
Two planes are parallel  if and only if , when produced indefinitely, do not intersect at any point.


 


The contemporary interpretation of the concept of parallelism declares that a plane is parallel to itself.

=== Line Parallel to Plane ===
Let L be a straight line.

Let P be a plane.

Then L and P are parallel  if and only if , when produced indefinitely, they do not intersect at any point.


Category:Definitions/Parallel

=== Surfaces ===
Let S_1 and S_2 be surfaces in ordinary space.

Let S_1 and S_2 have the property that:

:for every point P on S_1, a normal vector passing through P is also a normal vector to S_2
and:
:for every point Q on S_2, a normal vector passing through Q is also a normal vector to S_1.


Then S_1 and S_2 are parallel.


 ",Parallel
"['Definitions/Parallel Lines', 'Definitions/Parallel']",Definition:Parallel," 
: 
:Parallel straight lines are straight lines which, being in the same plane and being produced indefinitely in either direction, do not meet one another in either direction.
 ''
 

The contemporary interpretation of the concept of parallelism declares that a straight line is parallel to itself.",Definition:Parallel (Geometry)/Lines,,false," 
: 
:Parallel straight lines are straight lines which, being in the same plane and being produced indefinitely in either direction, do not meet one another in either direction.
 ”
 

The contemporary interpretation of the concept of parallelism declares that a straight line is parallel to itself.",Parallel
"['Definitions/Parallel Planes', 'Definitions/Parallel']",Definition:Parallel,"Two planes are parallel  if and only if , when produced indefinitely, do not intersect at any point.


 


The contemporary interpretation of the concept of parallelism declares that a plane is parallel to itself.",Definition:Parallel (Geometry)/Planes,,false,"Two planes are parallel  if and only if , when produced indefinitely, do not intersect at any point.


 


The contemporary interpretation of the concept of parallelism declares that a plane is parallel to itself.",Parallel
['Definitions/Parallel'],Definition:Parallel,"Let $L$ be a straight line.

Let $P$ be a plane.

Then $L$ and $P$ are parallel  if and only if , when produced indefinitely, they do not intersect at any point.


Category:Definitions/Parallel",Definition:Parallel (Geometry)/Line to Plane,,false,"Let L be a straight line.

Let P be a plane.

Then L and P are parallel  if and only if , when produced indefinitely, they do not intersect at any point.


Category:Definitions/Parallel",Parallel
['Definitions/Matroid Theory'],Definition:Parallel,"Let $M = left( S, mathscr I right)$ be a matroid.


Two elements $x, y in S$ are said to be parallel in $M$  if and only if  they are not loops but $leftlbrace x, y rightrbrace$ is a dependent subset of $S$.


That is, $x, y in S$ are parallel  if and only if :
:$leftlbrace x rightrbrace, leftlbrace y rightrbrace in mathscr I$ and $leftlbrace x, y rightrbrace notin mathscr I$.",Definition:Parallel (Matroid),,false,"Let M = ( S, ℐ) be a matroid.


Two elements x, y ∈ S are said to be parallel in M  if and only if  they are not loops but { x, y } is a dependent subset of S.


That is, x, y ∈ S are parallel  if and only if :
:{ x }, { y }∈ℐ and { x, y }∉ℐ.",Parallel
"['Definitions/Integral Equations', 'Definitions/Parameters']",Definition:Parameter,"Consider the integral equation:

:of the first kind:
::$f left(   right)x = lambda ds int_{a left(   right)x}^{b left(   right)x} K left(   right){x, y} g left(   right)y ,mathrm d x$

:of the second kind:
::$g left(   right)x = f left(   right)x + lambda ds int_{a left(   right)x}^{b left(   right)x} K left(   right){x, y} g left(   right)y ,mathrm d x$

:of the third kind:
::$u left(   right)x g left(   right)x = f left(   right)x + lambda ds int_{a left(   right)x}^{b left(   right)x} K left(   right){x, y} g left(   right)y ,mathrm d x$


The number $lambda$ is known as the parameter of the integral equation.",Definition:Integral Equation/Parameter,,false,"Consider the integral equation:

:of the first kind:
::f (   )x = λ∫_a (   )x^b (   )x K (   )x, y g (   )y  d x

:of the second kind:
::g (   )x = f (   )x + λ∫_a (   )x^b (   )x K (   )x, y g (   )y  d x

:of the third kind:
::u (   )x g (   )x = f (   )x + λ∫_a (   )x^b (   )x K (   )x, y g (   )y  d x


The number λ is known as the parameter of the integral equation.",Parameter
"['Definitions/Population Parameters', 'Definitions/Descriptive Statistics']",Definition:Parameter,A population parameter is a numerical description of a population.,Definition:Population Parameter,,false,A population parameter is a numerical description of a population.,Parameter
['Definitions/Differential Equations'],Definition:Parameter,"Let $f$ be a differential equation with general solution $F$.

A parameter of $F$ is an arbitrary constant arising from the solving of a primitive during the course of obtaining the solution of $f$.",Definition:Parameter of Differential Equation,,false,"Let f be a differential equation with general solution F.

A parameter of F is an arbitrary constant arising from the solving of a primitive during the course of obtaining the solution of f.",Parameter
"['Definitions/One-Parameter Families of Curves', 'Definitions/One-Parameter Families']",Definition:Parameter,"Let $f left(   right){x, y, c}$ define a one-parameter family of curves $F$.

The value $c$ is the parameter of $F$.",Definition:Family of Curves/One-Parameter/Parameter,,false,"Let f (   )x, y, c define a one-parameter family of curves F.

The value c is the parameter of F.",Parameter
['Definitions/Integers'],Definition:Parity,"Let $z in mathbb Z$ be an integer.

The parity of $z$ is whether it is even or odd.


That is:
:an integer of the form $z = 2 n$, where $n$ is an integer, is of even parity;
:an integer of the form $z = 2 n + 1$, where $n$ is an integer, is of odd parity.


:If $z_1$ and $z_2$ are either both even or both odd, $z_1$ and $z_2$ have the same parity. 
:If $z_1$ is even and $z_2$ is odd, then $z_1$ and $z_2$ have opposite parity.",Definition:Parity of Integer,,false,"Let z ∈ℤ be an integer.

The parity of z is whether it is even or odd.


That is:
:an integer of the form z = 2 n, where n is an integer, is of even parity;
:an integer of the form z = 2 n + 1, where n is an integer, is of odd parity.


:If z_1 and z_2 are either both even or both odd, z_1 and z_2 have the same parity. 
:If z_1 is even and z_2 is odd, then z_1 and z_2 have opposite parity.",Parity
['Definitions/Permutation Theory'],Definition:Parity,"Let $n in mathbb N$ be a natural number.

Let $S_n$ denote the symmetric group on $n$ letters.

Let $rho in S_n$, that is, let $rho$ be a permutation of $S_n$.

The parity of $rho$ is defined as follows:


=== Even Permutation ===
Let $n in mathbb N$ be a natural number.

Let $S_n$ denote the symmetric group on $n$ letters.

Let $rho in S_n$ be a permutation in $S_n$.


$rho$ is an even permutation  if and only if :
:$mathrm {sgn} left(   right)rho = 1$
where $mathrm {sgn}$ denotes the sign function.

=== Odd Permutation ===
Let $n in mathbb N$ be a natural number.

Let $S_n$ denote the symmetric group on $n$ letters.

Let $rho in S_n$ be a permutation in $S_n$.


$rho$ is an odd permutation  if and only if :

:$mathrm {sgn} left(   right)rho = -1$

where $mathrm {sgn} $ denotes the sign function.

where $mathrm {sgn} left(   right)rho$ denotes the sign of $rho$.",Definition:Parity of Permutation,,false,"Let n ∈ℕ be a natural number.

Let S_n denote the symmetric group on n letters.

Let ρ∈ S_n, that is, let ρ be a permutation of S_n.

The parity of ρ is defined as follows:


=== Even Permutation ===
Let n ∈ℕ be a natural number.

Let S_n denote the symmetric group on n letters.

Let ρ∈ S_n be a permutation in S_n.


ρ is an even permutation  if and only if :
:sgn(   )ρ = 1
where sgn denotes the sign function.

=== Odd Permutation ===
Let n ∈ℕ be a natural number.

Let S_n denote the symmetric group on n letters.

Let ρ∈ S_n be a permutation in S_n.


ρ is an odd permutation  if and only if :

:sgn(   )ρ = -1

where sgn denotes the sign function.

where sgn(   )ρ denotes the sign of ρ.",Parity
"['Definitions/Paths (Topology)', 'Definitions/Path-Connected Spaces', 'Definitions/Complex Analysis', 'Definitions/Topology']",Definition:Path,"Let $T = left( S, tau right)$ be a topological space.

Let $I subset mathbb R$ be the closed real interval $left[ a ,.,.,   right]b$.


A path in $T$ is a continuous mapping $gamma: I to S$.


The mapping $gamma$ can be referred as:
:a path (in $T$) joining $gamma left(   right)a$ and $gamma left(   right)b$
or:
:a path (in $T$) from $gamma left(   right)a$ to $gamma left(   right)b$.


It is common to refer to a point $z = gamma left(   right)t$ as a point on the path $gamma$, even though $z$ is in fact on the image of $gamma$.


=== Initial Point ===
Let $T$ be a topological space.

Let $I subset mathbb R$ be the closed real interval $left[ a ,.,.,   right]b$.

Let $gamma: I to T$ be a path in $T$.


The initial point of $gamma$ is $gamma left(   right)a$.

That is, the path starts (or begins) at $gamma left(   right)a$.

=== Final Point ===
Let $T$ be a topological space.

Let $I subset mathbb R$ be the closed real interval $left[ a ,.,.,   right]b$.

Let $gamma: I to T$ be a path in $T$.


The final point of $gamma$ is $gamma left(   right)b$.

That is, the path ends (or finishes) at $gamma left(   right)b$.

=== Endpoint ===
Let $T$ be a topological space.

Let $I subset mathbb R$ be the closed real interval $left[ a ,.,.,   right]b$.

Let $gamma: I to T$ be a path in $T$.


The initial point and final point of $gamma$ can be referred to as the endpoints of $gamma$


Category:Definitions/Path-Connected Spaces

=== Composable Paths ===
Let $T$ be a topological space.

Let $f, g: left[ 0 ,.,.,   right]1 to T$ be paths.


$f$ and $g$ are said to be composable paths if:

:$f left(   right)1 = g left(   right)0$.",Definition:Path (Topology),,false,"Let T = ( S, τ) be a topological space.

Let I ⊂ℝ be the closed real interval [ a  . . ]b.


A path in T is a continuous mapping γ: I → S.


The mapping γ can be referred as:
:a path (in T) joining γ(   )a and γ(   )b
or:
:a path (in T) from γ(   )a to γ(   )b.


It is common to refer to a point z = γ(   )t as a point on the path γ, even though z is in fact on the image of γ.


=== Initial Point ===
Let T be a topological space.

Let I ⊂ℝ be the closed real interval [ a  . . ]b.

Let γ: I → T be a path in T.


The initial point of γ is γ(   )a.

That is, the path starts (or begins) at γ(   )a.

=== Final Point ===
Let T be a topological space.

Let I ⊂ℝ be the closed real interval [ a  . . ]b.

Let γ: I → T be a path in T.


The final point of γ is γ(   )b.

That is, the path ends (or finishes) at γ(   )b.

=== Endpoint ===
Let T be a topological space.

Let I ⊂ℝ be the closed real interval [ a  . . ]b.

Let γ: I → T be a path in T.


The initial point and final point of γ can be referred to as the endpoints of γ


Category:Definitions/Path-Connected Spaces

=== Composable Paths ===
Let T be a topological space.

Let f, g: [ 0  . . ]1 → T be paths.


f and g are said to be composable paths if:

:f (   )1 = g (   )0.",Path
"['Definitions/Paths (Graph Theory)', 'Definitions/Graph Theory']",Definition:Path,"Let $G$ be an undirected graph.

A path in $G$ is a trail in $G$ in which all vertices (except perhaps the first and last ones) are distinct.


A path between two vertices $u$ and $v$ is called a $u$-$v$ path.


=== Subgraph ===
The set of vertices and edges which go to make up a path in a graph $G$ form a subgraph of $G$.

This subgraph itself is also referred to as a path in $G$.

=== Open Path ===
An open path is a path in which the first and last vertices are distinct.



=== Endpoint of Open Path ===
Let $P$ be an open path in a graph $G$.

The endpoints of $P$ are its first and last vertices.


Category:Definitions/Paths (Graph Theory)",Definition:Path (Graph Theory),,false,"Let G be an undirected graph.

A path in G is a trail in G in which all vertices (except perhaps the first and last ones) are distinct.


A path between two vertices u and v is called a u-v path.


=== Subgraph ===
The set of vertices and edges which go to make up a path in a graph G form a subgraph of G.

This subgraph itself is also referred to as a path in G.

=== Open Path ===
An open path is a path in which the first and last vertices are distinct.



=== Endpoint of Open Path ===
Let P be an open path in a graph G.

The endpoints of P are its first and last vertices.


Category:Definitions/Paths (Graph Theory)",Path
"['Definitions/Paths in Digraphs', 'Definitions/Paths (Graph Theory)', 'Definitions/Digraphs']",Definition:Path,"Let $D = left( V, E right)$ be a digraph.

A path $P$ in $D$ is:
:a sequence of vertices $v_1, v_2, ldots, v_n$ in $V$ and a sequence of arcs $e_1, e_2, ldots{}, e_{n - 1}$ in $E$ such that:
:$P$ begins with $v_1$ and ends with $v_n$
:in which each arc $e_j$ is incident from $v_j$ and incident to $v_{j + 1}$
:all arcs are distinct
:all vertices (except perhaps the first and last ones) are distinct.

A path between two vertices $u$ and $v$ is called a path from $u$ to $v$.


=== Predecessor ===
Let $D = left( V, E right)$ be a digraph.

Let $P$ be a path in $D$ such that the vertices of $P$ are $v_1, v_2, ldots, v_n$.

Let $v_j$ be a vertex of $P$ such that $j > 1$.

Then the predecessor (vertex) of $v_j$ is the vertex $v_{j - 1}$.


That is, if $v to w$ is an arc in $P$, $v$ is the predecessor (vertex) of $w$.

=== Successor ===
Let $D = left( V, E right)$ be a digraph.

Let $P$ be a path in $D$ such that the vertices of $P$ are $v_1, v_2, ldots, v_n$.

Let $v_j$ be a vertex of $P$ such that $j < n$.

Then the successor (vertex) of $v_j$ is the vertex $v_{j + 1}$.


That is, if $v to w$ is an arc in $P$, $w$ is the successor (vertex) of $v$.",Definition:Path (Graph Theory)/Digraph,,false,"Let D = ( V, E ) be a digraph.

A path P in D is:
:a sequence of vertices v_1, v_2, …, v_n in V and a sequence of arcs e_1, e_2, …, e_n - 1 in E such that:
:P begins with v_1 and ends with v_n
:in which each arc e_j is incident from v_j and incident to v_j + 1
:all arcs are distinct
:all vertices (except perhaps the first and last ones) are distinct.

A path between two vertices u and v is called a path from u to v.


=== Predecessor ===
Let D = ( V, E ) be a digraph.

Let P be a path in D such that the vertices of P are v_1, v_2, …, v_n.

Let v_j be a vertex of P such that j > 1.

Then the predecessor (vertex) of v_j is the vertex v_j - 1.


That is, if v → w is an arc in P, v is the predecessor (vertex) of w.

=== Successor ===
Let D = ( V, E ) be a digraph.

Let P be a path in D such that the vertices of P are v_1, v_2, …, v_n.

Let v_j be a vertex of P such that j < n.

Then the successor (vertex) of v_j is the vertex v_j + 1.


That is, if v → w is an arc in P, w is the successor (vertex) of v.",Path
"['Definitions/Cycles (Graph Theory)', 'Definitions/Circuits (Graph Theory)', 'Definitions/Paths (Graph Theory)', 'Definitions/Graph Theory']",Definition:Path,"A cycle is a circuit in which no vertex except the first (which is also the last) appears more than once.


An $n$-cycle is a cycle with $n$ vertices.


=== Subgraph ===
The set of vertices and edges which go to make up a cycle form a subgraph.

This subgraph itself is also referred to as a cycle.

=== Odd Cycle ===
An odd cycle is a cycle with odd length, that is, with an odd number of edges.

=== Even Cycle ===
An even cycle is a cycle with even length, that is, with an even number of edges.",Definition:Cycle (Graph Theory),,false,"A cycle is a circuit in which no vertex except the first (which is also the last) appears more than once.


An n-cycle is a cycle with n vertices.


=== Subgraph ===
The set of vertices and edges which go to make up a cycle form a subgraph.

This subgraph itself is also referred to as a cycle.

=== Odd Cycle ===
An odd cycle is a cycle with odd length, that is, with an odd number of edges.

=== Even Cycle ===
An even cycle is a cycle with even length, that is, with an even number of edges.",Path
"['Definitions/Number Theory', 'Definitions/Abundance', 'Definitions/Abundancy', 'Definitions/Perfect Numbers']",Definition:Perfect,"=== Definition 1 ===
A perfect number is a (strictly) positive integer equal to its aliquot sum.

=== Definition 2 ===
A perfect number $n$ is a (strictly) positive integer such that:
:$sigma_1 left(   right)n= 2 n$
where $sigma_1: mathbb Z_{>0} to mathbb Z_{>0}$ is the divisor sum function.

=== Definition 3 ===
Let $n in mathbb Z_{ge 0}$ be a positive integer.


Let $A left({n}right)$ denote the abundance of $n$.

$n$ is perfect  if and only if  $A left({n}right) = 0$.

=== Definition 4 ===
A perfect number $n$ is a (strictly) positive integer such that:
:$dfrac {sigma_1 left(   right)n} n = 2$
where $sigma_1: mathbb Z_{>0} to mathbb Z_{>0}$ is the divisor sum function.",Definition:Perfect Number,,false,"=== Definition 1 ===
A perfect number is a (strictly) positive integer equal to its aliquot sum.

=== Definition 2 ===
A perfect number n is a (strictly) positive integer such that:
:σ_1 (   )n= 2 n
where σ_1: ℤ_>0→ℤ_>0 is the divisor sum function.

=== Definition 3 ===
Let n ∈ℤ_≥ 0 be a positive integer.


Let A (n) denote the abundance of n.

n is perfect  if and only if  A (n) = 0.

=== Definition 4 ===
A perfect number n is a (strictly) positive integer such that:
:σ_1 (   )n n = 2
where σ_1: ℤ_>0→ℤ_>0 is the divisor sum function.",Perfect
"['Definitions/Topology', 'Definitions/Perfect Sets']",Definition:Perfect,"=== Definition 1 ===
A perfect set of a topological space $T = left({S, tau}right)$ is a subset $H subseteq S$ such that:
:$H = H'$
where $H'$ is the derived set of $H$.

That is, where:
:every point of $H$ is a limit point of $H$
and
:every limit point of $H$ is a point of $H$.

=== Definition 2 ===
A perfect set of a topological space $T = left({S, tau}right)$ is a subset $H subseteq S$ such that:
: $H$ is a closed set of $T$
: $H$ has no isolated points.

=== Definition 3 ===
A perfect set of a topological space $T = left( S, tau right)$ is a subset $H subseteq S$ such that:
:$H$ is dense-in-itself.
:$H$ contains all its limit points.",Definition:Perfect Set,,false,"=== Definition 1 ===
A perfect set of a topological space T = (S, τ) is a subset H ⊆ S such that:
:H = H'
where H' is the derived set of H.

That is, where:
:every point of H is a limit point of H
and
:every limit point of H is a point of H.

=== Definition 2 ===
A perfect set of a topological space T = (S, τ) is a subset H ⊆ S such that:
: H is a closed set of T
: H has no isolated points.

=== Definition 3 ===
A perfect set of a topological space T = ( S, τ) is a subset H ⊆ S such that:
:H is dense-in-itself.
:H contains all its limit points.",Perfect
['Definitions/Graph Theory'],Definition:Perfect,A graph is perfect if no two vertices have the same degree.,Definition:Perfect Graph,,false,A graph is perfect if no two vertices have the same degree.,Perfect
"['Definitions/Field Theory', 'Definitions/Perfect Fields']",Definition:Perfect,"Let $F$ be a field.


=== Definition 1 ===
Let $F$ be a field.


$F$ is a perfect field   if and only if  $F$ has no inseparable extensions.

=== Definition 2 ===
Let $F$ be a field.


$F$ is a perfect field  if and only if  one of the following holds:
:$mathrm {Char} left( F right) = 0$
:$mathrm {Char} left( F right) = p$ with $p$ prime and $operatorname {Frob}$ is an automorphism of $F$

where:
:$mathrm {Char} left( F right)$ denotes the characteristic of $F$
:$operatorname {Frob}$ denotes the Frobenius endomorphism on $F$",Definition:Perfect Field,,false,"Let F be a field.


=== Definition 1 ===
Let F be a field.


F is a perfect field   if and only if  F has no inseparable extensions.

=== Definition 2 ===
Let F be a field.


F is a perfect field  if and only if  one of the following holds:
:Char( F ) = 0
:Char( F ) = p with p prime and Frob is an automorphism of F

where:
:Char( F ) denotes the characteristic of F
:Frob denotes the Frobenius endomorphism on F",Perfect
['Definitions/Periodic Functions'],Definition:Period,"Let $f: mathbb R to mathbb R$ be a periodic real function.


The period of $f$ is the smallest value $L in mathbb R_{>0}$ such that:
:$forall x in mathbb R: f left(   right)x = f left(   right){x + L}$",Definition:Periodic Real Function/Period,,false,"Let f: ℝ→ℝ be a periodic real function.


The period of f is the smallest value L ∈ℝ_>0 such that:
:∀ x ∈ℝ: f (   )x = f (   )x + L",Period
"['Definitions/Real Analysis', 'Definitions/Periodic Functions']",Definition:Period,"Let $f: mathbb R to mathbb R$ be a real function.


Then $f$ is periodic  if and only if :
:$exists L in mathbb R_{ne 0}: forall x in mathbb R: f left(   right)x = f left(   right){x + L}$


=== Period ===
Let $f: mathbb R to mathbb R$ be a periodic real function.


The period of $f$ is the smallest value $L in mathbb R_{>0}$ such that:
:$forall x in mathbb R: f left(   right)x = f left(   right){x + L}$

=== Frequency ===
Let $f: mathbb R to mathbb R$ be a periodic real function.

The frequency $nu$ of $f$ is the reciprocal of the period $L$ of $f$:
:$nu = dfrac 1 L$

where:
:$forall x in X: f left(   right)x = f left(   right){x + L}$

=== Amplitude ===
Let $f: mathbb R to mathbb R$ be a periodic real function.


The amplitude of $f$ is the maximum absolute difference of the value of $f$ from a reference level.",Definition:Periodic Function/Real,,false,"Let f: ℝ→ℝ be a real function.


Then f is periodic  if and only if :
:∃ L ∈ℝ_ 0: ∀ x ∈ℝ: f (   )x = f (   )x + L


=== Period ===
Let f: ℝ→ℝ be a periodic real function.


The period of f is the smallest value L ∈ℝ_>0 such that:
:∀ x ∈ℝ: f (   )x = f (   )x + L

=== Frequency ===
Let f: ℝ→ℝ be a periodic real function.

The frequency ν of f is the reciprocal of the period L of f:
:ν =  1 L

where:
:∀ x ∈ X: f (   )x = f (   )x + L

=== Amplitude ===
Let f: ℝ→ℝ be a periodic real function.


The amplitude of f is the maximum absolute difference of the value of f from a reference level.",Period
"['Definitions/Permutable Primes', 'Definitions/Number Theory', 'Definitions/Recreational Mathematics', 'Definitions/Prime Numbers']",Definition:Permutable,"A permutable prime is a prime number $p$ which has the property that all anagrams of $p$ are prime.


=== Sequence ===
The sequence of permutable primes begins:
:$2, 3, 5, 7, 11, 13, 17, 31, 37, 71, 73, 79, 97, 113, 131, 199, 311, 337, 373, 733, 919, 991, R_{19}, R_{23}, R_{317}, R_{1091}, ldots$
where $R_n$ denotes the repunit of $n$ digits.

 

The smallest 
elements of the permutation sets of these are:
:$2, 3, 5, 7, 11, 13, 17, 37, 79, 113, 199, 337, R_{19}, R_{23}, R_{317}, R_{1091}, ldots$

 

 ",Definition:Permutable Prime,,false,"A permutable prime is a prime number p which has the property that all anagrams of p are prime.


=== Sequence ===
The sequence of permutable primes begins:
:2, 3, 5, 7, 11, 13, 17, 31, 37, 71, 73, 79, 97, 113, 131, 199, 311, 337, 373, 733, 919, 991, R_19, R_23, R_317, R_1091, …
where R_n denotes the repunit of n digits.

 

The smallest 
elements of the permutation sets of these are:
:2, 3, 5, 7, 11, 13, 17, 37, 79, 113, 199, 337, R_19, R_23, R_317, R_1091, …

 

 ",Permutable
['Definitions/Group Theory'],Definition:Permutable,"Let $left( G, circ right)$ be a group.

Let $H$ and $K$ be subgroups of $G$.

Let $H circ K$ denote the subset product of $H$ and $K$.


Then $H$ and $K$ are permutable  if and only if :
:$H circ K = K circ H$",Definition:Permutable Subgroups,,false,"Let ( G, ∘) be a group.

Let H and K be subgroups of G.

Let H ∘ K denote the subset product of H and K.


Then H and K are permutable  if and only if :
:H ∘ K = K ∘ H",Permutable
['Definitions/Commutativity'],Definition:Permutable,"Let $circ$ be a binary operation.


Two elements $x, y$ are said to commute (with each other)  if and only if :
:$x circ y = y circ x$


Thus $x$ and $y$ can be described as commutative (elements) under $circ$.",Definition:Commutative/Elements,,false,"Let ∘ be a binary operation.


Two elements x, y are said to commute (with each other)  if and only if :
:x ∘ y = y ∘ x


Thus x and y can be described as commutative (elements) under ∘.",Permutable
"['Definitions/Polar Coordinates', 'Definitions/Examples of Coordinate Systems', 'Definitions/Curvilinear Coordinates']",Definition:Polar,"Polar coordinates are a technique for unique identification of points on the plane.

A distinct point $O$ is identified.


=== Pole ===
Consider a system of polar coordinates used to identify points on the plane.

Let $O$ be the distinct point identified as the center of the frame.


The point $O$ is referred to as the pole of the polar coordinate plane.

=== Polar Axis ===
Let $O$ be the pole of the polar coordinate plane.


A ray is drawn from $O$, usually to the right, and referred to as the polar axis.",Definition:Polar Coordinates,,false,"Polar coordinates are a technique for unique identification of points on the plane.

A distinct point O is identified.


=== Pole ===
Consider a system of polar coordinates used to identify points on the plane.

Let O be the distinct point identified as the center of the frame.


The point O is referred to as the pole of the polar coordinate plane.

=== Polar Axis ===
Let O be the pole of the polar coordinate plane.


A ray is drawn from O, usually to the right, and referred to as the polar axis.",Polar
"['Definitions/Polar Coordinates', 'Definitions/Polar Axes']",Definition:Polar,"Let $O$ be the pole of the polar coordinate plane.


A ray is drawn from $O$, usually to the right, and referred to as the polar axis.",Definition:Polar Coordinates/Polar Axis,,false,"Let O be the pole of the polar coordinate plane.


A ray is drawn from O, usually to the right, and referred to as the polar axis.",Polar
"['Definitions/Polar Equations', 'Definitions/Polar Coordinates']",Definition:Polar,"A polar equation is an equation defining the locus of a set of points in the polar coordinate plane.

Such an equation is generally presented in terms of the variables:
:$r$: the radial coordinate
:$theta$: the angular coordinate",Definition:Polar Equation,,false,"A polar equation is an equation defining the locus of a set of points in the polar coordinate plane.

Such an equation is generally presented in terms of the variables:
:r: the radial coordinate
:θ: the angular coordinate",Polar
"['Definitions/Polars of Points', 'Definitions/Conic Sections', 'Definitions/Tangents', 'Definitions/Projective Geometry', 'Definitions/Analytic Geometry']",Definition:Polar,"Let $mathcal K$ be a conic section embedded in a Euclidean plane.

Let $P$ be an arbitrary point in that plane.

Let a secant line pass through $P$ and intersect $mathcal K$ at $L$ and $M$.


The polar of $P$ with respect to $mathcal K$ is the straight line upon which the tangents to $mathcal K$ intersect.


=== Circle ===
Let $mathcal C$ be a circle whose radius is $r$ and whose center is at the origin of a Cartesian plane.

Let $P = left( x_0, y_0 right)$ be an arbitrary point in the Cartesian plane.


The polar of $P$ with respect to $mathcal C$ is the straight line whose equation is given by:

:$x x_0 + y y_0 = r^2$


=== Pole ===
Let $mathcal C$ be a circle embedded in the plane.

Let $P$ be an arbitrary point in the plane.


Let $mathcal L$ be the polar of $P$ with respect to $mathcal C$.

Then $P$ is known as the pole of $mathcal L$.

=== Ellipse ===
Let $mathcal E$ be an ellipse embedded in a Cartesian plane in reduced form with the equation:
:$dfrac {x^2} {a^2} + dfrac {y^2} {b^2} = 1$


Let $P = left( x_0, y_0 right)$ be an arbitrary point in the Cartesian plane.


The polar of $P$ with respect to $mathcal E$ is the straight line whose equation is given by:

:$dfrac {x x_0} {a^2} + dfrac {y y_0} {b^2} = 1$


=== Pole ===
Let $mathcal E$ be an ellipse embedded in the plane.

Let $P$ be an arbitrary point in the plane.


Let $mathcal L$ be the polar of $P$ with respect to $mathcal E$.

Then $P$ is known as the pole of $mathcal L$.",Definition:Polar of Point,,false,"Let 𝒦 be a conic section embedded in a Euclidean plane.

Let P be an arbitrary point in that plane.

Let a secant line pass through P and intersect 𝒦 at L and M.


The polar of P with respect to 𝒦 is the straight line upon which the tangents to 𝒦 intersect.


=== Circle ===
Let 𝒞 be a circle whose radius is r and whose center is at the origin of a Cartesian plane.

Let P = ( x_0, y_0 ) be an arbitrary point in the Cartesian plane.


The polar of P with respect to 𝒞 is the straight line whose equation is given by:

:x x_0 + y y_0 = r^2


=== Pole ===
Let 𝒞 be a circle embedded in the plane.

Let P be an arbitrary point in the plane.


Let ℒ be the polar of P with respect to 𝒞.

Then P is known as the pole of ℒ.

=== Ellipse ===
Let ℰ be an ellipse embedded in a Cartesian plane in reduced form with the equation:
:x^2a^2 + y^2b^2 = 1


Let P = ( x_0, y_0 ) be an arbitrary point in the Cartesian plane.


The polar of P with respect to ℰ is the straight line whose equation is given by:

:x x_0a^2 + y y_0b^2 = 1


=== Pole ===
Let ℰ be an ellipse embedded in the plane.

Let P be an arbitrary point in the plane.


Let ℒ be the polar of P with respect to ℰ.

Then P is known as the pole of ℒ.",Polar
['Definitions/Spherical Coordinates'],Definition:Polar,The polar axis of a spherical coordinate system is the vertical straight line which passes through the origin $O$.,Definition:Spherical Coordinate System/Polar Axis,,false,The polar axis of a spherical coordinate system is the vertical straight line which passes through the origin O.,Polar
['Definitions/Spherical Triangles'],Definition:Polar,"Let $triangle ABC$ be a spherical triangle on the surface of a sphere whose center is $O$.

Let the sides $a, b, c$ of $triangle ABC$ be measured by the angles subtended at $O$, where $a, b, c$ are opposite $A, B, C$ respectively.


Let $A'$, $B'$ and $C'$ be the poles of the sides $BC$, $AC$ and $AB$ respectively which are in the same hemisphere as the points $A$, $B$ and $C$ respectively.

:

Then the spherical triangle $triangle A'B'C'$ is the polar triangle of $triangle ABC$.",Definition:Polar Triangle,,false,"Let ABC be a spherical triangle on the surface of a sphere whose center is O.

Let the sides a, b, c of ABC be measured by the angles subtended at O, where a, b, c are opposite A, B, C respectively.


Let A', B' and C' be the poles of the sides BC, AC and AB respectively which are in the same hemisphere as the points A, B and C respectively.

:

Then the spherical triangle A'B'C' is the polar triangle of ABC.",Polar
"['Definitions/Vectors', 'Definitions/Mechanics']",Definition:Polar,A polar vector is a vector quantity whose action is along a line drawn in the direction of the vector itself.,Definition:Polar Vector,,false,A polar vector is a vector quantity whose action is along a line drawn in the direction of the vector itself.,Polar
['Definitions/Ring Theory'],Definition:Positive Definite,"Let $left( R, +, times right)$ be a ring whose zero is denoted $0_R$.

Let $f: R to mathbb R$ be a (real-valued) function on $R$.


Then $f$ is positive definite  if and only if :

:$forall x in R: begin {cases} f left(   right)x = 0 & : x = 0_R \ f left(   right)x > 0 & : x ne 0_R end {cases}$

Category:Definitions/Ring Theory",Definition:Positive Definite (Ring),,false,"Let ( R, +, ×) be a ring whose zero is denoted 0_R.

Let f: R →ℝ be a (real-valued) function on R.


Then f is positive definite  if and only if :

:∀ x ∈ R:  f (   )x = 0     : x = 0_R 
 f (   )x > 0     : x  0_R

Category:Definitions/Ring Theory",Positive Definite
"['Definitions/Positive Definite Matrices', 'Definitions/Matrix Algebra', 'Definitions/Matrix Theory']",Definition:Positive Definite,"Let $mathbf A$ be a symmetric square matrix of order $n$.

=== Definition 1 ===
Let $mathbf A$ be a symmetric square matrix of order $n$.

$mathbf A$ is positive definite  if and only if :
:for all nonzero column matrices $mathbf x$ of order $n$, $mathbf x^intercal mathbf A mathbf x$ is strictly positive.

=== Definition 2 ===
Let $mathbf A$ be a symmetric square matrix of order $n$.

$mathbf A$ is positive definite  if and only if :
:all the eigenvalues of $mathbf A$ are strictly positive.",Definition:Positive Definite Matrix,,false,"Let 𝐀 be a symmetric square matrix of order n.

=== Definition 1 ===
Let 𝐀 be a symmetric square matrix of order n.

𝐀 is positive definite  if and only if :
:for all nonzero column matrices 𝐱 of order n, 𝐱^⊺𝐀𝐱 is strictly positive.

=== Definition 2 ===
Let 𝐀 be a symmetric square matrix of order n.

𝐀 is positive definite  if and only if :
:all the eigenvalues of 𝐀 are strictly positive.",Positive Definite
"['Definitions/Powers', 'Definitions/Algebra', 'Definitions/Numbers', 'Definitions/Real Analysis', 'Definitions/Complex Analysis', 'Definitions/Involution']",Definition:Power,"=== Natural Numbers ===
Let $mathbb N$ denote the natural numbers.


For each $m in mathbb N$, recursively define $e_m: mathbb N to mathbb N$ to be the mapping:
:$e_m left({n}right) = begin{cases}
1 & : n = 0 \
m times e_m left({x}right) & : n = x + 1
end{cases}$
where:
: $+$ denotes natural number addition.
: $times$ denotes natural number multiplication.


$e_m left({n}right)$ is then expressed as a binary operation in the form:
:$m^n := e_m left({n}right)$

and is called $m$ to the power of $n$.

=== Integers ===
Let $x in mathbb R$ be a real number.

Let $n in mathbb Z$ be an integer.

The expression $x^n$ is called $x$ to the power of $n$.

$x^n$ is defined recursively as:


:$x^n = begin {cases} 1 & : n = 0 \ & \ x times x^{n - 1} & : n > 0 \ & \ dfrac {x^{n + 1} } x & : n < 0 end {cases}$

where $dfrac {x^{n + 1} } x$ denotes division.


=== Even Power ===
Let $x in mathbb R$ be a real number.

Let $n in mathbb Z$ be an even integer.


Then $x^n$ is called an even power of $x$.


Category:Definitions/Integer Powers
Category:Definitions/Even Integers

=== Odd Power ===
Let $x in mathbb R$ be a real number.

Let $n in mathbb Z$ be an odd integer.


Then $x^n$ is called an odd power of $x$


Category:Definitions/Integer Powers
Category:Definitions/Odd Integers

=== Rational Numbers ===
Let $x in mathbb R$ be a real number such that $x > 0$.

Let $m in mathbb Z$ be an integer.

Let $y = sqrt [m] x$ be the $m$th root of $x$.


Then we can write $y = x^{1/m}$ which means the same thing as $y = sqrt [m] x$.


Thus we can define the power to a positive rational number:

Let $r = dfrac p q in mathbb Q$ be a positive rational number where $p in mathbb Z_{ge 0}, q in mathbb Z_{> 0}$.

Then $x^r$ is defined as:
:$x^r = x^{p/q} = left( sqrt [q] x right)^p = sqrt [q] {left( x^p right) }$


When $r = dfrac {-p} q in mathbb Q: r < 0$ we define:
:$x^r = x^{-p/q} = dfrac 1 {x^{p/q} }$
analogously for the negative integer definition.

=== Real Numbers ===
Let $x in mathbb R_{>0}$ be a (strictly) positive real number.

Let $r in mathbb R$ be a real number.


We define $x^r$ as:

:$x^r := exp left(   right){r ln x}$
where $exp$ denotes the exponential function.


This definition is an extension of the definition for rational $r$.

This follows from Logarithms of Powers and Exponential of Natural Logarithm: it can be seen that:
:$forall r in mathbb Q: exp left(   right){r ln x} = exp left(   right){ln left(   right){x^r} } = x^r$

 

=== Complex Numbers ===
Let $z, k in mathbb C$ be complex numbers.


$z$ to the power of $k$ is defined as the multifunction:

:$z^k := e^{k ln left( z right)}$

where:
:$e^z$ is the exponential function
:$ln$ is the  natural logarithm multifunction.


=== Principal Branch ===
The principal branch of a complex number raised to a complex power is defined as:

:$z^k = e^{k operatorname {Ln} z}$

where $operatorname {Ln} z$ is the principal branch of the natural logarithm.


=== Positive Real Base ===
Let $t > 0$ be a real number and let $k$ be a complex number.


The principal branch of a positive real number raised to a complex power is defined as:

:$t^k = e^{k ln t}$

where $ln$ is the natural logarithm of a positive real number.


Category:Definitions/Complex Powers

Category:Definitions/Complex Powers

=== Multiindices ===
Let $k = left langle {k_j}right rangle_{j = 1, ldots, n}$ be a multiindex indexed by $leftlbrace 1, ldots, n rightrbrace$.

Let $x = left( x_1, ldots, x_n right) in mathbb R^n$ be an ordered tuple of real numbers.


Then $x^k$ is defined as:

:$ds x^k := prod_{j mathop = 1}^n x_j^{k_j}$
where the powers on the   are integer powers.


Category:Definitions/Analysis

=== Power of Zero ===
Let $r in mathbb R$ be a real number.

(This includes the situation where $r in mathbb Z$ or $r in mathbb Q$.)

When $x=0$, $x^r$ is defined as follows:

:$0^r = begin{cases}
1 & : r = 0 \
0 & : r > 0 \
text{Undefined} & : r < 0 \
end{cases}$

This takes account of the awkward case $0^0$: it is ""generally accepted"" that $0^0 = 1$ as this convention agrees with certain general results which would otherwise need a special case.",Definition:Power (Algebra),,false,"=== Natural Numbers ===
Let ℕ denote the natural numbers.


For each m ∈ℕ, recursively define e_m: ℕ→ℕ to be the mapping:
:e_m (n) = 
1     : n = 0 

m × e_m (x)     : n = x + 1
where:
: + denotes natural number addition.
: × denotes natural number multiplication.


e_m (n) is then expressed as a binary operation in the form:
:m^n := e_m (n)

and is called m to the power of n.

=== Integers ===
Let x ∈ℝ be a real number.

Let n ∈ℤ be an integer.

The expression x^n is called x to the power of n.

x^n is defined recursively as:


:x^n =  1     : n = 0 
   
 x × x^n - 1    : n > 0 
   
x^n + 1 x     : n < 0

where x^n + 1 x denotes division.


=== Even Power ===
Let x ∈ℝ be a real number.

Let n ∈ℤ be an even integer.


Then x^n is called an even power of x.


Category:Definitions/Integer Powers
Category:Definitions/Even Integers

=== Odd Power ===
Let x ∈ℝ be a real number.

Let n ∈ℤ be an odd integer.


Then x^n is called an odd power of x


Category:Definitions/Integer Powers
Category:Definitions/Odd Integers

=== Rational Numbers ===
Let x ∈ℝ be a real number such that x > 0.

Let m ∈ℤ be an integer.

Let y = √(x) be the mth root of x.


Then we can write y = x^1/m which means the same thing as y = √(x).


Thus we can define the power to a positive rational number:

Let r =  p q ∈ℚ be a positive rational number where p ∈ℤ_≥ 0, q ∈ℤ_> 0.

Then x^r is defined as:
:x^r = x^p/q = ( √(x))^p = √(( x^p ) )


When r = -p q ∈ℚ: r < 0 we define:
:x^r = x^-p/q =  1 x^p/q
analogously for the negative integer definition.

=== Real Numbers ===
Let x ∈ℝ_>0 be a (strictly) positive real number.

Let r ∈ℝ be a real number.


We define x^r as:

:x^r := exp(   )r ln x
where exp denotes the exponential function.


This definition is an extension of the definition for rational r.

This follows from Logarithms of Powers and Exponential of Natural Logarithm: it can be seen that:
:∀ r ∈ℚ: exp(   )r ln x = exp(   )ln(   )x^r = x^r

 

=== Complex Numbers ===
Let z, k ∈ℂ be complex numbers.


z to the power of k is defined as the multifunction:

:z^k := e^k ln( z )

where:
:e^z is the exponential function
:ln is the  natural logarithm multifunction.


=== Principal Branch ===
The principal branch of a complex number raised to a complex power is defined as:

:z^k = e^k Ln z

where Ln z is the principal branch of the natural logarithm.


=== Positive Real Base ===
Let t > 0 be a real number and let k be a complex number.


The principal branch of a positive real number raised to a complex power is defined as:

:t^k = e^k ln t

where ln is the natural logarithm of a positive real number.


Category:Definitions/Complex Powers

Category:Definitions/Complex Powers

=== Multiindices ===
Let k = ⟨k_j⟩_j = 1, …, n be a multiindex indexed by { 1, …, n }.

Let x = ( x_1, …, x_n ) ∈ℝ^n be an ordered tuple of real numbers.


Then x^k is defined as:

:x^k := ∏_j  = 1^n x_j^k_j
where the powers on the   are integer powers.


Category:Definitions/Analysis

=== Power of Zero ===
Let r ∈ℝ be a real number.

(This includes the situation where r ∈ℤ or r ∈ℚ.)

When x=0, x^r is defined as follows:

:0^r = 
1     : r = 0 

0     : r > 0 
Undefined    : r < 0

This takes account of the awkward case 0^0: it is ""generally accepted"" that 0^0 = 1 as this convention agrees with certain general results which would otherwise need a special case.",Power
"['Definitions/Abstract Algebra', 'Definitions/Powers (Abstract Algebra)']",Definition:Power,"=== Magma ===
Let $left( S, circ right)$ be a magma which has no identity element.

Let $a in S$.


Let the mapping $circ^n a: mathbb N_{>0} to S$ be recursively defined as:

:$forall n in mathbb N_{>0}: circ^n a = begin{cases}
a & : n = 1 \
left( circ^r a right) circ a & : n = r + 1
end{cases}$


The mapping $circ^n a$ is known as the $n$th power of $a$ (under $circ$).


=== Notation ===


=== Magma with Identity ===
Let $left( S, circ right)$ be a magma with an identity element $e$.

Let $a in S$.


Let the mapping $circ^n a: mathbb N to S$ be recursively defined as:

:$forall n in S: circ^n a = begin{cases}
e & : n = 0 \
left( circ^r a right) circ a & : n = r + 1
end{cases}$


The mapping $circ^n a$ is known as the $n$th power of $a$ (under $circ$).


=== Notation ===


Furthermore:
:$a^0 = circ^0 a = e$

=== Semigroup ===
Let $left( S, circ right)$ be a semigroup which has no identity element.

Let $a in S$.


For $n in mathbb N_{>0}$, the $n$th power of $a$ (under $circ$) is defined as:

:$circ^n a = begin{cases} a & : n = 1 \ left( circ^m a right) circ a & : n = m + 1 end{cases}$

That is:
:$a^n = underbrace {a circ a circ cdots circ a}_{n text{ copies of } a}$

which from the General Associativity Theorem is unambiguous.


=== Notation ===


=== Monoid ===
Let $left( S, circ right)$ be a monoid whose identity element is $e$.

Let $a in S$.

Let $n in mathbb N$.


The definition $a^n = circ^n left(   right)a$ as the $n$th power of $a$ in a semigroup can be extended to allow an exponent of $0$:

:$a^n = begin {cases}
e & : n = 0 \
a^{n - 1} circ a & : n > 0
end{cases}$

or:

:$n cdot a = begin {cases}
e & : n = 0 \
left( left( n - 1 right) cdot a right) circ a & : n > 0
end{cases}$


The validity of this definition follows from the fact that a monoid has an identity element.


=== Invertible Element ===
Let $left( S, circ right)$ be a monoid whose identity element is $e$.

Let $b in S$ be invertible for $circ$.

Let $n in mathbb Z$.


The definition $b^n = circ^n left(   right)b$ as the $n$th power of $b$ in $left({S, circ}right)$ can be extended to include the inverse of $b$:

:$b^{-n} = left( b^{-1}  right)^n$


Category:Definitions/Monoids

=== Group ===
Let $left( G, circ right)$ be a group whose identity element is $e$.

Let $g in G$.

Let $n in mathbb Z$.


The definition $g^n = circ^n left(   right)g$ as the $n$th power of $g$ in a monoid can be extended to allow negative values of $n$:

:$g^n = begin{cases}
e & : n = 0 \
g^{n - 1} circ g & : n > 0 \
left( g^{-n}  right)^{-1} & : n < 0
end{cases}$

or

:$n cdot g = begin{cases}
e & : n = 0 \
left( left( n - 1 right) cdot g right) circ g & : n > 0 \
-left( -n cdot g right) & : n < 0
end{cases}$

The validity of this definition follows from the group axioms: $g$ has an inverse element.

=== Ring ===
Let $left( R, +, circ right)$ be a ring.

Let $r in R$.

Let $n in mathbb Z_{>0}$ be the set of strictly positive integers.

The $n$th power of $r$ in $R$ is defined as the $n$th power of $r$ with respect to the semigroup $left( R, circ right)$:

:$forall n in mathbb Z_{>0}: r^n = begin {cases}
r & : n = 1 \
r^{n - 1} circ r & : n > 1 
end {cases}$


If $R$ is a ring with unity where $1_R$ is that unity, the definition extends to $n in mathbb Z_{ge 0}$:

:$forall n in mathbb Z_{ge 0}: r^n = begin {cases}
1_R & : n = 0 \
r^{n - 1} circ r & : n > 0
end {cases}$

=== Field ===
Let $left( F, +, circ right)$ be a field with zero $0_F$ and unity $1_F$.


Let $a in F^*$ where $F^*$ denotes the set of elements of $F$ without the zero $0_F$.

Let $n in mathbb Z$ be an integer.


The $n$th power of $a$ in $F$ is defined as the $n$th power of $a$ with respect to the Abelian group $left( F^*, circ right)$:
:$forall n in mathbb Z: a^n = begin {cases}
1_F & : n = 0 \
a^{n - 1} circ a & : n > 0 \
left( a^{-1} right)^{-n} & : n < 0
end {cases}$


The definition of $n$th power of $a$ in $F$ as the the $n$th power of $a$ with respect to the monoid $left( F, circ right)$ can be extended to $0_F$ for positive values of $n$.


For all $n in mathbb Z_{ge 0}$ the $n$th power of $0_F$ in $F$ is defined:
:$left( 0_F right)^n = begin {cases}
1_F & : n = 0 \
0_F & : n > 0
end {cases}$


It should be noted that for all $n < 0$ the $n$th power of $0_F$ is not defined.",Definition:Power of Element,,false,"=== Magma ===
Let ( S, ∘) be a magma which has no identity element.

Let a ∈ S.


Let the mapping ∘^n a: ℕ_>0→ S be recursively defined as:

:∀ n ∈ℕ_>0: ∘^n a = 
a     : n = 1 
( ∘^r a ) ∘ a     : n = r + 1


The mapping ∘^n a is known as the nth power of a (under ∘).


=== Notation ===


=== Magma with Identity ===
Let ( S, ∘) be a magma with an identity element e.

Let a ∈ S.


Let the mapping ∘^n a: ℕ→ S be recursively defined as:

:∀ n ∈ S: ∘^n a = 
e     : n = 0 
( ∘^r a ) ∘ a     : n = r + 1


The mapping ∘^n a is known as the nth power of a (under ∘).


=== Notation ===


Furthermore:
:a^0 = ∘^0 a = e

=== Semigroup ===
Let ( S, ∘) be a semigroup which has no identity element.

Let a ∈ S.


For n ∈ℕ_>0, the nth power of a (under ∘) is defined as:

:∘^n a =  a     : n = 1 
( ∘^m a ) ∘ a     : n = m + 1

That is:
:a^n = a ∘ a ∘⋯∘ a_n  copies of  a

which from the General Associativity Theorem is unambiguous.


=== Notation ===


=== Monoid ===
Let ( S, ∘) be a monoid whose identity element is e.

Let a ∈ S.

Let n ∈ℕ.


The definition a^n = ∘^n (   )a as the nth power of a in a semigroup can be extended to allow an exponent of 0:

:a^n = 
e     : n = 0 

a^n - 1∘ a     : n > 0

or:

:n · a = 
e     : n = 0 
( ( n - 1 ) · a ) ∘ a     : n > 0


The validity of this definition follows from the fact that a monoid has an identity element.


=== Invertible Element ===
Let ( S, ∘) be a monoid whose identity element is e.

Let b ∈ S be invertible for ∘.

Let n ∈ℤ.


The definition b^n = ∘^n (   )b as the nth power of b in (S, ∘) can be extended to include the inverse of b:

:b^-n = ( b^-1)^n


Category:Definitions/Monoids

=== Group ===
Let ( G, ∘) be a group whose identity element is e.

Let g ∈ G.

Let n ∈ℤ.


The definition g^n = ∘^n (   )g as the nth power of g in a monoid can be extended to allow negative values of n:

:g^n = 
e     : n = 0 

g^n - 1∘ g     : n > 0 
( g^-n)^-1    : n < 0

or

:n · g = 
e     : n = 0 
( ( n - 1 ) · g ) ∘ g     : n > 0 

-( -n · g )     : n < 0

The validity of this definition follows from the group axioms: g has an inverse element.

=== Ring ===
Let ( R, +, ∘) be a ring.

Let r ∈ R.

Let n ∈ℤ_>0 be the set of strictly positive integers.

The nth power of r in R is defined as the nth power of r with respect to the semigroup ( R, ∘):

:∀ n ∈ℤ_>0: r^n = 
r     : n = 1 

r^n - 1∘ r     : n > 1


If R is a ring with unity where 1_R is that unity, the definition extends to n ∈ℤ_≥ 0:

:∀ n ∈ℤ_≥ 0: r^n = 
1_R     : n = 0 

r^n - 1∘ r     : n > 0

=== Field ===
Let ( F, +, ∘) be a field with zero 0_F and unity 1_F.


Let a ∈ F^* where F^* denotes the set of elements of F without the zero 0_F.

Let n ∈ℤ be an integer.


The nth power of a in F is defined as the nth power of a with respect to the Abelian group ( F^*, ∘):
:∀ n ∈ℤ: a^n = 
1_F     : n = 0 

a^n - 1∘ a     : n > 0 
( a^-1)^-n    : n < 0


The definition of nth power of a in F as the the nth power of a with respect to the monoid ( F, ∘) can be extended to 0_F for positive values of n.


For all n ∈ℤ_≥ 0 the nth power of 0_F in F is defined:
:( 0_F )^n = 
1_F     : n = 0 

0_F     : n > 0


It should be noted that for all n < 0 the nth power of 0_F is not defined.",Power
"['Definitions/Magmas', 'Definitions/Powers (Abstract Algebra)']",Definition:Power,"Let $left( S, circ right)$ be a magma which has no identity element.

Let $a in S$.


Let the mapping $circ^n a: mathbb N_{>0} to S$ be recursively defined as:

:$forall n in mathbb N_{>0}: circ^n a = begin{cases}
a & : n = 1 \
left( circ^r a right) circ a & : n = r + 1
end{cases}$


The mapping $circ^n a$ is known as the $n$th power of $a$ (under $circ$).


=== Notation ===
",Definition:Power of Element/Magma,,false,"Let ( S, ∘) be a magma which has no identity element.

Let a ∈ S.


Let the mapping ∘^n a: ℕ_>0→ S be recursively defined as:

:∀ n ∈ℕ_>0: ∘^n a = 
a     : n = 1 
( ∘^r a ) ∘ a     : n = r + 1


The mapping ∘^n a is known as the nth power of a (under ∘).


=== Notation ===
",Power
"['Definitions/Magmas', 'Definitions/Powers (Abstract Algebra)']",Definition:Power,"Let $left( S, circ right)$ be a magma with an identity element $e$.

Let $a in S$.


Let the mapping $circ^n a: mathbb N to S$ be recursively defined as:

:$forall n in S: circ^n a = begin{cases}
e & : n = 0 \
left( circ^r a right) circ a & : n = r + 1
end{cases}$


The mapping $circ^n a$ is known as the $n$th power of $a$ (under $circ$).


=== Notation ===


Furthermore:
:$a^0 = circ^0 a = e$",Definition:Power of Element/Magma with Identity,,false,"Let ( S, ∘) be a magma with an identity element e.

Let a ∈ S.


Let the mapping ∘^n a: ℕ→ S be recursively defined as:

:∀ n ∈ S: ∘^n a = 
e     : n = 0 
( ∘^r a ) ∘ a     : n = r + 1


The mapping ∘^n a is known as the nth power of a (under ∘).


=== Notation ===


Furthermore:
:a^0 = ∘^0 a = e",Power
"['Definitions/Semigroups', 'Definitions/Powers (Abstract Algebra)']",Definition:Power,"Let $left( S, circ right)$ be a semigroup which has no identity element.

Let $a in S$.


For $n in mathbb N_{>0}$, the $n$th power of $a$ (under $circ$) is defined as:

:$circ^n a = begin{cases} a & : n = 1 \ left( circ^m a right) circ a & : n = m + 1 end{cases}$

That is:
:$a^n = underbrace {a circ a circ cdots circ a}_{n text{ copies of } a}$

which from the General Associativity Theorem is unambiguous.


=== Notation ===
",Definition:Power of Element/Semigroup,,false,"Let ( S, ∘) be a semigroup which has no identity element.

Let a ∈ S.


For n ∈ℕ_>0, the nth power of a (under ∘) is defined as:

:∘^n a =  a     : n = 1 
( ∘^m a ) ∘ a     : n = m + 1

That is:
:a^n = a ∘ a ∘⋯∘ a_n  copies of  a

which from the General Associativity Theorem is unambiguous.


=== Notation ===
",Power
['Definitions/Monoids'],Definition:Power,"Let $left( S, circ right)$ be a monoid whose identity element is $e$.

Let $a in S$.

Let $n in mathbb N$.


The definition $a^n = circ^n left(   right)a$ as the $n$th power of $a$ in a semigroup can be extended to allow an exponent of $0$:

:$a^n = begin {cases}
e & : n = 0 \
a^{n - 1} circ a & : n > 0
end{cases}$

or:

:$n cdot a = begin {cases}
e & : n = 0 \
left( left( n - 1 right) cdot a right) circ a & : n > 0
end{cases}$


The validity of this definition follows from the fact that a monoid has an identity element.


=== Invertible Element ===
Let $left( S, circ right)$ be a monoid whose identity element is $e$.

Let $b in S$ be invertible for $circ$.

Let $n in mathbb Z$.


The definition $b^n = circ^n left(   right)b$ as the $n$th power of $b$ in $left({S, circ}right)$ can be extended to include the inverse of $b$:

:$b^{-n} = left( b^{-1}  right)^n$


Category:Definitions/Monoids",Definition:Power of Element/Monoid,,false,"Let ( S, ∘) be a monoid whose identity element is e.

Let a ∈ S.

Let n ∈ℕ.


The definition a^n = ∘^n (   )a as the nth power of a in a semigroup can be extended to allow an exponent of 0:

:a^n = 
e     : n = 0 

a^n - 1∘ a     : n > 0

or:

:n · a = 
e     : n = 0 
( ( n - 1 ) · a ) ∘ a     : n > 0


The validity of this definition follows from the fact that a monoid has an identity element.


=== Invertible Element ===
Let ( S, ∘) be a monoid whose identity element is e.

Let b ∈ S be invertible for ∘.

Let n ∈ℤ.


The definition b^n = ∘^n (   )b as the nth power of b in (S, ∘) can be extended to include the inverse of b:

:b^-n = ( b^-1)^n


Category:Definitions/Monoids",Power
['Definitions/Group Theory'],Definition:Power,"Let $left( G, circ right)$ be a group whose identity element is $e$.

Let $g in G$.

Let $n in mathbb Z$.


The definition $g^n = circ^n left(   right)g$ as the $n$th power of $g$ in a monoid can be extended to allow negative values of $n$:

:$g^n = begin{cases}
e & : n = 0 \
g^{n - 1} circ g & : n > 0 \
left( g^{-n}  right)^{-1} & : n < 0
end{cases}$

or

:$n cdot g = begin{cases}
e & : n = 0 \
left( left( n - 1 right) cdot g right) circ g & : n > 0 \
-left( -n cdot g right) & : n < 0
end{cases}$

The validity of this definition follows from the group axioms: $g$ has an inverse element.",Definition:Power of Element/Group,,false,"Let ( G, ∘) be a group whose identity element is e.

Let g ∈ G.

Let n ∈ℤ.


The definition g^n = ∘^n (   )g as the nth power of g in a monoid can be extended to allow negative values of n:

:g^n = 
e     : n = 0 

g^n - 1∘ g     : n > 0 
( g^-n)^-1    : n < 0

or

:n · g = 
e     : n = 0 
( ( n - 1 ) · g ) ∘ g     : n > 0 

-( -n · g )     : n < 0

The validity of this definition follows from the group axioms: g has an inverse element.",Power
['Definitions/Ring Theory'],Definition:Power,"Let $left( R, +, circ right)$ be a ring.

Let $r in R$.

Let $n in mathbb Z_{>0}$ be the set of strictly positive integers.

The $n$th power of $r$ in $R$ is defined as the $n$th power of $r$ with respect to the semigroup $left( R, circ right)$:

:$forall n in mathbb Z_{>0}: r^n = begin {cases}
r & : n = 1 \
r^{n - 1} circ r & : n > 1 
end {cases}$


If $R$ is a ring with unity where $1_R$ is that unity, the definition extends to $n in mathbb Z_{ge 0}$:

:$forall n in mathbb Z_{ge 0}: r^n = begin {cases}
1_R & : n = 0 \
r^{n - 1} circ r & : n > 0
end {cases}$",Definition:Power of Element/Ring,,false,"Let ( R, +, ∘) be a ring.

Let r ∈ R.

Let n ∈ℤ_>0 be the set of strictly positive integers.

The nth power of r in R is defined as the nth power of r with respect to the semigroup ( R, ∘):

:∀ n ∈ℤ_>0: r^n = 
r     : n = 1 

r^n - 1∘ r     : n > 1


If R is a ring with unity where 1_R is that unity, the definition extends to n ∈ℤ_≥ 0:

:∀ n ∈ℤ_≥ 0: r^n = 
1_R     : n = 0 

r^n - 1∘ r     : n > 0",Power
['Definitions/B-Algebras'],Definition:Power,"Let $left( X, circ right)$ be a $B$-algebra.

For any $x in X$ and $n in mathbb N$, define the $n$th power of $x$, denoted $x^n$, inductively:

:$x^n = begin{cases}
0 & text {if $n = 0$} \
x^{n - 1} circ left( 0 circ x right) & text {if $n ge 1$}
end{cases}$",Definition:Power (B-Algebra),,false,"Let ( X, ∘) be a B-algebra.

For any x ∈ X and n ∈ℕ, define the nth power of x, denoted x^n, inductively:

:x^n = 
0    if n = 0

x^n - 1∘( 0 ∘ x )    if n ≥ 1",Power
"['Definitions/Set Theory', 'Definitions/Cardinality']",Definition:Power,"Two sets (either finite or infinite) which are equivalent are said to have the same cardinality.

The cardinality of a set $S$ is written $leftlvert S rightrvert$.


=== Cardinality of Finite Set ===
Let $S$ be a finite set.

The cardinality $leftlvert S rightrvert$ of $S$ is the number of elements in $S$.

That is, if:
:$S sim mathbb N_{< n}$

where:
:$sim$ denotes set equivalence
:$mathbb N_{

Also note that from the definition of finite:
:$exists n in mathbb N: leftlvert S rightrvert = n iff S$ is finite.

=== Cardinality of Infinite Set ===
Let $S$ be an infinite set.

The cardinality $leftlvert S rightrvert$ of $S$ can be indicated as:
:$leftlvert S rightrvert = infty$

However, it needs to be noted that this just means that the cardinality of $S$ cannot be assigned a number $n in mathbb N$.


It means that $leftlvert S rightrvert$ is at least $aleph_0$ (aleph null).",Definition:Cardinality,,false,"Two sets (either finite or infinite) which are equivalent are said to have the same cardinality.

The cardinality of a set S is written | S |.


=== Cardinality of Finite Set ===
Let S be a finite set.

The cardinality | S | of S is the number of elements in S.

That is, if:
:S ∼ℕ_< n

where:
:∼ denotes set equivalence
:ℕ_

Also note that from the definition of finite:
:∃ n ∈ℕ: | S | = n  S is finite.

=== Cardinality of Infinite Set ===
Let S be an infinite set.

The cardinality | S | of S can be indicated as:
:| S | = ∞

However, it needs to be noted that this just means that the cardinality of S cannot be assigned a number n ∈ℕ.


It means that | S | is at least ℵ_0 (aleph null).",Power
"['Definitions/Set Theory', 'Definitions/Power Set']",Definition:Power,"The power set of a set $S$ is the set defined and denoted as:

:$mathcal P left( S right) := leftlbrace T: T subseteq S rightrbrace$

That is, the set whose elements are all of the subsets of $S$.


=== Class Theory ===
The power set of a set $x$ is the class of all the subsets of $x$:

:$mathcal P left( x right) := leftlbrace y: y subseteq x rightrbrace$


It is clear from the definition that:
:$y in mathcal P left( x right) iff y subseteq x$


=== Axiom of Powers ===

The concept of the power set is axiomatised in the Axiom of Powers in class theory:

 ",Definition:Power Set,,false,"The power set of a set S is the set defined and denoted as:

:𝒫( S ) := { T: T ⊆ S }

That is, the set whose elements are all of the subsets of S.


=== Class Theory ===
The power set of a set x is the class of all the subsets of x:

:𝒫( x ) := { y: y ⊆ x }


It is clear from the definition that:
:y ∈𝒫( x )  y ⊆ x


=== Axiom of Powers ===

The concept of the power set is axiomatised in the Axiom of Powers in class theory:

 ",Power
"['Definitions/Power (Physics)', 'Definitions/Physics', 'Definitions/Mechanics']",Definition:Power,"Power the amount of energy transferred or converted per unit time.

That is, the rate at which energy is transformed.


Power is a scalar quantity.


=== Instantaneous ===
The instantaneous power transformed at an instant of time is defined as:
:$p left(   right)t = dfrac {mathrm d E left(   right)t} {mathrm d t}$
where:
:$p left(   right)t$ is the power as a function of time
:$E left(   right)t$ is the energy as a function of time.

=== Symbol ===


=== Dimension ===
The dimension of measurement of power is $mathsf {M L}^2 mathsf T^{-3}$.

This derives from its definition as:
:$dfrac {text{Energy} } {text {Time} }$


Category:Definitions/Dimensions of Measurement
Category:Definitions/Power (Physics)

=== Unit ===
The SI unit of power  is the watt $mathrm W$.


=== Watt ===
The watt is the SI unit of power.


It is defined as being the rate at which $1$ joule of energy is uniformly transformed in $1$ second:

:$1 , mathrm W = 1 , mathrm J , mathrm s^{-1}$


Hence it can be understood as:
:the rate at which work is done when the velocity of a body is held constant at $1$ metre per second against a constant opposing force of $1$ newton
:the rate at which work is performed when an electric current of $1$ ampere flows across an electrical potential difference of $1$ volt.

=== Conversion Factors ===


=== Symbol ===
 

=== Base Units ===
The SI base units of the watt are:

:$mathrm W := mathrm {kg} , mathrm m^2 mathrm s^{-3}$
where:
:$mathrm {kg}$ denotes kilograms
:$mathrm m$ denotes metres
:$mathrm s$ denotes seconds (of time).

This arises from the definition of the watt as $mathrm J , mathrm s^{-1}$, that is, joules per second.
 ",Definition:Power (Physics),,false,"Power the amount of energy transferred or converted per unit time.

That is, the rate at which energy is transformed.


Power is a scalar quantity.


=== Instantaneous ===
The instantaneous power transformed at an instant of time is defined as:
:p (   )t = d E (   )td t
where:
:p (   )t is the power as a function of time
:E (   )t is the energy as a function of time.

=== Symbol ===


=== Dimension ===
The dimension of measurement of power is 𝖬 𝖫^2 𝖳^-3.

This derives from its definition as:
:EnergyTime


Category:Definitions/Dimensions of Measurement
Category:Definitions/Power (Physics)

=== Unit ===
The SI unit of power  is the watt W.


=== Watt ===
The watt is the SI unit of power.


It is defined as being the rate at which 1 joule of energy is uniformly transformed in 1 second:

:1  W = 1  J s^-1


Hence it can be understood as:
:the rate at which work is done when the velocity of a body is held constant at 1 metre per second against a constant opposing force of 1 newton
:the rate at which work is performed when an electric current of 1 ampere flows across an electrical potential difference of 1 volt.

=== Conversion Factors ===


=== Symbol ===
 

=== Base Units ===
The SI base units of the watt are:

:W := kg m^2 s^-3
where:
:kg denotes kilograms
:m denotes metres
:s denotes seconds (of time).

This arises from the definition of the watt as J s^-1, that is, joules per second.
 ",Power
"['Definitions/Ring Theory', 'Definitions/Factorization', 'Definitions/Prime Elements of Rings']",Definition:Prime Element,"Let $R$ be a commutative ring.

Let $p in R setminus leftlbrace 0 rightrbrace$ be any non-zero element of $R$.


Then $p$ is a prime element of $R$  if and only if :
:$(1): quad p$ is not a unit of $R$ 
:$(2): quad$ whenever $a, b in R$ such that $p$ divides $a b$, then either $p$ divides $a$ or $p$ divides $b$.",Definition:Prime Element of Ring,,false,"Let R be a commutative ring.

Let p ∈ R ∖{ 0 } be any non-zero element of R.


Then p is a prime element of R  if and only if :
:(1):    p is not a unit of R 
:(2): whenever a, b ∈ R such that p divides a b, then either p divides a or p divides b.",Prime Element
['Definitions/Order Theory'],Definition:Prime Element,"Let $left( S, wedge, preceq right)$ be a meet semilattice.

Let $p in S$.


Then $p$ is a prime element (of $left( S, wedge, preceq right)$)  if and only if :
:$forall x, y in S: left( x wedge y preceq p implies x preceq p text { or } y preceq p right)$",Definition:Prime Element (Order Theory),,false,"Let ( S, ∧, ≼) be a meet semilattice.

Let p ∈ S.


Then p is a prime element (of ( S, ∧, ≼))  if and only if :
:∀ x, y ∈ S: ( x ∧ y ≼ p  x ≼ p  or  y ≼ p )",Prime Element
"['Definitions/Ideal Theory', 'Definitions/Prime Ideals of Rings']",Definition:Prime Ideal,"Let $R$ be a ring.


A prime ideal of $R$ is a proper ideal $P$ such that:
:$I circ J subseteq P implies I subseteq P text { or } J subseteq P$
for any ideals $I$ and $J$ of $R$.",Definition:Prime Ideal of Ring,,false,"Let R be a ring.


A prime ideal of R is a proper ideal P such that:
:I ∘ J ⊆ P  I ⊆ P  or  J ⊆ P
for any ideals I and J of R.",Prime Ideal
['Definitions/Order Theory'],Definition:Prime Ideal,"Let $I$ be an ideal in an ordered set $S$.


Then $I$ is a prime ideal in $S$  if and only if  $S setminus I$ is a filter.",Definition:Prime Ideal (Order Theory),,false,"Let I be an ideal in an ordered set S.


Then I is a prime ideal in S  if and only if  S ∖ I is a filter.",Prime Ideal
"['Definitions/Primitives', 'Definitions/Integral Calculus']",Definition:Primitive,"=== Primitive of Real Function ===
Let $F$ be a real function which is continuous on the closed interval $left[ a ,.,.,   right]b$ and differentiable on the open interval $left( a ,.,.,   right)b$.

Let $f$ be a real function which is continuous on the open interval $left( a ,.,.,   right)b$.


Let:
:$forall x in left( a ,.,.,   right)b: F' left(   right)x = f left(   right)x$
where $F'$ denotes the derivative of $F$   $x$.


Then $F$ is a primitive of $f$, and is denoted:
:$ds F = int f left(   right)x ,mathrm d x$

=== Primitive of Complex Function ===
Let $F: D to mathbb C$ be a complex function which is complex-differentiable on a connected domain $D$.

Let $f: D to mathbb C$ be a continuous complex function.


Let:
:$forall z in D: F' left(   right)z = f left(   right)z$
where $F'$ denotes the derivative of $F$   $z$.


Then $F$ is a primitive of $f$, and is denoted:
:$ds F = int f left(   right)z ,mathrm d z$

=== Primitive of Vector-Valued Function ===
Let $U subset mathbb R$ be an open set in $mathbb R$.

Let $mathbf f: U to mathbb R^n$ be a vector-valued function on $U$:

:$forall x in U: mathbf f left(   right)x = ds sum_{k mathop = 1}^n f_k left(   right)x mathbf e_k$

where:
:$f_1, f_2, ldots, f_n$ are real functions from $U$ to $mathbb R$
:$left( mathbf e_1, mathbf e_2, ldots, mathbf e_k right)$ denotes the standard ordered basis on $mathbb R^n$.

Let $mathbf f$ be differentiable on $U$.


Let $mathbf g left(   right)x := dfrac mathrm d {mathrm d x} mathbf f left(   right)x$ be the derivative of $mathbf f$   $x$.


The primitive of $mathbf g$   $x$ is defined as:

:$ds int mathbf g left(   right)x ,mathrm d x := mathbf f left(   right)x + mathbf c$

where $mathbf c$ is a arbitrary constant vector.",Definition:Primitive (Calculus),,false,"=== Primitive of Real Function ===
Let F be a real function which is continuous on the closed interval [ a  . . ]b and differentiable on the open interval ( a  . . )b.

Let f be a real function which is continuous on the open interval ( a  . . )b.


Let:
:∀ x ∈( a  . . )b: F' (   )x = f (   )x
where F' denotes the derivative of F   x.


Then F is a primitive of f, and is denoted:
:F = ∫ f (   )x  d x

=== Primitive of Complex Function ===
Let F: D →ℂ be a complex function which is complex-differentiable on a connected domain D.

Let f: D →ℂ be a continuous complex function.


Let:
:∀ z ∈ D: F' (   )z = f (   )z
where F' denotes the derivative of F   z.


Then F is a primitive of f, and is denoted:
:F = ∫ f (   )z  d z

=== Primitive of Vector-Valued Function ===
Let U ⊂ℝ be an open set in ℝ.

Let 𝐟: U →ℝ^n be a vector-valued function on U:

:∀ x ∈ U: 𝐟(   )x = ∑_k  = 1^n f_k (   )x 𝐞_k

where:
:f_1, f_2, …, f_n are real functions from U to ℝ
:( 𝐞_1, 𝐞_2, …, 𝐞_k ) denotes the standard ordered basis on ℝ^n.

Let 𝐟 be differentiable on U.


Let 𝐠(   )x := dd x𝐟(   )x be the derivative of 𝐟   x.


The primitive of 𝐠   x is defined as:

:∫𝐠(   )x  d x := 𝐟(   )x + 𝐜

where 𝐜 is a arbitrary constant vector.",Primitive
['Definitions/Polynomial Theory'],Definition:Primitive,"Let $mathbb Q left[ X right]$ be the ring of polynomial forms over the field of rational numbers in the indeterminate $X$.

Let $f in mathbb Q left[ X right]$ be such that:
:$mathrm {cont} left( f right) = 1$
where $mathrm {cont} left( f right)$ is the content of $f$.


That is:
:The greatest common divisor of the coefficients of $f$ is equal to $1$.
 


Then $f$ is described as primitive.",Definition:Primitive Polynomial (Ring Theory),,false,"Let ℚ[ X ] be the ring of polynomial forms over the field of rational numbers in the indeterminate X.

Let f ∈ℚ[ X ] be such that:
:cont( f ) = 1
where cont( f ) is the content of f.


That is:
:The greatest common divisor of the coefficients of f is equal to 1.
 


Then f is described as primitive.",Primitive
['Definitions/Pythagorean Triples'],Definition:Primitive,"Let $left( x, y, z right)$ be a Pythagorean triple such that $x perp y$ (that is, $x$ and $y$ are coprime).

Then $left( x, y, z right)$ is a primitive Pythagorean triple.


=== Canonical Form ===
Let $left( x, y, z right)$ be a primitive Pythagorean triple.


The convention for representing $left( x, y, z right)$ as a (primitive) Pythagorean triple is that $x$ is the even element, while $y$ and $z$ are both odd.

This is the canonical form of a Pythagorean triple.",Definition:Pythagorean Triple/Primitive,,false,"Let ( x, y, z ) be a Pythagorean triple such that x ⊥ y (that is, x and y are coprime).

Then ( x, y, z ) is a primitive Pythagorean triple.


=== Canonical Form ===
Let ( x, y, z ) be a primitive Pythagorean triple.


The convention for representing ( x, y, z ) as a (primitive) Pythagorean triple is that x is the even element, while y and z are both odd.

This is the canonical form of a Pythagorean triple.",Primitive
"['Definitions/Abundance', 'Definitions/Abundancy', 'Definitions/Abundant Numbers', 'Definitions/Primitive Abundant Numbers']",Definition:Primitive,"A primitive abundant number is an abundant number whose aliquot parts are all deficient.


=== Sequence of Primitive Abundant Numbers ===
The sequence of primitive abundant numbers begins:
:$20, 70, 88, 104, 272, 304, 368, 464, 550, 572, 650, 748, 836, 945, 1184, 1312, ldots$

 ",Definition:Primitive Abundant Number,,false,"A primitive abundant number is an abundant number whose aliquot parts are all deficient.


=== Sequence of Primitive Abundant Numbers ===
The sequence of primitive abundant numbers begins:
:20, 70, 88, 104, 272, 304, 368, 464, 550, 572, 650, 748, 836, 945, 1184, 1312, …

 ",Primitive
"['Definitions/Semiperfect Numbers', 'Definitions/Primitive Semiperfect Numbers']",Definition:Primitive,"A primitive semiperfect number is a semiperfect number which is not a multiple of a smaller semiperfect number.


=== Sequence of Primitive Semiperfect Numbers ===
The sequence of primitive semiperfect numbers begins:
:$6, 20, 28, 88, 104, 272, 304, 350, 368, 464, 490, 496, 550, 572, ldots$

 ",Definition:Primitive Semiperfect Number,,false,"A primitive semiperfect number is a semiperfect number which is not a multiple of a smaller semiperfect number.


=== Sequence of Primitive Semiperfect Numbers ===
The sequence of primitive semiperfect numbers begins:
:6, 20, 28, 88, 104, 272, 304, 350, 368, 464, 490, 496, 550, 572, …

 ",Primitive
['Definitions/Recursion Theory'],Definition:Primitive,"=== Primitive Recursion on Several Variables ===
Let $f: mathbb N^k to mathbb N$ and $g: mathbb N^{k + 2} to mathbb N$ be functions.

Let $left( n_1, n_2, ldots, n_k right) in mathbb N^k$.

Then the function $h: mathbb N^{k + 1} to mathbb N$ is obtained from $f$ and $g$ by primitive recursion  if and only if :
:$forall n in mathbb N: h left(   right){n_1, n_2, ldots, n_k, n} = begin {cases}
f left(   right){n_1, n_2, ldots, n_k} & : n = 0 \
g left(   right){n_1, n_2, ldots, n_k, n - 1, h left(   right){n_1, n_2, ldots, n_k, n - 1} } & : n > 0 
end {cases}$


Category:Definitions/Recursion Theory

=== Primitive Recursion on One Variable ===
Let $a in mathbb N$ be a natural number.

Let $g: mathbb N^2 to mathbb N$ be a function.

Then the function $h: mathbb N to mathbb N$ is obtained from the constant $a$ and $g$ by primitive recursion  if and only if :
:$forall n in mathbb N: h left(   right)n = begin {cases}
a & : n = 0 \
g left(   right){n - 1, h left(   right){n - 1} } & : n > 0 
end{cases}$

=== Primitive Recursion on Partial Functions ===
Let $f: mathbb N^k to mathbb N$ and $g: mathbb N^{k+2} to mathbb N$ be partial functions.

Let $left( n_1, n_2, ldots, n_k right) in mathbb N^k$.

Then the partial function $h: mathbb N^{k + 1} to mathbb N$ is obtained from $f$ and $g$ by primitive recursion  if and only if :
:$forall n in mathbb N: h left(   right){n_1, n_2, ldots, n_k, n} approx begin {cases}
f left(   right){n_1, n_2, ldots, n_k} & : n = 0 \
g left(   right){n_1, n_2, ldots, n_k, n - 1, h left(   right){n_1, n_2, ldots, n_k, n - 1} } & : n > 0 
end{cases}$

where $approx$ is as defined in Partial Function Equality.


Note that $h left(   right){n_1, n_2, ldots, n_k, n}$ is defined only when:
:$h left(   right){n_1, n_2, ldots, n_k, n - 1}$ is defined
:$g left(   right){n_1, n_2, ldots, n_k, n - 1, h left(   right){n_1, n_2, ldots, n_k, n - 1} }$ is defined.


Category:Definitions/Recursion Theory

Category:Definitions/Recursion Theory",Definition:Primitive Recursion,,false,"=== Primitive Recursion on Several Variables ===
Let f: ℕ^k →ℕ and g: ℕ^k + 2→ℕ be functions.

Let ( n_1, n_2, …, n_k ) ∈ℕ^k.

Then the function h: ℕ^k + 1→ℕ is obtained from f and g by primitive recursion  if and only if :
:∀ n ∈ℕ: h (   )n_1, n_2, …, n_k, n = 
f (   )n_1, n_2, …, n_k    : n = 0 

g (   )n_1, n_2, …, n_k, n - 1, h (   )n_1, n_2, …, n_k, n - 1    : n > 0


Category:Definitions/Recursion Theory

=== Primitive Recursion on One Variable ===
Let a ∈ℕ be a natural number.

Let g: ℕ^2 →ℕ be a function.

Then the function h: ℕ→ℕ is obtained from the constant a and g by primitive recursion  if and only if :
:∀ n ∈ℕ: h (   )n = 
a     : n = 0 

g (   )n - 1, h (   )n - 1    : n > 0

=== Primitive Recursion on Partial Functions ===
Let f: ℕ^k →ℕ and g: ℕ^k+2→ℕ be partial functions.

Let ( n_1, n_2, …, n_k ) ∈ℕ^k.

Then the partial function h: ℕ^k + 1→ℕ is obtained from f and g by primitive recursion  if and only if :
:∀ n ∈ℕ: h (   )n_1, n_2, …, n_k, n≈
f (   )n_1, n_2, …, n_k    : n = 0 

g (   )n_1, n_2, …, n_k, n - 1, h (   )n_1, n_2, …, n_k, n - 1    : n > 0

where ≈ is as defined in Partial Function Equality.


Note that h (   )n_1, n_2, …, n_k, n is defined only when:
:h (   )n_1, n_2, …, n_k, n - 1 is defined
:g (   )n_1, n_2, …, n_k, n - 1, h (   )n_1, n_2, …, n_k, n - 1 is defined.


Category:Definitions/Recursion Theory

Category:Definitions/Recursion Theory",Primitive
['Definitions/Recursion Theory'],Definition:Primitive,"=== Function ===
A function is primitive recursive  if and only if  it can be obtained from basic primitive recursive functions using the operations of substitution and primitive recursion a finite number of times.


Category:Definitions/Recursion Theory
Category:Definitions/Primitive Recursive Functions

=== Set ===
Let $A subseteq mathbb N$.


Then $A$ is a primitive recursive set  if and only if  its characteristic function $chi_A$ is a primitive recursive function.


Category:Definitions/Recursion Theory

=== Relation ===
Let $mathcal R subseteq mathbb N^k$ be an $n$-ary relation on $mathbb N^k$.


Then $mathcal R$ is a primitive recursive relation  if and only if  its characteristic function $chi_mathcal R$ is a primitive recursive function.


Category:Definitions/Recursion Theory

Category:Definitions/Recursion Theory",Definition:Primitive Recursive,,false,"=== Function ===
A function is primitive recursive  if and only if  it can be obtained from basic primitive recursive functions using the operations of substitution and primitive recursion a finite number of times.


Category:Definitions/Recursion Theory
Category:Definitions/Primitive Recursive Functions

=== Set ===
Let A ⊆ℕ.


Then A is a primitive recursive set  if and only if  its characteristic function χ_A is a primitive recursive function.


Category:Definitions/Recursion Theory

=== Relation ===
Let ℛ⊆ℕ^k be an n-ary relation on ℕ^k.


Then ℛ is a primitive recursive relation  if and only if  its characteristic function χ_ℛ is a primitive recursive function.


Category:Definitions/Recursion Theory

Category:Definitions/Recursion Theory",Primitive
"['Definitions/Logic', 'Definitions/Definitions']",Definition:Primitive,"For a definition to not be circular, the definer must use already defined terms. 

However, this process cannot go on indefinitely. If we were to insist on everything being defined only using previously defined terms, we would enter an infinite regress.


Concepts that are not defined in terms of previously defined concepts are called undefined terms.

An undefined term is frequently explained by using an ostensive definition: that is, a statement that shows what something is, rather than explains.",Definition:Undefined Term,,false,"For a definition to not be circular, the definer must use already defined terms. 

However, this process cannot go on indefinitely. If we were to insist on everything being defined only using previously defined terms, we would enter an infinite regress.


Concepts that are not defined in terms of previously defined concepts are called undefined terms.

An undefined term is frequently explained by using an ostensive definition: that is, a statement that shows what something is, rather than explains.",Primitive
['Definitions/Field Extensions'],Definition:Primitive,"Let $F / K$ be a simple field extension such that $F = K left(   right)alpha$.


Then $alpha$ is a primitive element of $F$.",Definition:Primitive Element of Field Extension,,false,"Let F / K be a simple field extension such that F = K (   )α.


Then α is a primitive element of F.",Primitive
['Definitions/Number Theory'],Definition:Primitive Root,"Let $a, n in mathbb Z_{>0}$, that is, let $a$ and $n$ be strictly positive integers.

Let the multiplicative order of $a$ modulo $n$ be $phi left(   right)n$, where $phi left(   right)n$ is the Euler phi function of $n$.


Then $a$ is a primitive root of $n$ or a primitive root modulo $n$.",Definition:Primitive Root (Number Theory),,false,"Let a, n ∈ℤ_>0, that is, let a and n be strictly positive integers.

Let the multiplicative order of a modulo n be ϕ(   )n, where ϕ(   )n is the Euler phi function of n.


Then a is a primitive root of n or a primitive root modulo n.",Primitive Root
['Definitions/Roots of Unity'],Definition:Primitive Root,"Let $n in mathbb Z_{> 0}$ be a strictly positive integer.

Let $F$ be a field.

Let $U_n$ denote the set of all $n$-th roots of unity.


=== Definition 1 ===
Let $n in mathbb Z_{> 0}$ be a strictly positive integer.

Let $F$ be a field.

Let $U_n$ denote the set of all $n$-th roots of unity.


A primitive $n$th root of unity of $F$ is an element $alpha in U_n$ such that:

:$U_n = leftlbrace 1, alpha, ldots, alpha^{n - 1}  rightrbrace$

=== Definition 2 ===
Let $n in mathbb Z_{> 0}$ be a strictly positive integer.

Let $F$ be a field.

Let $U_n$ denote the set of all $n$-th roots of unity.


A primitive $n$th root of unity of $F$ is an element $alpha in U_n$ such that:
:$forall m : 0 < m < n : alpha^m ne 1$",Definition:Root of Unity/Primitive,,false,"Let n ∈ℤ_> 0 be a strictly positive integer.

Let F be a field.

Let U_n denote the set of all n-th roots of unity.


=== Definition 1 ===
Let n ∈ℤ_> 0 be a strictly positive integer.

Let F be a field.

Let U_n denote the set of all n-th roots of unity.


A primitive nth root of unity of F is an element α∈ U_n such that:

:U_n = { 1, α, …, α^n - 1}

=== Definition 2 ===
Let n ∈ℤ_> 0 be a strictly positive integer.

Let F be a field.

Let U_n denote the set of all n-th roots of unity.


A primitive nth root of unity of F is an element α∈ U_n such that:
:∀ m : 0 < m < n : α^m  1",Primitive Root
"['Definitions/Main Connective', 'Definitions/Propositional Logic', 'Definitions/Logic']",Definition:Principal,"In a compound statement, exactly one of its logical connectives has the largest scope.

That connective is called the main connective.

The scope of the main connective comprises the entire compound statement.


 ",Definition:Main Connective,,false,"In a compound statement, exactly one of its logical connectives has the largest scope.

That connective is called the main connective.

The scope of the main connective comprises the entire compound statement.


 ",Principal
"['Definitions/Principal Ideals of Preordered Sets', 'Definitions/Preorder Theory']",Definition:Principal,"Let $left( S, preceq right)$ be a preordered set.

Let $I$ be an ideal in $S$.

=== Definition 1 ===
Let $left( S, preceq right)$ be a preordered set.

Let $I$ be an ideal in $S$.


Then $I$ is a principal ideal  if and only if :
:$exists x in I: x$ is upper bound for $I$

=== Definition 2 ===
Let $left( S, preceq right)$ be a preordered set.

Let $I$ be an ideal in $S$.


Then $I$ is a principal ideal  if and only if :
:$exists x in S: I = x^preceq$
where $x^preceq$ denotes the lower closure of $x$.",Definition:Principal Ideal of Preordered Set,,false,"Let ( S, ≼) be a preordered set.

Let I be an ideal in S.

=== Definition 1 ===
Let ( S, ≼) be a preordered set.

Let I be an ideal in S.


Then I is a principal ideal  if and only if :
:∃ x ∈ I: x is upper bound for I

=== Definition 2 ===
Let ( S, ≼) be a preordered set.

Let I be an ideal in S.


Then I is a principal ideal  if and only if :
:∃ x ∈ S: I = x^≼
where x^≼ denotes the lower closure of x.",Principal
"['Definitions/Main Diagonal', 'Definitions/Matrix Diagonals', 'Definitions/Matrices']",Definition:Principal,"Let $mathbf A = left[ a right]_{m n}$ be a matrix.

The elements $a_{j j}: j in left[ 1 ,.,.,   right]{min leftlbrace m, n rightrbrace }$ constitute the main diagonal of $mathbf A$.

That is, the main diagonal of $mathbf A$ is the diagonal of $mathbf A$ from the top left corner, that is, the element $a_{1 1}$, running towards the lower right corner.


=== Diagonal Elements ===
The elements of the main diagonal of a matrix or a determinant are called the diagonal elements.",Definition:Matrix/Diagonal/Main,,false,"Let 𝐀 = [ a ]_m n be a matrix.

The elements a_j j: j ∈[ 1  . . ]min{ m, n } constitute the main diagonal of 𝐀.

That is, the main diagonal of 𝐀 is the diagonal of 𝐀 from the top left corner, that is, the element a_1 1, running towards the lower right corner.


=== Diagonal Elements ===
The elements of the main diagonal of a matrix or a determinant are called the diagonal elements.",Principal
['Definitions/Multifunctions'],Definition:Principal,"Let $A$ and $B$ be sets.

Let $f: A to B$ be a multifunction on $A$.

Let $leftlangle S_i rightrangle_{i mathop in I}$ be a partitioning of the codomain of $f$ into branches.


It is usual to distinguish one such branch of $f$ from the others, and label it the principal branch of $f$.


=== Principal Value ===
Let $A$ and $B$ be sets.

Let $f: A to B$ be a multifunction on $A$.

Let $x in A$ be an element of the domain of $f$.

The principal value of $x$ is the element $y$ of the principal branch of $f$ such that $f left(   right)x = y$.",Definition:Multifunction/Principal Branch,,false,"Let A and B be sets.

Let f: A → B be a multifunction on A.

Let ⟨ S_i ⟩_i ∈ I be a partitioning of the codomain of f into branches.


It is usual to distinguish one such branch of f from the others, and label it the principal branch of f.


=== Principal Value ===
Let A and B be sets.

Let f: A → B be a multifunction on A.

Let x ∈ A be an element of the domain of f.

The principal value of x is the element y of the principal branch of f such that f (   )x = y.",Principal
['Definitions/Argument of Complex Number'],Definition:Principal,"It is understood that the argument of a complex number $z$ is unique only up to multiples of $2 k pi$.

With this understanding, we can limit the choice of what $theta$ can be for any given $z$ by requiring that $theta$ lie in some half open interval of length $2 pi$.

The most usual of these are:
:$left[ 0 ,.,.,   right){2 pi}$
:$left( -pi ,.,.,   right]pi$

but in theory any such interval may be used.

This interval is known as the principal range.",Definition:Argument of Complex Number/Principal Range,,false,"It is understood that the argument of a complex number z is unique only up to multiples of 2 k π.

With this understanding, we can limit the choice of what θ can be for any given z by requiring that θ lie in some half open interval of length 2 π.

The most usual of these are:
:[ 0  . . )2 π
:( -π . . ]π

but in theory any such interval may be used.

This interval is known as the principal range.",Principal
['Definitions/Argument of Complex Number'],Definition:Principal,"Let $R$ be the principal range of the complex numbers $mathbb C$.

The unique value of $theta$ in $R$ is known as the principal argument, of $z$.

This is denoted $mathrm {Arg} left( z right)$.

Note the capital $A$.

The standard practice is for $R$ to be $left( -pi ,.,.,   right]pi$.

This ensures that the principal argument is continuous on the real axis for positive numbers.

Thus, if $z$ is represented in the complex plane, the principal argument $mathrm {Arg} left( z right)$ is intuitively defined as the angle which $z$ yields with the real ($y = 0$) axis.


 ",Definition:Argument of Complex Number/Principal Argument,,false,"Let R be the principal range of the complex numbers ℂ.

The unique value of θ in R is known as the principal argument, of z.

This is denoted Arg( z ).

Note the capital A.

The standard practice is for R to be ( -π . . ]π.

This ensures that the principal argument is continuous on the real axis for positive numbers.

Thus, if z is represented in the complex plane, the principal argument Arg( z ) is intuitively defined as the angle which z yields with the real (y = 0) axis.


 ",Principal
['Definitions/Complex Square Roots'],Definition:Principal,"Let $z in mathbb C$ be a complex number.

Let $z^{1/2} = leftlbrace w in mathbb C: w^2 = z rightrbrace$ be the square root of $z$.


The principal square root of $z$ is the principal branch of the $2$nd power of $w$.


Hence, by the conventional definition of the principal branch of the natural logarithm of $z$, it is the element $w$ of $z^{1/2}$ such that:
:$-dfrac pi 2 < arg w le dfrac pi 2$",Definition:Square Root/Complex Number/Principal Square Root,,false,"Let z ∈ℂ be a complex number.

Let z^1/2 = { w ∈ℂ: w^2 = z } be the square root of z.


The principal square root of z is the principal branch of the 2nd power of w.


Hence, by the conventional definition of the principal branch of the natural logarithm of z, it is the element w of z^1/2 such that:
:-π 2 <  w ≤π 2",Principal
['Definitions/Complex Powers'],Definition:Principal,"The principal branch of a complex number raised to a complex power is defined as:

:$z^k = e^{k operatorname {Ln} z}$

where $operatorname {Ln} z$ is the principal branch of the natural logarithm.


=== Positive Real Base ===
Let $t > 0$ be a real number and let $k$ be a complex number.


The principal branch of a positive real number raised to a complex power is defined as:

:$t^k = e^{k ln t}$

where $ln$ is the natural logarithm of a positive real number.


Category:Definitions/Complex Powers

Category:Definitions/Complex Powers",Definition:Power (Algebra)/Complex Number/Principal Branch,,false,"The principal branch of a complex number raised to a complex power is defined as:

:z^k = e^k Ln z

where Ln z is the principal branch of the natural logarithm.


=== Positive Real Base ===
Let t > 0 be a real number and let k be a complex number.


The principal branch of a positive real number raised to a complex power is defined as:

:t^k = e^k ln t

where ln is the natural logarithm of a positive real number.


Category:Definitions/Complex Powers

Category:Definitions/Complex Powers",Principal
"['Definitions/Integer Division', 'Definitions/Integers', 'Definitions/Number Theory', 'Definitions/Discrete Mathematics']",Definition:Principal,"Let $a, b in mathbb Z$ be integers such that $b ne 0$.

From the Division Theorem, we have that:

:$forall a, b in mathbb Z, b ne 0: exists_1 q, r in mathbb Z: a = q b + r, 0 le r < leftlvert b rightrvert$


The value $r$ is defined as the remainder of $a$ on division by $b$, or the remainder of $dfrac a b$'''.


=== Real Arguments ===

When $x, y in mathbb R$ the remainder is still defined:

Let $x, y in mathbb R$ be real numbers such that $y ne 0$.

The remainder of $x$ on division by $y$ is defined as the value of $r$ in the expression:

:$forall x, y in mathbb R, y ne 0: exists! q in mathbb Z, r in mathbb R: x = q y + r, 0 le r < leftlvert y rightrvert$


From the definition of the Modulo Operation:

:$x bmod y := x - y leftlfloor dfrac x y rightrfloor$

it can be seen that the remainder of $x$ on division by $y$ is defined as:
:$r = x bmod y$",Definition:Remainder,,false,"Let a, b ∈ℤ be integers such that b  0.

From the Division Theorem, we have that:

:∀ a, b ∈ℤ, b  0: ∃_1 q, r ∈ℤ: a = q b + r, 0 ≤ r < | b |


The value r is defined as the remainder of a on division by b, or the remainder of a b”'.


=== Real Arguments ===

When x, y ∈ℝ the remainder is still defined:

Let x, y ∈ℝ be real numbers such that y  0.

The remainder of x on division by y is defined as the value of r in the expression:

:∀ x, y ∈ℝ, y  0: ∃! q ∈ℤ, r ∈ℝ: x = q y + r, 0 ≤ r < | y |


From the definition of the Modulo Operation:

:x  y := x - y ⌊ x y ⌋

it can be seen that the remainder of x on division by y is defined as:
:r = x  y",Principal
['Definitions/Analytic Number Theory'],Definition:Principal,"Let $G$ be a finite abelian group.

The character $chi_0: G to mathbb C_{ne 0}$ defined as:

:$forall g in G: chi_0 left(   right)g = 1$

is the trivial character on $G$.",Definition:Trivial Character,,false,"Let G be a finite abelian group.

The character χ_0: G →ℂ_ 0 defined as:

:∀ g ∈ G: χ_0 (   )g = 1

is the trivial character on G.",Principal
"['Definitions/Principal Ideals of Rings', 'Definitions/Ideal Theory', 'Definitions/Commutative Algebra', 'Definitions/Integral Domains']",Definition:Principal,"Let $left( R, +, circ right)$ be a ring with unity.

Let $a in R$.


We define:

=== Definition 1 ===
Let $left( R, +, circ right)$ be a ring with unity.

Let $a in R$.

We define:

:$left( a right) = ds leftlbrace sum_{i mathop = 1}^n r_i circ a circ s_i: n in mathbb N, r_i, s_i in R rightrbrace$


The ideal $left( a right)$ is called the principal ideal of $R$ generated by $a$.

=== Definition 2 ===
Let $left( R, +, circ right)$ be a ring with unity.

Let $a in R$.

We define:

:$left( a right)$ is the smallest ideal of $left( R, +, circ right)$ containing $a$ as an element.


The ideal $left( a right)$ is called the principal ideal of $R$ generated by $a$.

=== Definition 3 ===
Let $left( R, +, circ right)$ be a ring with unity.

Let $a in R$.

We define:

:$left( a right)$ is the intersection of all ideals of $left( R, +, circ right)$ which contain $a$ as an element.


The ideal $left( a right)$ is called the principal ideal of $R$ generated by $a$.

=== Definition 4 ===
Let $left( R, +, circ right)$ be a ring with unity.

Let $a in R$.

We define:

:$left( a right)$ is an ideal of $left( R, +, circ right)$ such that every element of $left( a right)$ is of the form $a circ r$, where $r in R$


The ideal $left( a right)$ is called the principal ideal of $R$ generated by $a$.

The ideal $left( a right)$ is called the principal ideal of $R$ generated by $a$.",Definition:Principal Ideal of Ring,,false,"Let ( R, +, ∘) be a ring with unity.

Let a ∈ R.


We define:

=== Definition 1 ===
Let ( R, +, ∘) be a ring with unity.

Let a ∈ R.

We define:

:( a ) = {∑_i  = 1^n r_i ∘ a ∘ s_i: n ∈ℕ, r_i, s_i ∈ R }


The ideal ( a ) is called the principal ideal of R generated by a.

=== Definition 2 ===
Let ( R, +, ∘) be a ring with unity.

Let a ∈ R.

We define:

:( a ) is the smallest ideal of ( R, +, ∘) containing a as an element.


The ideal ( a ) is called the principal ideal of R generated by a.

=== Definition 3 ===
Let ( R, +, ∘) be a ring with unity.

Let a ∈ R.

We define:

:( a ) is the intersection of all ideals of ( R, +, ∘) which contain a as an element.


The ideal ( a ) is called the principal ideal of R generated by a.

=== Definition 4 ===
Let ( R, +, ∘) be a ring with unity.

Let a ∈ R.

We define:

:( a ) is an ideal of ( R, +, ∘) such that every element of ( a ) is of the form a ∘ r, where r ∈ R


The ideal ( a ) is called the principal ideal of R generated by a.

The ideal ( a ) is called the principal ideal of R generated by a.",Principal
"['Definitions/Principal Ideal Domains', 'Definitions/Ideal Theory', 'Definitions/Integral Domains']",Definition:Principal,A principal ideal domain is an integral domain in which every ideal is a principal ideal.,Definition:Principal Ideal Domain,,false,A principal ideal domain is an integral domain in which every ideal is a principal ideal.,Principal
['Definitions/Filter Theory'],Definition:Principal,"Let $S$ be a set.

Let $mathcal P left( S right)$ denote the power set of $S$.

Let $mathcal F subset mathcal P left( S right)$ be an ultrafilter on $S$ with a cluster point.


Then $mathcal F$ is a principal ultrafilter on $S$.


=== Nonprincipal Ultrafilter ===
Let $S$ be a set.

Let $mathcal P left( S right)$ denote the power set of $S$.


Let $mathcal F subset mathcal P left( S right)$ be an ultrafilter on $S$ which does not have a cluster point.

Then $mathcal F$ is a nonprincipal ultrafilter  on $S$.",Definition:Principal Ultrafilter,,false,"Let S be a set.

Let 𝒫( S ) denote the power set of S.

Let ℱ⊂𝒫( S ) be an ultrafilter on S with a cluster point.


Then ℱ is a principal ultrafilter on S.


=== Nonprincipal Ultrafilter ===
Let S be a set.

Let 𝒫( S ) denote the power set of S.


Let ℱ⊂𝒫( S ) be an ultrafilter on S which does not have a cluster point.

Then ℱ is a nonprincipal ultrafilter  on S.",Principal
"['Definitions/Principal (Economics)', 'Definitions/Economics']",Definition:Principal,Principal is defined as a quantity of money that is either borrowed or invested.,Definition:Principal (Economics),quantity,true,Principal is defined as a quantity of money that is either borrowed or invested.,Principal
"['Definitions/Principal Ideals of Rings', 'Definitions/Ideal Theory', 'Definitions/Commutative Algebra', 'Definitions/Integral Domains']",Definition:Principal Ideal,"Let $left( R, +, circ right)$ be a ring with unity.

Let $a in R$.


We define:

=== Definition 1 ===
Let $left( R, +, circ right)$ be a ring with unity.

Let $a in R$.

We define:

:$left( a right) = ds leftlbrace sum_{i mathop = 1}^n r_i circ a circ s_i: n in mathbb N, r_i, s_i in R rightrbrace$


The ideal $left( a right)$ is called the principal ideal of $R$ generated by $a$.

=== Definition 2 ===
Let $left( R, +, circ right)$ be a ring with unity.

Let $a in R$.

We define:

:$left( a right)$ is the smallest ideal of $left( R, +, circ right)$ containing $a$ as an element.


The ideal $left( a right)$ is called the principal ideal of $R$ generated by $a$.

=== Definition 3 ===
Let $left( R, +, circ right)$ be a ring with unity.

Let $a in R$.

We define:

:$left( a right)$ is the intersection of all ideals of $left( R, +, circ right)$ which contain $a$ as an element.


The ideal $left( a right)$ is called the principal ideal of $R$ generated by $a$.

=== Definition 4 ===
Let $left( R, +, circ right)$ be a ring with unity.

Let $a in R$.

We define:

:$left( a right)$ is an ideal of $left( R, +, circ right)$ such that every element of $left( a right)$ is of the form $a circ r$, where $r in R$


The ideal $left( a right)$ is called the principal ideal of $R$ generated by $a$.

The ideal $left( a right)$ is called the principal ideal of $R$ generated by $a$.",Definition:Principal Ideal of Ring,,false,"Let ( R, +, ∘) be a ring with unity.

Let a ∈ R.


We define:

=== Definition 1 ===
Let ( R, +, ∘) be a ring with unity.

Let a ∈ R.

We define:

:( a ) = {∑_i  = 1^n r_i ∘ a ∘ s_i: n ∈ℕ, r_i, s_i ∈ R }


The ideal ( a ) is called the principal ideal of R generated by a.

=== Definition 2 ===
Let ( R, +, ∘) be a ring with unity.

Let a ∈ R.

We define:

:( a ) is the smallest ideal of ( R, +, ∘) containing a as an element.


The ideal ( a ) is called the principal ideal of R generated by a.

=== Definition 3 ===
Let ( R, +, ∘) be a ring with unity.

Let a ∈ R.

We define:

:( a ) is the intersection of all ideals of ( R, +, ∘) which contain a as an element.


The ideal ( a ) is called the principal ideal of R generated by a.

=== Definition 4 ===
Let ( R, +, ∘) be a ring with unity.

Let a ∈ R.

We define:

:( a ) is an ideal of ( R, +, ∘) such that every element of ( a ) is of the form a ∘ r, where r ∈ R


The ideal ( a ) is called the principal ideal of R generated by a.

The ideal ( a ) is called the principal ideal of R generated by a.",Principal Ideal
"['Definitions/Principal Ideals of Preordered Sets', 'Definitions/Preorder Theory']",Definition:Principal Ideal,"Let $left( S, preceq right)$ be a preordered set.

Let $I$ be an ideal in $S$.

=== Definition 1 ===
Let $left( S, preceq right)$ be a preordered set.

Let $I$ be an ideal in $S$.


Then $I$ is a principal ideal  if and only if :
:$exists x in I: x$ is upper bound for $I$

=== Definition 2 ===
Let $left( S, preceq right)$ be a preordered set.

Let $I$ be an ideal in $S$.


Then $I$ is a principal ideal  if and only if :
:$exists x in S: I = x^preceq$
where $x^preceq$ denotes the lower closure of $x$.",Definition:Principal Ideal of Preordered Set,,false,"Let ( S, ≼) be a preordered set.

Let I be an ideal in S.

=== Definition 1 ===
Let ( S, ≼) be a preordered set.

Let I be an ideal in S.


Then I is a principal ideal  if and only if :
:∃ x ∈ I: x is upper bound for I

=== Definition 2 ===
Let ( S, ≼) be a preordered set.

Let I be an ideal in S.


Then I is a principal ideal  if and only if :
:∃ x ∈ S: I = x^≼
where x^≼ denotes the lower closure of x.",Principal Ideal
['Definitions/Multiplication'],Definition:Product,"Let $a times b$ denote the operation of multiplication on two objects $a$ and $b$.

Then the result $a times b$ is referred to as the product of $a$ and $b$.


Note that the nature of $a$ and $b$ has deliberately been left unspecified.

They could be, for example, numbers, matrices or more complex expressions constructed from such elements.",Definition:Multiplication/Product,,false,"Let a × b denote the operation of multiplication on two objects a and b.

Then the result a × b is referred to as the product of a and b.


Note that the nature of a and b has deliberately been left unspecified.

They could be, for example, numbers, matrices or more complex expressions constructed from such elements.",Product
"['Definitions/Continued Products', 'Definitions/Algebra', 'Definitions/Abstract Algebra']",Definition:Product,"Let $left( S, times right)$ be an algebraic structure where the operation $times$ is an operation derived from, or arising from, the multiplication operation on the natural numbers.

Let $left( a_1, a_2, ldots, a_n right) in S^n$ be an ordered $n$-tuple in $S$.


=== Definition by Index ===
Let $left( S, times right)$ be an algebraic structure where the operation $times$ is an operation derived from, or arising from, the multiplication operation on the natural numbers.

Let $left( a_1, a_2, ldots, a_n right) in S^n$ be an ordered $n$-tuple in $S$.


The composite is called the continued product of $left( a_1, a_2, ldots, a_n right)$, and is written:

:$ds prod_{j mathop = 1}^n a_j = left( a_1 times a_2 times cdots times a_n right)$

=== Definition by Inequality ===
Let $left( S, times right)$ be an algebraic structure where the operation $times$ is an operation derived from, or arising from, the multiplication operation on the natural numbers.

Let $left( a_1, a_2, ldots, a_n right) in S^n$ be an ordered $n$-tuple in $S$.


The continued product of $left( a_1, a_2, ldots, a_n right)$ can be written:
:$ds prod_{1 mathop le j mathop le n} a_j = left( a_1 times a_2 times cdots times a_n right)$

=== Definition by Propositional Function ===
Let $left( S, times right)$ be an algebraic structure where the operation $times$ is an operation derived from, or arising from, the multiplication operation on the natural numbers.

Let $left( a_1, a_2, ldots, a_n right) in S^n$ be an ordered $n$-tuple in $S$.


Let $R left(   right)j$ be a propositional function of $j$.

Then we can write:

:$ds prod_{R left(   right)j} a_j = text { the product of all $a_j$ such that $R left(   right)j$ holds}$.


If more than one propositional function is written under the product sign, they must all hold.


Such an operation on an ordered tuple is known as a continued product.


Note that the definition by inequality form $1 le j le n$ is a special case of such a propositional function.

Also note that the definition by index form $ds prod_{j mathop = 1}^n$ is merely another way of writing $ds prod_{1 mathop le j mathop le n}$.

Hence all instances of a continued product can be expressed in terms of a propositional function.


=== Iverson's Convention ===
Let $ds prod_{R left(   right)j} a_j$ be the continued product over all $a_j$ such that $j$ satisfies $R$.


This can also be expressed:
:$ds prod_{j mathop in mathbb Z} a_j^{left[ R left(   right)j right] }$
where $left[ R left(   right)j right]$ is Iverson's convention.",Definition:Continued Product,,false,"Let ( S, ×) be an algebraic structure where the operation × is an operation derived from, or arising from, the multiplication operation on the natural numbers.

Let ( a_1, a_2, …, a_n ) ∈ S^n be an ordered n-tuple in S.


=== Definition by Index ===
Let ( S, ×) be an algebraic structure where the operation × is an operation derived from, or arising from, the multiplication operation on the natural numbers.

Let ( a_1, a_2, …, a_n ) ∈ S^n be an ordered n-tuple in S.


The composite is called the continued product of ( a_1, a_2, …, a_n ), and is written:

:∏_j  = 1^n a_j = ( a_1 × a_2 ×⋯× a_n )

=== Definition by Inequality ===
Let ( S, ×) be an algebraic structure where the operation × is an operation derived from, or arising from, the multiplication operation on the natural numbers.

Let ( a_1, a_2, …, a_n ) ∈ S^n be an ordered n-tuple in S.


The continued product of ( a_1, a_2, …, a_n ) can be written:
:∏_1 ≤ j ≤ n a_j = ( a_1 × a_2 ×⋯× a_n )

=== Definition by Propositional Function ===
Let ( S, ×) be an algebraic structure where the operation × is an operation derived from, or arising from, the multiplication operation on the natural numbers.

Let ( a_1, a_2, …, a_n ) ∈ S^n be an ordered n-tuple in S.


Let R (   )j be a propositional function of j.

Then we can write:

:∏_R (   )j a_j =  the product of all a_j such that R (   )j holds.


If more than one propositional function is written under the product sign, they must all hold.


Such an operation on an ordered tuple is known as a continued product.


Note that the definition by inequality form 1 ≤ j ≤ n is a special case of such a propositional function.

Also note that the definition by index form ∏_j  = 1^n is merely another way of writing ∏_1 ≤ j ≤ n.

Hence all instances of a continued product can be expressed in terms of a propositional function.


=== Iverson's Convention ===
Let ∏_R (   )j a_j be the continued product over all a_j such that j satisfies R.


This can also be expressed:
:∏_j ∈ℤ a_j^[ R (   )j ]
where [ R (   )j ] is Iverson's convention.",Product
"['Definitions/Set Theory', 'Definitions/Cartesian Product']",Definition:Product,"Let $S$ and $T$ be sets.


The cartesian product $S times T$ of $S$ and $T$ is the set of ordered pairs $left( x, y right)$ with $x in S$ and $y in T$:

:$S times T = leftlbrace left( x, y right): x in S land y in T rightrbrace$


Another way of defining it is by:

:$left( x, y right) in S times T iff x in S, y in T$

More specifically:
:$forall p: left( p in S times T iff exists x: exists y: x in S land y in T land p = left( x, y right)  right)$


$S times T$ can be voiced $S$ cross $T$.


=== Class Theory ===
Let $A$ and $B$ be classes.


The cartesian product $A times B$ of $A$ and $B$ is the class of ordered pairs $left( x, y right)$ with $x in A$ and $y in B$:

:$A times B = leftlbrace left( x, y right): x in A land y in B rightrbrace$


Thus:
:$forall p: left( p in A times B iff exists x: exists y: x in A land y in B land p = left( x, y right)  right)$


$A times B$ can be voiced $A$ cross $B$.

=== Diagram ===


=== Finite Cartesian Product ===
Let $leftlangle S_n rightrangle$ be a sequence of sets. 

The (finite) cartesian product of $leftlangle S_n rightrangle$ is defined as:

:$ds prod_{k mathop = 1}^n S_k = leftlbrace left( x_1, x_2, ldots, x_n right): forall k in mathbb N^*_n: x_k in S_k rightrbrace$


It is also denoted $S_1 times S_2 times cdots times S_n$.

Thus $S_1 times S_2 times cdots times S_n$ is the set of all ordered $n$-tuples $left( x_1, x_2, ldots, x_n right)$ with $x_k in S_k$.


In particular:
:$ds prod_{k mathop = 1}^2 S_k = S_1 times S_2$

=== Countable Cartesian Product ===

The same notation can be used to define the (countable) cartesian product of an infinite sequence:

Let $leftlangle S_n rightrangle_{n mathop in mathbb N}$ be an infinite sequence of sets. 

The cartesian product of $leftlangle S_n rightrangle$ is defined as:

:$ds prod_{k mathop = 1}^infty S_k = leftlbrace left( x_1, x_2, ldots, x_n, ldots right): forall k in mathbb N: x_k in S_k rightrbrace$


It defines the concept:
:$S_1 times S_2 times cdots times S_n times cdots$

Thus $ds prod_{k mathop = 1}^infty S_k$ is the set of all infinite sequences $left( x_1, x_2, ldots, x_n, ldots right)$ with $x_k in S_k$.

=== Family of Sets ===
Let $I$ be an indexing set.

Let $leftlangle S_i rightrangle_{i mathop in I}$ be a family of sets indexed by $I$.

The Cartesian product of $leftlangle S_i rightrangle_{i mathop in I}$ is the set of all families $leftlangle s_i rightrangle_{i mathop in I}$ with $s_i in S_i$ for each $i in I$.


This can be denoted $ds prod_{i mathop in I} S_i$ or, if $I$ is understood, $ds prod_i S_i$.",Definition:Cartesian Product,,false,"Let S and T be sets.


The cartesian product S × T of S and T is the set of ordered pairs ( x, y ) with x ∈ S and y ∈ T:

:S × T = {( x, y ): x ∈ S  y ∈ T }


Another way of defining it is by:

:( x, y ) ∈ S × T  x ∈ S, y ∈ T

More specifically:
:∀ p: ( p ∈ S × T ∃ x: ∃ y: x ∈ S  y ∈ T  p = ( x, y )  )


S × T can be voiced S cross T.


=== Class Theory ===
Let A and B be classes.


The cartesian product A × B of A and B is the class of ordered pairs ( x, y ) with x ∈ A and y ∈ B:

:A × B = {( x, y ): x ∈ A  y ∈ B }


Thus:
:∀ p: ( p ∈ A × B ∃ x: ∃ y: x ∈ A  y ∈ B  p = ( x, y )  )


A × B can be voiced A cross B.

=== Diagram ===


=== Finite Cartesian Product ===
Let ⟨ S_n ⟩ be a sequence of sets. 

The (finite) cartesian product of ⟨ S_n ⟩ is defined as:

:∏_k  = 1^n S_k = {( x_1, x_2, …, x_n ): ∀ k ∈ℕ^*_n: x_k ∈ S_k }


It is also denoted S_1 × S_2 ×⋯× S_n.

Thus S_1 × S_2 ×⋯× S_n is the set of all ordered n-tuples ( x_1, x_2, …, x_n ) with x_k ∈ S_k.


In particular:
:∏_k  = 1^2 S_k = S_1 × S_2

=== Countable Cartesian Product ===

The same notation can be used to define the (countable) cartesian product of an infinite sequence:

Let ⟨ S_n ⟩_n ∈ℕ be an infinite sequence of sets. 

The cartesian product of ⟨ S_n ⟩ is defined as:

:∏_k  = 1^∞ S_k = {( x_1, x_2, …, x_n, …): ∀ k ∈ℕ: x_k ∈ S_k }


It defines the concept:
:S_1 × S_2 ×⋯× S_n ×⋯

Thus ∏_k  = 1^∞ S_k is the set of all infinite sequences ( x_1, x_2, …, x_n, …) with x_k ∈ S_k.

=== Family of Sets ===
Let I be an indexing set.

Let ⟨ S_i ⟩_i ∈ I be a family of sets indexed by I.

The Cartesian product of ⟨ S_i ⟩_i ∈ I is the set of all families ⟨ s_i ⟩_i ∈ I with s_i ∈ S_i for each i ∈ I.


This can be denoted ∏_i ∈ I S_i or, if I is understood, ∏_i S_i.",Product
['Definitions/Set Theory'],Definition:Product,"Let $S$ and $T$ be sets.

Let $P$ be a set and let $phi_1: P to S$ and $phi_2: P to T$ be mappings such that:

:For all sets $X$ and all mappings $f_1: X to S$ and $f_2: X to T$ there exists a unique mapping $h: X to P$ such that:
::$phi_1 circ h = f_1$
::$phi_2 circ h = f_2$

:that is, such that:

$quadquadbegin{xy}xymatrix@+1em@L+3px{
&
X
ar[ld]_*+{f_1}
ar@{-->}[d]^*+{h}
ar[rd]^*+{f_2}
\
S
&
P
ar[l]^*+{phi_1}
ar[r]_*+{phi_2}
&
T
}end{xy}$

:is a commutative diagram.


Then $P$, together with the mappings $phi_1$ and $phi_2$, is called a product of $S$ and $T$.


This product of $S$ and $T$ can be denoted $left( P, phi_1, phi_2 right)$.


=== Family of Sets ===
Let $leftlangle S_i rightrangle_{i mathop in I}$ be an indexed family of sets.

Let $P$ be a set.

Let $leftlangle phi_i rightrangle_{i mathop in I}$ be an indexed family of mappings $phi_i: P to S_i$ for all $i in I$ such that:

:For all sets $X$ and all indexed families $leftlangle f_i rightrangle_{i mathop in I}$ of mappings $f_i: X to S_i$ there exists a unique mapping $h: X to P$ such that:
::$forall i in I: phi_i circ h = f_i$

:that is, such that for all $i in I$:

$quad quad begin {xy} xymatrix@+1em@L+3px {
X
ar@{-->}[d]_*+{h}
ar[dr]^*+{f_i}
\
P
ar[r]_*{phi_i}
&
S_i
} end {xy}$

:is a commutative diagram.


Then $P$, together with the family of mappings $leftlangle phi_i rightrangle_{i mathop in I}$, is called a product of (the family) $leftlangle S_i rightrangle_{i mathop in I}$.


This product of $leftlangle S_i rightrangle_{i mathop in I}$ can be denoted $left( P, leftlangle phi_i rightrangle_{i mathop in I}  right)$.

=== Projection ===
Let $leftlangle S_i rightrangle_{i mathop in I}$ be an indexed family of sets.

Let $left( P, leftlangle phi_i rightrangle_{i mathop in I}  right)$ be a set product of $leftlangle S_i rightrangle_{i mathop in I}$.


The mappings $phi_i$ are the projections of $P$.",Definition:Set Product,,false,"Let S and T be sets.

Let P be a set and let ϕ_1: P → S and ϕ_2: P → T be mappings such that:

:For all sets X and all mappings f_1: X → S and f_2: X → T there exists a unique mapping h: X → P such that:
::ϕ_1 ∘ h = f_1
::ϕ_2 ∘ h = f_2

:that is, such that:

@+1em@L+3px   
X
[ld]_*+f_1@–>[d]^*+h[rd]^*+f_2

S
   
P
[l]^*+ϕ_1[r]_*+ϕ_2   
T

:is a commutative diagram.


Then P, together with the mappings ϕ_1 and ϕ_2, is called a product of S and T.


This product of S and T can be denoted ( P, ϕ_1, ϕ_2 ).


=== Family of Sets ===
Let ⟨ S_i ⟩_i ∈ I be an indexed family of sets.

Let P be a set.

Let ⟨ϕ_i ⟩_i ∈ I be an indexed family of mappings ϕ_i: P → S_i for all i ∈ I such that:

:For all sets X and all indexed families ⟨ f_i ⟩_i ∈ I of mappings f_i: X → S_i there exists a unique mapping h: X → P such that:
::∀ i ∈ I: ϕ_i ∘ h = f_i

:that is, such that for all i ∈ I:

@+1em@L+3px 
X
@–>[d]_*+h[dr]^*+f_i

P
[r]_*ϕ_i   
S_i

:is a commutative diagram.


Then P, together with the family of mappings ⟨ϕ_i ⟩_i ∈ I, is called a product of (the family) ⟨ S_i ⟩_i ∈ I.


This product of ⟨ S_i ⟩_i ∈ I can be denoted ( P, ⟨ϕ_i ⟩_i ∈ I).

=== Projection ===
Let ⟨ S_i ⟩_i ∈ I be an indexed family of sets.

Let ( P, ⟨ϕ_i ⟩_i ∈ I) be a set product of ⟨ S_i ⟩_i ∈ I.


The mappings ϕ_i are the projections of P.",Product
['Definitions/Cardinals'],Definition:Product,"Let $A$ and $B$ be sets.

Let $mathbf a$ and $mathbf b$ be the cardinals associated respectively with $A$ and $B$.


Then the product of $mathbf a$ and $mathbf b$ is defined as:
:$mathbf a mathbf b := operatorname {Card} left(   right){A times B}$
where:
:$A times B$ denotes the Cartesian product of $A$ and $B$
:$operatorname {Card} left(   right){A times B}$ denotes the cardinal associated with $A times B$.",Definition:Product of Cardinals,,false,"Let A and B be sets.

Let 𝐚 and 𝐛 be the cardinals associated respectively with A and B.


Then the product of 𝐚 and 𝐛 is defined as:
:𝐚𝐛 := Card(   )A × B
where:
:A × B denotes the Cartesian product of A and B
:Card(   )A × B denotes the cardinal associated with A × B.",Product
"['Definitions/Operations', 'Definitions/Abstract Algebra', 'Definitions/Multiplication']",Definition:Product,"Let $left( S, circ right)$ be an algebraic structure.

Let $circ$ be the operation on $left( S, circ right)$.


=== General Operation ===
Let $left( S, circ right)$ be an algebraic structure.

Let $circ$ be the operation on $left( S, circ right)$.


Let $z = x circ y$.

Then $z$ is called the product of $x$ and $y$.

This is an extension of the normal definition of product that is encountered in conventional arithmetic.


=== Left-Hand Product ===
Let $x$ and $y$ be elements which are operated on by a given operation $circ$.

The left-hand product of $x$ by $y$ is the product $y circ x$.

=== Right-Hand Product ===
Let $x$ and $y$ be elements which are operated on by a given operation $circ$.

The right-hand product of $x$ by $y$ is the product $x circ y$.

=== Group Product ===

Let $left( G, circ right)$ be a group.
Let $left( G, circ right)$ be a group.


The operation $circ$ can be referred to as the group law.

=== Ring Product ===

Let $left( R, *, circ right)$ be a ring.
Let $left( R, *, circ right)$ be a ring.


The distributive operation $circ$ in $left( R, *, circ right)$ is known as the (ring) product.

=== Field Product ===

Let $left( F, +, times right)$ be a field.
Let $left( F, +, times right)$ be a field.


The distributive operation $times$ in $left( F, +, times right)$ is known as the (field) product.

Category:Definitions/Operations
Category:Definitions/Abstract Algebra
Category:Definitions/Multiplication",Definition:Product (Abstract Algebra),,false,"Let ( S, ∘) be an algebraic structure.

Let ∘ be the operation on ( S, ∘).


=== General Operation ===
Let ( S, ∘) be an algebraic structure.

Let ∘ be the operation on ( S, ∘).


Let z = x ∘ y.

Then z is called the product of x and y.

This is an extension of the normal definition of product that is encountered in conventional arithmetic.


=== Left-Hand Product ===
Let x and y be elements which are operated on by a given operation ∘.

The left-hand product of x by y is the product y ∘ x.

=== Right-Hand Product ===
Let x and y be elements which are operated on by a given operation ∘.

The right-hand product of x by y is the product x ∘ y.

=== Group Product ===

Let ( G, ∘) be a group.
Let ( G, ∘) be a group.


The operation ∘ can be referred to as the group law.

=== Ring Product ===

Let ( R, *, ∘) be a ring.
Let ( R, *, ∘) be a ring.


The distributive operation ∘ in ( R, *, ∘) is known as the (ring) product.

=== Field Product ===

Let ( F, +, ×) be a field.
Let ( F, +, ×) be a field.


The distributive operation × in ( F, +, ×) is known as the (field) product.

Category:Definitions/Operations
Category:Definitions/Abstract Algebra
Category:Definitions/Multiplication",Product
['Definitions/Group Theory'],Definition:Product,"Let $left( G, circ right)$ be a group.

The term group product can have two different interpretations:


=== Group Law ===
Let $left( G, circ right)$ be a group.


The operation $circ$ can be referred to as the group law.

=== Product Element ===
Let $left( G, circ right)$ be a group.


Let $a, b in G$ such that $ = a circ b$.

Then $g$ is known as the product of $a$ and $b$.",Definition:Group Product,,false,"Let ( G, ∘) be a group.

The term group product can have two different interpretations:


=== Group Law ===
Let ( G, ∘) be a group.


The operation ∘ can be referred to as the group law.

=== Product Element ===
Let ( G, ∘) be a group.


Let a, b ∈ G such that = a ∘ b.

Then g is known as the product of a and b.",Product
['Definitions/Group Theory'],Definition:Product,"Let $left( G, circ right)$ be a group.


Let $a, b in G$ such that $ = a circ b$.

Then $g$ is known as the product of $a$ and $b$.",Definition:Group Product/Product Element,,false,"Let ( G, ∘) be a group.


Let a, b ∈ G such that = a ∘ b.

Then g is known as the product of a and b.",Product
['Definitions/Ring Theory'],Definition:Product,"Let $left( R, *, circ right)$ be a ring.


The distributive operation $circ$ in $left( R, *, circ right)$ is known as the (ring) product.",Definition:Ring (Abstract Algebra)/Product,,false,"Let ( R, *, ∘) be a ring.


The distributive operation ∘ in ( R, *, ∘) is known as the (ring) product.",Product
"['Definitions/Field Theory', 'Definitions/Multiplication']",Definition:Product,"Let $left( F, +, times right)$ be a field.


The distributive operation $times$ in $left( F, +, times right)$ is known as the (field) product.",Definition:Field (Abstract Algebra)/Product,,false,"Let ( F, +, ×) be a field.


The distributive operation × in ( F, +, ×) is known as the (field) product.",Product
"['Definitions/Matrix Products', 'Definitions/Matrix Theory', 'Definitions/Matrix Algebra', 'Definitions/Vector Algebra']",Definition:Product,"=== Matrix Product (Conventional) ===
Let $left( R, +, circ right)$ be a ring.

Let $mathbf A = left[ a right]_{m n}$ be an $m times n$ matrix over $R$.

Let $mathbf B = left[ b right]_{n p}$ be an $n times p$ matrix over $R$.

Then the matrix product of $mathbf A$ and $mathbf B$ is written $mathbf A mathbf B$ and is defined as follows.

Let $mathbf A mathbf B = mathbf C = left[ c right]_{m p}$.


Then:
:$ds forall i in left[ 1 ,.,.,   right]m, j in left[ 1 ,.,.,   right]p: c_{i j} = sum_{k mathop = 1}^n a_{i k} circ b_{k j}$


Thus $left[ c right]_{m p}$ is the $m times p$ matrix where each entry $c_{i j}$ is built by forming the (ring) product of each entry in the $i$'th row of $mathbf A$ with the corresponding entry in the $j$'th column of $mathbf B$ and adding up all those products.


This operation is called matrix multiplication, and $mathbf C$ is the matrix product of $mathbf A$ with $mathbf B$.


It follows that matrix multiplication is defined whenever the first matrix has the same number of columns as the second matrix has rows.


=== Pre-Multiplication ===
Let $left( R, +, circ right)$ be a ring.

Let $mathbf A = left[ a right]_{m n}$ be an $m times n$ matrix over $R$.

Let $mathbf B = left[ b right]_{n p}$ be an $n times p$ matrix over $R$.


Let $mathbf A mathbf B$ be the product of $mathbf A$ with $mathbf B$.

Then $mathbf B$ is pre-multiplied by $mathbf A$.

=== Post-Multiplication ===
Let $left( R, +, circ right)$ be a ring.

Let $mathbf A = left[ a right]_{m n}$ be an $m times n$ matrix over $R$.

Let $mathbf B = left[ b right]_{n p}$ be an $n times p$ matrix over $R$.


Let $mathbf A mathbf B$ be the product of $mathbf A$ with $mathbf B$.

Then $mathbf A$ is post-multiplied by $mathbf B$.

=== Using Einstein Summation Convention ===
Let $left( R, +, circ right)$ be a ring.

Let $mathbf A = left[ a right]_{m n}$ be an $m times n$ matrix over $R$.

Let $mathbf B = left[ b right]_{n p}$ be an $n times p$ matrix over $R$.


The matrix product of $mathbf A$ and $mathbf B$ can be expressed using the Einstein summation convention as:

Then:
:$c_{i j} := a_{i k} circ b_{k j}$


The index which appears twice in the expressions on the   is the entry $k$, which is the one summated over.

=== Conformable Matrices ===
Let $mathbf A$ and $mathbf B$ be matrices.

It needs to be emphasised that matrix product can be defined on $mathbf A$ and $mathbf B$  if and only if  $mathbf A$ and $mathbf B$ are conformable.


That is, if the number of rows of one is equal to the number of columns of the other.

=== Matrix Scalar Product ===
Let $mathbb F$ denote one of the standard number systems.

Let $mathcal M left(   right){m, n}$ be the $m times n$ matrix space over $mathbb F$.

Let $mathbf A = left[ a right]_{m n} in mathcal M left(   right){m, n}$.

Let $lambda in mathbb F$ be any element of $Bbb F$.


The operation of scalar multiplication of $mathbf A$ by $lambda$ is defined as follows.

Let $lambda mathbf A = mathbf C$.

Then:
:$forall i in left[ 1 ,.,.,   right]m, j in left[ 1 ,.,.,   right]n: c_{i j} = lambda a_{i j}$

$lambda mathbf A$ is the scalar product of $lambda$ and $mathbf A$.


Thus $mathbf C = left[ c right]_{m n}$ is the $m times n$ matrix composed of the product of $lambda$ with the corresponding elements of $mathbf A$.


=== Ring ===
Let $left( R, +, circ right)$ be a ring.

Let $mathbf A = left[ a right]_{m n}$ be an $m times n$ matrix over $left( R, +, circ right)$.

Let $lambda in R$ be any element of $R$.


The scalar product of $lambda$ and $mathbf A$ is defined as follows.

Let $lambda circ mathbf A = mathbf C$.

Then:
:$forall i in left[ 1 ,.,.,   right]m, j in left[ 1 ,.,.,   right]n: c_{i j} = lambda circ a_{i j}$


Thus $left[ c right]_{m n}$ is the $m times n$ matrix composed of the product of $lambda$ with the corresponding elements of $mathbf A$.


=== Scalar ===
Let $mathcal M left(   right){m, n}$ be a matrix space of order $m times n$ on which scalar multiplication is defined.

Let $mathbf A = left[ a right]_{m n} in mathcal M left(   right){m, n}$.


Let $lambda$ be an element of the underlying structure such that:

:$mathbf C = lambda mathbf A$

where the notation denotes scalar multiplication.


The element $lambda$ of the underlying structure of $mathcal M left(   right){m, n}$ is known as a scalar.


Category:Definitions/Matrix Scalar Product

=== Scalar ===
Let $mathcal M left(   right){m, n}$ be a matrix space of order $m times n$ on which scalar multiplication is defined.

Let $mathbf A = left[ a right]_{m n} in mathcal M left(   right){m, n}$.


Let $lambda$ be an element of the underlying structure such that:

:$mathbf C = lambda mathbf A$

where the notation denotes scalar multiplication.


The element $lambda$ of the underlying structure of $mathcal M left(   right){m, n}$ is known as a scalar.


Category:Definitions/Matrix Scalar Product

=== Commutative Matrix Product ===
 

=== Kronecker Product ===

Also known as matrix direct product:

Let $mathbf A = left[ a right]_{m n}$ and $mathbf B = left[ b right]_{p q}$ be matrices.

The Kronecker product of $mathbf A$ and $mathbf B$ is denoted $mathbf A otimes mathbf B$ and is defined as the block matrix:

:$mathbf A otimes mathbf B = begin{bmatrix}
a_{11} mathbf B & a_{12} mathbf B & cdots & a_{1n} mathbf B \
a_{21} mathbf B & a_{22} mathbf B & cdots & a_{2n} mathbf B \
vdots & vdots & ddots & vdots \
a_{m1} mathbf B & a_{m2} mathbf B & cdots & a_{mn} mathbf B
end{bmatrix}$


Writing this out in full:

:$mathbf A otimes mathbf B = begin{bmatrix}
a_{11} b_{11} & a_{11} b_{12} & cdots & a_{11} b_{1q} & cdots & cdots & a_{1n} b_{11} & a_{1n} b_{12} & cdots & a_{1n} b_{1q} \
a_{11} b_{21} & a_{11} b_{22} & cdots & a_{11} b_{2q} & cdots & cdots & a_{1n} b_{21} & a_{1n} b_{22} & cdots & a_{1n} b_{2q} \
vdots & vdots & ddots & vdots & & & vdots & vdots & ddots & vdots \
a_{11} b_{p1} & a_{11} b_{p2} & cdots & a_{11} b_{pq} & cdots & cdots & a_{1n} b_{p1} & a_{1n} b_{p2} & cdots & a_{1n} b_{pq} \
vdots & vdots & & vdots & ddots & & vdots & vdots & & vdots \
vdots & vdots & & vdots & & ddots & vdots & vdots & & vdots \
a_{m1} b_{11} & a_{m1} b_{12} & cdots & a_{m1} b_{1q} & cdots & cdots & a_{mn} b_{11} & a_{mn} b_{12} & cdots & a_{mn} b_{1q} \
a_{m1} b_{21} & a_{m1} b_{22} & cdots & a_{m1} b_{2q} & cdots & cdots & a_{mn} b_{21} & a_{mn} b_{22} & cdots & a_{mn} b_{2q} \
vdots & vdots & ddots & vdots & & & vdots & vdots & ddots & vdots \
a_{m1} b_{p1} & a_{m1} b_{p2} & cdots & a_{m1} b_{pq} & cdots & cdots & a_{mn} b_{p1} & a_{mn} b_{p2} & cdots & a_{mn} b_{pq} 
end{bmatrix}$


Thus, if:
:$mathbf A$ is a matrix with order $m times n$
:$mathbf B$ is a matrix with order $p times q$

then $mathbf A otimes mathbf B$ is a matrix with order $m p times n q$.

=== Hadamard Product ===

Also known as Matrix Entrywise Product or Schur Product:

Let $mathbf A = left[ a right]_{m n}$ and $mathbf B = left[ b right]_{m n}$ be $m times n$ matrices over a ring $left( R, +, times right)$.


The Hadamard product of $mathbf A$ and $mathbf B$ is written $mathbf A circ mathbf B$ and is defined as follows:

:$mathbf A circ mathbf B := mathbf C = left[ c right]_{m n}$

where:

:$forall i in left[ 1 ,.,.,   right]m, j in left[ 1 ,.,.,   right]n: c_{i j} = a_{i j} times b_{i j}$


=== Defined Operation ===


=== Frobenius Inner Product ===
 

=== Cracovian ===
 ",Definition:Matrix Product,,false,"=== Matrix Product (Conventional) ===
Let ( R, +, ∘) be a ring.

Let 𝐀 = [ a ]_m n be an m × n matrix over R.

Let 𝐁 = [ b ]_n p be an n × p matrix over R.

Then the matrix product of 𝐀 and 𝐁 is written 𝐀𝐁 and is defined as follows.

Let 𝐀𝐁 = 𝐂 = [ c ]_m p.


Then:
:∀ i ∈[ 1  . . ]m, j ∈[ 1  . . ]p: c_i j = ∑_k  = 1^n a_i k∘ b_k j


Thus [ c ]_m p is the m × p matrix where each entry c_i j is built by forming the (ring) product of each entry in the i'th row of 𝐀 with the corresponding entry in the j'th column of 𝐁 and adding up all those products.


This operation is called matrix multiplication, and 𝐂 is the matrix product of 𝐀 with 𝐁.


It follows that matrix multiplication is defined whenever the first matrix has the same number of columns as the second matrix has rows.


=== Pre-Multiplication ===
Let ( R, +, ∘) be a ring.

Let 𝐀 = [ a ]_m n be an m × n matrix over R.

Let 𝐁 = [ b ]_n p be an n × p matrix over R.


Let 𝐀𝐁 be the product of 𝐀 with 𝐁.

Then 𝐁 is pre-multiplied by 𝐀.

=== Post-Multiplication ===
Let ( R, +, ∘) be a ring.

Let 𝐀 = [ a ]_m n be an m × n matrix over R.

Let 𝐁 = [ b ]_n p be an n × p matrix over R.


Let 𝐀𝐁 be the product of 𝐀 with 𝐁.

Then 𝐀 is post-multiplied by 𝐁.

=== Using Einstein Summation Convention ===
Let ( R, +, ∘) be a ring.

Let 𝐀 = [ a ]_m n be an m × n matrix over R.

Let 𝐁 = [ b ]_n p be an n × p matrix over R.


The matrix product of 𝐀 and 𝐁 can be expressed using the Einstein summation convention as:

Then:
:c_i j := a_i k∘ b_k j


The index which appears twice in the expressions on the   is the entry k, which is the one summated over.

=== Conformable Matrices ===
Let 𝐀 and 𝐁 be matrices.

It needs to be emphasised that matrix product can be defined on 𝐀 and 𝐁  if and only if  𝐀 and 𝐁 are conformable.


That is, if the number of rows of one is equal to the number of columns of the other.

=== Matrix Scalar Product ===
Let 𝔽 denote one of the standard number systems.

Let ℳ(   )m, n be the m × n matrix space over 𝔽.

Let 𝐀 = [ a ]_m n∈ℳ(   )m, n.

Let λ∈𝔽 be any element of F.


The operation of scalar multiplication of 𝐀 by λ is defined as follows.

Let λ𝐀 = 𝐂.

Then:
:∀ i ∈[ 1  . . ]m, j ∈[ 1  . . ]n: c_i j = λ a_i j

λ𝐀 is the scalar product of λ and 𝐀.


Thus 𝐂 = [ c ]_m n is the m × n matrix composed of the product of λ with the corresponding elements of 𝐀.


=== Ring ===
Let ( R, +, ∘) be a ring.

Let 𝐀 = [ a ]_m n be an m × n matrix over ( R, +, ∘).

Let λ∈ R be any element of R.


The scalar product of λ and 𝐀 is defined as follows.

Let λ∘𝐀 = 𝐂.

Then:
:∀ i ∈[ 1  . . ]m, j ∈[ 1  . . ]n: c_i j = λ∘ a_i j


Thus [ c ]_m n is the m × n matrix composed of the product of λ with the corresponding elements of 𝐀.


=== Scalar ===
Let ℳ(   )m, n be a matrix space of order m × n on which scalar multiplication is defined.

Let 𝐀 = [ a ]_m n∈ℳ(   )m, n.


Let λ be an element of the underlying structure such that:

:𝐂 = λ𝐀

where the notation denotes scalar multiplication.


The element λ of the underlying structure of ℳ(   )m, n is known as a scalar.


Category:Definitions/Matrix Scalar Product

=== Scalar ===
Let ℳ(   )m, n be a matrix space of order m × n on which scalar multiplication is defined.

Let 𝐀 = [ a ]_m n∈ℳ(   )m, n.


Let λ be an element of the underlying structure such that:

:𝐂 = λ𝐀

where the notation denotes scalar multiplication.


The element λ of the underlying structure of ℳ(   )m, n is known as a scalar.


Category:Definitions/Matrix Scalar Product

=== Commutative Matrix Product ===
 

=== Kronecker Product ===

Also known as matrix direct product:

Let 𝐀 = [ a ]_m n and 𝐁 = [ b ]_p q be matrices.

The Kronecker product of 𝐀 and 𝐁 is denoted 𝐀⊗𝐁 and is defined as the block matrix:

:𝐀⊗𝐁 = [ a_11𝐁 a_12𝐁     ⋯ a_1n𝐁; a_21𝐁 a_22𝐁     ⋯ a_2n𝐁;     ⋮     ⋮     ⋱     ⋮; a_m1𝐁 a_m2𝐁     ⋯ a_mn𝐁 ]


Writing this out in full:

:𝐀⊗𝐁 = [ a_11 b_11 a_11 b_12         ⋯ a_11 b_1q         ⋯         ⋯ a_1n b_11 a_1n b_12         ⋯ a_1n b_1q; a_11 b_21 a_11 b_22         ⋯ a_11 b_2q         ⋯         ⋯ a_1n b_21 a_1n b_22         ⋯ a_1n b_2q;         ⋮         ⋮         ⋱         ⋮                             ⋮         ⋮         ⋱         ⋮; a_11 b_p1 a_11 b_p2         ⋯ a_11 b_pq         ⋯         ⋯ a_1n b_p1 a_1n b_p2         ⋯ a_1n b_pq;         ⋮         ⋮                   ⋮         ⋱                   ⋮         ⋮                   ⋮;         ⋮         ⋮                   ⋮                   ⋱         ⋮         ⋮                   ⋮; a_m1 b_11 a_m1 b_12         ⋯ a_m1 b_1q         ⋯         ⋯ a_mn b_11 a_mn b_12         ⋯ a_mn b_1q; a_m1 b_21 a_m1 b_22         ⋯ a_m1 b_2q         ⋯         ⋯ a_mn b_21 a_mn b_22         ⋯ a_mn b_2q;         ⋮         ⋮         ⋱         ⋮                             ⋮         ⋮         ⋱         ⋮; a_m1 b_p1 a_m1 b_p2         ⋯ a_m1 b_pq         ⋯         ⋯ a_mn b_p1 a_mn b_p2         ⋯ a_mn b_pq ]


Thus, if:
:𝐀 is a matrix with order m × n
:𝐁 is a matrix with order p × q

then 𝐀⊗𝐁 is a matrix with order m p × n q.

=== Hadamard Product ===

Also known as Matrix Entrywise Product or Schur Product:

Let 𝐀 = [ a ]_m n and 𝐁 = [ b ]_m n be m × n matrices over a ring ( R, +, ×).


The Hadamard product of 𝐀 and 𝐁 is written 𝐀∘𝐁 and is defined as follows:

:𝐀∘𝐁 := 𝐂 = [ c ]_m n

where:

:∀ i ∈[ 1  . . ]m, j ∈[ 1  . . ]n: c_i j = a_i j× b_i j


=== Defined Operation ===


=== Frobenius Inner Product ===
 

=== Cracovian ===
 ",Product
['Definitions/Category Theory'],Definition:Product,"=== Binary Product ===
Let $mathbf C$ be a metacategory.

Let $A$ and $B$ be objects of $mathbf C$.


A (binary) product diagram for $A$ and $B$ comprises an object $P$ and morphisms $p_1: P to A$, $p_2: P to B$:

::$begin{xy}xymatrix@+1em@L+3px{
 A
&
 P
  ar[l]_*+{p_1}
  ar[r]^*+{p_2}
&
 B
}end{xy}$

subjected to the following universal mapping property:


:For any object $X$ and morphisms $x_1, x_2$ like so:

::$begin{xy}xymatrix@+1em@L+3px{
 A
&
 X
  ar[l]_*+{x_1}
  ar[r]^*+{x_2}
&
 B
}end{xy}$

:there is a unique morphism $u: X to P$ such that:

::$begin{xy}xymatrix@+1em@L+3px{
&
 X
  ar[ld]_*+{x_1}
  ar@{-->}[d]^*+{u}
  ar[rd]^*+{x_2}

\
 A
&
 P
  ar[l]^*+{p_1}
  ar[r]_*+{p_2}
&
 B
}end{xy}$

:is a commutative diagram, i.e., $x_1 = p_1 circ u$ and $x_2 = p_2 circ u$.


In this situation, $P$ is called a (binary) product of $A$ and $B$ and may be denoted $A times B$.

Generally, one writes $leftlangle{x_1, x_2}rightrangle$ for the unique morphism $u$ determined by above diagram.


The morphisms $p_1$ and $p_2$ are often taken to be implicit.

They are called projections; if necessary, $p_1$ can be called the first projection and $p_2$ the second projection.
 

=== General Definition ===
Let $mathbf C$ be a metacategory.

Let $mathcal C$ be any collection of objects of $mathbf C$.

Let $mathbf {Dis}  left(   right)mathcal C$ be the discrete category on $mathcal C$, considered as a subcategory of $mathbf C$.


A product for $mathcal C$, denoted $ds prod mathcal C$, is a limit for the inclusion functor $D: mathbf {Dis}  left(   right)mathcal C to mathbf C$, considered as a diagram.


For an object $C$ in $mathcal C$, the associated morphism $ds prod mathcal C to C$ is denoted $mathrm {pr}_C$ and called the projection on $C$.

The whole construction is pictured in the following commutative diagram:

::$begin{xy}xymatrix@R-.5em@L+3px{
& &
 A
  ar@{-->}[dd]
  ar[dddl]_*+{a_C}
  ar[dddr]^*+{a_C'}

\ \ & &
 ds prod mathcal C
  ar[dl]^*{mathrm {pr}_C}
  ar[dr]_*{mathrm {pr}_{C'}}

\
 mathbf {Dis}  left(   right)mathcal C
&
 C
&
 dots quad dots
&
 C'
}end{xy}$


=== Finite Product ===
Let $mathbf C$ be a metacategory.

Let $ds prod mathcal C$ be a product for a finite set $mathcal C$ of objects of $mathbf C$.


Then $ds prod mathcal C$ is called a finite product.",Definition:Product (Category Theory),,false,"=== Binary Product ===
Let 𝐂 be a metacategory.

Let A and B be objects of 𝐂.


A (binary) product diagram for A and B comprises an object P and morphisms p_1: P → A, p_2: P → B:

::@+1em@L+3px
 A
   
 P
  [l]_*+p_1[r]^*+p_2   
 B

subjected to the following universal mapping property:


:For any object X and morphisms x_1, x_2 like so:

::@+1em@L+3px
 A
   
 X
  [l]_*+x_1[r]^*+x_2   
 B

:there is a unique morphism u: X → P such that:

::@+1em@L+3px   
 X
  [ld]_*+x_1@–>[d]^*+u[rd]^*+x_2

 A
   
 P
  [l]^*+p_1[r]_*+p_2   
 B

:is a commutative diagram, i.e., x_1 = p_1 ∘ u and x_2 = p_2 ∘ u.


In this situation, P is called a (binary) product of A and B and may be denoted A × B.

Generally, one writes ⟨x_1, x_2⟩ for the unique morphism u determined by above diagram.


The morphisms p_1 and p_2 are often taken to be implicit.

They are called projections; if necessary, p_1 can be called the first projection and p_2 the second projection.
 

=== General Definition ===
Let 𝐂 be a metacategory.

Let 𝒞 be any collection of objects of 𝐂.

Let 𝐃𝐢𝐬(   )𝒞 be the discrete category on 𝒞, considered as a subcategory of 𝐂.


A product for 𝒞, denoted ∏𝒞, is a limit for the inclusion functor D: 𝐃𝐢𝐬(   )𝒞→𝐂, considered as a diagram.


For an object C in 𝒞, the associated morphism ∏𝒞→ C is denoted pr_C and called the projection on C.

The whole construction is pictured in the following commutative diagram:

::@R-.5em@L+3px      
 A
  @–>[dd]
  [dddl]_*+a_C[dddr]^*+a_C'

      ∏𝒞[dl]^*pr_C[dr]_*pr_C'
𝐃𝐢𝐬(   )𝒞   
 C
   …  …   
 C'


=== Finite Product ===
Let 𝐂 be a metacategory.

Let ∏𝒞 be a product for a finite set 𝒞 of objects of 𝐂.


Then ∏𝒞 is called a finite product.",Product
"['Definitions/Mapping Theory', 'Definitions/Cartesian Product', 'Definitions/Metric Spaces', 'Definitions/Topology']",Definition:Projection,"Let $S_1, S_2, ldots, S_j, ldots, S_n$ be sets.

Let $ds prod_{i mathop = 1}^n S_i$ be the Cartesian product of $S_1, S_2, ldots, S_n$.

For each $j in leftlbrace 1, 2, ldots, n rightrbrace$, the $j$th projection on $ds S = prod_{i mathop = 1}^n S_i$ is the mapping $mathrm {pr}_j: S to S_j$ defined by:
:$mathrm {pr}_j left(   right){s_1, s_2, ldots, s_j, ldots, s_n} = s_j$

for all $left( s_1, s_2, ldots, s_n right) in S$.


=== Family of Sets ===
Let $leftlangle S_i rightrangle_{i mathop in I}$ be a family of sets.

Let $ds prod_{i mathop in I} S_i$ be the Cartesian product of $leftlangle S_i rightrangle_{i mathop in I}$.


For each $j in I$, the $j$th projection on $ds S = prod_{i mathop in I} S_i$ is the mapping $mathrm {pr}_j: S to S_j$ defined by:
:$mathrm {pr}_j left(   right){leftlangle s_i rightrangle_{i mathop in I} } = s_j$

where $leftlangle s_i rightrangle_{i mathop in I}$ is an arbitrary element of $ds prod_{i mathop in I} S_i$.",Definition:Projection (Mapping Theory),,false,"Let S_1, S_2, …, S_j, …, S_n be sets.

Let ∏_i  = 1^n S_i be the Cartesian product of S_1, S_2, …, S_n.

For each j ∈{ 1, 2, …, n }, the jth projection on S = ∏_i  = 1^n S_i is the mapping pr_j: S → S_j defined by:
:pr_j (   )s_1, s_2, …, s_j, …, s_n = s_j

for all ( s_1, s_2, …, s_n ) ∈ S.


=== Family of Sets ===
Let ⟨ S_i ⟩_i ∈ I be a family of sets.

Let ∏_i ∈ I S_i be the Cartesian product of ⟨ S_i ⟩_i ∈ I.


For each j ∈ I, the jth projection on S = ∏_i ∈ I S_i is the mapping pr_j: S → S_j defined by:
:pr_j (   )⟨ s_i ⟩_i ∈ I = s_j

where ⟨ s_i ⟩_i ∈ I is an arbitrary element of ∏_i ∈ I S_i.",Projection
['Definitions/Projections'],Definition:Projection,"Let $leftlangle S_i rightrangle_{i mathop in I}$ be an indexed family of sets.

Let $left( P, leftlangle phi_i rightrangle_{i mathop in I}  right)$ be a set product of $leftlangle S_i rightrangle_{i mathop in I}$.


The mappings $phi_i$ are the projections of $P$.",Definition:Set Product/Projection,,false,"Let ⟨ S_i ⟩_i ∈ I be an indexed family of sets.

Let ( P, ⟨ϕ_i ⟩_i ∈ I) be a set product of ⟨ S_i ⟩_i ∈ I.


The mappings ϕ_i are the projections of P.",Projection
['Definitions/Category Theory'],Definition:Projection,"=== Binary Product ===
Let $mathbf C$ be a metacategory.

Let $A$ and $B$ be objects of $mathbf C$.


A (binary) product diagram for $A$ and $B$ comprises an object $P$ and morphisms $p_1: P to A$, $p_2: P to B$:

::$begin{xy}xymatrix@+1em@L+3px{
 A
&
 P
  ar[l]_*+{p_1}
  ar[r]^*+{p_2}
&
 B
}end{xy}$

subjected to the following universal mapping property:


:For any object $X$ and morphisms $x_1, x_2$ like so:

::$begin{xy}xymatrix@+1em@L+3px{
 A
&
 X
  ar[l]_*+{x_1}
  ar[r]^*+{x_2}
&
 B
}end{xy}$

:there is a unique morphism $u: X to P$ such that:

::$begin{xy}xymatrix@+1em@L+3px{
&
 X
  ar[ld]_*+{x_1}
  ar@{-->}[d]^*+{u}
  ar[rd]^*+{x_2}

\
 A
&
 P
  ar[l]^*+{p_1}
  ar[r]_*+{p_2}
&
 B
}end{xy}$

:is a commutative diagram, i.e., $x_1 = p_1 circ u$ and $x_2 = p_2 circ u$.


In this situation, $P$ is called a (binary) product of $A$ and $B$ and may be denoted $A times B$.

Generally, one writes $leftlangle{x_1, x_2}rightrangle$ for the unique morphism $u$ determined by above diagram.


The morphisms $p_1$ and $p_2$ are often taken to be implicit.

They are called projections; if necessary, $p_1$ can be called the first projection and $p_2$ the second projection.
 

=== General Definition ===
Let $mathbf C$ be a metacategory.

Let $mathcal C$ be any collection of objects of $mathbf C$.

Let $mathbf {Dis}  left(   right)mathcal C$ be the discrete category on $mathcal C$, considered as a subcategory of $mathbf C$.


A product for $mathcal C$, denoted $ds prod mathcal C$, is a limit for the inclusion functor $D: mathbf {Dis}  left(   right)mathcal C to mathbf C$, considered as a diagram.


For an object $C$ in $mathcal C$, the associated morphism $ds prod mathcal C to C$ is denoted $mathrm {pr}_C$ and called the projection on $C$.

The whole construction is pictured in the following commutative diagram:

::$begin{xy}xymatrix@R-.5em@L+3px{
& &
 A
  ar@{-->}[dd]
  ar[dddl]_*+{a_C}
  ar[dddr]^*+{a_C'}

\ \ & &
 ds prod mathcal C
  ar[dl]^*{mathrm {pr}_C}
  ar[dr]_*{mathrm {pr}_{C'}}

\
 mathbf {Dis}  left(   right)mathcal C
&
 C
&
 dots quad dots
&
 C'
}end{xy}$


=== Finite Product ===
Let $mathbf C$ be a metacategory.

Let $ds prod mathcal C$ be a product for a finite set $mathcal C$ of objects of $mathbf C$.


Then $ds prod mathcal C$ is called a finite product.",Definition:Product (Category Theory),,false,"=== Binary Product ===
Let 𝐂 be a metacategory.

Let A and B be objects of 𝐂.


A (binary) product diagram for A and B comprises an object P and morphisms p_1: P → A, p_2: P → B:

::@+1em@L+3px
 A
   
 P
  [l]_*+p_1[r]^*+p_2   
 B

subjected to the following universal mapping property:


:For any object X and morphisms x_1, x_2 like so:

::@+1em@L+3px
 A
   
 X
  [l]_*+x_1[r]^*+x_2   
 B

:there is a unique morphism u: X → P such that:

::@+1em@L+3px   
 X
  [ld]_*+x_1@–>[d]^*+u[rd]^*+x_2

 A
   
 P
  [l]^*+p_1[r]_*+p_2   
 B

:is a commutative diagram, i.e., x_1 = p_1 ∘ u and x_2 = p_2 ∘ u.


In this situation, P is called a (binary) product of A and B and may be denoted A × B.

Generally, one writes ⟨x_1, x_2⟩ for the unique morphism u determined by above diagram.


The morphisms p_1 and p_2 are often taken to be implicit.

They are called projections; if necessary, p_1 can be called the first projection and p_2 the second projection.
 

=== General Definition ===
Let 𝐂 be a metacategory.

Let 𝒞 be any collection of objects of 𝐂.

Let 𝐃𝐢𝐬(   )𝒞 be the discrete category on 𝒞, considered as a subcategory of 𝐂.


A product for 𝒞, denoted ∏𝒞, is a limit for the inclusion functor D: 𝐃𝐢𝐬(   )𝒞→𝐂, considered as a diagram.


For an object C in 𝒞, the associated morphism ∏𝒞→ C is denoted pr_C and called the projection on C.

The whole construction is pictured in the following commutative diagram:

::@R-.5em@L+3px      
 A
  @–>[dd]
  [dddl]_*+a_C[dddr]^*+a_C'

      ∏𝒞[dl]^*pr_C[dr]_*pr_C'
𝐃𝐢𝐬(   )𝒞   
 C
   …  …   
 C'


=== Finite Product ===
Let 𝐂 be a metacategory.

Let ∏𝒞 be a product for a finite set 𝒞 of objects of 𝐂.


Then ∏𝒞 is called a finite product.",Projection
"['Definitions/Geometric Projections', 'Definitions/Geometry']",Definition:Projection,"=== Projection in Plane ===
Let $M$ and $N$ be distinct lines in the plane.

:

The projection on $M$ along $N$ is the mapping $mathrm {pr}_{M, N}$ such that:
:$forall x in mathbb R^2: mathrm {pr}_{M, N}  left(   right)x =$ the intersection of $M$ with the line through $x$ parallel to $N$.

 ",Definition:Projection (Geometry),,false,"=== Projection in Plane ===
Let M and N be distinct lines in the plane.

:

The projection on M along N is the mapping pr_M, N such that:
:∀ x ∈ℝ^2: pr_M, N(   )x = the intersection of M with the line through x parallel to N.

 ",Projection
"['Definitions/Vector Projections', 'Definitions/Vector Algebra']",Definition:Projection,"Let $mathbf u$ and $mathbf v$ be vector quantities.


=== Definition 1 ===
Let $mathbf u$ and $mathbf v$ be vector quantities.


The (vector) projection of $mathbf u$ onto $mathbf v$, denoted $mathrm {proj}_mathbf v mathbf u$, is the orthogonal projection of $mathbf u$ onto a straight line which is parallel to $mathbf v$.


Hence $mathrm {proj}_mathbf v mathbf u$ is a like vector to $mathbf v$ whose length is $leftlVert mathbf u rightrVert cos theta$, where:
:$leftlVert mathbf u rightrVert$ is the magnitude of $mathbf u$
:$cos theta$ is the angle between $mathbf u$ and $mathbf v$.


:

=== Definition 2 ===
Let $mathbf u$ and $mathbf v$ be vector quantities.


The (vector) projection of $mathbf u$ onto $mathbf v$ is defined and denoted:

:$mathrm {proj}_mathbf v mathbf u = dfrac {mathbf u cdot mathbf v} {leftlVert mathbf v rightrVert^2} mathbf v$

where:
:$cdot$ denotes the dot product
:$leftlVert mathbf v rightrVert$ denotes the magnitude of $mathbf v$.


:

=== Definition 3 ===
Let $mathbf u$ and $mathbf v$ be vector quantities.


The (vector) projection of $mathbf u$ onto $mathbf v$ is defined and denoted:

:$mathrm {proj}_mathbf v mathbf u = u_{parallel mathbf v} mathbf {hat v}$

where:
:$u_{parallel mathbf v}$ denotes the scalar projection of $mathbf u$ on $mathbf v$
:$mathbf {hat v}$ denotes the unit vector in the direction of $mathbf v$.


:

:",Definition:Vector Projection,,false,"Let 𝐮 and 𝐯 be vector quantities.


=== Definition 1 ===
Let 𝐮 and 𝐯 be vector quantities.


The (vector) projection of 𝐮 onto 𝐯, denoted proj_𝐯𝐮, is the orthogonal projection of 𝐮 onto a straight line which is parallel to 𝐯.


Hence proj_𝐯𝐮 is a like vector to 𝐯 whose length is ‖𝐮‖cosθ, where:
:‖𝐮‖ is the magnitude of 𝐮
:cosθ is the angle between 𝐮 and 𝐯.


:

=== Definition 2 ===
Let 𝐮 and 𝐯 be vector quantities.


The (vector) projection of 𝐮 onto 𝐯 is defined and denoted:

:proj_𝐯𝐮 = 𝐮·𝐯‖𝐯‖^2𝐯

where:
:· denotes the dot product
:‖𝐯‖ denotes the magnitude of 𝐯.


:

=== Definition 3 ===
Let 𝐮 and 𝐯 be vector quantities.


The (vector) projection of 𝐮 onto 𝐯 is defined and denoted:

:proj_𝐯𝐮 = u_∥𝐯𝐯̂

where:
:u_∥𝐯 denotes the scalar projection of 𝐮 on 𝐯
:𝐯̂ denotes the unit vector in the direction of 𝐯.


:

:",Projection
"['Definitions/Scalar Projections', 'Definitions/Vector Algebra']",Definition:Projection,"Let $mathbf u$ and $mathbf v$ be vector quantities.


=== Definition 1 ===
Let $mathbf u$ and $mathbf v$ be vector quantities.


The scalar projection of $mathbf u$ onto $mathbf v$, denoted $u_{parallel mathbf v}$, is the magnitude of the orthogonal projection of $mathbf u$ onto a straight line which is parallel to $mathbf v$.


Hence $u_{parallel mathbf v}$ is the magnitude $leftlVert mathbf u rightrVert cos theta$, where:
:$leftlVert mathbf u rightrVert$ is the magnitude of $mathbf u$
:$cos theta$ is the angle between $mathbf u$ and $mathbf v$.


:

=== Definition 2 ===
Let $mathbf u$ and $mathbf v$ be vector quantities.


The scalar projection of $mathbf u$ onto $mathbf v$ is defined and denoted:

:$u_{parallel mathbf v} = dfrac {mathbf u cdot mathbf v} {leftlVert mathbf v rightrVert }$

where:
:$cdot$ denotes the dot product
:$leftlVert mathbf v rightrVert$ denotes the magnitude of $mathbf v$.


:

=== Definition 3 ===
Let $mathbf u$ and $mathbf v$ be vector quantities.


The scalar projection of $mathbf u$ onto $mathbf v$ is defined and denoted:

:$u_{parallel mathbf v} = mathbf u cdot mathbf {hat v}$

where:
:$cdot$ denotes the dot product
:$mathbf {hat v}$ denotes the unit vector in the direction of $mathbf v$.


:

:",Definition:Scalar Projection,,false,"Let 𝐮 and 𝐯 be vector quantities.


=== Definition 1 ===
Let 𝐮 and 𝐯 be vector quantities.


The scalar projection of 𝐮 onto 𝐯, denoted u_∥𝐯, is the magnitude of the orthogonal projection of 𝐮 onto a straight line which is parallel to 𝐯.


Hence u_∥𝐯 is the magnitude ‖𝐮‖cosθ, where:
:‖𝐮‖ is the magnitude of 𝐮
:cosθ is the angle between 𝐮 and 𝐯.


:

=== Definition 2 ===
Let 𝐮 and 𝐯 be vector quantities.


The scalar projection of 𝐮 onto 𝐯 is defined and denoted:

:u_∥𝐯 = 𝐮·𝐯‖𝐯‖

where:
:· denotes the dot product
:‖𝐯‖ denotes the magnitude of 𝐯.


:

=== Definition 3 ===
Let 𝐮 and 𝐯 be vector quantities.


The scalar projection of 𝐮 onto 𝐯 is defined and denoted:

:u_∥𝐯 = 𝐮·𝐯̂

where:
:· denotes the dot product
:𝐯̂ denotes the unit vector in the direction of 𝐯.


:

:",Projection
['Definitions/Linear Transformations on Hilbert Spaces'],Definition:Projection,"Let $H$ be a Hilbert space.

Let $P in B left(   right)H$ be an idempotent operator.


Then $P$ is said to be a projection  if and only if :

:$ker P = left( mathrm {Img} left( P right) right)^perp$

where:
:$ker P$ denotes the kernel of $P$
:$mathrm {Img} left( P right)$ denotes the image of $P$
:$perp$ denotes orthocomplementation.",Definition:Projection (Hilbert Spaces),,false,"Let H be a Hilbert space.

Let P ∈ B (   )H be an idempotent operator.


Then P is said to be a projection  if and only if :

:P = ( Img( P ) )^⊥

where:
:P denotes the kernel of P
:Img( P ) denotes the image of P
:⊥ denotes orthocomplementation.",Projection
"['Definitions/Hilbert Spaces', 'Definitions/Linear Transformations on Hilbert Spaces']",Definition:Projection,"Let $H$ be a Hilbert space.

Let $K$ be a closed linear subspace of $H$.


Then the orthogonal projection on $K$ is the mapping $P_K: H to H$ defined by

:$k = P_K left(   right)h iff k in K$ and $d left(   right){h, k} = d left(   right){h, K}$

where the latter $d$ signifies distance to a set.


That $P_K$ is indeed a mapping is proved on Orthogonal Projection is Mapping.

 
 
 
The name orthogonal projection stems from the fact that $left( h - P_K left(   right)h right) perp K$.",Definition:Orthogonal Projection,,false,"Let H be a Hilbert space.

Let K be a closed linear subspace of H.


Then the orthogonal projection on K is the mapping P_K: H → H defined by

:k = P_K (   )h  k ∈ K and d (   )h, k = d (   )h, K

where the latter d signifies distance to a set.


That P_K is indeed a mapping is proved on Orthogonal Projection is Mapping.

 
 
 
The name orthogonal projection stems from the fact that ( h - P_K (   )h ) ⊥ K.",Projection
['Definitions/Subfields'],Definition:Proper,"Let $left( K, +, circ right)$ be a subfield of $left( F, +, circ right)$.


Then $left( K, +, circ right)$ is a proper subfield of $left( F, +, circ right)$  if and only if  $K ne F$.


That is, $left( K, +, circ right)$ is a proper subfield of $left( F, +, circ right)$  if and only if :
:$(1): quad left( K, +, circ right)$ is a subfield of $left( F, +, circ right)$
:$(2): quad K$ is a proper subset of $F$.",Definition:Subfield/Proper Subfield,,false,"Let ( K, +, ∘) be a subfield of ( F, +, ∘).


Then ( K, +, ∘) is a proper subfield of ( F, +, ∘)  if and only if  K  F.


That is, ( K, +, ∘) is a proper subfield of ( F, +, ∘)  if and only if :
:(1):   ( K, +, ∘) is a subfield of ( F, +, ∘)
:(2):    K is a proper subset of F.",Proper
['Definitions/Subgroups'],Definition:Proper,"Let $left( G, circ right)$ be a group.


Then $left( H, circ right)$ is a proper subgroup of $left( G, circ right)$  if and only if :

: $(1): quad left( H, circ right)$ is a subgroup of $left( G, circ right)$
: $(2): quad H ne G$, i.e. $H subset G$.


The notation $H < G$, or $G > H$, means:
: $H$ is a proper subgroup of $G$.


If $H$ is a subgroup of $G$, but it is not specified whether $H = G$ or not, then we write $H le G$, or $G ge H$.


=== Non-Trivial Proper Subgroup ===
Let $left( G, circ right)$ be a group.

Let $left( H, circ right)$ be a subgroup of $left( G, circ right)$ such that $leftlbrace e rightrbrace subset H subset G$, that is:
:$H ne leftlbrace e rightrbrace$
:$H ne G$

Then $left( H, circ right)$ is a non-trivial proper subgroup of $left( G, circ right)$.",Definition:Proper Subgroup,,false,"Let ( G, ∘) be a group.


Then ( H, ∘) is a proper subgroup of ( G, ∘)  if and only if :

: (1):   ( H, ∘) is a subgroup of ( G, ∘)
: (2):    H  G, i.e. H ⊂ G.


The notation H < G, or G > H, means:
: H is a proper subgroup of G.


If H is a subgroup of G, but it is not specified whether H = G or not, then we write H ≤ G, or G ≥ H.


=== Non-Trivial Proper Subgroup ===
Let ( G, ∘) be a group.

Let ( H, ∘) be a subgroup of ( G, ∘) such that { e }⊂ H ⊂ G, that is:
:H { e }
:H  G

Then ( H, ∘) is a non-trivial proper subgroup of ( G, ∘).",Proper
['Definitions/Ring Theory'],Definition:Proper,"Let $left( R, +, circ right)$ be a ring.


A subring $S$ of $R$ is a proper subring of $R$  if and only if  $S$ is neither the null ring nor $R$ itself.",Definition:Proper Subring,,false,"Let ( R, +, ∘) be a ring.


A subring S of R is a proper subring of R  if and only if  S is neither the null ring nor R itself.",Proper
['Definitions/Continua (Topology)'],Definition:Proper,"Let $T = left( S, tau right)$ be a topological space.

Let $H subseteq S$ be a continuum in $T$.


Let $K$ be a subcontinuum of $H$ such that $K ne H$.

That is, let $K$ be a proper subset of $H$.


Then $K$ is a proper subcontinuum of $H$.",Definition:Subcontinuum/Proper Subcontinuum,,false,"Let T = ( S, τ) be a topological space.

Let H ⊆ S be a continuum in T.


Let K be a subcontinuum of H such that K  H.

That is, let K be a proper subset of H.


Then K is a proper subcontinuum of H.",Proper
"['Definitions/Linear Algebra', 'Definitions/Vector Algebra']",Definition:Proper,"Let $K$ be a division ring.

Let $left( S, +, circ right)_K$ be a $K$-algebraic structure with one operation.


Let $T$ be a closed subset of $S$.

Let $left( T, +_T, circ_T right)_K$ be a $K$-vector space where:
:$+_T$ is the restriction of $+$ to $T times T$ and
:$circ_T$ is the restriction of $circ$ to $K times T$.


Then $left( T, +_T, circ_T right)_K$ is a (vector) subspace of $left( S, +, circ right)_K$.


=== Proper Subspace ===
Let $K$ be a division ring.

Let $left( S, +, circ right)_K$ be a $K$-algebraic structure with one operation.

Let $left( T, +_T, circ_T right)_K$ be a vector subspace of $left( S, +, circ right)_K$.


If $T$ is a proper subset of $S$, then $left( T, +_T, circ_T right)_K$ is a proper (vector) subspace of $left( S, +, circ right)_K$.

=== Hilbert Spaces ===
Let $K$ be a division ring.

Let $left({S, +, circ}right)_K$ be a $K$-algebraic structure with one operation.


Let $T$ be a closed subset of $S$.

Let $left({T, +_T, circ_T}right)_K$ be a $K$-vector space where:
: $+_T$ is the restriction of $+$ to $T times T$ and
: $circ_T$ is the restriction of $circ$ to $K times T$.


Then $left({T, +_T, circ_T}right)_K$ is a (vector) subspace of $left({S, +, circ}right)_K$.


When considering Hilbert spaces, one wants to deal with projections onto subspaces.

These projections however require the linear subspace to be closed in topological sense in order to be well-defined.

Therefore, in treatises of Hilbert spaces, one encounters the terminology linear manifold for the concept of vector subspace defined above.

The adapted definition of linear subspace is then that it is a topologically closed linear manifold.",Definition:Vector Subspace,,false,"Let K be a division ring.

Let ( S, +, ∘)_K be a K-algebraic structure with one operation.


Let T be a closed subset of S.

Let ( T, +_T, ∘_T )_K be a K-vector space where:
:+_T is the restriction of + to T × T and
:∘_T is the restriction of ∘ to K × T.


Then ( T, +_T, ∘_T )_K is a (vector) subspace of ( S, +, ∘)_K.


=== Proper Subspace ===
Let K be a division ring.

Let ( S, +, ∘)_K be a K-algebraic structure with one operation.

Let ( T, +_T, ∘_T )_K be a vector subspace of ( S, +, ∘)_K.


If T is a proper subset of S, then ( T, +_T, ∘_T )_K is a proper (vector) subspace of ( S, +, ∘)_K.

=== Hilbert Spaces ===
Let K be a division ring.

Let (S, +, ∘)_K be a K-algebraic structure with one operation.


Let T be a closed subset of S.

Let (T, +_T, ∘_T)_K be a K-vector space where:
: +_T is the restriction of + to T × T and
: ∘_T is the restriction of ∘ to K × T.


Then (T, +_T, ∘_T)_K is a (vector) subspace of (S, +, ∘)_K.


When considering Hilbert spaces, one wants to deal with projections onto subspaces.

These projections however require the linear subspace to be closed in topological sense in order to be well-defined.

Therefore, in treatises of Hilbert spaces, one encounters the terminology linear manifold for the concept of vector subspace defined above.

The adapted definition of linear subspace is then that it is a topologically closed linear manifold.",Proper
['Definitions/Module Theory'],Definition:Proper,"Let $left( R, +, circ right)$ be a ring.

Let $left( G, +_G, circ_G right)_R$ be an $R$-module.

Let $left( H, +_H, circ_H right)_R$ be a submodule of $left( G, +_G, circ_G right)_R$.

Let $H$ be a proper subset of $G$.


Then $left( H, +_H, circ_H right)_R$ is a proper submodule of $left( G, +_G, circ_G right)_R$.",Definition:Submodule/Proper,,false,"Let ( R, +, ∘) be a ring.

Let ( G, +_G, ∘_G )_R be an R-module.

Let ( H, +_H, ∘_H )_R be a submodule of ( G, +_G, ∘_G )_R.

Let H be a proper subset of G.


Then ( H, +_H, ∘_H )_R is a proper submodule of ( G, +_G, ∘_G )_R.",Proper
['Definitions/Ideal Theory'],Definition:Proper,"Let $left( R, +, circ right)$ be a ring.


A proper ideal $J$ of $left( R, +, circ right)$ is an ideal of $R$ such that $J$ is a proper subset of $R$.

That is, such that $J subseteq R$ and $J ne R$.",Definition:Ideal of Ring/Proper Ideal,,false,"Let ( R, +, ∘) be a ring.


A proper ideal J of ( R, +, ∘) is an ideal of R such that J is a proper subset of R.

That is, such that J ⊆ R and J  R.",Proper
['Definitions/Representation Theory'],Definition:Proper,"Let $left({V, phi}right)$ be a $G$-module.


A $G$-submodule of $V$ is called proper iff it is a proper vector subspace of $V$.

",Definition:Proper G-Submodule,,false,"Let (V, ϕ) be a G-module.


A G-submodule of V is called proper iff it is a proper vector subspace of V.

",Proper
['Definitions/Graph Colorings'],Definition:Proper,"Let $G = left( V, E right)$ be a simple graph.


=== Proper Vertex Coloring ===
Let $G = left( V, E right)$ be a simple graph.


A proper (vertex) $k$-coloring of $G$ is defined as a vertex coloring from a set of $k$ colors such that no two adjacent vertices share a common color.

That is, a proper $k$-coloring of $G$ is a mapping $c: V to leftlbrace 1, 2, ldots k rightrbrace$ such that:
:$forall e = leftlbrace u, v rightrbrace in E: c left(   right)u ne c left(   right)v$

=== Proper Edge Coloring ===
Let $G = left( V, E right)$ be a simple graph.


A proper (edge) $k$-coloring of $G$ is defined as an edge coloring from a set of $k$ colors such that no two adjacent edges share a common color.

That is, a proper $k$-coloring of $G$ is a mapping $c: E to leftlbrace 1, 2, ldots k rightrbrace$ such that:
:$forall v in V: forall e = leftlbrace u_k, v rightrbrace in E: c left(   right){leftlbrace u_i, v rightrbrace } ne c left(   right){leftlbrace u_j, v rightrbrace }$

Category:Definitions/Graph Colorings",Definition:Proper Coloring,,false,"Let G = ( V, E ) be a simple graph.


=== Proper Vertex Coloring ===
Let G = ( V, E ) be a simple graph.


A proper (vertex) k-coloring of G is defined as a vertex coloring from a set of k colors such that no two adjacent vertices share a common color.

That is, a proper k-coloring of G is a mapping c: V →{ 1, 2, … k } such that:
:∀ e = { u, v }∈ E: c (   )u  c (   )v

=== Proper Edge Coloring ===
Let G = ( V, E ) be a simple graph.


A proper (edge) k-coloring of G is defined as an edge coloring from a set of k colors such that no two adjacent edges share a common color.

That is, a proper k-coloring of G is a mapping c: E →{ 1, 2, … k } such that:
:∀ v ∈ V: ∀ e = { u_k, v }∈ E: c (   ){ u_i, v } c (   ){ u_j, v }

Category:Definitions/Graph Colorings",Proper
"['Definitions/Graph Colorings', 'Definitions/Vertices of Graphs']",Definition:Proper,"Let $G = left( V, E right)$ be a simple graph.


A proper (vertex) $k$-coloring of $G$ is defined as a vertex coloring from a set of $k$ colors such that no two adjacent vertices share a common color.

That is, a proper $k$-coloring of $G$ is a mapping $c: V to leftlbrace 1, 2, ldots k rightrbrace$ such that:
:$forall e = leftlbrace u, v rightrbrace in E: c left(   right)u ne c left(   right)v$",Definition:Proper Coloring/Vertex Coloring,,false,"Let G = ( V, E ) be a simple graph.


A proper (vertex) k-coloring of G is defined as a vertex coloring from a set of k colors such that no two adjacent vertices share a common color.

That is, a proper k-coloring of G is a mapping c: V →{ 1, 2, … k } such that:
:∀ e = { u, v }∈ E: c (   )u  c (   )v",Proper
['Definitions/Ancestor Nodes'],Definition:Proper,"Let $T$ be a rooted tree with root $r_T$.

Let $t$ be a node of $T$.


A proper ancestor node of $t$ is an ancestor node of $t$ that is not $t$ itself.",Definition:Rooted Tree/Ancestor Node/Proper,,false,"Let T be a rooted tree with root r_T.

Let t be a node of T.


A proper ancestor node of t is an ancestor node of t that is not t itself.",Proper
['Definitions/Normal Series'],Definition:Proper,"A proper refinement of a normal series is a refinement which is not equal to the original normal series.


That is, it contains extra (normal) subgroups which are not present in the original normal series.",Definition:Refinement of Normal Series/Proper Refinement,,false,"A proper refinement of a normal series is a refinement which is not equal to the original normal series.


That is, it contains extra (normal) subgroups which are not present in the original normal series.",Proper
['Definitions/Matrix Algebra'],Definition:Proper,"Let $mathbf Q$ be an orthogonal matrix.


Then $mathbf Q$ is a proper orthogonal matrix  if and only if :
:$det left(   right){mathbf Q} = 1$
where $det left(   right){mathbf Q}$ is the determinant of $mathbf Q$.",Definition:Proper Orthogonal Matrix,,false,"Let 𝐐 be an orthogonal matrix.


Then 𝐐 is a proper orthogonal matrix  if and only if :
:(   )𝐐 = 1
where (   )𝐐 is the determinant of 𝐐.",Proper
['Definitions/Formal Languages'],Definition:Proper,"Let $mathcal F$ be a formal language with alphabet $mathcal A$.

Let $mathbf A$ be a well-formed formula of $mathcal F$.

Let $mathbf B$ be a well-formed part of $mathbf A$.


Then $mathbf B$ is a proper well-formed part of $mathbf A$  if and only if  $mathbf B$ is not equal to $mathbf A$.",Definition:Well-Formed Part/Proper Well-Formed Part,,false,"Let ℱ be a formal language with alphabet 𝒜.

Let 𝐀 be a well-formed formula of ℱ.

Let 𝐁 be a well-formed part of 𝐀.


Then 𝐁 is a proper well-formed part of 𝐀  if and only if  𝐁 is not equal to 𝐀.",Proper
"['Definitions/Proper Divisors', 'Definitions/Divisors']",Definition:Proper,"Let $left( mathbb Z, +, times right)$ be the ring of integers.

Let $x, y in mathbb Z$.


Then $x$ divides $y$ is defined as:
:$x mathrel backslash y iff exists t in mathbb Z: y = t times x$


Then $x$ is a proper divisor of $y$  if and only if :

:$(1): quad x mathrel backslash y$
:$(2): quad leftlvert x rightrvert ne leftlvert y rightrvert$
:$(3): quad x ne pm 1$

That is:
:$(1): quad x$ is a divisor of $y$
:$(2): quad x$ and $y$ are not equal in absolute value
:$(3): quad x$ is not equal to either $1$ or $-1$.",Definition:Proper Divisor/Integer,,false,"Let ( ℤ, +, ×) be the ring of integers.

Let x, y ∈ℤ.


Then x divides y is defined as:
:x  y ∃ t ∈ℤ: y = t × x


Then x is a proper divisor of y  if and only if :

:(1):    x  y
:(2):   | x || y |
:(3):    x ± 1

That is:
:(1):    x is a divisor of y
:(2):    x and y are not equal in absolute value
:(3):    x is not equal to either 1 or -1.",Proper
['Definitions/Ring Theory'],Definition:Proper,A non-zero element of a ring which does not have a product inverse is called a proper element.,Definition:Proper Element of Ring,,false,A non-zero element of a ring which does not have a product inverse is called a proper element.,Proper
['Definitions/Zero Divisors'],Definition:Proper,"Let $left( R, +, circ right)$ be a ring.


A proper zero divisor of $R$ is an element $x in R^*$ such that:

:$exists y in R^*: x circ y = 0_R$

where $R^*$ is defined as $R setminus leftlbrace 0_R rightrbrace$.


That is, it is a zero divisor of $R$ which is specifically not $0_R$.


The presence of a proper zero divisor in a ring means that the product of two elements of the ring may be zero even if neither factor is zero.

That is, if $R$ has proper zero divisors, then $left( R^*, circ right)$ is not closed.",Definition:Proper Zero Divisor,,false,"Let ( R, +, ∘) be a ring.


A proper zero divisor of R is an element x ∈ R^* such that:

:∃ y ∈ R^*: x ∘ y = 0_R

where R^* is defined as R ∖{ 0_R }.


That is, it is a zero divisor of R which is specifically not 0_R.


The presence of a proper zero divisor in a ring means that the product of two elements of the ring may be zero even if neither factor is zero.

That is, if R has proper zero divisors, then ( R^*, ∘) is not closed.",Proper
"['Definitions/Proper Divisors', 'Definitions/Ring Theory']",Definition:Proper,"Let $left( D, +, circ right)$ be an integral domain whose zero is $0_D$ and whose unity is $1_D$.

Let $U$ be the group of units of $D$.

Let $x, y in D$.


Then $x$ is a proper divisor of $y$  if and only if :

:$(1): quad x mathrel backslash y$
:$(2): quad y nmid x$
:$(3): quad x notin U$

That is:
:$(1): quad x$ is a divisor of $y$
:$(2): quad x$ is not an associate of $y$
:$(3): quad x$ is not a unit of $D$


=== Integers ===

As the set of integers form an integral domain, the concept of a proper divisor is fully applicable to the integers.

Let $left( mathbb Z, +, times right)$ be the ring of integers.

Let $x, y in mathbb Z$.


Then $x$ divides $y$ is defined as:
:$x mathrel backslash y iff exists t in mathbb Z: y = t times x$


Then $x$ is a proper divisor of $y$  if and only if :

:$(1): quad x mathrel backslash y$
:$(2): quad leftlvert x rightrvert ne leftlvert y rightrvert$
:$(3): quad x ne pm 1$

That is:
:$(1): quad x$ is a divisor of $y$
:$(2): quad x$ and $y$ are not equal in absolute value
:$(3): quad x$ is not equal to either $1$ or $-1$.",Definition:Proper Divisor,,false,"Let ( D, +, ∘) be an integral domain whose zero is 0_D and whose unity is 1_D.

Let U be the group of units of D.

Let x, y ∈ D.


Then x is a proper divisor of y  if and only if :

:(1):    x  y
:(2):    y ∤ x
:(3):    x ∉ U

That is:
:(1):    x is a divisor of y
:(2):    x is not an associate of y
:(3):    x is not a unit of D


=== Integers ===

As the set of integers form an integral domain, the concept of a proper divisor is fully applicable to the integers.

Let ( ℤ, +, ×) be the ring of integers.

Let x, y ∈ℤ.


Then x divides y is defined as:
:x  y ∃ t ∈ℤ: y = t × x


Then x is a proper divisor of y  if and only if :

:(1):    x  y
:(2):   | x || y |
:(3):    x ± 1

That is:
:(1):    x is a divisor of y
:(2):    x and y are not equal in absolute value
:(3):    x is not equal to either 1 or -1.",Proper
['Definitions/Subsets'],Definition:Proper,"Let $S$ and $T$ be sets such that $S$ is a subset of $T$.

Let $S ne T$.

Then $S$ is referred to as a proper subset of $T$, and we write:
:$S subsetneq T$
or:
:$S subsetneqq T$


=== Proper Superset ===
If $S$ is a proper subset of $T$, then $T$ is a proper superset of $S$.

This can be expressed by the notation $T supsetneqq S$.",Definition:Proper Subset,,false,"Let S and T be sets such that S is a subset of T.

Let S  T.

Then S is referred to as a proper subset of T, and we write:
:S ⊊ T
or:
:S ⫋ T


=== Proper Superset ===
If S is a proper subset of T, then T is a proper superset of S.

This can be expressed by the notation T ⫌ S.",Proper
['Definitions/Class Theory'],Definition:Proper,"A proper class is a class which is not a set.

That is, $A$ is a proper class  if and only if :
:$neg exists x: x = A$
where $x$ is a set.",Definition:Class (Class Theory)/Proper Class,,false,"A proper class is a class which is not a set.

That is, A is a proper class  if and only if :
:∃ x: x = A
where x is a set.",Proper
['Definitions/Filter Theory'],Definition:Proper,"Let $left( S, preccurlyeq right)$ be an ordered set.

Let $mathcal F$ be a filter on $left( S, preccurlyeq right)$.


Then:
:$mathcal F$ is a proper filter on $S$
 if and only if :
:$mathcal F ne S$

That is,  if and only if  $mathcal F$ is a proper subset of $S$.",Definition:Filter/Proper Filter,,false,"Let ( S, ≼) be an ordered set.

Let ℱ be a filter on ( S, ≼).


Then:
:ℱ is a proper filter on S
 if and only if :
:ℱ S

That is,  if and only if  ℱ is a proper subset of S.",Proper
"['Definitions/Well-Orderings', 'Definitions/Class Theory']",Definition:Proper,"Let $V$ be a basic universe.

Let $A$ be a class.

Let $mathcal R$ be a well-ordering on $A$.


Then $mathcal R$ is a proper well-ordering  if and only if :
:every proper lower section of $A$ is a set.",Definition:Proper Well-Ordering,,false,"Let V be a basic universe.

Let A be a class.

Let ℛ be a well-ordering on A.


Then ℛ is a proper well-ordering  if and only if :
:every proper lower section of A is a set.",Proper
"['Definitions/Proper Mappings', 'Definitions/Topology', 'Definitions/Compact Spaces']",Definition:Proper,"Let $X$ and $Y$ be topological spaces.


A mapping $f: X to Y$ is proper  if and only if  for every compact subspace $K subset Y$, its preimage $f^{-1} left[ K right]$ is also compact.",Definition:Proper Mapping,,false,"Let X and Y be topological spaces.


A mapping f: X → Y is proper  if and only if  for every compact subspace K ⊂ Y, its preimage f^-1[ K ] is also compact.",Proper
"['Definitions/Topology', 'Definitions/Group Theory', 'Definitions/Group Actions', 'Definitions/Topological Groups']",Definition:Proper,"Let $G$ be a topological group.

Let $X$ be a topological space.


A group action $phi: G times X to X$ is called proper  if and only if  $phi$ is a proper mapping.


Here $Gtimes X$ is equipped with the product topology.",Definition:Proper Group Action,,false,"Let G be a topological group.

Let X be a topological space.


A group action ϕ: G × X → X is called proper  if and only if  ϕ is a proper mapping.


Here G× X is equipped with the product topology.",Proper
"['Definitions/Proper Names', 'Definitions/Predicate Logic']",Definition:Proper,"A proper name (or just name) is a symbol or collection of symbols used to identify a particular object uniquely.


In contrast with natural language, a proper name has a wider range than being the particular identifying label attached to a particular single entity (be it a person, or a place, or whatever else).

For example:
:Sloth is a proper name for the concept of being lazy.
:Rain is a proper name for the meteorological phenomenon of water falling from the sky.",Definition:Proper Name,,false,"A proper name (or just name) is a symbol or collection of symbols used to identify a particular object uniquely.


In contrast with natural language, a proper name has a wider range than being the particular identifying label attached to a particular single entity (be it a person, or a place, or whatever else).

For example:
:Sloth is a proper name for the concept of being lazy.
:Rain is a proper name for the meteorological phenomenon of water falling from the sky.",Proper
"['Definitions/Category Theory', 'Definitions/Pullbacks']",Definition:Pullback,"Let $mathbf C$ be a metacategory.

Let $f: A to C$ and $g: B to C$ be morphisms with common codomain.


A pullback of $f$ and $g$ is a commutative diagram:

::$begin{xy}xymatrix{
 P
  ar[r]^*+{p_1}
  ar[d]_*+{p_2}
&
 A
  ar[d]^*+{f}

\
 B
  ar[r]_*+{g}
&
 C
}end{xy}$

such that $f circ p_1 = g circ p_2$, subject to the following UMP:


:For any commutative diagram:

:::$begin{xy}xymatrix{
 Q
  ar[r]^*+{q_1}
  ar[d]_*+{q_2}
&
 A
  ar[d]^*+{f}

\
 B
  ar[r]_*+{g}
&
 C
}end{xy}$

:there is a unique morphism $u: Q to P$ making the following diagram commute:

:::$begin{xy}xymatrix@+1em{
 Q
  ar@/^/[rrd]^*+{q_1}
  ar@/_/[ddr]_*+{q_2}
  ar@{-->}[rd]^*+{u}

\
&
 P
  ar[r]_*+{p_1}
  ar[d]^*+{p_2}
&
 A
  ar[d]^*+{f}

\
&
 B
  ar[r]_*+{g}
&
 C
}end{xy}$


In this situation, $p_1$ is called the pullback of $f$ along $g$ and may be denoted as $g^* f$.

Similarly, $p_2$ is called the pullback of $g$ along $f$ and may be denoted $f^* g$.",Definition:Pullback (Category Theory),,false,"Let 𝐂 be a metacategory.

Let f: A → C and g: B → C be morphisms with common codomain.


A pullback of f and g is a commutative diagram:

::P
  [r]^*+p_1[d]_*+p_2   
 A
  [d]^*+f

 B
  [r]_*+g   
 C

such that f ∘ p_1 = g ∘ p_2, subject to the following UMP:


:For any commutative diagram:

:::Q
  [r]^*+q_1[d]_*+q_2   
 A
  [d]^*+f

 B
  [r]_*+g   
 C

:there is a unique morphism u: Q → P making the following diagram commute:

:::@+1em
 Q
  @/^/[rrd]^*+q_1@/_/[ddr]_*+q_2@–>[rd]^*+u
   
 P
  [r]_*+p_1[d]^*+p_2   
 A
  [d]^*+f
   
 B
  [r]_*+g   
 C


In this situation, p_1 is called the pullback of f along g and may be denoted as g^* f.

Similarly, p_2 is called the pullback of g along f and may be denoted f^* g.",Pullback
"['Definitions/Pullbacks', 'Definitions/Slice Categories']",Definition:Pullback,"Let $mathbf C$ be a metacategory having all pullbacks.

Let $f: C to D$ be a morphism of $mathbf C$.

Let $mathbf C mathop / C$ and $mathbf C mathop / D$ be the slice categories over $C$ and $D$, respectively.


The pullback functor $f^* : mathbf C mathop / D to mathbf C mathop / C$ associated to $f$ is defined by:

 
 
 
 

Explicitly, $f^* gamma$ is defined as the unique morphism fitting:

::$begin{xy}xymatrix@+1em@L+4px{
 A'
  ar[rr]^*{f_alpha}
  ar[dd]_*{f^* alpha}
  ar@{-->}[rd]_*{f^* gamma}
& &
 A
  ar[rd]^*{gamma}
  ar[dd]^(.4)*{alpha}

\ &
 B'
  ar[ld]^*{f^* beta}
  ar[rr] |{hole} ^(.3)*{f_beta}
& &
 B
  ar[ld]^*{beta}

\
 C
  ar[rr]_*{f}
& &
 D
}end{xy}$",Definition:Pullback Functor,,false,"Let 𝐂 be a metacategory having all pullbacks.

Let f: C → D be a morphism of 𝐂.

Let 𝐂 / C and 𝐂 / D be the slice categories over C and D, respectively.


The pullback functor f^* : 𝐂 / D →𝐂 / C associated to f is defined by:

 
 
 
 

Explicitly, f^* γ is defined as the unique morphism fitting:

::@+1em@L+4px
 A'
  [rr]^*f_α[dd]_*f^* α@–>[rd]_*f^* γ      
 A
  [rd]^*γ[dd]^(.4)*α
   
 B'
  [ld]^*f^* β[rr] | ^(.3)*f_β      
 B
  [ld]^*β

 C
  [rr]_*f      
 D",Pullback
"['Definitions/Group Theory', 'Definitions/Normality in Groups']",Definition:Pullback,"Let $G, H$ be groups.

Let $N lhd G, K lhd H$ be normal subgroups of $G$ and $H$ respectively.

Let:
:$G / N cong H / K$
where:
:$G / N$ denotes the quotient of $G$ by $N$
:$cong$ denotes group isomorphism.

Let $theta: G / N to H / K$ be such a group isomorphism.


The pullback $G times^theta H$ of $G$ and $H$ via $theta$ is the subset of $G times H$ of elements of the form $left( g, h right)$ where $theta left(   right){g N} = h K$.",Definition:Pullback of Quotient Group Isomorphism,,false,"Let G, H be groups.

Let N  G, K  H be normal subgroups of G and H respectively.

Let:
:G / N ≅ H / K
where:
:G / N denotes the quotient of G by N
:≅ denotes group isomorphism.

Let θ: G / N → H / K be such a group isomorphism.


The pullback G ×^θ H of G and H via θ is the subset of G × H of elements of the form ( g, h ) where θ(   )g N = h K.",Pullback
"['Definitions/Quadratic Forms (Linear Algebra)', 'Definitions/Bilinear Forms (Linear Algebra)', 'Definitions/Linear Algebra', 'Definitions/Module Theory', 'Definitions/Vector Spaces', 'Definitions/Quadratic Forms']",Definition:Quadratic Form,"Let $mathbb K$ be a field of characteristic $mathrm {Char} left( mathbb K right) ne 2$.

Let $V$ be a vector space over $mathbb K$.


A quadratic form on $V$ is a mapping $q : V mapsto mathbb K$ such that:
:$forall v in V : forall kappa in mathbb K : q left(   right){kappa v} = kappa^2 q left(   right)v$
:$b: V times V to mathbb K: left( v, w right) mapsto q left(   right){v + w} - q left(   right)v - q left(   right)w$ is a bilinear form


 ",Definition:Quadratic Form (Linear Algebra),,false,"Let 𝕂 be a field of characteristic Char( 𝕂)  2.

Let V be a vector space over 𝕂.


A quadratic form on V is a mapping q : V ↦𝕂 such that:
:∀ v ∈ V : ∀κ∈𝕂 : q (   )κ v = κ^2 q (   )v
:b: V × V →𝕂: ( v, w ) ↦ q (   )v + w - q (   )v - q (   )w is a bilinear form


 ",Quadratic Form
"['Definitions/Quadratic Forms (Polynomial Theory)', 'Definitions/Forms', 'Definitions/Polynomial Theory', 'Definitions/Quadratic Forms']",Definition:Quadratic Form,A quadratic form is a form whose variables are of degree $2$.,Definition:Quadratic Form (Polynomial Theory),,false,A quadratic form is a form whose variables are of degree 2.,Quadratic Form
"['Definitions/Compact Spaces', 'Definitions/Topology']",Definition:Quasi-Compact,"=== Definition 1 ===
A topological space $T = left( S, tau right)$ is compact  if and only if  every open cover for $S$ has a finite subcover.

=== Definition 2 ===
A topological space $T = left( S, tau right)$ is compact  if and only if  it satisfies the Finite Intersection Axiom.

=== Definition 3 ===
A topological space $T = left( S, tau right)$ is compact  if and only if  $tau$ has a sub-basis $mathcal B$ such that:
:from every cover of $S$ by elements of $mathcal B$, a finite subcover of $S$ can be selected.

=== Definition 4 ===
A topological space $T = left( S, tau right)$ is compact  if and only if  every filter on $S$ has a limit point in $S$.

=== Definition 5 ===
A topological space $T = left( S, tau right)$ is compact  if and only if  every ultrafilter on $S$ converges.",Definition:Compact Space/Topology,,false,"=== Definition 1 ===
A topological space T = ( S, τ) is compact  if and only if  every open cover for S has a finite subcover.

=== Definition 2 ===
A topological space T = ( S, τ) is compact  if and only if  it satisfies the Finite Intersection Axiom.

=== Definition 3 ===
A topological space T = ( S, τ) is compact  if and only if  τ has a sub-basis ℬ such that:
:from every cover of S by elements of ℬ, a finite subcover of S can be selected.

=== Definition 4 ===
A topological space T = ( S, τ) is compact  if and only if  every filter on S has a limit point in S.

=== Definition 5 ===
A topological space T = ( S, τ) is compact  if and only if  every ultrafilter on S converges.",Quasi-Compact
"['Definitions/Algebraic Geometry', 'Definitions/Schemes']",Definition:Quasi-Compact,"Let $left( X, mathcal O_X right)$ be a scheme.


Then $left( X, mathcal O_X right)$ is quasi-compact  if and only if  $X$ is compact.",Definition:Quasi-Compact Scheme,,false,"Let ( X, 𝒪_X ) be a scheme.


Then ( X, 𝒪_X ) is quasi-compact  if and only if  X is compact.",Quasi-Compact
"['Definitions/Algebraic Geometry', 'Definitions/Schemes']",Definition:Quasi-Compact,"Let $left( X, mathcal O_X right)$ and $left( Y, mathcal O_Y right)$ be schemes.

Let $f : left( X, mathcal O_X right) to left( Y, mathcal O_Y right)$ be a morphism of schemes.


$f$ is quasi-compact  if and only if  for all quasi-compact open subsets $U subset Y$, the set $f^{-1}  left(   right)U subset X$ is quasi-compact.",Definition:Quasi-Compact Morphism of Schemes,,false,"Let ( X, 𝒪_X ) and ( Y, 𝒪_Y ) be schemes.

Let f : ( X, 𝒪_X ) →( Y, 𝒪_Y ) be a morphism of schemes.


f is quasi-compact  if and only if  for all quasi-compact open subsets U ⊂ Y, the set f^-1(   )U ⊂ X is quasi-compact.",Quasi-Compact
"['Definitions/Roots of Numbers', 'Definitions/Real Analysis']",Definition:Radical,"Let $x, y in mathbb R_{ge 0}$ be positive real numbers.

Let $n in mathbb Z$ be an integer such that $n ne 0$.


Then $y$ is the positive $n$th root of $x$  if and only if :
:$y^n = x$

and we write:
:$y = sqrt[n] x$


Using the power notation, this can also be written:
:$y = x^{1/n}$


When $n = 2$, we write $y = sqrt x$ and call $y$ the positive square root of $x$.

When $n = 3$, we write $y = sqrt [3] x$ and call $y$ the cube root of $x$.


Note the special case where $x = 0 = y$:
:$0 = sqrt [n] 0$


=== Index ===
Let $sqrt [n] x$ denote the $n$th root of $x$.

The number $n$ is known as the index of the root.


If $n$ is not specified, that is $sqrt x$ is presented, this means the square root.

=== Extraction of Root ===
The process of evaluating roots of a given real number is referred to as extraction.",Definition:Root of Number,,false,"Let x, y ∈ℝ_≥ 0 be positive real numbers.

Let n ∈ℤ be an integer such that n  0.


Then y is the positive nth root of x  if and only if :
:y^n = x

and we write:
:y = √(x)


Using the power notation, this can also be written:
:y = x^1/n


When n = 2, we write y = √(x) and call y the positive square root of x.

When n = 3, we write y = √(x) and call y the cube root of x.


Note the special case where x = 0 = y:
:0 = √(0)


=== Index ===
Let √(x) denote the nth root of x.

The number n is known as the index of the root.


If n is not specified, that is √(x) is presented, this means the square root.

=== Extraction of Root ===
The process of evaluating roots of a given real number is referred to as extraction.",Radical
['Definitions/Field Extensions'],Definition:Radical,"Let $L / F$ be a field extension.

Then $L$ is a radical extension of $F$  if and only if  there exist $alpha_1, ldots, alpha_m in F$ and $n_1, ldots, n_2 in mathbb Z_{>0}$ such that:

:$(1): quad L = K left[ alpha_1, ldots, alpha_m right]$

:$(2): quad alpha_1^{n_1} in F$

:$(3): quad forall i in mathbb N_m: alpha_i^{n_i} in F left[ alpha_1, ldots, alpha_{i-1}  right]$

where $K left[ alpha_1, ldots, alpha_m right]$ and $F left[ alpha_1, ldots, alpha_{i-1}  right]$ are generated field extensions.

Category:Definitions/Field Extensions",Definition:Radical Extension,,false,"Let L / F be a field extension.

Then L is a radical extension of F  if and only if  there exist α_1, …, α_m ∈ F and n_1, …, n_2 ∈ℤ_>0 such that:

:(1):    L = K [ α_1, …, α_m ]

:(2):   α_1^n_1∈ F

:(3):   ∀ i ∈ℕ_m: α_i^n_i∈ F [ α_1, …, α_i-1]

where K [ α_1, …, α_m ] and F [ α_1, …, α_i-1] are generated field extensions.

Category:Definitions/Field Extensions",Radical
['Definitions/Bilinear Forms (Linear Algebra)'],Definition:Radical,"Let $mathbb K$ be a field.

Let $V$ be a vector space over $mathbb K$.

Let $b : Vtimes V to mathbb K$ be a reflexive bilinear form on $V$.


The radical of $V$ is the orthogonal complement of $V$:
:$operatorname {rad}  left(   right)V = V^perp$",Definition:Orthogonal (Bilinear Form)/Radical,,false,"Let 𝕂 be a field.

Let V be a vector space over 𝕂.

Let b : V× V →𝕂 be a reflexive bilinear form on V.


The radical of V is the orthogonal complement of V:
:rad(   )V = V^⊥",Radical
"['Definitions/Radicals of Integers', 'Definitions/Number Theory']",Definition:Radical,"Let $n in mathbb Z$ be an integer.

=== Definition 1 ===
Let $n in mathbb Z$ be an integer.

The radical of $n$ is the product of the individual prime factors of $n$.

=== Definition 2 ===
Let $n in mathbb Z$ be an integer.

The radical of $n$ is the largest square-free integer which divides $n$.",Definition:Radical of Integer,,false,"Let n ∈ℤ be an integer.

=== Definition 1 ===
Let n ∈ℤ be an integer.

The radical of n is the product of the individual prime factors of n.

=== Definition 2 ===
Let n ∈ℤ be an integer.

The radical of n is the largest square-free integer which divides n.",Radical
['Definitions/Ring Theory'],Definition:Radical,"Let $R$ be a commutative ring with unity.

Let $operatorname{maxspec} left(   right)R$ be the set of maximal ideals of $R$.


Then the Jacobson radical of $R$ is:
:$ds operatorname {Jac}  left(   right)R = bigcap_{m mathop in operatorname{maxspec} left(   right)R} m$

That is, it is the intersection of all maximal ideals of $R$.",Definition:Jacobson Radical,,false,"Let R be a commutative ring with unity.

Let maxspec(   )R be the set of maximal ideals of R.


Then the Jacobson radical of R is:
:Jac(   )R = ⋂_m ∈maxspec(   )R m

That is, it is the intersection of all maximal ideals of R.",Radical
['Definitions/Ring Theory'],Definition:Radical,"Let $A$ be a commutative ring with unity.


=== Definition 1 ===
Let $A$ be a commutative ring with unity.


The nilradical of $A$ is the subset consisting of all nilpotent elements of $A$.

=== Definition 2 ===
Let $A$ be a commutative ring with unity.

Let $mathrm {Spec} left( A right)$ denote the prime spectrum of $A$.


The nilradical of $A$ is:
:$ds mathrm {Nil} left( A right) = bigcap_{mathfrak p mathop in mathrm {Spec} left( A right)} mathfrak p$

That is, it is the intersection of all prime ideals of $A$.",Definition:Nilradical of Ring,,false,"Let A be a commutative ring with unity.


=== Definition 1 ===
Let A be a commutative ring with unity.


The nilradical of A is the subset consisting of all nilpotent elements of A.

=== Definition 2 ===
Let A be a commutative ring with unity.

Let Spec( A ) denote the prime spectrum of A.


The nilradical of A is:
:Nil( A ) = ⋂_𝔭∈Spec( A )𝔭

That is, it is the intersection of all prime ideals of A.",Radical
['Definitions/Ideal Theory'],Definition:Radical,"Let $A$ be a commutative ring with unity.

Let $I$ be an ideal of $A$.


=== Definition 1 ===
Let $A$ be a commutative ring with unity.

Let $I$ be an ideal of $A$.


The radical of $I$ is the ideal of elements of which some power is in $I$:
:$mathrm {Rad} left(   right)I := leftlbrace a in A: exists n in mathbb N_{>0} : a^n in I rightrbrace$

=== Definition 2 ===
Let $A$ be a commutative ring with unity.

Let $I$ be an ideal of $A$.


Let $A / I$ be the quotient ring of $A$ by $I$.

Let $mathrm {Nil} left( A / I right)$ be its nilradical.

Let $pi: A to A / I$ be the quotient epimorphism from $A$ onto $A / I$.


The radical of $I$ is the preimage of $mathrm {Nil} left( A / I right)$ under $pi$:
:$mathrm {Rad} left(   right)I = pi^{-1} left[ mathrm {Nil} left( A / I right)  right]$",Definition:Radical of Ideal of Ring,,false,"Let A be a commutative ring with unity.

Let I be an ideal of A.


=== Definition 1 ===
Let A be a commutative ring with unity.

Let I be an ideal of A.


The radical of I is the ideal of elements of which some power is in I:
:Rad(   )I := { a ∈ A: ∃ n ∈ℕ_>0 : a^n ∈ I }

=== Definition 2 ===
Let A be a commutative ring with unity.

Let I be an ideal of A.


Let A / I be the quotient ring of A by I.

Let Nil( A / I ) be its nilradical.

Let π: A → A / I be the quotient epimorphism from A onto A / I.


The radical of I is the preimage of Nil( A / I ) under π:
:Rad(   )I = π^-1[ Nil( A / I )  ]",Radical
['Definitions/Ideal Theory'],Definition:Radical,"Let $A$ be a commutative ring with unity.

Let $I$ be an ideal of $A$.


Then $I$ is a radical ideal  if and only if  it is equal to its radical.",Definition:Radical Ideal of Ring,,false,"Let A be a commutative ring with unity.

Let I be an ideal of A.


Then I is a radical ideal  if and only if  it is equal to its radical.",Radical
['Definitions/Circles'],Definition:Radius,":

A radius of a circle is a straight line segment whose endpoints are the center and the circumference of the circle.

In the above diagram, the line $AB$ is a radius.",Definition:Circle/Radius,,false,":

A radius of a circle is a straight line segment whose endpoints are the center and the circumference of the circle.

In the above diagram, the line AB is a radius.",Radius
['Definitions/Spheres'],Definition:Radius,"A radius of a sphere is a straight line segment whose endpoints are the center and the surface of the sphere.

The radius of a sphere is the length of one such radius.",Definition:Sphere/Geometry/Radius,,false,"A radius of a sphere is a straight line segment whose endpoints are the center and the surface of the sphere.

The radius of a sphere is the length of one such radius.",Radius
['Definitions/Curvature'],Definition:Radius,"The radius of curvature of a curve $C$ at a point $P$ is defined as the reciprocal of the absolute value of its curvature:

:$rho = dfrac 1 {leftlvert k rightrvert}$


=== Cartesian Coordinates ===
Let $C$ be a curve defined by a real function which is twice differentiable.

Let $C$ be embedded in a cartesian plane.


The radius of curvature of $C$ at a point $P$ can be expressed in cartesian coordinates as:
:$rho = leftlvert dfrac {left( 1 + y'^2 right)^{3/2} } {y}  rightrvert$

where:
 
 
 
 

=== Parametric Cartesian Form ===
Let $C$ be a curve defined by a real function which is twice differentiable.

Let $C$ be embedded in a cartesian plane and defined by the parametric equations:
:$begin{cases} x = x left(   right)t \ y = y left(   right)t end{cases}$


The radius of curvature $rho$ of $C$ at a point $P = left( x, y right)$ is given by:

:$rho = dfrac {left( x'^2 + y'^2 right)^{3/2} } {leftlvert x' y - y' x rightrvert }$
where:
:$x' = dfrac {mathrm d x} {mathrm d t}$ is the derivative of $x$   $t$ at $P$
:$y' = dfrac {mathrm d y} {mathrm d t}$ is the derivative of $y$   $t$ at $P$
:$x$ and $y$ are the second derivatives of $x$ and $y$   $t$ at $P$.",Definition:Radius of Curvature,,false,"The radius of curvature of a curve C at a point P is defined as the reciprocal of the absolute value of its curvature:

:ρ =  1 | k |


=== Cartesian Coordinates ===
Let C be a curve defined by a real function which is twice differentiable.

Let C be embedded in a cartesian plane.


The radius of curvature of C at a point P can be expressed in cartesian coordinates as:
:ρ = |( 1 + y'^2 )^3/2y|

where:
 
 
 
 

=== Parametric Cartesian Form ===
Let C be a curve defined by a real function which is twice differentiable.

Let C be embedded in a cartesian plane and defined by the parametric equations:
:x = x (   )t 
 y = y (   )t


The radius of curvature ρ of C at a point P = ( x, y ) is given by:

:ρ = ( x'^2 + y'^2 )^3/2| x' y - y' x |
where:
:x' = d xd t is the derivative of x   t at P
:y' = d yd t is the derivative of y   t at P
:x and y are the second derivatives of x and y   t at P.",Radius
"['Definitions/Position Vectors', 'Definitions/Displacement', 'Definitions/Vectors']",Definition:Radius,"Let $P$ be a point in a given frame of reference whose origin is $O$.

The position vector $mathbf p$ of $P$ is the displacement vector of $P$ from $O$.


=== Notation ===
",Definition:Position Vector,,false,"Let P be a point in a given frame of reference whose origin is O.

The position vector 𝐩 of P is the displacement vector of P from O.


=== Notation ===
",Radius
['Definitions/Open Balls'],Definition:Radius,"Let $M = left( A, d right)$ be a metric space or pseudometric space.

Let $a in A$.

Let $B_epsilon left(   right)a$ be the open $epsilon$-ball of $a$.


In $B_epsilon left(   right)a$, the value $epsilon$ is referred to as the radius of the open $epsilon$-ball.",Definition:Open Ball/Radius,,false,"Let M = ( A, d ) be a metric space or pseudometric space.

Let a ∈ A.

Let B_ϵ(   )a be the open ϵ-ball of a.


In B_ϵ(   )a, the value ϵ is referred to as the radius of the open ϵ-ball.",Radius
"['Definitions/Power Series', 'Definitions/Convergence']",Definition:Radius,"=== Real Domain ===
Let $xi in mathbb R$ be a real number.

Let $ds S left(   right)x = sum_{n mathop = 0}^infty a_n left( x - xi right)^n$ be a power series about $xi$.

Let $I$ be the interval of convergence of $S left(   right)x$.

Let the endpoints of $I$ be $xi - R$ and $xi + R$.

(This follows from the fact that $xi$ is the midpoint of $I$.)


Then $R$ is called the radius of convergence of $S left(   right)x$.


If $S left(   right)x$ is convergent over the whole of $mathbb R$, then $I = mathbb R$ and thus the radius of convergence is infinite.

=== Complex Domain ===
Let $xi in mathbb C$ be a complex number.

For $z in mathbb C$, let:
:$ds f left(   right)z = sum_{n mathop = 0}^infty a_n left( z - xi right)^n$
be a power series about $xi$.


The radius of convergence is the extended real number $R in overline mathbb R$ defined by:

:$R = ds inf leftlbrace leftlvert z - xi rightrvert: z in mathbb C, sum_{n mathop = 0}^infty a_n left( z - xi right)^n text{ is divergent}  rightrbrace$

where a divergent series is a series that is not convergent.

As usual, $inf varnothing = +infty$.",Definition:Radius of Convergence,,false,"=== Real Domain ===
Let ξ∈ℝ be a real number.

Let S (   )x = ∑_n  = 0^∞ a_n ( x - ξ)^n be a power series about ξ.

Let I be the interval of convergence of S (   )x.

Let the endpoints of I be ξ - R and ξ + R.

(This follows from the fact that ξ is the midpoint of I.)


Then R is called the radius of convergence of S (   )x.


If S (   )x is convergent over the whole of ℝ, then I = ℝ and thus the radius of convergence is infinite.

=== Complex Domain ===
Let ξ∈ℂ be a complex number.

For z ∈ℂ, let:
:f (   )z = ∑_n  = 0^∞ a_n ( z - ξ)^n
be a power series about ξ.


The radius of convergence is the extended real number R ∈R defined by:

:R = inf{| z - ξ|: z ∈ℂ, ∑_n  = 0^∞ a_n ( z - ξ)^n  is divergent}

where a divergent series is a series that is not convergent.

As usual, inf∅ = +∞.",Radius
"['Definitions/Relation Theory', 'Definitions/Mapping Theory', 'Definitions/Ranges (Relation Theory)']",Definition:Range,"Let $mathcal R subseteq S times T$ be a relation, or (usually) a mapping (which is, of course, itself a relation).


The range of $mathcal R$, denoted  is defined as one of two things, depending on the source.

On   it is denoted $mathrm {Rng} left( mathcal R right)$, but this may be non-standard.


=== Range as Codomain ===
Let $mathcal R subseteq S times T$ be a relation, or (usually) a mapping (which is, of course, itself a relation).


The range of $mathcal R$ can be defined as $T$.

As such, it is the same thing as the term codomain of $mathcal R$.

=== Range as Image ===
Let $mathcal R subseteq S times T$ be a relation, or (usually) a mapping (which is, of course, itself a relation).


The range of $mathcal R$ can be defined as:
:$mathrm {Rng} left( mathcal R right) = leftlbrace t in T: exists s in S: left( s, t right) in mathcal R rightrbrace$

Defined like this, it is the same as what is defined as the image of $mathcal R$.",Definition:Range of Relation,,false,"Let ℛ⊆ S × T be a relation, or (usually) a mapping (which is, of course, itself a relation).


The range of ℛ, denoted  is defined as one of two things, depending on the source.

On   it is denoted Rng( ℛ), but this may be non-standard.


=== Range as Codomain ===
Let ℛ⊆ S × T be a relation, or (usually) a mapping (which is, of course, itself a relation).


The range of ℛ can be defined as T.

As such, it is the same thing as the term codomain of ℛ.

=== Range as Image ===
Let ℛ⊆ S × T be a relation, or (usually) a mapping (which is, of course, itself a relation).


The range of ℛ can be defined as:
:Rng( ℛ) = { t ∈ T: ∃ s ∈ S: ( s, t ) ∈ℛ}

Defined like this, it is the same as what is defined as the image of ℛ.",Range
['Definitions/Real Functions'],Definition:Range,"Let $S subseteq mathbb R$.

Let $f: S to mathbb R$ be a real function.

The range of $f$ is the set of values that the dependent variable can take.


That is, it is the image set of $f$.",Definition:Real Function/Range,,false,"Let S ⊆ℝ.

Let f: S →ℝ be a real function.

The range of f is the set of values that the dependent variable can take.


That is, it is the image set of f.",Range
['Definitions/Descriptive Statistics'],Definition:Range,"Let $S$ be a set of observations of a quantitative variable.


The range of $S$ is defined as:
:$R left(   right)S := max left(   right)S - min left(   right)S$

where $max left(   right)S$ and $min left(   right)S$ are the greatest value of $S$ and the least value of $S$ respectively.",Definition:Range (Statistics),,false,"Let S be a set of observations of a quantitative variable.


The range of S is defined as:
:R (   )S := max(   )S - min(   )S

where max(   )S and min(   )S are the greatest value of S and the least value of S respectively.",Range
['Definitions/Axiom of Foundation'],Definition:Rank,"Let $A$ be a set.

Let $V$ denote the von Neumann hierarchy.


Then the rank of $A$ is the smallest ordinal $x$ such that $A in V left(   right){x + 1}$, given that $x$ exists.",Definition:Rank (Set Theory),,false,"Let A be a set.

Let V denote the von Neumann hierarchy.


Then the rank of A is the smallest ordinal x such that A ∈ V (   )x + 1, given that x exists.",Rank
['Definitions/Linear Algebra'],Definition:Rank,"=== Linear Transformation ===
Let $phi$ be a linear transformation from one vector space to another.

Let the image of $phi$ be finite-dimensional.


Then its dimension is called the rank of $phi$ and is denoted $rho left(   right)phi$.

=== Matrix ===
=== Definition 1 ===
Let $K$ be a field.

Let $mathbf A$ be an $m times n$ matrix over $K$.


Then the rank of $mathbf A$, denoted $rho left(   right){mathbf A}$, is the dimension of the subspace of $K^m$ generated by the columns of $mathbf A$.


That is, it is the dimension of the column space of $mathbf A$.

=== Definition 2 ===
Let $K$ be a field.

Let $mathbf A$ be an $m times n$ matrix over $K$.

Let $mathbf A$ be converted to echelon form $mathbf B$.

Let $mathbf B$ have exactly $k$ non-zero rows.

Then the rank of $mathbf A$, denoted $rho left(   right){mathbf A}$, is $k$.

=== Definition 3 ===
Let $K$ be a field.

Let $mathbf A$ be an $m times n$ matrix over $K$.

The rank of $mathbf A$, denoted $rho left(   right){mathbf A}$ is the largest number of elements in a linearly independent set of rows of $mathbf A$.

 
",Definition:Rank (Linear Algebra),,false,"=== Linear Transformation ===
Let ϕ be a linear transformation from one vector space to another.

Let the image of ϕ be finite-dimensional.


Then its dimension is called the rank of ϕ and is denoted ρ(   )ϕ.

=== Matrix ===
=== Definition 1 ===
Let K be a field.

Let 𝐀 be an m × n matrix over K.


Then the rank of 𝐀, denoted ρ(   )𝐀, is the dimension of the subspace of K^m generated by the columns of 𝐀.


That is, it is the dimension of the column space of 𝐀.

=== Definition 2 ===
Let K be a field.

Let 𝐀 be an m × n matrix over K.

Let 𝐀 be converted to echelon form 𝐁.

Let 𝐁 have exactly k non-zero rows.

Then the rank of 𝐀, denoted ρ(   )𝐀, is k.

=== Definition 3 ===
Let K be a field.

Let 𝐀 be an m × n matrix over K.

The rank of 𝐀, denoted ρ(   )𝐀 is the largest number of elements in a linearly independent set of rows of 𝐀.

 
",Rank
"['Definitions/Matrix Theory', 'Definitions/Rank of Matrix']",Definition:Rank,"=== Definition 1 ===
Let $K$ be a field.

Let $mathbf A$ be an $m times n$ matrix over $K$.


Then the rank of $mathbf A$, denoted $rho left(   right){mathbf A}$, is the dimension of the subspace of $K^m$ generated by the columns of $mathbf A$.


That is, it is the dimension of the column space of $mathbf A$.

=== Definition 2 ===
Let $K$ be a field.

Let $mathbf A$ be an $m times n$ matrix over $K$.

Let $mathbf A$ be converted to echelon form $mathbf B$.

Let $mathbf B$ have exactly $k$ non-zero rows.

Then the rank of $mathbf A$, denoted $rho left(   right){mathbf A}$, is $k$.

=== Definition 3 ===
Let $K$ be a field.

Let $mathbf A$ be an $m times n$ matrix over $K$.

The rank of $mathbf A$, denoted $rho left(   right){mathbf A}$ is the largest number of elements in a linearly independent set of rows of $mathbf A$.",Definition:Rank/Matrix,,false,"=== Definition 1 ===
Let K be a field.

Let 𝐀 be an m × n matrix over K.


Then the rank of 𝐀, denoted ρ(   )𝐀, is the dimension of the subspace of K^m generated by the columns of 𝐀.


That is, it is the dimension of the column space of 𝐀.

=== Definition 2 ===
Let K be a field.

Let 𝐀 be an m × n matrix over K.

Let 𝐀 be converted to echelon form 𝐁.

Let 𝐁 have exactly k non-zero rows.

Then the rank of 𝐀, denoted ρ(   )𝐀, is k.

=== Definition 3 ===
Let K be a field.

Let 𝐀 be an m × n matrix over K.

The rank of 𝐀, denoted ρ(   )𝐀 is the largest number of elements in a linearly independent set of rows of 𝐀.",Rank
['Definitions/Linear Algebra'],Definition:Rank,"Let $phi$ be a linear transformation from one vector space to another.

Let the image of $phi$ be finite-dimensional.


Then its dimension is called the rank of $phi$ and is denoted $rho left(   right)phi$.",Definition:Rank/Linear Transformation,,false,"Let ϕ be a linear transformation from one vector space to another.

Let the image of ϕ be finite-dimensional.


Then its dimension is called the rank of ϕ and is denoted ρ(   )ϕ.",Rank
['Definitions/Entire Functions'],Definition:Rank,"Let $f: mathbb C to mathbb C$ be an entire function.

Let $leftlangle a_n rightrangle$ be the sequence of non-zero zeroes of $f$, repeated according to multiplicity.


The rank of $f$ is:
:the smallest positive integer $p ge 0$ for which the series $ds sum_{n mathop = 1}^infty leftlvert a_n rightrvert^{-p - 1}$ converges
or:
:$infty$ if there is no such integer.


If $f$ has finitely many zeroes, its rank is $0$.",Definition:Rank of Entire Function,,false,"Let f: ℂ→ℂ be an entire function.

Let ⟨ a_n ⟩ be the sequence of non-zero zeroes of f, repeated according to multiplicity.


The rank of f is:
:the smallest positive integer p ≥ 0 for which the series ∑_n  = 1^∞| a_n |^-p - 1 converges
or:
:∞ if there is no such integer.


If f has finitely many zeroes, its rank is 0.",Rank
['Definitions/Group Theory'],Definition:Rank,"Let $F$ be a free group.


The rank of $F$ is the dimension of its abelianization as a module over $mathbb Z$.",Definition:Rank of Free Group,,false,"Let F be a free group.


The rank of F is the dimension of its abelianization as a module over ℤ.",Rank
['Definitions/Rankings'],Definition:Rank,"Let $S$ be a set of sample data which has been assigned a ranking $mathcal R$.

The index of a given $x in S$ according to $mathcal R$ is known as its rank.",Definition:Rank (Statistics),,false,"Let S be a set of sample data which has been assigned a ranking ℛ.

The index of a given x ∈ S according to ℛ is known as its rank.",Rank
"['Definitions/Rank Correlation Coefficients', 'Definitions/Correlation Coefficients', 'Definitions/Coefficients (Statistics)']",Definition:Rank,"A rank correlation coefficient is a statistical coefficient which measures the degree of agreement of a set of subjective rankings.


=== Spearman's Rank Correlation Coefficient ===
Let $X$ and $Y$ be two rankings assigned to the same set of entities.

The Spearman's rank correlation coefficient is the Pearson correlation coefficient between $X$ and $Y$.

=== Kendall's Rank Correlation Coefficient ===
Kendall's rank correlation coefficient is a test for consistency of $2$ sets of rankings $leftlangle a rightrangle_n$ and $leftlangle b rightrangle_n$ on a set $S$ of $n$ objects.

The set $R$ of ordered pairs $left( a_i, b_i right)$ is assembled:
:$R = leftlbrace left( a_i, b_i right): i in leftlbrace 1, 2, ldots, n rightrbrace  rightrbrace$
and ordered according to $leftlangle a rightrangle$.

The number $Q$ of elements of $S$ out of ranking order from $leftlangle b rightrangle$ is counted.

Kendall's rank correlation coefficient is then formed:
:$K = 1 - dfrac {4 Q} {n left( n + 1 right) }$
which takes values between $-1$ (complete disagreement) and $+1$ (complete agreement).

Complete disagreement happens when $leftlangle a rightrangle_n$ is in reverse order to $leftlangle b rightrangle_n$.

=== Kendall's Coefficient of Concordance ===
Kendall's coefficient of concordance is a test for consistency of more than $2$ sets of rankings.

Let $m$ judges independently award ranks $1$ to $n$ to a set of $n$ competitors.

Let $s_i$ be the sum of the rankings awarded to competitor $i$.

The mean $M$ of the values of $s_i$ is $dfrac 1 2 m left( n + 1 right)$.

The sum of the squares of the deviations from $M$ is given by:
:$S = ds sum_{i mathop = 1}^n left( s_i - M right)^2$

and Kendall's coefficient of concordance is given by:
:$W = dfrac {12 S} {m^2 n left( n^2 - 1 right) }$


=== Kendall's Coefficient of Concordance is between $0$ and $1$ ===
 ",Definition:Rank Correlation Coefficient,,false,"A rank correlation coefficient is a statistical coefficient which measures the degree of agreement of a set of subjective rankings.


=== Spearman's Rank Correlation Coefficient ===
Let X and Y be two rankings assigned to the same set of entities.

The Spearman's rank correlation coefficient is the Pearson correlation coefficient between X and Y.

=== Kendall's Rank Correlation Coefficient ===
Kendall's rank correlation coefficient is a test for consistency of 2 sets of rankings ⟨ a ⟩_n and ⟨ b ⟩_n on a set S of n objects.

The set R of ordered pairs ( a_i, b_i ) is assembled:
:R = {( a_i, b_i ): i ∈{ 1, 2, …, n }}
and ordered according to ⟨ a ⟩.

The number Q of elements of S out of ranking order from ⟨ b ⟩ is counted.

Kendall's rank correlation coefficient is then formed:
:K = 1 - 4 Qn ( n + 1 )
which takes values between -1 (complete disagreement) and +1 (complete agreement).

Complete disagreement happens when ⟨ a ⟩_n is in reverse order to ⟨ b ⟩_n.

=== Kendall's Coefficient of Concordance ===
Kendall's coefficient of concordance is a test for consistency of more than 2 sets of rankings.

Let m judges independently award ranks 1 to n to a set of n competitors.

Let s_i be the sum of the rankings awarded to competitor i.

The mean M of the values of s_i is 1 2 m ( n + 1 ).

The sum of the squares of the deviations from M is given by:
:S = ∑_i  = 1^n ( s_i - M )^2

and Kendall's coefficient of concordance is given by:
:W = 12 Sm^2 n ( n^2 - 1 )


=== Kendall's Coefficient of Concordance is between 0 and 1 ===
 ",Rank
"['Definitions/Well-Founded Relations', 'Definitions/Relation Theory']",Definition:Rank Function,"Let $left( S, mathcal R right)$ be a relational structure.

Let $left( T, prec right)$ be a strictly well-ordered set.

Let $operatorname {rk}: S to T$ be a mapping such that:
:$forall x, y in S: left( x ne y text { and } left( x, y right) in mathcal R right) implies operatorname {rk}  left(   right)x prec operatorname {rk}  left(   right)y$


$operatorname {rk}$ is known as a rank function for $mathcal R$.",Definition:Rank Function for Relation,,false,"Let ( S, ℛ) be a relational structure.

Let ( T, ≺) be a strictly well-ordered set.

Let rk: S → T be a mapping such that:
:∀ x, y ∈ S: ( x  y  and ( x, y ) ∈ℛ) rk(   )x ≺rk(   )y


rk is known as a rank function for ℛ.",Rank Function
['Definitions/Matroid Theory'],Definition:Rank Function,"Let $M = left( S, mathscr I right)$ be a matroid.


The rank function of $M$ is the mapping $rho : mathcal P left( S right) to mathbb Z$ from the power set of $S$ into the integers defined by:
:$forall A subseteq S : rho left(   right)A = max leftlbrace leftlvert X rightrvert : X subseteq A land X in mathscr I rightrbrace$
where $leftlvert A rightrvert$ denotes the cardinality of $A$.",Definition:Rank Function (Matroid),,false,"Let M = ( S, ℐ) be a matroid.


The rank function of M is the mapping ρ : 𝒫( S ) →ℤ from the power set of S into the integers defined by:
:∀ A ⊆ S : ρ(   )A = max{| X | : X ⊆ A  X ∈ℐ}
where | A | denotes the cardinality of A.",Rank Function
"['Definitions/Rational Numbers', 'Definitions/Standard Number Fields', 'Definitions/Numbers', 'Definitions/Fields of Quotients', 'Definitions/Field Theory']",Definition:Rational,"A number in the form $dfrac p q$, where both $p$ and $q$ are integers ($q$ non-zero), is called a rational number.

The set of all rational numbers is usually denoted $mathbb Q$.


Thus:
:$mathbb Q = leftlbrace dfrac p q: p in mathbb Z, q in mathbb Z_{ne 0}  rightrbrace$


=== Formal Definition ===
The field $left( mathbb Q, +, times right)$ of rational numbers is the field of quotients of the integral domain $left( mathbb Z, +, times right)$ of integers.

This is shown to exist in Existence of Field of Quotients.


In view of Field of Quotients is Unique, we construct the field of quotients of $mathbb Z$, give it a label $mathbb Q$ and call its elements rational numbers.

=== Canonical Form of Rational Number ===
Let $r in mathbb Q$ be a rational number.

The canonical form of $r$ is the expression $dfrac p q$, where:
:$r = dfrac p q: p in mathbb Z, q in mathbb Z_{>0}, p perp q$
where $p perp q$ denotes that $p$ and $q$ have no common divisor except $1$.


That is, in its canonical form, $r$ is expressed as $dfrac p q$ where:

:$p$ is an integer
:$q$ is a strictly positive integer
:$p$ and $q$ are coprime.

=== Geometrical Definition ===
The definitions of rational numbers and irrational numbers as specified in   is different from the contemporary definitions:

 
: 
:With these hypotheses, it is proved that there exist straight lines infinite in multitude which are commensurable and incommensurable respectively, some in length only, and others in square also, with an assigned straight line. Let then the assigned straight line be called rational, and those straight lines which are commensurable with it, whether in length and in square or square only, rational, but those which are incommensurable with it irrational.
 ''
 
: 
: And let the square on the assigned straight line be called rational and those areas which are commensurable with it rational, but those which are incommensurable with it irrational, and the straight lines which produce them irrational, that is, in case the areas are squares, the sides themselves, but in case they are any other rectilineal figures, the straight lines on which are described squares equal to them.
 ''
 ",Definition:Rational Number,,false,"A number in the form p q, where both p and q are integers (q non-zero), is called a rational number.

The set of all rational numbers is usually denoted ℚ.


Thus:
:ℚ = { p q: p ∈ℤ, q ∈ℤ_ 0}


=== Formal Definition ===
The field ( ℚ, +, ×) of rational numbers is the field of quotients of the integral domain ( ℤ, +, ×) of integers.

This is shown to exist in Existence of Field of Quotients.


In view of Field of Quotients is Unique, we construct the field of quotients of ℤ, give it a label ℚ and call its elements rational numbers.

=== Canonical Form of Rational Number ===
Let r ∈ℚ be a rational number.

The canonical form of r is the expression p q, where:
:r =  p q: p ∈ℤ, q ∈ℤ_>0, p ⊥ q
where p ⊥ q denotes that p and q have no common divisor except 1.


That is, in its canonical form, r is expressed as p q where:

:p is an integer
:q is a strictly positive integer
:p and q are coprime.

=== Geometrical Definition ===
The definitions of rational numbers and irrational numbers as specified in   is different from the contemporary definitions:

 
: 
:With these hypotheses, it is proved that there exist straight lines infinite in multitude which are commensurable and incommensurable respectively, some in length only, and others in square also, with an assigned straight line. Let then the assigned straight line be called rational, and those straight lines which are commensurable with it, whether in length and in square or square only, rational, but those which are incommensurable with it irrational.
 ”
 
: 
: And let the square on the assigned straight line be called rational and those areas which are commensurable with it rational, but those which are incommensurable with it irrational, and the straight lines which produce them irrational, that is, in case the areas are squares, the sides themselves, but in case they are any other rectilineal figures, the straight lines on which are described squares equal to them.
 ”
 ",Rational
"['Definitions/Rational Functions', 'Definitions/Polynomial Theory', 'Definitions/Analysis']",Definition:Rational,"Let $F$ be a field.

Let $P: F to F$ and $Q: F to F$ be polynomial functions on $F$.

Let $S$ be the set $F$ from which all the roots of $Q$ have been removed.

That is:
:$S = F setminus leftlbrace x in F: Q left(   right)x = 0 rightrbrace$


Then the equation $y = dfrac {P left(   right)x} {Q left(   right)x}$ defines a mapping from $S$ to $F$.


Such a mapping is called a rational function.


The concept is usually encountered where the polynomial functions $P$ and $Q$ are either real or complex:


=== Real Domain ===
Let $P: mathbb R to mathbb R$ and $Q: mathbb R to mathbb R$ be polynomial functions on the set of real numbers.

Let $S$ be the set $mathbb R$ from which all the roots of $Q$ have been removed.

That is:
:$S = mathbb R setminus leftlbrace x in mathbb R: Q left(   right)x = 0 rightrbrace$.


Then the equation $y = dfrac {P left(   right)x} {Q left(   right)x}$ defines a function from $S$ to $mathbb R$.


Such a function is known as a rational function.

=== Complex Domain ===
Let $P: mathbb C to mathbb C$ and $Q: mathbb C to mathbb C$ be polynomial functions on the set of complex numbers.

Let $S$ be the set $mathbb C$ from which all the roots of $Q$ have been removed.

That is:
:$S = mathbb C setminus leftlbrace z in mathbb C: Q left(   right)z = 0 rightrbrace$


Then the equation $y = dfrac {P left(   right)z} {Q left(   right)z}$ defines a function from $S$ to $mathbb C$.


Such a function is a rational (algebraic) function.",Definition:Rational Function,,false,"Let F be a field.

Let P: F → F and Q: F → F be polynomial functions on F.

Let S be the set F from which all the roots of Q have been removed.

That is:
:S = F ∖{ x ∈ F: Q (   )x = 0 }


Then the equation y = P (   )xQ (   )x defines a mapping from S to F.


Such a mapping is called a rational function.


The concept is usually encountered where the polynomial functions P and Q are either real or complex:


=== Real Domain ===
Let P: ℝ→ℝ and Q: ℝ→ℝ be polynomial functions on the set of real numbers.

Let S be the set ℝ from which all the roots of Q have been removed.

That is:
:S = ℝ∖{ x ∈ℝ: Q (   )x = 0 }.


Then the equation y = P (   )xQ (   )x defines a function from S to ℝ.


Such a function is known as a rational function.

=== Complex Domain ===
Let P: ℂ→ℂ and Q: ℂ→ℂ be polynomial functions on the set of complex numbers.

Let S be the set ℂ from which all the roots of Q have been removed.

That is:
:S = ℂ∖{ z ∈ℂ: Q (   )z = 0 }


Then the equation y = P (   )zQ (   )z defines a function from S to ℂ.


Such a function is a rational (algebraic) function.",Rational
"['Definitions/Infinite Half-Lines', 'Definitions/Lines']",Definition:Ray,"An infinite half-line is a line which terminates at an endpoint at one end, but has no such endpoint at the other.


=== Start Point ===
The endpoint of an infinite half-line is a known as its start point.


Category:Definitions/Lines",Definition:Line/Infinite Half-Line,,false,"An infinite half-line is a line which terminates at an endpoint at one end, but has no such endpoint at the other.


=== Start Point ===
The endpoint of an infinite half-line is a known as its start point.


Category:Definitions/Lines",Ray
['Definitions/Topology'],Definition:Ray,"Let $T = left( S, tau right)$ be a topological space.


A ray in $T$ is an embedding $R_{>0} to S$.

Category:Definitions/Topology",Definition:Ray (Topology),,false,"Let T = ( S, τ) be a topological space.


A ray in T is an embedding R_>0→ S.

Category:Definitions/Topology",Ray
"['Definitions/Order Theory', 'Definitions/Rays (Order Theory)']",Definition:Ray,"Let $left( S, preccurlyeq right)$ be a totally ordered set.

Let $prec$ be the reflexive reduction of $preccurlyeq$.

Let $a in S$ be any point in $S$.


=== Open Ray ===
Let $left( S, preccurlyeq right)$ be a totally ordered set.

Let $prec$ be the reflexive reduction of $preccurlyeq$.

Let $a in S$ be any point in $S$.


The following sets are called open rays or open half-lines:

:$leftlbrace x in S: a prec x rightrbrace$ (the strict upper closure of $a$), denoted $a^succ$
:$leftlbrace x in S: x prec a rightrbrace$ (the strict lower closure of $a$), denoted $a^prec$.

=== Closed Ray ===
Let $left( S, preccurlyeq right)$ be a totally ordered set.

Let $a in S$ be any point in $S$.


The following sets are called closed rays or closed half-lines:

:$leftlbrace x in S: a preccurlyeq x rightrbrace$ (the upper closure of $a$), denoted $a^succcurlyeq$
:$leftlbrace x in S: x preccurlyeq a rightrbrace$ (the lower closure of $a$), denoted $a^preccurlyeq$.

=== Upward-Pointing Ray ===
Let $left( S, preccurlyeq right)$ be a totally ordered set.

Let $prec$ be the reflexive reduction of $preccurlyeq$.

Let $a in S$ be any point in $S$.


An upward-pointing ray is a ray which is bounded below:

:an open ray $a^succ:= leftlbrace x in S: a prec x rightrbrace$
:a closed ray $a^succcurlyeq: leftlbrace x in S: a preccurlyeq x rightrbrace$

=== Downward-Pointing Ray ===
Let $left( S, preccurlyeq right)$ be a totally ordered set.

Let $prec$ be the reflexive reduction of $preccurlyeq$.

Let $a in S$ be any point in $S$.


A downward-pointing ray is a ray which is bounded above:

:an open ray $a^prec := leftlbrace x in S: x prec a rightrbrace$
:a closed ray $a^preccurlyeq : leftlbrace x in S: x preccurlyeq a rightrbrace$",Definition:Ray (Order Theory),,false,"Let ( S, ≼) be a totally ordered set.

Let ≺ be the reflexive reduction of ≼.

Let a ∈ S be any point in S.


=== Open Ray ===
Let ( S, ≼) be a totally ordered set.

Let ≺ be the reflexive reduction of ≼.

Let a ∈ S be any point in S.


The following sets are called open rays or open half-lines:

:{ x ∈ S: a ≺ x } (the strict upper closure of a), denoted a^≻
:{ x ∈ S: x ≺ a } (the strict lower closure of a), denoted a^≺.

=== Closed Ray ===
Let ( S, ≼) be a totally ordered set.

Let a ∈ S be any point in S.


The following sets are called closed rays or closed half-lines:

:{ x ∈ S: a ≼ x } (the upper closure of a), denoted a^≽
:{ x ∈ S: x ≼ a } (the lower closure of a), denoted a^≼.

=== Upward-Pointing Ray ===
Let ( S, ≼) be a totally ordered set.

Let ≺ be the reflexive reduction of ≼.

Let a ∈ S be any point in S.


An upward-pointing ray is a ray which is bounded below:

:an open ray a^≻:= { x ∈ S: a ≺ x }
:a closed ray a^≽: { x ∈ S: a ≼ x }

=== Downward-Pointing Ray ===
Let ( S, ≼) be a totally ordered set.

Let ≺ be the reflexive reduction of ≼.

Let a ∈ S be any point in S.


A downward-pointing ray is a ray which is bounded above:

:an open ray a^≺ := { x ∈ S: x ≺ a }
:a closed ray a^≼ : { x ∈ S: x ≼ a }",Ray
['Definitions/Stochastic Processes'],Definition:Realization,"Let $S$ be a stochastic process.

Let $T$ be a time series of observations of $S$ which has been acquired as $S$ evolves, according to the underlying probability distribution of $S$.


Then $T$ is referred to as a realization of $S$.


Thus we can regard the observation $z_t$ at some timestamp $t$, for example $t = 25$, as the realization of a random variable with probability mass function $p left(   right){z_t}$.

Similarly the observations $z_{t_1}$ and $z_{t_2}$ at times $t_1$ and $t_2$ can be regarded as the realizations of two random variables with joint probability mass function $p left(   right){z_{t_1} }$ and $p left(   right){z_{t_2} }$.


Similarly, the observations making an equispaced time series can be described by an $N$-dimensional random variable $left( z_1, z_2, dotsc, z_N right)$ with associated probability mass function $p left(   right){z_1, z_2, dotsc, z_N}$.",Definition:Realization of Stochastic Process,,false,"Let S be a stochastic process.

Let T be a time series of observations of S which has been acquired as S evolves, according to the underlying probability distribution of S.


Then T is referred to as a realization of S.


Thus we can regard the observation z_t at some timestamp t, for example t = 25, as the realization of a random variable with probability mass function p (   )z_t.

Similarly the observations z_t_1 and z_t_2 at times t_1 and t_2 can be regarded as the realizations of two random variables with joint probability mass function p (   )z_t_1 and p (   )z_t_2.


Similarly, the observations making an equispaced time series can be described by an N-dimensional random variable ( z_1, z_2, …, z_N ) with associated probability mass function p (   )z_1, z_2, …, z_N.",Realization
['Definitions/Model Theory for Predicate Logic'],Definition:Realization,"Let $mathcal M$ be an $mathcal L$-structure.

Let $A$ be a subset of the universe of $mathcal M$.

Let $mathcal L_A$ be the language consisting of $mathcal L$ along with constant symbols for each element of $A$.

Viewing $mathcal M$ as an $mathcal L_A$-structure by interpreting each new constant as the element for which it is named, let $operatorname {Th}_A left(   right)mathcal M$ be the set of $mathcal L_A$-sentences satisfied by $mathcal M$.


An $n$-type over $A$ is a set $p$ of $mathcal L_A$-formulas in $n$ free variables such that $p cup operatorname {Th}_A left(   right)mathcal M$ is satisfiable by some $mathcal L_A$-structure.

 


=== Complete Type ===

We say that an $n$-type $p$ is complete (over $A$)  if and only if :
:for every $mathcal L_A$-formula $phi$ in $n$ free variables, either $phi in p$ or $phi notin p$.


The set of complete $n$-types over $A$ is often denoted by $S_n^mathcal M left(   right)A$.


Given an $n$-tuple $bar b$ of elements from $mathcal M$, the type of $bar b$ over $A$ is the complete $n$-type consisting of those $mathcal L_A$-formulas $phi left(   right){x_1, dotsc, x_n}$ such that $mathcal M models phi left(   right){bar b}$.

It is often denoted by $operatorname {tp}^mathcal M left(   right){bar b / A}$.


=== Realization ===

Given an $mathcal L_A$-structure $mathcal N$, a type $p$ is realized by an element $bar b$ of $mathcal N$  if and only if :
:$forall phi in p: mathcal N models phi left(   right){bar b}$.

Such an element $bar b$ of $mathcal N$ is a realization of $p$.


=== Omission ===

We say that $mathcal N$ omits $p$  if and only if  $p$ is not realized in $mathcal N$.

Then $p$ is an omission from $mathcal N$.",Definition:Type,,false,"Let ℳ be an ℒ-structure.

Let A be a subset of the universe of ℳ.

Let ℒ_A be the language consisting of ℒ along with constant symbols for each element of A.

Viewing ℳ as an ℒ_A-structure by interpreting each new constant as the element for which it is named, let Th_A (   )ℳ be the set of ℒ_A-sentences satisfied by ℳ.


An n-type over A is a set p of ℒ_A-formulas in n free variables such that p ∪Th_A (   )ℳ is satisfiable by some ℒ_A-structure.

 


=== Complete Type ===

We say that an n-type p is complete (over A)  if and only if :
:for every ℒ_A-formula ϕ in n free variables, either ϕ∈ p or ϕ∉ p.


The set of complete n-types over A is often denoted by S_n^ℳ(   )A.


Given an n-tuple b̅ of elements from ℳ, the type of b̅ over A is the complete n-type consisting of those ℒ_A-formulas ϕ(   )x_1, …, x_n such that ℳϕ(   )b̅.

It is often denoted by tp^ℳ(   )b̅ / A.


=== Realization ===

Given an ℒ_A-structure 𝒩, a type p is realized by an element b̅ of 𝒩  if and only if :
:∀ϕ∈ p: 𝒩ϕ(   )b̅.

Such an element b̅ of 𝒩 is a realization of p.


=== Omission ===

We say that 𝒩 omits p  if and only if  p is not realized in 𝒩.

Then p is an omission from 𝒩.",Realization
"['Definitions/Reducible Fractions', 'Definitions/Vulgar Fractions', 'Definitions/Fractions']",Definition:Reducible,"Let $q = dfrac a b$ be a vulgar fraction.

Then $q$ is defined as being reducible  if and only if  $q$ is not in canonical form.

That is,  if and only if  there exists $r in mathbb Z: r ne 1$ such that $r$ is a divisor of both $a$ and $b$.

Such a fraction can therefore be reduced by dividing both $a$ and $b$ by $r$.",Definition:Reducible Fraction,,false,"Let q =  a b be a vulgar fraction.

Then q is defined as being reducible  if and only if  q is not in canonical form.

That is,  if and only if  there exists r ∈ℤ: r  1 such that r is a divisor of both a and b.

Such a fraction can therefore be reduced by dividing both a and b by r.",Reducible
"['Definitions/Reducible Polynomials', 'Definitions/Factorization', 'Definitions/Polynomial Theory']",Definition:Reducible,"=== Definition 1 ===
Let $K$ be a field.


A reducible polynomial over $K$ is a nonconstant polynomial over $K$ that can be expressed as the product of two polynomials over $K$ of smaller degree.

=== Definition 2 ===
Let $K$ be a field.


A reducible polynomial over $K$ is a polynomial over $K$ that can be expressed as the product of two nonconstant polynomials.",Definition:Reducible Polynomial,,false,"=== Definition 1 ===
Let K be a field.


A reducible polynomial over K is a nonconstant polynomial over K that can be expressed as the product of two polynomials over K of smaller degree.

=== Definition 2 ===
Let K be a field.


A reducible polynomial over K is a polynomial over K that can be expressed as the product of two nonconstant polynomials.",Reducible
['Definitions/Representation Theory'],Definition:Reducible,"Let $rho: G to mathrm {GL} left( V right)$ be a linear representation.


$rho$ is reducible  if and only if  there exists a non-trivial proper vector subspace $W$ of $V$ such that:
:$forall g in G: rho left(   right)g left(   right)W subseteq W$


That is, such that $W$ is invariant for every linear operator in the set $leftlbrace rho left(   right)g: g in G rightrbrace$.",Definition:Reducible Linear Representation,,false,"Let ρ: G →GL( V ) be a linear representation.


ρ is reducible  if and only if  there exists a non-trivial proper vector subspace W of V such that:
:∀ g ∈ G: ρ(   )g (   )W ⊆ W


That is, such that W is invariant for every linear operator in the set {ρ(   )g: g ∈ G }.",Reducible
['Definitions/Representation Theory'],Definition:Reducible,"Let $M$ be a $G$-module.

Then $M$ is reducible  if and only if  the corresponding linear representation is reducible.",Definition:Reducible G-Module,,false,"Let M be a G-module.

Then M is reducible  if and only if  the corresponding linear representation is reducible.",Reducible
"['Definitions/Mapping Reductions', 'Definitions/Turing Machines']",Definition:Reducible,"Let $unicode{x3a3}, unicode{x3a3}'$ be finite sets.

Let:

 
 
 
 

be sets of finite strings over $unicode{x3a3}$ and $unicode{x3a3}'$ respectively, where:
:$unicode{x3a3}^*$ denotes the set of all finite strings over the alphabet $unicode{x3a3}$.

Let $f : unicode{x3a3}^* to unicode{x3a3}'^*$ be a computable function such that, for all $w in unicode{x3a3}^*$:
:$w in L iff f left(   right)w in L'$

Then, $f$ is a mapping reduction from $L$ to $L'$.


If any such $f$ exists, then $L$ is mapping reducible to $L'$, which is denoted as:
:$L le_m L'$",Definition:Mapping Reduction,,false,"Let x3a3, x3a3' be finite sets.

Let:

 
 
 
 

be sets of finite strings over x3a3 and x3a3' respectively, where:
:x3a3^* denotes the set of all finite strings over the alphabet x3a3.

Let f : x3a3^* →x3a3'^* be a computable function such that, for all w ∈x3a3^*:
:w ∈ L  f (   )w ∈ L'

Then, f is a mapping reduction from L to L'.


If any such f exists, then L is mapping reducible to L', which is denoted as:
:L ≤_m L'",Reducible
['Definitions/Cardinals'],Definition:Regular,"Let $kappa$ be an infinite cardinal.


Then $kappa$ is a regular cardinal  if and only if :
:$mathrm {cf}  left(   right)kappa = kappa$

That is,  if and only if  the cofinality of $kappa$ is equal to itself.

 ",Definition:Regular Cardinal,,false,"Let κ be an infinite cardinal.


Then κ is a regular cardinal  if and only if :
:cf(   )κ = κ

That is,  if and only if  the cofinality of κ is equal to itself.

 ",Regular
['Definitions/Analytic Complex Functions'],Definition:Regular,"Let $U subset mathbb C$ be an open set.

Let $f : U to mathbb C$ be a complex function.


Then $f$ is analytic in $U$  if and only if  for every $z_0 in U$ there exists a sequence $leftlangle a_n rightrangle: mathbb N to mathbb C$ such that the series:
:$ds sum_{n mathop = 0}^infty a_n left( z - z_0 right)^n$
converges to $f left(   right)z$ in a neighborhood of $z_0$ in $U$.",Definition:Analytic Function/Complex Plane,,false,"Let U ⊂ℂ be an open set.

Let f : U →ℂ be a complex function.


Then f is analytic in U  if and only if  for every z_0 ∈ U there exists a sequence ⟨ a_n ⟩: ℕ→ℂ such that the series:
:∑_n  = 0^∞ a_n ( z - z_0 )^n
converges to f (   )z in a neighborhood of z_0 in U.",Regular
"['Definitions/Inverse Matrices', 'Definitions/Matrices']",Definition:Regular,"Let $left( R, +, circ right)$ be a ring with unity.

Let $n in mathbb Z_{>0}$ be a (strictly) positive integer.

Let $mathbf A$ be an element of the ring of square matrices $left( mathcal M_R left(   right)n, +, times right)$.


Then $mathbf A$ is invertible  if and only if :
:$exists mathbf B in left( mathcal M_R left(   right)n, +, times right): mathbf A mathbf B = mathbf I_n = mathbf B mathbf A$
where $mathbf I_n$ denotes the unit matrix of order $n$.


Such a $mathbf B$ is the inverse of $mathbf A$.

It is usually denoted $mathbf A^{-1}$.",Definition:Invertible Matrix,,false,"Let ( R, +, ∘) be a ring with unity.

Let n ∈ℤ_>0 be a (strictly) positive integer.

Let 𝐀 be an element of the ring of square matrices ( ℳ_R (   )n, +, ×).


Then 𝐀 is invertible  if and only if :
:∃𝐁∈( ℳ_R (   )n, +, ×): 𝐀𝐁 = 𝐈_n = 𝐁𝐀
where 𝐈_n denotes the unit matrix of order n.


Such a 𝐁 is the inverse of 𝐀.

It is usually denoted 𝐀^-1.",Regular
"['Definitions/Abstract Algebra', 'Definitions/Group Theory', 'Definitions/Regular Representations']",Definition:Regular,"Let $left( S, circ right)$ be a magma.


=== Left Regular Representation ===
Let $left( S, circ right)$ be a magma.

The mapping $lambda_a: S to S$ is defined as:

:$forall x in S: lambda_a left(   right)x = a circ x$


This is known as the left regular representation of $left( S, circ right)$ with respect to $a$.

=== Right Regular Representation ===
Let $left( S, circ right)$ be a magma.

The mapping $rho_a: S to S$ is defined as:

:$forall x in S: rho_a left(   right)x = x circ a$


This is known as the right regular representation of $left( S, circ right)$ with respect to $a$.",Definition:Regular Representations,,false,"Let ( S, ∘) be a magma.


=== Left Regular Representation ===
Let ( S, ∘) be a magma.

The mapping λ_a: S → S is defined as:

:∀ x ∈ S: λ_a (   )x = a ∘ x


This is known as the left regular representation of ( S, ∘) with respect to a.

=== Right Regular Representation ===
Let ( S, ∘) be a magma.

The mapping ρ_a: S → S is defined as:

:∀ x ∈ S: ρ_a (   )x = x ∘ a


This is known as the right regular representation of ( S, ∘) with respect to a.",Regular
"['Definitions/Left Regular Representation', 'Definitions/Regular Representations']",Definition:Regular,"Let $left( S, circ right)$ be a magma.

The mapping $lambda_a: S to S$ is defined as:

:$forall x in S: lambda_a left(   right)x = a circ x$


This is known as the left regular representation of $left( S, circ right)$ with respect to $a$.",Definition:Regular Representations/Left Regular Representation,,false,"Let ( S, ∘) be a magma.

The mapping λ_a: S → S is defined as:

:∀ x ∈ S: λ_a (   )x = a ∘ x


This is known as the left regular representation of ( S, ∘) with respect to a.",Regular
"['Definitions/Right Regular Representation', 'Definitions/Regular Representations']",Definition:Regular,"Let $left( S, circ right)$ be a magma.

The mapping $rho_a: S to S$ is defined as:

:$forall x in S: rho_a left(   right)x = x circ a$


This is known as the right regular representation of $left( S, circ right)$ with respect to $a$.",Definition:Regular Representations/Right Regular Representation,,false,"Let ( S, ∘) be a magma.

The mapping ρ_a: S → S is defined as:

:∀ x ∈ S: ρ_a (   )x = x ∘ a


This is known as the right regular representation of ( S, ∘) with respect to a.",Regular
['Definitions/Ring Theory'],Definition:Regular,"Let $A$ be a commutative ring with unity.

Let $a in A$.


Then $a$ is regular  if and only if  it is not a zero divisor.",Definition:Regular Element of Ring,,false,"Let A be a commutative ring with unity.

Let a ∈ A.


Then a is regular  if and only if  it is not a zero divisor.",Regular
"['Definitions/Polygons', 'Definitions/Regular Polygons']",Definition:Regular,"A regular polygon is a polygon which is both equilateral and equiangular.

That is, in which all the sides are the same length, and all the vertices have the same angle:

:


=== Center ===
The center of a regular polygon $P$ is defined as the point which is the center of the circumcircle of $P$.

:300px

In the above, $O$ is the center of the regular polygon.

=== Long Radius ===
The long radius of a regular polygon $P$ is defined as the distance from the center of $P$ to one of its vertices.

:

In the above, the length of $OA$ is the long radius of the regular polygon.

=== Apothem ===
The apothem of a regular polygon $P$ is defined as the perpendicular distance from the center of $P$ to one of its sides.

:

In the above, the length of $OM$ is the apothem of the regular polygon.",Definition:Polygon/Regular,,false,"A regular polygon is a polygon which is both equilateral and equiangular.

That is, in which all the sides are the same length, and all the vertices have the same angle:

:


=== Center ===
The center of a regular polygon P is defined as the point which is the center of the circumcircle of P.

:300px

In the above, O is the center of the regular polygon.

=== Long Radius ===
The long radius of a regular polygon P is defined as the distance from the center of P to one of its vertices.

:

In the above, the length of OA is the long radius of the regular polygon.

=== Apothem ===
The apothem of a regular polygon P is defined as the perpendicular distance from the center of P to one of its sides.

:

In the above, the length of OM is the apothem of the regular polygon.",Regular
['Definitions/Regular Polygons'],Definition:Regular,"A regular pentagon is a pentagon which is both equilateral and equiangular.

That is, a regular polygon with $5$ sides.

That is, a pentagon in which all the sides are the same length, and all the vertices have the same angle:

:",Definition:Pentagon/Regular,,false,"A regular pentagon is a pentagon which is both equilateral and equiangular.

That is, a regular polygon with 5 sides.

That is, a pentagon in which all the sides are the same length, and all the vertices have the same angle:

:",Regular
['Definitions/Regular Polygons'],Definition:Regular,"A regular hexagon is a hexagon which is both equilateral and equiangular.

That is, a regular polygon with $6$ sides.

That is, a hexagon in which all the sides are the same length, and all the vertices have the same angle:

:",Definition:Hexagon/Regular,,false,"A regular hexagon is a hexagon which is both equilateral and equiangular.

That is, a regular polygon with 6 sides.

That is, a hexagon in which all the sides are the same length, and all the vertices have the same angle:

:",Regular
['Definitions/Polyhedra'],Definition:Regular,"A regular polyhedron is a polyhedron:
:$(1): quad$ whose faces are congruent regular polygons
:$(2): quad$ each of whose vertices is the common vertex of the same number of faces.",Definition:Regular Polyhedron,,false,"A regular polyhedron is a polyhedron:
:(1): whose faces are congruent regular polygons
:(2): each of whose vertices is the common vertex of the same number of faces.",Regular
['Definitions/Topology'],Definition:Regular,"Let $T$ be a topological space.

Let $A subseteq T$.


Then $A$ is regular open in $T$  if and only if :
:$A = A^{- circ}$

That is,  if and only if  $A$ equals the interior of its closure.",Definition:Regular Open Set,,false,"Let T be a topological space.

Let A ⊆ T.


Then A is regular open in T  if and only if :
:A = A^- ∘

That is,  if and only if  A equals the interior of its closure.",Regular
['Definitions/Topology'],Definition:Regular,"Let $T$ be a topological space.

Let $A subseteq T$.


Then $A$ is regular closed in $T$  if and only if :
:$A = A^{circ -}$

That is,  if and only if  $A$ equals the closure of its interior.",Definition:Regular Closed Set,,false,"Let T be a topological space.

Let A ⊆ T.


Then A is regular closed in T  if and only if :
:A = A^∘ -

That is,  if and only if  A equals the closure of its interior.",Regular
['Definitions/Topology'],Definition:Regular,"Let $X$ and $Y$ be smooth manifolds.

Let $f: X to Y$ be a smooth mapping.


Then a point $y in Y$ is called a regular value of $f$  if and only if  the pushforward of $f$ at $x$:
: $f_* vert_x: T_x X to T_y Y$
 
is surjective for every $x in f^{-1}  left(   right)y subseteq X$.",Definition:Regular Value,,false,"Let X and Y be smooth manifolds.

Let f: X → Y be a smooth mapping.


Then a point y ∈ Y is called a regular value of f  if and only if  the pushforward of f at x:
: f_* |_x: T_x X → T_y Y
 
is surjective for every x ∈ f^-1(   )y ⊆ X.",Regular
"['Definitions/Regular Spaces', 'Definitions/T3 Spaces', 'Definitions/T0 Spaces', 'Definitions/Separation Axioms']",Definition:Regular,"Let $T = left( S, tau right)$ be a topological space.


$left( S, tau right)$ is a regular space  if and only if :
:$left( S, tau right)$ is a $T_3$ space
:$left( S, tau right)$ is a $T_0$ (Kolmogorov) space.


That is:
:$forall F subseteq S: complement_{S} left(   right)F in tau, y in complement_{S} left(   right)F: exists U, V in tau: F subseteq U, y in V: U cap V = varnothing$ 

:$forall x, y in S$, either:
::$exists U in tau: x in U, y notin U$
::$exists U in tau: y in U, x notin U$

 ",Definition:Regular Space,,false,"Let T = ( S, τ) be a topological space.


( S, τ) is a regular space  if and only if :
:( S, τ) is a T_3 space
:( S, τ) is a T_0 (Kolmogorov) space.


That is:
:∀ F ⊆ S: ∁_S(   )F ∈τ, y ∈∁_S(   )F: ∃ U, V ∈τ: F ⊆ U, y ∈ V: U ∩ V = ∅ 

:∀ x, y ∈ S, either:
::∃ U ∈τ: x ∈ U, y ∉ U
::∃ U ∈τ: y ∈ U, x ∉ U

 ",Regular
"['Definitions/Tychonoff Spaces', 'Definitions/Separation Axioms']",Definition:Regular,"Let $T = left( S, tau right)$ be a topological space.


$left( S, tau right)$ is a Tychonoff Space or completely regular space  if and only if :
:$left( S, tau right)$ is a $T_{3 frac 1 2}$ space
:$left( S, tau right)$ is a $T_0$ (Kolmogorov) space.


That is:

:For any closed set $F subseteq S$ and any point $y in S$ such that $y notin F$, there exists an Urysohn function for $F$ and $leftlbrace y rightrbrace$.

:$forall x, y in S$, either:
::$exists U in tau: x in U, y notin U$
::$exists U in tau: y in U, x notin U$

 ",Definition:Tychonoff Space,,false,"Let T = ( S, τ) be a topological space.


( S, τ) is a Tychonoff Space or completely regular space  if and only if :
:( S, τ) is a T_3 1/2 space
:( S, τ) is a T_0 (Kolmogorov) space.


That is:

:For any closed set F ⊆ S and any point y ∈ S such that y ∉ F, there exists an Urysohn function for F and { y }.

:∀ x, y ∈ S, either:
::∃ U ∈τ: x ∈ U, y ∉ U
::∃ U ∈τ: y ∈ U, x ∉ U

 ",Regular
"['Definitions/Regular Graphs', 'Definitions/Graph Theory']",Definition:Regular,"Let $G = left( V, E right)$ be an simple graph whose vertices all have the same degree $r$.

Then $G$ is called regular of degree $r$, or $r$-regular.",Definition:Regular Graph,,false,"Let G = ( V, E ) be an simple graph whose vertices all have the same degree r.

Then G is called regular of degree r, or r-regular.",Regular
['Definitions/Formal Systems'],Definition:Regular,"A regular expression is an algebraic structure on an alphabet $unicode{x3a3}$ defined as follows:

* The empty-set regular expression, $varnothing$, is a regular expression.

* The empty-word regular expression, $epsilon$, is a regular expression.

* Every $sigma in unicode{x3a3}$ is a regular expression. (These are called literals.)

* If $R_1$ and $R_2$ are regular expressions, $R_1 R_2$ is a regular expression (concatenation).

* If $R_1$ and $R_2$ are regular expressions, $R_1 mid R_2$ is a regular expression (alternation).

* If $R$ is a regular expression, $R^*$ is a regular expression (Kleene star).


Every regular expression $R$ on an alphabet $unicode{x3a3}$ defines a language $L left(   right)R subseteq unicode{x3a3}^*$, where $unicode{x3a3}^*$ is the set of all (finite-length) words (sequences) of symbols in $unicode{x3a3}$:

* $L left(   right)varnothing = varnothing$ (the empty set).

* $L left(   right)epsilon = leftlbrace left[ , right] rightrbrace$ (the set containing only the empty word).

* If $R$ is a literal $sigma$, $L left(   right)R = leftlbrace left[ sigma right] rightrbrace$ (i.e., the set containing only the single-symbol word “$sigma$”).

* If $R$ is a concatenation $R_1 R_2$, $L left(   right)R = leftlbrace w_1 w_2: w_1 in L left(   right){R_1}, w_2 in L left(   right){R_2}  rightrbrace$, where $w_1 w_2$ is the concatenation of the words $w_1$ and $w_2$.

* If $R$ is an alternation $R_1 mid R_2$, $L left(   right)R = L {R_1} cup L left(   right){R_2}$.

* If $R$ is a Kleene star $R_1^*$, $L left(   right)R$ is the smallest set satisfying the following:
** $left[ , right] in L left(   right)R$ (the empty word is in the set);
** if $w_1 in L left(   right)R$ and $w_2 in L left(   right){R_1}$, then $w_1 w_2 in L left(   right)R$.

Category:Definitions/Formal Systems",Definition:Regular Expression,,false,"A regular expression is an algebraic structure on an alphabet x3a3 defined as follows:

* The empty-set regular expression, ∅, is a regular expression.

* The empty-word regular expression, ϵ, is a regular expression.

* Every σ∈x3a3 is a regular expression. (These are called literals.)

* If R_1 and R_2 are regular expressions, R_1 R_2 is a regular expression (concatenation).

* If R_1 and R_2 are regular expressions, R_1 | R_2 is a regular expression (alternation).

* If R is a regular expression, R^* is a regular expression (Kleene star).


Every regular expression R on an alphabet x3a3 defines a language L (   )R ⊆x3a3^*, where x3a3^* is the set of all (finite-length) words (sequences) of symbols in x3a3:

* L (   )∅ = ∅ (the empty set).

* L (   )ϵ = {[  ] } (the set containing only the empty word).

* If R is a literal σ, L (   )R = {[ σ] } (i.e., the set containing only the single-symbol word “σ”).

* If R is a concatenation R_1 R_2, L (   )R = { w_1 w_2: w_1 ∈ L (   )R_1, w_2 ∈ L (   )R_2}, where w_1 w_2 is the concatenation of the words w_1 and w_2.

* If R is an alternation R_1 | R_2, L (   )R = L R_1∪ L (   )R_2.

* If R is a Kleene star R_1^*, L (   )R is the smallest set satisfying the following:
** [  ] ∈ L (   )R (the empty word is in the set);
** if w_1 ∈ L (   )R and w_2 ∈ L (   )R_1, then w_1 w_2 ∈ L (   )R.

Category:Definitions/Formal Systems",Regular
['Definitions/Group Actions'],Definition:Representation,"Let $G$ be a group.

Let $X$ be a set.

Let $left( Gamma left(   right)X, circ right)$ be the symmetric group on $X$.


A permutation representation of $G$ is a group homomorphism from $G$ to $left( Gamma left(   right)X, circ right)$.


=== Associated to Group Action ===
Let $G$ be a group.

Let $X$ be a set.

Let $left( Gamma left(   right)X, circ right)$ be the symmetric group on $X$.


Let $phi: G times X to X$ be a group action.

Define for $g in G$ the mapping $phi_g : X to X$ by:
:$phi_g left(   right)x = phi left(   right){g, x}$


The permutation representation of $G$ associated to the group action is the group homomorphism $G to left( Gamma left(   right)X, circ right)$ which sends $g$ to $phi_g$.",Definition:Permutation Representation,,false,"Let G be a group.

Let X be a set.

Let ( Γ(   )X, ∘) be the symmetric group on X.


A permutation representation of G is a group homomorphism from G to ( Γ(   )X, ∘).


=== Associated to Group Action ===
Let G be a group.

Let X be a set.

Let ( Γ(   )X, ∘) be the symmetric group on X.


Let ϕ: G × X → X be a group action.

Define for g ∈ G the mapping ϕ_g : X → X by:
:ϕ_g (   )x = ϕ(   )g, x


The permutation representation of G associated to the group action is the group homomorphism G →( Γ(   )X, ∘) which sends g to ϕ_g.",Representation
['Definitions/Representation Theory'],Definition:Representation,"=== Groups ===
Let $left( K, +, circ right)$ be a field.

Let $V$ be a vector space over $K$ of finite dimension.

Let $mathrm {GL} left( V right)$ be the general linear group of $V$.

Let $left( G, cdot right)$ be a finite group.


A linear representation of $G$ on $V$ is a group homomorphism $rho: G to mathrm {GL} left( V right)$.


=== Module associated to representation ===

Let $K left[ G right]$ be the group ring.

Let $operatorname{End}  left(   right)V$ be the endomorphism ring of $V$.

Let $K left[ G right] to operatorname{End}  left(   right)V$ be the ring homomorphism given by $rho : G to mathrm {GL} left( V right)$ and the Universal Property of Group Ring.


The $K left[ G right]$-module induced by the representation is the module induced by this homomorphism.

=== Algebras ===
Let $K$ be a field.

Let $A$ be an associative unitary algebra over $K$.

Then a (linear) representation of $A$ is a vector space $V$ over $K$ equipped with a homomorphism of algebras:

:$rho: A to operatorname {End}  left(   right)V$

where $operatorname {End}  left(   right)V$ is the endomorphism ring of $V$.


Category:Definitions/Representation Theory",Definition:Linear Representation,,false,"=== Groups ===
Let ( K, +, ∘) be a field.

Let V be a vector space over K of finite dimension.

Let GL( V ) be the general linear group of V.

Let ( G, ·) be a finite group.


A linear representation of G on V is a group homomorphism ρ: G →GL( V ).


=== Module associated to representation ===

Let K [ G ] be the group ring.

Let End(   )V be the endomorphism ring of V.

Let K [ G ] →End(   )V be the ring homomorphism given by ρ : G →GL( V ) and the Universal Property of Group Ring.


The K [ G ]-module induced by the representation is the module induced by this homomorphism.

=== Algebras ===
Let K be a field.

Let A be an associative unitary algebra over K.

Then a (linear) representation of A is a vector space V over K equipped with a homomorphism of algebras:

:ρ: A →End(   )V

where End(   )V is the endomorphism ring of V.


Category:Definitions/Representation Theory",Representation
['Definitions/Module Theory'],Definition:Representation,"Let $R$ be a ring.

Let $M$ be an abelian group.


A ring representation of $R$ on $M$ is a ring homomorphism from $R$ to the endomorphism ring $operatorname {End}  left(   right)M$.


=== Unital Ring Representation ===
Let $R$ be a ring with unity.

Let $M$ be an abelian group.


A unital ring representation of $R$ on $M$ is a ring representation $R to operatorname {End}  left(   right)M$ which is unital.

That is, it is a unital ring homomorphism from $R$ to the endomorphism ring $operatorname {End}  left(   right)M$.",Definition:Ring Representation,,false,"Let R be a ring.

Let M be an abelian group.


A ring representation of R on M is a ring homomorphism from R to the endomorphism ring End(   )M.


=== Unital Ring Representation ===
Let R be a ring with unity.

Let M be an abelian group.


A unital ring representation of R on M is a ring representation R →End(   )M which is unital.

That is, it is a unital ring homomorphism from R to the endomorphism ring End(   )M.",Representation
['Definitions/Category Theory'],Definition:Representation,"Let $mathbf C$ be a locally small category.

Let $mathbf{Set}$ be the category of sets.

Let $F : mathbf C to mathbf{Set}$ be a covariant functor.


A representation of $F$ is a pair $left( C, eta right)$ where $eta : operatorname {Hom}  left(   right){C, cdot} to F$ is a natural isomorphism with the covariant hom functor of $C$.",Definition:Representation of Functor,,false,"Let 𝐂 be a locally small category.

Let 𝐒𝐞𝐭 be the category of sets.

Let F : 𝐂→𝐒𝐞𝐭 be a covariant functor.


A representation of F is a pair ( C, η) where η : Hom(   )C, ·→ F is a natural isomorphism with the covariant hom functor of C.",Representation
['Definitions/Complex Analysis'],Definition:Residue,"Let $f: mathbb C to mathbb C$ be a complex function.

Let $z_0 in U subset mathbb C$ such that $f$ is analytic in $U setminus leftlbrace z_0 rightrbrace$.


Then by Existence of Laurent Series, there is a Laurent series:
:$ds sum_{j mathop = -infty}^infty a_j left( z - z_0 right)^j$
such that the sum converges to $f$ in $U - leftlbrace z_0 rightrbrace$.  


The residue at a point $z = z_0$ of $f$ is defined as $a_{-1}$ in that Laurent series.

It is denoted $mathrm {Res} left( f,   right){z_0}$ or just $mathrm {Res}  left(   right){z_0}$ when $f$ is understood.",Definition:Residue (Complex Analysis),,false,"Let f: ℂ→ℂ be a complex function.

Let z_0 ∈ U ⊂ℂ such that f is analytic in U ∖{ z_0 }.


Then by Existence of Laurent Series, there is a Laurent series:
:∑_j  = -∞^∞ a_j ( z - z_0 )^j
such that the sum converges to f in U - { z_0 }.  


The residue at a point z = z_0 of f is defined as a_-1 in that Laurent series.

It is denoted Res( f,   )z_0 or just Res(   )z_0 when f is understood.",Residue
"['Definitions/Residues (Number Theory)', 'Definitions/Number Theory']",Definition:Residue,"Let $m, n in mathbb N$ be natural numbers.

Let $a in mathbb Z$ be an integer such that $a$ is not divisible by $m$.

Then $a$ is a residue of $m$ of order $n$  if and only if :
:$exists x in mathbb Z: x^n equiv a pmod m$
where $equiv$ denotes modulo congruence.


=== Nonresidue ===
Let $m, n in mathbb N$ be natural numbers.

Let $a in mathbb Z$ be an integer such that $a$ is not divisible by $m$.


$a$ is a nonresidue of $m$ of order $n$  if and only if  there does not exist $x in mathbb Z$ such that:
:$x^n equiv a pmod m$

where $equiv$ denotes modulo congruence.",Definition:Residue (Number Theory),,false,"Let m, n ∈ℕ be natural numbers.

Let a ∈ℤ be an integer such that a is not divisible by m.

Then a is a residue of m of order n  if and only if :
:∃ x ∈ℤ: x^n ≡ a  m
where ≡ denotes modulo congruence.


=== Nonresidue ===
Let m, n ∈ℕ be natural numbers.

Let a ∈ℤ be an integer such that a is not divisible by m.


a is a nonresidue of m of order n  if and only if  there does not exist x ∈ℤ such that:
:x^n ≡ a  m

where ≡ denotes modulo congruence.",Residue
"['Definitions/Congruence (Number Theory)', 'Definitions/Residue Classes']",Definition:Residue,"Let $m in mathbb Z_{ne 0}$ be a non-zero integer.

Let $a, b in mathbb Z$.

Let $a equiv b pmod m$.


Then $b$ is a residue of $a$ modulo $m$.

Residue is another word for remainder, and is any integer congruent to $a$ modulo $m$.",Definition:Congruence (Number Theory)/Residue,,false,"Let m ∈ℤ_ 0 be a non-zero integer.

Let a, b ∈ℤ.

Let a ≡ b  m.


Then b is a residue of a modulo m.

Residue is another word for remainder, and is any integer congruent to a modulo m.",Residue
['Definitions/Local Rings'],Definition:Residue,"Let $R$ be a commutative local ring.

Let $m$ be its maximal ideal.


The residue field of $R$ is the quotient ring $R / m$.",Definition:Residue Field of Local Ring,,false,"Let R be a commutative local ring.

Let m be its maximal ideal.


The residue field of R is the quotient ring R / m.",Residue
"['Definitions/Vector Addition', 'Definitions/Addition', 'Definitions/Vectors']",Definition:Resultant,"Let $mathbf u$ and $mathbf v$ be vector quantities of the same physical property.


=== Component Definition ===
Let $mathbf u$ and $mathbf v$ be vector quantities of the same physical property.


Let $mathbf u$ and $mathbf v$ be represented by their components considered to be embedded in a real $n$-space:

 
 
 
 


Then the (vector) sum of $mathbf u$ and $mathbf v$ is defined as:
:$mathbf u + mathbf v := left( u_1 + v_1, u_2 + v_2, ldots, u_n + v_n right)$


Note that the $+$ on the   is conventional addition of numbers, while the $+$ on the   takes on a different meaning.

The distinction is implied by which operands are involved.

=== Triangle Law ===
Let $mathbf u$ and $mathbf v$ be vector quantities of the same physical property.

Let $mathbf u$ and $mathbf v$ be represented by arrows embedded in the plane such that:

:$mathbf u$ is represented by $vec {AB}$
:$mathbf v$ is represented by $vec {BC}$

that is, so that the initial point of $mathbf v$ is identified with the terminal point of $mathbf u$.

:

Then their (vector) sum $mathbf u + mathbf v$ is represented by the arrow $vec {AC}$.",Definition:Vector Sum,,false,"Let 𝐮 and 𝐯 be vector quantities of the same physical property.


=== Component Definition ===
Let 𝐮 and 𝐯 be vector quantities of the same physical property.


Let 𝐮 and 𝐯 be represented by their components considered to be embedded in a real n-space:

 
 
 
 


Then the (vector) sum of 𝐮 and 𝐯 is defined as:
:𝐮 + 𝐯 := ( u_1 + v_1, u_2 + v_2, …, u_n + v_n )


Note that the + on the   is conventional addition of numbers, while the + on the   takes on a different meaning.

The distinction is implied by which operands are involved.

=== Triangle Law ===
Let 𝐮 and 𝐯 be vector quantities of the same physical property.

Let 𝐮 and 𝐯 be represented by arrows embedded in the plane such that:

:𝐮 is represented by A⃗B⃗
:𝐯 is represented by B⃗C⃗

that is, so that the initial point of 𝐯 is identified with the terminal point of 𝐮.

:

Then their (vector) sum 𝐮 + 𝐯 is represented by the arrow A⃗C⃗.",Resultant
"['Definitions/Eliminants', 'Definitions/Simultaneous Linear Equations', 'Definitions/Linear Algebra']",Definition:Resultant,"Let $S$ be a system of simultaneous linear equations.

The eliminant of $S$ is the determinant formed by removing the variables between the equations.",Definition:Eliminant,,false,"Let S be a system of simultaneous linear equations.

The eliminant of S is the determinant formed by removing the variables between the equations.",Resultant
['Definitions/Language Definitions'],Definition:Right,"The direction right is that way:
:$to$",Definition:Right (Direction),,false,"The direction right is that way:
:→",Right
"['Definitions/Right Angles', 'Definitions/Angles']",Definition:Right,"A right angle is an angle that is equal to half of a straight angle.


=== Measurement of Right Angle ===
 ",Definition:Right Angle,,false,"A right angle is an angle that is equal to half of a straight angle.


=== Measurement of Right Angle ===
 ",Right
['Definitions/Language Definitions'],Definition:Right,"In an equation:
:$text {Expression $1$} = text {Expression $2$}$
the term $text {Expression $2$}$ is the right hand side.",Definition:Right Hand Side,,false,"In an equation:
:Expression 1 = Expression 2
the term Expression 2 is the right hand side.",Right
['Definitions/Relation Theory'],Definition:Right,"Let $mathcal R subseteq S times T$ be a relation.


Then $mathcal R$ is right-total  if and only if :
:$forall t in T: exists s in S: left( s, t right) in mathcal R$


That is,  if and only if  every element of $T$ is related to by some element of $S$.


That is,  if and only if :
:$mathrm {Img} left( mathcal R right) = T$
where $mathrm {Img} left( mathcal R right)$ denotes the image of $mathcal R$.",Definition:Right-Total Relation,,false,"Let ℛ⊆ S × T be a relation.


Then ℛ is right-total  if and only if :
:∀ t ∈ T: ∃ s ∈ S: ( s, t ) ∈ℛ


That is,  if and only if  every element of T is related to by some element of S.


That is,  if and only if :
:Img( ℛ) = T
where Img( ℛ) denotes the image of ℛ.",Right
"['Definitions/Reflexive Relations', 'Definitions/Quasi-Reflexive Relations', 'Definitions/Right Quasi-Reflexive Relations']",Definition:Right,"Let $mathcal R subseteq S times S$ be a relation in $S$.


=== Definition 1 ===
Let $mathcal R subseteq S times S$ be a relation in $S$.


$mathcal R$ is right quasi-reflexive  if and only if :

:$forall x, y in S: left( x, y right) in mathcal R implies left( y, y right) in mathcal R$

=== Definition 2 ===
Let $mathcal R subseteq S times S$ be a relation in $S$.


$mathcal R$ is right quasi-reflexive  if and only if :

:$forall y in mathrm {Img} left( mathcal R right): left( y, y right) in mathcal R$

where $mathrm {Img} left( mathcal R right)$ denotes the image set of $mathcal R$.",Definition:Right Quasi-Reflexive Relation,,false,"Let ℛ⊆ S × S be a relation in S.


=== Definition 1 ===
Let ℛ⊆ S × S be a relation in S.


ℛ is right quasi-reflexive  if and only if :

:∀ x, y ∈ S: ( x, y ) ∈ℛ( y, y ) ∈ℛ

=== Definition 2 ===
Let ℛ⊆ S × S be a relation in S.


ℛ is right quasi-reflexive  if and only if :

:∀ y ∈Img( ℛ): ( y, y ) ∈ℛ

where Img( ℛ) denotes the image set of ℛ.",Right
['Definitions/Euclidean Relations'],Definition:Right,"Let $mathcal R subseteq S times S$ be a relation in $S$.


$mathcal R$ is right-Euclidean  if and only if :

:$left( x, y right) in mathcal R land left( x, z right) in mathcal R implies left( y, z right) in mathcal R$",Definition:Euclidean Relation/Right-Euclidean,,false,"Let ℛ⊆ S × S be a relation in S.


ℛ is right-Euclidean  if and only if :

:( x, y ) ∈ℛ( x, z ) ∈ℛ( y, z ) ∈ℛ",Right
['Definitions/Relations'],Definition:Right,"Let $A$ be a class.

Let $mathcal R$ be a relation on $A$.


An element $x$ of $A$ is right normal with respect to $mathcal R$  if and only if :
:$forall y in A: mathcal R left(   right){y, x}$ holds.",Definition:Right Normal Element of Relation,,false,"Let A be a class.

Let ℛ be a relation on A.


An element x of A is right normal with respect to ℛ  if and only if :
:∀ y ∈ A: ℛ(   )y, x holds.",Right
"['Definitions/Mapping Theory', 'Definitions/Cancellability']",Definition:Right,"A mapping $f: X to Y$ is right cancellable (or right-cancellable)  if and only if :

:$forall Z: forall left( h_1, h_2: Y to Z right): h_1 circ f = h_2 circ f implies h_1 = h_2$

That is,  if and only if  for any set $Z$:
:If $h_1$ and $h_2$ are mappings from $Y$ to $Z$
:then $h_1 circ f = h_2 circ f$ implies $h_1 = h_2$.",Definition:Right Cancellable Mapping,,false,"A mapping f: X → Y is right cancellable (or right-cancellable)  if and only if :

:∀ Z: ∀( h_1, h_2: Y → Z ): h_1 ∘ f = h_2 ∘ f  h_1 = h_2

That is,  if and only if  for any set Z:
:If h_1 and h_2 are mappings from Y to Z
:then h_1 ∘ f = h_2 ∘ f implies h_1 = h_2.",Right
['Definitions/Inverse Mappings'],Definition:Right,"Let $S, T$ be sets where $S ne varnothing$, that is, $S$ is not empty.

Let $f: S to T$ be a mapping.


Let $g: T to S$ be a mapping such that:
:$f circ g = I_T$
where:
:$f circ g$ denotes the composite mapping $g$ followed by $f$
:$I_T$ is the identity mapping on $T$.


Then $g: T to S$ is called a right inverse (mapping) of $f$.",Definition:Right Inverse Mapping,,false,"Let S, T be sets where S ∅, that is, S is not empty.

Let f: S → T be a mapping.


Let g: T → S be a mapping such that:
:f ∘ g = I_T
where:
:f ∘ g denotes the composite mapping g followed by f
:I_T is the identity mapping on T.


Then g: T → S is called a right inverse (mapping) of f.",Right
['Definitions/Intervals'],Definition:Right,"Let $left( S, preccurlyeq right)$ be an ordered set.

Let $a, b in S$.


The right half-open interval between $a$ and $b$ is the set:

:$left[ a ,.,.,   right)b := a^succcurlyeq cap b^prec = leftlbrace s in S: left( a preccurlyeq s right) land left( s prec b right)  rightrbrace$

where:
:$a^succcurlyeq$ denotes the upper closure of $a$
:$b^prec$ denotes the strict lower closure of $b$.",Definition:Interval/Ordered Set/Right Half-Open,,false,"Let ( S, ≼) be an ordered set.

Let a, b ∈ S.


The right half-open interval between a and b is the set:

:[ a  . . )b := a^≽∩ b^≺ = { s ∈ S: ( a ≼ s ) ( s ≺ b )  }

where:
:a^≽ denotes the upper closure of a
:b^≺ denotes the strict lower closure of b.",Right
['Definitions/Orientation (Coordinate Axes)'],Definition:Right,"A Cartesian plane is defined as being right-handed if it has the following property:

Let a right hand be placed, with palm uppermost, such that the thumb points along the $x$-axis in the positive direction, such that the thumb and index finger are at right-angles to each other.

Then the index finger is pointed along the $y$-axis in the positive direction.


:",Definition:Orientation of Coordinate Axes/Cartesian Plane/Right-Handed,,false,"A Cartesian plane is defined as being right-handed if it has the following property:

Let a right hand be placed, with palm uppermost, such that the thumb points along the x-axis in the positive direction, such that the thumb and index finger are at right-angles to each other.

Then the index finger is pointed along the y-axis in the positive direction.


:",Right
"['Definitions/Orientation (Coordinate Axes)', 'Definitions/Right-Hand Rule']",Definition:Right,"A Cartesian $3$-Space is defined as being right-handed if it has the following property:

Let a right hand be placed such that:
:the thumb and index finger are at right-angles to each other
:the $3$rd finger is at right-angles to the thumb and index finger, upwards from the palm
:the thumb points along the $x$-axis in the positive direction
:the index finger points along the $y$-axis in the positive direction.

Then the $3$rd finger is pointed along the $z$-axis in the positive direction.


:",Definition:Orientation of Coordinate Axes/Cartesian 3-Space/Right-Handed,,false,"A Cartesian 3-Space is defined as being right-handed if it has the following property:

Let a right hand be placed such that:
:the thumb and index finger are at right-angles to each other
:the 3rd finger is at right-angles to the thumb and index finger, upwards from the palm
:the thumb points along the x-axis in the positive direction
:the index finger points along the y-axis in the positive direction.

Then the 3rd finger is pointed along the z-axis in the positive direction.


:",Right
['Definitions/Derivatives'],Definition:Right,"Let $B$ be a Banach space over the set of real numbers $mathbb R$.

Let $f: mathbb R to B$ be a mapping from $mathbb R$ to $B$.


The right-hand derivative of $f$ is defined as the right-hand limit:
:$ds f'_+ left(   right)x = lim_{h mathop to 0^+} frac {f left(   right){x + h} - f left(   right)x} h$

If the right-hand derivative exists, then $f$ is said to be right-hand differentiable at $x$.


=== Real Functions ===
Let $f: mathbb R to mathbb R$ be a real function.


The right-hand derivative of $f$ is defined as the right-hand limit:
:$ds f'_+ left(   right)x = lim_{h mathop to 0^+} frac {f left(   right){x + h} - f left(   right)x} h$

If the right-hand derivative exists, then $f$ is said to be right-hand differentiable at $x$.",Definition:Right-Hand Derivative,,false,"Let B be a Banach space over the set of real numbers ℝ.

Let f: ℝ→ B be a mapping from ℝ to B.


The right-hand derivative of f is defined as the right-hand limit:
:f'_+ (   )x = lim_h → 0^+f (   )x + h - f (   )x/h

If the right-hand derivative exists, then f is said to be right-hand differentiable at x.


=== Real Functions ===
Let f: ℝ→ℝ be a real function.


The right-hand derivative of f is defined as the right-hand limit:
:f'_+ (   )x = lim_h → 0^+f (   )x + h - f (   )x/h

If the right-hand derivative exists, then f is said to be right-hand differentiable at x.",Right
['Definitions/Limits of Real Functions'],Definition:Right,"Let $Bbb I = left( a ,.,.,   right)b$ be an open real interval.

Let $f: Bbb I to mathbb R$ be a real function.

Let $L in mathbb R$.


Suppose that:
:$forall epsilon in mathbb R_{>0}: exists delta in mathbb R_{>0}: forall x in Bbb I: a < x < a + delta implies leftlvert f left(   right)x - L rightrvert < epsilon$
where $mathbb R_{>0}$ denotes the set of strictly positive real numbers.

That is, for every real strictly positive $epsilon$ there exists a real strictly positive $delta$ such that every real number in the domain of $f$, greater than $a$ but within $delta$ of $a$, has an image within $epsilon$ of $L$.


:

Then $f left(   right)x$ is said to tend to the limit $L$ as $x$ tends to $a$ from the right, and we write:
:$f left(   right)x to L$ as $x to a^+$
or
:$ds lim_{x mathop to a^+} f left(   right)x = L$


This is voiced
:the limit of $f left(   right)x$ as $x$ tends to $a$ from the right
and such an $L$ is called:
:a limit from the right.",Definition:Limit of Real Function/Right,,false,"Let I = ( a  . . )b be an open real interval.

Let f:  I →ℝ be a real function.

Let L ∈ℝ.


Suppose that:
:∀ϵ∈ℝ_>0: ∃δ∈ℝ_>0: ∀ x ∈ I: a < x < a + δ| f (   )x - L | < ϵ
where ℝ_>0 denotes the set of strictly positive real numbers.

That is, for every real strictly positive ϵ there exists a real strictly positive δ such that every real number in the domain of f, greater than a but within δ of a, has an image within ϵ of L.


:

Then f (   )x is said to tend to the limit L as x tends to a from the right, and we write:
:f (   )x → L as x → a^+
or
:lim_x → a^+ f (   )x = L


This is voiced
:the limit of f (   )x as x tends to a from the right
and such an L is called:
:a limit from the right.",Right
['Definitions/Difference Quotients'],Definition:Right,"Let $V$ be a vector space over the real numbers $mathbb R$.

Let $f: mathbb R to V$ be a function.


A right difference quotient is an expression of the form:
:$dfrac {f left(   right){x + h} - f left(   right)x} h$
where $h > 0$ is a strictly positive real number.",Definition:Difference Quotient/Right,,false,"Let V be a vector space over the real numbers ℝ.

Let f: ℝ→ V be a function.


A right difference quotient is an expression of the form:
:f (   )x + h - f (   )x h
where h > 0 is a strictly positive real number.",Right
['Definitions/Continuous Real Functions'],Definition:Right,"Let $S subseteq mathbb R$ be an open subset of the real numbers $mathbb R$.

Let $f: S to mathbb R$ be a real function.


Let $x_0 in S$. 

Then $f$ is said to be right-continuous at $x_0$  if and only if  the limit from the right of $f left(   right)x$ as $x to x_0$ exists and:

:$ds lim_{substack {x mathop to x_0^+ \ x_0 mathop in A}} f left(   right)x = f left(   right){x_0}$

where $ds lim_{x mathop to x_0^+}$ is a limit from the right.


Furthermore, $f$ is said to be right-continuous  if and only if :

:$forall x_0 in S$, $f$ is right-continuous at $x_0$",Definition:Continuous Real Function/Right-Continuous,,false,"Let S ⊆ℝ be an open subset of the real numbers ℝ.

Let f: S →ℝ be a real function.


Let x_0 ∈ S. 

Then f is said to be right-continuous at x_0  if and only if  the limit from the right of f (   )x as x → x_0 exists and:

:lim_x → x_0^+ 
 x_0 ∈ A f (   )x = f (   )x_0

where lim_x → x_0^+ is a limit from the right.


Furthermore, f is said to be right-continuous  if and only if :

:∀ x_0 ∈ S, f is right-continuous at x_0",Right
['Definitions/Derivatives'],Definition:Right,"Let $f: mathbb R to mathbb R$ be a real function.


The right-hand derivative of $f$ is defined as the right-hand limit:
:$ds f'_+ left(   right)x = lim_{h mathop to 0^+} frac {f left(   right){x + h} - f left(   right)x} h$

If the right-hand derivative exists, then $f$ is said to be right-hand differentiable at $x$.",Definition:Right-Hand Derivative/Real Function,,false,"Let f: ℝ→ℝ be a real function.


The right-hand derivative of f is defined as the right-hand limit:
:f'_+ (   )x = lim_h → 0^+f (   )x + h - f (   )x/h

If the right-hand derivative exists, then f is said to be right-hand differentiable at x.",Right
['Definitions/Real Intervals'],Definition:Right,"Let $a, b in mathbb R$ be real numbers.


The right half-open (real) interval from $a$ to $b$ is the subset:
:$left[ a ,.,.,   right)b := leftlbrace x in mathbb R: a le x < b rightrbrace$",Definition:Real Interval/Half-Open/Right,,false,"Let a, b ∈ℝ be real numbers.


The right half-open (real) interval from a to b is the subset:
:[ a  . . )b := { x ∈ℝ: a ≤ x < b }",Right
['Definitions/Real Intervals'],Definition:Right,"There are two unbounded closed intervals involving a real number $a in mathbb R$, defined as:

 
 
 
 ",Definition:Real Interval/Unbounded Closed,,false,"There are two unbounded closed intervals involving a real number a ∈ℝ, defined as:

 
 
 
 ",Right
['Definitions/Real Intervals'],Definition:Right,"There are two unbounded open intervals involving a real number $a in mathbb R$, defined as:

 
 
 
 ",Definition:Real Interval/Unbounded Open,,false,"There are two unbounded open intervals involving a real number a ∈ℝ, defined as:

 
 
 
 ",Right
['Definitions/Zero Elements'],Definition:Right,"Let $left( S, circ right)$ be an algebraic structure.

An element $z_R in S$ is called a right zero element (or just right zero)  if and only if :
:$forall x in S: x circ z_R = z_R$",Definition:Right Zero,,false,"Let ( S, ∘) be an algebraic structure.

An element z_R ∈ S is called a right zero element (or just right zero)  if and only if :
:∀ x ∈ S: x ∘ z_R = z_R",Right
['Definitions/Identity Elements'],Definition:Right,"Let $left( S, circ right)$ be an algebraic structure.

An element $e_R in S$ is called a right identity (element)  if and only if :
:$forall x in S: x circ e_R = x$",Definition:Identity (Abstract Algebra)/Right Identity,,false,"Let ( S, ∘) be an algebraic structure.

An element e_R ∈ S is called a right identity (element)  if and only if :
:∀ x ∈ S: x ∘ e_R = x",Right
"['Definitions/Abstract Algebra', 'Definitions/Right Operation']",Definition:Right,"Let $S$ be a set.

For any $x, y in S$, the right operation on $S$ is the binary operation defined as:
:$forall x, y in S: x to y = y$",Definition:Right Operation,,false,"Let S be a set.

For any x, y ∈ S, the right operation on S is the binary operation defined as:
:∀ x, y ∈ S: x → y = y",Right
['Definitions/Cancellability'],Definition:Right,"Let $left( S, circ right)$ be an algebraic structure.


An element $x in left( S, circ right)$ is right cancellable  if and only if :

:$forall a, b in S: a circ x = b circ x implies a = b$",Definition:Cancellable Element/Right Cancellable,,false,"Let ( S, ∘) be an algebraic structure.


An element x ∈( S, ∘) is right cancellable  if and only if :

:∀ a, b ∈ S: a ∘ x = b ∘ x  a = b",Right
['Definitions/Cancellability'],Definition:Right,"Let $left( S, circ right)$ be an algebraic structure.


The operation $circ$ in $left( S, circ right)$ is right cancellable  if and only if :
:$forall a, b, c in S: a circ c = b circ c implies a = b$

That is,  if and only if  all elements of $left( S, circ right)$ are right cancellable.",Definition:Right Cancellable Operation,,false,"Let ( S, ∘) be an algebraic structure.


The operation ∘ in ( S, ∘) is right cancellable  if and only if :
:∀ a, b, c ∈ S: a ∘ c = b ∘ c  a = b

That is,  if and only if  all elements of ( S, ∘) are right cancellable.",Right
['Definitions/Distributive Operations'],Definition:Right,"Let $S$ be a set on which is defined two binary operations, defined on all the elements of $S times S$, denoted here as $circ$ and $*$.

The operation $circ$ is right distributive over the operation $*$  if and only if :

:$forall a, b, c in S: left( a * b right) circ c = left( a circ c right) * left( b circ c right)$",Definition:Distributive Operation/Right,,false,"Let S be a set on which is defined two binary operations, defined on all the elements of S × S, denoted here as ∘ and *.

The operation ∘ is right distributive over the operation *  if and only if :

:∀ a, b, c ∈ S: ( a * b ) ∘ c = ( a ∘ c ) * ( b ∘ c )",Right
['Definitions/Inverse Elements'],Definition:Right,"Let $left( S, circ right)$ be a monoid whose identity is $e_S$.

An element $x_R in S$ is called a right inverse of $x$  if and only if :
:$x circ x_R = e_S$",Definition:Inverse (Abstract Algebra)/Right Inverse,,false,"Let ( S, ∘) be a monoid whose identity is e_S.

An element x_R ∈ S is called a right inverse of x  if and only if :
:x ∘ x_R = e_S",Right
['Definitions/Quasigroups'],Definition:Right,"Let $left( S, circ right)$ be a magma. 


$left( S, circ right)$ is a right quasigroup  if and only if :
:for all $a in S$, the right regular representation $rho_a$ is a permutation on $S$.

That is:
:$forall a, b in S: exists ! x in S: x circ a = b$",Definition:Quasigroup/Right Quasigroup,,false,"Let ( S, ∘) be a magma. 


( S, ∘) is a right quasigroup  if and only if :
:for all a ∈ S, the right regular representation ρ_a is a permutation on S.

That is:
:∀ a, b ∈ S: ∃ ! x ∈ S: x ∘ a = b",Right
"['Definitions/Right Regular Representation', 'Definitions/Regular Representations']",Definition:Right,"Let $left( S, circ right)$ be a magma.

The mapping $rho_a: S to S$ is defined as:

:$forall x in S: rho_a left(   right)x = x circ a$


This is known as the right regular representation of $left( S, circ right)$ with respect to $a$.",Definition:Regular Representations/Right Regular Representation,,false,"Let ( S, ∘) be a magma.

The mapping ρ_a: S → S is defined as:

:∀ x ∈ S: ρ_a (   )x = x ∘ a


This is known as the right regular representation of ( S, ∘) with respect to a.",Right
['Definitions/Operations'],Definition:Right,"Let $x$ and $y$ be elements which are operated on by a given operation $circ$.

The right-hand product of $x$ by $y$ is the product $x circ y$.",Definition:Operation/Binary Operation/Product/Right,,false,"Let x and y be elements which are operated on by a given operation ∘.

The right-hand product of x by y is the product x ∘ y.",Right
['Definitions/Naturally Ordered Semigroup'],Definition:Right,"Let $left( S, circ, preceq right)$ be a positively totally ordered semigroup.


Then $left( S, circ, preceq right)$ is a right naturally totally ordered semigroup  if and only if :

:$forall a, b in S: a prec b implies exists x in S: b = a circ x$",Definition:Right Naturally Totally Ordered Semigroup,,false,"Let ( S, ∘, ≼) be a positively totally ordered semigroup.


Then ( S, ∘, ≼) is a right naturally totally ordered semigroup  if and only if :

:∀ a, b ∈ S: a ≺ b ∃ x ∈ S: b = a ∘ x",Right
['Definitions/Cosets'],Definition:Right,"Let $left( S, circ right)$ be an algebraic structure.

Let $left( H, circ right)$ be a subgroup of $left( S, circ right)$.


The right coset of $y$ modulo $H$, or right coset of $H$ by $y$, is:

:$H circ y = leftlbrace x in S: exists h in H: x = h circ y rightrbrace$


That is, it is the subset product with singleton:

:$H circ y = H circ leftlbrace y rightrbrace$",Definition:Coset/Right Coset,,false,"Let ( S, ∘) be an algebraic structure.

Let ( H, ∘) be a subgroup of ( S, ∘).


The right coset of y modulo H, or right coset of H by y, is:

:H ∘ y = { x ∈ S: ∃ h ∈ H: x = h ∘ y }


That is, it is the subset product with singleton:

:H ∘ y = H ∘{ y }",Right
['Definitions/Cosets'],Definition:Right,"Let $G$ be a group.

Let $H$ be a subgroup of $G$.


The right coset space (of $G$ modulo $H$) is the quotient set of $G$ by right congruence modulo $H$, denoted $G / H^r$.

It is the set of all the right cosets of $H$ in $G$.",Definition:Coset Space/Right Coset Space,,false,"Let G be a group.

Let H be a subgroup of G.


The right coset space (of G modulo H) is the quotient set of G by right congruence modulo H, denoted G / H^r.

It is the set of all the right cosets of H in G.",Right
['Definitions/Transversals (Group Theory)'],Definition:Right,"Let $G$ be a group.

Let $H$ be a subgroup of $G$.

Let $S subseteq G$ be a subset of $G$.


$S$ is a right transversal for $H$ in $G$  if and only if  every right coset of $H$ contains exactly one element of $S$.",Definition:Transversal (Group Theory)/Right Transversal,,false,"Let G be a group.

Let H be a subgroup of G.

Let S ⊆ G be a subset of G.


S is a right transversal for H in G  if and only if  every right coset of H contains exactly one element of S.",Right
['Definitions/Group Actions'],Definition:Right,"Let $X$ be a set.

Let $left( G, circ right)$ be a group whose identity is $e$.


A right group action is a mapping $phi: X times G to X$ such that:

:$forall left( x, g right) in X times G : x * g := phi left(   right){x, g} in X$

in such a way that the right group action axioms are satisfied:
 ",Definition:Group Action/Right Group Action,,false,"Let X be a set.

Let ( G, ∘) be a group whose identity is e.


A right group action is a mapping ϕ: X × G → X such that:

:∀( x, g ) ∈ X × G : x * g := ϕ(   )x, g∈ X

in such a way that the right group action axioms are satisfied:
 ",Right
['Definitions/Subset Product Action'],Definition:Right,"Let $left( G, circ right)$ be a group.

Let $mathcal P left( G right)$ be the power set of $G$.


The (right) subset product action of $G$ is the group action $*: G times mathcal P left( G right) to mathcal P left( G right)$:
:$forall g in G, S in mathcal P left( G right): g * S = S circ g$",Definition:Subset Product Action/Right,,false,"Let ( G, ∘) be a group.

Let 𝒫( G ) be the power set of G.


The (right) subset product action of G is the group action *: G ×𝒫( G ) →𝒫( G ):
:∀ g ∈ G, S ∈𝒫( G ): g * S = S ∘ g",Right
['Definitions/Congruence Modulo Subgroup'],Definition:Right,"Let $G$ be a group.

Let $H$ be a subgroup of $G$.


We can use $H$ to define a relation on $G$ as follows:

:$mathcal R^r_H = leftlbrace left( x, y right) in G times G: x y^{-1} in H rightrbrace$

This is called right congruence modulo $H$.",Definition:Congruence Modulo Subgroup/Right Congruence,,false,"Let G be a group.

Let H be a subgroup of G.


We can use H to define a relation on G as follows:

:ℛ^r_H = {( x, y ) ∈ G × G: x y^-1∈ H }

This is called right congruence modulo H.",Right
['Definitions/Zero Divisors'],Definition:Right,"Let $left( R, +, circ right)$ be a ring.


A right zero divisor (in $R$) is an element $x in R$ such that:
: $exists y in R^*: y circ x = 0_R$

where $R^*$ is defined as $R setminus leftlbrace 0_R rightrbrace$.",Definition:Right Zero Divisor,,false,"Let ( R, +, ∘) be a ring.


A right zero divisor (in R) is an element x ∈ R such that:
: ∃ y ∈ R^*: y ∘ x = 0_R

where R^* is defined as R ∖{ 0_R }.",Right
['Definitions/Linear Ring Actions'],Definition:Right,"Let $R$ be a ring.

Let $M$ be an abelian group.

Let $circ : M times R to M$ be a mapping from the cartesian product $M times R$.


$circ$ is a right linear ring action of $R$ on $M$  if and only if  $circ$ satisfies the right ring action axioms:
 ",Definition:Linear Ring Action/Right,,false,"Let R be a ring.

Let M be an abelian group.

Let ∘ : M × R → M be a mapping from the cartesian product M × R.


∘ is a right linear ring action of R on M  if and only if  ∘ satisfies the right ring action axioms:
 ",Right
['Definitions/Ideal Theory'],Definition:Right,"Let $left( R, +, circ right)$ be a ring.

Let $left( J, + right)$ be a subgroup of $left( R, + right)$.


$J$ is a right ideal of $R$  if and only if :
:$forall j in J: forall r in R: j circ r in J$

that is,  if and only if :
:$forall r in R: J circ r subseteq J$",Definition:Ideal of Ring/Right Ideal,,false,"Let ( R, +, ∘) be a ring.

Let ( J, + ) be a subgroup of ( R, + ).


J is a right ideal of R  if and only if :
:∀ j ∈ J: ∀ r ∈ R: j ∘ r ∈ J

that is,  if and only if :
:∀ r ∈ R: J ∘ r ⊆ J",Right
['Definitions/Maximal Ideals of Rings'],Definition:Right,"Let $R$ be a ring.


A right ideal $J$ of $R$ is a maximal right ideal  if and only if :

:$(1): quad J subsetneq R$
:$(2): quad$ There is no right ideal $K$ of $R$ such that $J subsetneq K subsetneq R$.


Category:Definitions/Maximal Ideals of Rings",Definition:Maximal Ideal of Ring/Right,,false,"Let R be a ring.


A right ideal J of R is a maximal right ideal  if and only if :

:(1):    J ⊊ R
:(2): There is no right ideal K of R such that J ⊊ K ⊊ R.


Category:Definitions/Maximal Ideals of Rings",Right
"['Definitions/Right Modules', 'Definitions/Module Theory']",Definition:Right,"Let $left( R, +_R, times_R right)$ be a ring.

Let $left( G, +_G right)$ be an abelian group.


A right module over $R$ is an $R$-algebraic structure $left( G, +_G, circ right)_R$ with one operation $circ$, the (right) ring action, which satisfies the right module axioms:
 ",Definition:Right Module,,false,"Let ( R, +_R, ×_R ) be a ring.

Let ( G, +_G ) be an abelian group.


A right module over R is an R-algebraic structure ( G, +_G, ∘)_R with one operation ∘, the (right) ring action, which satisfies the right module axioms:
 ",Right
"['Definitions/Right-Truncatable Primes', 'Definitions/Prime Numbers', 'Definitions/Recreational Mathematics']",Definition:Right,"A right-truncatable prime is a prime number which remains prime when any number of digits are removed from the right hand end.


=== Sequence ===
 ",Definition:Right-Truncatable Prime,,false,"A right-truncatable prime is a prime number which remains prime when any number of digits are removed from the right hand end.


=== Sequence ===
 ",Right
"['Definitions/Examples of Categories', 'Definitions/Module Theory']",Definition:Right,"Let $R$ be a ring.


The category of right $R$-modules is the category $mathbf {Mod-R}$ with:

 ",Definition:Category of Right Modules,,false,"Let R be a ring.


The category of right R-modules is the category 𝐌𝐨𝐝-𝐑 with:

 ",Right
['Definitions/Cancellability'],Definition:Right Cancellable,"Let $left( S, circ right)$ be an algebraic structure.


An element $x in left( S, circ right)$ is right cancellable  if and only if :

:$forall a, b in S: a circ x = b circ x implies a = b$",Definition:Cancellable Element/Right Cancellable,,false,"Let ( S, ∘) be an algebraic structure.


An element x ∈( S, ∘) is right cancellable  if and only if :

:∀ a, b ∈ S: a ∘ x = b ∘ x  a = b",Right Cancellable
['Definitions/Cancellability'],Definition:Right Cancellable,"Let $left( S, circ right)$ be an algebraic structure.


The operation $circ$ in $left( S, circ right)$ is right cancellable  if and only if :
:$forall a, b, c in S: a circ c = b circ c implies a = b$

That is,  if and only if  all elements of $left( S, circ right)$ are right cancellable.",Definition:Right Cancellable Operation,,false,"Let ( S, ∘) be an algebraic structure.


The operation ∘ in ( S, ∘) is right cancellable  if and only if :
:∀ a, b, c ∈ S: a ∘ c = b ∘ c  a = b

That is,  if and only if  all elements of ( S, ∘) are right cancellable.",Right Cancellable
"['Definitions/Mapping Theory', 'Definitions/Cancellability']",Definition:Right Cancellable,"A mapping $f: X to Y$ is right cancellable (or right-cancellable)  if and only if :

:$forall Z: forall left( h_1, h_2: Y to Z right): h_1 circ f = h_2 circ f implies h_1 = h_2$

That is,  if and only if  for any set $Z$:
:If $h_1$ and $h_2$ are mappings from $Y$ to $Z$
:then $h_1 circ f = h_2 circ f$ implies $h_1 = h_2$.",Definition:Right Cancellable Mapping,,false,"A mapping f: X → Y is right cancellable (or right-cancellable)  if and only if :

:∀ Z: ∀( h_1, h_2: Y → Z ): h_1 ∘ f = h_2 ∘ f  h_1 = h_2

That is,  if and only if  for any set Z:
:If h_1 and h_2 are mappings from Y to Z
:then h_1 ∘ f = h_2 ∘ f implies h_1 = h_2.",Right Cancellable
['Definitions/Inverse Mappings'],Definition:Right Inverse,"Let $S, T$ be sets where $S ne varnothing$, that is, $S$ is not empty.

Let $f: S to T$ be a mapping.


Let $g: T to S$ be a mapping such that:
:$f circ g = I_T$
where:
:$f circ g$ denotes the composite mapping $g$ followed by $f$
:$I_T$ is the identity mapping on $T$.


Then $g: T to S$ is called a right inverse (mapping) of $f$.",Definition:Right Inverse Mapping,,false,"Let S, T be sets where S ∅, that is, S is not empty.

Let f: S → T be a mapping.


Let g: T → S be a mapping such that:
:f ∘ g = I_T
where:
:f ∘ g denotes the composite mapping g followed by f
:I_T is the identity mapping on T.


Then g: T → S is called a right inverse (mapping) of f.",Right Inverse
['Definitions/Inverse Elements'],Definition:Right Inverse,"Let $left( S, circ right)$ be a monoid whose identity is $e_S$.

An element $x_R in S$ is called a right inverse of $x$  if and only if :
:$x circ x_R = e_S$",Definition:Inverse (Abstract Algebra)/Right Inverse,,false,"Let ( S, ∘) be a monoid whose identity is e_S.

An element x_R ∈ S is called a right inverse of x  if and only if :
:x ∘ x_R = e_S",Right Inverse
['Definitions/Inverse Matrices'],Definition:Right Inverse,"Let $m, n in mathbb Z_{>0}$ be a (strictly) positive integer.


Let $mathbf A = left[ a right]_{m n}$ be a matrix of order $m times n$.

Let $mathbf B = left[ b right]_{n m}$ be a matrix of order $n times m$ such that:
:$mathbf A mathbf B = mathbf I_m$

where $mathbf I_m$ denotes the unit matrix of order $m$.


Then $mathbf B$ is known as a right inverse (matrix) of $mathbf A$.",Definition:Inverse Matrix/Right,,false,"Let m, n ∈ℤ_>0 be a (strictly) positive integer.


Let 𝐀 = [ a ]_m n be a matrix of order m × n.

Let 𝐁 = [ b ]_n m be a matrix of order n × m such that:
:𝐀𝐁 = 𝐈_m

where 𝐈_m denotes the unit matrix of order m.


Then 𝐁 is known as a right inverse (matrix) of 𝐀.",Right Inverse
"['Definitions/Roots of Numbers', 'Definitions/Real Analysis']",Definition:Root,"Let $x, y in mathbb R_{ge 0}$ be positive real numbers.

Let $n in mathbb Z$ be an integer such that $n ne 0$.


Then $y$ is the positive $n$th root of $x$  if and only if :
:$y^n = x$

and we write:
:$y = sqrt[n] x$


Using the power notation, this can also be written:
:$y = x^{1/n}$


When $n = 2$, we write $y = sqrt x$ and call $y$ the positive square root of $x$.

When $n = 3$, we write $y = sqrt [3] x$ and call $y$ the cube root of $x$.


Note the special case where $x = 0 = y$:
:$0 = sqrt [n] 0$


=== Index ===
Let $sqrt [n] x$ denote the $n$th root of $x$.

The number $n$ is known as the index of the root.


If $n$ is not specified, that is $sqrt x$ is presented, this means the square root.

=== Extraction of Root ===
The process of evaluating roots of a given real number is referred to as extraction.",Definition:Root of Number,,false,"Let x, y ∈ℝ_≥ 0 be positive real numbers.

Let n ∈ℤ be an integer such that n  0.


Then y is the positive nth root of x  if and only if :
:y^n = x

and we write:
:y = √(x)


Using the power notation, this can also be written:
:y = x^1/n


When n = 2, we write y = √(x) and call y the positive square root of x.

When n = 3, we write y = √(x) and call y the cube root of x.


Note the special case where x = 0 = y:
:0 = √(0)


=== Index ===
Let √(x) denote the nth root of x.

The number n is known as the index of the root.


If n is not specified, that is √(x) is presented, this means the square root.

=== Extraction of Root ===
The process of evaluating roots of a given real number is referred to as extraction.",Root
"['Definitions/Roots of Equations', 'Definitions/Roots', 'Definitions/Algebra']",Definition:Root,"Let $E left(   right)x$ be a mathematical expression representing an equation which is dependent upon a variable $x$.

A root of $E left(   right)x$ is a constant which, when substituted for $x$ in $E left(   right)x$, makes $E left(   right)x$ a true statement.


=== Extraction of Root ===
The process of finding roots of a given equation is referred to as extraction.",Definition:Root of Equation,,false,"Let E (   )x be a mathematical expression representing an equation which is dependent upon a variable x.

A root of E (   )x is a constant which, when substituted for x in E (   )x, makes E (   )x a true statement.


=== Extraction of Root ===
The process of finding roots of a given equation is referred to as extraction.",Root
"['Definitions/Roots of Mappings', 'Definitions/Ring Theory', 'Definitions/Field Theory', 'Definitions/Real Analysis', 'Definitions/Complex Analysis']",Definition:Root,"Let $f: R to R$ be a mapping on a ring $R$.

Let $x in R$.


Then the values of $x$ for which $f left(   right)x = 0_R$ are known as the roots of the mapping $f$.",Definition:Root of Mapping,,false,"Let f: R → R be a mapping on a ring R.

Let x ∈ R.


Then the values of x for which f (   )x = 0_R are known as the roots of the mapping f.",Root
"['Definitions/Roots of Polynomials', 'Definitions/Polynomial Theory', 'Definitions/Ring Theory']",Definition:Root,"Let $R$ be a commutative ring with unity.

Let $f in R left[ x right]$ be a polynomial over $R$.


A root in $R$ of $f$ is an element $x in R$ for which $f left(   right)x = 0$, where $f left(   right)x$ denotes the image of $f$ under the evaluation homomorphism at $x$.",Definition:Root of Polynomial,,false,"Let R be a commutative ring with unity.

Let f ∈ R [ x ] be a polynomial over R.


A root in R of f is an element x ∈ R for which f (   )x = 0, where f (   )x denotes the image of f under the evaluation homomorphism at x.",Root
['Definitions/Root Nodes'],Definition:Root,"Let $T$ be a rooted tree.

The root node of $T$ is the node of $T$ which is distinguished from the others by being the ancestor node of every node of $T$.",Definition:Rooted Tree/Root Node,,false,"Let T be a rooted tree.

The root node of T is the node of T which is distinguished from the others by being the ancestor node of every node of T.",Root
"['Definitions/Rooted Trees', 'Definitions/Graph Theory', 'Definitions/Tree Theory']",Definition:Root,"A rooted tree is a tree with a countable number of nodes, in which a particular node is distinguished from the others and called the root node:

:

=== Root Node ===
Let $T$ be a rooted tree.

The root node of $T$ is the node of $T$ which is distinguished from the others by being the ancestor node of every node of $T$.

=== Parent ===
Let $T$ be a rooted tree whose root is $r_T$.

Let $t$ be a node of $T$.

From Path in Tree is Unique, there is only one path from $t$ to $r_T$.

Let $pi: T setminus leftlbrace r_T rightrbrace to T$ be the mapping defined by:

:$pi left(   right)t := text {the node adjacent to $t$ on the path to $r_T$}$


Then $pi left(   right)t$ is known as the parent node of $t$.

The mapping $pi$ is called the parent mapping.

=== Ancestor ===
Let $T$ be a rooted tree with root $r_T$.

Let $t$ be a node of $T$.


An ancestor node of $t$ is a node in the path from $t$ to $r_T$.


=== Proper Ancestor ===
Let $T$ be a rooted tree with root $r_T$.

Let $t$ be a node of $T$.


A proper ancestor node of $t$ is an ancestor node of $t$ that is not $t$ itself.

=== Child Node ===
Let $T$ be a rooted tree with root $r_T$.

Let $t$ be a node of $T$.

The child nodes of $t$ are the elements of the set:
:$leftlbrace s in T: pi left(   right)s = t rightrbrace$
where $pi left(   right)s$ denotes the parent mapping of $s$.

That is, the children of $t$ are all the nodes of $T$ of which $t$ is the parent.


=== Grandchild Node ===
Let $T$ be a rooted tree with root $r_T$.

Let $t$ be a node of $T$.


A child of a child node of a node $t$ can be referred to as a grandchild node of $t$.

In terms of the parent mapping $pi$ of $T$, a grandchild node of $t$ is a node $s$ such that:

:$pi left(   right){pi left(   right)s} = t$


Category:Definitions/Descendant Nodes

=== Descendant ===
Let $T$ be a rooted tree with root $r_T$.

Let $t$ be a node of $T$.


A descendant node $s$ of a $t$ is a node such that $t$ is in the path from $s$ to $r_T$.

That is, the descendant nodes of $t$ are all the nodes of $T$ of which $t$ is an ancestor node.


=== Proper Descendant ===
Let $T$ be a rooted tree with root $r_T$.

Let $t$ be a node of $T$.


A proper descendant node of $t$ is a descendant of $t$ which is not $t$ itself.


Category:Definitions/Descendant Nodes

=== Sibling ===
Let $T$ be a rooted tree with root $r_T$.

Two children of the same node of $T$ are called siblings.

That is, siblings are nodes which both have the same parent.


Category:Definitions/Rooted Trees

=== Leaf Node ===
Let $v$ be a node of a tree $T$.

Then $v$ is a leaf node of a $T$  if and only if  $v$ is of degree $1$.


If $T$ is a rooted tree, this is equivalent to saying that $v$ has no child nodes.

=== Branch ===
Let $T$ be a rooted tree with root node $r_T$.

A subset $Gamma$ of $T$ is a branch  if and only if  all the following conditions hold:
:$(1): quad$ The root node $r_T$ belongs to $Gamma$
:$(2): quad$ The parent of each node in $Gamma setminus leftlbrace r_T rightrbrace$ is in $Gamma$
:$(3): quad$ Each node in $Gamma$ either:
::$text {(a)}: quad$ is a leaf node of $T$
:or:
::$text {(b)}: quad$ has exactly one child node in $Gamma$.


Informally, a branch of a rooted tree is the path from the root to a leaf.


=== Finite ===
Let $T$ be a rooted tree with root node $r_T$.

Let $Gamma$ be a branch of $T$.


Then $Gamma$ is finite  if and only if  it has exactly one leaf node.

=== Infinite ===
Let $T$ be a rooted tree with root node $r_T$.

Let $Gamma$ be a branch of $T$.


Then $Gamma$ is infinite  if and only if  it has no leaf node at the end.",Definition:Rooted Tree,,false,"A rooted tree is a tree with a countable number of nodes, in which a particular node is distinguished from the others and called the root node:

:

=== Root Node ===
Let T be a rooted tree.

The root node of T is the node of T which is distinguished from the others by being the ancestor node of every node of T.

=== Parent ===
Let T be a rooted tree whose root is r_T.

Let t be a node of T.

From Path in Tree is Unique, there is only one path from t to r_T.

Let π: T ∖{ r_T }→ T be the mapping defined by:

:π(   )t := the node adjacent to t on the path to r_T


Then π(   )t is known as the parent node of t.

The mapping π is called the parent mapping.

=== Ancestor ===
Let T be a rooted tree with root r_T.

Let t be a node of T.


An ancestor node of t is a node in the path from t to r_T.


=== Proper Ancestor ===
Let T be a rooted tree with root r_T.

Let t be a node of T.


A proper ancestor node of t is an ancestor node of t that is not t itself.

=== Child Node ===
Let T be a rooted tree with root r_T.

Let t be a node of T.

The child nodes of t are the elements of the set:
:{ s ∈ T: π(   )s = t }
where π(   )s denotes the parent mapping of s.

That is, the children of t are all the nodes of T of which t is the parent.


=== Grandchild Node ===
Let T be a rooted tree with root r_T.

Let t be a node of T.


A child of a child node of a node t can be referred to as a grandchild node of t.

In terms of the parent mapping π of T, a grandchild node of t is a node s such that:

:π(   )π(   )s = t


Category:Definitions/Descendant Nodes

=== Descendant ===
Let T be a rooted tree with root r_T.

Let t be a node of T.


A descendant node s of a t is a node such that t is in the path from s to r_T.

That is, the descendant nodes of t are all the nodes of T of which t is an ancestor node.


=== Proper Descendant ===
Let T be a rooted tree with root r_T.

Let t be a node of T.


A proper descendant node of t is a descendant of t which is not t itself.


Category:Definitions/Descendant Nodes

=== Sibling ===
Let T be a rooted tree with root r_T.

Two children of the same node of T are called siblings.

That is, siblings are nodes which both have the same parent.


Category:Definitions/Rooted Trees

=== Leaf Node ===
Let v be a node of a tree T.

Then v is a leaf node of a T  if and only if  v is of degree 1.


If T is a rooted tree, this is equivalent to saying that v has no child nodes.

=== Branch ===
Let T be a rooted tree with root node r_T.

A subset Γ of T is a branch  if and only if  all the following conditions hold:
:(1): The root node r_T belongs to Γ
:(2): The parent of each node in Γ∖{ r_T } is in Γ
:(3): Each node in Γ either:
::(a): is a leaf node of T
:or:
::(b): has exactly one child node in Γ.


Informally, a branch of a rooted tree is the path from the root to a leaf.


=== Finite ===
Let T be a rooted tree with root node r_T.

Let Γ be a branch of T.


Then Γ is finite  if and only if  it has exactly one leaf node.

=== Infinite ===
Let T be a rooted tree with root node r_T.

Let Γ be a branch of T.


Then Γ is infinite  if and only if  it has no leaf node at the end.",Root
['Definitions/Propositional Tableaus'],Definition:Root,"Let $left({T, mathbf H, Phi}right)$ be a labeled tree for propositional logic.


The countable set $mathbf H$ of WFFs of propositional logic is called the hypothesis set.

The elements of $mathbf H$ are known as hypothesis WFFs.


The hypothesis set $mathbf H$ is considered to be attached to the root node of $T$.",Definition:Labeled Tree for Propositional Logic/Hypothesis Set,,false,"Let (T, 𝐇, Φ) be a labeled tree for propositional logic.


The countable set 𝐇 of WFFs of propositional logic is called the hypothesis set.

The elements of 𝐇 are known as hypothesis WFFs.


The hypothesis set 𝐇 is considered to be attached to the root node of T.",Root
"['Definitions/Geometric Rotations', 'Definitions/Isometries (Euclidean Geometry)', 'Definitions/Euclidean Geometry', 'Definitions/Analytic Geometry']",Definition:Rotation,"A rotation in the context of Euclidean geometry is an isometry on a Euclidean Space $mathbb R^n$ as follows.

A rotation is defined usually for either:
:$n = 2$, representing the plane
or:
:$n = 3$, representing ordinary space.


=== Rotation in the Plane ===
A rotation $r_alpha$ in the plane is an isometry on the Euclidean Space $Gamma = mathbb R^2$ as follows.


Let $O$ be a distinguished point in $Gamma$, which has the property that:
:$r_alpha left(   right)O = O$

That is, $O$ maps to itself.


Let $P in Gamma$ such that $P ne O$.

Let $OP$ be joined by a straight line.

Let a straight line $OP'$ be constructed such that:
:$(1): quad OP' = OP$
:$(2): angle POP' = alpha$ such that $OP to OP'$ is in the anticlockwise direction:

:


Then:
:$r_alpha left(   right)P = P'$

Thus $r_alpha$ is a rotation (in the plane) of (angle) $alpha$ about (the axis) $O$.

=== Rotation in Space ===
A rotation $r_theta$ in space is an isometry on the Euclidean Space $Gamma = mathbb R^3$ as follows.


Let $AB$ be a distinguished straight line in $Gamma$, which has the property that:
:$forall P in AB: r_theta left(   right)P = P$

That is, all points on $AB$ map to themselves.


Let $P in Gamma$ such that $P notin AB$.

Let a straight line be constructed from $P$ to $O$ on $AB$ such that $OP$ is perpendicular to $AB$.

Let a straight line $OP'$ be constructed perpendicular to $AB$ such that:
:$(1): quad OP' = OP$
:$(2): quad angle POP' = theta$ such that $OP to OP'$ is in the anticlockwise direction:


:


Then:
:$r_theta left(   right)P = P'$

Thus $r_theta$ is a rotation (in space) of (angle) $theta$ about (the axis) $O$.


 

=== Axis of Rotation ===
Let $r_theta$ be a rotation in the Euclidean Space $Gamma = mathbb R^n$.


The set $A$ of points in $Gamma$ such that:
:$forall P in A: r_theta left(   right)P = P$

is called the axis of rotation of $r_theta$.

=== Vector Form ===
A space rotation $r_theta$ can be expressed as an axial vector $mathbf r_theta$ such that:
:the direction of $mathbf r_theta$ is defined to be its axis of rotation
:the length of $mathbf r_theta$ specifies its angle of rotation of $mathbf r_theta$ to an appropriate scale.

:

=== Right-Hand Rule ===
Let $mathbf V$ be an axial vector acting with respect to an axis of rotation $R$.

Consider a right hand with its fingers curled round $R$ so that the fingers are pointed in the direction of rotation of $mathbf V$ around $R$.


The right-hand rule is the convention that the direction of $mathbf V$ is the direction in which the thumb is pointing:


:",Definition:Rotation (Geometry),,false,"A rotation in the context of Euclidean geometry is an isometry on a Euclidean Space ℝ^n as follows.

A rotation is defined usually for either:
:n = 2, representing the plane
or:
:n = 3, representing ordinary space.


=== Rotation in the Plane ===
A rotation r_α in the plane is an isometry on the Euclidean Space Γ = ℝ^2 as follows.


Let O be a distinguished point in Γ, which has the property that:
:r_α(   )O = O

That is, O maps to itself.


Let P ∈Γ such that P  O.

Let OP be joined by a straight line.

Let a straight line OP' be constructed such that:
:(1):    OP' = OP
:(2): ∠ POP' = α such that OP → OP' is in the anticlockwise direction:

:


Then:
:r_α(   )P = P'

Thus r_α is a rotation (in the plane) of (angle) α about (the axis) O.

=== Rotation in Space ===
A rotation r_θ in space is an isometry on the Euclidean Space Γ = ℝ^3 as follows.


Let AB be a distinguished straight line in Γ, which has the property that:
:∀ P ∈ AB: r_θ(   )P = P

That is, all points on AB map to themselves.


Let P ∈Γ such that P ∉ AB.

Let a straight line be constructed from P to O on AB such that OP is perpendicular to AB.

Let a straight line OP' be constructed perpendicular to AB such that:
:(1):    OP' = OP
:(2):   ∠ POP' = θ such that OP → OP' is in the anticlockwise direction:


:


Then:
:r_θ(   )P = P'

Thus r_θ is a rotation (in space) of (angle) θ about (the axis) O.


 

=== Axis of Rotation ===
Let r_θ be a rotation in the Euclidean Space Γ = ℝ^n.


The set A of points in Γ such that:
:∀ P ∈ A: r_θ(   )P = P

is called the axis of rotation of r_θ.

=== Vector Form ===
A space rotation r_θ can be expressed as an axial vector 𝐫_θ such that:
:the direction of 𝐫_θ is defined to be its axis of rotation
:the length of 𝐫_θ specifies its angle of rotation of 𝐫_θ to an appropriate scale.

:

=== Right-Hand Rule ===
Let 𝐕 be an axial vector acting with respect to an axis of rotation R.

Consider a right hand with its fingers curled round R so that the fingers are pointed in the direction of rotation of 𝐕 around R.


The right-hand rule is the convention that the direction of 𝐕 is the direction in which the thumb is pointing:


:",Rotation
['Definitions/Permutation Theory'],Definition:Rotation,"Let $left( a_1, ldots, a_n right)$ be a string over an alphabet $A$.

A rotation is a mapping $r: A^n to A^n$ given by:

:$left( a_1, ldots, a_n right) mapsto left( a_{phi left(   right)1}, cdots, a_{phi left(   right)n}  right)$

where $phi$ is a permutation on n letters.

Category:Definitions/Permutation Theory",Definition:Rotation (Permutation Theory),,false,"Let ( a_1, …, a_n ) be a string over an alphabet A.

A rotation is a mapping r: A^n → A^n given by:

:( a_1, …, a_n ) ↦( a_ϕ(   )1, ⋯, a_ϕ(   )n)

where ϕ is a permutation on n letters.

Category:Definitions/Permutation Theory",Rotation
['Definitions/Truth Tables'],Definition:Row,"A row of a truth table is one of the horizontal lines that consists of instances of the symbols $T$ and $F$.

Each row contains the truth values of each of the boolean interpretations of the statement forms according to the propositional variables that comprise them.

There are as many rows in a truth table as there are combinations of $T$ and $F$ for all the propositional variables that constitute the statement forms.",Definition:Truth Table/Row,,false,"A row of a truth table is one of the horizontal lines that consists of instances of the symbols T and F.

Each row contains the truth values of each of the boolean interpretations of the statement forms according to the propositional variables that comprise them.

There are as many rows in a truth table as there are combinations of T and F for all the propositional variables that constitute the statement forms.",Row
['Definitions/Matrices'],Definition:Row,"Let $mathbf A$ be an $m times n$ matrix.

For each $i in left[ 1 ,.,.,   right]m$, the rows of $mathbf A$ are the ordered $n$-tuples:
:$r_i = left( a_{i 1}, a_{i 2}, ldots, a_{i n}  right)$

where $r_i$ is called the $i$th row of $mathbf A$.


A row of an $m times n$ matrix can also be treated as a $1 times n$ row matrix in its own right:
:$r_i = begin {bmatrix} a_{i 1} & a_{i 2} & cdots & a_{i n} end {bmatrix}$
for $i = 1, 2, ldots, m$.",Definition:Matrix/Row,,false,"Let 𝐀 be an m × n matrix.

For each i ∈[ 1  . . ]m, the rows of 𝐀 are the ordered n-tuples:
:r_i = ( a_i 1, a_i 2, …, a_i n)

where r_i is called the ith row of 𝐀.


A row of an m × n matrix can also be treated as a 1 × n row matrix in its own right:
:r_i = [ a_i 1 a_i 2     ⋯ a_i n ]
for i = 1, 2, …, m.",Row
['Definitions/Latin Squares'],Definition:Row,"Let $mathbf L$ be a Latin square.

The rows of $mathbf L$ are the lines of elements reading across the page.",Definition:Latin Square/Row,,false,"Let 𝐋 be a Latin square.

The rows of 𝐋 are the lines of elements reading across the page.",Row
"['Definitions/Vector Algebra', 'Definitions/Linear Algebra']",Definition:Scalar Field,"Let $left( G, +_G, circ right)_K$ be a vector space, where:

:$left( K, +_K, times_K right)$ is a field

:$left( G, +_G right)$ is an abelian group $left( G, +_G right)$

:$circ: K times G to G$ is a binary operation.


Then the field $left( K, +_K, times_K right)$ is called the scalar field of $left( G, +_G, circ right)_K$.


If the scalar field is understood, then $left( G, +_G, circ right)_K$ can be rendered $left( G, +_G, circ right)$.",Definition:Scalar Field (Linear Algebra),,false,"Let ( G, +_G, ∘)_K be a vector space, where:

:( K, +_K, ×_K ) is a field

:( G, +_G ) is an abelian group ( G, +_G )

:∘: K × G → G is a binary operation.


Then the field ( K, +_K, ×_K ) is called the scalar field of ( G, +_G, ∘)_K.


If the scalar field is understood, then ( G, +_G, ∘)_K can be rendered ( G, +_G, ∘).",Scalar Field
"['Definitions/Scalar Fields (Physics)', 'Definitions/Fields (Physics)', 'Definitions/Physics']",Definition:Scalar Field,"Let $F$ be a field which acts on a region of space $S$.

Let the point-function giving rise to $F$ be a scalar quantity.


Then $F$ is a scalar field.",Definition:Scalar Field (Physics),,false,"Let F be a field which acts on a region of space S.

Let the point-function giving rise to F be a scalar quantity.


Then F is a scalar field.",Scalar Field
['Definitions/Secant Function'],Definition:Secant,"=== Definition from Triangle ===
:

In the above right triangle, we are concerned about the angle $theta$.

The secant of $angle theta$ is defined as being $dfrac{text{Hypotenuse}} {text{Adjacent}}$.

=== Definition from Circle ===
=== First Quadrant ===
Consider a unit circle $C$ whose center is at the origin of a cartesian plane.


:


Let $P$ be the point on $C$ in the first quadrant such that $theta$ is the angle made by $OP$ with the $x$-axis.

Let a tangent line be drawn to touch $C$ at $A = left( 1, 0 right)$.

Let $OP$ be produced to meet this tangent line at $B$.


Then the secant of $theta$ is defined as the length of $OB$.

Hence in the first quadrant, the secant is positive.

=== Second Quadrant ===
Consider a unit circle $C$ whose center is at the origin of a cartesian plane.


:


Let $P$ be the point on $C$ in the second quadrant such that $theta$ is the angle made by $OP$ with the $x$-axis.

Let a tangent line be drawn to touch $C$ at $A = left( 1, 0 right)$.

Let $OP$ be produced to meet this tangent line at $B$.


Then the secant of $theta$ is defined as the length of $OB$.

As $OP$ needs to be produced in the opposite direction to $P$, the secant is therefore a negative function in the second quadrant.

=== Third Quadrant ===
Consider a unit circle $C$ whose center is at the origin of a cartesian plane.


:


Let $P$ be the point on $C$ in the third quadrant such that $theta$ is the angle made by $OP$ with the $x$-axis.

Let a tangent line be drawn to touch $C$ at $A = left( 1, 0 right)$.

Let $OP$ be produced to meet this tangent line at $B$.


Then the secant of $theta$ is defined as the length of $OB$.


As $OP$ needs to be produced in the opposite direction to $P$, the secant is therefore a negative function in the third quadrant.

=== Fourth Quadrant ===
Consider a unit circle $C$ whose center is at the origin of a cartesian plane.


:


Let $P$ be the point on $C$ in the fourth quadrant such that $theta$ is the angle made by $OP$ with the $x$-axis.

Let a tangent line be drawn to touch $C$ at $A = left( 1, 0 right)$.

Let $OP$ be produced to meet this tangent line at $B$.


Then the secant of $theta$ is defined as the length of $OB$.

Hence in the fourth quadrant, the secant is positive.

=== Real Function ===
Let $x in mathbb R$ be a real number.

The real function $sec x$ is defined as:

:$sec x = dfrac 1 {cos x}$

where $cos x$ is the cosine of $x$.


The definition is valid for all $x in mathbb R$ such that $cos x ne 0$.


Category:Definitions/Secant Function

=== Complex Function ===
Let $z in mathbb C$ be a complex number.

The complex function $sec z$ is defined as:

:$sec z = dfrac 1 {cos z}$

where $cos z$ is the cosine of $z$.


The definition is valid for all $z in mathbb C$ such that $cos z ne 0$.


Category:Definitions/Secant Function",Definition:Secant Function,,false,"=== Definition from Triangle ===
:

In the above right triangle, we are concerned about the angle θ.

The secant of ∠θ is defined as being HypotenuseAdjacent.

=== Definition from Circle ===
=== First Quadrant ===
Consider a unit circle C whose center is at the origin of a cartesian plane.


:


Let P be the point on C in the first quadrant such that θ is the angle made by OP with the x-axis.

Let a tangent line be drawn to touch C at A = ( 1, 0 ).

Let OP be produced to meet this tangent line at B.


Then the secant of θ is defined as the length of OB.

Hence in the first quadrant, the secant is positive.

=== Second Quadrant ===
Consider a unit circle C whose center is at the origin of a cartesian plane.


:


Let P be the point on C in the second quadrant such that θ is the angle made by OP with the x-axis.

Let a tangent line be drawn to touch C at A = ( 1, 0 ).

Let OP be produced to meet this tangent line at B.


Then the secant of θ is defined as the length of OB.

As OP needs to be produced in the opposite direction to P, the secant is therefore a negative function in the second quadrant.

=== Third Quadrant ===
Consider a unit circle C whose center is at the origin of a cartesian plane.


:


Let P be the point on C in the third quadrant such that θ is the angle made by OP with the x-axis.

Let a tangent line be drawn to touch C at A = ( 1, 0 ).

Let OP be produced to meet this tangent line at B.


Then the secant of θ is defined as the length of OB.


As OP needs to be produced in the opposite direction to P, the secant is therefore a negative function in the third quadrant.

=== Fourth Quadrant ===
Consider a unit circle C whose center is at the origin of a cartesian plane.


:


Let P be the point on C in the fourth quadrant such that θ is the angle made by OP with the x-axis.

Let a tangent line be drawn to touch C at A = ( 1, 0 ).

Let OP be produced to meet this tangent line at B.


Then the secant of θ is defined as the length of OB.

Hence in the fourth quadrant, the secant is positive.

=== Real Function ===
Let x ∈ℝ be a real number.

The real function x is defined as:

:x =  1 cos x

where cos x is the cosine of x.


The definition is valid for all x ∈ℝ such that cos x  0.


Category:Definitions/Secant Function

=== Complex Function ===
Let z ∈ℂ be a complex number.

The complex function z is defined as:

:z =  1 cos z

where cos z is the cosine of z.


The definition is valid for all z ∈ℂ such that cos z  0.


Category:Definitions/Secant Function",Secant
['Definitions/Analytic Geometry'],Definition:Secant,"Let $f: mathbb R to mathbb R$ be a real function.

Let the graph of $f$ be depicted on a Cartesian plane.


:


A secant to $f$ is a straight line which intersects the graph of $f$ in (at least) two points.


In the above diagram, the secant is the line $AB$ in  .",Definition:Secant Line,,false,"Let f: ℝ→ℝ be a real function.

Let the graph of f be depicted on a Cartesian plane.


:


A secant to f is a straight line which intersects the graph of f in (at least) two points.


In the above diagram, the secant is the line AB in  .",Secant
"['Definitions/Order Theory', 'Definitions/Upper Sections']",Definition:Section,"Let $left( S, preceq right)$ be an ordered set.

Let $U subseteq S$.


=== Definition 1 ===
Let $left( S, preceq right)$ be an ordered set.

Let $U subseteq S$.


$U$ is an upper section in $S$  if and only if :

:$forall u in U: forall s in S: u preceq s implies s in U$

=== Definition 2 ===
Let $left( S, preceq right)$ be an ordered set.

Let $U subseteq S$.


$U$ is an upper section in $S$  if and only if :

:$U^succeq subseteq U$

where $U^succeq$ is the upper closure of $U$.

=== Definition 3 ===
Let $left( S, preceq right)$ be an ordered set.

Let $U subseteq S$.


$U$ is an upper section in $S$  if and only if :

:$U^succeq = U$

where $U^succeq$ is the upper closure of $U$.",Definition:Upper Section,,false,"Let ( S, ≼) be an ordered set.

Let U ⊆ S.


=== Definition 1 ===
Let ( S, ≼) be an ordered set.

Let U ⊆ S.


U is an upper section in S  if and only if :

:∀ u ∈ U: ∀ s ∈ S: u ≼ s  s ∈ U

=== Definition 2 ===
Let ( S, ≼) be an ordered set.

Let U ⊆ S.


U is an upper section in S  if and only if :

:U^≽⊆ U

where U^≽ is the upper closure of U.

=== Definition 3 ===
Let ( S, ≼) be an ordered set.

Let U ⊆ S.


U is an upper section in S  if and only if :

:U^≽ = U

where U^≽ is the upper closure of U.",Section
"['Definitions/Order Theory', 'Definitions/Lower Sections']",Definition:Section,"Let $left( S, preceq right)$ be an ordered set.

Let $L subseteq S$.


=== Definition 1 ===
Let $left( S, preceq right)$ be an ordered set.

Let $L subseteq S$.


$L$ is a lower section in $S$  if and only if :
:$forall l in L, s in S: s preceq l implies s in L$

=== Definition 2 ===
Let $left( S, preceq right)$ be an ordered set.

Let $L subseteq S$.


$L$ is a lower section in $S$  if and only if :
:$L^preceq subseteq L$

where $L^preceq$ is the lower closure of $L$.

=== Definition 3 ===
Let $left( S, preceq right)$ be an ordered set.

Let $L subseteq S$.


$L$ is a lower section in $S$  if and only if :
:$L^preceq = L$

where $L^preceq$ is the lower closure of $L$.

=== Class Theory ===
 
Let $A$ be a class under a total ordering $preccurlyeq$.

Let $L$ be a subclass of $A$ such that:
:$forall x in L: forall a in A setminus L: x preccurlyeq a$
where $A setminus L$ is the difference between $A$ and $L$.

Then $L$ is known as a lower section of $A$.",Definition:Lower Section,,false,"Let ( S, ≼) be an ordered set.

Let L ⊆ S.


=== Definition 1 ===
Let ( S, ≼) be an ordered set.

Let L ⊆ S.


L is a lower section in S  if and only if :
:∀ l ∈ L, s ∈ S: s ≼ l  s ∈ L

=== Definition 2 ===
Let ( S, ≼) be an ordered set.

Let L ⊆ S.


L is a lower section in S  if and only if :
:L^≼⊆ L

where L^≼ is the lower closure of L.

=== Definition 3 ===
Let ( S, ≼) be an ordered set.

Let L ⊆ S.


L is a lower section in S  if and only if :
:L^≼ = L

where L^≼ is the lower closure of L.

=== Class Theory ===
 
Let A be a class under a total ordering ≼.

Let L be a subclass of A such that:
:∀ x ∈ L: ∀ a ∈ A ∖ L: x ≼ a
where A ∖ L is the difference between A and L.

Then L is known as a lower section of A.",Section
"['Definitions/Golden Mean', 'Definitions/Fibonacci Numbers', 'Definitions/Real Analysis', 'Definitions/Number Theory', 'Definitions/Algebra', 'Definitions/Geometry', 'Definitions/Specific Numbers']",Definition:Section,"=== Definition 1 ===
Let a line segment $AB$ be divided at $C$ such that:
:$AB : AC = AC : BC$

Then the golden mean $phi$ is defined as:
:$phi := dfrac {AB} {AC}$

=== Definition 2 ===
The golden mean is the unique positive real number $phi$ satisfying:
:$phi = dfrac {1 + sqrt 5} 2$

=== Definition 3 ===
The golden mean is the unique positive real number $phi$ satisfying:
:$phi = dfrac 1 {phi - 1}$",Definition:Golden Mean,,false,"=== Definition 1 ===
Let a line segment AB be divided at C such that:
:AB : AC = AC : BC

Then the golden mean ϕ is defined as:
:ϕ := ABAC

=== Definition 2 ===
The golden mean is the unique positive real number ϕ satisfying:
:ϕ = 1 + √(5) 2

=== Definition 3 ===
The golden mean is the unique positive real number ϕ satisfying:
:ϕ =  1 ϕ - 1",Section
"['Definitions/Order Theory', 'Definitions/Real Analysis', 'Definitions/Dedekind Cuts']",Definition:Section,"Let $left( S, preceq right)$ be a totally ordered set.


=== Definition 1 ===
Let $left( S, preceq right)$ be a totally ordered set.


A Dedekind cut of $left( S, preceq right)$ is a non-empty proper subset $L subsetneq S$ such that:
:$(1): quad forall x in L: forall y in S: y prec x implies y in L$ ($L$ is a lower section in $S$)
:$(2): quad forall x in L: exists y in L: x prec y$

=== Definition 2 ===
Let $left( S, preceq right)$ be a totally ordered set.


A Dedekind cut of $left( S, preceq right)$ is an ordered pair $left( L, R right)$ such that:
:$(1): quad leftlbrace L, R rightrbrace$ is a partition of $S$.
:$(2): quad L$ does not have a greatest element.
:$(3): quad forall x in L: forall y in R: x prec y$.",Definition:Dedekind Cut,,false,"Let ( S, ≼) be a totally ordered set.


=== Definition 1 ===
Let ( S, ≼) be a totally ordered set.


A Dedekind cut of ( S, ≼) is a non-empty proper subset L ⊊ S such that:
:(1):   ∀ x ∈ L: ∀ y ∈ S: y ≺ x  y ∈ L (L is a lower section in S)
:(2):   ∀ x ∈ L: ∃ y ∈ L: x ≺ y

=== Definition 2 ===
Let ( S, ≼) be a totally ordered set.


A Dedekind cut of ( S, ≼) is an ordered pair ( L, R ) such that:
:(1):   { L, R } is a partition of S.
:(2):    L does not have a greatest element.
:(3):   ∀ x ∈ L: ∀ y ∈ R: x ≺ y.",Section
['Definitions/Geometry'],Definition:Section,"=== Section of Line by Line ===
Let a geometrical line $A$ cross over (or intersect) another line $B$.

The point where they cross is called the section of the $B$ by $A$ (or equivalently, of $A$ by $B$).


Category:Definitions/Geometry

=== Plane Section ===
Let $F$ be a $3$-dimensional figure.

A plane section of $F$ is the intersection of $F$ with a plane.

Category:Definitions/Geometry",Definition:Section (Geometry),,false,"=== Section of Line by Line ===
Let a geometrical line A cross over (or intersect) another line B.

The point where they cross is called the section of the B by A (or equivalently, of A by B).


Category:Definitions/Geometry

=== Plane Section ===
Let F be a 3-dimensional figure.

A plane section of F is the intersection of F with a plane.

Category:Definitions/Geometry",Section
['Definitions/Geometry'],Definition:Section,"Let a geometrical line $A$ cross over (or intersect) another line $B$.

The point where they cross is called the section of the $B$ by $A$ (or equivalently, of $A$ by $B$).


Category:Definitions/Geometry",Definition:Section of Line by Line,,false,"Let a geometrical line A cross over (or intersect) another line B.

The point where they cross is called the section of the B by A (or equivalently, of A by B).


Category:Definitions/Geometry",Section
['Definitions/Geometry'],Definition:Section,"The intersection of two lines $AB$ and $CD$ is denoted by $AB cap CD$.

The intersection of two geometric figures is the set of points shared by both figures.


Note that this use of $cap$ is consistent with that of its more usual context of set intersection.


When two lines intersect, they are said to cut each other.",Definition:Intersection (Geometry),,false,"The intersection of two lines AB and CD is denoted by AB ∩ CD.

The intersection of two geometric figures is the set of points shared by both figures.


Note that this use of ∩ is consistent with that of its more usual context of set intersection.


When two lines intersect, they are said to cut each other.",Section
"['Definitions/Plane Sections', 'Definitions/Solid Geometry']",Definition:Section,"Let $F$ be a $3$-dimensional figure.

A plane section of $F$ is the intersection of $F$ with a plane.",Definition:Plane Section,,false,"Let F be a 3-dimensional figure.

A plane section of F is the intersection of F with a plane.",Section
"['Definitions/Plane Sections', 'Definitions/Solid Geometry']",Definition:Section,A cross-section of a 3-dimensional figure $F$ is a plane section of $F$ with a plane which is perpendicular to an axis of $F$.,Definition:Cross-Section,,false,A cross-section of a 3-dimensional figure F is a plane section of F with a plane which is perpendicular to an axis of F.,Section
['Definitions/Topology'],Definition:Section,"Let $M, E$ be topological spaces. 

Let $pi: E to M$ be a continuous surjection. 

Let $I_M: M to M$ be the identity mapping on $M$. 


Then a section of $E$ is a continuous mapping $s: M to E$ such that $pi circ s = I_M$.",Definition:Section (Topology),,false,"Let M, E be topological spaces. 

Let π: E → M be a continuous surjection. 

Let I_M: M → M be the identity mapping on M. 


Then a section of E is a continuous mapping s: M → E such that π∘ s = I_M.",Section
"['Definitions/Bisection', 'Definitions/Geometry']",Definition:Section,"To bisect a finite geometrical object is to cut it in half, that is, into two equal parts.

The act of cutting in half is known as bisection.


=== Bisector ===
A bisector is an object which bisects another object.


=== Angle Bisector ===
:

Let $angle ABC$ be an angle.

The angle bisector of $angle ABC$ is the straight line which bisects $angle ABC$.


In the above diagram, $BD$ is the angle bisector of $angle ABC$.

Thus $angle ABD cong angle DBC$ and $angle ABD + angle DBC = angle ABC$.


=== Internal ===
:

Let $angle APB$ be an angle.

The internal angle bisector of $angle APB$ is the straight line which bisects $angle APB$.


In the above diagram, $PC$ is the internal angle bisector of $angle APB$.

Thus $angle APC cong angle BPC$ and $angle APC + angle BPC = angle APB$.

=== External ===
:

Let $angle APB$ be an angle.

Let $BP$ be produced beyond $P$ to $B'$.

The external angle bisector of $angle APB$ is the straight line which bisects $angle APB'$.


In the above diagram, $PD$ is the external angle bisector of $angle APB$.

Thus $angle APD cong angle B'PD$ and $angle APD + angle B'PD = angle APB'$.

=== Line Bisector ===
 

=== Perpendicular Bisector ===
Let $AB$ be a line segment.

The perpendicular bisector of $AB$ is the straight line which:

:is perpendicular to $AB$

:passes through the point which bisects $AB$.


:

=== Midpoint of Line ===
Let $L = AB$ be a line segment whose endpoints are $A$ and $B$.

Let $M$ be a point on $L$ such that the line segment $AM$ is equal to the line segment $MB$.

That is, let $M$ be the bisector of $L$.


Then $M$ is the midpoint of $L$.",Definition:Bisection,,false,"To bisect a finite geometrical object is to cut it in half, that is, into two equal parts.

The act of cutting in half is known as bisection.


=== Bisector ===
A bisector is an object which bisects another object.


=== Angle Bisector ===
:

Let ∠ ABC be an angle.

The angle bisector of ∠ ABC is the straight line which bisects ∠ ABC.


In the above diagram, BD is the angle bisector of ∠ ABC.

Thus ∠ ABD ≅∠ DBC and ∠ ABD + ∠ DBC = ∠ ABC.


=== Internal ===
:

Let ∠ APB be an angle.

The internal angle bisector of ∠ APB is the straight line which bisects ∠ APB.


In the above diagram, PC is the internal angle bisector of ∠ APB.

Thus ∠ APC ≅∠ BPC and ∠ APC + ∠ BPC = ∠ APB.

=== External ===
:

Let ∠ APB be an angle.

Let BP be produced beyond P to B'.

The external angle bisector of ∠ APB is the straight line which bisects ∠ APB'.


In the above diagram, PD is the external angle bisector of ∠ APB.

Thus ∠ APD ≅∠ B'PD and ∠ APD + ∠ B'PD = ∠ APB'.

=== Line Bisector ===
 

=== Perpendicular Bisector ===
Let AB be a line segment.

The perpendicular bisector of AB is the straight line which:

:is perpendicular to AB

:passes through the point which bisects AB.


:

=== Midpoint of Line ===
Let L = AB be a line segment whose endpoints are A and B.

Let M be a point on L such that the line segment AM is equal to the line segment MB.

That is, let M be the bisector of L.


Then M is the midpoint of L.",Section
"['Definitions/Segments of Circles', 'Definitions/Circles']",Definition:Segment,":

 
: 
:A segment of a circle is the figure contained by a straight line and a circumference of a circle.
 ''
 

=== Base ===
:

The base of a segment of a circle is the straight line forming one of the boundaries of the seqment.

In the above diagram, $AB$ is the base of the highlighted segment.


Category:Definitions/Segments of Circles

=== Angle of a Segment ===
 
: 
:An angle of a segment is that contained by a straight line and a circumference of a circle.
 ''
 
That is, it is the angle the base makes with the circumference where they meet.


It can also be defined as the angle between the base and the tangent to the circle at the end of the base:

:


Category:Definitions/Segments of Circles

=== Angle in a Segment ===
 
: 
:An angle in a segment is the angle which, when a point is taken on the circumference of the segment and straight lines are joined from it to the extremities of the straight line which is the base of the segment, is contained by the straight lines so joined.
 ''
 
: 
:And, when the straight lines containing the angle cut off a circumference, the angle is said to stand upon that circumference.
 ''
 

:

Such a segment is said to admit the angle specified.


Category:Definitions/Segments of Circles

=== Similar Segments ===
 
: 
:Similar segments of circles are those which admit equal angles, or in which the angles are equal to one another.
 ''
 


Category:Definitions/Segments of Circles

=== Major Segment ===
Let $AB$ be a chord of a circle $mathcal C$ defined by the points $A$ and $B$ on the circumference of $mathcal C$.

The major segment of $mathcal C$   $AB$ is the segment between $AB$ and the major arc of $mathcal C$ between $A$ and $B$.

=== Minor Segment ===
Let $AB$ be a chord of a circle $mathcal C$ defined by the points $A$ and $B$ on the circumference of $mathcal C$.

The minor segment of $mathcal C$   $AB$ is the segment between $AB$ and the minor arc of $mathcal C$ between $A$ and $B$.",Definition:Segment of Circle,,false,":

 
: 
:A segment of a circle is the figure contained by a straight line and a circumference of a circle.
 ”
 

=== Base ===
:

The base of a segment of a circle is the straight line forming one of the boundaries of the seqment.

In the above diagram, AB is the base of the highlighted segment.


Category:Definitions/Segments of Circles

=== Angle of a Segment ===
 
: 
:An angle of a segment is that contained by a straight line and a circumference of a circle.
 ”
 
That is, it is the angle the base makes with the circumference where they meet.


It can also be defined as the angle between the base and the tangent to the circle at the end of the base:

:


Category:Definitions/Segments of Circles

=== Angle in a Segment ===
 
: 
:An angle in a segment is the angle which, when a point is taken on the circumference of the segment and straight lines are joined from it to the extremities of the straight line which is the base of the segment, is contained by the straight lines so joined.
 ”
 
: 
:And, when the straight lines containing the angle cut off a circumference, the angle is said to stand upon that circumference.
 ”
 

:

Such a segment is said to admit the angle specified.


Category:Definitions/Segments of Circles

=== Similar Segments ===
 
: 
:Similar segments of circles are those which admit equal angles, or in which the angles are equal to one another.
 ”
 


Category:Definitions/Segments of Circles

=== Major Segment ===
Let AB be a chord of a circle 𝒞 defined by the points A and B on the circumference of 𝒞.

The major segment of 𝒞   AB is the segment between AB and the major arc of 𝒞 between A and B.

=== Minor Segment ===
Let AB be a chord of a circle 𝒞 defined by the points A and B on the circumference of 𝒞.

The minor segment of 𝒞   AB is the segment between AB and the minor arc of 𝒞 between A and B.",Segment
['Definitions/Lines'],Definition:Segment,"A line segment is any line (straight or not) which terminates at two points.


=== Straight Line Segment ===
A straight line segment is a line segment which is straight.


 
:A straight line segment can be drawn joining any two points.
 


Thus a definition for straight line which is frequently encountered is:
:A straight line is the shortest distance between two points.
This is all very well but it assumes that the line in question terminates at two particular endpoints.

=== Endpoint ===
Each of the points at either end of a line segment is called an endpoint of that line segment.

Similarly, the point at which an infinite half-line terminates is called the endpoint of that line.


 
: 
:The extremities of a line are points.
 ''
 

=== Midpoint ===
Let $L = AB$ be a line segment whose endpoints are $A$ and $B$.

Let $M$ be a point on $L$ such that the line segment $AM$ is equal to the line segment $MB$.

That is, let $M$ be the bisector of $L$.


Then $M$ is the midpoint of $L$.",Definition:Line/Segment,,false,"A line segment is any line (straight or not) which terminates at two points.


=== Straight Line Segment ===
A straight line segment is a line segment which is straight.


 
:A straight line segment can be drawn joining any two points.
 


Thus a definition for straight line which is frequently encountered is:
:A straight line is the shortest distance between two points.
This is all very well but it assumes that the line in question terminates at two particular endpoints.

=== Endpoint ===
Each of the points at either end of a line segment is called an endpoint of that line segment.

Similarly, the point at which an infinite half-line terminates is called the endpoint of that line.


 
: 
:The extremities of a line are points.
 ”
 

=== Midpoint ===
Let L = AB be a line segment whose endpoints are A and B.

Let M be a point on L such that the line segment AM is equal to the line segment MB.

That is, let M be the bisector of L.


Then M is the midpoint of L.",Segment
"['Definitions/Initial Segments', 'Definitions/Order Theory']",Definition:Segment,"Let $left( S, preceq right)$ be a well-ordered set.

Let $a in S$.


The initial segment (of $S$) determined by $a$ is defined as:

:$S_a := leftlbrace b in S: b preceq a land b ne a rightrbrace$

which can also be rendered as:

:$S_a := leftlbrace b in S: b prec a rightrbrace$


That is, $S_a$ is the set of all elements of $S$ that strictly precede $a$.

That is, $S_a$ is the strict lower closure of $a$ (in $S$).


By extension, $S_a$ is described as an initial segment (of $S$).


=== Class Theoretical Definition ===
 
Let $A$ be a class.

Let $preceq$ be a well-ordering on $A$.

Let $a in A$.


The initial segment (of $A$) determined by $a$ is defined as:

:$A_a := leftlbrace b in S: b preceq a land b ne a rightrbrace$

which can also be rendered as:

:$A_a := leftlbrace b in S: b prec a rightrbrace$",Definition:Initial Segment,,false,"Let ( S, ≼) be a well-ordered set.

Let a ∈ S.


The initial segment (of S) determined by a is defined as:

:S_a := { b ∈ S: b ≼ a  b  a }

which can also be rendered as:

:S_a := { b ∈ S: b ≺ a }


That is, S_a is the set of all elements of S that strictly precede a.

That is, S_a is the strict lower closure of a (in S).


By extension, S_a is described as an initial segment (of S).


=== Class Theoretical Definition ===
 
Let A be a class.

Let ≼ be a well-ordering on A.

Let a ∈ A.


The initial segment (of A) determined by a is defined as:

:A_a := { b ∈ S: b ≼ a  b  a }

which can also be rendered as:

:A_a := { b ∈ S: b ≺ a }",Segment
['Definitions/Lower Closures'],Definition:Segment,"Let $left( S, preccurlyeq right)$ be an ordered set.

Let $a in S$.


The lower closure of $a$ (in $S$) is defined as:

:$a^preccurlyeq := leftlbrace b in S: b preccurlyeq a rightrbrace$


That is, $a^preccurlyeq$ is the set of all elements of $S$ that precede $a$.


=== Class Theory ===
 
Let $A$ be a class under an ordering $preccurlyeq$.

Let $a in A$.


The lower closure of $a$ (in $A$) is defined as:

:$a^preccurlyeq := leftlbrace b in A: b preccurlyeq a rightrbrace$",Definition:Lower Closure/Element,,false,"Let ( S, ≼) be an ordered set.

Let a ∈ S.


The lower closure of a (in S) is defined as:

:a^≼ := { b ∈ S: b ≼ a }


That is, a^≼ is the set of all elements of S that precede a.


=== Class Theory ===
 
Let A be a class under an ordering ≼.

Let a ∈ A.


The lower closure of a (in A) is defined as:

:a^≼ := { b ∈ A: b ≼ a }",Segment
['Definitions/Matrix Theory'],Definition:Segment,"Let $mathbf A$ be a matrix with $m$ rows and $n$ columns.


A submatrix of $mathbf A$ is a matrix formed by selecting from $mathbf A$:
:a subset of the rows
and:
:a subset of the columns
and forming a new matrix by using those entries, in the same relative positions, that appear in both the rows and columns of those selected.",Definition:Submatrix,,false,"Let 𝐀 be a matrix with m rows and n columns.


A submatrix of 𝐀 is a matrix formed by selecting from 𝐀:
:a subset of the rows
and:
:a subset of the columns
and forming a new matrix by using those entries, in the same relative positions, that appear in both the rows and columns of those selected.",Segment
"['Definitions/Countability Axioms', 'Definitions/Separable Spaces']",Definition:Separable,"A topological space $T = left( S, tau right)$ is separable  if and only if  there exists a countable subset of $S$ which is everywhere dense in $T$.


=== Normed Vector Space ===
Let $M = left( X, leftlVert , cdot , rightrVert  right)$ be a normed vector space.

Let $Y subseteq X$ be a subset of $X$.

Let $Y$ be countable set and (everywhere) dense in $X$.

In other words, suppose $Y = leftlbrace y_i : i in mathbb N rightrbrace$ such that:

:$forall x in X : forall epsilon in mathbb R_{> 0} : epsilon > 0 : exists y_{n mathop in mathbb N} in Y : leftlVert y_n - x rightrVert < epsilon$


Then $X$ is separable.

 ",Definition:Separable Space,,false,"A topological space T = ( S, τ) is separable  if and only if  there exists a countable subset of S which is everywhere dense in T.


=== Normed Vector Space ===
Let M = ( X, ‖ · ‖) be a normed vector space.

Let Y ⊆ X be a subset of X.

Let Y be countable set and (everywhere) dense in X.

In other words, suppose Y = { y_i : i ∈ℕ} such that:

:∀ x ∈ X : ∀ϵ∈ℝ_> 0 : ϵ > 0 : ∃ y_n ∈ℕ∈ Y : ‖ y_n - x ‖ < ϵ


Then X is separable.

 ",Separable
['Definitions/Countability Axioms'],Definition:Separable,"A topological space $T = left( S, tau right)$ is second-countable or satisfies the Second Axiom of Countability  if and only if  its topology has a countable basis.",Definition:Second-Countable Space,,false,"A topological space T = ( S, τ) is second-countable or satisfies the Second Axiom of Countability  if and only if  its topology has a countable basis.",Separable
['Definitions/Field Extensions'],Definition:Separable,"Let $L/K$ be a field extension.

Let $alphain L$.


Then $alpha$ is separable over $K$  if and only if  its minimal polynomial over $K$ is separable.",Definition:Separable Element,,false,"Let L/K be a field extension.

Let α∈ L.


Then α is separable over K  if and only if  its minimal polynomial over K is separable.",Separable
['Definitions/Field Extensions'],Definition:Separable,"Let $K$ be a field.

Let $L/K$ be an algebraic field extension.


Then $L/K$ is a separable extension  if and only if  every $alphain L$ is separable over $K$.

That is:
:For every $alpha in L$, its minimal polynomial over $K$ is separable.",Definition:Separable Extension,,false,"Let K be a field.

Let L/K be an algebraic field extension.


Then L/K is a separable extension  if and only if  every α∈ L is separable over K.

That is:
:For every α∈ L, its minimal polynomial over K is separable.",Separable
"['Definitions/Separable Polynomials', 'Definitions/Polynomial Theory', 'Definitions/Field Theory']",Definition:Separable,"Let $K$ be a field.

Let $P left(   right)X in K left[ X right]$ be a polynomial of degree $n$.


=== Definition 1 ===
Let $K$ be a field.

Let $P left(   right)X in K left[ X right]$ be a polynomial of degree $n$.


$P$ is separable  if and only if  its roots are distinct in an algebraic closure of $K$.


Category:Definitions/Separable Polynomials

=== Definition 2 ===
Let $K$ be a field.

Let $P left(   right)X in K left[ X right]$ be a polynomial of degree $n$.


$P$ is separable  if and only if  it has no double roots in every field extension of $K$.


Category:Definitions/Separable Polynomials

=== Definition 3 ===
Let $K$ be a field.

Let $P left(   right)X in K left[ X right]$ be a polynomial of degree $n$.


$P$ is separable  if and only if  it has $n$ distinct roots in every field extension where $P$ splits.


Category:Definitions/Separable Polynomials",Definition:Separable Polynomial,,false,"Let K be a field.

Let P (   )X ∈ K [ X ] be a polynomial of degree n.


=== Definition 1 ===
Let K be a field.

Let P (   )X ∈ K [ X ] be a polynomial of degree n.


P is separable  if and only if  its roots are distinct in an algebraic closure of K.


Category:Definitions/Separable Polynomials

=== Definition 2 ===
Let K be a field.

Let P (   )X ∈ K [ X ] be a polynomial of degree n.


P is separable  if and only if  it has no double roots in every field extension of K.


Category:Definitions/Separable Polynomials

=== Definition 3 ===
Let K be a field.

Let P (   )X ∈ K [ X ] be a polynomial of degree n.


P is separable  if and only if  it has n distinct roots in every field extension where P splits.


Category:Definitions/Separable Polynomials",Separable
"['Definitions/Separable Differential Equations', 'Definitions/Ordinary Differential Equations']",Definition:Separable,"A first order ordinary differential equation which can be expressed in the form:
:$dfrac {mathrm d y} {mathrm d x} = g left(   right)x h left(   right)y$
is known as a separable differential equation.


Its general solution is found by solving the integration:
:$ds int frac {mathrm d y} {h left(   right)y} = int g left(   right)x ,mathrm d x + C$


=== General Form ===
A first order ordinary differential equation which can be expressed in the form:
:$g_1 left(   right)x h_1 left(   right)y + g_2 left(   right)x h_2 left(   right)y dfrac {mathrm d y} {mathrm d x} = 0$
is known as a separable differential equation.


Its general solution is found by solving the integration:
:$ds int frac {g_1 left(   right)x} {g_2 left(   right)x} ,mathrm d x + int frac {h_2 left(   right)y} {h_1 left(   right)y} ,mathrm d y = C$",Definition:Separable Differential Equation,,false,"A first order ordinary differential equation which can be expressed in the form:
:d yd x = g (   )x h (   )y
is known as a separable differential equation.


Its general solution is found by solving the integration:
:∫d y/h (   )y = ∫ g (   )x  d x + C


=== General Form ===
A first order ordinary differential equation which can be expressed in the form:
:g_1 (   )x h_1 (   )y + g_2 (   )x h_2 (   )y d yd x = 0
is known as a separable differential equation.


Its general solution is found by solving the integration:
:∫g_1 (   )x/g_2 (   )x d x + ∫h_2 (   )y/h_1 (   )y d y = C",Separable
['Definitions/Topology'],Definition:Separated,"Let $left( S, tau right)$ be a topological space.

Let $x, y in S$ such that both of the following hold:

:$exists U in tau: x in U, y notin U$
:$exists V in tau: y in V, x notin V$


Then $x$ and $y$ are separated points.",Definition:Separated Points,,false,"Let ( S, τ) be a topological space.

Let x, y ∈ S such that both of the following hold:

:∃ U ∈τ: x ∈ U, y ∉ U
:∃ V ∈τ: y ∈ V, x ∉ V


Then x and y are separated points.",Separated
['Definitions/Topology'],Definition:Separated,"Let $T = left( S, tau right)$ be a topological space.

Let $A, B subseteq S$.


=== Definition 1 ===
Let $T = left( S, tau right)$ be a topological space.

Let $A, B subseteq S$.


$A$ and $B$ are separated (in $T$)  if and only if :
:$A^- cap B = A cap B^- = varnothing$
where:
:$A^-$ denotes the closure of $A$ in $T$
:$varnothing$ denotes the empty set.


$A$ and $B$ are said to be separated sets (of $T$).

=== Definition 2 ===
Let $T = left( S, tau right)$ be a topological space.

Let $A, B subseteq S$.


$A$ and $B$ are separated (in $T$)  if and only if  there exist $U,Vintau$ with:
:$A subset U$ and $U cap B = varnothing$
:$B subset V$ and $V cap A = varnothing$
where $varnothing$ denotes the empty set.


$A$ and $B$ are said to be separated sets (of $T$).

Category:Definitions/Topology

$A$ and $B$ are said to be separated sets (of $T$).",Definition:Separated Sets,,false,"Let T = ( S, τ) be a topological space.

Let A, B ⊆ S.


=== Definition 1 ===
Let T = ( S, τ) be a topological space.

Let A, B ⊆ S.


A and B are separated (in T)  if and only if :
:A^- ∩ B = A ∩ B^- = ∅
where:
:A^- denotes the closure of A in T
:∅ denotes the empty set.


A and B are said to be separated sets (of T).

=== Definition 2 ===
Let T = ( S, τ) be a topological space.

Let A, B ⊆ S.


A and B are separated (in T)  if and only if  there exist U,V∈τ with:
:A ⊂ U and U ∩ B = ∅
:B ⊂ V and V ∩ A = ∅
where ∅ denotes the empty set.


A and B are said to be separated sets (of T).

Category:Definitions/Topology

A and B are said to be separated sets (of T).",Separated
"['Definitions/Separated by Neighborhoods', 'Definitions/Separation Axioms']",Definition:Separated,"Let $T = left( S, tau right)$ be a topological space.


=== Sets ===
Let $T = left( S, tau right)$ be a topological space.


=== Definition 1 ===

Let $T = left( S, tau right)$ be a topological space.


Let $A, B subseteq S$ such that:
:$exists N_A, N_B subseteq S: exists U, V in tau: A subseteq U subseteq N_A, B subseteq V subseteq N_B: N_A cap N_B = varnothing$


That is, that $A$ and $B$ both have neighborhoods in $T$ which are disjoint.


Then $A$ and $B$ are described as separated by neighborhoods.


Category:Definitions/Separated by Neighborhoods


=== Definition 2 ===
Let $T = left( S, tau right)$ be a topological space.


Let $A, B subseteq S$ such that:
:$exists U, V in tau: A subseteq U, B subseteq V: U cap V = varnothing$


That is, that $A$ and $B$ both have open neighborhoods in $T$ which are disjoint.


Then $A$ and $B$ are described as separated by (open) neighborhoods.



Category:Definitions/Separated by Neighborhoods

=== Points ===
Let $T = left({S, tau}right)$ be a topological space.


=== Definition 1 ===

Let $T = left( S, tau right)$ be a topological space.


Let $x, y in S$ such that:

:$exists N_x, N_y subseteq S: exists U, V in tau: x in U subseteq N_x, y in V subseteq N_y: N_x cap N_y = varnothing$


That is, that $x$ and $y$ both have neighborhoods in $T$ which are disjoint.


Then $x$ and $y$ are described as separated by neighborhoods.


Thus two points are separated by neighborhoods $x$ and $y$  if and only if  the two singleton sets $leftlbrace x rightrbrace$ and $leftlbrace y rightrbrace$ are separated by neighborhoods as sets.

Category:Definitions/Separated by Neighborhoods


=== Definition 2 ===
Let $T = left( S, tau right)$ be a topological space.


Let $x, y in S$ such that:

:$exists U, V in tau: x in U, y in V: U cap V = varnothing$


That is, that $x$ and $y$ both have open neighborhoods in $T$ which are disjoint.


Then $x$ and $y$ are described as separated by (open) neighborhoods.


Thus two points $x$ and $y$ are separated by neighborhoods  if and only if  the two singleton sets $leftlbrace x rightrbrace$ and $leftlbrace y rightrbrace$ are separated by open neighborhoods as sets.

Category:Definitions/Separated by Neighborhoods


Thus two points $x$ and $y$ are separated by neighborhoods  if and only if  the two singleton sets $left{{x}right}$ and $left{{y}right}$ are separated by neighborhoods as sets.",Definition:Separated by Neighborhoods,,false,"Let T = ( S, τ) be a topological space.


=== Sets ===
Let T = ( S, τ) be a topological space.


=== Definition 1 ===

Let T = ( S, τ) be a topological space.


Let A, B ⊆ S such that:
:∃ N_A, N_B ⊆ S: ∃ U, V ∈τ: A ⊆ U ⊆ N_A, B ⊆ V ⊆ N_B: N_A ∩ N_B = ∅


That is, that A and B both have neighborhoods in T which are disjoint.


Then A and B are described as separated by neighborhoods.


Category:Definitions/Separated by Neighborhoods


=== Definition 2 ===
Let T = ( S, τ) be a topological space.


Let A, B ⊆ S such that:
:∃ U, V ∈τ: A ⊆ U, B ⊆ V: U ∩ V = ∅


That is, that A and B both have open neighborhoods in T which are disjoint.


Then A and B are described as separated by (open) neighborhoods.



Category:Definitions/Separated by Neighborhoods

=== Points ===
Let T = (S, τ) be a topological space.


=== Definition 1 ===

Let T = ( S, τ) be a topological space.


Let x, y ∈ S such that:

:∃ N_x, N_y ⊆ S: ∃ U, V ∈τ: x ∈ U ⊆ N_x, y ∈ V ⊆ N_y: N_x ∩ N_y = ∅


That is, that x and y both have neighborhoods in T which are disjoint.


Then x and y are described as separated by neighborhoods.


Thus two points are separated by neighborhoods x and y  if and only if  the two singleton sets { x } and { y } are separated by neighborhoods as sets.

Category:Definitions/Separated by Neighborhoods


=== Definition 2 ===
Let T = ( S, τ) be a topological space.


Let x, y ∈ S such that:

:∃ U, V ∈τ: x ∈ U, y ∈ V: U ∩ V = ∅


That is, that x and y both have open neighborhoods in T which are disjoint.


Then x and y are described as separated by (open) neighborhoods.


Thus two points x and y are separated by neighborhoods  if and only if  the two singleton sets { x } and { y } are separated by open neighborhoods as sets.

Category:Definitions/Separated by Neighborhoods


Thus two points x and y are separated by neighborhoods  if and only if  the two singleton sets {x} and {y} are separated by neighborhoods as sets.",Separated
['Definitions/Separated by Neighborhoods'],Definition:Separated,"Let $T = left( S, tau right)$ be a topological space.


Let $x, y in S$ such that:

:$exists N_x, N_y subseteq S: exists U, V in tau: x in U subseteq N_x, y in V subseteq N_y: N_x cap N_y = varnothing$


That is, that $x$ and $y$ both have neighborhoods in $T$ which are disjoint.


Then $x$ and $y$ are described as separated by neighborhoods.


Thus two points are separated by neighborhoods $x$ and $y$  if and only if  the two singleton sets $leftlbrace x rightrbrace$ and $leftlbrace y rightrbrace$ are separated by neighborhoods as sets.

Category:Definitions/Separated by Neighborhoods",Definition:Points Separated by Neighborhoods/Neighborhoods,,false,"Let T = ( S, τ) be a topological space.


Let x, y ∈ S such that:

:∃ N_x, N_y ⊆ S: ∃ U, V ∈τ: x ∈ U ⊆ N_x, y ∈ V ⊆ N_y: N_x ∩ N_y = ∅


That is, that x and y both have neighborhoods in T which are disjoint.


Then x and y are described as separated by neighborhoods.


Thus two points are separated by neighborhoods x and y  if and only if  the two singleton sets { x } and { y } are separated by neighborhoods as sets.

Category:Definitions/Separated by Neighborhoods",Separated
['Definitions/Separated by Neighborhoods'],Definition:Separated,"Let $T = left( S, tau right)$ be a topological space.


Let $A, B subseteq S$ such that:
:$exists N_A, N_B subseteq S: exists U, V in tau: A subseteq U subseteq N_A, B subseteq V subseteq N_B: N_A cap N_B = varnothing$


That is, that $A$ and $B$ both have neighborhoods in $T$ which are disjoint.


Then $A$ and $B$ are described as separated by neighborhoods.


Category:Definitions/Separated by Neighborhoods",Definition:Sets Separated by Neighborhoods/Neighborhoods,,false,"Let T = ( S, τ) be a topological space.


Let A, B ⊆ S such that:
:∃ N_A, N_B ⊆ S: ∃ U, V ∈τ: A ⊆ U ⊆ N_A, B ⊆ V ⊆ N_B: N_A ∩ N_B = ∅


That is, that A and B both have neighborhoods in T which are disjoint.


Then A and B are described as separated by neighborhoods.


Category:Definitions/Separated by Neighborhoods",Separated
"['Definitions/Separated by Closed Neighborhoods', 'Definitions/Completely Hausdorff Spaces', 'Definitions/Separation Axioms']",Definition:Separated,"Let $T = left( S, tau right)$ be a topological space.


=== Sets ===
Let $T = left( S, tau right)$ be a topological space.


Let $A, B subseteq S$ such that:
:$exists N_A, N_B subseteq S: exists U, V in tau: A subseteq U subseteq N_A, B subseteq V subseteq N_B: N_A^- cap N_B^- = varnothing$
where $N_A^-$ and $N_B^-$ are the closures in $T$ of $N_A$ and $N_B$ respectively.

That is, that $A$ and $B$ both have neighborhoods in $T$ whose closures are disjoint.


Then $A$ and $B$ are described as separated by closed neighborhoods.


Category:Definitions/Separated by Closed Neighborhoods

=== Points ===
Let $T = left( S, tau right)$ be a topological space.


Let $x, y in S$ such that:

:$exists N_x, N_y subseteq S: exists U, V in tau: x subseteq U subseteq N_x, y subseteq V subseteq N_y: N_x^- cap N_y^- = varnothing$

where $N_x^-$ and $N_y^-$ are the closures in $T$ of $N_x$ and $N_y$ respectively.


That is, that $x$ and $y$ both have neighborhoods in $T$ whose closures are disjoint.


Then $x$ and $y$ are described as separated by closed neighborhoods.


Thus two points are separated by closed neighborhoods $x$ and $y$  if and only if  the two singleton sets $leftlbrace x rightrbrace$ and $leftlbrace y rightrbrace$ are separated (as sets) by closed neighborhoods.",Definition:Separated by Closed Neighborhoods,,false,"Let T = ( S, τ) be a topological space.


=== Sets ===
Let T = ( S, τ) be a topological space.


Let A, B ⊆ S such that:
:∃ N_A, N_B ⊆ S: ∃ U, V ∈τ: A ⊆ U ⊆ N_A, B ⊆ V ⊆ N_B: N_A^- ∩ N_B^- = ∅
where N_A^- and N_B^- are the closures in T of N_A and N_B respectively.

That is, that A and B both have neighborhoods in T whose closures are disjoint.


Then A and B are described as separated by closed neighborhoods.


Category:Definitions/Separated by Closed Neighborhoods

=== Points ===
Let T = ( S, τ) be a topological space.


Let x, y ∈ S such that:

:∃ N_x, N_y ⊆ S: ∃ U, V ∈τ: x ⊆ U ⊆ N_x, y ⊆ V ⊆ N_y: N_x^- ∩ N_y^- = ∅

where N_x^- and N_y^- are the closures in T of N_x and N_y respectively.


That is, that x and y both have neighborhoods in T whose closures are disjoint.


Then x and y are described as separated by closed neighborhoods.


Thus two points are separated by closed neighborhoods x and y  if and only if  the two singleton sets { x } and { y } are separated (as sets) by closed neighborhoods.",Separated
['Definitions/Separated by Closed Neighborhoods'],Definition:Separated,"Let $T = left( S, tau right)$ be a topological space.


Let $x, y in S$ such that:

:$exists N_x, N_y subseteq S: exists U, V in tau: x subseteq U subseteq N_x, y subseteq V subseteq N_y: N_x^- cap N_y^- = varnothing$

where $N_x^-$ and $N_y^-$ are the closures in $T$ of $N_x$ and $N_y$ respectively.


That is, that $x$ and $y$ both have neighborhoods in $T$ whose closures are disjoint.


Then $x$ and $y$ are described as separated by closed neighborhoods.


Thus two points are separated by closed neighborhoods $x$ and $y$  if and only if  the two singleton sets $leftlbrace x rightrbrace$ and $leftlbrace y rightrbrace$ are separated (as sets) by closed neighborhoods.",Definition:Separated by Closed Neighborhoods/Points,,false,"Let T = ( S, τ) be a topological space.


Let x, y ∈ S such that:

:∃ N_x, N_y ⊆ S: ∃ U, V ∈τ: x ⊆ U ⊆ N_x, y ⊆ V ⊆ N_y: N_x^- ∩ N_y^- = ∅

where N_x^- and N_y^- are the closures in T of N_x and N_y respectively.


That is, that x and y both have neighborhoods in T whose closures are disjoint.


Then x and y are described as separated by closed neighborhoods.


Thus two points are separated by closed neighborhoods x and y  if and only if  the two singleton sets { x } and { y } are separated (as sets) by closed neighborhoods.",Separated
['Definitions/Separated by Closed Neighborhoods'],Definition:Separated,"Let $T = left( S, tau right)$ be a topological space.


Let $A, B subseteq S$ such that:
:$exists N_A, N_B subseteq S: exists U, V in tau: A subseteq U subseteq N_A, B subseteq V subseteq N_B: N_A^- cap N_B^- = varnothing$
where $N_A^-$ and $N_B^-$ are the closures in $T$ of $N_A$ and $N_B$ respectively.

That is, that $A$ and $B$ both have neighborhoods in $T$ whose closures are disjoint.


Then $A$ and $B$ are described as separated by closed neighborhoods.


Category:Definitions/Separated by Closed Neighborhoods",Definition:Separated by Closed Neighborhoods/Sets,,false,"Let T = ( S, τ) be a topological space.


Let A, B ⊆ S such that:
:∃ N_A, N_B ⊆ S: ∃ U, V ∈τ: A ⊆ U ⊆ N_A, B ⊆ V ⊆ N_B: N_A^- ∩ N_B^- = ∅
where N_A^- and N_B^- are the closures in T of N_A and N_B respectively.

That is, that A and B both have neighborhoods in T whose closures are disjoint.


Then A and B are described as separated by closed neighborhoods.


Category:Definitions/Separated by Closed Neighborhoods",Separated
"['Definitions/Separation Axioms', 'Definitions/Hausdorff Spaces']",Definition:Separated,"Let $T = left( S, tau right)$ be a topological space.


=== Definition 1 ===

Let $T = left( S, tau right)$ be a topological space.


$left( S, tau right)$ is a Hausdorff space or $T_2$ space  if and only if :
:$forall x, y in S, x ne y: exists U, V in tau: x in U, y in V: U cap V = varnothing$ 

That is:
:for any two distinct elements $x, y in S$ there exist disjoint open sets $U, V in tau$ containing $x$ and $y$ respectively.


That is:
:$left( S, tau right)$ is a $T_2$ space  if and only if  every two elements in $S$ are separated by open sets.

=== Definition 2 ===
Let $T = left( S, tau right)$ be a topological space.


$left( S, tau right)$ is a Hausdorff space or $T_2$ space  if and only if  each point of $S$ is the intersection of all its closed neighborhoods.

=== Definition 3 ===
Let $T = left( S, tau right)$ be a topological space.


$left( S, tau right)$ is a Hausdorff space or $T_2$ space  if and only if :
:$forall x, y in S, x ne y: exists N_x, N_y subseteq S: exists U, V in tau: x in U subseteq N_x, y in V subseteq N_y: N_x cap N_y = varnothing$

That is:
:for any two distinct elements $x, y in S$ there exist disjoint neighborhoods $N_x, N_y subseteq S$ containing $x$ and $y$ respectively.


That is:
:$left( S, tau right)$ is a $T_2$ space  if and only if  every two elements in $S$ are separated by neighborhoods.",Definition:Hausdorff Space,,false,"Let T = ( S, τ) be a topological space.


=== Definition 1 ===

Let T = ( S, τ) be a topological space.


( S, τ) is a Hausdorff space or T_2 space  if and only if :
:∀ x, y ∈ S, x  y: ∃ U, V ∈τ: x ∈ U, y ∈ V: U ∩ V = ∅ 

That is:
:for any two distinct elements x, y ∈ S there exist disjoint open sets U, V ∈τ containing x and y respectively.


That is:
:( S, τ) is a T_2 space  if and only if  every two elements in S are separated by open sets.

=== Definition 2 ===
Let T = ( S, τ) be a topological space.


( S, τ) is a Hausdorff space or T_2 space  if and only if  each point of S is the intersection of all its closed neighborhoods.

=== Definition 3 ===
Let T = ( S, τ) be a topological space.


( S, τ) is a Hausdorff space or T_2 space  if and only if :
:∀ x, y ∈ S, x  y: ∃ N_x, N_y ⊆ S: ∃ U, V ∈τ: x ∈ U ⊆ N_x, y ∈ V ⊆ N_y: N_x ∩ N_y = ∅

That is:
:for any two distinct elements x, y ∈ S there exist disjoint neighborhoods N_x, N_y ⊆ S containing x and y respectively.


That is:
:( S, τ) is a T_2 space  if and only if  every two elements in S are separated by neighborhoods.",Separated
"['Definitions/Sides of Polygons', 'Definitions/Polygons']",Definition:Side,":

The line segments which make up a polygon are known as its sides.

Thus, in the polygon above, the sides are identified as $a, b, c, d$ and $e$.",Definition:Polygon/Side,,false,":

The line segments which make up a polygon are known as its sides.

Thus, in the polygon above, the sides are identified as a, b, c, d and e.",Side
['Definitions/Surfaces'],Definition:Side,"From the definition of surface, it follows that a plane locally separates space into two sides.

Thus the sides of a plane are the parts of that space into which the plane separates it.


Category:Definitions/Surfaces",Definition:Plane Surface/Side,,false,"From the definition of surface, it follows that a plane locally separates space into two sides.

Thus the sides of a plane are the parts of that space into which the plane separates it.


Category:Definitions/Surfaces",Side
['Definitions/Surfaces'],Definition:Side,"Let $S$ be a surface.

By definition, $S$ locally separates space into two sides.

Thus the sides of $S$ are the parts of that space into which $S$ separates it.


Category:Definitions/Surfaces",Definition:Side of Surface,,false,"Let S be a surface.

By definition, S locally separates space into two sides.

Thus the sides of S are the parts of that space into which S separates it.


Category:Definitions/Surfaces",Side
['Definitions/Euclidean Number Theory'],Definition:Side,"The side of a plane number is one of the (natural) numbers which are its divisors.


=== Example ===


Category:Definitions/Euclidean Number Theory",Definition:Plane Number/Side,,false,"The side of a plane number is one of the (natural) numbers which are its divisors.


=== Example ===


Category:Definitions/Euclidean Number Theory",Side
['Definitions/Euclidean Number Theory'],Definition:Side,"The side of a solid number is one of the (natural) numbers which are its divisors.


=== Example ===


Category:Definitions/Euclidean Number Theory",Definition:Solid Number/Side,,false,"The side of a solid number is one of the (natural) numbers which are its divisors.


=== Example ===


Category:Definitions/Euclidean Number Theory",Side
"['Definitions/Sign of Permutation', 'Definitions/Permutation Theory', 'Definitions/Algebra']",Definition:Sign,"Let $n in mathbb N$ be a natural number.

Let $mathbb N_n$ denote the set of natural numbers $leftlbrace 1, 2, ldots, n rightrbrace$.

Let $left( x_1, x_2, ldots, x_n right)$ be an ordered $n$-tuple of real numbers.

Let $pi$ be a permutation of $mathbb N_n$.

Let $Delta_n left(   right){x_1, x_2, ldots, x_n}$ be the product of differences of $left( x_1, x_2, ldots, x_n right)$.

Let $pi cdot Delta_n left(   right){x_1, x_2, ldots, x_n}$ be defined as:

:$pi cdot Delta_n left(   right){x_1, x_2, ldots, x_n} := Delta_n left(   right){x_{pi left(   right)1}, x_{pi left(   right)2}, ldots, x_{pi left(   right)n} }$


The sign of $pi in S_n$ is defined as:

:$mathrm {sgn} left(   right)pi = begin {cases}
dfrac {Delta_n} {pi cdot Delta_n} & : Delta_n ne 0 \
0 & : Delta_n = 0 end {cases}$",Definition:Sign of Permutation,,false,"Let n ∈ℕ be a natural number.

Let ℕ_n denote the set of natural numbers { 1, 2, …, n }.

Let ( x_1, x_2, …, x_n ) be an ordered n-tuple of real numbers.

Let π be a permutation of ℕ_n.

Let Δ_n (   )x_1, x_2, …, x_n be the product of differences of ( x_1, x_2, …, x_n ).

Let π·Δ_n (   )x_1, x_2, …, x_n be defined as:

:π·Δ_n (   )x_1, x_2, …, x_n := Δ_n (   )x_π(   )1, x_π(   )2, …, x_π(   )n


The sign of π∈ S_n is defined as:

:sgn(   )π = Δ_nπ·Δ_n    : Δ_n  0 

0     : Δ_n = 0",Sign
"['Definitions/Arithmetic', 'Definitions/Algebra', 'Definitions/Abstract Algebra']",Definition:Sign,"In the context of arithmetic and algebra, the term sign is used to mean one of the operators:

* Addition: $+$
* Subtraction: $-$
* Multiplication: $times$
* Division: $div$

It can also be used to describe a general operator in the context of abstract algebra: $circ$ and so on.",Definition:Sign (Arithmetic),,false,"In the context of arithmetic and algebra, the term sign is used to mean one of the operators:

* Addition: +
* Subtraction: -
* Multiplication: ×
* Division: ÷

It can also be used to describe a general operator in the context of abstract algebra: ∘ and so on.",Sign
['Definitions/Numbers'],Definition:Sign,"The sign of a number is the symbol indicating whether it is:
: positive, denoted by the symbol $+$
or:
: negative, denoted by the symbol $-$.


Hence a number's sign has evolved to define the fact of the number being positive or negative independently of the symbol itself.


Thus:
:the sign of $3.14159$ is positive
and
:the sign of $-75$ is negative.",Definition:Sign of Number,symbol,true,"The sign of a number is the symbol indicating whether it is:
: positive, denoted by the symbol +
or:
: negative, denoted by the symbol -.


Hence a number's sign has evolved to define the fact of the number being positive or negative independently of the symbol itself.


Thus:
:the sign of 3.14159 is positive
and
:the sign of -75 is negative.",Sign
"['Definitions/Signum Function', 'Definitions/Number Theory', 'Definitions/Real Analysis', 'Definitions/Discrete Mathematics']",Definition:Sign,"Let $X subseteq mathbb R$ be a subset of the real numbers.


The signum function $mathrm {sgn}: X to leftlbrace -1, 0, 1 rightrbrace$ is defined as:
:$forall x in X: mathrm {sgn} left(   right)x := left[ x > 0 right] - left[ x < 0 right]$
where $left[ x > 0 right]$ etc. denotes Iverson's convention.


That is:
:$forall x in X: mathrm {sgn} left(   right)x := begin {cases} -1 & : x < 0 \ 0 & : x = 0 \ 1 & : x > 0 end {cases}$


=== Graph of Signum Function ===
The graph of the signum function is illustrated below:


:

=== Natural Numbers ===
The signum function $mathrm {sgn}: mathbb N to leftlbrace 0, 1 rightrbrace$ is the restriction of the signum function to the natural numbers, defined as:
:$forall n in mathbb N: mathrm {sgn} left(   right)n := begin {cases} 0 & : n = 0 \ 1 & : n > 0 end{cases}$


=== Signum Complement ===
Let $mathrm {sgn}: mathbb N to leftlbrace 0, 1 rightrbrace$ be the signum function on the natural numbers.

The signum complement function $overline mathrm {sgn}: mathbb N to leftlbrace 0, 1 rightrbrace$ is defined as:
:$forall n in mathbb N: overline mathrm {sgn} left(   right)n := begin {cases} 1 & : n = 0 \ 0 & : n > 0 end {cases}$",Definition:Signum Function,,false,"Let X ⊆ℝ be a subset of the real numbers.


The signum function sgn: X →{ -1, 0, 1 } is defined as:
:∀ x ∈ X: sgn(   )x := [ x > 0 ] - [ x < 0 ]
where [ x > 0 ] etc. denotes Iverson's convention.


That is:
:∀ x ∈ X: sgn(   )x :=  -1     : x < 0 
 0     : x = 0 
 1     : x > 0


=== Graph of Signum Function ===
The graph of the signum function is illustrated below:


:

=== Natural Numbers ===
The signum function sgn: ℕ→{ 0, 1 } is the restriction of the signum function to the natural numbers, defined as:
:∀ n ∈ℕ: sgn(   )n :=  0     : n = 0 
 1     : n > 0


=== Signum Complement ===
Let sgn: ℕ→{ 0, 1 } be the signum function on the natural numbers.

The signum complement function sgn: ℕ→{ 0, 1 } is defined as:
:∀ n ∈ℕ: sgn(   )n :=  1     : n = 0 
 0     : n > 0",Sign
['Definitions/Formal Languages'],Definition:Signature,"Let $mathcal L$ be a formal language.

A choice of vocabulary for $mathcal L$ is called a signature for $mathcal L$.


=== Signature for Predicate Logic ===
Let $mathcal L_1$ be the language of predicate logic.


Then a signature for $mathcal L_1$ is an explicit choice of the alphabet of $mathcal L_1$.

That is to say, it amounts to choosing, for each $n in mathbb N$:

:A collection $mathcal F_n$ of $n$-ary function symbols
:A collection $mathcal P_n$ of $n$-ary relation symbols

It is often conceptually enlightening to explicitly address the $0$-ary function symbols separately, as constant symbols.",Definition:Signature (Logic),,false,"Let ℒ be a formal language.

A choice of vocabulary for ℒ is called a signature for ℒ.


=== Signature for Predicate Logic ===
Let ℒ_1 be the language of predicate logic.


Then a signature for ℒ_1 is an explicit choice of the alphabet of ℒ_1.

That is to say, it amounts to choosing, for each n ∈ℕ:

:A collection ℱ_n of n-ary function symbols
:A collection 𝒫_n of n-ary relation symbols

It is often conceptually enlightening to explicitly address the 0-ary function symbols separately, as constant symbols.",Signature
"['Definitions/Sign of Permutation', 'Definitions/Permutation Theory', 'Definitions/Algebra']",Definition:Signature,"Let $n in mathbb N$ be a natural number.

Let $mathbb N_n$ denote the set of natural numbers $leftlbrace 1, 2, ldots, n rightrbrace$.

Let $left( x_1, x_2, ldots, x_n right)$ be an ordered $n$-tuple of real numbers.

Let $pi$ be a permutation of $mathbb N_n$.

Let $Delta_n left(   right){x_1, x_2, ldots, x_n}$ be the product of differences of $left( x_1, x_2, ldots, x_n right)$.

Let $pi cdot Delta_n left(   right){x_1, x_2, ldots, x_n}$ be defined as:

:$pi cdot Delta_n left(   right){x_1, x_2, ldots, x_n} := Delta_n left(   right){x_{pi left(   right)1}, x_{pi left(   right)2}, ldots, x_{pi left(   right)n} }$


The sign of $pi in S_n$ is defined as:

:$mathrm {sgn} left(   right)pi = begin {cases}
dfrac {Delta_n} {pi cdot Delta_n} & : Delta_n ne 0 \
0 & : Delta_n = 0 end {cases}$",Definition:Sign of Permutation,,false,"Let n ∈ℕ be a natural number.

Let ℕ_n denote the set of natural numbers { 1, 2, …, n }.

Let ( x_1, x_2, …, x_n ) be an ordered n-tuple of real numbers.

Let π be a permutation of ℕ_n.

Let Δ_n (   )x_1, x_2, …, x_n be the product of differences of ( x_1, x_2, …, x_n ).

Let π·Δ_n (   )x_1, x_2, …, x_n be defined as:

:π·Δ_n (   )x_1, x_2, …, x_n := Δ_n (   )x_π(   )1, x_π(   )2, …, x_π(   )n


The sign of π∈ S_n is defined as:

:sgn(   )π = Δ_nπ·Δ_n    : Δ_n  0 

0     : Δ_n = 0",Signature
['Definitions/Euclidean Geometry'],Definition:Similar,"Two rectilineal figures are similar  if and only if :
:They have corresponding angles, all of which are equal
:They have corresponding sides, all of which are proportional.


=== Informal Definition ===

Two geometric figures are similar if they have the same shape but not necessarily the same size.

It is intuitively understood what it means for two figures to have the same shape.


=== Algebraic Definition ===

Two geometric figures are similar if one can be transformed into the other by means of a similarity mapping.


=== Euclid's Definition ===

 
: 
:Similar rectilineal figures are such as have their angles severally equal and the sides about the equal angles proportional.
 ''
 ",Definition:Similar Figures,,false,"Two rectilineal figures are similar  if and only if :
:They have corresponding angles, all of which are equal
:They have corresponding sides, all of which are proportional.


=== Informal Definition ===

Two geometric figures are similar if they have the same shape but not necessarily the same size.

It is intuitively understood what it means for two figures to have the same shape.


=== Algebraic Definition ===

Two geometric figures are similar if one can be transformed into the other by means of a similarity mapping.


=== Euclid's Definition ===

 
: 
:Similar rectilineal figures are such as have their angles severally equal and the sides about the equal angles proportional.
 ”
 ",Similar
['Definitions/Triangles'],Definition:Similar,"Similar triangles are triangles whose corresponding angles are the same, but whose corresponding sides may be of different lengths.

:

Thus $triangle ABC$ is similar to $triangle DEF$:
:$angle ABC = angle EFD$
:$angle BCA = angle EDF$
:$angle CAB = angle DEF$",Definition:Similar Triangles,triangles,true,"Similar triangles are triangles whose corresponding angles are the same, but whose corresponding sides may be of different lengths.

:

Thus ABC is similar to DEF:
:∠ ABC = ∠ EFD
:∠ BCA = ∠ EDF
:∠ CAB = ∠ DEF",Similar
['Definitions/Segments of Circles'],Definition:Similar," 
: 
:Similar segments of circles are those which admit equal angles, or in which the angles are equal to one another.
 ''
 


Category:Definitions/Segments of Circles",Definition:Segment of Circle/Similar,,false," 
: 
:Similar segments of circles are those which admit equal angles, or in which the angles are equal to one another.
 ”
 


Category:Definitions/Segments of Circles",Similar
['Definitions/Solid Geometry'],Definition:Similar," 

 

Category:Definitions/Solid Geometry",Definition:Similar Solid Figures,,false," 

 

Category:Definitions/Solid Geometry",Similar
['Definitions/Right Circular Cones'],Definition:Similar,"Let $h_1$ and $h_2$ be the lengths of the axes of two right circular cones.

Let $d_1$ and $d_2$ be the lengths of the diameters of the bases of the two right circular cones.

Then the two right circular cones are similar  if and only if :

:$dfrac {h_1} {h_2} = dfrac {d_1} {d_2}$


 
: 
:Similar cones and cylinders are those in which the axes and the diameters of the bases are proportional.
 ''
 


Category:Definitions/Right Circular Cones",Definition:Right Circular Cone/Similar Cones,,false,"Let h_1 and h_2 be the lengths of the axes of two right circular cones.

Let d_1 and d_2 be the lengths of the diameters of the bases of the two right circular cones.

Then the two right circular cones are similar  if and only if :

:h_1h_2 = d_1d_2


 
: 
:Similar cones and cylinders are those in which the axes and the diameters of the bases are proportional.
 ”
 


Category:Definitions/Right Circular Cones",Similar
['Definitions/Cylinders'],Definition:Similar,"Let $h_1$ and $h_2$ be the heights of two cylinders.

Let $d_1$ and $d_2$ be the diameters of the bases of the two cylinders.

Then the two cylinders are similar  if and only if :

:$dfrac {h_1} {h_2} = dfrac {d_1} {d_2}$


 
: 
:Similar cones and cylinders are those in which the axes and the diameters of the bases are proportional.
 ''
 


Category:Definitions/Cylinders",Definition:Cylinder/Similar Cylinders,,false,"Let h_1 and h_2 be the heights of two cylinders.

Let d_1 and d_2 be the diameters of the bases of the two cylinders.

Then the two cylinders are similar  if and only if :

:h_1h_2 = d_1d_2


 
: 
:Similar cones and cylinders are those in which the axes and the diameters of the bases are proportional.
 ”
 


Category:Definitions/Cylinders",Similar
['Definitions/Euclidean Geometry'],Definition:Similar,"Similar planes are plane figures which are similar.

Category:Definitions/Euclidean Geometry",Definition:Similar Planes,plane figures,true,"Similar planes are plane figures which are similar.

Category:Definitions/Euclidean Geometry",Similar
"['Definitions/Angles', 'Definitions/Solid Geometry']",Definition:Similar," 

Category:Definitions/Angles
Category:Definitions/Solid Geometry",Definition:Similar Inclination,,false," 

Category:Definitions/Angles
Category:Definitions/Solid Geometry",Similar
['Definitions/Solid Geometry'],Definition:Similar,Two similar solid figures are said to be in a similar situation  if and only if  corresponding surfaces are similarly inclined and when corresponding edges are parallel.,Definition:Similar Situation,,false,Two similar solid figures are said to be in a similar situation  if and only if  corresponding surfaces are similarly inclined and when corresponding edges are parallel.,Similar
['Definitions/Euclidean Number Theory'],Definition:Similar,"Let $m$ and $n$ be plane numbers.

Let:
:$m = p_1 times p_2$ where $p_1 le p_2$
:$n = q_1 times q_2$ where $q_1 le q_2$

Then $m$ and $n$ are similar  if and only if :
:$p_1 : q_1 = p_2 : q_2$


That is:
:$dfrac {p_1} {q_1} = dfrac {p_2} {q_2}$


 
: 
:Similar plane and solid numbers are those which have their sides proportional.
 ''
 

Category:Definitions/Euclidean Number Theory",Definition:Plane Number/Similar Numbers,,false,"Let m and n be plane numbers.

Let:
:m = p_1 × p_2 where p_1 ≤ p_2
:n = q_1 × q_2 where q_1 ≤ q_2

Then m and n are similar  if and only if :
:p_1 : q_1 = p_2 : q_2


That is:
:p_1q_1 = p_2q_2


 
: 
:Similar plane and solid numbers are those which have their sides proportional.
 ”
 

Category:Definitions/Euclidean Number Theory",Similar
['Definitions/Euclidean Number Theory'],Definition:Similar,"Let $m$ and $n$ be solid numbers.

Let:
: $m = p_1 times p_2 times p_3$ where $p_1 le p_2 le p_3$
: $n = q_1 times q_2 times q_3$ where $q_1 le q_2 le q_3$

Then $m$ and $n$ are similar  if and only if :
:$p_1 : q_1 = p_2 : q_2 = p_3 : q_3$


 
: 
:Similar plane and solid numbers are those which have their sides proportional.
 ''
 

Category:Definitions/Euclidean Number Theory",Definition:Solid Number/Similar Numbers,,false,"Let m and n be solid numbers.

Let:
: m = p_1 × p_2 × p_3 where p_1 ≤ p_2 ≤ p_3
: n = q_1 × q_2 × q_3 where q_1 ≤ q_2 ≤ q_3

Then m and n are similar  if and only if :
:p_1 : q_1 = p_2 : q_2 = p_3 : q_3


 
: 
:Similar plane and solid numbers are those which have their sides proportional.
 ”
 

Category:Definitions/Euclidean Number Theory",Similar
"['Definitions/Matrix Similarity', 'Definitions/Matrix Equivalence', 'Definitions/Matrix Theory', 'Definitions/Matrix Algebra']",Definition:Similar,"Let $R$ be a ring with unity.

Let $n in mathbb N_{>0}$ be a natural number.

Let $mathbf A, mathbf B$ be square matrices of order $n$ over $R$.


=== Definition 1 ===
Let $R$ be a ring with unity.

Let $n in mathbb N_{>0}$ be a natural number.

Let $mathbf A, mathbf B$ be square matrices of order $n$ over $R$.

Let there exist an invertible square matrix $mathbf P$ of order $n$ over $R$ such that $mathbf B = mathbf P^{-1} mathbf A mathbf P$.


Then $mathbf A$ and $mathbf B$ are similar.


We write:
:$mathbf A sim mathbf B$

=== Definition 2 ===
Let $R$ be a ring with unity.

Let $n in mathbb N_{>0}$ be a natural number.

Let $mathbf A, mathbf B$ be square matrices of order $n$ over $R$.


$mathbf A$ and $mathbf B$ are similar  if and only if  they are the relative matrices, to (possibly) different ordered bases, of the same linear operator.


We write:
:$mathbf A sim mathbf B$


We write:
:$mathbf A sim mathbf B$",Definition:Matrix Similarity,,false,"Let R be a ring with unity.

Let n ∈ℕ_>0 be a natural number.

Let 𝐀, 𝐁 be square matrices of order n over R.


=== Definition 1 ===
Let R be a ring with unity.

Let n ∈ℕ_>0 be a natural number.

Let 𝐀, 𝐁 be square matrices of order n over R.

Let there exist an invertible square matrix 𝐏 of order n over R such that 𝐁 = 𝐏^-1𝐀𝐏.


Then 𝐀 and 𝐁 are similar.


We write:
:𝐀∼𝐁

=== Definition 2 ===
Let R be a ring with unity.

Let n ∈ℕ_>0 be a natural number.

Let 𝐀, 𝐁 be square matrices of order n over R.


𝐀 and 𝐁 are similar  if and only if  they are the relative matrices, to (possibly) different ordered bases, of the same linear operator.


We write:
:𝐀∼𝐁


We write:
:𝐀∼𝐁",Similar
"['Definitions/Linear Algebra', 'Definitions/Similarity Mappings']",Definition:Similar,"Let $G$ be a vector space over a field $K$.

Let $beta in K$.

Let $s_beta: G to G$ be the mapping on $G$ defined as:
:$forall mathbf x in G: s_beta left(   right){mathbf x} = beta mathbf x$


$s_beta$ is called a similarity (mapping).


=== Scale Factor ===
Let $G$ be a vector space over a field $K$.

Let $beta in K$.

Let $s_beta: G to G$ be a similarity mapping on $G$:
:$forall mathbf x in G: s_beta left(   right){mathbf x} = beta mathbf x$


The coefficient $beta$ is called the scale factor of $s_beta$.",Definition:Similarity Mapping,,false,"Let G be a vector space over a field K.

Let β∈ K.

Let s_β: G → G be the mapping on G defined as:
:∀𝐱∈ G: s_β(   )𝐱 = β𝐱


s_β is called a similarity (mapping).


=== Scale Factor ===
Let G be a vector space over a field K.

Let β∈ K.

Let s_β: G → G be a similarity mapping on G:
:∀𝐱∈ G: s_β(   )𝐱 = β𝐱


The coefficient β is called the scale factor of s_β.",Similar
"['Definitions/Set Equivalence', 'Definitions/Set Theory']",Definition:Similar,"Let $S$ and $T$ be sets.

Then $S$ and $T$ are equivalent  if and only if :
:there exists a bijection $f: S to T$ between the elements of $S$ and those of $T$.

That is,  if and only if  they have the same cardinality.


This can be written $S sim T$.


If $S$ and $T$ are not equivalent we write $S nsim T$.",Definition:Set Equivalence,,false,"Let S and T be sets.

Then S and T are equivalent  if and only if :
:there exists a bijection f: S → T between the elements of S and those of T.

That is,  if and only if  they have the same cardinality.


This can be written S ∼ T.


If S and T are not equivalent we write S  T.",Similar
"['Definitions/Linear Algebra', 'Definitions/Similarity Mappings']",Definition:Similarity,"Let $G$ be a vector space over a field $K$.

Let $beta in K$.

Let $s_beta: G to G$ be the mapping on $G$ defined as:
:$forall mathbf x in G: s_beta left(   right){mathbf x} = beta mathbf x$


$s_beta$ is called a similarity (mapping).


=== Scale Factor ===
Let $G$ be a vector space over a field $K$.

Let $beta in K$.

Let $s_beta: G to G$ be a similarity mapping on $G$:
:$forall mathbf x in G: s_beta left(   right){mathbf x} = beta mathbf x$


The coefficient $beta$ is called the scale factor of $s_beta$.",Definition:Similarity Mapping,,false,"Let G be a vector space over a field K.

Let β∈ K.

Let s_β: G → G be the mapping on G defined as:
:∀𝐱∈ G: s_β(   )𝐱 = β𝐱


s_β is called a similarity (mapping).


=== Scale Factor ===
Let G be a vector space over a field K.

Let β∈ K.

Let s_β: G → G be a similarity mapping on G:
:∀𝐱∈ G: s_β(   )𝐱 = β𝐱


The coefficient β is called the scale factor of s_β.",Similarity
['Definitions/Fractals'],Definition:Similarity,"Let $S$ be a geometric object.

$S$ has the property of self-similarity  if and only if :
:every point of $S$ is contained in a copy of $S$ at a smaller scale.


 ",Definition:Self-Similarity,,false,"Let S be a geometric object.

S has the property of self-similarity  if and only if :
:every point of S is contained in a copy of S at a smaller scale.


 ",Similarity
"['Definitions/Matrix Similarity', 'Definitions/Matrix Equivalence', 'Definitions/Matrix Theory', 'Definitions/Matrix Algebra']",Definition:Similarity,"Let $R$ be a ring with unity.

Let $n in mathbb N_{>0}$ be a natural number.

Let $mathbf A, mathbf B$ be square matrices of order $n$ over $R$.


=== Definition 1 ===
Let $R$ be a ring with unity.

Let $n in mathbb N_{>0}$ be a natural number.

Let $mathbf A, mathbf B$ be square matrices of order $n$ over $R$.

Let there exist an invertible square matrix $mathbf P$ of order $n$ over $R$ such that $mathbf B = mathbf P^{-1} mathbf A mathbf P$.


Then $mathbf A$ and $mathbf B$ are similar.


We write:
:$mathbf A sim mathbf B$

=== Definition 2 ===
Let $R$ be a ring with unity.

Let $n in mathbb N_{>0}$ be a natural number.

Let $mathbf A, mathbf B$ be square matrices of order $n$ over $R$.


$mathbf A$ and $mathbf B$ are similar  if and only if  they are the relative matrices, to (possibly) different ordered bases, of the same linear operator.


We write:
:$mathbf A sim mathbf B$


We write:
:$mathbf A sim mathbf B$",Definition:Matrix Similarity,,false,"Let R be a ring with unity.

Let n ∈ℕ_>0 be a natural number.

Let 𝐀, 𝐁 be square matrices of order n over R.


=== Definition 1 ===
Let R be a ring with unity.

Let n ∈ℕ_>0 be a natural number.

Let 𝐀, 𝐁 be square matrices of order n over R.

Let there exist an invertible square matrix 𝐏 of order n over R such that 𝐁 = 𝐏^-1𝐀𝐏.


Then 𝐀 and 𝐁 are similar.


We write:
:𝐀∼𝐁

=== Definition 2 ===
Let R be a ring with unity.

Let n ∈ℕ_>0 be a natural number.

Let 𝐀, 𝐁 be square matrices of order n over R.


𝐀 and 𝐁 are similar  if and only if  they are the relative matrices, to (possibly) different ordered bases, of the same linear operator.


We write:
:𝐀∼𝐁


We write:
:𝐀∼𝐁",Similarity
"['Definitions/Set Equivalence', 'Definitions/Set Theory']",Definition:Similarity,"Let $S$ and $T$ be sets.

Then $S$ and $T$ are equivalent  if and only if :
:there exists a bijection $f: S to T$ between the elements of $S$ and those of $T$.

That is,  if and only if  they have the same cardinality.


This can be written $S sim T$.


If $S$ and $T$ are not equivalent we write $S nsim T$.",Definition:Set Equivalence,,false,"Let S and T be sets.

Then S and T are equivalent  if and only if :
:there exists a bijection f: S → T between the elements of S and those of T.

That is,  if and only if  they have the same cardinality.


This can be written S ∼ T.


If S and T are not equivalent we write S  T.",Similarity
['Definitions/Normality in Groups'],Definition:Simple,"A group $G$ is simple  if and only if  it has only $G$ and the trivial group as normal subgroups.

That is,  if and only if  the composition length of $G$ is $1$.",Definition:Simple Group,,false,"A group G is simple  if and only if  it has only G and the trivial group as normal subgroups.

That is,  if and only if  the composition length of G is 1.",Simple
['Definitions/Field Extensions'],Definition:Simple,"Let $E / F$ be a field extension.


Then $E$ is a simple extension over $F$  if and only if :
:$exists alpha in E: E = F left[ alpha right]$
where $F left[ alpha right]$ is the field extension generated by $alpha$.",Definition:Simple Field Extension,,false,"Let E / F be a field extension.


Then E is a simple extension over F  if and only if :
:∃α∈ E: E = F [ α]
where F [ α] is the field extension generated by α.",Simple
"['Definitions/Simple Continued Fractions', 'Definitions/Continued Fractions']",Definition:Simple,"Let $mathbb R$ be the field of real numbers.


=== Simple Finite Continued Fraction ===
Let $mathbb R$ be the set of real numbers.

Let $n ge 0$ be a natural number.


A simple finite continued fraction of length $n$ is a finite continued fraction in $mathbb R$ of length $n$ whose partial denominators are integers that are strictly positive, except perhaps the first.

That is, it is a finite sequence $a: left[ 0 ,.,.,   right]n to mathbb Z$ with $a_n > 0$ for $n > 0$.

=== Simple Infinite Continued Fraction ===
Let $mathbb R$ be the field of real numbers.


A simple infinite continued fraction is a infinite continued fraction in $mathbb R$ whose partial denominators are integers that are strictly positive, except perhaps the first.

That is, it is a sequence $a: mathbb N_{ge 0} to mathbb Z$ with $a_n > 0$ for $n > 0$.",Definition:Continued Fraction/Simple,,false,"Let ℝ be the field of real numbers.


=== Simple Finite Continued Fraction ===
Let ℝ be the set of real numbers.

Let n ≥ 0 be a natural number.


A simple finite continued fraction of length n is a finite continued fraction in ℝ of length n whose partial denominators are integers that are strictly positive, except perhaps the first.

That is, it is a finite sequence a: [ 0  . . ]n →ℤ with a_n > 0 for n > 0.

=== Simple Infinite Continued Fraction ===
Let ℝ be the field of real numbers.


A simple infinite continued fraction is a infinite continued fraction in ℝ whose partial denominators are integers that are strictly positive, except perhaps the first.

That is, it is a sequence a: ℕ_≥ 0→ℤ with a_n > 0 for n > 0.",Simple
['Definitions/Simple Continued Fractions'],Definition:Simple,"Let $mathbb R$ be the set of real numbers.

Let $n ge 0$ be a natural number.


A simple finite continued fraction of length $n$ is a finite continued fraction in $mathbb R$ of length $n$ whose partial denominators are integers that are strictly positive, except perhaps the first.

That is, it is a finite sequence $a: left[ 0 ,.,.,   right]n to mathbb Z$ with $a_n > 0$ for $n > 0$.",Definition:Continued Fraction/Simple/Finite,,false,"Let ℝ be the set of real numbers.

Let n ≥ 0 be a natural number.


A simple finite continued fraction of length n is a finite continued fraction in ℝ of length n whose partial denominators are integers that are strictly positive, except perhaps the first.

That is, it is a finite sequence a: [ 0  . . ]n →ℤ with a_n > 0 for n > 0.",Simple
['Definitions/Vector Analysis'],Definition:Simple,"Let $mathbb R^n$ be a real cartesian space of $n$ dimensions.

Let $C_1, ldots, C_n$ be directed smooth curves in $mathbb R^n$.

Let $C_i$ be parameterized by the smooth path $rho_i: left[ a_i ,.,.,   right]{b_i} to mathbb R^n$ for all $i in leftlbrace 1, 2, ldots, n rightrbrace$.

Let $C$ be the contour in $mathbb R^n$ defined by the finite sequence $C_1, ldots, C_n$.


$C$ is a simple contour  if and only if :

:$(1): quad$ For all $i, j in leftlbrace 1, ldots, n rightrbrace, t_1 in left[ a_i ,.,.,   right){b_i}, t_2 in left[ a_j ,.,.,   right){b_j}$ with $t_1 ne t_2$, we have $rho_i left(   right){t_1} ne rho_j left(   right){t_2}$

:$(2): quad$ For all $k in leftlbrace 1, ldots, n rightrbrace, t in left[ a_k ,.,.,   right){b_k}$ where either $k ne 1$ or $t ne a_1$, we have $rho_k left(   right)t ne rho_n left(   right){b_n}$.


Thus a simple contour is a contour that does not intersect itself.


=== Complex Plane ===

The definition carries over to the complex plane, in which context it is usually applied:

Let $C_1, ldots, C_n$ be directed smooth curves in the complex plane $mathbb C$.

Let $C_k$ be parameterized by the smooth path $gamma_k: left[ a_k ,.,.,   right]{b_k} to mathbb C$ for all $k in leftlbrace 1, ldots, n rightrbrace$.

Let $C$ be the contour defined by the finite sequence $C_1, ldots, C_n$.


$C$ is a simple contour  if and only if :

:$(1): quad$ For all $j, k in leftlbrace 1, ldots, n rightrbrace, t_1 in left[ a_j ,.,.,   right){b_j}, t_2 in left[ a_k ,.,.,   right){b_k}$ with $t_1 ne t_2$, we have $gamma_j left(   right){t_1} ne gamma_j left(   right){t_2}$.

:$(2): quad$ For all $k in leftlbrace 1, ldots, n rightrbrace, t in left[ a_k ,.,.,   right){b_k}$ where either $k ne 1$ or $t ne a_1$, we have $gamma_k left(   right)t ne gamma_n left(   right){b_n}$.


Thus a simple contour is a contour that does not intersect itself.",Definition:Contour/Simple,,false,"Let ℝ^n be a real cartesian space of n dimensions.

Let C_1, …, C_n be directed smooth curves in ℝ^n.

Let C_i be parameterized by the smooth path ρ_i: [ a_i  . . ]b_i→ℝ^n for all i ∈{ 1, 2, …, n }.

Let C be the contour in ℝ^n defined by the finite sequence C_1, …, C_n.


C is a simple contour  if and only if :

:(1): For all i, j ∈{ 1, …, n }, t_1 ∈[ a_i  . . )b_i, t_2 ∈[ a_j  . . )b_j with t_1  t_2, we have ρ_i (   )t_1ρ_j (   )t_2

:(2): For all k ∈{ 1, …, n }, t ∈[ a_k  . . )b_k where either k  1 or t  a_1, we have ρ_k (   )t ρ_n (   )b_n.


Thus a simple contour is a contour that does not intersect itself.


=== Complex Plane ===

The definition carries over to the complex plane, in which context it is usually applied:

Let C_1, …, C_n be directed smooth curves in the complex plane ℂ.

Let C_k be parameterized by the smooth path γ_k: [ a_k  . . ]b_k→ℂ for all k ∈{ 1, …, n }.

Let C be the contour defined by the finite sequence C_1, …, C_n.


C is a simple contour  if and only if :

:(1): For all j, k ∈{ 1, …, n }, t_1 ∈[ a_j  . . )b_j, t_2 ∈[ a_k  . . )b_k with t_1  t_2, we have γ_j (   )t_1γ_j (   )t_2.

:(2): For all k ∈{ 1, …, n }, t ∈[ a_k  . . )b_k where either k  1 or t  a_1, we have γ_k (   )t γ_n (   )b_n.


Thus a simple contour is a contour that does not intersect itself.",Simple
"['Definitions/Simple Graphs', 'Definitions/Graph Theory']",Definition:Simple,"A simple graph is a graph which is:

:An undirected graph, that is, the edges are defined as doubleton sets of vertices and not ordered pairs

:Not a multigraph, that is, there is no more than one edge between each pair of vertices

:Not a loop-graph, that is, there are no loops, that is, edges which start and end at the same vertex

:Not a weighted graph, that is, the edges are not mapped to a number.


=== Formal Definition ===
Let $V$ be a set.

Let $mathcal R$ be an endorelation on $V$ which is antireflexive and symmetric.

Let $E$ be the set whose elements of the form:
:$leftlbrace left( v_a, v_b right), left( v_b, v_a right)  rightrbrace$.
where $left( v_a, v_b right)$ and $left( v_b, v_a right)$ are elements of $mathcal R$ 


A simple graph is an ordered pair $G = left( V, E right)$, where $V$ and $E$ are defined as above.


$V$ is called the vertex set.

$E$ is called the edge set.",Definition:Simple Graph,,false,"A simple graph is a graph which is:

:An undirected graph, that is, the edges are defined as doubleton sets of vertices and not ordered pairs

:Not a multigraph, that is, there is no more than one edge between each pair of vertices

:Not a loop-graph, that is, there are no loops, that is, edges which start and end at the same vertex

:Not a weighted graph, that is, the edges are not mapped to a number.


=== Formal Definition ===
Let V be a set.

Let ℛ be an endorelation on V which is antireflexive and symmetric.

Let E be the set whose elements of the form:
:{( v_a, v_b ), ( v_b, v_a )  }.
where ( v_a, v_b ) and ( v_b, v_a ) are elements of ℛ 


A simple graph is an ordered pair G = ( V, E ), where V and E are defined as above.


V is called the vertex set.

E is called the edge set.",Simple
['Definitions/Digraphs'],Definition:Simple,"Let $D = left( V, E right)$ be a digraph.

If the relation $E$ in $D$ is also specifically asymmetric, then $D$ is called a simple digraph.

That is, in a simple digraph there are no pairs of arcs (like there are between $v_1$ and $v_4$ in the diagram above) which go in both directions between two vertices.",Definition:Digraph/Simple Digraph,,false,"Let D = ( V, E ) be a digraph.

If the relation E in D is also specifically asymmetric, then D is called a simple digraph.

That is, in a simple digraph there are no pairs of arcs (like there are between v_1 and v_4 in the diagram above) which go in both directions between two vertices.",Simple
"['Definitions/Edges of Graphs', 'Definitions/Multigraphs']",Definition:Simple,"Let $G = left( V, E right)$ be a multigraph.


A simple edge is an edge $u v$ of $G$ which is the only edge of $G$ which is incident to both $u$ and $v$.


Category:Definitions/Edges of Graphs
Category:Definitions/Multigraphs",Definition:Multigraph/Simple Edge,,false,"Let G = ( V, E ) be a multigraph.


A simple edge is an edge u v of G which is the only edge of G which is incident to both u and v.


Category:Definitions/Edges of Graphs
Category:Definitions/Multigraphs",Simple
['Definitions/Measure Theory'],Definition:Simple,"Let $left( X, unicode{x3a3} right)$ be a measurable space.

A real-valued function $f: X to mathbb R$ is said to be a simple function  if and only if  it is a finite linear combination of characteristic functions:

:$ds f = sum_{k mathop = 1}^n a_k chi_{S_k}$

where $a_1, a_2, ldots, a_n$ are real numbers and each of the sets $S_k$ is $unicode{x3a3}$-measurable.


=== Positive Simple Function ===

When all of the $a_i$ are positive, $f$ is also said to be positive.

 

=== Banach Space ===
Let $mathbb F in leftlbrace mathbb R, mathbb C rightrbrace$. 

Let $I$ be a real interval.

Let $X$ be a Banach space over $mathbb F$. 

Let $f : I to X$ be a function.


We say that $f$ is simple  if and only if  there exists: 

:Lebesgue measurable subsets $Omega_1, ldots, Omega_r$ of $I$ with finite Lebesgue measure
:$x_1, ldots, x_r in X$ 

such that: 

:$ds f left(   right)t = sum_{r mathop = 1}^n x_r chi_{Omega_r}  left(   right)t$

for each $t in I$.",Definition:Simple Function,,false,"Let ( X, x3a3) be a measurable space.

A real-valued function f: X →ℝ is said to be a simple function  if and only if  it is a finite linear combination of characteristic functions:

:f = ∑_k  = 1^n a_k χ_S_k

where a_1, a_2, …, a_n are real numbers and each of the sets S_k is x3a3-measurable.


=== Positive Simple Function ===

When all of the a_i are positive, f is also said to be positive.

 

=== Banach Space ===
Let 𝔽∈{ℝ, ℂ}. 

Let I be a real interval.

Let X be a Banach space over 𝔽. 

Let f : I → X be a function.


We say that f is simple  if and only if  there exists: 

:Lebesgue measurable subsets Ω_1, …, Ω_r of I with finite Lebesgue measure
:x_1, …, x_r ∈ X 

such that: 

:f (   )t = ∑_r  = 1^n x_r χ_Ω_r(   )t

for each t ∈ I.",Simple
['Definitions/Predicate Logic'],Definition:Singular,"A singular statement is a statement whose subject is identified by means of a proper name.

More generally, it is a statement which contains no variables, either bound or free.


=== Individuating Description ===
An individuating description is a predicate whose purpose is to uniquely identify a particular object.

=== Designatory Function ===
A designatory function is a propositional function which, on replacement of the operand with a constant, becomes an individuating description.",Definition:Singular Statement,,false,"A singular statement is a statement whose subject is identified by means of a proper name.

More generally, it is a statement which contains no variables, either bound or free.


=== Individuating Description ===
An individuating description is a predicate whose purpose is to uniquely identify a particular object.

=== Designatory Function ===
A designatory function is a propositional function which, on replacement of the operand with a constant, becomes an individuating description.",Singular
"['Definitions/Matrices', 'Definitions/Non-Invertible Matrices']",Definition:Singular,"Let $left( R, +, circ right)$ be a ring with unity.

Let $n in mathbb Z_{>0}$ be a (strictly) positive integer.

Let $mathbf A$ be an element of the ring of square matrices $left( mathcal M_R left(   right)n, +, times right)$.


=== Definition 1 ===
Let $left( R, +, circ right)$ be a ring with unity.

Let $n in mathbb Z_{>0}$ be a (strictly) positive integer.

Let $mathbf A$ be an element of the ring of square matrices $left( mathcal M_R left(   right)n, +, times right)$.


Let $mathbf A$ have no inverse.

Then $mathbf A$ is referred to as non-invertible.

=== Definition 2 ===
Let $left( R, +, circ right)$ be a ring with unity.

Let $n in mathbb Z_{>0}$ be a (strictly) positive integer.

Let $mathbf A$ be an element of the ring of square matrices $left( mathcal M_R left(   right)n, +, times right)$.


Let the determinant of $mathbf A$ be equal to $0$.

Then $mathbf A$ is referred to as non-invertible.",Definition:Non-Invertible Matrix,,false,"Let ( R, +, ∘) be a ring with unity.

Let n ∈ℤ_>0 be a (strictly) positive integer.

Let 𝐀 be an element of the ring of square matrices ( ℳ_R (   )n, +, ×).


=== Definition 1 ===
Let ( R, +, ∘) be a ring with unity.

Let n ∈ℤ_>0 be a (strictly) positive integer.

Let 𝐀 be an element of the ring of square matrices ( ℳ_R (   )n, +, ×).


Let 𝐀 have no inverse.

Then 𝐀 is referred to as non-invertible.

=== Definition 2 ===
Let ( R, +, ∘) be a ring with unity.

Let n ∈ℤ_>0 be a (strictly) positive integer.

Let 𝐀 be an element of the ring of square matrices ( ℳ_R (   )n, +, ×).


Let the determinant of 𝐀 be equal to 0.

Then 𝐀 is referred to as non-invertible.",Singular
['Definitions/Cardinals'],Definition:Singular,"Let $kappa$ be an infinite cardinal.


Then $kappa$ is a singular cardinal  if and only if  $mathrm {cf}  left(   right)kappa < kappa$.

That is, the cofinality of $kappa$ is less than itself.

 ",Definition:Singular Cardinal,,false,"Let κ be an infinite cardinal.


Then κ is a singular cardinal  if and only if  cf(   )κ < κ.

That is, the cofinality of κ is less than itself.

 ",Singular
"['Definitions/Singular Points', 'Definitions/Singularity Theory', 'Definitions/Analysis']",Definition:Singular,"=== Real Analysis ===

Let $C$ be a locus.
Let $C$ be a locus.

A point $P in C$ is called a singular point  if and only if  $P$ does not have a unique tangent to $C$ which is itself differentiable.

=== Complex Analysis ===
Let $U subseteq mathbb C$ be an open set.

Let $f : U to mathbb C$ be a complex function.


A singular point of $f$ is a point at which $f$ is not analytic.",Definition:Singular Point,,false,"=== Real Analysis ===

Let C be a locus.
Let C be a locus.

A point P ∈ C is called a singular point  if and only if  P does not have a unique tangent to C which is itself differentiable.

=== Complex Analysis ===
Let U ⊆ℂ be an open set.

Let f : U →ℂ be a complex function.


A singular point of f is a point at which f is not analytic.",Singular
"['Definitions/Skew Lines', 'Definitions/Solid Geometry']",Definition:Skew,"Let $L_1$ and $L_2$ be two straight lines in $3$-dimensional Euclidean space.


$L_1$ and $L_2$ are said to be skew  if and only if , when produced, they are neither intersecting nor parallel.",Definition:Skew Lines,,false,"Let L_1 and L_2 be two straight lines in 3-dimensional Euclidean space.


L_1 and L_2 are said to be skew  if and only if , when produced, they are neither intersecting nor parallel.",Skew
"['Definitions/Ring Theory', 'Definitions/Field Theory']",Definition:Skew,A skew field is a division ring whose ring product is specifically not commutative.,Definition:Skew Field,,false,A skew field is a division ring whose ring product is specifically not commutative.,Skew
"['Definitions/Skewness', 'Definitions/Statistics', 'Definitions/Probability Theory']",Definition:Skew,"Skewness is a measure of the asymmetry of a probability distribution about its mean.


Let $X$ be a random variable with mean $mu$ and standard deviation $sigma$.

Then the skewness of $X$, usually denoted $gamma_1$, is defined as: 

:$gamma_1 = mathsf E left( left( dfrac {X - mu} sigma right)^3 right)$

where $mathsf E left( X right)$ denotes the expectation of $X$.


=== Coefficient of Skewness ===
Let $X$ be a random variable with mean $mu$ and standard deviation $sigma$.

The coefficient of skewness of $X$ is the coefficient:
:$gamma_1 = mathsf E left( left( dfrac {X - mu} sigma right)^3 right)$
where $mu_i$ denotes the $i$th central moment of $X$.",Definition:Skewness,,false,"Skewness is a measure of the asymmetry of a probability distribution about its mean.


Let X be a random variable with mean μ and standard deviation σ.

Then the skewness of X, usually denoted γ_1, is defined as: 

:γ_1 = 𝖤( ( X - μσ)^3 )

where 𝖤( X ) denotes the expectation of X.


=== Coefficient of Skewness ===
Let X be a random variable with mean μ and standard deviation σ.

The coefficient of skewness of X is the coefficient:
:γ_1 = 𝖤( ( X - μσ)^3 )
where μ_i denotes the ith central moment of X.",Skew
"['Definitions/Class Theory', 'Definitions/Set Theory']",Definition:Small,"Let $A$ denote an arbitrary class.


Then $A$ is said to be small  if and only if :

:$exists x: x = A$

where $=$ denotes class equality and $x$ is a set variable.


That is, a class is small  if and only if  it is equal to some set variable.


To denote that a class $A$ is small, the notation $mathcal M left(   right)A$ may be used.

Thus:
:$mathcal M left(   right)A iff exists x: x = A$",Definition:Small Class,,false,"Let A denote an arbitrary class.


Then A is said to be small  if and only if :

:∃ x: x = A

where = denotes class equality and x is a set variable.


That is, a class is small  if and only if  it is equal to some set variable.


To denote that a class A is small, the notation ℳ(   )A may be used.

Thus:
:ℳ(   )A ∃ x: x = A",Small
['Definitions/Category Theory'],Definition:Small,"Let $mathbf C$ be a metacategory.


Then $mathbf C$ is said to be small  if and only if  both of the following hold:

:The collection of objects $mathbf C_0$ is a set;
:The collection of morphisms $mathbf C_1$ is a set.",Definition:Small Category,,false,"Let 𝐂 be a metacategory.


Then 𝐂 is said to be small  if and only if  both of the following hold:

:The collection of objects 𝐂_0 is a set;
:The collection of morphisms 𝐂_1 is a set.",Small
"['Definitions/Differentiable Real Functions', 'Definitions/Topology', 'Definitions/Differentiability Classes']",Definition:Smooth,"A real function is smooth  if and only if  it is of differentiability class $C^infty$.

That is,  if and only if  it admits of continuous derivatives of all orders.


 ",Definition:Smooth Real Function,,false,"A real function is smooth  if and only if  it is of differentiability class C^∞.

That is,  if and only if  it admits of continuous derivatives of all orders.


 ",Smooth
['Definitions/Manifolds'],Definition:Smooth,"Let $M, N$ be smooth manifolds. 

Denote $m := dim M$ and $n := dim N$. 

Let $phi: M to N$ be a mapping. 


Then $phi$ is a smooth mapping  if and only if :
:for every chart $left( U, kappa right)$ on $M$ and every chart $left( V, xi right)$ on $N$ such that $V cap phi left(   right)U ne varnothing$, the mapping:
::$ds xi circ phi circ kappa^{-1}: kappa left(   right)U subseteq mathbb R^m to xi left(   right){V cap phi left(   right)U} subseteq mathbb R^n$
:is smooth.",Definition:Smooth Mapping,,false,"Let M, N be smooth manifolds. 

Denote m :=  M and n :=  N. 

Let ϕ: M → N be a mapping. 


Then ϕ is a smooth mapping  if and only if :
:for every chart ( U, κ) on M and every chart ( V, ξ) on N such that V ∩ϕ(   )U ∅, the mapping:
::ξ∘ϕ∘κ^-1: κ(   )U ⊆ℝ^m →ξ(   )V ∩ϕ(   )U⊆ℝ^n
:is smooth.",Smooth
"['Definitions/Smooth Manifolds', 'Definitions/Topological Manifolds', 'Definitions/Differentiable Manifolds']",Definition:Smooth,"Let $M$ be a second-countable locally Euclidean space of dimension $d$. 

Let $mathscr F$ be a smooth differentiable structure on $M$.


Then $left( M, mathscr F right)$ is called a smooth manifold of dimension $d$.",Definition:Topological Manifold/Smooth Manifold,,false,"Let M be a second-countable locally Euclidean space of dimension d. 

Let ℱ be a smooth differentiable structure on M.


Then ( M, ℱ) is called a smooth manifold of dimension d.",Smooth
['Definitions/Homotopy Theory'],Definition:Smooth,"Let $X$ and $Y$ be topological spaces.

Let $f: X to Y$, $g: X to Y$ be smooth mappings.


Then $f$ and $g$ are smoothly homotopic  if and only if  there exists a smooth mapping:
: $H: X times left[{0 ,.,., 1}right] to Y$
such that:
: $H left({x, 0}right) = f left({x}right)$
and:
: $H left({x, 1}right) = g left({x}right)$


$H$ is called a smooth homotopy between $f$ and $g$.",Definition:Smooth Homotopy,,false,"Let X and Y be topological spaces.

Let f: X → Y, g: X → Y be smooth mappings.


Then f and g are smoothly homotopic  if and only if  there exists a smooth mapping:
: H: X ×[0  . .  1] → Y
such that:
: H (x, 0) = f (x)
and:
: H (x, 1) = g (x)


H is called a smooth homotopy between f and g.",Smooth
"['Definitions/Mapping Theory', 'Definitions/Algebra']",Definition:Solution,"Let $P: X to leftlbrace mathrm T, mathrm F rightrbrace$ be a propositional function defined on a domain $X$.

Let $S = P^{-1}  left(   right)mathrm T$ be the fiber of truth (under $P$).


Then an element of $S$ is known as a solution of $P$.


This terminology is usual when $P$ is an equation in the context of algebra.",Definition:Fiber of Truth/Solution,,false,"Let P: X →{T, F} be a propositional function defined on a domain X.

Let S = P^-1(   )T be the fiber of truth (under P).


Then an element of S is known as a solution of P.


This terminology is usual when P is an equation in the context of algebra.",Solution
"['Definitions/Solutions to Differential Equations', 'Definitions/Differential Equations']",Definition:Solution,"Let $Phi$ be a differential equation defined on a domain $D$.

Let $phi$ be a function which satisfies $Phi$ on the whole of $D$.


Then $phi$ is known as a solution of $Phi$.


Note that, in general, there may be more than one solution to a given differential equation.

On the other hand, there may be none at all.


=== General Solution ===
Let $Phi$ be a differential equation.

The general solution to $Phi$ is the set of all functions $phi$ that satisfy $Phi$.


 

=== Particular Solution ===
Let $Phi$ be a differential equation.

Let $S$ denote the solution set of $Phi$.

A particular solution of $Phi$ is the element of $S$, or subset of $S$, which satisfies a particular boundary condition of $Phi$.

=== Weak Solution ===
A weak solution is a solution to a non-standard formulation of a differential equation.

 ",Definition:Differential Equation/Solution,,false,"Let Φ be a differential equation defined on a domain D.

Let ϕ be a function which satisfies Φ on the whole of D.


Then ϕ is known as a solution of Φ.


Note that, in general, there may be more than one solution to a given differential equation.

On the other hand, there may be none at all.


=== General Solution ===
Let Φ be a differential equation.

The general solution to Φ is the set of all functions ϕ that satisfy Φ.


 

=== Particular Solution ===
Let Φ be a differential equation.

Let S denote the solution set of Φ.

A particular solution of Φ is the element of S, or subset of S, which satisfies a particular boundary condition of Φ.

=== Weak Solution ===
A weak solution is a solution to a non-standard formulation of a differential equation.

 ",Solution
"['Definitions/General Solutions to Differential Equations', 'Definitions/Solutions to Differential Equations', 'Definitions/Differential Equations']",Definition:Solution,"Let $Phi$ be a differential equation.

The general solution to $Phi$ is the set of all functions $phi$ that satisfy $Phi$.


 ",Definition:Differential Equation/Solution/General Solution,,false,"Let Φ be a differential equation.

The general solution to Φ is the set of all functions ϕ that satisfy Φ.


 ",Solution
"['Definitions/Particular Solutions to Differential Equations', 'Definitions/Solutions to Differential Equations']",Definition:Solution,"Let $Phi$ be a differential equation.

Let $S$ denote the solution set of $Phi$.

A particular solution of $Phi$ is the element of $S$, or subset of $S$, which satisfies a particular boundary condition of $Phi$.",Definition:Differential Equation/Solution/Particular Solution,,false,"Let Φ be a differential equation.

Let S denote the solution set of Φ.

A particular solution of Φ is the element of S, or subset of S, which satisfies a particular boundary condition of Φ.",Solution
"['Definitions/Mapping Theory', 'Definitions/Symbolic Logic']",Definition:Solution,"Let $P: X to leftlbrace mathrm T, mathrm F rightrbrace$ be a propositional function defined on a domain $X$.


The fiber of truth (under $P$) is the preimage, or fiber, of $mathrm T$ under $P$:
:$P^{-1}  left(   right)mathrm T := leftlbrace x in X: P left(   right)x = mathrm T rightrbrace$


That is, the elements of $X$ whose image under $P$ is $mathrm T$.


=== Solution ===
Let $P: X to leftlbrace mathrm T, mathrm F rightrbrace$ be a propositional function defined on a domain $X$.

Let $S = P^{-1}  left(   right)mathrm T$ be the fiber of truth (under $P$).


Then an element of $S$ is known as a solution of $P$.


This terminology is usual when $P$ is an equation in the context of algebra.",Definition:Fiber of Truth,,false,"Let P: X →{T, F} be a propositional function defined on a domain X.


The fiber of truth (under P) is the preimage, or fiber, of T under P:
:P^-1(   )T := { x ∈ X: P (   )x = T}


That is, the elements of X whose image under P is T.


=== Solution ===
Let P: X →{T, F} be a propositional function defined on a domain X.

Let S = P^-1(   )T be the fiber of truth (under P).


Then an element of S is known as a solution of P.


This terminology is usual when P is an equation in the context of algebra.",Solution
['Definitions/Simultaneous Equations'],Definition:Solution,"An ordered $n$-tuple $left( x_1, x_2, ldots, x_n right)$ which satisfies each of the equations in a system of $m$ simultaneous equations in $n$ variables is called a solution of the system.",Definition:Simultaneous Equations/Solution,,false,"An ordered n-tuple ( x_1, x_2, …, x_n ) which satisfies each of the equations in a system of m simultaneous equations in n variables is called a solution of the system.",Solution
['Definitions/Game Theory'],Definition:Solution,"Let $G$ be a game.

A solution of $G$ is a systematic description of the outcomes that may emerge in a family of games.",Definition:Solution of Game,,false,"Let G be a game.

A solution of G is a systematic description of the outcomes that may emerge in a family of games.",Solution
['Definitions/Polynomial Congruences'],Definition:Solution,"Let:
:$P left(   right)x equiv 0 pmod n$
be a polynomial congruence.


A solution of $P left(   right)x equiv 0 pmod n$ is a residue class modulo $n$ such that any element of that class satisfies the congruence.",Definition:Polynomial Congruence/Solution,,false,"Let:
:P (   )x ≡ 0  n
be a polynomial congruence.


A solution of P (   )x ≡ 0  n is a residue class modulo n such that any element of that class satisfies the congruence.",Solution
['Definitions/Cartesian Product'],Definition:Space,"Let $S$ be a set.

The cartesian $n$th power of $S$, or $S$ to the power of $n$, is defined as:

:$ds S^n = prod_{k mathop = 1}^n S = leftlbrace left( x_1, x_2, ldots, x_n right): forall k in mathbb N^*_n: x_k in S rightrbrace$


Thus $S^n = underbrace {S times S times cdots times S}_{text{$n$ times} }$

Alternatively it can be defined recursively:

:$S^n = begin{cases}
S: & n = 1 \
S times S^{n - 1} & n > 1
end{cases}$


The set $S^n$ called a cartesian space.


An element $x_j$ of an ordered tuple $left( x_1, x_2, ldots, x_n right)$ of a cartesian space $S^n$ is known as a basis element of $S^n$.


=== Two Dimensions ===

$n = 2$ is frequently taken as a special case:

Let $S$ be a set.

The cartesian $2$nd power of $S$ is:

:$S^2 = S times S = leftlbrace left( x_1, x_2 right): x_1, x_2 in S rightrbrace$


The set $S^2$ called a cartesian space of $2$ dimensions.


=== Cartesian Plane ===

When $S$ is the set of real numbers $mathbb R$, the cartesian product takes on a special significance.


The Cartesian plane is a Cartesian coordinate system of $2$ dimensions.

Every point on the plane can be identified uniquely by means of an ordered pair of real coordinates $left( x, y right)$, as follows:


Identify one distinct point on the plane as the origin $O$.

Select a point $P$ on the plane different from $O$.

Construct an infinite straight line through $O$ and $P$ and call it the $x$-axis.


Identify the $x$-axis with the real number line such that:
:$0$ is identified with the origin $O$
:$1$ is identified with the point $P$

The orientation of the $x$-axis is determined by the relative positions of $O$ and $P$.

It is conventional to locate $P$ to the right of $O$, so as to arrange that:

:to the right of the origin, the numbers on the $x$-axis are positive
:to the left of the origin, the numbers on the $x$-axis are negative.


Construct an infinite straight line through $O$ perpendicular to the $x$-axis and call it the $y$-axis.

Identify the point $P'$ on the $y$-axis such that $OP' = OP$.

Identify the $y$-axis with the real number line such that:
:$0$ is identified with the origin $O$
:$1$ is identified with the point $P'$


The orientation of the $y$-axis is determined by the position of $P'$ relative to $O$.

It is conventional to locate $P'$ such that, if one were to imagine being positioned at $O$ and facing along the $x$-axis towards $P$, then $P'$ is on the left. 

Hence with the conventional orientation of the $x$-axis as horizontal and increasing to the right:

:going vertically ""up"" the page or screen from the origin, the numbers on the $y$-axis are positive
:going vertically ""down"" the page or screen from the origin, the numbers on the $y$-axis are negative.


=== Cartesian Coordinate Pair ===


Hence:
:the point $P$ is identified with the coordinates $left( 1, 0 right)$
:the point $P'$ is identified with the coordinates $left( 0, 1 right)$.


=== $x$ Coordinate ===
Consider a Cartesian coordinate system $C$ with an $x$-axis.

Let a point $Q$ be positioned in $C$.


Let $x$ be the length of the line segment from the origin $O$ to the foot of the perpendicular from $Q$ to the $x$-axis.

Then $x$ is known as the $x$ coordinate.

If $Q$ is in the positive direction along the real number line that is the $x$-axis, then $x$ is positive.

If $Q$ is in the negative direction along the real number line that is the $x$-axis, then $x$ is negative.

=== $y$ Coordinate ===
Consider a Cartesian coordinate system $C$ with a $y$-axis.

Let a point $Q$ be positioned in $C$.


Let $y$ be the length of the line segment from the origin $O$ to the foot of the perpendicular from $Q$ to the $y$-axis.

Then $y$ is known as the $y$ coordinate.

If $Q$ is in the positive direction along the real number line that is the $y$-axis, then $y$ is positive.

If $Q$ is in the negative direction along the real number line that is the $y$-axis, then $y$ is negative.

=== Three Dimensions ===

$n = 3$ is another special case:

Let $S$ be a set.

The cartesian $3$rd power of $S$ is:

:$S^3 = S times S times S = leftlbrace left( x_1, x_2, x_3 right): x_1, x_2, x_3 in S rightrbrace$


The set $S^3$ called a cartesian space of $3$ dimensions.


=== Cartesian 3-Space ===

When $S$ is the set of real numbers $mathbb R$, the cartesian product takes on a special significance.

The Cartesian $3$-space is a Cartesian coordinate system of $3$ dimensions.


=== Definition by Axes ===


Every point in ordinary $3$-space can be identified uniquely by means of an ordered triple of real coordinates  $left( x, y, z right)$, as follows:

Construct a Cartesian plane, with origin $O$ and axes identified as the $x$-axis and $y$-axis.

Recall the identification of the point $P$ with the coordinate pair $left( 1, 0 right)$ in the $x$-$y$ plane.


Construct an infinite straight line through $O$ perpendicular to both the $x$-axis and the$y$-axis and call it the $z$-axis.

Identify the point $P' '$ on the $z$-axis such that $OP' ' = OP$.

Identify the $z$-axis with the real number line such that:
:$0$ is identified with the origin $O$
:$1$ is identified with the point $P$


=== Orientation ===


Consider a Cartesian $3$-Space.

Let the $x$-axis, $y$-axis and $z$-axis be defined.


Let a point $P$ be identified on the $x$-axis, different from $O$, with the coordinate pair $left( 1, 0 right)$ in the $x$-$y$ plane.

Let the point $P'$ be identified on the $y$-axis such that $OP' = OP$.


It remains to identify the point $P'  '$ on the $z$-axis such that $OP' ' = OP$.


==== Right-Handed ====


==== Left-Handed ====


Category:Definitions/Cartesian Coordinate Systems

=== Cartesian Coordinate Triple ===


=== Definition by Planes ===


Every point in ordinary $3$-space can be identified uniquely by means of an ordered triple of real coordinates  $left( x, y, z right)$, as follows:

Identify one distinct point in space as the origin $O$.

Let $3$ distinct planes be constructed through $O$ such that all are perpendicular.

Each pair of these $3$ planes intersect in a straight line that passes through $O$.

Let $X$, $Y$ and $Z$ be points, other than $O$, one on each of these $3$ lines of intersection.


Then the lines $OX$, $OY$ and $OZ$ are named the $x$-axis, $y$-axis and $z$-axis respectively.


Select a point $P$ on the $x$-axis different from $O$.

Let $P$ be identified with the coordinate pair $left( 1, 0 right)$ in the $x$-$y$ plane.

Identify the point $P'$ on the $y$-axis such that $OP' = OP$.

Identify the point $P' '$ on the $z$-axis such that $OP' ' = OP$.


=== Orientation ===


Consider a Cartesian $3$-Space.

Let the $x$-axis, $y$-axis and $z$-axis be defined.


Let a point $P$ be identified on the $x$-axis, different from $O$, with the coordinate pair $left( 1, 0 right)$ in the $x$-$y$ plane.

Let the point $P'$ be identified on the $y$-axis such that $OP' = OP$.


It remains to identify the point $P'  '$ on the $z$-axis such that $OP' ' = OP$.


==== Right-Handed ====


==== Left-Handed ====


Category:Definitions/Cartesian Coordinate Systems

=== Coordinate Planes ===
Consider the Cartesian $3$-space defined by $3$ distinct perpendicular planes through the origin $O$.

These $3$ planes are known as the coordinate planes of the Cartesian $3$-space.


=== $x$-$y$ Plane ===


=== $y$-$z$ Plane ===


=== $x$-$z$ Plane ===


=== Cartesian Coordinate Triple ===


=== Orientation ===


Consider a Cartesian $3$-Space.

Let the $x$-axis, $y$-axis and $z$-axis be defined.


Let a point $P$ be identified on the $x$-axis, different from $O$, with the coordinate pair $left( 1, 0 right)$ in the $x$-$y$ plane.

Let the point $P'$ be identified on the $y$-axis such that $OP' = OP$.


It remains to identify the point $P'  '$ on the $z$-axis such that $OP' ' = OP$.


==== Right-Handed ====


Consider a Cartesian $3$-Space.

Let the $x$-axis, $y$-axis and $z$-axis be defined.


Let a point $P$ be identified on the $x$-axis, different from $O$, with the coordinate pair $left( 1, 0 right)$ in the $x$-$y$ plane.

Let the point $P'$ be identified on the $y$-axis such that $OP' = OP$.

It remains to identify the point $P' '$ on the $z$-axis such that $OP' ' = OP$.

The orientation of the $z$-axis is determined by the position of $P' '$ relative to $O$.


The Cartesian $3$-Space is defined as right-handed when $P' '$ is located as follows.

Let the coordinate axes be oriented as follows:

Imagine being positioned, standing on the $x$-$y$ plane at $O$, and facing along the $x$-axis towards $P$, with $P'$ on the left. 

Then $P' '$ is then one unit above the $x$-$y$ plane.


Hence, let the coordinate axes be oriented as follows:

:Let the $x$-axis increase from West to East.
:Let the $y$-axis increase from South to North.

Then the $z$-axis increases from below to above.


Simiarly, let the $x$-$y$ plane be identified with the plane of the page or screen such aligned perpendicular to the line of sight such that:

:the $x$-axis increases from left to right.
:the $y$-axis increases from bottom to top.

Then the $z$-axis increases from behind to in front (that is, from further away to closer in).


Category:Definitions/Cartesian Coordinate Systems

==== Left-Handed ====


Consider a Cartesian $3$-Space.

Let the $x$-axis, $y$-axis and $z$-axis be defined.


Let a point $P$ be identified on the $x$-axis, different from $O$, with the coordinate pair $left( 1, 0 right)$ in the $x$-$y$ plane.

Let the point $P'$ be identified on the $y$-axis such that $OP' = OP$.

It remains to identify the point $P' '$ on the $z$-axis such that $OP' ' = OP$.

The orientation of the $z$-axis is determined by the position of $P' '$ relative to $O$.


The Cartesian $3$-Space is defined as left-handed when $P' '$ is located as follows.

Let the coordinate axes be oriented as follows:

Imagine being positioned, standing on the $x$-$y$ plane at $O$, and facing along the $x$-axis towards $P$, with $P'$ on the left. 

$P' '$ is then one unit below the $x$-$y$ plane.


Hence, let the coordinate axes be oriented as follows:

:Let the $x$-axis increase from West to East.
:Let the $y$-axis increase from South to North.

Then the $z$-axis increases from above to below.


Simiarly, let the $x$-$y$ plane be identified with the plane of the page or screen such aligned perpendicular to the line of sight such that:

:the $x$-axis increases from left to right.
:the $y$-axis increases from bottom to top.

Then the $z$-axis increases from in front to behind (that is, from closer in to further away).

Category:Definitions/Cartesian Coordinate Systems

Category:Definitions/Cartesian Coordinate Systems

=== Cartesian Coordinate Triple ===


Hence:
:the point $P$ is identified with the coordinates $left( 1, 0, 0 right)$
:the point $P'$ is identified with the coordinates $left( 0, 1, 0 right)$.
:the point $P' '$ is identified with the coordinates $left( 0, 0, 1 right)$.

=== Family of Sets ===
Let $I$ be an indexing set.

Let $leftlangle S_i rightrangle_{i mathop in I}$ be a family of sets indexed by $I$.

Let $ds prod_{i mathop in I} S_i$ be the Cartesian product of $leftlangle S_i rightrangle_{i mathop in I}$.

Let $S$ be a set such that:
:$forall i in I: S_i = S$


=== Definition 1 ===
Let $I$ be an indexing set.

Let $leftlangle S_i rightrangle_{i mathop in I}$ be an family of sets indexed by $I$.

Let $ds prod_{i mathop in I} S_i$ be the Cartesian product of $leftlangle S_i rightrangle_{i mathop in I}$.

Let $S$ be a set such that:
:$forall i in I: S_i = S$


The Cartesian space of $S$ indexed by $I$ is the set of all families $leftlangle s_i rightrangle_{i mathop in I}$ with $s_i in S$ for each $i in I$:
:$S_I := ds prod_I S = leftlbrace leftlangle s_i rightrangle_{i mathop in I}: s_i in S rightrbrace$

=== Definition 2 ===
Let $I$ be an indexing set.

Let $leftlangle S_i rightrangle_{i mathop in I}$ be an family of sets indexed by $I$.

Let $ds prod_{i mathop in I} S_i$ be the Cartesian product of $leftlangle S_i rightrangle_{i mathop in I}$.

Let $S$ be a set such that:
:$forall i in I: S_i = S$


The Cartesian space of $S$ indexed by $I$ is defined and denoted as:
:$ds S^I := leftlbrace f: left( f: I to S right) land left( forall i in I: left( f left(   right)i in S right)  right)  rightrbrace$

=== Real Cartesian Space ===

When $S$ is the set of real numbers $mathbb R$, the cartesian product takes on a special significance.

Let $n in mathbb N_{>0}$.

Then $mathbb R^n$ is the cartesian product defined as follows:

:$ds mathbb R^n = underbrace {mathbb R times mathbb R times cdots times mathbb R}_{text {$n$ times} } = prod_{k mathop = 1}^n mathbb R$


Similarly, $mathbb R^n$ can be defined as the set of all real $n$-tuples:

:$mathbb R^n = leftlbrace left( x_1, x_2, ldots, x_n right): x_1, x_2, ldots, x_n in mathbb R rightrbrace$


=== Cartesian Plane ===

The Cartesian plane is a Cartesian coordinate system of $2$ dimensions.

Every point on the plane can be identified uniquely by means of an ordered pair of real coordinates $left( x, y right)$, as follows:


Identify one distinct point on the plane as the origin $O$.

Select a point $P$ on the plane different from $O$.

Construct an infinite straight line through $O$ and $P$ and call it the $x$-axis.


Identify the $x$-axis with the real number line such that:
:$0$ is identified with the origin $O$
:$1$ is identified with the point $P$

The orientation of the $x$-axis is determined by the relative positions of $O$ and $P$.

It is conventional to locate $P$ to the right of $O$, so as to arrange that:

:to the right of the origin, the numbers on the $x$-axis are positive
:to the left of the origin, the numbers on the $x$-axis are negative.


Construct an infinite straight line through $O$ perpendicular to the $x$-axis and call it the $y$-axis.

Identify the point $P'$ on the $y$-axis such that $OP' = OP$.

Identify the $y$-axis with the real number line such that:
:$0$ is identified with the origin $O$
:$1$ is identified with the point $P'$


The orientation of the $y$-axis is determined by the position of $P'$ relative to $O$.

It is conventional to locate $P'$ such that, if one were to imagine being positioned at $O$ and facing along the $x$-axis towards $P$, then $P'$ is on the left. 

Hence with the conventional orientation of the $x$-axis as horizontal and increasing to the right:

:going vertically ""up"" the page or screen from the origin, the numbers on the $y$-axis are positive
:going vertically ""down"" the page or screen from the origin, the numbers on the $y$-axis are negative.


=== Cartesian Coordinate Pair ===


Hence:
:the point $P$ is identified with the coordinates $left( 1, 0 right)$
:the point $P'$ is identified with the coordinates $left( 0, 1 right)$.


=== $x$ Coordinate ===
Consider a Cartesian coordinate system $C$ with an $x$-axis.

Let a point $Q$ be positioned in $C$.


Let $x$ be the length of the line segment from the origin $O$ to the foot of the perpendicular from $Q$ to the $x$-axis.

Then $x$ is known as the $x$ coordinate.

If $Q$ is in the positive direction along the real number line that is the $x$-axis, then $x$ is positive.

If $Q$ is in the negative direction along the real number line that is the $x$-axis, then $x$ is negative.

=== $y$ Coordinate ===
Consider a Cartesian coordinate system $C$ with a $y$-axis.

Let a point $Q$ be positioned in $C$.


Let $y$ be the length of the line segment from the origin $O$ to the foot of the perpendicular from $Q$ to the $y$-axis.

Then $y$ is known as the $y$ coordinate.

If $Q$ is in the positive direction along the real number line that is the $y$-axis, then $y$ is positive.

If $Q$ is in the negative direction along the real number line that is the $y$-axis, then $y$ is negative.

=== Cartesian 3-Space ===
The Cartesian $3$-space is a Cartesian coordinate system of $3$ dimensions.


=== Definition by Axes ===


Every point in ordinary $3$-space can be identified uniquely by means of an ordered triple of real coordinates  $left( x, y, z right)$, as follows:

Construct a Cartesian plane, with origin $O$ and axes identified as the $x$-axis and $y$-axis.

Recall the identification of the point $P$ with the coordinate pair $left( 1, 0 right)$ in the $x$-$y$ plane.


Construct an infinite straight line through $O$ perpendicular to both the $x$-axis and the$y$-axis and call it the $z$-axis.

Identify the point $P' '$ on the $z$-axis such that $OP' ' = OP$.

Identify the $z$-axis with the real number line such that:
:$0$ is identified with the origin $O$
:$1$ is identified with the point $P$


=== Orientation ===


Consider a Cartesian $3$-Space.

Let the $x$-axis, $y$-axis and $z$-axis be defined.


Let a point $P$ be identified on the $x$-axis, different from $O$, with the coordinate pair $left( 1, 0 right)$ in the $x$-$y$ plane.

Let the point $P'$ be identified on the $y$-axis such that $OP' = OP$.


It remains to identify the point $P'  '$ on the $z$-axis such that $OP' ' = OP$.


==== Right-Handed ====


==== Left-Handed ====


Category:Definitions/Cartesian Coordinate Systems

=== Cartesian Coordinate Triple ===


=== Definition by Planes ===


Every point in ordinary $3$-space can be identified uniquely by means of an ordered triple of real coordinates  $left( x, y, z right)$, as follows:

Identify one distinct point in space as the origin $O$.

Let $3$ distinct planes be constructed through $O$ such that all are perpendicular.

Each pair of these $3$ planes intersect in a straight line that passes through $O$.

Let $X$, $Y$ and $Z$ be points, other than $O$, one on each of these $3$ lines of intersection.


Then the lines $OX$, $OY$ and $OZ$ are named the $x$-axis, $y$-axis and $z$-axis respectively.


Select a point $P$ on the $x$-axis different from $O$.

Let $P$ be identified with the coordinate pair $left( 1, 0 right)$ in the $x$-$y$ plane.

Identify the point $P'$ on the $y$-axis such that $OP' = OP$.

Identify the point $P' '$ on the $z$-axis such that $OP' ' = OP$.


=== Orientation ===


Consider a Cartesian $3$-Space.

Let the $x$-axis, $y$-axis and $z$-axis be defined.


Let a point $P$ be identified on the $x$-axis, different from $O$, with the coordinate pair $left( 1, 0 right)$ in the $x$-$y$ plane.

Let the point $P'$ be identified on the $y$-axis such that $OP' = OP$.


It remains to identify the point $P'  '$ on the $z$-axis such that $OP' ' = OP$.


==== Right-Handed ====


==== Left-Handed ====


Category:Definitions/Cartesian Coordinate Systems

=== Coordinate Planes ===
Consider the Cartesian $3$-space defined by $3$ distinct perpendicular planes through the origin $O$.

These $3$ planes are known as the coordinate planes of the Cartesian $3$-space.


=== $x$-$y$ Plane ===


=== $y$-$z$ Plane ===


=== $x$-$z$ Plane ===


=== Cartesian Coordinate Triple ===


=== Orientation ===


Consider a Cartesian $3$-Space.

Let the $x$-axis, $y$-axis and $z$-axis be defined.


Let a point $P$ be identified on the $x$-axis, different from $O$, with the coordinate pair $left( 1, 0 right)$ in the $x$-$y$ plane.

Let the point $P'$ be identified on the $y$-axis such that $OP' = OP$.


It remains to identify the point $P'  '$ on the $z$-axis such that $OP' ' = OP$.


==== Right-Handed ====


Consider a Cartesian $3$-Space.

Let the $x$-axis, $y$-axis and $z$-axis be defined.


Let a point $P$ be identified on the $x$-axis, different from $O$, with the coordinate pair $left( 1, 0 right)$ in the $x$-$y$ plane.

Let the point $P'$ be identified on the $y$-axis such that $OP' = OP$.

It remains to identify the point $P' '$ on the $z$-axis such that $OP' ' = OP$.

The orientation of the $z$-axis is determined by the position of $P' '$ relative to $O$.


The Cartesian $3$-Space is defined as right-handed when $P' '$ is located as follows.

Let the coordinate axes be oriented as follows:

Imagine being positioned, standing on the $x$-$y$ plane at $O$, and facing along the $x$-axis towards $P$, with $P'$ on the left. 

Then $P' '$ is then one unit above the $x$-$y$ plane.


Hence, let the coordinate axes be oriented as follows:

:Let the $x$-axis increase from West to East.
:Let the $y$-axis increase from South to North.

Then the $z$-axis increases from below to above.


Simiarly, let the $x$-$y$ plane be identified with the plane of the page or screen such aligned perpendicular to the line of sight such that:

:the $x$-axis increases from left to right.
:the $y$-axis increases from bottom to top.

Then the $z$-axis increases from behind to in front (that is, from further away to closer in).


Category:Definitions/Cartesian Coordinate Systems

==== Left-Handed ====


Consider a Cartesian $3$-Space.

Let the $x$-axis, $y$-axis and $z$-axis be defined.


Let a point $P$ be identified on the $x$-axis, different from $O$, with the coordinate pair $left( 1, 0 right)$ in the $x$-$y$ plane.

Let the point $P'$ be identified on the $y$-axis such that $OP' = OP$.

It remains to identify the point $P' '$ on the $z$-axis such that $OP' ' = OP$.

The orientation of the $z$-axis is determined by the position of $P' '$ relative to $O$.


The Cartesian $3$-Space is defined as left-handed when $P' '$ is located as follows.

Let the coordinate axes be oriented as follows:

Imagine being positioned, standing on the $x$-$y$ plane at $O$, and facing along the $x$-axis towards $P$, with $P'$ on the left. 

$P' '$ is then one unit below the $x$-$y$ plane.


Hence, let the coordinate axes be oriented as follows:

:Let the $x$-axis increase from West to East.
:Let the $y$-axis increase from South to North.

Then the $z$-axis increases from above to below.


Simiarly, let the $x$-$y$ plane be identified with the plane of the page or screen such aligned perpendicular to the line of sight such that:

:the $x$-axis increases from left to right.
:the $y$-axis increases from bottom to top.

Then the $z$-axis increases from in front to behind (that is, from closer in to further away).

Category:Definitions/Cartesian Coordinate Systems

Category:Definitions/Cartesian Coordinate Systems

=== Cartesian Coordinate Triple ===


Hence:
:the point $P$ is identified with the coordinates $left( 1, 0, 0 right)$
:the point $P'$ is identified with the coordinates $left( 0, 1, 0 right)$.
:the point $P' '$ is identified with the coordinates $left( 0, 0, 1 right)$.

=== Countable-Dimensional Real Cartesian Space ===
The countable cartesian product defined as:
:$ds mathbb R^omega := mathbb R times mathbb R times cdots = prod_mathbb N mathbb R$

is called the countable-dimensional real cartesian space.

Thus, $mathbb R^omega$ can be defined as the set of all real sequences:

:$mathbb R^omega = leftlbrace leftlangle x_1, x_2, ldots rightrangle: x_1, x_2, ldots in mathbb R rightrbrace$
 ",Definition:Cartesian Product/Cartesian Space,,false,"Let S be a set.

The cartesian nth power of S, or S to the power of n, is defined as:

:S^n = ∏_k  = 1^n S = {( x_1, x_2, …, x_n ): ∀ k ∈ℕ^*_n: x_k ∈ S }


Thus S^n = S × S ×⋯× S_n times

Alternatively it can be defined recursively:

:S^n = 
S:     n = 1 

S × S^n - 1    n > 1


The set S^n called a cartesian space.


An element x_j of an ordered tuple ( x_1, x_2, …, x_n ) of a cartesian space S^n is known as a basis element of S^n.


=== Two Dimensions ===

n = 2 is frequently taken as a special case:

Let S be a set.

The cartesian 2nd power of S is:

:S^2 = S × S = {( x_1, x_2 ): x_1, x_2 ∈ S }


The set S^2 called a cartesian space of 2 dimensions.


=== Cartesian Plane ===

When S is the set of real numbers ℝ, the cartesian product takes on a special significance.


The Cartesian plane is a Cartesian coordinate system of 2 dimensions.

Every point on the plane can be identified uniquely by means of an ordered pair of real coordinates ( x, y ), as follows:


Identify one distinct point on the plane as the origin O.

Select a point P on the plane different from O.

Construct an infinite straight line through O and P and call it the x-axis.


Identify the x-axis with the real number line such that:
:0 is identified with the origin O
:1 is identified with the point P

The orientation of the x-axis is determined by the relative positions of O and P.

It is conventional to locate P to the right of O, so as to arrange that:

:to the right of the origin, the numbers on the x-axis are positive
:to the left of the origin, the numbers on the x-axis are negative.


Construct an infinite straight line through O perpendicular to the x-axis and call it the y-axis.

Identify the point P' on the y-axis such that OP' = OP.

Identify the y-axis with the real number line such that:
:0 is identified with the origin O
:1 is identified with the point P'


The orientation of the y-axis is determined by the position of P' relative to O.

It is conventional to locate P' such that, if one were to imagine being positioned at O and facing along the x-axis towards P, then P' is on the left. 

Hence with the conventional orientation of the x-axis as horizontal and increasing to the right:

:going vertically ""up"" the page or screen from the origin, the numbers on the y-axis are positive
:going vertically ""down"" the page or screen from the origin, the numbers on the y-axis are negative.


=== Cartesian Coordinate Pair ===


Hence:
:the point P is identified with the coordinates ( 1, 0 )
:the point P' is identified with the coordinates ( 0, 1 ).


=== x Coordinate ===
Consider a Cartesian coordinate system C with an x-axis.

Let a point Q be positioned in C.


Let x be the length of the line segment from the origin O to the foot of the perpendicular from Q to the x-axis.

Then x is known as the x coordinate.

If Q is in the positive direction along the real number line that is the x-axis, then x is positive.

If Q is in the negative direction along the real number line that is the x-axis, then x is negative.

=== y Coordinate ===
Consider a Cartesian coordinate system C with a y-axis.

Let a point Q be positioned in C.


Let y be the length of the line segment from the origin O to the foot of the perpendicular from Q to the y-axis.

Then y is known as the y coordinate.

If Q is in the positive direction along the real number line that is the y-axis, then y is positive.

If Q is in the negative direction along the real number line that is the y-axis, then y is negative.

=== Three Dimensions ===

n = 3 is another special case:

Let S be a set.

The cartesian 3rd power of S is:

:S^3 = S × S × S = {( x_1, x_2, x_3 ): x_1, x_2, x_3 ∈ S }


The set S^3 called a cartesian space of 3 dimensions.


=== Cartesian 3-Space ===

When S is the set of real numbers ℝ, the cartesian product takes on a special significance.

The Cartesian 3-space is a Cartesian coordinate system of 3 dimensions.


=== Definition by Axes ===


Every point in ordinary 3-space can be identified uniquely by means of an ordered triple of real coordinates  ( x, y, z ), as follows:

Construct a Cartesian plane, with origin O and axes identified as the x-axis and y-axis.

Recall the identification of the point P with the coordinate pair ( 1, 0 ) in the x-y plane.


Construct an infinite straight line through O perpendicular to both the x-axis and they-axis and call it the z-axis.

Identify the point P' ' on the z-axis such that OP' ' = OP.

Identify the z-axis with the real number line such that:
:0 is identified with the origin O
:1 is identified with the point P


=== Orientation ===


Consider a Cartesian 3-Space.

Let the x-axis, y-axis and z-axis be defined.


Let a point P be identified on the x-axis, different from O, with the coordinate pair ( 1, 0 ) in the x-y plane.

Let the point P' be identified on the y-axis such that OP' = OP.


It remains to identify the point P'  ' on the z-axis such that OP' ' = OP.


==== Right-Handed ====


==== Left-Handed ====


Category:Definitions/Cartesian Coordinate Systems

=== Cartesian Coordinate Triple ===


=== Definition by Planes ===


Every point in ordinary 3-space can be identified uniquely by means of an ordered triple of real coordinates  ( x, y, z ), as follows:

Identify one distinct point in space as the origin O.

Let 3 distinct planes be constructed through O such that all are perpendicular.

Each pair of these 3 planes intersect in a straight line that passes through O.

Let X, Y and Z be points, other than O, one on each of these 3 lines of intersection.


Then the lines OX, OY and OZ are named the x-axis, y-axis and z-axis respectively.


Select a point P on the x-axis different from O.

Let P be identified with the coordinate pair ( 1, 0 ) in the x-y plane.

Identify the point P' on the y-axis such that OP' = OP.

Identify the point P' ' on the z-axis such that OP' ' = OP.


=== Orientation ===


Consider a Cartesian 3-Space.

Let the x-axis, y-axis and z-axis be defined.


Let a point P be identified on the x-axis, different from O, with the coordinate pair ( 1, 0 ) in the x-y plane.

Let the point P' be identified on the y-axis such that OP' = OP.


It remains to identify the point P'  ' on the z-axis such that OP' ' = OP.


==== Right-Handed ====


==== Left-Handed ====


Category:Definitions/Cartesian Coordinate Systems

=== Coordinate Planes ===
Consider the Cartesian 3-space defined by 3 distinct perpendicular planes through the origin O.

These 3 planes are known as the coordinate planes of the Cartesian 3-space.


=== x-y Plane ===


=== y-z Plane ===


=== x-z Plane ===


=== Cartesian Coordinate Triple ===


=== Orientation ===


Consider a Cartesian 3-Space.

Let the x-axis, y-axis and z-axis be defined.


Let a point P be identified on the x-axis, different from O, with the coordinate pair ( 1, 0 ) in the x-y plane.

Let the point P' be identified on the y-axis such that OP' = OP.


It remains to identify the point P'  ' on the z-axis such that OP' ' = OP.


==== Right-Handed ====


Consider a Cartesian 3-Space.

Let the x-axis, y-axis and z-axis be defined.


Let a point P be identified on the x-axis, different from O, with the coordinate pair ( 1, 0 ) in the x-y plane.

Let the point P' be identified on the y-axis such that OP' = OP.

It remains to identify the point P' ' on the z-axis such that OP' ' = OP.

The orientation of the z-axis is determined by the position of P' ' relative to O.


The Cartesian 3-Space is defined as right-handed when P' ' is located as follows.

Let the coordinate axes be oriented as follows:

Imagine being positioned, standing on the x-y plane at O, and facing along the x-axis towards P, with P' on the left. 

Then P' ' is then one unit above the x-y plane.


Hence, let the coordinate axes be oriented as follows:

:Let the x-axis increase from West to East.
:Let the y-axis increase from South to North.

Then the z-axis increases from below to above.


Simiarly, let the x-y plane be identified with the plane of the page or screen such aligned perpendicular to the line of sight such that:

:the x-axis increases from left to right.
:the y-axis increases from bottom to top.

Then the z-axis increases from behind to in front (that is, from further away to closer in).


Category:Definitions/Cartesian Coordinate Systems

==== Left-Handed ====


Consider a Cartesian 3-Space.

Let the x-axis, y-axis and z-axis be defined.


Let a point P be identified on the x-axis, different from O, with the coordinate pair ( 1, 0 ) in the x-y plane.

Let the point P' be identified on the y-axis such that OP' = OP.

It remains to identify the point P' ' on the z-axis such that OP' ' = OP.

The orientation of the z-axis is determined by the position of P' ' relative to O.


The Cartesian 3-Space is defined as left-handed when P' ' is located as follows.

Let the coordinate axes be oriented as follows:

Imagine being positioned, standing on the x-y plane at O, and facing along the x-axis towards P, with P' on the left. 

P' ' is then one unit below the x-y plane.


Hence, let the coordinate axes be oriented as follows:

:Let the x-axis increase from West to East.
:Let the y-axis increase from South to North.

Then the z-axis increases from above to below.


Simiarly, let the x-y plane be identified with the plane of the page or screen such aligned perpendicular to the line of sight such that:

:the x-axis increases from left to right.
:the y-axis increases from bottom to top.

Then the z-axis increases from in front to behind (that is, from closer in to further away).

Category:Definitions/Cartesian Coordinate Systems

Category:Definitions/Cartesian Coordinate Systems

=== Cartesian Coordinate Triple ===


Hence:
:the point P is identified with the coordinates ( 1, 0, 0 )
:the point P' is identified with the coordinates ( 0, 1, 0 ).
:the point P' ' is identified with the coordinates ( 0, 0, 1 ).

=== Family of Sets ===
Let I be an indexing set.

Let ⟨ S_i ⟩_i ∈ I be a family of sets indexed by I.

Let ∏_i ∈ I S_i be the Cartesian product of ⟨ S_i ⟩_i ∈ I.

Let S be a set such that:
:∀ i ∈ I: S_i = S


=== Definition 1 ===
Let I be an indexing set.

Let ⟨ S_i ⟩_i ∈ I be an family of sets indexed by I.

Let ∏_i ∈ I S_i be the Cartesian product of ⟨ S_i ⟩_i ∈ I.

Let S be a set such that:
:∀ i ∈ I: S_i = S


The Cartesian space of S indexed by I is the set of all families ⟨ s_i ⟩_i ∈ I with s_i ∈ S for each i ∈ I:
:S_I := ∏_I S = {⟨ s_i ⟩_i ∈ I: s_i ∈ S }

=== Definition 2 ===
Let I be an indexing set.

Let ⟨ S_i ⟩_i ∈ I be an family of sets indexed by I.

Let ∏_i ∈ I S_i be the Cartesian product of ⟨ S_i ⟩_i ∈ I.

Let S be a set such that:
:∀ i ∈ I: S_i = S


The Cartesian space of S indexed by I is defined and denoted as:
:S^I := { f: ( f: I → S ) ( ∀ i ∈ I: ( f (   )i ∈ S )  )  }

=== Real Cartesian Space ===

When S is the set of real numbers ℝ, the cartesian product takes on a special significance.

Let n ∈ℕ_>0.

Then ℝ^n is the cartesian product defined as follows:

:ℝ^n = ℝ×ℝ×⋯×ℝ_n times = ∏_k  = 1^n ℝ


Similarly, ℝ^n can be defined as the set of all real n-tuples:

:ℝ^n = {( x_1, x_2, …, x_n ): x_1, x_2, …, x_n ∈ℝ}


=== Cartesian Plane ===

The Cartesian plane is a Cartesian coordinate system of 2 dimensions.

Every point on the plane can be identified uniquely by means of an ordered pair of real coordinates ( x, y ), as follows:


Identify one distinct point on the plane as the origin O.

Select a point P on the plane different from O.

Construct an infinite straight line through O and P and call it the x-axis.


Identify the x-axis with the real number line such that:
:0 is identified with the origin O
:1 is identified with the point P

The orientation of the x-axis is determined by the relative positions of O and P.

It is conventional to locate P to the right of O, so as to arrange that:

:to the right of the origin, the numbers on the x-axis are positive
:to the left of the origin, the numbers on the x-axis are negative.


Construct an infinite straight line through O perpendicular to the x-axis and call it the y-axis.

Identify the point P' on the y-axis such that OP' = OP.

Identify the y-axis with the real number line such that:
:0 is identified with the origin O
:1 is identified with the point P'


The orientation of the y-axis is determined by the position of P' relative to O.

It is conventional to locate P' such that, if one were to imagine being positioned at O and facing along the x-axis towards P, then P' is on the left. 

Hence with the conventional orientation of the x-axis as horizontal and increasing to the right:

:going vertically ""up"" the page or screen from the origin, the numbers on the y-axis are positive
:going vertically ""down"" the page or screen from the origin, the numbers on the y-axis are negative.


=== Cartesian Coordinate Pair ===


Hence:
:the point P is identified with the coordinates ( 1, 0 )
:the point P' is identified with the coordinates ( 0, 1 ).


=== x Coordinate ===
Consider a Cartesian coordinate system C with an x-axis.

Let a point Q be positioned in C.


Let x be the length of the line segment from the origin O to the foot of the perpendicular from Q to the x-axis.

Then x is known as the x coordinate.

If Q is in the positive direction along the real number line that is the x-axis, then x is positive.

If Q is in the negative direction along the real number line that is the x-axis, then x is negative.

=== y Coordinate ===
Consider a Cartesian coordinate system C with a y-axis.

Let a point Q be positioned in C.


Let y be the length of the line segment from the origin O to the foot of the perpendicular from Q to the y-axis.

Then y is known as the y coordinate.

If Q is in the positive direction along the real number line that is the y-axis, then y is positive.

If Q is in the negative direction along the real number line that is the y-axis, then y is negative.

=== Cartesian 3-Space ===
The Cartesian 3-space is a Cartesian coordinate system of 3 dimensions.


=== Definition by Axes ===


Every point in ordinary 3-space can be identified uniquely by means of an ordered triple of real coordinates  ( x, y, z ), as follows:

Construct a Cartesian plane, with origin O and axes identified as the x-axis and y-axis.

Recall the identification of the point P with the coordinate pair ( 1, 0 ) in the x-y plane.


Construct an infinite straight line through O perpendicular to both the x-axis and they-axis and call it the z-axis.

Identify the point P' ' on the z-axis such that OP' ' = OP.

Identify the z-axis with the real number line such that:
:0 is identified with the origin O
:1 is identified with the point P


=== Orientation ===


Consider a Cartesian 3-Space.

Let the x-axis, y-axis and z-axis be defined.


Let a point P be identified on the x-axis, different from O, with the coordinate pair ( 1, 0 ) in the x-y plane.

Let the point P' be identified on the y-axis such that OP' = OP.


It remains to identify the point P'  ' on the z-axis such that OP' ' = OP.


==== Right-Handed ====


==== Left-Handed ====


Category:Definitions/Cartesian Coordinate Systems

=== Cartesian Coordinate Triple ===


=== Definition by Planes ===


Every point in ordinary 3-space can be identified uniquely by means of an ordered triple of real coordinates  ( x, y, z ), as follows:

Identify one distinct point in space as the origin O.

Let 3 distinct planes be constructed through O such that all are perpendicular.

Each pair of these 3 planes intersect in a straight line that passes through O.

Let X, Y and Z be points, other than O, one on each of these 3 lines of intersection.


Then the lines OX, OY and OZ are named the x-axis, y-axis and z-axis respectively.


Select a point P on the x-axis different from O.

Let P be identified with the coordinate pair ( 1, 0 ) in the x-y plane.

Identify the point P' on the y-axis such that OP' = OP.

Identify the point P' ' on the z-axis such that OP' ' = OP.


=== Orientation ===


Consider a Cartesian 3-Space.

Let the x-axis, y-axis and z-axis be defined.


Let a point P be identified on the x-axis, different from O, with the coordinate pair ( 1, 0 ) in the x-y plane.

Let the point P' be identified on the y-axis such that OP' = OP.


It remains to identify the point P'  ' on the z-axis such that OP' ' = OP.


==== Right-Handed ====


==== Left-Handed ====


Category:Definitions/Cartesian Coordinate Systems

=== Coordinate Planes ===
Consider the Cartesian 3-space defined by 3 distinct perpendicular planes through the origin O.

These 3 planes are known as the coordinate planes of the Cartesian 3-space.


=== x-y Plane ===


=== y-z Plane ===


=== x-z Plane ===


=== Cartesian Coordinate Triple ===


=== Orientation ===


Consider a Cartesian 3-Space.

Let the x-axis, y-axis and z-axis be defined.


Let a point P be identified on the x-axis, different from O, with the coordinate pair ( 1, 0 ) in the x-y plane.

Let the point P' be identified on the y-axis such that OP' = OP.


It remains to identify the point P'  ' on the z-axis such that OP' ' = OP.


==== Right-Handed ====


Consider a Cartesian 3-Space.

Let the x-axis, y-axis and z-axis be defined.


Let a point P be identified on the x-axis, different from O, with the coordinate pair ( 1, 0 ) in the x-y plane.

Let the point P' be identified on the y-axis such that OP' = OP.

It remains to identify the point P' ' on the z-axis such that OP' ' = OP.

The orientation of the z-axis is determined by the position of P' ' relative to O.


The Cartesian 3-Space is defined as right-handed when P' ' is located as follows.

Let the coordinate axes be oriented as follows:

Imagine being positioned, standing on the x-y plane at O, and facing along the x-axis towards P, with P' on the left. 

Then P' ' is then one unit above the x-y plane.


Hence, let the coordinate axes be oriented as follows:

:Let the x-axis increase from West to East.
:Let the y-axis increase from South to North.

Then the z-axis increases from below to above.


Simiarly, let the x-y plane be identified with the plane of the page or screen such aligned perpendicular to the line of sight such that:

:the x-axis increases from left to right.
:the y-axis increases from bottom to top.

Then the z-axis increases from behind to in front (that is, from further away to closer in).


Category:Definitions/Cartesian Coordinate Systems

==== Left-Handed ====


Consider a Cartesian 3-Space.

Let the x-axis, y-axis and z-axis be defined.


Let a point P be identified on the x-axis, different from O, with the coordinate pair ( 1, 0 ) in the x-y plane.

Let the point P' be identified on the y-axis such that OP' = OP.

It remains to identify the point P' ' on the z-axis such that OP' ' = OP.

The orientation of the z-axis is determined by the position of P' ' relative to O.


The Cartesian 3-Space is defined as left-handed when P' ' is located as follows.

Let the coordinate axes be oriented as follows:

Imagine being positioned, standing on the x-y plane at O, and facing along the x-axis towards P, with P' on the left. 

P' ' is then one unit below the x-y plane.


Hence, let the coordinate axes be oriented as follows:

:Let the x-axis increase from West to East.
:Let the y-axis increase from South to North.

Then the z-axis increases from above to below.


Simiarly, let the x-y plane be identified with the plane of the page or screen such aligned perpendicular to the line of sight such that:

:the x-axis increases from left to right.
:the y-axis increases from bottom to top.

Then the z-axis increases from in front to behind (that is, from closer in to further away).

Category:Definitions/Cartesian Coordinate Systems

Category:Definitions/Cartesian Coordinate Systems

=== Cartesian Coordinate Triple ===


Hence:
:the point P is identified with the coordinates ( 1, 0, 0 )
:the point P' is identified with the coordinates ( 0, 1, 0 ).
:the point P' ' is identified with the coordinates ( 0, 0, 1 ).

=== Countable-Dimensional Real Cartesian Space ===
The countable cartesian product defined as:
:ℝ^ω := ℝ×ℝ×⋯ = ∏_ℕℝ

is called the countable-dimensional real cartesian space.

Thus, ℝ^ω can be defined as the set of all real sequences:

:ℝ^ω = {⟨ x_1, x_2, …⟩: x_1, x_2, …∈ℝ}
 ",Space
"['Definitions/Topological Spaces', 'Definitions/Topology', 'Definitions/Abstract Spaces']",Definition:Space,"Let $S$ be a set.

Let $tau$ be a topology on $S$.

That is, let $tau subseteq mathcal P left( S right)$ satisfy the open set axioms:
 

Then the ordered pair $left( S, tau right)$ is called a topological space.

The elements of $tau$ are called open sets of $left( S, tau right)$.


In a topological space $left( S, tau right)$, we consider $S$ to be the universal set.",Definition:Topological Space,,false,"Let S be a set.

Let τ be a topology on S.

That is, let τ⊆𝒫( S ) satisfy the open set axioms:
 

Then the ordered pair ( S, τ) is called a topological space.

The elements of τ are called open sets of ( S, τ).


In a topological space ( S, τ), we consider S to be the universal set.",Space
"['Definitions/Separation Axioms', 'Definitions/Hausdorff Spaces']",Definition:Space,"Let $T = left( S, tau right)$ be a topological space.


=== Definition 1 ===

Let $T = left( S, tau right)$ be a topological space.


$left( S, tau right)$ is a Hausdorff space or $T_2$ space  if and only if :
:$forall x, y in S, x ne y: exists U, V in tau: x in U, y in V: U cap V = varnothing$ 

That is:
:for any two distinct elements $x, y in S$ there exist disjoint open sets $U, V in tau$ containing $x$ and $y$ respectively.


That is:
:$left( S, tau right)$ is a $T_2$ space  if and only if  every two elements in $S$ are separated by open sets.

=== Definition 2 ===
Let $T = left( S, tau right)$ be a topological space.


$left( S, tau right)$ is a Hausdorff space or $T_2$ space  if and only if  each point of $S$ is the intersection of all its closed neighborhoods.

=== Definition 3 ===
Let $T = left( S, tau right)$ be a topological space.


$left( S, tau right)$ is a Hausdorff space or $T_2$ space  if and only if :
:$forall x, y in S, x ne y: exists N_x, N_y subseteq S: exists U, V in tau: x in U subseteq N_x, y in V subseteq N_y: N_x cap N_y = varnothing$

That is:
:for any two distinct elements $x, y in S$ there exist disjoint neighborhoods $N_x, N_y subseteq S$ containing $x$ and $y$ respectively.


That is:
:$left( S, tau right)$ is a $T_2$ space  if and only if  every two elements in $S$ are separated by neighborhoods.",Definition:Hausdorff Space,,false,"Let T = ( S, τ) be a topological space.


=== Definition 1 ===

Let T = ( S, τ) be a topological space.


( S, τ) is a Hausdorff space or T_2 space  if and only if :
:∀ x, y ∈ S, x  y: ∃ U, V ∈τ: x ∈ U, y ∈ V: U ∩ V = ∅ 

That is:
:for any two distinct elements x, y ∈ S there exist disjoint open sets U, V ∈τ containing x and y respectively.


That is:
:( S, τ) is a T_2 space  if and only if  every two elements in S are separated by open sets.

=== Definition 2 ===
Let T = ( S, τ) be a topological space.


( S, τ) is a Hausdorff space or T_2 space  if and only if  each point of S is the intersection of all its closed neighborhoods.

=== Definition 3 ===
Let T = ( S, τ) be a topological space.


( S, τ) is a Hausdorff space or T_2 space  if and only if :
:∀ x, y ∈ S, x  y: ∃ N_x, N_y ⊆ S: ∃ U, V ∈τ: x ∈ U ⊆ N_x, y ∈ V ⊆ N_y: N_x ∩ N_y = ∅

That is:
:for any two distinct elements x, y ∈ S there exist disjoint neighborhoods N_x, N_y ⊆ S containing x and y respectively.


That is:
:( S, τ) is a T_2 space  if and only if  every two elements in S are separated by neighborhoods.",Space
"['Definitions/Vector Spaces', 'Definitions/Vector Algebra', 'Definitions/Linear Algebra', 'Definitions/Abstract Spaces']",Definition:Space,"=== Definition 1 ===
Let $left( K, +_K, times_K right)$ be a field.

Let $left( G, +_G right)$ be an abelian group.

Let $left( G, +_G, circ right)_K$ be a unitary $K$-module.


Then $left( G, +_G, circ right)_K$ is a vector space over $K$ or a $K$-vector space.


That is, a vector space is a unitary module whose scalar ring is a field.

=== Definition 2 ===
Let $left( K, +_K, times_K right)$ be a field whose unity is $1_K$.

Let $left( G, +_G right)$ be an abelian group.

Let $left( mathrm {End}  left(   right)G, +, circ right)$ be the endomorphism ring of $left( G, +_G right)$ such that $I_G$ is the identity mapping.

Let $cdot: left( K, +_K, times_K right) to left( mathrm {End}  left(   right)G, +, circ right)$ be a ring homomorphism from $K$ to $mathrm {End}  left(   right)G$ which maps $1_K$ to $I_G$.


Then $left( G, +_G, cdot, K right)$ is a vector space over $K$ or a $K$-vector space.


=== Vector Space Axioms ===
The vector space axioms are the defining properties of a vector space.

Let $left( G, +_G, circ right)_K$ be a vector space over $K$ where:

:$G$ is a set of objects, called vectors.

:$+_G: G times G to G$ is a binary operation on $G$

:$left( K, +, cdot right)$ is a division ring whose unity is $1_K$

:$circ: K times G to G$ is a binary operation

The usual situation is for $K$ to be one of the standard number fields $mathbb R$ or $mathbb C$.


The vector space axioms consist of the abelian group axioms:

 
 
 
 
 
 
 


together with the properties of a unitary module:

 
 
 
 
 
 

=== Vector ===
Let $V = left( G, +_G, circ right)_K$ be a vector space over $K$, where:

:$left( G, +_G right)$ is an abelian group

:$left( K, +_K, times_K right)$ is the scalar field of $V$.


The elements of the abelian group $left( G, +_G right)$ are called vectors.

=== Zero Vector ===
Let $left( R, +_R, times_R right)$ be a ring.

Let $left( G, +_G right)$ be an abelian group.

Let $left( G, +_G, circ right)_R$ be an $R$-module.


The identity of $left( G, +_G right)$ is usually denoted $boldsymbol 0$, or some variant of this, and called the zero vector:

:$forall mathbf a in left( G, +_G, circ right)_R: boldsymbol 0 +_G mathbf a = mathbf a = mathbf a +_G boldsymbol 0$


Note that on occasion it is advantageous to denote the zero vector differently, for example by $e$, or $boldsymbol 0_V$ or $boldsymbol 0_G$, in order to highlight the fact that the zero vector is not the same object as the zero scalar.


=== Zero Vector in $mathbb R^n$ ===
Let $left( mathbb R^n, +, times right)_mathbb R$ be a real vector space.

The zero vector in $left( mathbb R^n, +, times right)_mathbb R$ is:

:$mathbf 0_{n times 1} := begin {bmatrix} 0 \ 0 \ vdots \ 0 end {bmatrix}$

where $0 in mathbb R$.

=== Zero Vector Quantity ===
A vector quantity whose magnitude is zero is referred to as a zero vector.

=== Vector Space Axioms ===
The vector space axioms are the defining properties of a vector space.

Let $left( G, +_G, circ right)_K$ be a vector space over $K$ where:

:$G$ is a set of objects, called vectors.

:$+_G: G times G to G$ is a binary operation on $G$

:$left( K, +, cdot right)$ is a division ring whose unity is $1_K$

:$circ: K times G to G$ is a binary operation

The usual situation is for $K$ to be one of the standard number fields $mathbb R$ or $mathbb C$.


The vector space axioms consist of the abelian group axioms:

 
 
 
 
 
 
 


together with the properties of a unitary module:

 
 
 
 
 
 

=== Vector ===
Let $V = left( G, +_G, circ right)_K$ be a vector space over $K$, where:

:$left( G, +_G right)$ is an abelian group

:$left( K, +_K, times_K right)$ is the scalar field of $V$.


The elements of the abelian group $left( G, +_G right)$ are called vectors.

=== Zero Vector ===
Let $left( R, +_R, times_R right)$ be a ring.

Let $left( G, +_G right)$ be an abelian group.

Let $left( G, +_G, circ right)_R$ be an $R$-module.


The identity of $left( G, +_G right)$ is usually denoted $boldsymbol 0$, or some variant of this, and called the zero vector:

:$forall mathbf a in left( G, +_G, circ right)_R: boldsymbol 0 +_G mathbf a = mathbf a = mathbf a +_G boldsymbol 0$


Note that on occasion it is advantageous to denote the zero vector differently, for example by $e$, or $boldsymbol 0_V$ or $boldsymbol 0_G$, in order to highlight the fact that the zero vector is not the same object as the zero scalar.


=== Zero Vector in $mathbb R^n$ ===
Let $left( mathbb R^n, +, times right)_mathbb R$ be a real vector space.

The zero vector in $left( mathbb R^n, +, times right)_mathbb R$ is:

:$mathbf 0_{n times 1} := begin {bmatrix} 0 \ 0 \ vdots \ 0 end {bmatrix}$

where $0 in mathbb R$.

=== Zero Vector Quantity ===
A vector quantity whose magnitude is zero is referred to as a zero vector.",Definition:Vector Space,,false,"=== Definition 1 ===
Let ( K, +_K, ×_K ) be a field.

Let ( G, +_G ) be an abelian group.

Let ( G, +_G, ∘)_K be a unitary K-module.


Then ( G, +_G, ∘)_K is a vector space over K or a K-vector space.


That is, a vector space is a unitary module whose scalar ring is a field.

=== Definition 2 ===
Let ( K, +_K, ×_K ) be a field whose unity is 1_K.

Let ( G, +_G ) be an abelian group.

Let ( End(   )G, +, ∘) be the endomorphism ring of ( G, +_G ) such that I_G is the identity mapping.

Let ·: ( K, +_K, ×_K ) →( End(   )G, +, ∘) be a ring homomorphism from K to End(   )G which maps 1_K to I_G.


Then ( G, +_G, ·, K ) is a vector space over K or a K-vector space.


=== Vector Space Axioms ===
The vector space axioms are the defining properties of a vector space.

Let ( G, +_G, ∘)_K be a vector space over K where:

:G is a set of objects, called vectors.

:+_G: G × G → G is a binary operation on G

:( K, +, ·) is a division ring whose unity is 1_K

:∘: K × G → G is a binary operation

The usual situation is for K to be one of the standard number fields ℝ or ℂ.


The vector space axioms consist of the abelian group axioms:

 
 
 
 
 
 
 


together with the properties of a unitary module:

 
 
 
 
 
 

=== Vector ===
Let V = ( G, +_G, ∘)_K be a vector space over K, where:

:( G, +_G ) is an abelian group

:( K, +_K, ×_K ) is the scalar field of V.


The elements of the abelian group ( G, +_G ) are called vectors.

=== Zero Vector ===
Let ( R, +_R, ×_R ) be a ring.

Let ( G, +_G ) be an abelian group.

Let ( G, +_G, ∘)_R be an R-module.


The identity of ( G, +_G ) is usually denoted 0, or some variant of this, and called the zero vector:

:∀𝐚∈( G, +_G, ∘)_R:  0 +_G 𝐚 = 𝐚 = 𝐚 +_G  0


Note that on occasion it is advantageous to denote the zero vector differently, for example by e, or 0_V or 0_G, in order to highlight the fact that the zero vector is not the same object as the zero scalar.


=== Zero Vector in ℝ^n ===
Let ( ℝ^n, +, ×)_ℝ be a real vector space.

The zero vector in ( ℝ^n, +, ×)_ℝ is:

:0_n × 1 := [ 0; 0; ⋮; 0 ]

where 0 ∈ℝ.

=== Zero Vector Quantity ===
A vector quantity whose magnitude is zero is referred to as a zero vector.

=== Vector Space Axioms ===
The vector space axioms are the defining properties of a vector space.

Let ( G, +_G, ∘)_K be a vector space over K where:

:G is a set of objects, called vectors.

:+_G: G × G → G is a binary operation on G

:( K, +, ·) is a division ring whose unity is 1_K

:∘: K × G → G is a binary operation

The usual situation is for K to be one of the standard number fields ℝ or ℂ.


The vector space axioms consist of the abelian group axioms:

 
 
 
 
 
 
 


together with the properties of a unitary module:

 
 
 
 
 
 

=== Vector ===
Let V = ( G, +_G, ∘)_K be a vector space over K, where:

:( G, +_G ) is an abelian group

:( K, +_K, ×_K ) is the scalar field of V.


The elements of the abelian group ( G, +_G ) are called vectors.

=== Zero Vector ===
Let ( R, +_R, ×_R ) be a ring.

Let ( G, +_G ) be an abelian group.

Let ( G, +_G, ∘)_R be an R-module.


The identity of ( G, +_G ) is usually denoted 0, or some variant of this, and called the zero vector:

:∀𝐚∈( G, +_G, ∘)_R:  0 +_G 𝐚 = 𝐚 = 𝐚 +_G  0


Note that on occasion it is advantageous to denote the zero vector differently, for example by e, or 0_V or 0_G, in order to highlight the fact that the zero vector is not the same object as the zero scalar.


=== Zero Vector in ℝ^n ===
Let ( ℝ^n, +, ×)_ℝ be a real vector space.

The zero vector in ( ℝ^n, +, ×)_ℝ is:

:0_n × 1 := [ 0; 0; ⋮; 0 ]

where 0 ∈ℝ.

=== Zero Vector Quantity ===
A vector quantity whose magnitude is zero is referred to as a zero vector.",Space
"['Definitions/Column Space', 'Definitions/Matrix Theory', 'Definitions/Linear Algebra']",Definition:Space,"Let $R$ be a ring.

Let:

:$mathbf A_{m times n} = begin{bmatrix}
a_{1 1} & a_{1 2} & cdots & a_{1 n} \
a_{2 1} & a_{2 2} & cdots & a_{2 n} \
vdots & vdots & ddots & vdots \
a_{m 1} & a_{m 2} & cdots & a_{m n} \
end{bmatrix}$

be a matrix over $R$ such that every column is defined as a vector:

:$forall i: 1 le i le m: begin {bmatrix} a_{1 i} \ a_{2 i} \ vdots \ a_{m i} end {bmatrix} in mathbf V$

where $mathbf V$ is some vector space.


Then the column space of $mathbf A$ is the linear span of all such column vectors:

:$mathrm C left(   right){mathbf A} = mathrm {span} left(   right){begin {bmatrix} a_{1 1} \ a_{2 1} \ vdots \ a_{m 1} end {bmatrix}, begin {bmatrix} a_{1 2} \ a_{2 2} \ vdots \ a_{m 2} end {bmatrix}, cdots, begin {bmatrix} a_{1 n} \ a_{2 n} \ vdots \ a_{m n} end {bmatrix} }$",Definition:Column Space,,false,"Let R be a ring.

Let:

:𝐀_m × n = [ a_1 1 a_1 2     ⋯ a_1 n; a_2 1 a_2 2     ⋯ a_2 n;     ⋮     ⋮     ⋱     ⋮; a_m 1 a_m 2     ⋯ a_m n;       ]

be a matrix over R such that every column is defined as a vector:

:∀ i: 1 ≤ i ≤ m: [ a_1 i; a_2 i;     ⋮; a_m i ]∈𝐕

where 𝐕 is some vector space.


Then the column space of 𝐀 is the linear span of all such column vectors:

:C(   )𝐀 = span(   )[ a_1 1; a_2 1;     ⋮; a_m 1 ], [ a_1 2; a_2 2;     ⋮; a_m 2 ], ⋯, [ a_1 n; a_2 n;     ⋮; a_m n ]",Space
['Definitions/Linear Algebra'],Definition:Space,"Let $R$ be a ring.

Let $mathbf A$ be a matrix over $R$.

Let $mathbf A^intercal$ be the transpose of $mathbf A$.
 
Let the columns of $mathbf A^intercal$ be members of a vector space.

The row space of $mathbf A$ is defined as the column space of $mathbf A^intercal$.",Definition:Row Space,,false,"Let R be a ring.

Let 𝐀 be a matrix over R.

Let 𝐀^⊺ be the transpose of 𝐀.
 
Let the columns of 𝐀^⊺ be members of a vector space.

The row space of 𝐀 is defined as the column space of 𝐀^⊺.",Space
"['Definitions/Null Spaces', 'Definitions/Linear Algebra']",Definition:Space,"Let:
$quad mathbf A_{m times n} = begin {bmatrix}
a_{11} & a_{12} & cdots & a_{1n} \
a_{21} & a_{22} & cdots & a_{2n} \
vdots & vdots & ddots & vdots \
a_{m1} & a_{m2} & cdots & a_{mn} \
end {bmatrix}$,  $mathbf x_{n times 1} = begin {bmatrix} x_1 \ x_2 \ vdots \ x_n end {bmatrix}$, $mathbf 0_{m times 1} = begin {bmatrix} 0 \ 0 \ vdots \ 0 end {bmatrix}$

be matrices where each column is a member of a real vector space.

The set of all solutions to $mathbf A mathbf x = mathbf 0$:

:$mathrm N left(   right){mathbf A} = leftlbrace mathbf x in mathbb R^n : mathbf {A x} = mathbf 0 rightrbrace$

is called the null space of $mathbf A$.


 ",Definition:Null Space,,false,"Let:
𝐀_m × n = [ a_11 a_12    ⋯ a_1n; a_21 a_22    ⋯ a_2n;    ⋮    ⋮    ⋱    ⋮; a_m1 a_m2    ⋯ a_mn;      ],  𝐱_n × 1 = [ x_1; x_2;   ⋮; x_n ], 0_m × 1 = [ 0; 0; ⋮; 0 ]

be matrices where each column is a member of a real vector space.

The set of all solutions to 𝐀𝐱 = 0:

:N(   )𝐀 = {𝐱∈ℝ^n : 𝐀 𝐱 = 0}

is called the null space of 𝐀.


 ",Space
['Definitions/Bilinear Forms (Linear Algebra)'],Definition:Space,"Let $mathbb K$ be a field.


A bilinear space over $mathbb K$ is a pair $left( V, f right)$ where:
:$V$ be a vector space over $mathbb K$ of finite dimension $n > 0$
:$f$ is a bilinear form on $V$.",Definition:Bilinear Space,,false,"Let 𝕂 be a field.


A bilinear space over 𝕂 is a pair ( V, f ) where:
:V be a vector space over 𝕂 of finite dimension n > 0
:f is a bilinear form on V.",Space
['Definitions/Quadratic Forms (Linear Algebra)'],Definition:Space,"Let $mathbb K$ be a field.


A quadratic space over $mathbb K$ is a pair $left( V, q right)$ where:
:$V$ is a vector space over $mathbb K$ of finite dimension $n > 0$
:$q$ is a quadratic form on $V$.",Definition:Quadratic Space,,false,"Let 𝕂 be a field.


A quadratic space over 𝕂 is a pair ( V, q ) where:
:V is a vector space over 𝕂 of finite dimension n > 0
:q is a quadratic form on V.",Space
"['Definitions/Ordinary Space', 'Definitions/Geometry', 'Definitions/Projective Geometry', 'Definitions/Physics']",Definition:Space,"Ordinary space (or just space) is a word used to mean the universe we live in.

The intuitive belief is that space is $3$-dimensional and therefore isomorphic to the real vector space $mathbb R^3$.


Hence ordinary space is usually taken as an alternative term for Euclidean $3$-dimensional space.",Definition:Ordinary Space,,false,"Ordinary space (or just space) is a word used to mean the universe we live in.

The intuitive belief is that space is 3-dimensional and therefore isomorphic to the real vector space ℝ^3.


Hence ordinary space is usually taken as an alternative term for Euclidean 3-dimensional space.",Space
"['Definitions/Real Vector Spaces', 'Definitions/Examples of Vector Spaces', 'Definitions/Real Analysis', 'Definitions/Analytic Geometry']",Definition:Space,"Let $mathbb R$ be the set of real numbers.


Then the $mathbb R$-module $mathbb R^n$ is called the real ($n$-dimensional) vector space.",Definition:Real Vector Space,,false,"Let ℝ be the set of real numbers.


Then the ℝ-module ℝ^n is called the real (n-dimensional) vector space.",Space
"['Definitions/Generators of Modules', 'Definitions/Module Theory', 'Definitions/Linear Algebra']",Definition:Spanning Set,"Let $R$ be a ring.

Let $M$ be an $R$-module.

Let $S subseteq M$ be a subset.


=== Definition 1 ===
Let $R$ be a ring.

Let $M$ be an $R$-module.

Let $S subseteq M$ be a subset.


$S$ is a generator of $M$  if and only if  $M$ is the submodule generated by $S$.

=== Definition 2 ===
Let $R$ be a ring.

Let $M$ be an $R$-module.

Let $S subseteq M$ be a subset.


$S$ is a generator of $M$  if and only if  $M$ has no proper submodule containing $S$.",Definition:Generator of Module,,false,"Let R be a ring.

Let M be an R-module.

Let S ⊆ M be a subset.


=== Definition 1 ===
Let R be a ring.

Let M be an R-module.

Let S ⊆ M be a subset.


S is a generator of M  if and only if  M is the submodule generated by S.

=== Definition 2 ===
Let R be a ring.

Let M be an R-module.

Let S ⊆ M be a subset.


S is a generator of M  if and only if  M has no proper submodule containing S.",Spanning Set
"['Definitions/Generators of Vector Spaces', 'Definitions/Vector Spaces', 'Definitions/Linear Algebra']",Definition:Spanning Set,"Let $K$ be a division ring.

Let $mathbf V$ be a vector space over $K$.

Let $S subseteq mathbf V$ be a subset of $mathbf V$.


$S$ is a generator of $mathbf V$  if and only if  every element of $mathbf V$ is a linear combination of elements of $S$.",Definition:Generator of Vector Space,,false,"Let K be a division ring.

Let 𝐕 be a vector space over K.

Let S ⊆𝐕 be a subset of 𝐕.


S is a generator of 𝐕  if and only if  every element of 𝐕 is a linear combination of elements of S.",Spanning Set
['Definitions/Commutative Algebra'],Definition:Spectrum,"Let $A$ be a commutative ring with unity.


The prime spectrum of $A$ is the set of prime ideals $mathfrak p$ of $A$:

:$mathrm {Spec} left( A right) = leftlbrace mathfrak p lhd A: mathfrak p text{ is prime}  rightrbrace$

where $mathfrak p lhd A$ indicates that $mathfrak p$ is an ideal of $A$.",Definition:Prime Spectrum of Ring,,false,"Let A be a commutative ring with unity.


The prime spectrum of A is the set of prime ideals 𝔭 of A:

:Spec( A ) = {𝔭 A: 𝔭 is prime}

where 𝔭 A indicates that 𝔭 is an ideal of A.",Spectrum
['Definitions/Commutative Algebra'],Definition:Spectrum,"Let $A$ be a commutative ring with unity.


The maximal spectrum of $A$ is the set of maximal ideals of $A$:

:$operatorname{Max} : mathrm {Spec} left( A right) = leftlbrace mathfrak m lhd A : mathfrak m text { is maximal}  rightrbrace$

where $I lhd A$ indicates that $I$ is an ideal of $A$.


The notation $operatorname {Max} : mathrm {Spec} left( A right)$ is also a shorthand for the locally ringed space
:$left( operatorname {Max} : mathrm {Spec} left( A right), tau, mathcal O_{operatorname {Max Spec}  left(   right)A}  right)$
where:
:$tau$ is the Zariski topology on $operatorname {Max Spec}  left(   right)A$
:$mathcal O_{operatorname {Max Spec}  left(   right)A}$ is the structure sheaf of $operatorname {Max Spec}  left(   right)A$",Definition:Maximal Spectrum of Ring,,false,"Let A be a commutative ring with unity.


The maximal spectrum of A is the set of maximal ideals of A:

:Max Spec( A ) = {𝔪 A : 𝔪 is maximal}

where I  A indicates that I is an ideal of A.


The notation Max Spec( A ) is also a shorthand for the locally ringed space
:( Max Spec( A ), τ, 𝒪_Max Spec(   )A)
where:
:τ is the Zariski topology on Max Spec(   )A
:𝒪_Max Spec(   )A is the structure sheaf of Max Spec(   )A",Spectrum
"['Spectra (Spectral Theory)', 'Definitions/Spectra (Spectral Theory)', 'Definitions/Bounded Linear Operators', 'Definitions/Banach Spaces', 'Definitions/Spectra (Spectral Theory)']",Definition:Spectrum,"Let $left( X, leftlVert cdot rightrVert_X right)$ be a Banach space over $mathbb C$. 

Let $A : X to X$ be a bounded linear operator.

Let $rho left(   right)A$ be the resolvent set of $A$. 

Let: 

:$sigma left(   right)A = mathbb C setminus rho left(   right)A$


We say that $sigma left(   right)A$ is the spectrum of $A$.",Definition:Spectrum (Spectral Theory)/Bounded Linear Operator,,false,"Let ( X, ‖·‖_X ) be a Banach space over ℂ. 

Let A : X → X be a bounded linear operator.

Let ρ(   )A be the resolvent set of A. 

Let: 

:σ(   )A = ℂ∖ρ(   )A


We say that σ(   )A is the spectrum of A.",Spectrum
['Definitions/Game Theory'],Definition:State,"Let $G$ be a game whose outcome is determined by the realization of a random variable $X$.

Each of the possible values that can be taken by $X$ is known as a state of $G$.",Definition:State of Game,,false,"Let G be a game whose outcome is determined by the realization of a random variable X.

Each of the possible values that can be taken by X is known as a state of G.",State
"['Definitions/States of Matter', 'Definitions/Matter']",Definition:State,"Matter can be in one of the following states:

=== Solid ===
Solid is one of the fundamental states of matter.

A body is solid  if and only if  it retains its volume and has a well-definable shape without the need for a container.

=== Liquid ===
Liquid is one of the fundamental states of matter.

A body is liquid  if and only if  it retains its volume, but in order to maintain a specific shape it needs a container.

=== Gas ===
Gas is one of the fundamental states of matter.

A body is gas  if and only if  it can be contained only if it is fully surrounded by a solid, or in a bubble of liquid, or held together by gravitational pull.

=== Plasma ===
Plasma is one of the fundamental states of matter.

A body is plasma  if and only if  it consists of partially ionized gas and electrons.",Definition:State of Matter,,false,"Matter can be in one of the following states:

=== Solid ===
Solid is one of the fundamental states of matter.

A body is solid  if and only if  it retains its volume and has a well-definable shape without the need for a container.

=== Liquid ===
Liquid is one of the fundamental states of matter.

A body is liquid  if and only if  it retains its volume, but in order to maintain a specific shape it needs a container.

=== Gas ===
Gas is one of the fundamental states of matter.

A body is gas  if and only if  it can be contained only if it is fully surrounded by a solid, or in a bubble of liquid, or held together by gravitational pull.

=== Plasma ===
Plasma is one of the fundamental states of matter.

A body is plasma  if and only if  it consists of partially ionized gas and electrons.",State
['Definitions/Conditional'],Definition:Strong,"In a conditional $p implies q$, the statement $p$ is stronger than $q$.",Definition:Conditional/Language of Conditional/Strong,,false,"In a conditional p  q, the statement p is stronger than q.",Strong
['Definitions/Compact Spaces'],Definition:Strong,"Let $T = left({S, tau}right)$ be a topological space.


=== Definition 1 ===
Let $T = left( S, tau right)$ be a topological space.


The space $T$ is strongly locally compact  if and only if :
:every point of $S$ is contained in an open set whose closure is compact.

=== Definition 2 ===
Let $T = left({S, tau}right)$ be a topological space.


The space $T$ is strongly locally compact  if and only if :
:every point has a closed compact neighborhood.
That is:
:every point of $S$ is contained in an open set which is contained in a closed compact subspace.",Definition:Strongly Locally Compact Space,,false,"Let T = (S, τ) be a topological space.


=== Definition 1 ===
Let T = ( S, τ) be a topological space.


The space T is strongly locally compact  if and only if :
:every point of S is contained in an open set whose closure is compact.

=== Definition 2 ===
Let T = (S, τ) be a topological space.


The space T is strongly locally compact  if and only if :
:every point has a closed compact neighborhood.
That is:
:every point of S is contained in an open set which is contained in a closed compact subspace.",Strong
['Definitions/Formal Semantics'],Definition:Structure,"Let $mathcal L$ be a formal language.

Part of specifying a formal semantics $mathscr M$ for $mathcal L$ is to specify structures $mathcal M$ for $mathscr M$.


A structure can in principle be any object one can think of.

However, to get a useful formal semantics, the structures should support a meaningful definition of validity for the WFFs of $mathcal L$.


It is common that structures are sets, often endowed with a number of relations or functions.


=== Structure for Predicate Logic ===
Let $mathcal L_1$ be the language of predicate logic.


A structure $mathcal A$ for $mathcal L_1$ comprises:

:$(1): quad$ A non-empty set $A$;
:$(2): quad$ For each function symbol $f$ of arity $n$, a mapping $f_mathcal A: A^n to A$;
:$(3): quad$ For each predicate symbol $p$ of arity $n$, a mapping $p_mathcal A: A^n to Bbb B$

where $Bbb B$ denotes the set of truth values.

$A$ is called the underlying set of $mathcal A$.

$f_mathcal A$ and $p_mathcal A$ are called the interpretations of $f$ and $p$ in $mathcal A$, respectively.


We remark that function symbols of arity $0$ are interpreted as constants in $A$.

To avoid pathological situations with the interpretation of arity-$0$ function symbols, it is essential that $A$ be non-empty.

Also, the predicate symbols may be interpreted as relations via their characteristic functions.


 ",Definition:Formal Semantics/Structure,,false,"Let ℒ be a formal language.

Part of specifying a formal semantics ℳ for ℒ is to specify structures ℳ for ℳ.


A structure can in principle be any object one can think of.

However, to get a useful formal semantics, the structures should support a meaningful definition of validity for the WFFs of ℒ.


It is common that structures are sets, often endowed with a number of relations or functions.


=== Structure for Predicate Logic ===
Let ℒ_1 be the language of predicate logic.


A structure 𝒜 for ℒ_1 comprises:

:(1): A non-empty set A;
:(2): For each function symbol f of arity n, a mapping f_𝒜: A^n → A;
:(3): For each predicate symbol p of arity n, a mapping p_𝒜: A^n → B

where B denotes the set of truth values.

A is called the underlying set of 𝒜.

f_𝒜 and p_𝒜 are called the interpretations of f and p in 𝒜, respectively.


We remark that function symbols of arity 0 are interpreted as constants in A.

To avoid pathological situations with the interpretation of arity-0 function symbols, it is essential that A be non-empty.

Also, the predicate symbols may be interpreted as relations via their characteristic functions.


 ",Structure
"['Definitions/Predicate Logic', 'Definitions/Formal Semantics', 'Definitions/Model Theory for Predicate Logic']",Definition:Structure,"Let $mathcal L_1$ be the language of predicate logic.


A structure $mathcal A$ for $mathcal L_1$ comprises:

:$(1): quad$ A non-empty set $A$;
:$(2): quad$ For each function symbol $f$ of arity $n$, a mapping $f_mathcal A: A^n to A$;
:$(3): quad$ For each predicate symbol $p$ of arity $n$, a mapping $p_mathcal A: A^n to Bbb B$

where $Bbb B$ denotes the set of truth values.

$A$ is called the underlying set of $mathcal A$.

$f_mathcal A$ and $p_mathcal A$ are called the interpretations of $f$ and $p$ in $mathcal A$, respectively.


We remark that function symbols of arity $0$ are interpreted as constants in $A$.

To avoid pathological situations with the interpretation of arity-$0$ function symbols, it is essential that $A$ be non-empty.

Also, the predicate symbols may be interpreted as relations via their characteristic functions.


 ",Definition:Structure for Predicate Logic,,false,"Let ℒ_1 be the language of predicate logic.


A structure 𝒜 for ℒ_1 comprises:

:(1): A non-empty set A;
:(2): For each function symbol f of arity n, a mapping f_𝒜: A^n → A;
:(3): For each predicate symbol p of arity n, a mapping p_𝒜: A^n → B

where B denotes the set of truth values.

A is called the underlying set of 𝒜.

f_𝒜 and p_𝒜 are called the interpretations of f and p in 𝒜, respectively.


We remark that function symbols of arity 0 are interpreted as constants in A.

To avoid pathological situations with the interpretation of arity-0 function symbols, it is essential that A be non-empty.

Also, the predicate symbols may be interpreted as relations via their characteristic functions.


 ",Structure
"['Definitions/Relation Theory', 'Definitions/Relational Structures']",Definition:Structure,"A relational structure is an ordered pair $left( S, mathcal R right)$, where:
:$S$ is a set
:$mathcal R$ is an endorelation on $S$.",Definition:Relational Structure,,false,"A relational structure is an ordered pair ( S, ℛ), where:
:S is a set
:ℛ is an endorelation on S.",Structure
"['Definitions/Abstract Algebra', 'Definitions/Algebraic Structures']",Definition:Structure,"An algebraic structure with $n$ operations is an ordered tuple:
:$left( S, circ_1, circ_2, ldots, circ_n right)$
where:
:$S$ is a set
:$circ_1, circ_2, ldots, circ_n$ are $n$ binary operations which are defined on all the elements of $S times S$.


=== One Operation ===
An algebraic structure with $1$ operation is an ordered pair:
:$left( S, circ right)$
where:
:$S$ is a set
:$circ$ is a binary operation defined on all the elements of $S times S$.

=== Two Operations ===
An algebraic structure with $2$ operations is an ordered triple:
:$left( S, circ, * right)$
where:
:$S$ is a set
:$circ$ and $*$ are binary operations defined on all the elements of $S times S$.",Definition:Algebraic Structure,,false,"An algebraic structure with n operations is an ordered tuple:
:( S, ∘_1, ∘_2, …, ∘_n )
where:
:S is a set
:∘_1, ∘_2, …, ∘_n are n binary operations which are defined on all the elements of S × S.


=== One Operation ===
An algebraic structure with 1 operation is an ordered pair:
:( S, ∘)
where:
:S is a set
:∘ is a binary operation defined on all the elements of S × S.

=== Two Operations ===
An algebraic structure with 2 operations is an ordered triple:
:( S, ∘, * )
where:
:S is a set
:∘ and * are binary operations defined on all the elements of S × S.",Structure
"['Definitions/Algebraic Geometry', 'Definitions/Ringed Spaces']",Definition:Structure Sheaf,"A ringed space is a pair $left( X, mathcal O_X right)$ where:
:$X$ is a topological space
:$mathcal O_X$ is a sheaf of commutative rings with unity on $X$.



=== Structure Sheaf ===
Let $left( X, mathcal O_X right)$ be a ringed space.


The structure sheaf of $left( X, mathcal O_X right)$ is the term $mathcal O_X$.",Definition:Ringed Space,,false,"A ringed space is a pair ( X, 𝒪_X ) where:
:X is a topological space
:𝒪_X is a sheaf of commutative rings with unity on X.



=== Structure Sheaf ===
Let ( X, 𝒪_X ) be a ringed space.


The structure sheaf of ( X, 𝒪_X ) is the term 𝒪_X.",Structure Sheaf
['Definitions/Algebraic Geometry'],Definition:Structure Sheaf,"Let $A$ be a commutative ring with unity.

Let $left( mathrm {Spec} left( A right), tau right)$ be its spectrum with Zariski topology $tau$


=== Definition 1 ===

Note that Principal Open Subsets form Basis of Zariski Topology on Prime Spectrum.

We define the structure sheaf of $mathrm {Spec} left( A right)$ to be the sheaf induced by a sheaf on this basis defined as follows:
:For $f in A$, $mathcal O left(   right){X left(   right)f}$ is the localization of $A$ at $f$
:For $f, g in A$ with $X left(   right)f supset X left(   right)g$, the restriction is the induced homomorphism of $A$-algebras $A_f to A_g$.


=== Definition 2 ===

 


=== Definition 3 ===


 ",Definition:Structure Sheaf of Spectrum of Ring,,false,"Let A be a commutative ring with unity.

Let ( Spec( A ), τ) be its spectrum with Zariski topology τ


=== Definition 1 ===

Note that Principal Open Subsets form Basis of Zariski Topology on Prime Spectrum.

We define the structure sheaf of Spec( A ) to be the sheaf induced by a sheaf on this basis defined as follows:
:For f ∈ A, 𝒪(   )X (   )f is the localization of A at f
:For f, g ∈ A with X (   )f ⊃ X (   )g, the restriction is the induced homomorphism of A-algebras A_f → A_g.


=== Definition 2 ===

 


=== Definition 3 ===


 ",Structure Sheaf
"['Definitions/Subadditive Functions', 'Definitions/Abstract Algebra', 'Definitions/Analysis']",Definition:Subadditive Function,"Let $left( S, +_S right)$ and $left( T, +_T, preceq right)$ be semigroups such that $left( T, +_T, preceq right)$ is ordered.


Let $f: S to T$ be a mapping from $S$ to $T$ which satisfies the relation:
:$forall a, b in S: f left(   right){a +_S b} preceq f left(   right)a +_T f left(   right)b$


Then $f$ is defined as being subadditive.


The usual context in which this is encountered is where $S$ and $T$ are both the set of real numbers $mathbb R$ (or a subset of them).",Definition:Subadditive Function (Conventional),,false,"Let ( S, +_S ) and ( T, +_T, ≼) be semigroups such that ( T, +_T, ≼) is ordered.


Let f: S → T be a mapping from S to T which satisfies the relation:
:∀ a, b ∈ S: f (   )a +_S b≼ f (   )a +_T f (   )b


Then f is defined as being subadditive.


The usual context in which this is encountered is where S and T are both the set of real numbers ℝ (or a subset of them).",Subadditive Function
"['Definitions/Set Systems', 'Definitions/Measure Theory']",Definition:Subadditive Function,"Let $mathcal S$ be an algebra of sets.

Let $f: mathcal S to overline mathbb R$ be a function, where $overline mathbb R$ denotes the extended set of real numbers.


Then $f$ is defined to be subadditive (or sub-additive)  if and only if :
:$forall S, T in mathcal S: f left(   right){S cup T} le f left(   right)S + f left(   right)T$


That is, for any two elements of $mathcal S$, $f$ applied to their union is not greater than the sum of $f$ of the individual elements.",Definition:Subadditive Function (Measure Theory),,false,"Let 𝒮 be an algebra of sets.

Let f: 𝒮→R be a function, where R denotes the extended set of real numbers.


Then f is defined to be subadditive (or sub-additive)  if and only if :
:∀ S, T ∈𝒮: f (   )S ∪ T≤ f (   )S + f (   )T


That is, for any two elements of 𝒮, f applied to their union is not greater than the sum of f of the individual elements.",Subadditive Function
"['Definitions/Set Systems', 'Definitions/Measure Theory']",Definition:Subadditive Function,"Let $unicode{x3a3}$ be a $sigma$-algebra over a set $X$.

Let $f: unicode{x3a3} to overline mathbb R$ be a function, where $overline mathbb R$ denotes the set of extended real numbers.


Then $f$ is defined as countably subadditive  if and only if  for any sequence $leftlangle E_n rightrangle_{n mathop in mathbb N}$ of elements of $unicode{x3a3}$:

:$ds f left(   right){bigcup_{n mathop = 0}^infty E_n} le sum_{n mathop = 0}^infty f left(   right){E_n}$",Definition:Countably Subadditive Function,,false,"Let x3a3 be a σ-algebra over a set X.

Let f: x3a3→R be a function, where R denotes the set of extended real numbers.


Then f is defined as countably subadditive  if and only if  for any sequence ⟨ E_n ⟩_n ∈ℕ of elements of x3a3:

:f (   )⋃_n  = 0^∞ E_n≤∑_n  = 0^∞ f (   )E_n",Subadditive Function
"['Definitions/Real Analysis', 'Definitions/Subdivisions (Real Analysis)']",Definition:Subdivision,"Let $left[ a ,.,.,   right]b$ be a closed interval of the set $mathbb R$ of real numbers.


=== Finite ===
Let $left[ a ,.,.,   right]b$ be a closed interval of the set $mathbb R$ of real numbers.


Let $x_0, x_1, x_2, ldots, x_{n - 1}, x_n$ be points of $mathbb R$ such that:

:$a = x_0 < x_1 < x_2 < cdots < x_{n - 1} < x_n = b$


Then $leftlbrace x_0, x_1, x_2, ldots, x_{n - 1}, x_n rightrbrace$ form a finite subdivision of $left[ a ,.,.,   right]b$.


=== Normal Subdivision ===
Let $left[ a ,.,.,   right]b$ be a closed interval of the set $mathbb R$ of real numbers.

Let $P = leftlbrace x_0, x_1, x_2, ldots, x_{n - 1}, x_n rightrbrace$ form a (finite) subdivision of $left[ a ,.,.,   right]b$.


$P$ is a normal subdivision of $left[ a ,.,.,   right]b$  if and only if :
:the length of every interval of the form $left[ x_i ,.,.,   right]{x_{i + 1} }$ is the same as every other.


That is,  if and only if :

:$exists c in mathbb R_{> 0}: forall i in mathbb N_{< n}: x_{i + 1} - x_i = c$

=== Infinite ===
Let $left[ a ,.,.,   right]b$ be a closed interval of the set $mathbb R$ of real numbers.


Let $x_0, x_1, x_2, ldots$ be an infinite number of points of $mathbb R$ such that:

:$a = x_0 < x_1 < x_2 < cdots < x_{n - 1} < ldots le b$


Then $leftlbrace x_0, x_1, x_2, ldots rightrbrace$ forms an infinite subdivision of $left[ a ,.,.,   right]b$.


 ",Definition:Subdivision (Real Analysis),,false,"Let [ a  . . ]b be a closed interval of the set ℝ of real numbers.


=== Finite ===
Let [ a  . . ]b be a closed interval of the set ℝ of real numbers.


Let x_0, x_1, x_2, …, x_n - 1, x_n be points of ℝ such that:

:a = x_0 < x_1 < x_2 < ⋯ < x_n - 1 < x_n = b


Then { x_0, x_1, x_2, …, x_n - 1, x_n } form a finite subdivision of [ a  . . ]b.


=== Normal Subdivision ===
Let [ a  . . ]b be a closed interval of the set ℝ of real numbers.

Let P = { x_0, x_1, x_2, …, x_n - 1, x_n } form a (finite) subdivision of [ a  . . ]b.


P is a normal subdivision of [ a  . . ]b  if and only if :
:the length of every interval of the form [ x_i  . . ]x_i + 1 is the same as every other.


That is,  if and only if :

:∃ c ∈ℝ_> 0: ∀ i ∈ℕ_< n: x_i + 1 - x_i = c

=== Infinite ===
Let [ a  . . ]b be a closed interval of the set ℝ of real numbers.


Let x_0, x_1, x_2, … be an infinite number of points of ℝ such that:

:a = x_0 < x_1 < x_2 < ⋯ < x_n - 1 < …≤ b


Then { x_0, x_1, x_2, …} forms an infinite subdivision of [ a  . . ]b.


 ",Subdivision
"['Definitions/Subdivisions (Graph Theory)', 'Definitions/Graph Theory']",Definition:Subdivision,"Let $G = left( V, E right)$ be a graph.


=== Edge Subdivision ===
Let $G = left( V, E right)$ be a graph.


The edge subdivision operation for an edge $leftlbrace u, v rightrbrace in E$ is the deletion of $leftlbrace u, v rightrbrace$ from $G$ and the addition of two edges $leftlbrace u, w rightrbrace$ and $leftlbrace w, v rightrbrace$ along with the new vertex $w$. 


This operation generates a new graph $H$:
:$H = left( V cup leftlbrace w rightrbrace, left( E setminus leftlbrace u, v rightrbrace  right) cup leftlbrace leftlbrace u, w rightrbrace, leftlbrace w, v rightrbrace  rightrbrace  right)$

=== Graph Subdivision ===
Let $G = left( V, E right)$ be a graph.


A graph which has been derived from $G$ by a sequence of edge subdivision operations is called a subdivision of $G$.",Definition:Subdivision (Graph Theory),,false,"Let G = ( V, E ) be a graph.


=== Edge Subdivision ===
Let G = ( V, E ) be a graph.


The edge subdivision operation for an edge { u, v }∈ E is the deletion of { u, v } from G and the addition of two edges { u, w } and { w, v } along with the new vertex w. 


This operation generates a new graph H:
:H = ( V ∪{ w }, ( E ∖{ u, v }) ∪{{ u, w }, { w, v }})

=== Graph Subdivision ===
Let G = ( V, E ) be a graph.


A graph which has been derived from G by a sequence of edge subdivision operations is called a subdivision of G.",Subdivision
"['Definitions/Subdivisions (Graph Theory)', 'Definitions/Edges of Graphs']",Definition:Subdivision,"Let $G = left( V, E right)$ be a graph.


The edge subdivision operation for an edge $leftlbrace u, v rightrbrace in E$ is the deletion of $leftlbrace u, v rightrbrace$ from $G$ and the addition of two edges $leftlbrace u, w rightrbrace$ and $leftlbrace w, v rightrbrace$ along with the new vertex $w$. 


This operation generates a new graph $H$:
:$H = left( V cup leftlbrace w rightrbrace, left( E setminus leftlbrace u, v rightrbrace  right) cup leftlbrace leftlbrace u, w rightrbrace, leftlbrace w, v rightrbrace  rightrbrace  right)$",Definition:Subdivision (Graph Theory)/Edge,,false,"Let G = ( V, E ) be a graph.


The edge subdivision operation for an edge { u, v }∈ E is the deletion of { u, v } from G and the addition of two edges { u, w } and { w, v } along with the new vertex w. 


This operation generates a new graph H:
:H = ( V ∪{ w }, ( E ∖{ u, v }) ∪{{ u, w }, { w, v }})",Subdivision
['Definitions/Subdivisions (Graph Theory)'],Definition:Subdivision,"Let $G = left( V, E right)$ be a graph.


A graph which has been derived from $G$ by a sequence of edge subdivision operations is called a subdivision of $G$.",Definition:Subdivision (Graph Theory)/Graph,,false,"Let G = ( V, E ) be a graph.


A graph which has been derived from G by a sequence of edge subdivision operations is called a subdivision of G.",Subdivision
['Definitions/Topology'],Definition:Subspace,"Let $T = left( S, tau right)$ be a topological space.

Let $H subseteq S$ be a non-empty subset of $S$.


Define:
:$tau_H := leftlbrace U cap H: U in tau rightrbrace subseteq mathcal P left( H right)$

where $mathcal P left( H right)$ denotes the power set of $H$.


Then the topological space $T_H = left( H, tau_H right)$ is called a (topological) subspace of $T$.


The set $tau_H$ is referred to as the subspace topology on $H$ (induced by $tau$).",Definition:Topological Subspace,,false,"Let T = ( S, τ) be a topological space.

Let H ⊆ S be a non-empty subset of S.


Define:
:τ_H := { U ∩ H: U ∈τ}⊆𝒫( H )

where 𝒫( H ) denotes the power set of H.


Then the topological space T_H = ( H, τ_H ) is called a (topological) subspace of T.


The set τ_H is referred to as the subspace topology on H (induced by τ).",Subspace
"['Definitions/Metric Subspaces', 'Definitions/Metric Spaces']",Definition:Subspace,"Let $left( A, d right)$ be a metric space.

Let $H subseteq A$.

Let $d_H: H times H to mathbb R$ be the restriction $d restriction_{H times H}$ of $d$ to $H$.

That is, let $forall x, y in H: d_H left(   right){x, y} = d left(   right){x, y}$.


Then $d_H$ is the metric induced on $H$ by $d$ or the subspace metric of $d$ (with respect to $H$).


The metric space $left( H, d_H right)$ is called a metric subspace of $left( A, d right)$.",Definition:Metric Subspace,,false,"Let ( A, d ) be a metric space.

Let H ⊆ A.

Let d_H: H × H →ℝ be the restriction d _H × H of d to H.

That is, let ∀ x, y ∈ H: d_H (   )x, y = d (   )x, y.


Then d_H is the metric induced on H by d or the subspace metric of d (with respect to H).


The metric space ( H, d_H ) is called a metric subspace of ( A, d ).",Subspace
"['Definitions/Linear Algebra', 'Definitions/Vector Algebra']",Definition:Subspace,"Let $K$ be a division ring.

Let $left( S, +, circ right)_K$ be a $K$-algebraic structure with one operation.


Let $T$ be a closed subset of $S$.

Let $left( T, +_T, circ_T right)_K$ be a $K$-vector space where:
:$+_T$ is the restriction of $+$ to $T times T$ and
:$circ_T$ is the restriction of $circ$ to $K times T$.


Then $left( T, +_T, circ_T right)_K$ is a (vector) subspace of $left( S, +, circ right)_K$.


=== Proper Subspace ===
Let $K$ be a division ring.

Let $left( S, +, circ right)_K$ be a $K$-algebraic structure with one operation.

Let $left( T, +_T, circ_T right)_K$ be a vector subspace of $left( S, +, circ right)_K$.


If $T$ is a proper subset of $S$, then $left( T, +_T, circ_T right)_K$ is a proper (vector) subspace of $left( S, +, circ right)_K$.

=== Hilbert Spaces ===
Let $K$ be a division ring.

Let $left({S, +, circ}right)_K$ be a $K$-algebraic structure with one operation.


Let $T$ be a closed subset of $S$.

Let $left({T, +_T, circ_T}right)_K$ be a $K$-vector space where:
: $+_T$ is the restriction of $+$ to $T times T$ and
: $circ_T$ is the restriction of $circ$ to $K times T$.


Then $left({T, +_T, circ_T}right)_K$ is a (vector) subspace of $left({S, +, circ}right)_K$.


When considering Hilbert spaces, one wants to deal with projections onto subspaces.

These projections however require the linear subspace to be closed in topological sense in order to be well-defined.

Therefore, in treatises of Hilbert spaces, one encounters the terminology linear manifold for the concept of vector subspace defined above.

The adapted definition of linear subspace is then that it is a topologically closed linear manifold.",Definition:Vector Subspace,,false,"Let K be a division ring.

Let ( S, +, ∘)_K be a K-algebraic structure with one operation.


Let T be a closed subset of S.

Let ( T, +_T, ∘_T )_K be a K-vector space where:
:+_T is the restriction of + to T × T and
:∘_T is the restriction of ∘ to K × T.


Then ( T, +_T, ∘_T )_K is a (vector) subspace of ( S, +, ∘)_K.


=== Proper Subspace ===
Let K be a division ring.

Let ( S, +, ∘)_K be a K-algebraic structure with one operation.

Let ( T, +_T, ∘_T )_K be a vector subspace of ( S, +, ∘)_K.


If T is a proper subset of S, then ( T, +_T, ∘_T )_K is a proper (vector) subspace of ( S, +, ∘)_K.

=== Hilbert Spaces ===
Let K be a division ring.

Let (S, +, ∘)_K be a K-algebraic structure with one operation.


Let T be a closed subset of S.

Let (T, +_T, ∘_T)_K be a K-vector space where:
: +_T is the restriction of + to T × T and
: ∘_T is the restriction of ∘ to K × T.


Then (T, +_T, ∘_T)_K is a (vector) subspace of (S, +, ∘)_K.


When considering Hilbert spaces, one wants to deal with projections onto subspaces.

These projections however require the linear subspace to be closed in topological sense in order to be well-defined.

Therefore, in treatises of Hilbert spaces, one encounters the terminology linear manifold for the concept of vector subspace defined above.

The adapted definition of linear subspace is then that it is a topologically closed linear manifold.",Subspace
['Definitions/Compound Statements'],Definition:Substatement,A substatement of a compound statement is one of the statements that comprise it.,Definition:Compound Statement/Substatement,,false,A substatement of a compound statement is one of the statements that comprise it.,Substatement
['Definitions/Symbolic Logic'],Definition:Substatement,A substatement of a statement form $mathbf A$ is another statement form which occurs as a part of $mathbf A$.,Definition:Statement Form/Substatement,,false,A substatement of a statement form 𝐀 is another statement form which occurs as a part of 𝐀.,Substatement
['Definitions/Set Theory'],Definition:Substitution,"Let $S$ and $T$ be non-empty sets such that $T$ is not a subset of $S$.

Let:
:$s in S$
:$t in T setminus S$
where $setminus$ denotes set difference.

Let $S'$ be the set defined as:
:$S' = left( S setminus leftlbrace s rightrbrace right) cup leftlbrace t rightrbrace$

That is, $S'$ is the set obtained by removing $s$ and replacing it with $t$ which is not in $S$.


The operation of replacing $s$ with $t$ is known as substitution.


Category:Definitions/Set Theory",Definition:Substitution (Set Theory),,false,"Let S and T be non-empty sets such that T is not a subset of S.

Let:
:s ∈ S
:t ∈ T ∖ S
where ∖ denotes set difference.

Let S' be the set defined as:
:S' = ( S ∖{ s }) ∪{ t }

That is, S' is the set obtained by removing s and replacing it with t which is not in S.


The operation of replacing s with t is known as substitution.


Category:Definitions/Set Theory",Substitution
['Definitions/Formal Languages'],Definition:Substitution,"=== Substitution for Well-Formed Part ===
Let $mathcal F$ be a formal language with alphabet $mathcal A$.

Let $mathbf B$ be a well-formed formula of $mathcal F$.

Let $mathbf A$ be a well-formed part of $mathbf B$.

Let $mathbf A'$ be another well-formed formula.


Then the substitution of $mathbf A'$ for $mathbf A$ in $mathbf B$ is the collation resulting from $mathbf B$ by replacing all occurrences of $mathbf A$ in $mathbf B$ by $mathbf A'$.

It is denoted as $mathbf B left(   right){mathbf A' mathbin {//} mathbf A}$.


Note that it is not immediate that $mathbf B left(   right){mathbf A' mathbin {//} mathbf A}$ is a well-formed formula of $mathcal F$.

This is either accepted as an axiom or proven as a theorem about the formal language $mathcal F$.


=== Example ===


=== Substitution for Letter ===
Let $mathcal F$ be a formal language with alphabet $mathcal A$.

Let $mathbf B$ be a well-formed formula of $mathcal F$.

Let $p$ be a letter of $mathcal F$.

Let $mathbf A$ be another well-formed formula.

Then the substitution of $mathbf A$ for $p$ in $mathbf B$ is the collation resulting from $mathbf B$ by replacing all occurrences of $p$ in $mathbf B$ by $mathbf A$.

It is denoted as $mathbf B left(   right){mathbf A mathbin {//} p}$.


Note that it is not immediate that $mathbf B left(   right){mathbf A mathbin {//} p}$ is a well-formed formula of $mathcal F$.

This is either accepted as an axiom or proven as a theorem about the formal language $mathcal F$.

=== Substitution of Term for Variable ===
=== Substitution of Term in Term ===
Let $beta, tau$ be terms of predicate logic.

Let $x in mathrm{VAR}$ be a variable.

Let $beta left({x gets tau}right)$ be the term resulting from $beta$ by replacing all occurrences of $x$ by $tau$.


Then $beta left({x gets tau}right)$ is called the substitution instance of $beta$ substituting $tau$ for $x$.

=== Substitution of Term in WFF ===
Let $mathbf A$ be a WFF of predicate logic.

Let $tau$ be a term of predicate logic.

Let $x in mathrm{VAR}$ be a variable.

Let $mathbf A left({x gets tau}right)$ be the WFF resulting from $mathbf A$ by replacing all free occurrences of $x$ by $tau$.


Then $mathbf A left({x gets tau}right)$ is called the substitution instance of $mathbf A$ substituting $tau$ for $x$.

Category:Definitions/Language of Predicate Logic

=== Substitution for Metasymbol ===
Let $S_1$ be a statement form.

Let $p$ be a metasymbol which occurs one or more times in $S_1$.

Let $T$ be a statement.

Let $S_2$ be the string formed by replacing every occurrence of $p$ in $S_1$ with $T$.


Then $S_2$ results from the substitution of $p$ by $T$ in $S_1$.

$S_2$ is called a substitution instance of $S_1$.",Definition:Substitution (Formal Systems),,false,"=== Substitution for Well-Formed Part ===
Let ℱ be a formal language with alphabet 𝒜.

Let 𝐁 be a well-formed formula of ℱ.

Let 𝐀 be a well-formed part of 𝐁.

Let 𝐀' be another well-formed formula.


Then the substitution of 𝐀' for 𝐀 in 𝐁 is the collation resulting from 𝐁 by replacing all occurrences of 𝐀 in 𝐁 by 𝐀'.

It is denoted as 𝐁(   )𝐀' //𝐀.


Note that it is not immediate that 𝐁(   )𝐀' //𝐀 is a well-formed formula of ℱ.

This is either accepted as an axiom or proven as a theorem about the formal language ℱ.


=== Example ===


=== Substitution for Letter ===
Let ℱ be a formal language with alphabet 𝒜.

Let 𝐁 be a well-formed formula of ℱ.

Let p be a letter of ℱ.

Let 𝐀 be another well-formed formula.

Then the substitution of 𝐀 for p in 𝐁 is the collation resulting from 𝐁 by replacing all occurrences of p in 𝐁 by 𝐀.

It is denoted as 𝐁(   )𝐀// p.


Note that it is not immediate that 𝐁(   )𝐀// p is a well-formed formula of ℱ.

This is either accepted as an axiom or proven as a theorem about the formal language ℱ.

=== Substitution of Term for Variable ===
=== Substitution of Term in Term ===
Let β, τ be terms of predicate logic.

Let x ∈VAR be a variable.

Let β(x τ) be the term resulting from β by replacing all occurrences of x by τ.


Then β(x τ) is called the substitution instance of β substituting τ for x.

=== Substitution of Term in WFF ===
Let 𝐀 be a WFF of predicate logic.

Let τ be a term of predicate logic.

Let x ∈VAR be a variable.

Let 𝐀(x τ) be the WFF resulting from 𝐀 by replacing all free occurrences of x by τ.


Then 𝐀(x τ) is called the substitution instance of 𝐀 substituting τ for x.

Category:Definitions/Language of Predicate Logic

=== Substitution for Metasymbol ===
Let S_1 be a statement form.

Let p be a metasymbol which occurs one or more times in S_1.

Let T be a statement.

Let S_2 be the string formed by replacing every occurrence of p in S_1 with T.


Then S_2 results from the substitution of p by T in S_1.

S_2 is called a substitution instance of S_1.",Substitution
['Definitions/Formal Languages'],Definition:Substitution,"Let $mathcal F$ be a formal language with alphabet $mathcal A$.

Let $mathbf B$ be a well-formed formula of $mathcal F$.

Let $mathbf A$ be a well-formed part of $mathbf B$.

Let $mathbf A'$ be another well-formed formula.


Then the substitution of $mathbf A'$ for $mathbf A$ in $mathbf B$ is the collation resulting from $mathbf B$ by replacing all occurrences of $mathbf A$ in $mathbf B$ by $mathbf A'$.

It is denoted as $mathbf B left(   right){mathbf A' mathbin {//} mathbf A}$.


Note that it is not immediate that $mathbf B left(   right){mathbf A' mathbin {//} mathbf A}$ is a well-formed formula of $mathcal F$.

This is either accepted as an axiom or proven as a theorem about the formal language $mathcal F$.


=== Example ===
",Definition:Substitution (Formal Systems)/Well-Formed Part,,false,"Let ℱ be a formal language with alphabet 𝒜.

Let 𝐁 be a well-formed formula of ℱ.

Let 𝐀 be a well-formed part of 𝐁.

Let 𝐀' be another well-formed formula.


Then the substitution of 𝐀' for 𝐀 in 𝐁 is the collation resulting from 𝐁 by replacing all occurrences of 𝐀 in 𝐁 by 𝐀'.

It is denoted as 𝐁(   )𝐀' //𝐀.


Note that it is not immediate that 𝐁(   )𝐀' //𝐀 is a well-formed formula of ℱ.

This is either accepted as an axiom or proven as a theorem about the formal language ℱ.


=== Example ===
",Substitution
['Definitions/Formal Languages'],Definition:Substitution,"Let $mathcal F$ be a formal language with alphabet $mathcal A$.

Let $mathbf B$ be a well-formed formula of $mathcal F$.

Let $p$ be a letter of $mathcal F$.

Let $mathbf A$ be another well-formed formula.

Then the substitution of $mathbf A$ for $p$ in $mathbf B$ is the collation resulting from $mathbf B$ by replacing all occurrences of $p$ in $mathbf B$ by $mathbf A$.

It is denoted as $mathbf B left(   right){mathbf A mathbin {//} p}$.


Note that it is not immediate that $mathbf B left(   right){mathbf A mathbin {//} p}$ is a well-formed formula of $mathcal F$.

This is either accepted as an axiom or proven as a theorem about the formal language $mathcal F$.",Definition:Substitution (Formal Systems)/Letter,,false,"Let ℱ be a formal language with alphabet 𝒜.

Let 𝐁 be a well-formed formula of ℱ.

Let p be a letter of ℱ.

Let 𝐀 be another well-formed formula.

Then the substitution of 𝐀 for p in 𝐁 is the collation resulting from 𝐁 by replacing all occurrences of p in 𝐁 by 𝐀.

It is denoted as 𝐁(   )𝐀// p.


Note that it is not immediate that 𝐁(   )𝐀// p is a well-formed formula of ℱ.

This is either accepted as an axiom or proven as a theorem about the formal language ℱ.",Substitution
['Definitions/Predicate Logic'],Definition:Substitution,"Let $mathbf C$ be a WFF of the language of predicate logic $mathcal L_1$.

Consider the (abbreviated) WFF $Q x: mathbf C$ where $Q$ is a quantifier.

Let $y$ be another variable such that:

:$y$ is freely substitutable for $x$ in $mathbf C$
:$y$ does not occur freely in $mathbf C$.


Let $mathbf C'$ be the WFF resulting from substituting $y$ for all free occurrences of $x$ in $mathbf C$.

The change from $Q x: mathbf C$ to $Q y: mathbf C'$ is called alphabetic substitution.",Definition:Alphabetic Substitution,,false,"Let 𝐂 be a WFF of the language of predicate logic ℒ_1.

Consider the (abbreviated) WFF Q x: 𝐂 where Q is a quantifier.

Let y be another variable such that:

:y is freely substitutable for x in 𝐂
:y does not occur freely in 𝐂.


Let 𝐂' be the WFF resulting from substituting y for all free occurrences of x in 𝐂.

The change from Q x: 𝐂 to Q y: 𝐂' is called alphabetic substitution.",Substitution
['Definitions/Formal Systems'],Definition:Substitution,"Let $S_1$ be a statement form.

Let $p$ be a metasymbol which occurs one or more times in $S_1$.

Let $T$ be a statement.

Let $S_2$ be the string formed by replacing every occurrence of $p$ in $S_1$ with $T$.


Then $S_2$ results from the substitution of $p$ by $T$ in $S_1$.

$S_2$ is called a substitution instance of $S_1$.",Definition:Substitution (Formal Systems)/Metasymbol,,false,"Let S_1 be a statement form.

Let p be a metasymbol which occurs one or more times in S_1.

Let T be a statement.

Let S_2 be the string formed by replacing every occurrence of p in S_1 with T.


Then S_2 results from the substitution of p by T in S_1.

S_2 is called a substitution instance of S_1.",Substitution
['Definitions/Mathematical Logic'],Definition:Substitution,"=== Mapping ===

Let $S$ be a set.

Let $f: S^t to S$ be a mapping.

Let $left{{g_1: S^k to S, g_2: S^k to S, ldots, g_t: S^k to S}right}$ be a set of mappings.

Let the mapping $h: S^k to S$ be defined as:
:$h left({s_1, s_2, ldots, s_k}right) = f left({g_1 left({s_1, s_2, ldots, s_k}right), g_2 left({s_1, s_2, ldots, s_k}right), ldots, g_t left({s_1, s_2, ldots, s_k}right)}right)$


Then $h$ is said to be obtained from $f, g_1, g_2, ldots, g_k$ by substitution.


The definition can be generalized in the following ways:
* It can apply to mappings which operate on variously different sets.
* Each of $g_1, g_2, ldots, g_t$ may have different arities. If $g$ is a mapping of $m$ variables where $m > k$, we can always consider it a mapping of $k$ variables in which the additional variables play no part. So if $g_i$ is a mapping of $k_i$ variables, we can take $k = max left{{k_i: i = 1, 2, ldots, t}right}$ and then each $g_i$ is then a mapping of $k$ variables.


=== Partial Function ===

Let $f: mathbb N^t to mathbb N$ be a partial function.

Let $left{{g_1: mathbb N^k to mathbb N, g_2: mathbb N^k to mathbb N, ldots, g_t: mathbb N^k to mathbb N}right}$ be a set of partial functions.

Let the partial function $h: mathbb N^k to mathbb N$ be defined as:
:$h left({n_1, n_2, ldots, n_k}right) approx f left({g_1 left({n_1, n_2, ldots, n_k}right), g_2 left({n_1, n_2, ldots, n_k}right), ldots, g_t left({n_1, n_2, ldots, n_k}right)}right)$
where $approx$ is as defined in Partial Function Equality.


Then $h$ is said to be obtained from $f, g_1, g_2, ldots, g_k$ by substitution.


Note that $h left({n_1, n_2, ldots, n_k}right)$ is defined only when:
* All of $g_1 left({n_1, n_2, ldots, n_k}right), g_2 left({n_1, n_2, ldots, n_k}right), ldots, g_t left({n_1, n_2, ldots, n_k}right)$ are defined
* $f left({g_1 left({n_1, n_2, ldots, n_k}right), g_2 left({n_1, n_2, ldots, n_k}right), ldots, g_t left({n_1, n_2, ldots, n_k}right)}right)$ is defined.",Definition:Substitution (Mathematical Logic),,false,"=== Mapping ===

Let S be a set.

Let f: S^t → S be a mapping.

Let {g_1: S^k → S, g_2: S^k → S, …, g_t: S^k → S} be a set of mappings.

Let the mapping h: S^k → S be defined as:
:h (s_1, s_2, …, s_k) = f (g_1 (s_1, s_2, …, s_k), g_2 (s_1, s_2, …, s_k), …, g_t (s_1, s_2, …, s_k))


Then h is said to be obtained from f, g_1, g_2, …, g_k by substitution.


The definition can be generalized in the following ways:
* It can apply to mappings which operate on variously different sets.
* Each of g_1, g_2, …, g_t may have different arities. If g is a mapping of m variables where m > k, we can always consider it a mapping of k variables in which the additional variables play no part. So if g_i is a mapping of k_i variables, we can take k = max{k_i: i = 1, 2, …, t} and then each g_i is then a mapping of k variables.


=== Partial Function ===

Let f: ℕ^t →ℕ be a partial function.

Let {g_1: ℕ^k →ℕ, g_2: ℕ^k →ℕ, …, g_t: ℕ^k →ℕ} be a set of partial functions.

Let the partial function h: ℕ^k →ℕ be defined as:
:h (n_1, n_2, …, n_k) ≈ f (g_1 (n_1, n_2, …, n_k), g_2 (n_1, n_2, …, n_k), …, g_t (n_1, n_2, …, n_k))
where ≈ is as defined in Partial Function Equality.


Then h is said to be obtained from f, g_1, g_2, …, g_k by substitution.


Note that h (n_1, n_2, …, n_k) is defined only when:
* All of g_1 (n_1, n_2, …, n_k), g_2 (n_1, n_2, …, n_k), …, g_t (n_1, n_2, …, n_k) are defined
* f (g_1 (n_1, n_2, …, n_k), g_2 (n_1, n_2, …, n_k), …, g_t (n_1, n_2, …, n_k)) is defined.",Substitution
"['Definitions/Subgraphs', 'Definitions/Tree Theory']",Definition:Subtree,"Let $T = left( V, E right)$ be a tree.


A subtree of $T$ is a subgraph of $T$ that is also a tree.",Definition:Subtree (Graph Theory),,false,"Let T = ( V, E ) be a tree.


A subtree of T is a subgraph of T that is also a tree.",Subtree
"['Definitions/Subgraphs', 'Definitions/Rooted Trees']",Definition:Subtree,"Let $left( T, r_T right)$ be a rooted tree.


A rooted subtree of $T$ is a rooted tree $left( S, r_S right)$ such that:

:$S$ is a subtree of $T$
:$r_S = r_T$

Note that the second condition implies that $r_T in S$.",Definition:Rooted Subtree,,false,"Let ( T, r_T ) be a rooted tree.


A rooted subtree of T is a rooted tree ( S, r_S ) such that:

:S is a subtree of T
:r_S = r_T

Note that the second condition implies that r_T ∈ S.",Subtree
['Definitions/Set Theory'],Definition:Subtree,"Let $left( T, preceq right)$ be a tree.

A subtree of $left( T, preceq right)$ is an ordered subset $left( S, preceq right)$ with the property that:
:for every $forall s in S: forall t in T: t preceq s implies t in S$

That is, such that $left( S, preceq right)$ is a lower closure of $left( T, preceq right)$.


Category:Definitions/Set Theory",Definition:Tree (Set Theory)/Subtree,,false,"Let ( T, ≼) be a tree.

A subtree of ( T, ≼) is an ordered subset ( S, ≼) with the property that:
:for every ∀ s ∈ S: ∀ t ∈ T: t ≼ s  t ∈ S

That is, such that ( S, ≼) is a lower closure of ( T, ≼).


Category:Definitions/Set Theory",Subtree
"['Definitions/Order Theory', 'Definitions/Successor Elements']",Definition:Successor,"Let $preceq$ be an ordering.

Let $a, b$ be such that $a preceq b$.


Then $b$ succeeds $a$.

$a$ is then described as being a successor of $b$.",Definition:Succeed,,false,"Let ≼ be an ordering.

Let a, b be such that a ≼ b.


Then b succeeds a.

a is then described as being a successor of b.",Successor
['Definitions/Successor Elements'],Definition:Successor,"Let $left( S, preceq right)$ be an ordered set.

Let $a, b in S$.


Then $a$ is an immediate successor (element) to $b$  if and only if  $b$ is an immediate predecessor (element) to $a$.

That is,  if and only if :
:$(1): quad b prec a$
:$(2): quad nexists c in S: b prec c prec a$

That is, there exists no element strictly between $b$ and $a$ in the ordering $preceq$.

That is:
:$a prec b$ and $left( a ,.,.,   right)b = varnothing$
where $left( a ,.,.,   right)b$ denotes the open interval from $a$ to $b$.


We say that $a$ immediately succeeds $b$.


=== Class Theory ===
 
Let $A$ be an ordered class under an ordering $preccurlyeq$.

Let $a, b in A$.


Then $a$ is an immediate successor (element) to $b$  if and only if  $b$ is an immediate predecessor (element) to $a$.

That is,  if and only if :
:$(1): quad b prec a$
:$(2): quad nexists c in S: b prec c prec a$

We say that $a$ immediately succeeds $b$.",Definition:Immediate Successor Element,,false,"Let ( S, ≼) be an ordered set.

Let a, b ∈ S.


Then a is an immediate successor (element) to b  if and only if  b is an immediate predecessor (element) to a.

That is,  if and only if :
:(1):    b ≺ a
:(2):   ∄ c ∈ S: b ≺ c ≺ a

That is, there exists no element strictly between b and a in the ordering ≼.

That is:
:a ≺ b and ( a  . . )b = ∅
where ( a  . . )b denotes the open interval from a to b.


We say that a immediately succeeds b.


=== Class Theory ===
 
Let A be an ordered class under an ordering ≼.

Let a, b ∈ A.


Then a is an immediate successor (element) to b  if and only if  b is an immediate predecessor (element) to a.

That is,  if and only if :
:(1):    b ≺ a
:(2):   ∄ c ∈ S: b ≺ c ≺ a

We say that a immediately succeeds b.",Successor
['Definitions/Successor Mapping'],Definition:Successor,"Let $V$ be a basic universe.

Let $s: V to V$ denote the successor mapping on $V$.


For $x in V$, the result of applying the successor mapping on $x$ is denoted $x^+$:

:$x^+ := s left(   right)x = x cup leftlbrace x rightrbrace$

$x^+$ is referred to as the successor (set) of $x$.",Definition:Successor Mapping/Successor Set,,false,"Let V be a basic universe.

Let s: V → V denote the successor mapping on V.


For x ∈ V, the result of applying the successor mapping on x is denoted x^+:

:x^+ := s (   )x = x ∪{ x }

x^+ is referred to as the successor (set) of x.",Successor
"['Definitions/Class Theory', 'Definitions/Successor Mapping']",Definition:Successor,"Let $V$ be a basic universe.

The successor mapping $s$ is the mapping on $V$ defined and denoted:
:$forall x in V: s left(   right)x := x cup leftlbrace x rightrbrace$
where $x$ is a set in $V$.


=== Peano Structure ===
Let $left( P, s, 0 right)$ be a Peano structure.


Then the mapping $s: P to P$ is called the successor mapping on $P$.


=== Successor Element ===
Let $left( P, s, 0 right)$ be a Peano structure.

Let mapping $s: P to P$ denote the successor mapping on $P$.


The image element $s left(   right)x$ of an element $x$ is called the successor element or just successor of $x$.

=== Successor Mapping on Natural Numbers ===
Let $mathbb N$ be the set of natural numbers.

Let $s: mathbb N to mathbb N$ be the mapping defined as:

:$s = leftlbrace left( x, y right): x in mathbb N, y = x + 1 rightrbrace$


Considering $mathbb N$ defined as a Peano structure, this is seen to be an instance of a successor mapping.",Definition:Successor Mapping,,false,"Let V be a basic universe.

The successor mapping s is the mapping on V defined and denoted:
:∀ x ∈ V: s (   )x := x ∪{ x }
where x is a set in V.


=== Peano Structure ===
Let ( P, s, 0 ) be a Peano structure.


Then the mapping s: P → P is called the successor mapping on P.


=== Successor Element ===
Let ( P, s, 0 ) be a Peano structure.

Let mapping s: P → P denote the successor mapping on P.


The image element s (   )x of an element x is called the successor element or just successor of x.

=== Successor Mapping on Natural Numbers ===
Let ℕ be the set of natural numbers.

Let s: ℕ→ℕ be the mapping defined as:

:s = {( x, y ): x ∈ℕ, y = x + 1 }


Considering ℕ defined as a Peano structure, this is seen to be an instance of a successor mapping.",Successor
"['Definitions/Ordinals', 'Definitions/Successor Mapping']",Definition:Successor,"A successor ordinal is the successor set of an ordinal:
:$alpha^+ := s left(   right)alpha = alpha cup leftlbrace alpha rightrbrace$",Definition:Successor Ordinal,,false,"A successor ordinal is the successor set of an ordinal:
:α^+ := s (   )α = α∪{α}",Successor
['Definitions/Addition'],Definition:Sum,"Let $a + b$ denote the operation of addition on two objects $a$ and $b$.

Then the result $a + b$ is referred to as the sum of $a$ and $b$.


Note that the nature of $a$ and $b$ has deliberately been left unspecified.

They could be, for example, numbers, matrices or more complex expressions constructed from such elements.",Definition:Addition/Sum,,false,"Let a + b denote the operation of addition on two objects a and b.

Then the result a + b is referred to as the sum of a and b.


Note that the nature of a and b has deliberately been left unspecified.

They could be, for example, numbers, matrices or more complex expressions constructed from such elements.",Sum
"['Definitions/Summations', 'Definitions/Algebra', 'Definitions/Abstract Algebra']",Definition:Sum,"Let $left( S, + right)$ be an algebraic structure where the operation $+$ is an operation derived from, or arising from, the addition operation on the natural numbers.

Let $left( a_1, a_2, ldots, a_n right) in S^n$ be an ordered $n$-tuple in $S$.


=== Definition by Index ===
Let $left( S, + right)$ be an algebraic structure where the operation $+$ is an operation derived from, or arising from, the addition operation on the natural numbers.

Let $left( a_1, a_2, ldots, a_n right) in S^n$ be an ordered $n$-tuple in $S$.


The composite is called the summation of $left( a_1, a_2, ldots, a_n right)$, and is written:

:$ds sum_{j mathop = 1}^n a_j = left( a_1 + a_2 + cdots + a_n right)$


=== Summand ===
Let $left( S, + right)$ be an algebraic structure where the operation $+$ is an operation derived from, or arising from, the addition operation on the natural numbers.

Let $leftlbrace a_1, a_2, ldots, a_n rightrbrace subseteq S$ be a set of elements of $S$.

Let $R left(   right)j$ be a propositional function of $j$.

Let:
:$ds sum_{R left(   right)j} a_j$
be an instance of a summation on $leftlbrace a_1, a_2, ldots, a_n rightrbrace$.


The set of elements $leftlbrace a_j in S: 1 le j le n, R left(   right)j rightrbrace$ is called the summand.

=== Definition by Inequality ===
Let $left( S, + right)$ be an algebraic structure where the operation $+$ is an operation derived from, or arising from, the addition operation on the natural numbers.

Let $left( a_1, a_2, ldots, a_n right) in S^n$ be an ordered $n$-tuple in $S$.


The summation of $left( a_1, a_2, ldots, a_n right)$ can be written:
:$ds sum_{1 mathop le j mathop le n} a_j = left( a_1 + a_2 + cdots + a_n right)$


=== Multiple Indices ===
Let $ds sum_{0 mathop le j mathop le n} a_j$ denote the summation of $left( a_0, a_1, a_2, ldots, a_n right)$.


Summands with multiple indices can be denoted by propositional functions in several variables, for example:

:$ds sum_{0 mathop le i mathop le n} left( sum_{0 mathop le j mathop le n} a_{i j}  right) = sum_{0 mathop le i, j mathop le n} a_{i j}$


:$ds sum_{0 mathop le i mathop le n} left( sum_{0 mathop le j mathop le i} a_{i j}  right) = sum_{0 mathop le j mathop le i mathop le n} a_{i j}$

=== Definition by Propositional Function ===
Let $left( S, + right)$ be an algebraic structure where the operation $+$ is an operation derived from, or arising from, the addition operation on the natural numbers.

Let $left( a_1, a_2, ldots, a_n right) in S^n$ be an ordered $n$-tuple in $S$.


Let $R left(   right)j$ be a propositional function of $j$.

Then we can write the summation as:

:$ds sum_{R left(   right)j} a_j = text{ The sum of all $a_j$ such that $R left(   right)j$ holds}$.


If more than one propositional function is written under the summation sign, they must all hold.


Such an operation on an ordered tuple is known as a summation.


Note that the definition by inequality form $1 le j le n$ is a special case of such a propositional function.

Also note that the definition by index form $ds sum_{j mathop = 1}^n$ is merely another way of writing $ds sum_{1 mathop le j mathop le n}$.

Hence all instances of a summation can be expressed in terms of a propositional function.


=== Iverson's Convention ===
Let $ds sum_{R left(   right)j} a_j$ be the summation over all $a_j$ such that $j$ satisfies $R$.


This can also be expressed:
:$ds sum_{j mathop in mathbb Z} a_j left[ R left(   right)j right]$
where $left[ R left(   right)j right]$ is Iverson's convention.

=== Iverson's Convention ===
Let $ds sum_{R left(   right)j} a_j$ be the summation over all $a_j$ such that $j$ satisfies $R$.


This can also be expressed:
:$ds sum_{j mathop in mathbb Z} a_j left[ R left(   right)j right]$
where $left[ R left(   right)j right]$ is Iverson's convention.

=== Summation over Finite Subset ===
Let $left( G, + right)$ be a commutative monoid.


Let $F subseteq G$ be a finite subset of $G$.


Let $leftlbrace e_1, e_2, ldots, e_n rightrbrace$ be a finite enumeration of $F$.

Let $left( e_1, e_2, ldots, e_n right)$ be the ordered tuple formed from the bijection $e: left[ 1 ,.,.,   right]n to F$.


The summation over $F$, denoted $ds sum_{g mathop in F} g$, is defined as the summation over $left( e_1, e_2, ldots, e_n right)$:
:$ds sum_{g mathop in F} g = sum_{i mathop = 1}^n e_i$

=== Summation over Finite Index ===
Let $left( G, + right)$ be a commutative monoid.


Let $leftlangle g_i rightrangle_{i mathop in I}$ be an indexed subset of $G$ where the indexing set $I$ is finite.


Let $leftlbrace e_1, e_2, ldots, e_n rightrbrace$ be a finite enumeration of $I$.

Let $left( g_{e_1}, g_{e_2}, ldots, g_{e_n}  right)$ be the ordered tuple formed from the composite mapping $g circ e: left[ 1 ,.,.,   right]n to G$.


The summation over $I$, denoted $ds sum_{i mathop in I} g_i$, is defined as the summation over $left( g_{e_1}, g_{e_2}, ldots, g_{e_n}  right)$:
:$ds sum_{i mathop in I} g_i = sum_{k mathop = 1}^n g_{e_k}$",Definition:Summation,,false,"Let ( S, + ) be an algebraic structure where the operation + is an operation derived from, or arising from, the addition operation on the natural numbers.

Let ( a_1, a_2, …, a_n ) ∈ S^n be an ordered n-tuple in S.


=== Definition by Index ===
Let ( S, + ) be an algebraic structure where the operation + is an operation derived from, or arising from, the addition operation on the natural numbers.

Let ( a_1, a_2, …, a_n ) ∈ S^n be an ordered n-tuple in S.


The composite is called the summation of ( a_1, a_2, …, a_n ), and is written:

:∑_j  = 1^n a_j = ( a_1 + a_2 + ⋯ + a_n )


=== Summand ===
Let ( S, + ) be an algebraic structure where the operation + is an operation derived from, or arising from, the addition operation on the natural numbers.

Let { a_1, a_2, …, a_n }⊆ S be a set of elements of S.

Let R (   )j be a propositional function of j.

Let:
:∑_R (   )j a_j
be an instance of a summation on { a_1, a_2, …, a_n }.


The set of elements { a_j ∈ S: 1 ≤ j ≤ n, R (   )j } is called the summand.

=== Definition by Inequality ===
Let ( S, + ) be an algebraic structure where the operation + is an operation derived from, or arising from, the addition operation on the natural numbers.

Let ( a_1, a_2, …, a_n ) ∈ S^n be an ordered n-tuple in S.


The summation of ( a_1, a_2, …, a_n ) can be written:
:∑_1 ≤ j ≤ n a_j = ( a_1 + a_2 + ⋯ + a_n )


=== Multiple Indices ===
Let ∑_0 ≤ j ≤ n a_j denote the summation of ( a_0, a_1, a_2, …, a_n ).


Summands with multiple indices can be denoted by propositional functions in several variables, for example:

:∑_0 ≤ i ≤ n( ∑_0 ≤ j ≤ n a_i j) = ∑_0 ≤ i, j ≤ n a_i j


:∑_0 ≤ i ≤ n( ∑_0 ≤ j ≤ i a_i j) = ∑_0 ≤ j ≤ i ≤ n a_i j

=== Definition by Propositional Function ===
Let ( S, + ) be an algebraic structure where the operation + is an operation derived from, or arising from, the addition operation on the natural numbers.

Let ( a_1, a_2, …, a_n ) ∈ S^n be an ordered n-tuple in S.


Let R (   )j be a propositional function of j.

Then we can write the summation as:

:∑_R (   )j a_j =  The sum of all a_j such that R (   )j holds.


If more than one propositional function is written under the summation sign, they must all hold.


Such an operation on an ordered tuple is known as a summation.


Note that the definition by inequality form 1 ≤ j ≤ n is a special case of such a propositional function.

Also note that the definition by index form ∑_j  = 1^n is merely another way of writing ∑_1 ≤ j ≤ n.

Hence all instances of a summation can be expressed in terms of a propositional function.


=== Iverson's Convention ===
Let ∑_R (   )j a_j be the summation over all a_j such that j satisfies R.


This can also be expressed:
:∑_j ∈ℤ a_j [ R (   )j ]
where [ R (   )j ] is Iverson's convention.

=== Iverson's Convention ===
Let ∑_R (   )j a_j be the summation over all a_j such that j satisfies R.


This can also be expressed:
:∑_j ∈ℤ a_j [ R (   )j ]
where [ R (   )j ] is Iverson's convention.

=== Summation over Finite Subset ===
Let ( G, + ) be a commutative monoid.


Let F ⊆ G be a finite subset of G.


Let { e_1, e_2, …, e_n } be a finite enumeration of F.

Let ( e_1, e_2, …, e_n ) be the ordered tuple formed from the bijection e: [ 1  . . ]n → F.


The summation over F, denoted ∑_g ∈ F g, is defined as the summation over ( e_1, e_2, …, e_n ):
:∑_g ∈ F g = ∑_i  = 1^n e_i

=== Summation over Finite Index ===
Let ( G, + ) be a commutative monoid.


Let ⟨ g_i ⟩_i ∈ I be an indexed subset of G where the indexing set I is finite.


Let { e_1, e_2, …, e_n } be a finite enumeration of I.

Let ( g_e_1, g_e_2, …, g_e_n) be the ordered tuple formed from the composite mapping g ∘ e: [ 1  . . ]n → G.


The summation over I, denoted ∑_i ∈ I g_i, is defined as the summation over ( g_e_1, g_e_2, …, g_e_n):
:∑_i ∈ I g_i = ∑_k  = 1^n g_e_k",Sum
"['Definitions/Real Analysis', 'Definitions/Abstract Algebra']",Definition:Support,"=== Real-Valued Function on an Abstract Set ===
Let $S$ be a set.

Let $f: S to mathbb R$ be a real-valued function.


The support of $f$ is the set of elements $x$ of $S$ whose values under $f$ are non-zero.

That is:
:$mathrm {supp} left(   right)f := leftlbrace x in S: f left(   right)x ne 0 rightrbrace$


That is, the support of a function whose codomain is the set of real numbers is generally defined to be the subset of its domain which maps to anywhere that is not $0$.

Category:Definitions/Real Analysis

=== General Real-Valued Function in $mathbb R^n$ ===

=== General Algebraic Structure ===

Let $left( A, * right)$ be an algebraic structure with an identity element $e$.

Let $S$ be a set.

Let $f: S to A$ be a mapping.


The support of $f$ is the set:
:$mathrm {supp} left(   right)f = leftlbrace s in S : f left(   right)s ne e rightrbrace$
 ",Definition:Support of Mapping to Algebraic Structure,,false,"=== Real-Valued Function on an Abstract Set ===
Let S be a set.

Let f: S →ℝ be a real-valued function.


The support of f is the set of elements x of S whose values under f are non-zero.

That is:
:supp(   )f := { x ∈ S: f (   )x  0 }


That is, the support of a function whose codomain is the set of real numbers is generally defined to be the subset of its domain which maps to anywhere that is not 0.

Category:Definitions/Real Analysis

=== General Real-Valued Function in ℝ^n ===

=== General Algebraic Structure ===

Let ( A, * ) be an algebraic structure with an identity element e.

Let S be a set.

Let f: S → A be a mapping.


The support of f is the set:
:supp(   )f = { s ∈ S : f (   )s  e }
 ",Support
['Definitions/Real Analysis'],Definition:Support,"Let $S$ be a set.

Let $f: S to mathbb R$ be a real-valued function.


The support of $f$ is the set of elements $x$ of $S$ whose values under $f$ are non-zero.

That is:
:$mathrm {supp} left(   right)f := leftlbrace x in S: f left(   right)x ne 0 rightrbrace$


That is, the support of a function whose codomain is the set of real numbers is generally defined to be the subset of its domain which maps to anywhere that is not $0$.

Category:Definitions/Real Analysis",Definition:Support of Mapping to Algebraic Structure/Real-Valued Function,,false,"Let S be a set.

Let f: S →ℝ be a real-valued function.


The support of f is the set of elements x of S whose values under f are non-zero.

That is:
:supp(   )f := { x ∈ S: f (   )x  0 }


That is, the support of a function whose codomain is the set of real numbers is generally defined to be the subset of its domain which maps to anywhere that is not 0.

Category:Definitions/Real Analysis",Support
['Definitions/Module Theory'],Definition:Support,"Let $A$ be a commutative ring with unity.

Let $M$ be a unitary $A$-module.


The support $mathrm {supp} left(   right)M$ of $M$ is the set of prime ideals $P$ of $A$ such that the localization of $M$ at $P$ is nonzero:
:$mathrm {supp} left(   right)M = leftlbrace P in mathrm {Spec} left( A right) : M_P ne 0 rightrbrace$

where $mathrm {Spec} left( A right)$ is the spectrum of $A$.

Category:Definitions/Module Theory",Definition:Support of Module,,false,"Let A be a commutative ring with unity.

Let M be a unitary A-module.


The support supp(   )M of M is the set of prime ideals P of A such that the localization of M at P is nonzero:
:supp(   )M = { P ∈Spec( A ) : M_P  0 }

where Spec( A ) is the spectrum of A.

Category:Definitions/Module Theory",Support
"['Definitions/Abstract Algebra', 'Definitions/Direct Products']",Definition:Support,"Let $leftlangle left( S_i, circ_i right)  rightrangle_{i mathop in I}$ be a family of algebraic structures with identity.

Let $ds S = prod_{i mathop in I} S_i$ be their direct product.

Let $e_i$ be an identity of $S_i$ for all $i in I$.

Let $m = leftlangle m_i rightrangle_{i mathop in I} in S$.


The support of $m$ is defined as:
:$mathrm {supp} leftlbrace i in I: m_i ne e_i rightrbrace$

 


=== Finite Support ===

The element is said to have finite support  if and only if  its support is a finite set.",Definition:Support of Element of Direct Product,,false,"Let ⟨( S_i, ∘_i )  ⟩_i ∈ I be a family of algebraic structures with identity.

Let S = ∏_i ∈ I S_i be their direct product.

Let e_i be an identity of S_i for all i ∈ I.

Let m = ⟨ m_i ⟩_i ∈ I∈ S.


The support of m is defined as:
:supp{ i ∈ I: m_i  e_i }

 


=== Finite Support ===

The element is said to have finite support  if and only if  its support is a finite set.",Support
['Definitions/Topology'],Definition:Support,"=== Continuous Real-Valued Function in $mathbb R^n$ ===
Let $f: mathbb R^n to mathbb R$ be a continuous real-valued function.

The support of $f$ is the closure of the set of elements $x$ of $mathbb R^n$ whose values under $f$ are non-zero.

That is:
:$mathrm {supp} left(   right)f = mathrm {cl} leftlbrace x in mathbb R^n: f left(   right)x ne 0 rightrbrace$


Category:Definitions/Real Analysis

=== General topological group ===

Let $X$ be a topological space.

Let $G$ be a topological group with identity $e$.

Let $f : X to G$ be a continuous mapping.


The support of $f$ is the closure of the set of elements of $X$ that do not map to $e$ under $f$:
:$mathrm {supp} left(   right)f = mathrm {cl} leftlbrace x in X: f left(   right)x ne e rightrbrace$",Definition:Support of Continuous Mapping,,false,"=== Continuous Real-Valued Function in ℝ^n ===
Let f: ℝ^n →ℝ be a continuous real-valued function.

The support of f is the closure of the set of elements x of ℝ^n whose values under f are non-zero.

That is:
:supp(   )f = cl{ x ∈ℝ^n: f (   )x  0 }


Category:Definitions/Real Analysis

=== General topological group ===

Let X be a topological space.

Let G be a topological group with identity e.

Let f : X → G be a continuous mapping.


The support of f is the closure of the set of elements of X that do not map to e under f:
:supp(   )f = cl{ x ∈ X: f (   )x  e }",Support
['Definitions/Real Analysis'],Definition:Support,"Let $f: mathbb R^n to mathbb R$ be a continuous real-valued function.

The support of $f$ is the closure of the set of elements $x$ of $mathbb R^n$ whose values under $f$ are non-zero.

That is:
:$mathrm {supp} left(   right)f = mathrm {cl} leftlbrace x in mathbb R^n: f left(   right)x ne 0 rightrbrace$


Category:Definitions/Real Analysis",Definition:Support of Continuous Mapping/Real-Valued,,false,"Let f: ℝ^n →ℝ be a continuous real-valued function.

The support of f is the closure of the set of elements x of ℝ^n whose values under f are non-zero.

That is:
:supp(   )f = cl{ x ∈ℝ^n: f (   )x  0 }


Category:Definitions/Real Analysis",Support
['Definitions/Real Analysis'],Definition:Support,"Let $Omega subseteq mathbb R^n$ be an open set.

 

Let $mathcal D left(   right)Omega$ be the space of continuous functions compactly supported in $Omega$.

Let $mathcal D' left(   right)Omega$ be the distribution space.

Let $T in mathcal D' left(   right)Omega$ be a distribution.


The support $mathrm {supp}  left(   right)T subseteq Omega$ of $T$ is defined by:
:$ds x notin mathrm {supp}  left(   right)T$  if and only if :
::there exists an open neighborhood $U$ of $x$ such that:
:::for all $phi in mathcal D left(   right)Omega$ such that $mathrm {supp}  left(   right)phi subseteq U$:
::::$T left(   right)phi = 0$


Category:Definitions/Real Analysis",Definition:Support of Distribution,,false,"Let Ω⊆ℝ^n be an open set.

 

Let 𝒟(   )Ω be the space of continuous functions compactly supported in Ω.

Let 𝒟' (   )Ω be the distribution space.

Let T ∈𝒟' (   )Ω be a distribution.


The support supp(   )T ⊆Ω of T is defined by:
:x ∉supp(   )T  if and only if :
::there exists an open neighborhood U of x such that:
:::for all ϕ∈𝒟(   )Ω such that supp(   )ϕ⊆ U:
::::T (   )ϕ = 0


Category:Definitions/Real Analysis",Support
"['Definitions/Mapping Theory', 'Definitions/Permutation Theory']",Definition:Support,"Let $S$ be a set.

Let $f$ be a permutation on $S$.


The support of $f$ is the subset of moved elements:
:$mathrm {supp} left(   right)f = leftlbrace x in S: f left(   right)x ne x rightrbrace$


 ",Definition:Support of Permutation,,false,"Let S be a set.

Let f be a permutation on S.


The support of f is the subset of moved elements:
:supp(   )f = { x ∈ S: f (   )x  x }


 ",Support
['Definitions/Characteristic Functions of Sets'],Definition:Support,"Let $S$ be a set

Let $E subseteq S$ be a subset.

Let $chi_E: S to leftlbrace 0, 1 rightrbrace$ be the characteristic function of $E$.


The support of $chi_E$, denoted $mathrm {supp} left(   right){chi_E}$, is the set $E$.

That is:

:$mathrm {supp} left(   right){chi_E} = leftlbrace x in S: chi_E left(   right)x = 1 rightrbrace$",Definition:Characteristic Function (Set Theory)/Set/Support,,false,"Let S be a set

Let E ⊆ S be a subset.

Let χ_E: S →{ 0, 1 } be the characteristic function of E.


The support of χ_E, denoted supp(   )χ_E, is the set E.

That is:

:supp(   )χ_E = { x ∈ S: χ_E (   )x = 1 }",Support
['Definitions/Suprema'],Definition:Supremum,"Let $left( S, preccurlyeq right)$ be an ordered set.

Let $T subseteq S$.


An element $c in S$ is the supremum of $T$ in $S$  if and only if :

:$(1): quad c$ is an upper bound of $T$ in $S$
:$(2): quad c preccurlyeq d$ for all upper bounds $d$ of $T$ in $S$.


If there exists a supremum of $T$ (in $S$), we say that:
:$T$ admits a supremum (in $S$) or
:$T$ has a supremum (in $S$).


=== Finite Supremum ===
Let $left( S, preccurlyeq right)$ be an ordered set.

Let $T subseteq S$ admit a supremum $sup T$.


If $T$ is finite, $sup T$ is called a finite supremum.

=== Subset of Real Numbers ===

The concept is usually encountered where $left( S, preccurlyeq right)$ is the set of real numbers under the usual ordering $left( mathbb R, le right)$:

Let $T subseteq mathbb R$ be a subset of the real numbers.


A real number $c in mathbb R$ is the supremum of $T$ in $mathbb R$  if and only if :

:$(1): quad c$ is an upper bound of $T$ in $mathbb R$
:$(2): quad c le d$ for all upper bounds $d$ of $T$ in $mathbb R$.


If there exists a supremum of $T$ (in $mathbb R$), we say that:
:$T$ admits a supremum (in $mathbb R$) or
:$T$ has a supremum (in $mathbb R$).


The supremum of $T$ is denoted $sup T$ or $sup left(   right)T$.


=== Definition by Propositional Function ===
Let $leftlangle a_j rightrangle_{j mathop in I}$ be a family of elements of the real numbers $mathbb R$ indexed by $I$.

Let $R left(   right)j$ be a propositional function of $j in I$.


Then we can define the supremum of $leftlangle a_j rightrangle_{j mathop in I}$ as:

:$ds sup_{R left(   right)j} a_j := text { the supremum of all $a_j$ such that $R left(   right)j$ holds}$


If more than one propositional function is written under the supremum sign, they must all hold.


=== Finite Range ===
Let $leftlangle a_j rightrangle_{j mathop in I}$ be a family of elements of the real numbers $mathbb R$ indexed by $I$.

Let $R left(   right)j$ be a propositional function of $j in I$.


Let the fiber of truth of $R left(   right)j$ be finite.

Then the supremum of $leftlangle a_j rightrangle_{j mathop in I}$ can be expressed as:

:$ds max_{R left(   right)j} a_j = text { the maxmum of all $a_j$ such that $R left(   right)j$ holds}$

and can be referred to as the maximum of $leftlangle a_j rightrangle_{j mathop in I}$.


If more than one propositional function is written under the supremum sign, they must all hold.

=== Vacuous Supremum ===
Take the indexed supremum:
:$ds sup _{Phi left(   right)j} a_j$
where $Phi left(   right)j$ is a propositional function of $j$.

Suppose that there are no values of $j$ for which $Phi left(   right)j$ is true.

Then $ds sup_{Phi left(   right)j} a_j$ is defined as being $-infty$.

This supremum is called a vacuous supremum.


This is because:
:$forall a in mathbb R: sup leftlbrace a, -infty rightrbrace = a$

Hence for all $j$ for which $Phi left(   right)j$ is false, the supremum is unaffected.


In this context $-infty$ is considered as minus infinity, the hypothetical quantity that has the property:

:$forall n in mathbb Z: -infty < n$

The supremum of $T$ is denoted $sup T$ or $sup left(   right)T$.",Definition:Supremum of Set,,false,"Let ( S, ≼) be an ordered set.

Let T ⊆ S.


An element c ∈ S is the supremum of T in S  if and only if :

:(1):    c is an upper bound of T in S
:(2):    c ≼ d for all upper bounds d of T in S.


If there exists a supremum of T (in S), we say that:
:T admits a supremum (in S) or
:T has a supremum (in S).


=== Finite Supremum ===
Let ( S, ≼) be an ordered set.

Let T ⊆ S admit a supremum sup T.


If T is finite, sup T is called a finite supremum.

=== Subset of Real Numbers ===

The concept is usually encountered where ( S, ≼) is the set of real numbers under the usual ordering ( ℝ, ≤):

Let T ⊆ℝ be a subset of the real numbers.


A real number c ∈ℝ is the supremum of T in ℝ  if and only if :

:(1):    c is an upper bound of T in ℝ
:(2):    c ≤ d for all upper bounds d of T in ℝ.


If there exists a supremum of T (in ℝ), we say that:
:T admits a supremum (in ℝ) or
:T has a supremum (in ℝ).


The supremum of T is denoted sup T or sup(   )T.


=== Definition by Propositional Function ===
Let ⟨ a_j ⟩_j ∈ I be a family of elements of the real numbers ℝ indexed by I.

Let R (   )j be a propositional function of j ∈ I.


Then we can define the supremum of ⟨ a_j ⟩_j ∈ I as:

:sup_R (   )j a_j :=  the supremum of all a_j such that R (   )j holds


If more than one propositional function is written under the supremum sign, they must all hold.


=== Finite Range ===
Let ⟨ a_j ⟩_j ∈ I be a family of elements of the real numbers ℝ indexed by I.

Let R (   )j be a propositional function of j ∈ I.


Let the fiber of truth of R (   )j be finite.

Then the supremum of ⟨ a_j ⟩_j ∈ I can be expressed as:

:max_R (   )j a_j =  the maxmum of all a_j such that R (   )j holds

and can be referred to as the maximum of ⟨ a_j ⟩_j ∈ I.


If more than one propositional function is written under the supremum sign, they must all hold.

=== Vacuous Supremum ===
Take the indexed supremum:
:sup _Φ(   )j a_j
where Φ(   )j is a propositional function of j.

Suppose that there are no values of j for which Φ(   )j is true.

Then sup_Φ(   )j a_j is defined as being -∞.

This supremum is called a vacuous supremum.


This is because:
:∀ a ∈ℝ: sup{ a, -∞} = a

Hence for all j for which Φ(   )j is false, the supremum is unaffected.


In this context -∞ is considered as minus infinity, the hypothetical quantity that has the property:

:∀ n ∈ℤ: -∞ < n

The supremum of T is denoted sup T or sup(   )T.",Supremum
['Definitions/Suprema'],Definition:Supremum,"Let $T subseteq mathbb R$ be a subset of the real numbers.


A real number $c in mathbb R$ is the supremum of $T$ in $mathbb R$  if and only if :

:$(1): quad c$ is an upper bound of $T$ in $mathbb R$
:$(2): quad c le d$ for all upper bounds $d$ of $T$ in $mathbb R$.


If there exists a supremum of $T$ (in $mathbb R$), we say that:
:$T$ admits a supremum (in $mathbb R$) or
:$T$ has a supremum (in $mathbb R$).


The supremum of $T$ is denoted $sup T$ or $sup left(   right)T$.


=== Definition by Propositional Function ===
Let $leftlangle a_j rightrangle_{j mathop in I}$ be a family of elements of the real numbers $mathbb R$ indexed by $I$.

Let $R left(   right)j$ be a propositional function of $j in I$.


Then we can define the supremum of $leftlangle a_j rightrangle_{j mathop in I}$ as:

:$ds sup_{R left(   right)j} a_j := text { the supremum of all $a_j$ such that $R left(   right)j$ holds}$


If more than one propositional function is written under the supremum sign, they must all hold.


=== Finite Range ===
Let $leftlangle a_j rightrangle_{j mathop in I}$ be a family of elements of the real numbers $mathbb R$ indexed by $I$.

Let $R left(   right)j$ be a propositional function of $j in I$.


Let the fiber of truth of $R left(   right)j$ be finite.

Then the supremum of $leftlangle a_j rightrangle_{j mathop in I}$ can be expressed as:

:$ds max_{R left(   right)j} a_j = text { the maxmum of all $a_j$ such that $R left(   right)j$ holds}$

and can be referred to as the maximum of $leftlangle a_j rightrangle_{j mathop in I}$.


If more than one propositional function is written under the supremum sign, they must all hold.

=== Vacuous Supremum ===
Take the indexed supremum:
:$ds sup _{Phi left(   right)j} a_j$
where $Phi left(   right)j$ is a propositional function of $j$.

Suppose that there are no values of $j$ for which $Phi left(   right)j$ is true.

Then $ds sup_{Phi left(   right)j} a_j$ is defined as being $-infty$.

This supremum is called a vacuous supremum.


This is because:
:$forall a in mathbb R: sup leftlbrace a, -infty rightrbrace = a$

Hence for all $j$ for which $Phi left(   right)j$ is false, the supremum is unaffected.


In this context $-infty$ is considered as minus infinity, the hypothetical quantity that has the property:

:$forall n in mathbb Z: -infty < n$",Definition:Supremum of Set/Real Numbers,,false,"Let T ⊆ℝ be a subset of the real numbers.


A real number c ∈ℝ is the supremum of T in ℝ  if and only if :

:(1):    c is an upper bound of T in ℝ
:(2):    c ≤ d for all upper bounds d of T in ℝ.


If there exists a supremum of T (in ℝ), we say that:
:T admits a supremum (in ℝ) or
:T has a supremum (in ℝ).


The supremum of T is denoted sup T or sup(   )T.


=== Definition by Propositional Function ===
Let ⟨ a_j ⟩_j ∈ I be a family of elements of the real numbers ℝ indexed by I.

Let R (   )j be a propositional function of j ∈ I.


Then we can define the supremum of ⟨ a_j ⟩_j ∈ I as:

:sup_R (   )j a_j :=  the supremum of all a_j such that R (   )j holds


If more than one propositional function is written under the supremum sign, they must all hold.


=== Finite Range ===
Let ⟨ a_j ⟩_j ∈ I be a family of elements of the real numbers ℝ indexed by I.

Let R (   )j be a propositional function of j ∈ I.


Let the fiber of truth of R (   )j be finite.

Then the supremum of ⟨ a_j ⟩_j ∈ I can be expressed as:

:max_R (   )j a_j =  the maxmum of all a_j such that R (   )j holds

and can be referred to as the maximum of ⟨ a_j ⟩_j ∈ I.


If more than one propositional function is written under the supremum sign, they must all hold.

=== Vacuous Supremum ===
Take the indexed supremum:
:sup _Φ(   )j a_j
where Φ(   )j is a propositional function of j.

Suppose that there are no values of j for which Φ(   )j is true.

Then sup_Φ(   )j a_j is defined as being -∞.

This supremum is called a vacuous supremum.


This is because:
:∀ a ∈ℝ: sup{ a, -∞} = a

Hence for all j for which Φ(   )j is false, the supremum is unaffected.


In this context -∞ is considered as minus infinity, the hypothetical quantity that has the property:

:∀ n ∈ℤ: -∞ < n",Supremum
['Definitions/Mapping Theory'],Definition:Supremum,"Let $S$ be a set.

Let $left( T, preceq right)$ be an ordered set.

Let $f: S to T$ be a mapping from $S$ to $T$.

Let $f left[ S right]$, the image of $f$, admit a supremum.


Then the supremum of $f$ (on $S$) is defined by:
:$ds sup_{x mathop in S} f left(   right)x = sup f left[ S right]$


=== Real-Valued Function ===
Let $f: S to mathbb R$ be a real-valued function.

Let $f$ be bounded above on $S$.


=== Definition 1 ===
Let $f: S to mathbb R$ be a real-valued function.

Let $f$ be bounded above on $S$.


The supremum of $f$ on $S$ is defined by:
:$ds sup_{x mathop in S} f left(   right)x := sup f left[ S right]$
where
:$sup f left[ S right]$ is the supremum in $mathbb R$ of the image of $S$ under $f$.

=== Definition 2 ===
Let $f: S to mathbb R$ be a real-valued function.

Let $f$ be bounded above on $S$.


The supremum of $f$ on $S$ is defined as $ds sup_{x mathop in S} f left(   right)x := K in mathbb R$ such that:

:$(1): quad forall x in S: f left(   right)x le K$
:$(2): quad exists x in S: forall epsilon in mathbb R_{>0}: f left(   right)x > K - epsilon$",Definition:Supremum of Mapping,,false,"Let S be a set.

Let ( T, ≼) be an ordered set.

Let f: S → T be a mapping from S to T.

Let f [ S ], the image of f, admit a supremum.


Then the supremum of f (on S) is defined by:
:sup_x ∈ S f (   )x = sup f [ S ]


=== Real-Valued Function ===
Let f: S →ℝ be a real-valued function.

Let f be bounded above on S.


=== Definition 1 ===
Let f: S →ℝ be a real-valued function.

Let f be bounded above on S.


The supremum of f on S is defined by:
:sup_x ∈ S f (   )x := sup f [ S ]
where
:sup f [ S ] is the supremum in ℝ of the image of S under f.

=== Definition 2 ===
Let f: S →ℝ be a real-valued function.

Let f be bounded above on S.


The supremum of f on S is defined as sup_x ∈ S f (   )x := K ∈ℝ such that:

:(1):   ∀ x ∈ S: f (   )x ≤ K
:(2):   ∃ x ∈ S: ∀ϵ∈ℝ_>0: f (   )x > K - ϵ",Supremum
['Definitions/Suprema'],Definition:Supremum,"Let $f: S to mathbb R$ be a real-valued function.

Let $f$ be bounded above on $S$.


=== Definition 1 ===
Let $f: S to mathbb R$ be a real-valued function.

Let $f$ be bounded above on $S$.


The supremum of $f$ on $S$ is defined by:
:$ds sup_{x mathop in S} f left(   right)x := sup f left[ S right]$
where
:$sup f left[ S right]$ is the supremum in $mathbb R$ of the image of $S$ under $f$.

=== Definition 2 ===
Let $f: S to mathbb R$ be a real-valued function.

Let $f$ be bounded above on $S$.


The supremum of $f$ on $S$ is defined as $ds sup_{x mathop in S} f left(   right)x := K in mathbb R$ such that:

:$(1): quad forall x in S: f left(   right)x le K$
:$(2): quad exists x in S: forall epsilon in mathbb R_{>0}: f left(   right)x > K - epsilon$",Definition:Supremum of Mapping/Real-Valued Function,,false,"Let f: S →ℝ be a real-valued function.

Let f be bounded above on S.


=== Definition 1 ===
Let f: S →ℝ be a real-valued function.

Let f be bounded above on S.


The supremum of f on S is defined by:
:sup_x ∈ S f (   )x := sup f [ S ]
where
:sup f [ S ] is the supremum in ℝ of the image of S under f.

=== Definition 2 ===
Let f: S →ℝ be a real-valued function.

Let f be bounded above on S.


The supremum of f on S is defined as sup_x ∈ S f (   )x := K ∈ℝ such that:

:(1):   ∀ x ∈ S: f (   )x ≤ K
:(2):   ∃ x ∈ S: ∀ϵ∈ℝ_>0: f (   )x > K - ϵ",Supremum
"['Definitions/Sequences', 'Definitions/Suprema']",Definition:Supremum,"A special case of a supremum of a mapping is a supremum of a sequence, where the domain of the mapping is $mathbb N$.

Let $left( T, preceq right)$ be an ordered set.

Let $leftlangle x_n rightrangle$ be a sequence in $T$.


Let $leftlbrace x_n: n in mathbb N rightrbrace$ admit a supremum.


Then the supremum of $leftlangle x_n rightrangle$) is defined as:
:$ds sup left(   right){leftlangle x_n rightrangle } = sup left(   right){leftlbrace x_n: n in mathbb N rightrbrace }$",Definition:Supremum of Sequence,,false,"A special case of a supremum of a mapping is a supremum of a sequence, where the domain of the mapping is ℕ.

Let ( T, ≼) be an ordered set.

Let ⟨ x_n ⟩ be a sequence in T.


Let { x_n: n ∈ℕ} admit a supremum.


Then the supremum of ⟨ x_n ⟩) is defined as:
:sup(   )⟨ x_n ⟩ = sup(   ){ x_n: n ∈ℕ}",Supremum
"['Definitions/Sequences', 'Definitions/Suprema']",Definition:Supremum,"Let $leftlangle x_n rightrangle$ be a real sequence.


Let $leftlbrace x_n: n in mathbb N rightrbrace$ admit a supremum.


Then the supremum of $leftlangle x_n rightrangle$) is defined as:
:$ds sup left(   right){leftlangle x_n rightrangle } = sup left(   right){leftlbrace x_n: n in mathbb N rightrbrace }$",Definition:Supremum of Real Sequence,,false,"Let ⟨ x_n ⟩ be a real sequence.


Let { x_n: n ∈ℕ} admit a supremum.


Then the supremum of ⟨ x_n ⟩) is defined as:
:sup(   )⟨ x_n ⟩ = sup(   ){ x_n: n ∈ℕ}",Supremum
"['Definitions/Relation Theory', 'Definitions/Symmetric Relations']",Definition:Symmetry,"Let $mathcal R subseteq S times S$ be a relation in $S$.


=== Symmetric ===
Let $mathcal R subseteq S times S$ be a relation in $S$.


=== Definition 1 ===
Let $mathcal R subseteq S times S$ be a relation in $S$.

$mathcal R$ is symmetric  if and only if :

:$left( x, y right) in mathcal R implies left( y, x right) in mathcal R$

=== Definition 2 ===
Let $mathcal R subseteq S times S$ be a relation in $S$.


$mathcal R$ is symmetric  if and only if  it equals its inverse:
:$mathcal R^{-1} = mathcal R$

=== Definition 3 ===
Let $mathcal R subseteq S times S$ be a relation in $S$.

$mathcal R$ is symmetric  if and only if  it is a subset of its inverse:
:$mathcal R subseteq mathcal R^{-1}$

=== Class Theory ===
 
Let $V$ be a basic universe.

Let $mathcal R subseteq V times V$ be a relation in $V$.


$mathcal R$ is symmetric  if and only if :

:$left( x, y right) in mathcal R implies left( y, x right) in mathcal R$

=== Asymmetric ===
Let $mathcal R subseteq S times S$ be a relation in $S$.


=== Definition 1 ===
Let $mathcal R subseteq S times S$ be a relation in $S$.


$mathcal R$ is asymmetric  if and only if :

:$left( x, y right) in mathcal R implies left( y, x right) notin mathcal R$

=== Definition 2 ===
Let $mathcal R subseteq S times S$ be a relation in $S$.


$mathcal R$ is asymmetric  if and only if  it and its inverse are disjoint:
:$mathcal R cap mathcal R^{-1} = varnothing$

=== Antisymmetric ===
Let $mathcal R subseteq S times S$ be a relation in $S$.


=== Definition 1 ===
Let $S$ be a set.

Let $mathcal R subseteq S times S$ be a relation in $S$.

$mathcal R$ is antisymmetric  if and only if :
:$left( x, y right) in mathcal R land left( y, x right) in mathcal R implies x = y$
that is:
:$leftlbrace left( x, y right), left( y, x right)  rightrbrace subseteq mathcal R implies x = y$

=== Definition 2 ===
Let $mathcal R subseteq S times S$ be a relation in $S$.

$mathcal R$ is antisymmetric  if and only if :
:$left( x, y right) in mathcal R land x ne y implies left( y, x right) notin mathcal R$

=== Class Theory ===
 
Let $V$ be a basic universe.

Let $mathcal R subseteq V times V$ be a relation in $V$.


=== Definition 1 ===
Let $V$ be a basic universe.

Let $mathcal R subseteq V times V$ be a relation in $V$.


$mathcal R$ is antisymmetric  if and only if :
:$left( x, y right) in mathcal R land left( y, x right) in mathcal R implies x = y$
that is:
:$leftlbrace left( x, y right), left( y, x right)  rightrbrace subseteq mathcal R implies x = y$

=== Definition 2 ===
Let $V$ be a basic universe.

Let $mathcal R subseteq V times V$ be a relation in $V$.


$mathcal R$ is antisymmetric  if and only if :
:$left( x, y right) in mathcal R land x ne y implies left( y, x right) notin mathcal R$

=== Non-symmetric ===
Let $mathcal R subseteq S times S$ be a relation in $S$.

$mathcal R$ is non-symmetric  if and only if  it is neither symmetric nor asymmetric.",Definition:Symmetry (Relation),,false,"Let ℛ⊆ S × S be a relation in S.


=== Symmetric ===
Let ℛ⊆ S × S be a relation in S.


=== Definition 1 ===
Let ℛ⊆ S × S be a relation in S.

ℛ is symmetric  if and only if :

:( x, y ) ∈ℛ( y, x ) ∈ℛ

=== Definition 2 ===
Let ℛ⊆ S × S be a relation in S.


ℛ is symmetric  if and only if  it equals its inverse:
:ℛ^-1 = ℛ

=== Definition 3 ===
Let ℛ⊆ S × S be a relation in S.

ℛ is symmetric  if and only if  it is a subset of its inverse:
:ℛ⊆ℛ^-1

=== Class Theory ===
 
Let V be a basic universe.

Let ℛ⊆ V × V be a relation in V.


ℛ is symmetric  if and only if :

:( x, y ) ∈ℛ( y, x ) ∈ℛ

=== Asymmetric ===
Let ℛ⊆ S × S be a relation in S.


=== Definition 1 ===
Let ℛ⊆ S × S be a relation in S.


ℛ is asymmetric  if and only if :

:( x, y ) ∈ℛ( y, x ) ∉ℛ

=== Definition 2 ===
Let ℛ⊆ S × S be a relation in S.


ℛ is asymmetric  if and only if  it and its inverse are disjoint:
:ℛ∩ℛ^-1 = ∅

=== Antisymmetric ===
Let ℛ⊆ S × S be a relation in S.


=== Definition 1 ===
Let S be a set.

Let ℛ⊆ S × S be a relation in S.

ℛ is antisymmetric  if and only if :
:( x, y ) ∈ℛ( y, x ) ∈ℛ x = y
that is:
:{( x, y ), ( y, x )  }⊆ℛ x = y

=== Definition 2 ===
Let ℛ⊆ S × S be a relation in S.

ℛ is antisymmetric  if and only if :
:( x, y ) ∈ℛ x  y ( y, x ) ∉ℛ

=== Class Theory ===
 
Let V be a basic universe.

Let ℛ⊆ V × V be a relation in V.


=== Definition 1 ===
Let V be a basic universe.

Let ℛ⊆ V × V be a relation in V.


ℛ is antisymmetric  if and only if :
:( x, y ) ∈ℛ( y, x ) ∈ℛ x = y
that is:
:{( x, y ), ( y, x )  }⊆ℛ x = y

=== Definition 2 ===
Let V be a basic universe.

Let ℛ⊆ V × V be a relation in V.


ℛ is antisymmetric  if and only if :
:( x, y ) ∈ℛ x  y ( y, x ) ∉ℛ

=== Non-symmetric ===
Let ℛ⊆ S × S be a relation in S.

ℛ is non-symmetric  if and only if  it is neither symmetric nor asymmetric.",Symmetry
"['Definitions/Geometry', 'Definitions/Mapping Theory', 'Definitions/Symmetry Mappings']",Definition:Symmetry,"A symmetry mapping of a geometric figure is a bijection from the figure to itself which preserves the distance between points.

In other words, it is a self-congruence.


Intuitively and informally, a symmetry mapping is a movement of the figure so that it looks exactly the same after it has been moved.",Definition:Symmetry Mapping,,false,"A symmetry mapping of a geometric figure is a bijection from the figure to itself which preserves the distance between points.

In other words, it is a self-congruence.


Intuitively and informally, a symmetry mapping is a movement of the figure so that it looks exactly the same after it has been moved.",Symmetry
"['Definitions/Examples of Groups', 'Definitions/Symmetry Groups']",Definition:Symmetry,"Let $P$ be a geometric figure.

Let $S_P$ be the set of all symmetries of $P$.

Let $left( S_P, circ right)$ be the algebraic structure such that $circ$ denotes the composition of mappings.


Then $left( S_P, circ right)$ is called the symmetry group of $P$.",Definition:Symmetry Group,,false,"Let P be a geometric figure.

Let S_P be the set of all symmetries of P.

Let ( S_P, ∘) be the algebraic structure such that ∘ denotes the composition of mappings.


Then ( S_P, ∘) is called the symmetry group of P.",Symmetry
"['Definitions/Systems (Electronics)', 'Definitions/Electronics']",Definition:System,A system is a configuration of electrical devices designed for a specific utilitarian purpose.,Definition:System (Electronics),configuration,true,A system is a configuration of electrical devices designed for a specific utilitarian purpose.,System
['Definitions/Topology'],Definition:System,"Let $T = left( S, tau right)$ be a topological space.

A neighborhood system is a family $leftlangle mathcal N_x rightrangle_{x mathop in S}$ indexed by points of $S$, such that $mathcal N_x$ is a local basis at $x$ for $x in S$.",Definition:Neighborhood System,,false,"Let T = ( S, τ) be a topological space.

A neighborhood system is a family ⟨𝒩_x ⟩_x ∈ S indexed by points of S, such that 𝒩_x is a local basis at x for x ∈ S.",System
"['Definitions/Systems of Neighborhoods', 'Definitions/Neighborhoods']",Definition:System,"Let $M = left( A, d right)$ be a metric space.

Let $a in A$.

Let $mathcal N_a$ be the set of all neighborhoods of $a$ in $M$.


Then $mathcal N_a$ is the system of neighborhoods of the point $a$.",Definition:System of Neighborhoods,,false,"Let M = ( A, d ) be a metric space.

Let a ∈ A.

Let 𝒩_a be the set of all neighborhoods of a in M.


Then 𝒩_a is the system of neighborhoods of the point a.",System
['Definitions/Dynamical Systems Theory'],Definition:System,"Let $left( S, tau right)$ be a topological space.

Let $f: S to S$ be a continuous mapping.


Then $left( S, f right)$ is called a topological dynamical system.",Definition:Topological Dynamical System,,false,"Let ( S, τ) be a topological space.

Let f: S → S be a continuous mapping.


Then ( S, f ) is called a topological dynamical system.",System
"['Definitions/Formal Systems', 'Definitions/Symbolic Logic', 'Definitions/Proof Systems']",Definition:System,"Let $mathcal L$ be a formal language.


A proof system $mathscr P$ for $mathcal L$ comprises:

* Axioms and/or axiom schemata;
* Rules of inference for deriving theorems.


It is usual that a proof system does this by declaring certain arguments concerning $mathcal L$ to be valid.

Informally, a proof system amounts to a precise account of what constitutes a (formal) proof.


=== Rule of Inference ===
Let $mathcal L$ be a formal language.

Part of defining a proof system $mathscr P$ for $mathcal L$ is to specify its rules of inference.


A rule of inference is a specification of a valid means to conclude new theorems in $mathscr P$ from given theorems and axioms of $mathscr P$.

Often, the formulation of rules of inference also appeals to the notion of provable consequence to be able to deal with assumptions as part of a proof.

=== Axiom ===
Let $mathcal L$ be a formal language.

Part of defining a proof system $mathscr P$ for $mathcal L$ is to specify its axioms.


An axiom of $mathscr P$ is a well-formed formula of $mathcal L$ that $mathscr P$ approves of by definition.


=== Axiom Schema ===
Let $mathcal L$ be a formal language.

Part of defining a proof system $mathscr P$ for $mathcal L$ is to specify its axiom schemata.


An axiom schema is a well-formed formula $phi$ of $mathcal L$, except for it containing one or more variables which are outside $mathcal L$ itself.

This formula can then be used to represent an infinite number of individual axioms in one statement.


Namely, each of these variables is allowed to take a specified range of values, most commonly WFFs.

Each WFF $psi$ that results from $phi$ by a valid choice of values for all the variables is then an axiom of $mathscr P$.

=== Formal Proof ===
Let $mathscr P$ be a proof system for a formal language $mathcal L$.


Let $phi$ be a WFF of $mathcal L$.

A formal proof of $phi$ in $mathscr P$ is a collection of axioms and rules of inference of $mathscr P$ that leads to the conclusion that $phi$ is a theorem of $mathscr P$.


The term formal proof is also used to refer to specific presentations of such collections.

For example, the term applies to tableau proofs in natural deduction.",Definition:Proof System,,false,"Let ℒ be a formal language.


A proof system 𝒫 for ℒ comprises:

* Axioms and/or axiom schemata;
* Rules of inference for deriving theorems.


It is usual that a proof system does this by declaring certain arguments concerning ℒ to be valid.

Informally, a proof system amounts to a precise account of what constitutes a (formal) proof.


=== Rule of Inference ===
Let ℒ be a formal language.

Part of defining a proof system 𝒫 for ℒ is to specify its rules of inference.


A rule of inference is a specification of a valid means to conclude new theorems in 𝒫 from given theorems and axioms of 𝒫.

Often, the formulation of rules of inference also appeals to the notion of provable consequence to be able to deal with assumptions as part of a proof.

=== Axiom ===
Let ℒ be a formal language.

Part of defining a proof system 𝒫 for ℒ is to specify its axioms.


An axiom of 𝒫 is a well-formed formula of ℒ that 𝒫 approves of by definition.


=== Axiom Schema ===
Let ℒ be a formal language.

Part of defining a proof system 𝒫 for ℒ is to specify its axiom schemata.


An axiom schema is a well-formed formula ϕ of ℒ, except for it containing one or more variables which are outside ℒ itself.

This formula can then be used to represent an infinite number of individual axioms in one statement.


Namely, each of these variables is allowed to take a specified range of values, most commonly WFFs.

Each WFF ψ that results from ϕ by a valid choice of values for all the variables is then an axiom of 𝒫.

=== Formal Proof ===
Let 𝒫 be a proof system for a formal language ℒ.


Let ϕ be a WFF of ℒ.

A formal proof of ϕ in 𝒫 is a collection of axioms and rules of inference of 𝒫 that leads to the conclusion that ϕ is a theorem of 𝒫.


The term formal proof is also used to refer to specific presentations of such collections.

For example, the term applies to tableau proofs in natural deduction.",System
['Definitions/Proof Systems'],Definition:System,"Let $mathcal L$ be a logical language.

Let $mathscr P$ be a proof system.


Then $mathscr P$ is independent if it is not possible to derive one axiom or rule of inference of $mathscr P$ from the others.

 ",Definition:Independent Proof System,,false,"Let ℒ be a logical language.

Let 𝒫 be a proof system.


Then 𝒫 is independent if it is not possible to derive one axiom or rule of inference of 𝒫 from the others.

 ",System
"['Definitions/Proof Systems', 'Definitions/Propositional Logic', 'Definitions/Predicate Logic']",Definition:System,"Gentzen proof systems are a class of proof systems for propositional and predicate logic.

Their characteristics include:

* The presence of few axioms and many rules of inference.
* Use of formal, sequent-like notation involving the turnstile $vdash$.
* Proofs whose structure can be viewed as rooted trees.

Specific instances may deviate from this general scheme at some points.


 
 ",Definition:Gentzen Proof System,,false,"Gentzen proof systems are a class of proof systems for propositional and predicate logic.

Their characteristics include:

* The presence of few axioms and many rules of inference.
* Use of formal, sequent-like notation involving the turnstile ⊢.
* Proofs whose structure can be viewed as rooted trees.

Specific instances may deviate from this general scheme at some points.


 
 ",System
['Definitions/Proof Systems'],Definition:System,"Let $mathcal L$ be a logical language.

Let $mathscr P$ be a proof system for $mathcal L$.


Then $mathscr P$ is consistent  if and only if :

:There exists a logical formula $phi$ such that $not vdash_{mathscr P} phi$

That is, some logical formula $phi$ is not a theorem of $mathscr P$.


=== Propositional Logic ===
Let $mathcal L_0$ be the language of propositional logic.

Let $mathscr P$ be a proof system for $mathcal L_0$.


=== Definition 1 ===
Let $mathcal L_0$ be the language of propositional logic.

Let $mathscr P$ be a proof system for $mathcal L_0$.


Then $mathscr P$ is consistent  if and only if :

:There exists a logical formula $phi$ such that $not vdash_{mathscr P} phi$

That is, some logical formula $phi$ is not a theorem of $mathscr P$.


Category:Definitions/Proof Systems
Category:Definitions/Propositional Logic

=== Definition 2 ===
Let $mathcal L$ be the language of propositional logic.

Let $mathscr P$ be a proof system for $mathcal L_0$.

Suppose that in $mathscr P$, the Rule of Explosion (Variant 3) holds.


Then $mathscr P$ is consistent  if and only if :

:For every logical formula $phi$, not both of $phi$ and $neg phi$ are theorems of $mathscr P$",Definition:Consistent (Logic)/Proof System,,false,"Let ℒ be a logical language.

Let 𝒫 be a proof system for ℒ.


Then 𝒫 is consistent  if and only if :

:There exists a logical formula ϕ such that ⊬_𝒫ϕ

That is, some logical formula ϕ is not a theorem of 𝒫.


=== Propositional Logic ===
Let ℒ_0 be the language of propositional logic.

Let 𝒫 be a proof system for ℒ_0.


=== Definition 1 ===
Let ℒ_0 be the language of propositional logic.

Let 𝒫 be a proof system for ℒ_0.


Then 𝒫 is consistent  if and only if :

:There exists a logical formula ϕ such that ⊬_𝒫ϕ

That is, some logical formula ϕ is not a theorem of 𝒫.


Category:Definitions/Proof Systems
Category:Definitions/Propositional Logic

=== Definition 2 ===
Let ℒ be the language of propositional logic.

Let 𝒫 be a proof system for ℒ_0.

Suppose that in 𝒫, the Rule of Explosion (Variant 3) holds.


Then 𝒫 is consistent  if and only if :

:For every logical formula ϕ, not both of ϕ and ϕ are theorems of 𝒫",System
"['Definitions/Sound Proof Systems', 'Definitions/Proof Systems']",Definition:System,"Let $mathcal L$ be a logical language.

Let $mathscr P$ be a proof system for $mathcal L$.

Let $mathscr M$ be a formal semantics for $mathcal L$.


Then $mathscr P$ is said to be sound for $mathscr M$  if and only if :

:Every $mathscr P$-theorem is an $mathscr M$-tautology.

Symbolically, this can be expressed as the statement that, for every logical formula $phi$ of $mathcal L$:

:$vdash_{mathscr P} phi$ implies $models_{mathscr M} phi$


=== Strongly Sound Proof System ===
Let $mathcal L$ be a logical language.

Let $mathscr P$ be a proof system for $mathcal L$.

let $mathscr M$ be a formal semantics for $mathcal L$.


$mathscr P$ is strongly sound for $mathscr M$  if and only if :

:Every $mathscr P$-provable consequence is an $mathscr M$-semantic consequence.

Symbolically, this can be expressed as the statement that, for every collection of logical formulas $mathcal F$, and logical formula $phi$ of $mathcal L$:

:$mathcal F vdash_{mathscr P} phi$ implies $mathcal F models_{mathscr M} phi$",Definition:Sound Proof System,,false,"Let ℒ be a logical language.

Let 𝒫 be a proof system for ℒ.

Let ℳ be a formal semantics for ℒ.


Then 𝒫 is said to be sound for ℳ  if and only if :

:Every 𝒫-theorem is an ℳ-tautology.

Symbolically, this can be expressed as the statement that, for every logical formula ϕ of ℒ:

:⊢_𝒫ϕ implies _ℳϕ


=== Strongly Sound Proof System ===
Let ℒ be a logical language.

Let 𝒫 be a proof system for ℒ.

let ℳ be a formal semantics for ℒ.


𝒫 is strongly sound for ℳ  if and only if :

:Every 𝒫-provable consequence is an ℳ-semantic consequence.

Symbolically, this can be expressed as the statement that, for every collection of logical formulas ℱ, and logical formula ϕ of ℒ:

:ℱ⊢_𝒫ϕ implies ℱ_ℳϕ",System
"['Definitions/Complete Proof Systems', 'Definitions/Proof Systems']",Definition:System,"Let $mathcal L$ be a logical language.

Let $mathscr P$ be a proof system for $mathcal L$, and let $mathscr M$ be a formal semantics for $mathcal L$.


$mathscr P$ is strongly complete for $mathscr M$  if and only if :

:Every $mathscr M$-semantic consequence is a $mathscr P$-provable consequence.

Symbolically, this can be expressed as the statement that, for every collection $mathcal F$ of logical formulas, and every logical formula $phi$ of $mathcal L$:

:$mathcal F models_{mathscr M} phi$ implies $mathcal F vdash_{mathscr P} phi$",Definition:Complete Proof System/Strongly Complete,,false,"Let ℒ be a logical language.

Let 𝒫 be a proof system for ℒ, and let ℳ be a formal semantics for ℒ.


𝒫 is strongly complete for ℳ  if and only if :

:Every ℳ-semantic consequence is a 𝒫-provable consequence.

Symbolically, this can be expressed as the statement that, for every collection ℱ of logical formulas, and every logical formula ϕ of ℒ:

:ℱ_ℳϕ implies ℱ⊢_𝒫ϕ",System
['Definitions/Sound Proof Systems'],Definition:System,"Let $mathcal L$ be a logical language.

Let $mathscr P$ be a proof system for $mathcal L$.

let $mathscr M$ be a formal semantics for $mathcal L$.


$mathscr P$ is strongly sound for $mathscr M$  if and only if :

:Every $mathscr P$-provable consequence is an $mathscr M$-semantic consequence.

Symbolically, this can be expressed as the statement that, for every collection of logical formulas $mathcal F$, and logical formula $phi$ of $mathcal L$:

:$mathcal F vdash_{mathscr P} phi$ implies $mathcal F models_{mathscr M} phi$",Definition:Sound Proof System/Strongly Sound,,false,"Let ℒ be a logical language.

Let 𝒫 be a proof system for ℒ.

let ℳ be a formal semantics for ℒ.


𝒫 is strongly sound for ℳ  if and only if :

:Every 𝒫-provable consequence is an ℳ-semantic consequence.

Symbolically, this can be expressed as the statement that, for every collection of logical formulas ℱ, and logical formula ϕ of ℒ:

:ℱ⊢_𝒫ϕ implies ℱ_ℳϕ",System
['Definitions/Collations'],Definition:System,"A key feature of collations is the presence of methods to collate a number of collations into a new one.

A collection of collations, together with a collection of such collation methods may be called a collation system.


For example, words and the method of concatenation.


Category:Definitions/Collations",Definition:Collation/Collation System,,false,"A key feature of collations is the presence of methods to collate a number of collations into a new one.

A collection of collations, together with a collection of such collation methods may be called a collation system.


For example, words and the method of concatenation.


Category:Definitions/Collations",System
['Definitions/Formal Systems'],Definition:System,"A formal system is a formal language $mathcal L$ together with a deductive apparatus for $mathcal L$.


Let $mathcal F$ be a formal system consisting of a formal language with deductive apparatus $mathcal D$.

By applying the formal grammar of $mathcal L$, one constructs well-formed formulae in $mathcal L$.

Of such a well-formed formula, one can then use the deductive apparatus $mathcal D$ to determine whether or not it is a theorem in $mathcal F$.",Definition:Formal System,,false,"A formal system is a formal language ℒ together with a deductive apparatus for ℒ.


Let ℱ be a formal system consisting of a formal language with deductive apparatus 𝒟.

By applying the formal grammar of ℒ, one constructs well-formed formulae in ℒ.

Of such a well-formed formula, one can then use the deductive apparatus 𝒟 to determine whether or not it is a theorem in ℱ.",System
['Definitions/Differential Equations'],Definition:System,"A system of differential equations is autonomous if all of the differential equations which it comprises are themselves autonomous.


Category:Definitions/Differential Equations",Definition:Differential Equation/System/Autonomous,,false,"A system of differential equations is autonomous if all of the differential equations which it comprises are themselves autonomous.


Category:Definitions/Differential Equations",System
['Definitions/Coordinate Systems'],Definition:System,"Let $R$ be a ring with unity.

Let $leftlangle a_k rightrangle_{1 mathop le k mathop le n}$ be an ordered basis of a free unitary $R$-module $G$.


Then $leftlangle a_k rightrangle_{1 mathop le k mathop le n}$ can be referred to as a coordinate system.


=== Coordinate Function ===
Let $left langle {a_n} right rangle$ be a coordinate system of a unitary $R$-module $G$.

For each $x in G$ let $x_1, x_2, ldots, x_n$ be the coordinates of $x$ relative to $left langle {a_n} right rangle$.


Then for $i = 1, ldots, n$ the mapping $f_i : G to R$ defined by $f_i left({x}right) = x_i$ is called the $i$-th coordinate function on $G$ relative to $left langle {a_n} right rangle$.


Category:Definitions/Coordinate Systems

=== Coordinates on Affine Space ===
Let $mathcal E$ be an affine space of dimension $n$ over a field $k$.

Let $mathcal R = left( p_0, e_1, ldots, e_n right)$ be an affine frame in $mathcal E$.

Let $p in mathcal E$ be a point.

Since Affine Coordinates are Well-Defined, there exists a unique ordered tuple $left( lambda_1, ldots, lambda_n right) in k^n$ such that:
:$ds p = p_0 + sum_{i mathop = 1}^n lambda_i e_i$


The numbers $lambda_1, ldots, lambda_n$ are the coordinates of $p$ in the frame $mathcal R$.


Category:Definitions/Affine Geometry

=== Coordinate ===
Let $leftlangle a_n rightrangle$ be a coordinate system of a unitary $R$-module $G$.

Let $ds x in G: x = sum_{k mathop = 1}^n lambda_k a_k$.

The scalars $lambda_1, lambda_2, ldots, lambda_n$ can be referred to as the coordinates of $x$ relative to $leftlangle a_n rightrangle$.


=== Elements of Ordered Pair ===
Let $left( a, b right)$ be an ordered pair.

The following terminology is used:
:$a$ is called the first coordinate
:$b$ is called the second coordinate.

This definition is compatible with the equivalent definition in the context of Cartesian coordinate systems.

=== Origin ===
The origin of a coordinate system is the zero vector.

In the $x y$-plane, it is the point:
:$O = left( 0, 0 right)$
and in general, in the Euclidean space $mathbb R^n$:
:$O = underbrace {left( 0, 0, ldots, 0 right) }_{text{$n$ coordinates} }$


Thus it is the point where the axes cross over each other.",Definition:Coordinate System,,false,"Let R be a ring with unity.

Let ⟨ a_k ⟩_1 ≤ k ≤ n be an ordered basis of a free unitary R-module G.


Then ⟨ a_k ⟩_1 ≤ k ≤ n can be referred to as a coordinate system.


=== Coordinate Function ===
Let ⟨a_n⟩ be a coordinate system of a unitary R-module G.

For each x ∈ G let x_1, x_2, …, x_n be the coordinates of x relative to ⟨a_n⟩.


Then for i = 1, …, n the mapping f_i : G → R defined by f_i (x) = x_i is called the i-th coordinate function on G relative to ⟨a_n⟩.


Category:Definitions/Coordinate Systems

=== Coordinates on Affine Space ===
Let ℰ be an affine space of dimension n over a field k.

Let ℛ = ( p_0, e_1, …, e_n ) be an affine frame in ℰ.

Let p ∈ℰ be a point.

Since Affine Coordinates are Well-Defined, there exists a unique ordered tuple ( λ_1, …, λ_n ) ∈ k^n such that:
:p = p_0 + ∑_i  = 1^n λ_i e_i


The numbers λ_1, …, λ_n are the coordinates of p in the frame ℛ.


Category:Definitions/Affine Geometry

=== Coordinate ===
Let ⟨ a_n ⟩ be a coordinate system of a unitary R-module G.

Let x ∈ G: x = ∑_k  = 1^n λ_k a_k.

The scalars λ_1, λ_2, …, λ_n can be referred to as the coordinates of x relative to ⟨ a_n ⟩.


=== Elements of Ordered Pair ===
Let ( a, b ) be an ordered pair.

The following terminology is used:
:a is called the first coordinate
:b is called the second coordinate.

This definition is compatible with the equivalent definition in the context of Cartesian coordinate systems.

=== Origin ===
The origin of a coordinate system is the zero vector.

In the x y-plane, it is the point:
:O = ( 0, 0 )
and in general, in the Euclidean space ℝ^n:
:O = ( 0, 0, …, 0 ) _n coordinates


Thus it is the point where the axes cross over each other.",System
"['Definitions/Rectilinear Coordinate Systems', 'Definitions/Orthogonal Coordinate Systems', 'Definitions/Coordinate Systems']",Definition:System,A coordinate system whose coordinate axes are straight lines is called a system of rectilinear coordinate system.,Definition:Rectilinear Coordinate System,,false,A coordinate system whose coordinate axes are straight lines is called a system of rectilinear coordinate system.,System
"['Definitions/Coordinate Systems', 'Definitions/Curvilinear Coordinates']",Definition:System,"A coordinate system such that at least one of the coordinate axes is a curved line is called a system of curvilinear coordinates.


=== Cartesian Representation ===
The relation between curvilinear coordinates and Cartesian coordinates can be expressed as:

 
 
 
 
 


 
 
 
 
 


where:
:$left( x, y, z right)$ denotes the Cartesian coordinates
:$left( q_1, q_2, q_3 right)$ denotes their curvilinear equivalents.",Definition:Curvilinear Coordinate System,,false,"A coordinate system such that at least one of the coordinate axes is a curved line is called a system of curvilinear coordinates.


=== Cartesian Representation ===
The relation between curvilinear coordinates and Cartesian coordinates can be expressed as:

 
 
 
 
 


 
 
 
 
 


where:
:( x, y, z ) denotes the Cartesian coordinates
:( q_1, q_2, q_3 ) denotes their curvilinear equivalents.",System
"['Definitions/Orthogonal Coordinate Systems', 'Definitions/Coordinate Systems']",Definition:System,"An orthogonal coordinate system is a coordinate system in which the coordinate axes are pairwise perpendicular.


=== Orthogonal Curvilinear Coordinates ===
Let $mathcal K$ be a curvilinear coordinate system in $3$-space.

Let $mathcal Q_1$, $mathcal Q_2$ and $mathcal Q_3$ denote the one-parameter families that define the curvilinear coordinates.



Let the relation between those curvilinear coordinates and Cartesian coordinates be expressed as:

 
 
 
 
 

where:
:$left( x, y, z right)$ denotes the Cartesian coordinates of an arbitrary point $P$
:$left( q_1, q_2, q_3 right)$ denotes the curvilinear coordinates of $P$.


Let these equations have the property that the metric of $mathcal K$ between coordinate surfaces of $mathcal Q_i$ and $mathcal Q_j$ is zero where $i ne j$.


That is, for every point $P$ expressible as $left( x, y, z right)$ and $left( q_1, q_2, q_3 right)$:

:$dfrac {partial x} {partial q_i} dfrac {partial x} {partial q_j} + dfrac {partial y} {partial q_i} dfrac {partial y} {partial q_j} + dfrac {partial z} {partial q_i} dfrac {partial z} {partial q_j} = 0$

wherever $i ne j$.


Then $mathcal K$ is an orthogonal curvilinear coordinate system.

=== Rectangular Coordinate System ===
A rectangular coordinate system is a Cartesian coordinate system in which each of the coordinate axes are perpendicular to each other.",Definition:Orthogonal Coordinate System,,false,"An orthogonal coordinate system is a coordinate system in which the coordinate axes are pairwise perpendicular.


=== Orthogonal Curvilinear Coordinates ===
Let 𝒦 be a curvilinear coordinate system in 3-space.

Let 𝒬_1, 𝒬_2 and 𝒬_3 denote the one-parameter families that define the curvilinear coordinates.



Let the relation between those curvilinear coordinates and Cartesian coordinates be expressed as:

 
 
 
 
 

where:
:( x, y, z ) denotes the Cartesian coordinates of an arbitrary point P
:( q_1, q_2, q_3 ) denotes the curvilinear coordinates of P.


Let these equations have the property that the metric of 𝒦 between coordinate surfaces of 𝒬_i and 𝒬_j is zero where i  j.


That is, for every point P expressible as ( x, y, z ) and ( q_1, q_2, q_3 ):

:∂ x∂ q_i∂ x∂ q_j + ∂ y∂ q_i∂ y∂ q_j + ∂ z∂ q_i∂ z∂ q_j = 0

wherever i  j.


Then 𝒦 is an orthogonal curvilinear coordinate system.

=== Rectangular Coordinate System ===
A rectangular coordinate system is a Cartesian coordinate system in which each of the coordinate axes are perpendicular to each other.",System
"['Definitions/Cartesian Coordinate Systems', 'Definitions/Examples of Coordinate Systems', 'Definitions/Examples of Rectilinear Coordinate Systems', 'Definitions/Analytic Geometry']",Definition:System,"A Cartesian coordinate system is a coordinate system in which the position of a point is determined by its relation to a set of perpendicular straight lines.

These straight lines are referred to as coordinate axes.",Definition:Cartesian Coordinate System,,false,"A Cartesian coordinate system is a coordinate system in which the position of a point is determined by its relation to a set of perpendicular straight lines.

These straight lines are referred to as coordinate axes.",System
['Definitions/Cartesian Coordinate Systems'],Definition:System,A rectangular coordinate system is a Cartesian coordinate system in which each of the coordinate axes are perpendicular to each other.,Definition:Rectangular Coordinate System,,false,A rectangular coordinate system is a Cartesian coordinate system in which each of the coordinate axes are perpendicular to each other.,System
"['Definitions/Oblique Coordinate Systems', 'Definitions/Coordinate Systems']",Definition:System,"An oblique coordinate system is a coordinate system in which the position of a point is determined by its relation to a set of straight coordinate axes which are oblique.

:",Definition:Oblique Coordinate System,,false,"An oblique coordinate system is a coordinate system in which the position of a point is determined by its relation to a set of straight coordinate axes which are oblique.

:",System
['Definitions/Abstract Algebra'],Definition:System,"An algebraic system is a mathematical system $mathcal S = left( E, O right)$ where:

:$E$ is a non-empty set of elements

:$O$ is a set of finitary operations on $E$.",Definition:Algebraic System,,false,"An algebraic system is a mathematical system 𝒮 = ( E, O ) where:

:E is a non-empty set of elements

:O is a set of finitary operations on E.",System
['Definitions/Abstract Algebra'],Definition:System,"A mathematical system is a set $mathcal S = left( E, O, A right)$ where:

:$E$ is a non-empty set of elements

:$O$ is a set of relations and operations on the elements of $E$

:$A$ is a set of axioms concerning the elements of $E$ and $O$.


=== Abstract System ===

A mathematical system $mathcal S = left( E, O, A right)$ is classed as abstract  if and only if  the elements of $E$ and $O$ are defined only by their properties as specified in $A$.


=== Concrete System ===

A mathematical system $mathcal S = left( E, O, A right)$ is classed as concrete  if and only if  the elements of $E$ and $O$ are understood as objects independently of their existence in $mathcal S$ itself.


The distinction between abstract and concrete is of questionable value from a modern standpoint, as it is a moot point, for example, as to whether the natural numbers exist independently of Peano's axioms or are specifically defined by them.


=== Algebraic System ===

A mathematical system $mathcal S = left( E, O, A right)$ is classed as algebraic  if and only if  it has many of the properties of the set of integers.

This is usually because such a system is itself an abstraction of certain properties of the integers.

The axioms are usually not considered as separate entities from the operations, as their nature is implicit in the operations themselves.


Specifically, an algebraic system can be defined as follows:
An algebraic system is a mathematical system $mathcal S = left( E, O right)$ where:

:$E$ is a non-empty set of elements

:$O$ is a set of finitary operations on $E$.",Definition:Mathematical System,,false,"A mathematical system is a set 𝒮 = ( E, O, A ) where:

:E is a non-empty set of elements

:O is a set of relations and operations on the elements of E

:A is a set of axioms concerning the elements of E and O.


=== Abstract System ===

A mathematical system 𝒮 = ( E, O, A ) is classed as abstract  if and only if  the elements of E and O are defined only by their properties as specified in A.


=== Concrete System ===

A mathematical system 𝒮 = ( E, O, A ) is classed as concrete  if and only if  the elements of E and O are understood as objects independently of their existence in 𝒮 itself.


The distinction between abstract and concrete is of questionable value from a modern standpoint, as it is a moot point, for example, as to whether the natural numbers exist independently of Peano's axioms or are specifically defined by them.


=== Algebraic System ===

A mathematical system 𝒮 = ( E, O, A ) is classed as algebraic  if and only if  it has many of the properties of the set of integers.

This is usually because such a system is itself an abstraction of certain properties of the integers.

The axioms are usually not considered as separate entities from the operations, as their nature is implicit in the operations themselves.


Specifically, an algebraic system can be defined as follows:
An algebraic system is a mathematical system 𝒮 = ( E, O ) where:

:E is a non-empty set of elements

:O is a set of finitary operations on E.",System
"['Definitions/Astronomy', 'Definitions/Celestial Mechanics', 'Definitions/Solar System']",Definition:System,The solar system is the system of celestial bodies which are under the direct influence of the gravitational field of the sun.,Definition:Solar System,,false,The solar system is the system of celestial bodies which are under the direct influence of the gravitational field of the sun.,System
['Definitions/Number Systems'],Definition:System,A number system is a technique for representing numbers.,Definition:Number System,,false,A number system is a technique for representing numbers.,System
['Definitions/Number Bases'],Definition:System,"The vigesimal system is base $20$ notation.

That is, every number $x in mathbb R$ is expressed in the form:
:$ds x = sum_{j mathop in mathbb Z} r_j 20^j$
where:
:$forall j in mathbb Z: r_j in leftlbrace 0, 1, ldots, 19 rightrbrace$",Definition:Vigesimal System,,false,"The vigesimal system is base 20 notation.

That is, every number x ∈ℝ is expressed in the form:
:x = ∑_j ∈ℤ r_j 20^j
where:
:∀ j ∈ℤ: r_j ∈{ 0, 1, …, 19 }",System
"['Definitions/Number Bases', 'Definitions/Golden Mean', 'Definitions/Golden Mean Number System']",Definition:System,"The golden mean number system is a system for representing a non-negative real number $x$ by a sequence of zeroes and  ones using the golden mean $phi$ as a number base.


=== Equivalent Representations ===
Let $x in mathbb R_{ge 0}$ have two representations $S_1$ and $S_2$ in the golden mean number system.

Then $S_1$ and $S_2$ are equivalent representations.


Category:Definitions/Golden Mean Number System

=== Simplest Form ===
Let $x in mathbb R_{ge 0}$ have a representation $S$ in the golden mean number system.

Then $S$ is the simplest form for $x$  if and only if :

:$(1): quad$ No two adjacent $1$s appear in $S$
:$(2): quad S$ does not end with the infinite sequence $cdotp ldots 010101 ldots$

=== Simplification ===
Consider the golden mean number system.

Let $x in mathbb R_{ge 0}$ have a representation which includes the string $011$, say:
:$x = p011q$

where $p$ and $q$ are strings in $left{ {0, 1}right}$.


From 100 in Golden Mean Number System is Equivalent to 011, $x$ can also be written as:
:$x = p100q$


The expression $p100q$ is a simplification of $p011q$.

=== Expansion ===
Consider the golden mean number system.

Let $x in mathbb R_{ge 0}$ have a representation which includes the string $100$, say:
:$x = p100q$

where $p$ and $q$ are strings in $left{ {0, 1}right}$.


From 100 in Golden Mean Number System is Equivalent to 011, $x$ can also be written as:
:$x = p011q$


The expression $p011q$ is an expansion of $p011q$.",Definition:Golden Mean Number System,,false,"The golden mean number system is a system for representing a non-negative real number x by a sequence of zeroes and  ones using the golden mean ϕ as a number base.


=== Equivalent Representations ===
Let x ∈ℝ_≥ 0 have two representations S_1 and S_2 in the golden mean number system.

Then S_1 and S_2 are equivalent representations.


Category:Definitions/Golden Mean Number System

=== Simplest Form ===
Let x ∈ℝ_≥ 0 have a representation S in the golden mean number system.

Then S is the simplest form for x  if and only if :

:(1): No two adjacent 1s appear in S
:(2):    S does not end with the infinite sequence … 010101 …

=== Simplification ===
Consider the golden mean number system.

Let x ∈ℝ_≥ 0 have a representation which includes the string 011, say:
:x = p011q

where p and q are strings in {0, 1}.


From 100 in Golden Mean Number System is Equivalent to 011, x can also be written as:
:x = p100q


The expression p100q is a simplification of p011q.

=== Expansion ===
Consider the golden mean number system.

Let x ∈ℝ_≥ 0 have a representation which includes the string 100, say:
:x = p100q

where p and q are strings in {0, 1}.


From 100 in Golden Mean Number System is Equivalent to 011, x can also be written as:
:x = p011q


The expression p011q is an expansion of p011q.",System
['Definitions/Number Bases'],Definition:System,The factorial number system is a mixed radix positional numeral system in which the digit in the $i$th column from the right is of base $i$.,Definition:Factorial Number System,,false,The factorial number system is a mixed radix positional numeral system in which the digit in the ith column from the right is of base i.,System
"['Definitions/Decimal System', 'Definitions/Decimal', 'Definitions/Number Systems', 'Definitions/Units of Measurement']",Definition:System,A decimal system is a system of measurement in which the standard multiples and fractions of the units of measurement are powers of $10$.,Definition:Decimal System,,false,A decimal system is a system of measurement in which the standard multiples and fractions of the units of measurement are powers of 10.,System
['Definitions/Binomial Coefficients'],Definition:System,"The combinatorial number system is a system for representing a positive integer $m$ by a sequence of digits which are the upper coefficient of a sequence of $n$ binomial coefficients for some $n in mathbb Z_{>0}$:

:$m := k_1 k_2 k_3 ldots k_n$

where:
:$m = dbinom {k_1} 1 + dbinom {k_2} 2 + dbinom {k_3} 3 + cdots + dbinom {k_n} n$
:$0 le k_1 < k_2 < cdots < k_n$",Definition:Combinatorial Number System,,false,"The combinatorial number system is a system for representing a positive integer m by a sequence of digits which are the upper coefficient of a sequence of n binomial coefficients for some n ∈ℤ_>0:

:m := k_1 k_2 k_3 … k_n

where:
:m = k_1 1 + k_2 2 + k_3 3 + ⋯ + k_n n
:0 ≤ k_1 < k_2 < ⋯ < k_n",System
"['Definitions/Fibonacci Numbers', 'Definitions/Number Bases']",Definition:System,"Zeckendorf representation is a system for representing a positive integer $m$ by a sequence of digits which are the indices of a sequence of $r$ Fibonacci numbers:

:$n := k_1 k_2 k_3 ldots k_r$

where:
:$n = F_{k_1} + F_{k_2} + F_{k_3} + cdots + F_{k_r}$
:$k_1 gg k_2 gg k_3 gg cdots gg k_r gg 0$

where $n gg k$ denotes that $n ge k + 2$.",Definition:Zeckendorf Representation,,false,"Zeckendorf representation is a system for representing a positive integer m by a sequence of digits which are the indices of a sequence of r Fibonacci numbers:

:n := k_1 k_2 k_3 … k_r

where:
:n = F_k_1 + F_k_2 + F_k_3 + ⋯ + F_k_r
:k_1 ≫ k_2 ≫ k_3 ≫⋯≫ k_r ≫ 0

where n ≫ k denotes that n ≥ k + 2.",System
['Definitions/Number Systems'],Definition:System,"A positional number system is a number system with the following properties:

:It has a set of numerals which represent a subset of the numbers.

:The number being represented is written as a string of these numerals, which represent a different value according to their position in the numerals.

The design of the positional number system is such that all numbers can be represented by such a string, which may or may not be infinite in length.",Definition:Positional Numeral System,,false,"A positional number system is a number system with the following properties:

:It has a set of numerals which represent a subset of the numbers.

:The number being represented is written as a string of these numerals, which represent a different value according to their position in the numerals.

The design of the positional number system is such that all numbers can be represented by such a string, which may or may not be infinite in length.",System
"['Definitions/Numeral Systems', 'Definitions/Babylonian Number System', 'Definitions/Babylonian Mathematics']",Definition:System,"The number system as used in the   was a positional numeral system where the number base was a combination of decimal (base $10$) and sexagesimal (base $60$).

The characters were written in   by a combination of:
:a thin vertical wedge shape, to indicate the digit $1$
:a fat horizontal wedge shape, to indicate the digit $10$
arranged in groups to indicate the digits $2$ to $9$ and $20$ to $50$.


:


At $59$ the pattern stops, and the number $60$ is represented by the digit $1$ once again.

Thus these groupings were placed side by side:
:the rightmost grouping would indicate a number from $1$ to $59$
:the one to the left of that would indicate a number from $60 times 1$ to $60 times 59$
and so on, each grouping further to the left indicating another multiplication by $60$


For fractional numbers there was no actual radix point. Instead, the distinction was inferred by context.


The fact that they had no symbol to indicate the zero digit means that this was not a true positional numeral system as such.


For informal everyday arithmetic, they used a decimal system which was the decimal part of the full sexagesimal system.",Definition:Babylonian Number System,,false,"The number system as used in the   was a positional numeral system where the number base was a combination of decimal (base 10) and sexagesimal (base 60).

The characters were written in   by a combination of:
:a thin vertical wedge shape, to indicate the digit 1
:a fat horizontal wedge shape, to indicate the digit 10
arranged in groups to indicate the digits 2 to 9 and 20 to 50.


:


At 59 the pattern stops, and the number 60 is represented by the digit 1 once again.

Thus these groupings were placed side by side:
:the rightmost grouping would indicate a number from 1 to 59
:the one to the left of that would indicate a number from 60 × 1 to 60 × 59
and so on, each grouping further to the left indicating another multiplication by 60


For fractional numbers there was no actual radix point. Instead, the distinction was inferred by context.


The fact that they had no symbol to indicate the zero digit means that this was not a true positional numeral system as such.


For informal everyday arithmetic, they used a decimal system which was the decimal part of the full sexagesimal system.",System
['Definitions/Number-Naming Systems'],Definition:System,"There are various number-naming systems for naming large numbers (that is: greater than $1 , 000 , 000$).


=== Short Scale ===
The short scale system is the number-naming system which uses:
:the word million for $10^6 = 1 , 000 , 000$
:the Latin-derived prefixes bi-, tri-, quadri-, quint-, etc. for each further multiple of $1 , 000$, appended to the root -(i)llion, corresponding to the indices $2$, $3$, $4$, $5$, $ldots$


Thus:

 
 
 
 
 
 

Thus one $n$-illion equals $1000 times 10^{3 n}$ or $10^{3 n + 3}$

=== Long Scale ===
The long scale system is the number-naming system which uses:
:the word million for $10^6 = 1 , 000 , 000$
:the Latin-derived prefixes bi-, tri-, quadri-, quint-, etc. for each further multiple of $1 , 000 , 000$, appended to the root -(i)llion, corresponding to the indices $2$, $3$, $4$, $5$, $ldots$


Thus:

 
 
 
 
 
 

Thus one $n$-illion equals $10^{6 n}$.


Additional terms are occasionally found to fill some of the gaps, but these are rare nowadays:

 
 
 
 ",Definition:Number-Naming System,,false,"There are various number-naming systems for naming large numbers (that is: greater than 1   000   000).


=== Short Scale ===
The short scale system is the number-naming system which uses:
:the word million for 10^6 = 1   000   000
:the Latin-derived prefixes bi-, tri-, quadri-, quint-, etc. for each further multiple of 1   000, appended to the root -(i)llion, corresponding to the indices 2, 3, 4, 5, …


Thus:

 
 
 
 
 
 

Thus one n-illion equals 1000 × 10^3 n or 10^3 n + 3

=== Long Scale ===
The long scale system is the number-naming system which uses:
:the word million for 10^6 = 1   000   000
:the Latin-derived prefixes bi-, tri-, quadri-, quint-, etc. for each further multiple of 1   000   000, appended to the root -(i)llion, corresponding to the indices 2, 3, 4, 5, …


Thus:

 
 
 
 
 
 

Thus one n-illion equals 10^6 n.


Additional terms are occasionally found to fill some of the gaps, but these are rare nowadays:

 
 
 
 ",System
"['Definitions/Metric System', 'Definitions/Units of Measurement']",Definition:System,"The metric system is the colloquial term for the system of measurement based on the metre.

Its main characteristic is that its units are constructed on a decimal system.",Definition:Metric System,,false,"The metric system is the colloquial term for the system of measurement based on the metre.

Its main characteristic is that its units are constructed on a decimal system.",System
['Definitions/Physics'],Definition:System,"A physical system is a portion of the physical universe which has been chosen for investigation for a particular purpose.


Category:Definitions/Physics",Definition:Physical System,,false,"A physical system is a portion of the physical universe which has been chosen for investigation for a particular purpose.


Category:Definitions/Physics",System
"['Definitions/Set Systems', 'Definitions/Measure Theory', 'Definitions/Dynkin Systems']",Definition:System,"Let $X$ be a set, and let $mathcal D subseteq mathcal P left( X right)$ be a collection of subsets of $X$.


Then $mathcal D$ is called a Dynkin system (on $X$)  if and only if  it satisfies the following conditions:

:$(1): quad X in mathcal D$
:$(2): quad forall D in mathcal D: X setminus D in mathcal D$
:$(3): quad$ For all pairwise disjoint sequences $leftlangle D_n rightrangle_{n mathop in mathbb N}$ in $mathcal D$, $ds bigcup_{n mathop in mathbb N} D_n in mathcal D$",Definition:Dynkin System,,false,"Let X be a set, and let 𝒟⊆𝒫( X ) be a collection of subsets of X.


Then 𝒟 is called a Dynkin system (on X)  if and only if  it satisfies the following conditions:

:(1):    X ∈𝒟
:(2):   ∀ D ∈𝒟: X ∖ D ∈𝒟
:(3): For all pairwise disjoint sequences ⟨ D_n ⟩_n ∈ℕ in 𝒟, ⋃_n ∈ℕ D_n ∈𝒟",System
['Definitions/Dynkin Systems'],Definition:System,"Let $X$ be a set.

Let $mathcal G subseteq mathcal P left( X right)$ be a collection of subsets of $X$.


Then the Dynkin system generated by $mathcal G$, denoted $delta left(   right)mathcal G$, is the smallest Dynkin system on $X$ that contains $mathcal G$.

That is, $delta left(   right)mathcal G$ is subject to:

:$(1):quad mathcal G subseteq delta left(   right)mathcal G$
:$(2):quad mathcal G subseteq mathcal D implies delta left(   right)mathcal G subseteq mathcal D$ for any Dynkin system $mathcal D$ on $X$


In fact, $delta left(   right)mathcal G$ always exists, and is unique, as proved on Existence and Uniqueness of Dynkin System Generated by Collection of Subsets.


=== Generator ===

One says that $mathcal G$ is a generator for $delta left(   right)mathcal G$.",Definition:Dynkin System Generated by Collection of Subsets,,false,"Let X be a set.

Let 𝒢⊆𝒫( X ) be a collection of subsets of X.


Then the Dynkin system generated by 𝒢, denoted δ(   )𝒢, is the smallest Dynkin system on X that contains 𝒢.

That is, δ(   )𝒢 is subject to:

:(1):  𝒢⊆δ(   )𝒢
:(2):  𝒢⊆𝒟δ(   )𝒢⊆𝒟 for any Dynkin system 𝒟 on X


In fact, δ(   )𝒢 always exists, and is unique, as proved on Existence and Uniqueness of Dynkin System Generated by Collection of Subsets.


=== Generator ===

One says that 𝒢 is a generator for δ(   )𝒢.",System
"['Definitions/Dynamical Systems', 'Definitions/Dynamical Systems Theory']",Definition:System,"A dynamical system is a non-linear system in which a function describes the time dependence of a point in a geometrical space.


=== Flow ===
In a dynamical system, a set of time-dependent equations is known as flow.

 ",Definition:Dynamical System,,false,"A dynamical system is a non-linear system in which a function describes the time dependence of a point in a geometrical space.


=== Flow ===
In a dynamical system, a set of time-dependent equations is known as flow.

 ",System
"['Definitions/Numeral Systems', 'Definitions/Numbers']",Definition:System,"A numeral system is:
:a set of symbols that is used to represent a specific subset of the set of numbers (usually natural numbers), referred to as numerals
:a set of rules which define how to combine the numerals so as to be able to express other numbers.",Definition:Numeral System,,false,"A numeral system is:
:a set of symbols that is used to represent a specific subset of the set of numbers (usually natural numbers), referred to as numerals
:a set of rules which define how to combine the numerals so as to be able to express other numbers.",System
['Definitions/Matroid Theory'],Definition:System,"Let $S$ be a finite set.

Let $mathscr F$ be a set of subsets of $S$ satisfying the independence system axioms:
 

The ordered pair $I = left( S, mathscr F right)$ is called an independence system on $S$.",Definition:Independence System,,false,"Let S be a finite set.

Let ℱ be a set of subsets of S satisfying the independence system axioms:
 

The ordered pair I = ( S, ℱ) is called an independence system on S.",System
['Definitions/Order Theory'],Definition:System,"Let $L$ be an ordered set.

Let $S$ be a system of $L$.


Then $S$ is closure  if and only if  $S$ inherits infima.",Definition:Closure System,,false,"Let L be an ordered set.

Let S be a system of L.


Then S is closure  if and only if  S inherits infima.",System
['Definitions/Numbers'],Definition:System,"There are five main classes of number:

:$(1): quad$ The natural numbers: $mathbb N = leftlbrace 0, 1, 2, 3, ldots rightrbrace$
::$(1 text a): quad$ The non-zero natural numbers: $mathbb N_{>0} = leftlbrace 1, 2, 3, ldots rightrbrace$
:$(2): quad$ The integers: $mathbb Z = leftlbrace ldots, -3, -2, -1, 0, 1, 2, 3, ldots rightrbrace$
:$(3): quad$ The rational numbers: $mathbb Q = leftlbrace p / q: p, q in mathbb Z, q ne 0 rightrbrace$
:$(4): quad$ The real numbers: $mathbb R = leftlbrace x: x = leftlangle s_n rightrangle  rightrbrace$ where $leftlangle s_n rightrangle$ is a Cauchy sequence in $mathbb Q$
:$(5): quad$ The complex numbers: $mathbb C = leftlbrace a + i b: a, b in mathbb R, i^2 = -1 rightrbrace$


It is possible to categorize numbers further, for example:

:The set of algebraic numbers $mathbb A$ is the subset of the complex numbers which are roots of polynomials with rational coefficients.  The algebraic numbers include the rational numbers, $sqrt 2$, and the golden section $varphi$.

:The set of transcendental numbers is the set of all the real numbers which are not algebraic.  The transcendental numbers include $pi, e$ and $sqrt 2^{sqrt 2}$.

:The set of prime numbers (sometimes referred to as $mathbb P$) is the subset of the integers which have exactly two positive divisors, $1$ and the number itself.  The first several positive primes are $2, 3, 5, 7, 11, 13, ldots$",Definition:Number,,false,"There are five main classes of number:

:(1): The natural numbers: ℕ = { 0, 1, 2, 3, …}
::(1 a): The non-zero natural numbers: ℕ_>0 = { 1, 2, 3, …}
:(2): The integers: ℤ = {…, -3, -2, -1, 0, 1, 2, 3, …}
:(3): The rational numbers: ℚ = { p / q: p, q ∈ℤ, q  0 }
:(4): The real numbers: ℝ = { x: x = ⟨ s_n ⟩} where ⟨ s_n ⟩ is a Cauchy sequence in ℚ
:(5): The complex numbers: ℂ = { a + i b: a, b ∈ℝ, i^2 = -1 }


It is possible to categorize numbers further, for example:

:The set of algebraic numbers 𝔸 is the subset of the complex numbers which are roots of polynomials with rational coefficients.  The algebraic numbers include the rational numbers, √(2), and the golden section φ.

:The set of transcendental numbers is the set of all the real numbers which are not algebraic.  The transcendental numbers include π, e and √(2)^√(2).

:The set of prime numbers (sometimes referred to as ℙ) is the subset of the integers which have exactly two positive divisors, 1 and the number itself.  The first several positive primes are 2, 3, 5, 7, 11, 13, …",System
"['Definitions/Imperial', 'Definitions/Mass']",Definition:System,"There are three imperial systems of measurement of mass:

* Avoirdupois

* Apothecaries' weights

* Troy


=== Grain ===
The grain is the imperial unit of mass which is used as the basis of all three of the imperial weight systems.",Definition:Imperial/Mass,,false,"There are three imperial systems of measurement of mass:

* Avoirdupois

* Apothecaries' weights

* Troy


=== Grain ===
The grain is the imperial unit of mass which is used as the basis of all three of the imperial weight systems.",System
['Definitions/Design Theory'],Definition:System,"A Steiner triple system of order $v$ is a BIBD with block size $3$, and each pair of points occurring together in exactly $1$ block (called a triple).",Definition:Steiner Triple System,,false,"A Steiner triple system of order v is a BIBD with block size 3, and each pair of points occurring together in exactly 1 block (called a triple).",System
"['Definitions/Non-Linear Systems', 'Definitions/Differential Equations', 'Definitions/Applied Mathematics', 'Definitions/Branches of Mathematics']",Definition:System,A non-linear system is a system of differential equations which are non-linear.,Definition:Non-Linear System,,false,A non-linear system is a system of differential equations which are non-linear.,System
"['Definitions/Proof Systems', 'Definitions/Propositional Logic', 'Definitions/Predicate Logic']",Definition:System,"Hilbert proof systems are a class of proof systems for propositional and predicate logic.

Their characteristics include:

* The presence of many axioms;
* Typically only Modus Ponendo Ponens as a rule of inference.

Specific instances may deviate from this general scheme at some points.",Definition:Hilbert Proof System,,false,"Hilbert proof systems are a class of proof systems for propositional and predicate logic.

Their characteristics include:

* The presence of many axioms;
* Typically only Modus Ponendo Ponens as a rule of inference.

Specific instances may deviate from this general scheme at some points.",System
['Definitions/Residue Classes'],Definition:System,"Let $m in mathbb Z_{ne 0}$ be a non-zero integer.


Let $S := leftlbrace r_1, r_2, dotsb, r_s rightrbrace$ be a set of integers with the properties that:

:$(1): quad i ne j implies r_i not equiv r_j pmod m$

:$(2): quad forall n in mathbb Z: exists r_i in S: n equiv r_i pmod m$


Then $S$ is a complete residue system modulo $m$.",Definition:Complete Residue System,,false,"Let m ∈ℤ_ 0 be a non-zero integer.


Let S := { r_1, r_2, …, r_s } be a set of integers with the properties that:

:(1):    i  j  r_i ≢r_j  m

:(2):   ∀ n ∈ℤ: ∃ r_i ∈ S: n ≡ r_i  m


Then S is a complete residue system modulo m.",System
"['Definitions/Residue Classes', 'Definitions/Reduced Residue Systems', 'Definitions/Modulo Arithmetic']",Definition:System,"Let $m in mathbb Z_{> 0}$ be a (strictly) positive integer.


The reduced residue system modulo $m$, denoted $mathbb Z'_m$, is the set of all residue classes of $k$ (modulo $m$) which are prime to $m$:

:$mathbb Z'_m = leftlbrace left[!left[ k right]!right]_{ }m in mathbb Z_m: k perp m rightrbrace$


Thus $mathbb Z'_m$ is the set of all coprime residue classes modulo $m$:
:$mathbb Z'_m = leftlbrace left[!left[ a_1 right]!right]_{ }m, left[!left[ a_2 right]!right]_{ }m, ldots, left[!left[ a_{phi left(   right)m}  right]!right]_{ }m rightrbrace$
where:
:$forall k: a_k perp m$
:$phi left(   right)m$ denotes the Euler phi function of $m$.",Definition:Reduced Residue System,,false,"Let m ∈ℤ_> 0 be a (strictly) positive integer.


The reduced residue system modulo m, denoted ℤ'_m, is the set of all residue classes of k (modulo m) which are prime to m:

:ℤ'_m = {[[ k ]]_m ∈ℤ_m: k ⊥ m }


Thus ℤ'_m is the set of all coprime residue classes modulo m:
:ℤ'_m = {[[ a_1 ]]_m, [[ a_2 ]]_m, …, [[ a_ϕ(   )m]]_m }
where:
:∀ k: a_k ⊥ m
:ϕ(   )m denotes the Euler phi function of m.",System
"['Definitions/Set Systems', 'Definitions/Set Theory']",Definition:System,"A set of sets is a set, whose elements are themselves all sets.


Those elements can themselves be assumed to be subsets of some particular fixed set which is frequently referred to as the universe.",Definition:Set of Sets,,false,"A set of sets is a set, whose elements are themselves all sets.


Those elements can themselves be assumed to be subsets of some particular fixed set which is frequently referred to as the universe.",System
"['Definitions/Linear Algebra', 'Definitions/Simultaneous Equations']",Definition:System,"A system of simultaneous equations is a set of equations:

:$forall i in leftlbrace 1, 2, ldots, m rightrbrace : f_i left(   right){x_1, x_2, ldots x_n} = beta_i$


That is:

 
 
 
 
 
 


=== Linear Equations ===
A system of simultaneous linear equations is a set of linear equations:

:$ds forall i in leftlbrace 1, 2, ldots, m rightrbrace : sum_{j mathop = 1}^n alpha_{i j} x_j = beta_i$


That is:

 
 
 
 
 
 


=== Solution ===
Consider the system of simultaneous linear equations:

:$ds forall i in leftlbrace 1, 2, ldots, m rightrbrace : sum_{j mathop = 1}^n alpha_{i j} x_j = beta_i$


That is:

 
 
 
 
 
 


Let $left( x_1, x_2, ldots, x_n right)$ satisfy each of the linear equations in $ds sum_{j mathop = 1}^n alpha_{i j} x_j = beta_i$.

Then $left( x_1, x_2, ldots, x_n right)$ is referred to as a solution (to the system of simultaneous linear equations).

=== Matrix Representation ===
A system of simultaneous linear equations can be (and commonly is) expressed in its matrix representation:
:$mathbf A mathbf x = mathbf b$
where:

$quad mathbf A = begin {bmatrix}
alpha_{1 1} & alpha_{1 2} & cdots & alpha_{1 n} \
alpha_{2 1} & alpha_{2 2} & cdots & alpha_{2 n} \
vdots & vdots & ddots & vdots \
alpha_{m 1} & alpha_{m 2} & cdots & alpha_{m n} \
end {bmatrix}$,  $mathbf x = begin {bmatrix} x_1 \ x_2 \ vdots \ x_n end {bmatrix}$, $mathbf b = begin {bmatrix} beta_1 \ beta_2 \ vdots \ beta_m end {bmatrix}$

are matrices.


=== Matrix of Coefficients ===
Consider the system of simultaneous linear equations can be expressed as:

:$ds forall i in leftlbrace 1, 2, ldots, m rightrbrace : sum_{j mathop = 1}^n alpha_{i j} x_j = beta_i$

expressed in matrix representation as:
:$mathbf A mathbf x = mathbf b$


The matrix $mathbf A$ is known as the matrix of coeffficients of the system.

=== Augmented Matrix ===
Consider the system of simultaneous linear equations:

:$ds forall i in leftlbrace 1, 2, ldots, m rightrbrace : sum_{j mathop = 1}^n alpha_{i j} x_j = beta_i$

expressed in matrix representation as:
:$mathbf A mathbf x = mathbf b$


Let $begin {bmatrix} mathbf A & mathbf b end {bmatrix}$ be the block matrix formed from $mathbf A$ and $mathbf b$.

Then $begin {bmatrix} mathbf A & mathbf b end {bmatrix}$ is known as the augmented matrix of the system.


Thus:

$quad begin {bmatrix} mathbf A & mathbf b end {bmatrix} = begin {bmatrix}
alpha_{1 1} & alpha_{1 2} & cdots & alpha_{1 n} & beta_1 \
alpha_{2 1} & alpha_{2 2} & cdots & alpha_{2 n} & beta_2 \
vdots & vdots & ddots & vdots & vdots \
alpha_{m 1} & alpha_{m 2} & cdots & alpha_{m n} & beta_m \
end {bmatrix}$",Definition:Simultaneous Equations,,false,"A system of simultaneous equations is a set of equations:

:∀ i ∈{ 1, 2, …, m } : f_i (   )x_1, x_2, … x_n = β_i


That is:

 
 
 
 
 
 


=== Linear Equations ===
A system of simultaneous linear equations is a set of linear equations:

:∀ i ∈{ 1, 2, …, m } : ∑_j  = 1^n α_i j x_j = β_i


That is:

 
 
 
 
 
 


=== Solution ===
Consider the system of simultaneous linear equations:

:∀ i ∈{ 1, 2, …, m } : ∑_j  = 1^n α_i j x_j = β_i


That is:

 
 
 
 
 
 


Let ( x_1, x_2, …, x_n ) satisfy each of the linear equations in ∑_j  = 1^n α_i j x_j = β_i.

Then ( x_1, x_2, …, x_n ) is referred to as a solution (to the system of simultaneous linear equations).

=== Matrix Representation ===
A system of simultaneous linear equations can be (and commonly is) expressed in its matrix representation:
:𝐀𝐱 = 𝐛
where:

𝐀 = [ α_1 1 α_1 2     ⋯ α_1 n; α_2 1 α_2 2     ⋯ α_2 n;     ⋮     ⋮     ⋱     ⋮; α_m 1 α_m 2     ⋯ α_m n;       ],  𝐱 = [ x_1; x_2;   ⋮; x_n ], 𝐛 = [ β_1; β_2;   ⋮; β_m ]

are matrices.


=== Matrix of Coefficients ===
Consider the system of simultaneous linear equations can be expressed as:

:∀ i ∈{ 1, 2, …, m } : ∑_j  = 1^n α_i j x_j = β_i

expressed in matrix representation as:
:𝐀𝐱 = 𝐛


The matrix 𝐀 is known as the matrix of coeffficients of the system.

=== Augmented Matrix ===
Consider the system of simultaneous linear equations:

:∀ i ∈{ 1, 2, …, m } : ∑_j  = 1^n α_i j x_j = β_i

expressed in matrix representation as:
:𝐀𝐱 = 𝐛


Let [ 𝐀 𝐛 ] be the block matrix formed from 𝐀 and 𝐛.

Then [ 𝐀 𝐛 ] is known as the augmented matrix of the system.


Thus:

[ 𝐀 𝐛 ] = [ α_1 1 α_1 2     ⋯ α_1 n   β_1; α_2 1 α_2 2     ⋯ α_2 n   β_2;     ⋮     ⋮     ⋱     ⋮     ⋮; α_m 1 α_m 2     ⋯ α_m n   β_m;       ]",System
"['Definitions/Physics', 'Definitions/Units of Measurement', 'Definitions/Systems of Measurement']",Definition:System,A system of measurement is a set of fundamental units of measurement with which one can measure any measurable physical property.,Definition:System of Measurement,,false,A system of measurement is a set of fundamental units of measurement with which one can measure any measurable physical property.,System
"['Definitions/Systems of Measurement', 'Definitions/US Volume System']",Definition:System,"The US volume system is a system of measurement based on the imperial system and adapted by the emergent  .


=== Pint ===
The (US) pint is a unit of volume used in the United States.

 
 
 
 
 
 
 
 

=== Quart ===
The (US) quart is a unit of volume used in the United States.

 
 
 
 
 
 
 
 

=== Gallon ===
The (US) gallon is a unit of volume used in the United States.

It is defined as $231$ cubic inches, which is exactly $3.78541 , 1784$ litres.


=== Conversion Factors ===


=== Symbol ===
 ",Definition:US Volume System,,false,"The US volume system is a system of measurement based on the imperial system and adapted by the emergent  .


=== Pint ===
The (US) pint is a unit of volume used in the United States.

 
 
 
 
 
 
 
 

=== Quart ===
The (US) quart is a unit of volume used in the United States.

 
 
 
 
 
 
 
 

=== Gallon ===
The (US) gallon is a unit of volume used in the United States.

It is defined as 231 cubic inches, which is exactly 3.78541   1784 litres.


=== Conversion Factors ===


=== Symbol ===
 ",System
['Definitions/Order Theory'],Definition:System,"Let $L = left({S, preceq}right)$ be an ordered set.


The system of $L$ is an ordered subset of $L$.",Definition:System (Order Theory),,false,"Let L = (S, ≼) be an ordered set.


The system of L is an ordered subset of L.",System
"['Definitions/Systems of Measurement', 'Definitions/CGS']",Definition:System,"CGS is the centimetre-gram-second standard system of units of measurement.


This system is rarely used nowadays, the SI units having largely taken over.


=== CGS Base Units ===
The base units of the CGS system are as follows:

 
 
 
 
 
 
 
| centimetre
| $mathrm{cm}$
| Length
| $l$
 
| gram
| $mathrm g$
| Mass
| $m$
 
| second
| $mathrm s$
| Time
| $t$
 

=== CGS Derived Units ===
The derived units of the CGS system include the following:


==== Dyne ====
The dyne is the CGS unit of force.

It is defined as being:

:The amount of force required to accelerate a mass of one gram at a rate of one centimetre per second squared.


This arises from Newton's Second Law of Motion, which states that such an acceleration exists under the influence of such a force.


=== Conversion Factors ===


=== Symbol ===
 

=== Base Units ===
The CGS base units of the dyne are:

:$mathrm {dyn} := mathrm g , mathrm {cm} , mathrm s^{-2}$
where:
:$mathrm g$ denotes grams
:$mathrm {cm}$ denotes centimetres
:$mathrm s$ denotes seconds (of time).

==== Erg ====
The erg is the CGS unit of energy:


It is defined as being:

:the energy transferred to (or work done on) a body when a force of $1$ dyne acts on that body in the direction of the force's motion through a distance of $1$ centimetre.

Thus:
:$1 , mathrm {erg} = 1 , mathrm {dyn} , mathrm {cm}$


=== Conversion Factors ===


=== Symbol ===
 

=== Base Units ===
The CGS base units of the erg are:

:$1  mathrm {erg} = 1  mathrm g  mathrm {cm}^2  mathrm s^{-2}$
where:
:$mathrm g$ denotes grams
:$mathrm {cm}$ denotes centimetres
:$mathrm s$ denotes seconds (of time).

Category:Definitions/Units of Measurement
Category:Definitions/CGS",Definition:CGS,,false,"CGS is the centimetre-gram-second standard system of units of measurement.


This system is rarely used nowadays, the SI units having largely taken over.


=== CGS Base Units ===
The base units of the CGS system are as follows:

 
 
 
 
 
 
 
| centimetre
| cm
| Length
| l
 
| gram
| g
| Mass
| m
 
| second
| s
| Time
| t
 

=== CGS Derived Units ===
The derived units of the CGS system include the following:


==== Dyne ====
The dyne is the CGS unit of force.

It is defined as being:

:The amount of force required to accelerate a mass of one gram at a rate of one centimetre per second squared.


This arises from Newton's Second Law of Motion, which states that such an acceleration exists under the influence of such a force.


=== Conversion Factors ===


=== Symbol ===
 

=== Base Units ===
The CGS base units of the dyne are:

:dyn := g cm s^-2
where:
:g denotes grams
:cm denotes centimetres
:s denotes seconds (of time).

==== Erg ====
The erg is the CGS unit of energy:


It is defined as being:

:the energy transferred to (or work done on) a body when a force of 1 dyne acts on that body in the direction of the force's motion through a distance of 1 centimetre.

Thus:
:1  erg = 1  dyn cm


=== Conversion Factors ===


=== Symbol ===
 

=== Base Units ===
The CGS base units of the erg are:

:1  erg = 1  g cm^2  s^-2
where:
:g denotes grams
:cm denotes centimetres
:s denotes seconds (of time).

Category:Definitions/Units of Measurement
Category:Definitions/CGS",System
['Definitions/Residue Classes'],Definition:System,"Let $m in mathbb Z_{> 0}$.

The least positive reduced residue system modulo $m$ is the set of integers:
:$leftlbrace a_1, a_2, ldots, a_{phi left(   right)m}  rightrbrace$
with the following properties:
:$phi left(   right)m$ is the Euler $phi$ function
:$forall i: 0 < a_i < m$
:each of which is prime to $m$
:no two of which are congruent modulo $m$.",Definition:Reduced Residue System/Least Positive,,false,"Let m ∈ℤ_> 0.

The least positive reduced residue system modulo m is the set of integers:
:{ a_1, a_2, …, a_ϕ(   )m}
with the following properties:
:ϕ(   )m is the Euler ϕ function
:∀ i: 0 < a_i < m
:each of which is prime to m
:no two of which are congruent modulo m.",System
['Definitions/Fiber Bundles'],Definition:System,"Let $B = left( E, M, pi, F right)$ be a fiber bundle. 

Let $mathcal U = leftlbrace U_alpha subseteq M: alpha in I rightrbrace$ be an open cover of $M$ with index set $I$. 

Let $left( U_alpha, chi_alpha right)$ be local trivializations for all $alpha in I$. 


The set $leftlbrace left( U_alpha, chi_alpha right): alpha in I rightrbrace$ is called a system of local trivializations of $E$ on $M$.",Definition:Fiber Bundle/System of Local Trivializations,,false,"Let B = ( E, M, π, F ) be a fiber bundle. 

Let 𝒰 = { U_α⊆ M: α∈ I } be an open cover of M with index set I. 

Let ( U_α, χ_α) be local trivializations for all α∈ I. 


The set {( U_α, χ_α): α∈ I } is called a system of local trivializations of E on M.",System
['Definitions/Differential Equations'],Definition:System,"A system of differential equations is a set of simultaneous differential equations.

The solutions for each of the differential equations are in general expected to be consistent.


=== Autonomous System ===
A system of differential equations is autonomous if all of the differential equations which it comprises are themselves autonomous.


Category:Definitions/Differential Equations

Category:Definitions/Differential Equations",Definition:Differential Equation/System,,false,"A system of differential equations is a set of simultaneous differential equations.

The solutions for each of the differential equations are in general expected to be consistent.


=== Autonomous System ===
A system of differential equations is autonomous if all of the differential equations which it comprises are themselves autonomous.


Category:Definitions/Differential Equations

Category:Definitions/Differential Equations",System
"['Definitions/Reference Tables', 'Definitions/Tools', 'Definitions/Proof Techniques']",Definition:Table,"A reference table is a set of pages, arranged usually in book form, containing arrays of (usually) numbers arranged in rows and columns for ease of look-up.

They have been generally superseded by computers now, but facility in their use is generally considered advantageous.",Definition:Reference Table,,false,"A reference table is a set of pages, arranged usually in book form, containing arrays of (usually) numbers arranged in rows and columns for ease of look-up.

They have been generally superseded by computers now, but facility in their use is generally considered advantageous.",Table
"['Definitions/Cayley Tables', 'Definitions/Abstract Algebra']",Definition:Table,"A Cayley table is a technique for describing an algebraic structure (usually a finite group) by putting all the products in a square array:

$qquad begin {array} {c|cccc}
circ & a & b & c & d \
hline
a & a & a & b & a \
b & b & c & a & d \
c & d & e & f & a \
d & c & d & a & b \
end {array}$


The column down the   denotes the first (leading) operand of the operation.

The row across the top denotes the second (following) operand of the operation.

Thus, in the above Cayley table:
:$c circ a = d$


If desired, the symbol denoting the operation itself can be put in the upper left corner, but this is not essential if there is no ambiguity.


The order in which the rows and columns are placed is immaterial.

However, it is conventional, when representing an algebraic structure with an identity element, to place that element at the head of the first row and column.


=== Entry ===
The occurrences in a Cayley table of the elements of the algebraic structure it defines are called the entries of the Cayley table.


Category:Definitions/Cayley Tables",Definition:Cayley Table,,false,"A Cayley table is a technique for describing an algebraic structure (usually a finite group) by putting all the products in a square array:

[ ∘ a b c d; a a a b a; b b c a d; c d e f a; d c d a b;   ]


The column down the   denotes the first (leading) operand of the operation.

The row across the top denotes the second (following) operand of the operation.

Thus, in the above Cayley table:
:c ∘ a = d


If desired, the symbol denoting the operation itself can be put in the upper left corner, but this is not essential if there is no ambiguity.


The order in which the rows and columns are placed is immaterial.

However, it is conventional, when representing an algebraic structure with an identity element, to place that element at the head of the first row and column.


=== Entry ===
The occurrences in a Cayley table of the elements of the algebraic structure it defines are called the entries of the Cayley table.


Category:Definitions/Cayley Tables",Table
['Definitions/Proof Systems'],Definition:Tableau Proof,"A tableau proof for a proof system is a technique for presenting a logical argument in the form of a formal proof in a straightforward, standard form.

On  , the proof system is usually natural deduction.


A tableau proof is a sequence of lines specifying the order of premises, assumptions, inferences and conclusion in support of an argument.


Each line of a tableau proof has a particular format. It consists of the following parts:

* Line: The line number of the proof. This is a simple numbering from 1 upwards.
* Pool: The list of all the lines containing the pool of assumptions for the formula introduced on this line.
* Formula: The propositional formula introduced on this line.
* Rule: The justification for introducing this line. This should be the rule of inference being used to derive this line.
* Depends on: The lines (if any) upon which this line directly depends. For premises and assumptions, this field will be empty.


Optionally, a comment may be added to explicitly point out possible intricacies.

If any assumptions are discharged on a certain line, for the sake of clarity it is preferred that such be mentioned explicitly in a comment.


At the end of a tableau proof, the only lines upon which the proof depends may be those which contain the premises.


=== Length ===
The length of a tableau proof is the number of lines it has.


Category:Definitions/Proof Systems",Definition:Tableau Proof (Formal Systems),,false,"A tableau proof for a proof system is a technique for presenting a logical argument in the form of a formal proof in a straightforward, standard form.

On  , the proof system is usually natural deduction.


A tableau proof is a sequence of lines specifying the order of premises, assumptions, inferences and conclusion in support of an argument.


Each line of a tableau proof has a particular format. It consists of the following parts:

* Line: The line number of the proof. This is a simple numbering from 1 upwards.
* Pool: The list of all the lines containing the pool of assumptions for the formula introduced on this line.
* Formula: The propositional formula introduced on this line.
* Rule: The justification for introducing this line. This should be the rule of inference being used to derive this line.
* Depends on: The lines (if any) upon which this line directly depends. For premises and assumptions, this field will be empty.


Optionally, a comment may be added to explicitly point out possible intricacies.

If any assumptions are discharged on a certain line, for the sake of clarity it is preferred that such be mentioned explicitly in a comment.


At the end of a tableau proof, the only lines upon which the proof depends may be those which contain the premises.


=== Length ===
The length of a tableau proof is the number of lines it has.


Category:Definitions/Proof Systems",Tableau Proof
"['Definitions/Propositional Tableaus', 'Definitions/Proof Systems']",Definition:Tableau Proof,"Let $mathbf H$ be a set of WFFs of propositional logic.

Let $mathbf A$ be a WFF.


A tableau proof of $mathbf A$ from $mathbf H$ is a tableau confutation of $mathbf H cup leftlbrace neg mathbf A rightrbrace$.


This definition also applies when $mathbf H = varnothing$.

Then a tableau proof of $mathbf A$ is a tableau confutation of $leftlbrace neg mathbf A rightrbrace$.


If there exists a tableau proof of $mathbf A$ from $mathbf H$, one can write:
:$mathbf H vdash_{mathrm{PT} } mathbf A$

Specifically, the notation:
:$vdash_{mathrm{PT} } mathbf A$
means that there exists a tableau proof of $mathbf A$.


=== Proof System ===
",Definition:Tableau Proof (Propositional Tableaus),,false,"Let 𝐇 be a set of WFFs of propositional logic.

Let 𝐀 be a WFF.


A tableau proof of 𝐀 from 𝐇 is a tableau confutation of 𝐇∪{𝐀}.


This definition also applies when 𝐇 = ∅.

Then a tableau proof of 𝐀 is a tableau confutation of {𝐀}.


If there exists a tableau proof of 𝐀 from 𝐇, one can write:
:𝐇⊢_PT𝐀

Specifically, the notation:
:⊢_PT𝐀
means that there exists a tableau proof of 𝐀.


=== Proof System ===
",Tableau Proof
"['Definitions/Tangents', 'Definitions/Analytic Geometry']",Definition:Tangent,"Let $f: mathbb R to mathbb R$ be a real function.

Let the graph of $f$ be depicted on a Cartesian plane.


:


Let $A = left( x, f left(   right)x right)$ be a point on $G$.


The tangent to $f$ at $A$ is defined as:
:$ds lim_{h mathop to 0} frac {f left(   right){x + h} - f left(   right)x} h$


Thus the tangent to $f$ at $x$ can be considered as the secant $AB$ to $G$ where:
:$B = left( x + h, f left(   right){x + h}  right)$
as $B$ gets closer and closer to $A$.

By taking $h$ smaller and smaller, the secant approaches more and more closely the [[Definition:Tangent Line|tangent]] to $G$ at $A$.


Hence the tangent to $f$ is a straight line which intersects the graph of $f$ locally at a single point.


:


In the above diagram, the tangent is the straight line passing through $A$.


=== Tangent to Circle ===
 
: 
:A straight line is said to touch a circle which, meeting the circle and being produced, does not cut the circle.
 ''
 

:

In the above diagram, the line is tangent to the circle at the point $C$.


=== Tangent Circles ===
 
: 
:Circles are said to touch one another which, meeting one another, do not cut one another.
 ''
 

:

In the above diagram, the two circles are tangent to each other at the point $C$.


Category:Definitions/Circles
Category:Definitions/Tangents

=== Point of Contact ===
Let $f: mathbb R to mathbb R$ be a real function.

Let the graph of $f$ be depicted on a Cartesian plane.


:


Let $A = left( x, f left(   right)x right)$ be a point on $G$.

Let $mathcal L$ be tangent to $f$ at $A$.

Then $A$ is known as the point of contact of $mathcal L$ to $f$.",Definition:Tangent Line,,false,"Let f: ℝ→ℝ be a real function.

Let the graph of f be depicted on a Cartesian plane.


:


Let A = ( x, f (   )x ) be a point on G.


The tangent to f at A is defined as:
:lim_h → 0f (   )x + h - f (   )x/h


Thus the tangent to f at x can be considered as the secant AB to G where:
:B = ( x + h, f (   )x + h)
as B gets closer and closer to A.

By taking h smaller and smaller, the secant approaches more and more closely the [[Definition:Tangent Line|tangent]] to G at A.


Hence the tangent to f is a straight line which intersects the graph of f locally at a single point.


:


In the above diagram, the tangent is the straight line passing through A.


=== Tangent to Circle ===
 
: 
:A straight line is said to touch a circle which, meeting the circle and being produced, does not cut the circle.
 ”
 

:

In the above diagram, the line is tangent to the circle at the point C.


=== Tangent Circles ===
 
: 
:Circles are said to touch one another which, meeting one another, do not cut one another.
 ”
 

:

In the above diagram, the two circles are tangent to each other at the point C.


Category:Definitions/Circles
Category:Definitions/Tangents

=== Point of Contact ===
Let f: ℝ→ℝ be a real function.

Let the graph of f be depicted on a Cartesian plane.


:


Let A = ( x, f (   )x ) be a point on G.

Let ℒ be tangent to f at A.

Then A is known as the point of contact of ℒ to f.",Tangent
['Definitions/Tangent Function'],Definition:Tangent,":

In the above right triangle, we are concerned about the angle $theta$.

The tangent of $angle theta$ is defined as being $dfrac{text{Opposite}} {text{Adjacent}}$.",Definition:Tangent Function/Definition from Triangle,,false,":

In the above right triangle, we are concerned about the angle θ.

The tangent of ∠θ is defined as being OppositeAdjacent.",Tangent
['Definitions/Tangent Function'],Definition:Tangent,"Let $x in mathbb R$ be a real number.

The real function $tan x$ is defined as:

:$tan x = dfrac {sin x} {cos x}$

where:
: $sin x$ is the sine of $x$
: $cos x$ is the cosine of $x$.

The definition is valid for all $x in mathbb R$ such that $cos x ne 0$.",Definition:Tangent Function/Real,,false,"Let x ∈ℝ be a real number.

The real function tan x is defined as:

:tan x = sin xcos x

where:
: sin x is the sine of x
: cos x is the cosine of x.

The definition is valid for all x ∈ℝ such that cos x  0.",Tangent
['Definitions/Tangent Function'],Definition:Tangent,"Let $z in mathbb C$ be a complex number.

The complex function $tan z$ is defined as:

:$tan z = dfrac {sin z} {cos z}$

where:
: $sin z$ is the sine of $z$
: $cos z$ is the cosine of $z$.

The definition is valid for all $z in mathbb C$ such that $cos z ne 0$.",Definition:Tangent Function/Complex,,false,"Let z ∈ℂ be a complex number.

The complex function tan z is defined as:

:tan z = sin zcos z

where:
: sin z is the sine of z
: cos z is the cosine of z.

The definition is valid for all z ∈ℂ such that cos z  0.",Tangent
['Definitions/Abelian Groups'],Definition:Tensor Product,"Let $A$ and $B$ be abelian groups.


=== Definition 1: by universal property ===

Their tensor product is a pair $left( A otimes B, theta right)$ where:
:$A otimes B$ is an abelian group
:$theta : A times B to A otimes B$ is a biadditive mapping such that, for every ordered pair $left( C, omega right)$ where:
:$C$ is an abelian group
:$omega : A times B to C$ is a biadditive mapping
there exists a unique group homomorphism $g : A otimes B to C$ such that $omega = g circ theta$.


=== Definition 2: construction ===

Their tensor product is the pair $left( A otimes B, theta right)$ where:
:$A otimes B$ is the quotient of the free abelian group $mathbb Z^{left( A times B right) }$ on the cartesian product $A times B$ by the subgroup generated by the elements of the form:
:::$left( a_1 + a_2, b right) - left( a_1, b right) - left( a_2, b right)$
:::$left( a, b_1 + b_2 right) - left( a, b_1 right) - left( a, b_2 right)$
::for $a, a_1, a_2 in A$, $b, b_1, b_2 in B$, where we denote $left( a, b right)$ for its image under the canonical mapping $A times B to mathbb Z^{left( A times B right) }$.
:$theta : A times B to A otimes B$ is the composition of the canonical mapping $A times B to mathbb Z^{left( A times B right) }$ with the quotient group epimorphism $mathbb Z^{left( A times B right) } to A otimes B$.",Definition:Tensor Product of Abelian Groups,,false,"Let A and B be abelian groups.


=== Definition 1: by universal property ===

Their tensor product is a pair ( A ⊗ B, θ) where:
:A ⊗ B is an abelian group
:θ : A × B → A ⊗ B is a biadditive mapping such that, for every ordered pair ( C, ω) where:
:C is an abelian group
:ω : A × B → C is a biadditive mapping
there exists a unique group homomorphism g : A ⊗ B → C such that ω = g ∘θ.


=== Definition 2: construction ===

Their tensor product is the pair ( A ⊗ B, θ) where:
:A ⊗ B is the quotient of the free abelian group ℤ^( A × B ) on the cartesian product A × B by the subgroup generated by the elements of the form:
:::( a_1 + a_2, b ) - ( a_1, b ) - ( a_2, b )
:::( a, b_1 + b_2 ) - ( a, b_1 ) - ( a, b_2 )
::for a, a_1, a_2 ∈ A, b, b_1, b_2 ∈ B, where we denote ( a, b ) for its image under the canonical mapping A × B →ℤ^( A × B ).
:θ : A × B → A ⊗ B is the composition of the canonical mapping A × B →ℤ^( A × B ) with the quotient group epimorphism ℤ^( A × B ) → A ⊗ B.",Tensor Product
"['Definitions/Module Theory', 'Definitions/Tensor Algebra', 'Definitions/Homological Algebra']",Definition:Tensor Product,"=== Commutative ring ===

Let $R$ be a commutative ring with unity.

Let $M$ and $N$ be $R$-modules.


=== Definition 1 ===

Their tensor product is a pair $left( M otimes_R N, theta right)$ where:
:$M otimes_R N$ is an $R$-module
:$theta : M times N to M otimes_R N$ is an $R$-bilinear mapping
satisfying the following universal property:
:For every pair $left( P, omega right)$ of an $R$-module and an $R$-bilinear mapping $omega : M times N to P$, there exists a unique $R$-module homomorphism $f: M otimes_R N to P$ with $omega = f circ theta$.


=== Definition 2 ===

Their tensor product is the pair $left( M otimes_R N, theta right)$, where:
:$M otimes_R N$ is the quotient of the free $R$-module $R^{left( M times N right) }$ on the direct product $M times N$, by the submodule generated by the set of elements of the form:
::$left( lambda m_1 + m_2, n right) - lambda left( m_1, n right) - left( m_2, n right)$
::$left( m, lambda n_1 + n_2 right) - lambda left( m, n_1 right) - left( m, n_2 right)$
::for $m, m_1, m_2 in M$, $n, n_1, n_2 in N$ and $lambda in R$, where we denote $left( m, n right)$ for its image under the canonical mapping $M times N to R^{left( M times N right) }$.
:$theta : M times N to M otimes_R N$ is the composition of the canonical mapping $M times N to R^{left( M times N right) }$ with the quotient module homomorphism $R^{left( M times N right) } to M otimes_R N$.


=== Noncommutative ring ===

Let $R$ be a ring.

Let $M$ be a $R$-right module.

Let $N$ be a $R$-left module.


First construct a left module as a direct sum of all free left modules with a basis that is a single ordered pair in $M times N$ which is denoted $R left(   right){m, n}$.

:$T = ds bigoplus_{s mathop in M mathop times N} R s$


That this is indeed a module is demonstrated in Tensor Product is Module.


Next for all $m, m' in M$, $n, n' in N$ and $r in R$ we construct the following free left modules.

:$L_{m, m', n}$ with a basis of $left( m + m', n right)$, $left( m, n right)$ and $left( m', n right)$
:$R_{m, n, n'}$ with a basis of $left( m, n + n' right)$, $left( m, n right)$ and $left( m, n' right)$
:$A_{r, m, n}$ with a basis of $r left( m, n right)$ and $left( m r, n right)$
:$B_{r, m, n}$ with a basis of $r left( m, n right)$ and $left( m, r n right)$

Let:

:$D = ds bigoplus_{r in R, n, n' in N, m, m' in M}  left(   right){L_{m, m', n} oplus R_{m, n, n'} oplus A_{r, m, n} oplus B_{r, m, n} }$

The tensor product $M otimes_R N$ is then our quotient module $T / D$.",Definition:Tensor Product of Modules,,false,"=== Commutative ring ===

Let R be a commutative ring with unity.

Let M and N be R-modules.


=== Definition 1 ===

Their tensor product is a pair ( M ⊗_R N, θ) where:
:M ⊗_R N is an R-module
:θ : M × N → M ⊗_R N is an R-bilinear mapping
satisfying the following universal property:
:For every pair ( P, ω) of an R-module and an R-bilinear mapping ω : M × N → P, there exists a unique R-module homomorphism f: M ⊗_R N → P with ω = f ∘θ.


=== Definition 2 ===

Their tensor product is the pair ( M ⊗_R N, θ), where:
:M ⊗_R N is the quotient of the free R-module R^( M × N ) on the direct product M × N, by the submodule generated by the set of elements of the form:
::( λ m_1 + m_2, n ) - λ( m_1, n ) - ( m_2, n )
::( m, λ n_1 + n_2 ) - λ( m, n_1 ) - ( m, n_2 )
::for m, m_1, m_2 ∈ M, n, n_1, n_2 ∈ N and λ∈ R, where we denote ( m, n ) for its image under the canonical mapping M × N → R^( M × N ).
:θ : M × N → M ⊗_R N is the composition of the canonical mapping M × N → R^( M × N ) with the quotient module homomorphism R^( M × N ) → M ⊗_R N.


=== Noncommutative ring ===

Let R be a ring.

Let M be a R-right module.

Let N be a R-left module.


First construct a left module as a direct sum of all free left modules with a basis that is a single ordered pair in M × N which is denoted R (   )m, n.

:T = ⊕_s ∈ M × N R s


That this is indeed a module is demonstrated in Tensor Product is Module.


Next for all m, m' ∈ M, n, n' ∈ N and r ∈ R we construct the following free left modules.

:L_m, m', n with a basis of ( m + m', n ), ( m, n ) and ( m', n )
:R_m, n, n' with a basis of ( m, n + n' ), ( m, n ) and ( m, n' )
:A_r, m, n with a basis of r ( m, n ) and ( m r, n )
:B_r, m, n with a basis of r ( m, n ) and ( m, r n )

Let:

:D = ⊕_r ∈ R, n, n' ∈ N, m, m' ∈ M(   )L_m, m', n⊕ R_m, n, n'⊕ A_r, m, n⊕ B_r, m, n

The tensor product M ⊗_R N is then our quotient module T / D.",Tensor Product
"['Definitions/Predicate Logic', 'Definitions/Language of Predicate Logic']",Definition:Term,"Part of specifying the language of predicate logic $mathcal L_1$ is the introduction of terms.


The terms of $mathcal L_1$ are identified by the following bottom-up grammar:

 
 
 
 

Colloquially, we can think of a term as an expression signifying an object.",Definition:Language of Predicate Logic/Formal Grammar/Term,,false,"Part of specifying the language of predicate logic ℒ_1 is the introduction of terms.


The terms of ℒ_1 are identified by the following bottom-up grammar:

 
 
 
 

Colloquially, we can think of a term as an expression signifying an object.",Term
['Definitions/Predicate Logic'],Definition:Term,"Both the subject and the predicate of a simple statement are referred to as its (logical) terms.


Note that this use of the word term is a more specialized use of the word term as used in algebra.",Definition:Logical Term,,false,"Both the subject and the predicate of a simple statement are referred to as its (logical) terms.


Note that this use of the word term is a more specialized use of the word term as used in algebra.",Term
"['Definitions/Algebra', 'Definitions/Symbolic Logic']",Definition:Term,"A term is either a variable or a constant.


Let $a circ b$ be an expression.

Then each of $a$ and $b$ are known as the terms of the expression.


The word term is usually used when the operation $circ$ is addition, that is $+$.",Definition:Term of Expression,,false,"A term is either a variable or a constant.


Let a ∘ b be an expression.

Then each of a and b are known as the terms of the expression.


The word term is usually used when the operation ∘ is addition, that is +.",Term
['Definitions/Polynomial Theory'],Definition:Term,"Let $P = a_n x^n + a_{n - 1} x^{n - 1} + cdots + a_1 x + a_0$ be a polynomial.

Each of the expressions $a_i x^i$, for $0 le i le n$, is referred to as a term of $P$.",Definition:Polynomial/Term,,false,"Let P = a_n x^n + a_n - 1 x^n - 1 + ⋯ + a_1 x + a_0 be a polynomial.

Each of the expressions a_i x^i, for 0 ≤ i ≤ n, is referred to as a term of P.",Term
['Definitions/Fractions'],Definition:Term,"The terms of a fraction are referred to as the numerator and the denominator:

=== Numerator ===
Let $dfrac a b$ be a fraction.

The term $a$ is known as the numerator of $dfrac a b$.

=== Denominator ===
Let $dfrac a b$ be a fraction.

The term $b$ is known as the denominator of $dfrac a b$.

A helpful mnemonic to remember which goes on top and which goes on the bottom is ""Numerator Over Denominator"", which deserves a ""nod"" for being correct.",Definition:Fraction/Term,,false,"The terms of a fraction are referred to as the numerator and the denominator:

=== Numerator ===
Let a b be a fraction.

The term a is known as the numerator of a b.

=== Denominator ===
Let a b be a fraction.

The term b is known as the denominator of a b.

A helpful mnemonic to remember which goes on top and which goes on the bottom is ""Numerator Over Denominator"", which deserves a ""nod"" for being correct.",Term
['Definitions/Sequences'],Definition:Term,"The elements of a sequence are known as its terms.


Let $leftlangle x_n rightrangle$ be a sequence.

Then the $k$th term of $leftlangle x_n rightrangle$ is the ordered pair $left( k, x_k right)$.


=== Index ===
Let $leftlangle x_n rightrangle$ be a sequence.

Let $x_k$ be the $k$th term of $leftlangle x_n rightrangle$.

Then the integer $k$ is known as the index of $x_k$.",Definition:Term of Sequence,,false,"The elements of a sequence are known as its terms.


Let ⟨ x_n ⟩ be a sequence.

Then the kth term of ⟨ x_n ⟩ is the ordered pair ( k, x_k ).


=== Index ===
Let ⟨ x_n ⟩ be a sequence.

Let x_k be the kth term of ⟨ x_n ⟩.

Then the integer k is known as the index of x_k.",Term
['Definitions/Indexed Families'],Definition:Term,"Let $I$ and $S$ be sets.

Let $x: I to S$ be a mapping.

Let $x_i$ denote the image of an element $i in I$ of the domain $I$ of $x$.

Let $leftlangle x_i rightrangle_{i mathop in I}$ denote the set of the images of all the element $i in I$ under $x$.


The image of $x$ at an index $i$ is referred to as a term of the (indexed) family, and is denoted $x_i$.


=== Notation ===
The family of elements $x$ of $S$ indexed by $I$ is often seen with one of the following notations:

:$leftlangle x_i rightrangle_{i mathop in I}$

:$left( x_i right)_{i mathop in I}$

:$leftlbrace x_i rightrbrace_{i mathop in I}$


There is little consistency in the literature, but $left( x_i right)_{i mathop in I}$ is perhaps most common.

The preferred notation on   is $leftlangle x_i rightrangle_{i mathop in I}$.

The subscripted $i in I$ is often left out, if it is obvious in the particular context.


Note the use of $x_i$ to denote the image of the index $i$ under the indexing function $x$.

As $x$ is actually a mapping, one would expect the conventional notation $x left(   right)i$.

However, this is generally not used, and $x_i$ is used instead.


Category:Definitions/Indexed Families",Definition:Indexing Set/Term,,false,"Let I and S be sets.

Let x: I → S be a mapping.

Let x_i denote the image of an element i ∈ I of the domain I of x.

Let ⟨ x_i ⟩_i ∈ I denote the set of the images of all the element i ∈ I under x.


The image of x at an index i is referred to as a term of the (indexed) family, and is denoted x_i.


=== Notation ===
The family of elements x of S indexed by I is often seen with one of the following notations:

:⟨ x_i ⟩_i ∈ I

:( x_i )_i ∈ I

:{ x_i }_i ∈ I


There is little consistency in the literature, but ( x_i )_i ∈ I is perhaps most common.

The preferred notation on   is ⟨ x_i ⟩_i ∈ I.

The subscripted i ∈ I is often left out, if it is obvious in the particular context.


Note the use of x_i to denote the image of the index i under the indexing function x.

As x is actually a mapping, one would expect the conventional notation x (   )i.

However, this is generally not used, and x_i is used instead.


Category:Definitions/Indexed Families",Term
['Definitions/Ordered Tuples'],Definition:Term,"Let $n in mathbb N_{>0}$.

Let $leftlangle a_k rightrangle_{k mathop in mathbb N^*_n}$ be an ordered tuple.

The ordered pair $left( k, a_k right)$ is called the $k$th term of the ordered tuple for each $k in mathbb N^*_n$.",Definition:Ordered Tuple/Term,,false,"Let n ∈ℕ_>0.

Let ⟨ a_k ⟩_k ∈ℕ^*_n be an ordered tuple.

The ordered pair ( k, a_k ) is called the kth term of the ordered tuple for each k ∈ℕ^*_n.",Term
['Definitions/Language Definitions'],Definition:Term,"A term is a noun which is assigned a specified definition within mathematics.

In a more specialized context, the word term is used for an element  of an expression.

Category:Definitions/Language Definitions",Definition:Term (Natural Language),noun,true,"A term is a noun which is assigned a specified definition within mathematics.

In a more specialized context, the word term is used for an element  of an expression.

Category:Definitions/Language Definitions",Term
['Definitions/Top'],Definition:Top,"Top is a constant of propositional logic interpreted to mean the canonical, undoubted tautology whose truth nobody could possibly ever question.

The symbol used is $top$.",Definition:Top (Logic),,false,"Top is a constant of propositional logic interpreted to mean the canonical, undoubted tautology whose truth nobody could possibly ever question.

The symbol used is ⊤.",Top
"['Definitions/Top of Lattice', 'Definitions/Lattice Theory']",Definition:Top,"Let $left( S, vee, wedge, preceq right)$ be a lattice.


=== Definition 1 ===
Let $left( S, vee, wedge, preceq right)$ be a lattice.

Let $S$ admit a greatest element $top$.


Then $top$ is called the top of $S$.

=== Definition 2 ===
Let $left( S, vee, wedge, preceq right)$ be a lattice.

Let $wedge$ have an identity element $top$.


Then $top$ is called the top of $S$.",Definition:Top of Lattice,,false,"Let ( S, ∨, ∧, ≼) be a lattice.


=== Definition 1 ===
Let ( S, ∨, ∧, ≼) be a lattice.

Let S admit a greatest element ⊤.


Then ⊤ is called the top of S.

=== Definition 2 ===
Let ( S, ∨, ∧, ≼) be a lattice.

Let ∧ have an identity element ⊤.


Then ⊤ is called the top of S.",Top
['Definitions/Order of Group Elements'],Definition:Torsion,"Let $G$ be a group.


An element of finite order of $G$ is also known as a torsion element of $G$.


Category:Definitions/Order of Group Elements",Definition:Order of Group Element/Finite/Also known as,,false,"Let G be a group.


An element of finite order of G is also known as a torsion element of G.


Category:Definitions/Order of Group Elements",Torsion
['Definitions/Examples of Subgroups'],Definition:Torsion,"Let $G$ be an abelian group.


Its torsion subgroup $T left(   right)G$ is the subgroup of all torsion elements.",Definition:Torsion Subgroup,,false,"Let G be an abelian group.


Its torsion subgroup T (   )G is the subgroup of all torsion elements.",Torsion
['Definitions/Group Theory'],Definition:Torsion,"Let $G$ be a group.


Then $G$ is a torsion group  if and only if  all its elements are torsion elements.",Definition:Torsion Group,,false,"Let G be a group.


Then G is a torsion group  if and only if  all its elements are torsion elements.",Torsion
['Definitions/Module Theory'],Definition:Torsion,"Let $R$ be a commutative ring with unity.

Let $M$ be a unitary module over $R$.

Let $m in M$.


Then $m$ is a torsion element  if and only if  there exists a regular element $a in R$ with $a m = 0$.",Definition:Torsion Element of Module,,false,"Let R be a commutative ring with unity.

Let M be a unitary module over R.

Let m ∈ M.


Then m is a torsion element  if and only if  there exists a regular element a ∈ R with a m = 0.",Torsion
['Definitions/Module Theory'],Definition:Torsion,"Let $R$ be a commutative ring with unity.

Let $M$ be a unitary module over $R$.


The torsion submodule $T(M)$ of $M$ is the submodule of all torsion elements of $M$.",Definition:Torsion Submodule,,false,"Let R be a commutative ring with unity.

Let M be a unitary module over R.


The torsion submodule T(M) of M is the submodule of all torsion elements of M.",Torsion
"['Definitions/Module Theory', 'Definitions/Commutative Algebra']",Definition:Torsion,"Let $R$ be a commutative ring with unity.

Let $M$ be a unitary module over $R$.


Then $M$ is a torsion module  if and only if  every element of $M$ is of torsion, that is, $M$ equals its torsion submodule $T left(   right)M$.",Definition:Torsion Module,,false,"Let R be a commutative ring with unity.

Let M be a unitary module over R.


Then M is a torsion module  if and only if  every element of M is of torsion, that is, M equals its torsion submodule T (   )M.",Torsion
['Definitions/Field Theory'],Definition:Trace,"Let $K$ be a field and $L / K$ a finite field extension of $K$.

Then by Vector Space on Field Extension is Vector Space, $L$ is naturally a vector space over $K$.

Let $alpha in L$, and $theta_alpha$ be the linear operator:

:$theta_alpha: L to L: beta mapsto alphabeta$


The trace $operatorname {Tr}_{L / K}  left(   right)alpha$ of $alpha$ is the trace of this linear operator.",Definition:Trace (Field Theory),,false,"Let K be a field and L / K a finite field extension of K.

Then by Vector Space on Field Extension is Vector Space, L is naturally a vector space over K.

Let α∈ L, and θ_α be the linear operator:

:θ_α: L → L: β↦αβ


The trace Tr_L / K(   )α of α is the trace of this linear operator.",Trace
"['Definitions/Linear Algebra', 'Definitions/Matrix Algebra']",Definition:Trace,"=== Matrix ===
Let $A = left[ a right]_n$ be a square matrix of order $n$.


The trace of $A$ is:

:$ds mathrm {tr} left(   right)A = sum_{i mathop = 1}^n a_{ii}$


=== Using Einstein Summation Convention ===
Let $A = left[ a_{ij}  right]_{1 mathop le i, j mathop le n}$ be a matrix.


The trace of $A$, using the Einstein summation convention, is:

:$mathrm {tr} left(   right)A = a_{ii}$

=== Linear Operator ===
Let $V$ be a vector space.

Let $A: V to V$ be a linear operator of $V$.


The trace of $A$ is the trace of the matrix of $A$ with respect to some basis.


Category:Definitions/Linear Algebra

Category:Definitions/Linear Algebra
Category:Definitions/Matrix Algebra",Definition:Trace (Linear Algebra),,false,"=== Matrix ===
Let A = [ a ]_n be a square matrix of order n.


The trace of A is:

:tr(   )A = ∑_i  = 1^n a_ii


=== Using Einstein Summation Convention ===
Let A = [ a_ij]_1 ≤ i, j ≤ n be a matrix.


The trace of A, using the Einstein summation convention, is:

:tr(   )A = a_ii

=== Linear Operator ===
Let V be a vector space.

Let A: V → V be a linear operator of V.


The trace of A is the trace of the matrix of A with respect to some basis.


Category:Definitions/Linear Algebra

Category:Definitions/Linear Algebra
Category:Definitions/Matrix Algebra",Trace
"['Definitions/Traces of Matrices', 'Definitions/Matrix Theory']",Definition:Trace,"Let $A = left[ a right]_n$ be a square matrix of order $n$.


The trace of $A$ is:

:$ds mathrm {tr} left(   right)A = sum_{i mathop = 1}^n a_{ii}$


=== Using Einstein Summation Convention ===
Let $A = left[ a_{ij}  right]_{1 mathop le i, j mathop le n}$ be a matrix.


The trace of $A$, using the Einstein summation convention, is:

:$mathrm {tr} left(   right)A = a_{ii}$",Definition:Trace (Linear Algebra)/Matrix,,false,"Let A = [ a ]_n be a square matrix of order n.


The trace of A is:

:tr(   )A = ∑_i  = 1^n a_ii


=== Using Einstein Summation Convention ===
Let A = [ a_ij]_1 ≤ i, j ≤ n be a matrix.


The trace of A, using the Einstein summation convention, is:

:tr(   )A = a_ii",Trace
['Definitions/Linear Algebra'],Definition:Trace,"Let $V$ be a vector space.

Let $A: V to V$ be a linear operator of $V$.


The trace of $A$ is the trace of the matrix of $A$ with respect to some basis.


Category:Definitions/Linear Algebra",Definition:Trace (Linear Algebra)/Linear Operator,,false,"Let V be a vector space.

Let A: V → V be a linear operator of V.


The trace of A is the trace of the matrix of A with respect to some basis.


Category:Definitions/Linear Algebra",Trace
"['Definitions/Trace Sigma-Algebras', 'Definitions/Sigma-Algebras', 'Definitions/Trace Sigma-Algebras']",Definition:Trace,"Let $X$ be a set, and let $unicode{x3a3}$ be a $sigma$-algebra on $X$.

Let $E subseteq X$ be a subset of $X$.


Then the trace $sigma$-algebra (of $E$ in $unicode{x3a3}$), $unicode{x3a3}_E$, is defined as:

:$unicode{x3a3}_E := leftlbrace E cap S: S in unicode{x3a3} rightrbrace$


It is a $sigma$-algebra on $E$, as proved on Trace $sigma$-Algebra is $sigma$-Algebra.",Definition:Trace Sigma-Algebra,,false,"Let X be a set, and let x3a3 be a σ-algebra on X.

Let E ⊆ X be a subset of X.


Then the trace σ-algebra (of E in x3a3), x3a3_E, is defined as:

:x3a3_E := { E ∩ S: S ∈x3a3}


It is a σ-algebra on E, as proved on Trace σ-Algebra is σ-Algebra.",Trace
['Definitions/Mathematical Logic'],Definition:Trace,"Let $P$ be a URM program.

The trace table of $P$ consists of:
* The stage of computation;
* The number of the instruction of $P$ that is about to be performed;
* A list of the contents of all the registers used by $P$ at this point.

Thus the trace table is a list of the states of the URM program at each stage.",Definition:Trace Table,,false,"Let P be a URM program.

The trace table of P consists of:
* The stage of computation;
* The number of the instruction of P that is about to be performed;
* A list of the contents of all the registers used by P at this point.

Thus the trace table is a list of the states of the URM program at each stage.",Trace
['Definitions/Riemannian Geometry'],Definition:Trace,"Let $left( M, g right)$ be a Riemannian manifold.

Let $h$ be a covariant $k$-tensor field with $k ge 2$.

Let $h^sharp$ be a $left( 1, k - 1 right)$-tensor field obtained from $h$ by raising its index.


Then the trace of $h$   $g$ is  a covariant $left( k - 2 right)$-tensor field defined as:

:$mathrm {tr}_g h := mathrm {tr} left(   right){h^sharp}$

where $mathrm {tr}$ is the trace over a covariant and a contravariant index.
 ",Definition:Trace of Tensor,,false,"Let ( M, g ) be a Riemannian manifold.

Let h be a covariant k-tensor field with k ≥ 2.

Let h^♯ be a ( 1, k - 1 )-tensor field obtained from h by raising its index.


Then the trace of h   g is  a covariant ( k - 2 )-tensor field defined as:

:tr_g h := tr(   )h^♯

where tr is the trace over a covariant and a contravariant index.
 ",Trace
"['Definitions/Ring Theory', 'Definitions/Field Extensions', 'Definitions/Polynomial Theory']",Definition:Transcendental,"=== Field Extension ===
A field extension $E / F$ is said to be transcendental  if and only if :
:$exists alpha in E: alpha$ is transcendental over $F$

That is, a field extension is transcendental  if and only if  it contains at least one transcendental element.


=== Transcendental Element ===
Let $E / F$ be a field extension.

Let $alpha in E$.


Then $alpha$ is transcendental over $F$  if and only if :
: $nexists f left(   right)x in F left[ x right] setminus leftlbrace 0 rightrbrace: f left(   right)alpha = 0$
where $f left(   right)x$ denotes a polynomial in $x$ over $F$.

=== Transcendental over Integral Domain ===
Let $left( R, +, circ right)$ be a commutative ring with unity whose zero is $0_R$ and whose unity is $1_R$.

Let $left( D, +, circ right)$ be an integral subdomain of $R$.

Let $x in R$.


Then $x$ is transcendental over $D$  if and only if :
:$ds forall n in mathbb Z_{ge 0}: sum_{k mathop = 0}^n a_k circ x^k = 0_R implies forall k: 0 le k le n: a_k = 0_R$


That is, $x$ is transcendental over $D$  if and only if  the only way to express $0_R$ as a polynomial in $x$ over $D$ is by the null polynomial.",Definition:Transcendental (Abstract Algebra),,false,"=== Field Extension ===
A field extension E / F is said to be transcendental  if and only if :
:∃α∈ E: α is transcendental over F

That is, a field extension is transcendental  if and only if  it contains at least one transcendental element.


=== Transcendental Element ===
Let E / F be a field extension.

Let α∈ E.


Then α is transcendental over F  if and only if :
: ∄ f (   )x ∈ F [ x ] ∖{ 0 }: f (   )α = 0
where f (   )x denotes a polynomial in x over F.

=== Transcendental over Integral Domain ===
Let ( R, +, ∘) be a commutative ring with unity whose zero is 0_R and whose unity is 1_R.

Let ( D, +, ∘) be an integral subdomain of R.

Let x ∈ R.


Then x is transcendental over D  if and only if :
:∀ n ∈ℤ_≥ 0: ∑_k  = 0^n a_k ∘ x^k = 0_R ∀ k: 0 ≤ k ≤ n: a_k = 0_R


That is, x is transcendental over D  if and only if  the only way to express 0_R as a polynomial in x over D is by the null polynomial.",Transcendental
"['Definitions/Field Extensions', 'Definitions/Polynomial Theory']",Definition:Transcendental,"A field extension $E / F$ is said to be transcendental  if and only if :
:$exists alpha in E: alpha$ is transcendental over $F$

That is, a field extension is transcendental  if and only if  it contains at least one transcendental element.


=== Transcendental Element ===
Let $E / F$ be a field extension.

Let $alpha in E$.


Then $alpha$ is transcendental over $F$  if and only if :
: $nexists f left(   right)x in F left[ x right] setminus leftlbrace 0 rightrbrace: f left(   right)alpha = 0$
where $f left(   right)x$ denotes a polynomial in $x$ over $F$.",Definition:Transcendental (Abstract Algebra)/Field Extension,,false,"A field extension E / F is said to be transcendental  if and only if :
:∃α∈ E: α is transcendental over F

That is, a field extension is transcendental  if and only if  it contains at least one transcendental element.


=== Transcendental Element ===
Let E / F be a field extension.

Let α∈ E.


Then α is transcendental over F  if and only if :
: ∄ f (   )x ∈ F [ x ] ∖{ 0 }: f (   )α = 0
where f (   )x denotes a polynomial in x over F.",Transcendental
"['Definitions/Ring Theory', 'Definitions/Polynomial Theory']",Definition:Transcendental,"Let $left( R, +, circ right)$ be a commutative ring with unity whose zero is $0_R$ and whose unity is $1_R$.

Let $left( D, +, circ right)$ be an integral subdomain of $R$.

Let $x in R$.


Then $x$ is transcendental over $D$  if and only if :
:$ds forall n in mathbb Z_{ge 0}: sum_{k mathop = 0}^n a_k circ x^k = 0_R implies forall k: 0 le k le n: a_k = 0_R$


That is, $x$ is transcendental over $D$  if and only if  the only way to express $0_R$ as a polynomial in $x$ over $D$ is by the null polynomial.",Definition:Transcendental (Abstract Algebra)/Ring,,false,"Let ( R, +, ∘) be a commutative ring with unity whose zero is 0_R and whose unity is 1_R.

Let ( D, +, ∘) be an integral subdomain of R.

Let x ∈ R.


Then x is transcendental over D  if and only if :
:∀ n ∈ℤ_≥ 0: ∑_k  = 0^n a_k ∘ x^k = 0_R ∀ k: 0 ≤ k ≤ n: a_k = 0_R


That is, x is transcendental over D  if and only if  the only way to express 0_R as a polynomial in x over D is by the null polynomial.",Transcendental
"['Definitions/Transcendental Numbers', 'Definitions/Numbers', 'Definitions/Analysis']",Definition:Transcendental,"A number (either real or complex) is transcendental  if and only if  it is not algebraic.


=== Transcendental Number over Field ===

Some sources define a transcendental number over a more general field:

Let $F$ be a field.

Let $z$ be a complex number.

$z$ is a transcendental number over $F$  if and only if  $z$ cannot be expressed as a root of a polynomial with coefficients in $F$.",Definition:Transcendental Number,,false,"A number (either real or complex) is transcendental  if and only if  it is not algebraic.


=== Transcendental Number over Field ===

Some sources define a transcendental number over a more general field:

Let F be a field.

Let z be a complex number.

z is a transcendental number over F  if and only if  z cannot be expressed as a root of a polynomial with coefficients in F.",Transcendental
"['Definitions/Entire Functions', 'Definitions/Complex Analysis']",Definition:Transcendental,"Let $f$ be an entire function that has an essential singularity at $infty$.

Then $f$ is a transcendental entire function.",Definition:Entire Function/Transcendental,,false,"Let f be an entire function that has an essential singularity at ∞.

Then f is a transcendental entire function.",Transcendental
['Definitions/Number Theory'],Definition:Transfer Function,"Let $C, D subseteq mathbb C$ with $z in C implies z + 1 in C$.

Let $F: C to D$ and $H: D to D$ be holomorphic functions.

Let $H left(   right){F left(   right)z} = F left(   right){z + 1}$ for all $z in C$.

Then $F$ is said to be a superfunction of $H$, and $H$ is called a transfer function of $F$.

That is, superfunctions are iterations of transfer functions.",Definition:Superfunction,,false,"Let C, D ⊆ℂ with z ∈ C  z + 1 ∈ C.

Let F: C → D and H: D → D be holomorphic functions.

Let H (   )F (   )z = F (   )z + 1 for all z ∈ C.

Then F is said to be a superfunction of H, and H is called a transfer function of F.

That is, superfunctions are iterations of transfer functions.",Transfer Function
['Definitions/Time Series Analysis'],Definition:Transfer Function,"A transfer function, in the context of time series analysis, is a function of time which theoretically models the future output for each possible input.",Definition:Transfer Function (Time Series Analysis),,false,"A transfer function, in the context of time series analysis, is a function of time which theoretically models the future output for each possible input.",Transfer Function
"['Definitions/Linear Transformations', 'Definitions/Linear Algebra', 'Definitions/Vector Spaces', 'Definitions/Linearity']",Definition:Transformation,"A linear transformation is a homomorphism from one module to another.


Hence, let $R$ be a ring.

Let $M = left( G, +_G, circ right)_R$ and $N = left( H, +_H, otimes right)_R$ be $R$-modules.

Let $phi: G to H$ be a mapping.

Then $phi$ is a linear transformation  if and only if :
:$(1): quad forall x, y in G: phi left(   right){x +_G y} = phi left(   right)x +_H phi left(   right)y$
:$(2): quad forall x in G: forall lambda in R: phi left(   right){lambda circ x} = lambda otimes phi left(   right)x$


=== Definition in a Vector Space ===
Let $V, W$ be vector spaces over a field (or, more generally, division ring) $K$.


A mapping $A: V to W$ is a linear transformation  if and only if :

:$forall v_1, v_2 in V, lambda in K: A left(   right){lambda v_1 + v_2} = lambda A left(   right){v_1} + A left(   right){v_2}$


That is, a homomorphism from one vector space to another.


=== Linear Operator on Vector Space ===
A linear operator on a vector space is a linear transformation from a vector space into itself.

=== Linear Operator ===
A linear operator is a linear transformation from a module into itself.


=== Linear Operator on Vector Space ===
A linear operator on a vector space is a linear transformation from a vector space into itself.",Definition:Linear Transformation,,false,"A linear transformation is a homomorphism from one module to another.


Hence, let R be a ring.

Let M = ( G, +_G, ∘)_R and N = ( H, +_H, ⊗)_R be R-modules.

Let ϕ: G → H be a mapping.

Then ϕ is a linear transformation  if and only if :
:(1):   ∀ x, y ∈ G: ϕ(   )x +_G y = ϕ(   )x +_H ϕ(   )y
:(2):   ∀ x ∈ G: ∀λ∈ R: ϕ(   )λ∘ x = λ⊗ϕ(   )x


=== Definition in a Vector Space ===
Let V, W be vector spaces over a field (or, more generally, division ring) K.


A mapping A: V → W is a linear transformation  if and only if :

:∀ v_1, v_2 ∈ V, λ∈ K: A (   )λ v_1 + v_2 = λ A (   )v_1 + A (   )v_2


That is, a homomorphism from one vector space to another.


=== Linear Operator on Vector Space ===
A linear operator on a vector space is a linear transformation from a vector space into itself.

=== Linear Operator ===
A linear operator is a linear transformation from a module into itself.


=== Linear Operator on Vector Space ===
A linear operator on a vector space is a linear transformation from a vector space into itself.",Transformation
"['Definitions/Evaluation Linear Transformations', 'Definitions/Module Theory']",Definition:Transformation,"Let $R$ be a commutative ring with unity.

Let $G$ be an $R$-module.

Let $G^*$ be the algebraic dual of $G$.

Let $G^{**}$ be the double dual of $G^*$.


For each $x in G$, we define the mapping $x^wedge: G^* to R$ as:
:$forall t in G^*: x^wedge left(   right)t = t left(   right)x$


The mapping $J: G to G^{**}$ defined as:
:$forall x in G: J left(   right)x = x^wedge$
is called the evaluation linear transformation from $G$ into $G^{**}$.


It is usual to denote the mapping $t: G^* to R$ as follows:

:$forall x in G, t in G^*: leftlangle x,   rightrangle t := t left(   right)x$",Definition:Evaluation Linear Transformation/Module Theory,,false,"Let R be a commutative ring with unity.

Let G be an R-module.

Let G^* be the algebraic dual of G.

Let G^** be the double dual of G^*.


For each x ∈ G, we define the mapping x^∧: G^* → R as:
:∀ t ∈ G^*: x^∧(   )t = t (   )x


The mapping J: G → G^** defined as:
:∀ x ∈ G: J (   )x = x^∧
is called the evaluation linear transformation from G into G^**.


It is usual to denote the mapping t: G^* → R as follows:

:∀ x ∈ G, t ∈ G^*: ⟨ x,   ⟩ t := t (   )x",Transformation
['Definitions/Complex Functions'],Definition:Transformation,"A complex transformation is a mapping on the complex plane $f: mathbb C to mathbb C$ which is specifically not a multifunction.


Let $z = x + i y$ be a complex variable.

Let $w = u + i v = f left(   right)z$.


Then $w$ can be expressed as:
:$u + i v = f left(   right){x + i y}$

such that:
:$u = u left(   right){x, y}$
and:
:$v = v left(   right){x, y}$
are real functions of two variables.


Thus a point $P = left( x, y right)$ in the complex plane is transformed to a point $P' = left( u left(   right){x, y}, v left(   right){x, y}  right)$ by $f$.

Thus $P'$ is the image of $P$ under $f$.",Definition:Complex Transformation,,false,"A complex transformation is a mapping on the complex plane f: ℂ→ℂ which is specifically not a multifunction.


Let z = x + i y be a complex variable.

Let w = u + i v = f (   )z.


Then w can be expressed as:
:u + i v = f (   )x + i y

such that:
:u = u (   )x, y
and:
:v = v (   )x, y
are real functions of two variables.


Thus a point P = ( x, y ) in the complex plane is transformed to a point P' = ( u (   )x, y, v (   )x, y) by f.

Thus P' is the image of P under f.",Transformation
['Definitions/Group Actions'],Definition:Transformation,"Let $X$ be a set.

Let $left( G, circ right)$ be a group whose identity is $e$.


=== Left Group Action ===
Let $X$ be a set.

Let $left( G, circ right)$ be a group whose identity is $e$.


A (left) group action is an operation $phi: G times X to X$ such that:

:$forall left( g, x right) in G times X: g * x := phi left(   right){g, x} in X$

in such a way that the group action axioms are satisfied:
 

=== Right Group Action ===
Let $X$ be a set.

Let $left( G, circ right)$ be a group whose identity is $e$.


A right group action is a mapping $phi: X times G to X$ such that:

:$forall left( x, g right) in X times G : x * g := phi left(   right){x, g} in X$

in such a way that the right group action axioms are satisfied:
 

The group $G$ thus acts on the set $X$.

The group $G$ can be referred to as the group of transformations, or a transformation group.


=== From Permutation Representation ===
Let $G$ be a group.

Let $X$ be a set.

Let $left( Gamma left(   right)X, circ right)$ be the symmetric group on $X$.

Let $rho: G to left( Gamma left(   right)X, circ right)$ be a permutation representation.


The group action of $G$ associated to the permutation representation $rho$ is the group action $phi: G times X to X$ defined by:
:$phi left(   right){g, x} = rho_g left(   right)x$

where $rho_g : X to X$ is the permutation representation associated to $rho$ for $g in G$ by $rho_g left(   right)x = phi left(   right){g, x}$.",Definition:Group Action,,false,"Let X be a set.

Let ( G, ∘) be a group whose identity is e.


=== Left Group Action ===
Let X be a set.

Let ( G, ∘) be a group whose identity is e.


A (left) group action is an operation ϕ: G × X → X such that:

:∀( g, x ) ∈ G × X: g * x := ϕ(   )g, x∈ X

in such a way that the group action axioms are satisfied:
 

=== Right Group Action ===
Let X be a set.

Let ( G, ∘) be a group whose identity is e.


A right group action is a mapping ϕ: X × G → X such that:

:∀( x, g ) ∈ X × G : x * g := ϕ(   )x, g∈ X

in such a way that the right group action axioms are satisfied:
 

The group G thus acts on the set X.

The group G can be referred to as the group of transformations, or a transformation group.


=== From Permutation Representation ===
Let G be a group.

Let X be a set.

Let ( Γ(   )X, ∘) be the symmetric group on X.

Let ρ: G →( Γ(   )X, ∘) be a permutation representation.


The group action of G associated to the permutation representation ρ is the group action ϕ: G × X → X defined by:
:ϕ(   )g, x = ρ_g (   )x

where ρ_g : X → X is the permutation representation associated to ρ for g ∈ G by ρ_g (   )x = ϕ(   )g, x.",Transformation
['Definitions/Group Actions'],Definition:Transformation,"Let $G$ be a group whose identity is $e$.

Let $X$ be a set.

Let $phi: G times X to X$ be a group action.


Then $G$ is an effective transformation group for $phi$  if and only if  $phi$ is faithful.",Definition:Effective Transformation Group,,false,"Let G be a group whose identity is e.

Let X be a set.

Let ϕ: G × X → X be a group action.


Then G is an effective transformation group for ϕ  if and only if  ϕ is faithful.",Transformation
"['Definitions/Category Theory', 'Definitions/Functors', 'Definitions/Natural Transformations']",Definition:Transformation,"Let $mathbf C$ and $mathbf D$ be categories.


=== Covariant Functors ===
Let $mathbf C$ and $mathbf D$ be categories.

Let $F, G : mathbf C to mathbf D$ be covariant functors.


A natural transformation $eta$ from $F$ to $G$ is a mapping on $mathbf C$ such that:
 

:$(1): quad$ For all $x in mathbf C$, $eta_x$ is a morphism from $F left(   right)x$ to $G left(   right)x$.

:$(2): quad$ For all $x, y in C$ and morphism $f: x to y$, the following diagram commutes:
 

$quad quad xymatrix{
F left(   right)x ar[d]^{eta_x} ar[r]^{F left(   right)f} & F left(   right)y ar[d]^{eta_y} \
G left(   right)x ar[r]^{G left(   right)f}                 & G left(   right)y}$

=== Contravariant Functors ===
Let $mathbf C$ and $mathbf D$ be categories.

Let $F, G: mathbf C to mathbf D$ be contravariant functors.


A natural transformation $eta$ from $F$ to $G$ is a mapping on $mathbf C$ such that:

:$(1): quad$ For all $x in mathbf C$, $eta_x$ is a morphism from $F left(   right)x$ to $G left(   right)x$.

:$(2): quad$ For all $x, y in C$ and morphism $f: x to y$, the following diagram commutes:
 

$quad quad xymatrix{
F left(   right)x ar[d]^{eta_x} & F left(   right)y ar[d]^{eta_y} ar[l]^{F left(   right)f}  \
G left(   right)x                 & G left(   right)y ar[l]^{G left(   right)f} }$",Definition:Natural Transformation,,false,"Let 𝐂 and 𝐃 be categories.


=== Covariant Functors ===
Let 𝐂 and 𝐃 be categories.

Let F, G : 𝐂→𝐃 be covariant functors.


A natural transformation η from F to G is a mapping on 𝐂 such that:
 

:(1): For all x ∈𝐂, η_x is a morphism from F (   )x to G (   )x.

:(2): For all x, y ∈ C and morphism f: x → y, the following diagram commutes:
 

F (   )x [d]^η_x[r]^F (   )f    F (   )y [d]^η_y

G (   )x [r]^G (   )f    G (   )y

=== Contravariant Functors ===
Let 𝐂 and 𝐃 be categories.

Let F, G: 𝐂→𝐃 be contravariant functors.


A natural transformation η from F to G is a mapping on 𝐂 such that:

:(1): For all x ∈𝐂, η_x is a morphism from F (   )x to G (   )x.

:(2): For all x, y ∈ C and morphism f: x → y, the following diagram commutes:
 

F (   )x [d]^η_x    F (   )y [d]^η_y[l]^F (   )f

G (   )x                     G (   )y [l]^G (   )f",Transformation
['Definitions/Natural Transformations'],Definition:Transformation,"Let $mathbf C$ and $mathbf D$ be categories.

Let $F, G : mathbf C to mathbf D$ be covariant functors.


A natural transformation $eta$ from $F$ to $G$ is a mapping on $mathbf C$ such that:
 

:$(1): quad$ For all $x in mathbf C$, $eta_x$ is a morphism from $F left(   right)x$ to $G left(   right)x$.

:$(2): quad$ For all $x, y in C$ and morphism $f: x to y$, the following diagram commutes:
 

$quad quad xymatrix{
F left(   right)x ar[d]^{eta_x} ar[r]^{F left(   right)f} & F left(   right)y ar[d]^{eta_y} \
G left(   right)x ar[r]^{G left(   right)f}                 & G left(   right)y}$",Definition:Natural Transformation/Covariant Functors,,false,"Let 𝐂 and 𝐃 be categories.

Let F, G : 𝐂→𝐃 be covariant functors.


A natural transformation η from F to G is a mapping on 𝐂 such that:
 

:(1): For all x ∈𝐂, η_x is a morphism from F (   )x to G (   )x.

:(2): For all x, y ∈ C and morphism f: x → y, the following diagram commutes:
 

F (   )x [d]^η_x[r]^F (   )f    F (   )y [d]^η_y

G (   )x [r]^G (   )f    G (   )y",Transformation
['Definitions/Natural Transformations'],Definition:Transformation,"Let $mathbf C$ and $mathbf D$ be categories.

Let $F, G: mathbf C to mathbf D$ be contravariant functors.


A natural transformation $eta$ from $F$ to $G$ is a mapping on $mathbf C$ such that:

:$(1): quad$ For all $x in mathbf C$, $eta_x$ is a morphism from $F left(   right)x$ to $G left(   right)x$.

:$(2): quad$ For all $x, y in C$ and morphism $f: x to y$, the following diagram commutes:
 

$quad quad xymatrix{
F left(   right)x ar[d]^{eta_x} & F left(   right)y ar[d]^{eta_y} ar[l]^{F left(   right)f}  \
G left(   right)x                 & G left(   right)y ar[l]^{G left(   right)f} }$",Definition:Natural Transformation/Contravariant Functors,,false,"Let 𝐂 and 𝐃 be categories.

Let F, G: 𝐂→𝐃 be contravariant functors.


A natural transformation η from F to G is a mapping on 𝐂 such that:

:(1): For all x ∈𝐂, η_x is a morphism from F (   )x to G (   )x.

:(2): For all x, y ∈ C and morphism f: x → y, the following diagram commutes:
 

F (   )x [d]^η_x    F (   )y [d]^η_y[l]^F (   )f

G (   )x                     G (   )y [l]^G (   )f",Transformation
"['Definitions/Lorentz Transformations', 'Definitions/Special Theory of Relativity']",Definition:Transformation,"The Lorentz transformation is a transformation which changes the position and motion in one inertial frame of reference to a different inertial frame of reference.

The equations governing such a transformation must satisfy the postulates of the special theory of relativity.


 ",Definition:Lorentz Transformation,,false,"The Lorentz transformation is a transformation which changes the position and motion in one inertial frame of reference to a different inertial frame of reference.

The equations governing such a transformation must satisfy the postulates of the special theory of relativity.


 ",Transformation
"['Definitions/Tschirnhaus Transformations', 'Definitions/Polynomial Theory']",Definition:Transformation,"Let $f left(   right)x$ be a polynomial over a field $k$:

:$f left(   right)x = a_n x^n + a_{n - 1} x^{n - 1} + a_{n - 2} x^{n - 2} + cdots + a_1 x + a_0$


Then the Tschirnhaus transformation is the linear substitution $x = y - dfrac {a_{n - 1} } {n a_n}$.

The Tschirnhaus transformation produces a resulting polynomial $f' left(   right)y$ which is depressed, as shown on Tschirnhaus Transformation yields Depressed Polynomial.

This technique is used in the derivation of Cardano's Formula for the roots of the general cubic.

 ",Definition:Tschirnhaus Transformation,,false,"Let f (   )x be a polynomial over a field k:

:f (   )x = a_n x^n + a_n - 1 x^n - 1 + a_n - 2 x^n - 2 + ⋯ + a_1 x + a_0


Then the Tschirnhaus transformation is the linear substitution x = y - a_n - 1n a_n.

The Tschirnhaus transformation produces a resulting polynomial f' (   )y which is depressed, as shown on Tschirnhaus Transformation yields Depressed Polynomial.

This technique is used in the derivation of Cardano's Formula for the roots of the general cubic.

 ",Transformation
"['Definitions/Transitive Closures', 'Definitions/Transitive Relations', 'Definitions/Examples of Closure Operators']",Definition:Transitive,"=== Smallest Transitive Superset ===
Let $mathcal R$ be a relation on a set $S$.


The transitive closure of $mathcal R$ is defined as the smallest transitive relation on $S$ which contains $mathcal R$ as a subset.


The transitive closure of $mathcal R$ is denoted $mathcal R^+$.

=== Intersection of Transitive Supersets ===
Let $mathcal R$ be a relation on a set $S$.


The transitive closure of $mathcal R$ is defined as the intersection of all transitive relations on $S$ which contain $mathcal R$.


The transitive closure of $mathcal R$ is denoted $mathcal R^+$.

=== Finite Chain ===
Let $mathcal R$ be a relation on a set or class $S$.


The transitive closure of $mathcal R$ is the relation $mathcal R^+$ defined as follows:

For $x, y in S$, $x mathrel {mathcal R^+} y$  if and only if  for some $n in mathbb N_{>0}$ there exist $s_0, s_1, dots, s_n in S$ such that $s_0 = x$, $s_n = y$, and:

 
 
 
 
 
 


That is:

:$forall k in mathbb N_n: s_k mathrel mathcal R s_{k + 1}$

=== Union of Compositions ===
Let $mathcal R$ be a relation on a set $S$.

Let:

:$mathcal R^n := begin{cases}
mathcal R & : n = 1 \
mathcal R^{n-1} circ mathcal R & : n > 1
end{cases}$

where $circ$ denotes composition of relations.

Finally, let:

:$ds mathcal R^+ = bigcup_{i mathop = 1}^infty mathcal R^i$


Then $mathcal R^+$ is called the transitive closure of $mathcal R$.",Definition:Transitive Closure (Relation Theory),,false,"=== Smallest Transitive Superset ===
Let ℛ be a relation on a set S.


The transitive closure of ℛ is defined as the smallest transitive relation on S which contains ℛ as a subset.


The transitive closure of ℛ is denoted ℛ^+.

=== Intersection of Transitive Supersets ===
Let ℛ be a relation on a set S.


The transitive closure of ℛ is defined as the intersection of all transitive relations on S which contain ℛ.


The transitive closure of ℛ is denoted ℛ^+.

=== Finite Chain ===
Let ℛ be a relation on a set or class S.


The transitive closure of ℛ is the relation ℛ^+ defined as follows:

For x, y ∈ S, x ℛ^+ y  if and only if  for some n ∈ℕ_>0 there exist s_0, s_1, …, s_n ∈ S such that s_0 = x, s_n = y, and:

 
 
 
 
 
 


That is:

:∀ k ∈ℕ_n: s_k ℛ s_k + 1

=== Union of Compositions ===
Let ℛ be a relation on a set S.

Let:

:ℛ^n := ℛ    : n = 1 
ℛ^n-1∘ℛ    : n > 1

where ∘ denotes composition of relations.

Finally, let:

:ℛ^+ = ⋃_i  = 1^∞ℛ^i


Then ℛ^+ is called the transitive closure of ℛ.",Transitive
"['Definitions/Transitive Relations', 'Definitions/Reflexive Relations', 'Definitions/Reflexive Transitive Closures']",Definition:Transitive,"Let $mathcal R$ be a relation on a set $S$.


=== Smallest Reflexive Transitive Superset ===
Let $mathcal R$ be a relation on a set $S$.

The reflexive transitive closure of $mathcal R$ is denoted $mathcal R^*$, and is defined as the smallest reflexive and transitive relation on $S$ which contains $mathcal R$.

=== Reflexive Closure of Transitive Closure ===
Let $mathcal R$ be a relation on a set $S$.

The reflexive transitive closure of $mathcal R$ is denoted $mathcal R^*$, and is defined as the reflexive closure of the transitive closure of $mathcal R$:

:$mathcal R^* = left( mathcal R^+ right)^=$

=== Transitive Closure of Reflexive Closure ===
Let $mathcal R$ be a relation on a set $S$.

The reflexive transitive closure of $mathcal R$ is denoted $mathcal R^*$, and is defined as the transitive closure of the reflexive closure of $mathcal R$:
:$mathcal R^* = left( mathcal R^= right)^+$",Definition:Reflexive Transitive Closure,,false,"Let ℛ be a relation on a set S.


=== Smallest Reflexive Transitive Superset ===
Let ℛ be a relation on a set S.

The reflexive transitive closure of ℛ is denoted ℛ^*, and is defined as the smallest reflexive and transitive relation on S which contains ℛ.

=== Reflexive Closure of Transitive Closure ===
Let ℛ be a relation on a set S.

The reflexive transitive closure of ℛ is denoted ℛ^*, and is defined as the reflexive closure of the transitive closure of ℛ:

:ℛ^* = ( ℛ^+ )^=

=== Transitive Closure of Reflexive Closure ===
Let ℛ be a relation on a set S.

The reflexive transitive closure of ℛ is denoted ℛ^*, and is defined as the transitive closure of the reflexive closure of ℛ:
:ℛ^* = ( ℛ^= )^+",Transitive
"['Definitions/Transitive Reductions', 'Definitions/Transitive Relations', 'Definitions/Graph Theory']",Definition:Transitive,"=== Relation Theory ===
Let $mathcal R$ be a relation on a set $S$.


A transitive reduction of $mathcal R$ is denoted $mathcal R^-$, and is defined as a minimal relation on $S$ which has the same transitive closure as $mathcal R$.

=== Graph Theory ===

The concept of transitive reduction is usually encountered in the field of graph theory where it has considerable importance:

Let $G = left( V, E right)$ be a loop-digraph.

Let $G$ be expressed formally as a relational structure $mathcal G$.

A transitive reduction of $G$ is denoted $G^-$, and is defined as a transitive reduction of the relation $mathcal G$.

Hence it is a minimal loop-digraph on $V$ which has the same transitive closure as $mathcal G$.",Definition:Transitive Reduction,,false,"=== Relation Theory ===
Let ℛ be a relation on a set S.


A transitive reduction of ℛ is denoted ℛ^-, and is defined as a minimal relation on S which has the same transitive closure as ℛ.

=== Graph Theory ===

The concept of transitive reduction is usually encountered in the field of graph theory where it has considerable importance:

Let G = ( V, E ) be a loop-digraph.

Let G be expressed formally as a relational structure 𝒢.

A transitive reduction of G is denoted G^-, and is defined as a transitive reduction of the relation 𝒢.

Hence it is a minimal loop-digraph on V which has the same transitive closure as 𝒢.",Transitive
['Definitions/Group Actions'],Definition:Transitive,"Let $G$ be a group.

Let $S$ be a set.

Let $*: G times S to S$ be a group action.


The group action is transitive  if and only if  for any $x, y in S$ there exists $g in G$ such that $g * x = y$.


That is,  if and only if  for all $x in S$:
:$mathrm {Orb} left( x right) = S$
where $mathrm {Orb} left( x right)$ denotes the orbit of $x in S$ under $*$.


=== $n$-transitive Action ===
Let $G$ be a group.

Let $S$ be a set.

Let $*: G times S to S$ be a group action.

Let $ngeq1$ be a natural number.


The group action is $n$-transitive  if and only if  for any two ordered $n$-tuples $(x_1, ldots, x_n)$ and $(y_1, ldots, y_n)$ of pairwise distinct elements of $S$, there exists $gin G$ such that:
:$forall iin {1, ldots, n} : g * x_i = y_i$


Category:Definitions/Group Actions",Definition:Transitive Group Action,,false,"Let G be a group.

Let S be a set.

Let *: G × S → S be a group action.


The group action is transitive  if and only if  for any x, y ∈ S there exists g ∈ G such that g * x = y.


That is,  if and only if  for all x ∈ S:
:Orb( x ) = S
where Orb( x ) denotes the orbit of x ∈ S under *.


=== n-transitive Action ===
Let G be a group.

Let S be a set.

Let *: G × S → S be a group action.

Let n≥1 be a natural number.


The group action is n-transitive  if and only if  for any two ordered n-tuples (x_1, …, x_n) and (y_1, …, y_n) of pairwise distinct elements of S, there exists g∈ G such that:
:∀ i∈{1, …, n} : g * x_i = y_i


Category:Definitions/Group Actions",Transitive
['Definitions/Symmetric Groups'],Definition:Transitive,"Let $S_n$ denote the symmetric group on $n$ letters for $n in mathbb N$.

Let $H$ be a subgroup of $S_n$.

Let $H$ be such that:
:for every pair of elements $i, j in mathbb N_n$ there exists $pi in H$ such that $pi left(   right)i = j$.


Then $H$ is called a transitive subgroup of $S_n$.",Definition:Transitive Subgroup,,false,"Let S_n denote the symmetric group on n letters for n ∈ℕ.

Let H be a subgroup of S_n.

Let H be such that:
:for every pair of elements i, j ∈ℕ_n there exists π∈ H such that π(   )i = j.


Then H is called a transitive subgroup of S_n.",Transitive
"['Definitions/Class Theory', 'Definitions/Transitive Classes']",Definition:Transitive,"Let $A$ denote a class, which can be either a set or a proper class.

Then $A$ is transitive  if and only if  every element of $A$ is also a subclass of $A$.


That is, $A$ is transitive  if and only if :
:$x in A implies x subseteq A$

or:
:$forall x: forall y: left( x in y land y in A implies x in A right)$",Definition:Transitive Class,,false,"Let A denote a class, which can be either a set or a proper class.

Then A is transitive  if and only if  every element of A is also a subclass of A.


That is, A is transitive  if and only if :
:x ∈ A  x ⊆ A

or:
:∀ x: ∀ y: ( x ∈ y  y ∈ A  x ∈ A )",Transitive
['Definitions/Relational Closures'],Definition:Transitive,"=== Definition 1 ===
Let $x$ be a set.

Then the transitive closure of $x$ is the smallest transitive superset of $x$.

The following is not equivalent to the above, but they are almost the same.

=== Definition 2 ===
Let $x$ be a set.

For each natural number $n in mathbb N_{ge 0}$ let:

: $bigcup^n x = underbrace{bigcup bigcup cdots bigcup}_n x$


Then the transitive closure of $x$ is the union of the sets:
:$left{ {x}right}, x, bigcup x, bigcup^2 x, dots, bigcup^n x, dots$


More precisely:

Let $F$ be the mapping on the universal class defined by letting:
:$F left({a}right) = bigcup a$
for each set $a$.

Let $G$ be the mapping on the natural numbers defined recursively by letting:

: $G left({0}right) = left{ {x}right}$
: $G left({n^+}right) = F left({G left({n}right)}right)$

for each natural number $n$.

Then the transitive closure of $x$ is defined as the union of the image of $G$.",Definition:Transitive Closure (Set Theory),,false,"=== Definition 1 ===
Let x be a set.

Then the transitive closure of x is the smallest transitive superset of x.

The following is not equivalent to the above, but they are almost the same.

=== Definition 2 ===
Let x be a set.

For each natural number n ∈ℕ_≥ 0 let:

: ⋃^n x = ⋃⋃⋯⋃_n x


Then the transitive closure of x is the union of the sets:
:{x}, x, ⋃ x, ⋃^2 x, …, ⋃^n x, …


More precisely:

Let F be the mapping on the universal class defined by letting:
:F (a) = ⋃ a
for each set a.

Let G be the mapping on the natural numbers defined recursively by letting:

: G (0) = {x}
: G (n^+) = F (G (n))

for each natural number n.

Then the transitive closure of x is defined as the union of the image of G.",Transitive
['Definitions/Linear Transformations'],Definition:Transpose,"Let $R$ be a commutative ring.

Let $G$ and $H$ be $R$-modules.

Let $G^*$ and $H^*$ be the algebraic duals of $G$ and $H$ respectively.


Let $mathcal L_R left(   right){G, H}$ be the set of all linear transformations from $G$ to $H$.

Let $u in mathcal L_R left(   right){G, H}$.


The transpose of $u$ is the mapping $u^intercal: H^* to G^*$ defined as:
:$forall y in H^*: u^intercal left(   right)y = y circ u$
where $y circ u$ is the composition of $y$ and $u$.",Definition:Transpose of Linear Transformation,,false,"Let R be a commutative ring.

Let G and H be R-modules.

Let G^* and H^* be the algebraic duals of G and H respectively.


Let ℒ_R (   )G, H be the set of all linear transformations from G to H.

Let u ∈ℒ_R (   )G, H.


The transpose of u is the mapping u^⊺: H^* → G^* defined as:
:∀ y ∈ H^*: u^⊺(   )y = y ∘ u
where y ∘ u is the composition of y and u.",Transpose
['Definitions/Matrix Theory'],Definition:Transpose,"Let $mathbf A = left[ alpha right]_{m n}$ be an $m times n$ matrix over a set.


Then the transpose of $mathbf A$ is denoted $mathbf A^intercal$ and is defined as:

:$mathbf A^intercal = left[ beta right]_{n m}: forall i in left[ 1 ,.,.,   right]n, j in left[ 1 ,.,.,   right]m: beta_{i j} = alpha_{j i}$",Definition:Transpose of Matrix,,false,"Let 𝐀 = [ α]_m n be an m × n matrix over a set.


Then the transpose of 𝐀 is denoted 𝐀^⊺ and is defined as:

:𝐀^⊺ = [ β]_n m: ∀ i ∈[ 1  . . ]n, j ∈[ 1  . . ]m: β_i j = α_j i",Transpose
"['Definitions/Straight Lines', 'Definitions/Transversals (Geometry)']",Definition:Transversal,"A transversal of two straight lines lying in the same plane is a straight line which intersects them in two different points.

The transversal is said to cut the two lines that it crosses.

:

In the above diagram, $EF$ is a transversal of the lines $AB$ and $CD$.


It is also apparent that:
:$AB$ is a transversal of the lines $EF$ and $CD$
:$CD$ is a transversal of the lines $EF$ and $AB$
although this is not as obvious.


=== Interior Angle ===
:


An interior angle of a transversal is an angle which is between the two lines cut by that transversal.

In the above figure, the interior angles with respect to the transversal $EF$ are:
:$angle AHJ$
:$angle CJH$
:$angle BHJ$
:$angle DJH$

=== Exterior Angle ===
:


An exterior angle of a transversal is an angle which is not between the two lines cut by a transversal.

In the above figure, the exterior angles with respect to the transversal $EF$ are:
:$angle AHE$
:$angle CJF$
:$angle BHE$
:$angle DJF$

=== Alternate Angles ===
:


Alternate angles are interior angles of a transversal which are on opposite sides and different lines.

In the above figure, the pairs of alternate angles with respect to the transversal $EF$ are:
:$angle AHJ$ and $angle DJH$
:$angle CJH$ and $angle BHJ$

=== Corresponding Angles ===
:


Corresponding angles are the angles in equivalent positions on the two lines cut by a transversal with respect to that transversal.

In the above figure, the corresponding angles with respect to the transversal $EF$ are:
:$angle AHJ$ and $angle CJF$
:$angle AHE$ and $angle CJH$
:$angle BHE$ and $angle DJH$
:$angle BHJ$ and $angle DJF$",Definition:Transversal (Geometry),,false,"A transversal of two straight lines lying in the same plane is a straight line which intersects them in two different points.

The transversal is said to cut the two lines that it crosses.

:

In the above diagram, EF is a transversal of the lines AB and CD.


It is also apparent that:
:AB is a transversal of the lines EF and CD
:CD is a transversal of the lines EF and AB
although this is not as obvious.


=== Interior Angle ===
:


An interior angle of a transversal is an angle which is between the two lines cut by that transversal.

In the above figure, the interior angles with respect to the transversal EF are:
:∠ AHJ
:∠ CJH
:∠ BHJ
:∠ DJH

=== Exterior Angle ===
:


An exterior angle of a transversal is an angle which is not between the two lines cut by a transversal.

In the above figure, the exterior angles with respect to the transversal EF are:
:∠ AHE
:∠ CJF
:∠ BHE
:∠ DJF

=== Alternate Angles ===
:


Alternate angles are interior angles of a transversal which are on opposite sides and different lines.

In the above figure, the pairs of alternate angles with respect to the transversal EF are:
:∠ AHJ and ∠ DJH
:∠ CJH and ∠ BHJ

=== Corresponding Angles ===
:


Corresponding angles are the angles in equivalent positions on the two lines cut by a transversal with respect to that transversal.

In the above figure, the corresponding angles with respect to the transversal EF are:
:∠ AHJ and ∠ CJF
:∠ AHE and ∠ CJH
:∠ BHE and ∠ DJH
:∠ BHJ and ∠ DJF",Transversal
"['Definitions/Group Theory', 'Definitions/Transversals (Group Theory)']",Definition:Transversal,"Let $G$ be a group.

Let $H$ be a subgroup of $G$.

Let $S subseteq G$ be a subset of $G$.


=== Left Transversal ===
Let $G$ be a group.

Let $H$ be a subgroup of $G$.

Let $S subseteq G$ be a subset of $G$.


$S$ is a left transversal for $H$ in $G$  if and only if  every left coset of $H$ contains exactly one element of $S$.

=== Right Transversal ===
Let $G$ be a group.

Let $H$ be a subgroup of $G$.

Let $S subseteq G$ be a subset of $G$.


$S$ is a right transversal for $H$ in $G$  if and only if  every right coset of $H$ contains exactly one element of $S$.

=== Transversal ===

A transversal for $H$ in $G$ is either a left transversal or a right transversal.


 
Clearly if $S$ is a transversal for $H$ it contains $left[ G :   right]H$ elements, where $left[ G :   right]H$ denotes the index of $H$ in $G$.",Definition:Transversal (Group Theory),,false,"Let G be a group.

Let H be a subgroup of G.

Let S ⊆ G be a subset of G.


=== Left Transversal ===
Let G be a group.

Let H be a subgroup of G.

Let S ⊆ G be a subset of G.


S is a left transversal for H in G  if and only if  every left coset of H contains exactly one element of S.

=== Right Transversal ===
Let G be a group.

Let H be a subgroup of G.

Let S ⊆ G be a subset of G.


S is a right transversal for H in G  if and only if  every right coset of H contains exactly one element of S.

=== Transversal ===

A transversal for H in G is either a left transversal or a right transversal.


 
Clearly if S is a transversal for H it contains [ G :   ]H elements, where [ G :   ]H denotes the index of H in G.",Transversal
"['Definitions/Graph Theory', 'Definitions/Tree Theory']",Definition:Tree,"=== Definition 1===
A tree is a simple connected graph with no circuits.


:

=== Definition 2===
A tree is a simple connected graph with no cycles.

=== Node ===
The vertices of a tree are called its nodes.

=== Leaf Node ===
Let $v$ be a node of a tree $T$.

Then $v$ is a leaf node of a $T$  if and only if  $v$ is of degree $1$.


If $T$ is a rooted tree, this is equivalent to saying that $v$ has no child nodes.",Definition:Tree (Graph Theory),,false,"=== Definition 1===
A tree is a simple connected graph with no circuits.


:

=== Definition 2===
A tree is a simple connected graph with no cycles.

=== Node ===
The vertices of a tree are called its nodes.

=== Leaf Node ===
Let v be a node of a tree T.

Then v is a leaf node of a T  if and only if  v is of degree 1.


If T is a rooted tree, this is equivalent to saying that v has no child nodes.",Tree
"['Definitions/Rooted Trees', 'Definitions/Graph Theory', 'Definitions/Tree Theory']",Definition:Tree,"A rooted tree is a tree with a countable number of nodes, in which a particular node is distinguished from the others and called the root node:

:

=== Root Node ===
Let $T$ be a rooted tree.

The root node of $T$ is the node of $T$ which is distinguished from the others by being the ancestor node of every node of $T$.

=== Parent ===
Let $T$ be a rooted tree whose root is $r_T$.

Let $t$ be a node of $T$.

From Path in Tree is Unique, there is only one path from $t$ to $r_T$.

Let $pi: T setminus leftlbrace r_T rightrbrace to T$ be the mapping defined by:

:$pi left(   right)t := text {the node adjacent to $t$ on the path to $r_T$}$


Then $pi left(   right)t$ is known as the parent node of $t$.

The mapping $pi$ is called the parent mapping.

=== Ancestor ===
Let $T$ be a rooted tree with root $r_T$.

Let $t$ be a node of $T$.


An ancestor node of $t$ is a node in the path from $t$ to $r_T$.


=== Proper Ancestor ===
Let $T$ be a rooted tree with root $r_T$.

Let $t$ be a node of $T$.


A proper ancestor node of $t$ is an ancestor node of $t$ that is not $t$ itself.

=== Child Node ===
Let $T$ be a rooted tree with root $r_T$.

Let $t$ be a node of $T$.

The child nodes of $t$ are the elements of the set:
:$leftlbrace s in T: pi left(   right)s = t rightrbrace$
where $pi left(   right)s$ denotes the parent mapping of $s$.

That is, the children of $t$ are all the nodes of $T$ of which $t$ is the parent.


=== Grandchild Node ===
Let $T$ be a rooted tree with root $r_T$.

Let $t$ be a node of $T$.


A child of a child node of a node $t$ can be referred to as a grandchild node of $t$.

In terms of the parent mapping $pi$ of $T$, a grandchild node of $t$ is a node $s$ such that:

:$pi left(   right){pi left(   right)s} = t$


Category:Definitions/Descendant Nodes

=== Descendant ===
Let $T$ be a rooted tree with root $r_T$.

Let $t$ be a node of $T$.


A descendant node $s$ of a $t$ is a node such that $t$ is in the path from $s$ to $r_T$.

That is, the descendant nodes of $t$ are all the nodes of $T$ of which $t$ is an ancestor node.


=== Proper Descendant ===
Let $T$ be a rooted tree with root $r_T$.

Let $t$ be a node of $T$.


A proper descendant node of $t$ is a descendant of $t$ which is not $t$ itself.


Category:Definitions/Descendant Nodes

=== Sibling ===
Let $T$ be a rooted tree with root $r_T$.

Two children of the same node of $T$ are called siblings.

That is, siblings are nodes which both have the same parent.


Category:Definitions/Rooted Trees

=== Leaf Node ===
Let $v$ be a node of a tree $T$.

Then $v$ is a leaf node of a $T$  if and only if  $v$ is of degree $1$.


If $T$ is a rooted tree, this is equivalent to saying that $v$ has no child nodes.

=== Branch ===
Let $T$ be a rooted tree with root node $r_T$.

A subset $Gamma$ of $T$ is a branch  if and only if  all the following conditions hold:
:$(1): quad$ The root node $r_T$ belongs to $Gamma$
:$(2): quad$ The parent of each node in $Gamma setminus leftlbrace r_T rightrbrace$ is in $Gamma$
:$(3): quad$ Each node in $Gamma$ either:
::$text {(a)}: quad$ is a leaf node of $T$
:or:
::$text {(b)}: quad$ has exactly one child node in $Gamma$.


Informally, a branch of a rooted tree is the path from the root to a leaf.


=== Finite ===
Let $T$ be a rooted tree with root node $r_T$.

Let $Gamma$ be a branch of $T$.


Then $Gamma$ is finite  if and only if  it has exactly one leaf node.

=== Infinite ===
Let $T$ be a rooted tree with root node $r_T$.

Let $Gamma$ be a branch of $T$.


Then $Gamma$ is infinite  if and only if  it has no leaf node at the end.",Definition:Rooted Tree,,false,"A rooted tree is a tree with a countable number of nodes, in which a particular node is distinguished from the others and called the root node:

:

=== Root Node ===
Let T be a rooted tree.

The root node of T is the node of T which is distinguished from the others by being the ancestor node of every node of T.

=== Parent ===
Let T be a rooted tree whose root is r_T.

Let t be a node of T.

From Path in Tree is Unique, there is only one path from t to r_T.

Let π: T ∖{ r_T }→ T be the mapping defined by:

:π(   )t := the node adjacent to t on the path to r_T


Then π(   )t is known as the parent node of t.

The mapping π is called the parent mapping.

=== Ancestor ===
Let T be a rooted tree with root r_T.

Let t be a node of T.


An ancestor node of t is a node in the path from t to r_T.


=== Proper Ancestor ===
Let T be a rooted tree with root r_T.

Let t be a node of T.


A proper ancestor node of t is an ancestor node of t that is not t itself.

=== Child Node ===
Let T be a rooted tree with root r_T.

Let t be a node of T.

The child nodes of t are the elements of the set:
:{ s ∈ T: π(   )s = t }
where π(   )s denotes the parent mapping of s.

That is, the children of t are all the nodes of T of which t is the parent.


=== Grandchild Node ===
Let T be a rooted tree with root r_T.

Let t be a node of T.


A child of a child node of a node t can be referred to as a grandchild node of t.

In terms of the parent mapping π of T, a grandchild node of t is a node s such that:

:π(   )π(   )s = t


Category:Definitions/Descendant Nodes

=== Descendant ===
Let T be a rooted tree with root r_T.

Let t be a node of T.


A descendant node s of a t is a node such that t is in the path from s to r_T.

That is, the descendant nodes of t are all the nodes of T of which t is an ancestor node.


=== Proper Descendant ===
Let T be a rooted tree with root r_T.

Let t be a node of T.


A proper descendant node of t is a descendant of t which is not t itself.


Category:Definitions/Descendant Nodes

=== Sibling ===
Let T be a rooted tree with root r_T.

Two children of the same node of T are called siblings.

That is, siblings are nodes which both have the same parent.


Category:Definitions/Rooted Trees

=== Leaf Node ===
Let v be a node of a tree T.

Then v is a leaf node of a T  if and only if  v is of degree 1.


If T is a rooted tree, this is equivalent to saying that v has no child nodes.

=== Branch ===
Let T be a rooted tree with root node r_T.

A subset Γ of T is a branch  if and only if  all the following conditions hold:
:(1): The root node r_T belongs to Γ
:(2): The parent of each node in Γ∖{ r_T } is in Γ
:(3): Each node in Γ either:
::(a): is a leaf node of T
:or:
::(b): has exactly one child node in Γ.


Informally, a branch of a rooted tree is the path from the root to a leaf.


=== Finite ===
Let T be a rooted tree with root node r_T.

Let Γ be a branch of T.


Then Γ is finite  if and only if  it has exactly one leaf node.

=== Infinite ===
Let T be a rooted tree with root node r_T.

Let Γ be a branch of T.


Then Γ is infinite  if and only if  it has no leaf node at the end.",Tree
['Definitions/Set Theory'],Definition:Tree,"Let $left( T, preceq right)$ be an ordered set.

Let $left( T, preceq right)$ be such that for every $t in T$, the lower closure of $t$:
:$t^preceq := leftlbrace s in T: s preceq t rightrbrace$ 

is well-ordered by $preceq$.


Then $left( T, preceq right)$ is a tree.


=== Branch ===
Let $left( T, preceq right)$ be a tree.

A branch of $left( T, preceq right)$ is a maximal chain in $left( T, preceq right)$.


Category:Definitions/Set Theory

=== Subtree ===
Let $left( T, preceq right)$ be a tree.

A subtree of $left( T, preceq right)$ is an ordered subset $left( S, preceq right)$ with the property that:
:for every $forall s in S: forall t in T: t preceq s implies t in S$

That is, such that $left( S, preceq right)$ is a lower closure of $left( T, preceq right)$.


Category:Definitions/Set Theory

Category:Definitions/Set Theory",Definition:Tree (Set Theory),,false,"Let ( T, ≼) be an ordered set.

Let ( T, ≼) be such that for every t ∈ T, the lower closure of t:
:t^≼ := { s ∈ T: s ≼ t } 

is well-ordered by ≼.


Then ( T, ≼) is a tree.


=== Branch ===
Let ( T, ≼) be a tree.

A branch of ( T, ≼) is a maximal chain in ( T, ≼).


Category:Definitions/Set Theory

=== Subtree ===
Let ( T, ≼) be a tree.

A subtree of ( T, ≼) is an ordered subset ( S, ≼) with the property that:
:for every ∀ s ∈ S: ∀ t ∈ T: t ≼ s  t ∈ S

That is, such that ( S, ≼) is a lower closure of ( T, ≼).


Category:Definitions/Set Theory

Category:Definitions/Set Theory",Tree
"['Definitions/Triangles', 'Definitions/Polygons', 'Definitions/Simplices']",Definition:Triangle,":

A triangle is a polygon with exactly three sides.


Thus a triangle is a $2$-simplex.


Because it is a polygon, it follows that it also has three vertices and three angles.",Definition:Triangle (Geometry),,false,":

A triangle is a polygon with exactly three sides.


Thus a triangle is a 2-simplex.


Because it is a polygon, it follows that it also has three vertices and three angles.",Triangle
"['Definitions/Complete Graphs', 'Definitions/Examples of Graphs']",Definition:Triangle,"The complete graph $K_3$ of order $3$ is called a triangle.

:

Category:Definitions/Complete Graphs
Category:Definitions/Examples of Graphs",Definition:Triangle (Graph Theory),,false,"The complete graph K_3 of order 3 is called a triangle.

:

Category:Definitions/Complete Graphs
Category:Definitions/Examples of Graphs",Triangle
['Definitions/Set Partitions'],Definition:Trivial,"Let $S$ be a set such that $S ne varnothing$.

There are two partitions on $S$ which are referred to as the trivial partitions on $S$:


=== Singleton Partition ===
The singleton partition on $S$ is defined as:
:$mathcal P = leftlbrace S rightrbrace$

That is, it is a partition with only one component.


Category:Definitions/Set Partitions

=== Partition of Singletons ===
Let $S$ be a set such that $S ne varnothing$.


The partition of singletons on $S$ is defined as:
:$mathcal P = leftlbrace leftlbrace x rightrbrace: x in S rightrbrace$

That is, it is a partition such that every component is a singleton.


Category:Definitions/Set Partitions

Category:Definitions/Set Partitions",Definition:Trivial Partition,,false,"Let S be a set such that S ∅.

There are two partitions on S which are referred to as the trivial partitions on S:


=== Singleton Partition ===
The singleton partition on S is defined as:
:𝒫 = { S }

That is, it is a partition with only one component.


Category:Definitions/Set Partitions

=== Partition of Singletons ===
Let S be a set such that S ∅.


The partition of singletons on S is defined as:
:𝒫 = {{ x }: x ∈ S }

That is, it is a partition such that every component is a singleton.


Category:Definitions/Set Partitions

Category:Definitions/Set Partitions",Trivial
['Definitions/Quotient Mappings'],Definition:Trivial,"Let $Delta_S$ be the diagonal relation on a set $S$.

As $Delta_S$ is an equivalence, we can form the quotient mapping:
:$q_{Delta_S}: S to S / Delta_S$.


This quotient mapping is called the trivial quotient of $S$.",Definition:Trivial Quotient,,false,"Let Δ_S be the diagonal relation on a set S.

As Δ_S is an equivalence, we can form the quotient mapping:
:q_Δ_S: S → S / Δ_S.


This quotient mapping is called the trivial quotient of S.",Trivial
"['Definitions/Examples of Relations', 'Definitions/Examples of Equivalence Relations']",Definition:Trivial,"The trivial relation is the relation $mathcal R subseteq S times T$ in $S$ to $T$ such that every element of $S$ relates to every element in $T$:

:$mathcal R: S times T: forall left( s, t right) in S times T: left( s, t right) in mathcal R$


That is:
:$mathcal R = S times T$
the relation which equals the product of the sets on which it is defined.",Definition:Trivial Relation,,false,"The trivial relation is the relation ℛ⊆ S × T in S to T such that every element of S relates to every element in T:

:ℛ: S × T: ∀( s, t ) ∈ S × T: ( s, t ) ∈ℛ


That is:
:ℛ = S × T
the relation which equals the product of the sets on which it is defined.",Trivial
['Definitions/Examples of Groups'],Definition:Trivial,A trivial group is a group with only one element $e$.,Definition:Trivial Group,,false,A trivial group is a group with only one element e.,Trivial
['Definitions/Ring Theory'],Definition:Trivial,"A ring $left( R, +, circ right)$ is a trivial ring  if and only if :

:$forall x, y in R: x circ y = 0_R$",Definition:Trivial Ring,,false,"A ring ( R, +, ∘) is a trivial ring  if and only if :

:∀ x, y ∈ R: x ∘ y = 0_R",Trivial
['Definitions/Analytic Number Theory'],Definition:Trivial,"Let $left( G, + right)$ be a finite abelian group.

Let $left( mathbb C_{ne 0}, times right)$ be the multiplicative group of complex numbers.


A character of $G$ is a group homomorphism:

:$chi: G to mathbb C_{ne 0}$",Definition:Character (Number Theory),,false,"Let ( G, + ) be a finite abelian group.

Let ( ℂ_ 0, ×) be the multiplicative group of complex numbers.


A character of G is a group homomorphism:

:χ: G →ℂ_ 0",Trivial
['Definitions/Factorization'],Definition:Trivial,"Let $left( D, +, circ right)$ be an integral domain.

Let $left( U_D, circ right)$ be the group of units of $left( D, +, circ right)$.


A factorization in $left( D, +, circ right)$ of the form $x = u circ y$, where $u in U_D$ (that is, where $x$ is an associate of $y$) is called a trivial factorization.


=== Non-Trivial Factorization ===
Let $left( D, +, circ right)$ be an integral domain.

Let $left( U_D, circ right)$ be the group of units of $left( D, +, circ right)$.


A factorization in $left( D, +, circ right)$ of the form $x = z circ y$, where neither $y$ nor $z$ is a unit of $D$, is called a non-trivial factorization.",Definition:Trivial Factorization,,false,"Let ( D, +, ∘) be an integral domain.

Let ( U_D, ∘) be the group of units of ( D, +, ∘).


A factorization in ( D, +, ∘) of the form x = u ∘ y, where u ∈ U_D (that is, where x is an associate of y) is called a trivial factorization.


=== Non-Trivial Factorization ===
Let ( D, +, ∘) be an integral domain.

Let ( U_D, ∘) be the group of units of ( D, +, ∘).


A factorization in ( D, +, ∘) of the form x = z ∘ y, where neither y nor z is a unit of D, is called a non-trivial factorization.",Trivial
['Definitions/Filters on Sets'],Definition:Trivial,"Let $S$ be a set.


A filter $mathcal F$ on $S$ by definition specifically does not include the empty set $varnothing$.

If a filter $mathcal F$ were to include $varnothing$, then from Empty Set is Subset of All Sets it would follow that every subset of $S$ would have to be in $mathcal F$, and so $mathcal F = mathcal P left( S right)$.


Such a ""filter"" is called the trivial filter on $S$.


Category:Definitions/Filters on Sets",Definition:Filter on Set/Trivial Filter,,false,"Let S be a set.


A filter ℱ on S by definition specifically does not include the empty set ∅.

If a filter ℱ were to include ∅, then from Empty Set is Subset of All Sets it would follow that every subset of S would have to be in ℱ, and so ℱ = 𝒫( S ).


Such a ""filter"" is called the trivial filter on S.


Category:Definitions/Filters on Sets",Trivial
"['Definitions/Indiscrete Topology', 'Definitions/Examples of Topologies']",Definition:Trivial,"Let $S ne varnothing$ be a set.

Let $tau = leftlbrace S, varnothing rightrbrace$.


Then $tau$ is called the indiscrete topology on $S$.


A topological space $left( S, leftlbrace S, varnothing rightrbrace  right)$ is known as an indiscrete space.",Definition:Indiscrete Topology,,false,"Let S ∅ be a set.

Let τ = { S, ∅}.


Then τ is called the indiscrete topology on S.


A topological space ( S, { S, ∅}) is known as an indiscrete space.",Trivial
['Definitions/Examples of Topologies'],Definition:Trivial,"A trivial topological space is a topological space with only one element.


The open sets of a trivial topological space $T = left( leftlbrace s rightrbrace, tau right)$ are $varnothing$ and $leftlbrace s rightrbrace$.",Definition:Trivial Topological Space,,false,"A trivial topological space is a topological space with only one element.


The open sets of a trivial topological space T = ( { s }, τ) are ∅ and { s }.",Trivial
['Definitions/Vector Spaces'],Definition:Trivial,"Let $V$ be a vector space with zero vector $mathbf 0$.


Then the set $(mathbf 0) := left{{mathbf 0}right}$ is called the zero subspace of $V$.


This name is appropriate as $(mathbf 0)$ is in fact a subspace of $V$, as proved in Zero Subspace is Subspace.


 

Category:Definitions/Vector Spaces",Definition:Zero Subspace,,false,"Let V be a vector space with zero vector 0.


Then the set (0) := {0} is called the zero subspace of V.


This name is appropriate as (0) is in fact a subspace of V, as proved in Zero Subspace is Subspace.


 

Category:Definitions/Vector Spaces",Trivial
"['Definitions/Examples of Norms', 'Definitions/Trivial Norms']",Definition:Trivial," Division Ring 
Let $left( R, +, circ right)$ be a division ring, and denote its zero by $0_R$.


Then the map $leftlVert cdot rightrVert: R to mathbb R_{ge 0}$ given by:

:$leftlVert x rightrVert = begin{cases}
  0 & : text{if $x = 0_R$}\
  1 & : text{otherwise}
end{cases}$

defines a norm on $R$, called the trivial norm.

 Vector Space 
Let $left( K, +, circ right)$ be a division ring endowed with the trivial norm.

Let $V$ be a vector space over $K$, with zero $0_V$.


Then the map $leftlVert cdot rightrVert: V to mathbb R_+ cup leftlbrace 0 rightrbrace$ given by:

:$leftlVert x rightrVert = begin{cases}
  0 & : text {if $x = 0_V$} \
  1 & : text {otherwise}
end{cases}$

defines a norm on $V$, called the trivial norm.",Definition:Trivial Norm,,false," Division Ring 
Let ( R, +, ∘) be a division ring, and denote its zero by 0_R.


Then the map ‖·‖: R →ℝ_≥ 0 given by:

:‖ x ‖ = 
  0     : if x = 0_R

  1     : otherwise

defines a norm on R, called the trivial norm.

 Vector Space 
Let ( K, +, ∘) be a division ring endowed with the trivial norm.

Let V be a vector space over K, with zero 0_V.


Then the map ‖·‖: V →ℝ_+ ∪{ 0 } given by:

:‖ x ‖ = 
  0     : if x = 0_V

  1     : otherwise

defines a norm on V, called the trivial norm.",Trivial
['Definitions/Category Theory'],Definition:Trivial,"The following categories can be seen described as trivial:


=== Zero Catgegory ===
The category $mathbf 0$, zero, is the empty category:


:$qquad$


with:
:no objects
and consequently:
:no morphisms.

=== One Category ===
The category one $mathbf 1$, is the category with:

 

=== Discrete Category ===
Let $mathcal C$ be a metacategory.


Then $mathcal C$ is said to be discrete  if and only if  it comprises only identity morphisms.

If the collection $mathcal C$ constitutes the objects of $mathbf C$, then $mathbf C$ may also be denoted $mathbf {Dis}  left(   right)mathcal C$.",Definition:Trivial Category,,false,"The following categories can be seen described as trivial:


=== Zero Catgegory ===
The category 0, zero, is the empty category:


:


with:
:no objects
and consequently:
:no morphisms.

=== One Category ===
The category one 1, is the category with:

 

=== Discrete Category ===
Let 𝒞 be a metacategory.


Then 𝒞 is said to be discrete  if and only if  it comprises only identity morphisms.

If the collection 𝒞 constitutes the objects of 𝐂, then 𝐂 may also be denoted 𝐃𝐢𝐬(   )𝒞.",Trivial
['Definitions/Riemann Zeta Function'],Definition:Trivial,"The trivial zeroes of the Riemann $zeta$ function are the strictly negative even integers :

:$leftlbrace n in mathbb Z: n = -2 times k: k in mathbb N_{ne 0}  rightrbrace = leftlbrace -2, -4, -6, ldots rightrbrace$",Definition:Riemann Zeta Function/Zero/Trivial,,false,"The trivial zeroes of the Riemann ζ function are the strictly negative even integers :

:{ n ∈ℤ: n = -2 × k: k ∈ℕ_ 0} = { -2, -4, -6, …}",Trivial
"['Definitions/Set Theory', 'Definitions/Set Union']",Definition:Union,"Let $S$ and $T$ be sets.


The (set) union of $S$ and $T$ is the set $S cup T$, which consists of all the elements which are contained in either (or both) of $S$ and $T$:
:$x in S cup T iff x in S lor x in T$

or, slightly more formally:
:$A = S cup T iff forall z: left( z in A iff z in S lor z in T right)$


We can write:
:$S cup T := leftlbrace x: x in S lor x in T rightrbrace$

and can voice it $S$ union $T$.


It can be seen that, in this form, $cup$ is a binary operation which acts on sets.


=== Set of Sets ===
Let $mathbb S$ be a set of sets.

The union of $mathbb S$ is:
:$bigcup mathbb S := leftlbrace x: exists X in mathbb S: x in X rightrbrace$
That is, the set of all elements of all elements of $mathbb S$.


Thus the general union of two sets can be defined as:
:$bigcup leftlbrace S, T rightrbrace = S cup T$

=== Family of Sets ===
Let $I$ be an indexing set.

Let $leftlangle S_i rightrangle_{i mathop in I}$ be a family of sets indexed by $I$.


Then the union of $leftlangle S_i rightrangle$ is defined as:

:$ds bigcup_{i mathop in I} S_i := leftlbrace x: exists i in I: x in S_i rightrbrace$


=== In the context of the Universal Set ===

In treatments of set theory in which the concept of the universal set is recognised, this can be expressed as follows.

Let $mathbb U$ be a universal set.

Let $I$ be an indexing set.

Let $leftlangle S_i rightrangle_{i mathop in I}$ be an indexed family of subsets of $mathbb U$.


Then the union of $leftlangle S_i rightrangle$ is defined and denoted as:

:$ds bigcup_{i mathop in I} S_i := leftlbrace x in mathbb U: exists i in I: x in S_i rightrbrace$

=== Subsets of General Set ===
This definition is the same when the universal set $mathbb U$ is replaced by any set $X$, which may or may not be a universal set:

Let $I$ be an indexing set.

Let $leftlangle S_i rightrangle_{i mathop in I}$ be an indexed family of subsets of a set $X$.


Then the union of $leftlangle S_i rightrangle$ is defined as:

:$ds bigcup_{i mathop in I} S_i := leftlbrace x in X: exists i in I: x in S_i rightrbrace$
where $i$ is a bound variable.

=== Countable Union ===
Let $mathbb S$ be a set of sets.

Let $leftlangle S_n rightrangle_{n mathop in mathbb N}$ be a sequence in $mathbb S$.

Let $S$ be the union of $leftlangle S_n rightrangle_{n mathop in mathbb N}$:
:$ds S = bigcup_{n mathop in mathbb N} S_n$


Then $S$ is a countable union of sets in $mathbb S$.

=== Finite Union ===
Let $S = S_1 cup S_2 cup ldots cup S_n$.

Then:
:$ds S = bigcup_{i mathop in mathbb N^*_n} S_i = leftlbrace x: exists i in mathbb N^*_n: x in S_i rightrbrace$
where $mathbb N^*_n = leftlbrace 1, 2, 3, ldots, n rightrbrace$.


If it is clear from the context that $i in mathbb N^*_n$, we can also write $ds bigcup_{mathbb N^*_n} S_i$.


 ",Definition:Set Union,,false,"Let S and T be sets.


The (set) union of S and T is the set S ∪ T, which consists of all the elements which are contained in either (or both) of S and T:
:x ∈ S ∪ T  x ∈ S  x ∈ T

or, slightly more formally:
:A = S ∪ T ∀ z: ( z ∈ A  z ∈ S  z ∈ T )


We can write:
:S ∪ T := { x: x ∈ S  x ∈ T }

and can voice it S union T.


It can be seen that, in this form, ∪ is a binary operation which acts on sets.


=== Set of Sets ===
Let 𝕊 be a set of sets.

The union of 𝕊 is:
:⋃𝕊 := { x: ∃ X ∈𝕊: x ∈ X }
That is, the set of all elements of all elements of 𝕊.


Thus the general union of two sets can be defined as:
:⋃{ S, T } = S ∪ T

=== Family of Sets ===
Let I be an indexing set.

Let ⟨ S_i ⟩_i ∈ I be a family of sets indexed by I.


Then the union of ⟨ S_i ⟩ is defined as:

:⋃_i ∈ I S_i := { x: ∃ i ∈ I: x ∈ S_i }


=== In the context of the Universal Set ===

In treatments of set theory in which the concept of the universal set is recognised, this can be expressed as follows.

Let 𝕌 be a universal set.

Let I be an indexing set.

Let ⟨ S_i ⟩_i ∈ I be an indexed family of subsets of 𝕌.


Then the union of ⟨ S_i ⟩ is defined and denoted as:

:⋃_i ∈ I S_i := { x ∈𝕌: ∃ i ∈ I: x ∈ S_i }

=== Subsets of General Set ===
This definition is the same when the universal set 𝕌 is replaced by any set X, which may or may not be a universal set:

Let I be an indexing set.

Let ⟨ S_i ⟩_i ∈ I be an indexed family of subsets of a set X.


Then the union of ⟨ S_i ⟩ is defined as:

:⋃_i ∈ I S_i := { x ∈ X: ∃ i ∈ I: x ∈ S_i }
where i is a bound variable.

=== Countable Union ===
Let 𝕊 be a set of sets.

Let ⟨ S_n ⟩_n ∈ℕ be a sequence in 𝕊.

Let S be the union of ⟨ S_n ⟩_n ∈ℕ:
:S = ⋃_n ∈ℕ S_n


Then S is a countable union of sets in 𝕊.

=== Finite Union ===
Let S = S_1 ∪ S_2 ∪…∪ S_n.

Then:
:S = ⋃_i ∈ℕ^*_n S_i = { x: ∃ i ∈ℕ^*_n: x ∈ S_i }
where ℕ^*_n = { 1, 2, 3, …, n }.


If it is clear from the context that i ∈ℕ^*_n, we can also write ⋃_ℕ^*_n S_i.


 ",Union
"['Definitions/Relation Theory', 'Definitions/Set Union']",Definition:Union,"Let $S$ and $T$ be sets.

Let $mathcal R_1$ and $mathcal R_2$ be relations on $S times T$.


The union of $mathcal R_1$ and $mathcal R_2$ is the relation $mathcal Q$ defined by:

:$mathcal Q := mathcal R_1 cup mathcal R_2$

where $cup$ denotes set union.


Explicitly, for $s in S$ and $t in T$, we have:

:$s mathrel mathcal Q t$  if and only if  $s mathrel {mathcal R_1} t$ or $s mathrel {mathcal R_2} t$


=== General Definition ===
Let $S$ and $T$ be sets.

Let $mathscr R$ be a collection of relations on $S times T$.


The union of $mathscr R$ is the relation $mathcal R$ defined by:

:$ds mathcal R = bigcup mathscr R$

where $bigcup$ denotes set union.


Explicitly, for $s in S$ and $t in T$:

:$s mathrel mathcal R t$  if and only if  for some $mathcal Q in mathscr R$, $s mathrel mathcal Q t$",Definition:Union of Relations,,false,"Let S and T be sets.

Let ℛ_1 and ℛ_2 be relations on S × T.


The union of ℛ_1 and ℛ_2 is the relation 𝒬 defined by:

:𝒬 := ℛ_1 ∪ℛ_2

where ∪ denotes set union.


Explicitly, for s ∈ S and t ∈ T, we have:

:s 𝒬 t  if and only if  s ℛ_1 t or s ℛ_2 t


=== General Definition ===
Let S and T be sets.

Let ℛ be a collection of relations on S × T.


The union of ℛ is the relation ℛ defined by:

:ℛ = ⋃ℛ

where ⋃ denotes set union.


Explicitly, for s ∈ S and t ∈ T:

:s ℛ t  if and only if  for some 𝒬∈ℛ, s 𝒬 t",Union
"['Definitions/Mapping Theory', 'Definitions/Set Union', 'Definitions/Union Mappings']",Definition:Union,"Let:

:$(1): quad f_1: S_1 to T_1$ be a mapping from $S_1$ to $T_1$

:$(2): quad f_2: S_2 to T_2$ be a mapping from $S_2$ to $T_2$

Let $f_1$ and $f_2$ be combinable, that is, that they agree on $S_1 cap S_2$.


Then the union mapping $f = f_1 cup f_2$ of $f_1$ and $f_2$ is:

:$f: S_1 cup S_2 to T_1 cup T_2: f left(   right)s = begin{cases}
f_1 left(   right)s : & s in S_1 \
f_2 left(   right)s : & s in S_2
end{cases}$


=== Finite Set of Mappings ===
Let $S = leftlbrace f_1, f_2, ldots, f_n rightrbrace$ denote a finite set of mappings.


The union mapping $f$ of $S$ is defined when:

:$forall i, j in leftlbrace 1, 2, ldots, n rightrbrace: f_i$ and $f_j$ are combinable

and is defined as:

:$forall x in ds bigcup leftlbrace mathrm {Dom} left( f_i right): i in leftlbrace 1, 2, ldots, n rightrbrace  rightrbrace x in mathrm {Dom} left( f_i right) implies f = f_i left(   right)x$

=== Family of Mappings ===
Let $I$ be an indexing set.

Let $F = leftlangle f_i rightrangle_{i mathop in I}$ be a family of mappings indexed by $I$

The union mapping $f$ of $F$ is defined when:

:$forall i, j in I: f_i$ and $f_j$ are combinable

and is defined as:

:$forall x in ds bigcup leftlbrace mathrm {Dom} left( f_i right): i in I rightrbrace x in mathrm {Dom} left( f_i right) implies f = f_i left(   right)x$",Definition:Union Mapping,,false,"Let:

:(1):    f_1: S_1 → T_1 be a mapping from S_1 to T_1

:(2):    f_2: S_2 → T_2 be a mapping from S_2 to T_2

Let f_1 and f_2 be combinable, that is, that they agree on S_1 ∩ S_2.


Then the union mapping f = f_1 ∪ f_2 of f_1 and f_2 is:

:f: S_1 ∪ S_2 → T_1 ∪ T_2: f (   )s = 
f_1 (   )s :     s ∈ S_1 

f_2 (   )s :     s ∈ S_2


=== Finite Set of Mappings ===
Let S = { f_1, f_2, …, f_n } denote a finite set of mappings.


The union mapping f of S is defined when:

:∀ i, j ∈{ 1, 2, …, n }: f_i and f_j are combinable

and is defined as:

:∀ x ∈⋃{Dom( f_i ): i ∈{ 1, 2, …, n }} x ∈Dom( f_i )  f = f_i (   )x

=== Family of Mappings ===
Let I be an indexing set.

Let F = ⟨ f_i ⟩_i ∈ I be a family of mappings indexed by I

The union mapping f of F is defined when:

:∀ i, j ∈ I: f_i and f_j are combinable

and is defined as:

:∀ x ∈⋃{Dom( f_i ): i ∈ I } x ∈Dom( f_i )  f = f_i (   )x",Union
['Definitions/Relation Theory'],Definition:Union,"Let:

:$(1): quad mathcal R_1 subseteq S_1 times T_1$ be a relation on $S_1 times T_1$

:$(2): quad mathcal R_2 subseteq S_2 times T_2$ be a relation on $S_2 times T_2$

Let $mathcal R_1$ and $mathcal R_2$ be combinable, that is, that they agree on $S_1 cap S_2$.


Then the union relation (or combined relation) $mathcal R$ of $mathcal R_1$ and $mathcal R_2$ is:

:$mathcal R subseteq left( S_1 cup S_2 right) times left( T_1 cup T_2 right): mathcal R left(   right)s =
begin{cases}
 mathcal R_1 left(   right)s : & s in S_1 \
 mathcal R_2 left(   right)s : & s in S_2
end{cases}$",Definition:Union Relation,,false,"Let:

:(1):   ℛ_1 ⊆ S_1 × T_1 be a relation on S_1 × T_1

:(2):   ℛ_2 ⊆ S_2 × T_2 be a relation on S_2 × T_2

Let ℛ_1 and ℛ_2 be combinable, that is, that they agree on S_1 ∩ S_2.


Then the union relation (or combined relation) ℛ of ℛ_1 and ℛ_2 is:

:ℛ⊆( S_1 ∪ S_2 ) ×( T_1 ∪ T_2 ): ℛ(   )s =
ℛ_1 (   )s :     s ∈ S_1 
ℛ_2 (   )s :     s ∈ S_2",Union
"['Definitions/Set Union', 'Definitions/Indexed Families']",Definition:Union,"Let $I$ be an indexing set.

Let $leftlangle S_i rightrangle_{i mathop in I}$ be a family of sets indexed by $I$.


Then the union of $leftlangle S_i rightrangle$ is defined as:

:$ds bigcup_{i mathop in I} S_i := leftlbrace x: exists i in I: x in S_i rightrbrace$


=== In the context of the Universal Set ===

In treatments of set theory in which the concept of the universal set is recognised, this can be expressed as follows.

Let $mathbb U$ be a universal set.

Let $I$ be an indexing set.

Let $leftlangle S_i rightrangle_{i mathop in I}$ be an indexed family of subsets of $mathbb U$.


Then the union of $leftlangle S_i rightrangle$ is defined and denoted as:

:$ds bigcup_{i mathop in I} S_i := leftlbrace x in mathbb U: exists i in I: x in S_i rightrbrace$

=== Subsets of General Set ===
This definition is the same when the universal set $mathbb U$ is replaced by any set $X$, which may or may not be a universal set:

Let $I$ be an indexing set.

Let $leftlangle S_i rightrangle_{i mathop in I}$ be an indexed family of subsets of a set $X$.


Then the union of $leftlangle S_i rightrangle$ is defined as:

:$ds bigcup_{i mathop in I} S_i := leftlbrace x in X: exists i in I: x in S_i rightrbrace$
where $i$ is a bound variable.",Definition:Set Union/Family of Sets,,false,"Let I be an indexing set.

Let ⟨ S_i ⟩_i ∈ I be a family of sets indexed by I.


Then the union of ⟨ S_i ⟩ is defined as:

:⋃_i ∈ I S_i := { x: ∃ i ∈ I: x ∈ S_i }


=== In the context of the Universal Set ===

In treatments of set theory in which the concept of the universal set is recognised, this can be expressed as follows.

Let 𝕌 be a universal set.

Let I be an indexing set.

Let ⟨ S_i ⟩_i ∈ I be an indexed family of subsets of 𝕌.


Then the union of ⟨ S_i ⟩ is defined and denoted as:

:⋃_i ∈ I S_i := { x ∈𝕌: ∃ i ∈ I: x ∈ S_i }

=== Subsets of General Set ===
This definition is the same when the universal set 𝕌 is replaced by any set X, which may or may not be a universal set:

Let I be an indexing set.

Let ⟨ S_i ⟩_i ∈ I be an indexed family of subsets of a set X.


Then the union of ⟨ S_i ⟩ is defined as:

:⋃_i ∈ I S_i := { x ∈ X: ∃ i ∈ I: x ∈ S_i }
where i is a bound variable.",Union
"['Definitions/Examples of Topologies', 'Definitions/Real Intervals']",Definition:Union,"Let $left( mathbb R, tau_d right)$ be the real number line $mathbb R$ under the usual (Euclidean) topology $tau_d$.

Let $a, b, c in mathbb R$ where $a < b < c$.

Let $A$ be the union of the two open intervals:
:$A := left( a ,.,.,   right)b cup left( b ,.,.,   right)c$


Then $left( A, tau_d right)$ is the union of adjacent open intervals.",Definition:Union of Adjacent Open Intervals,,false,"Let ( ℝ, τ_d ) be the real number line ℝ under the usual (Euclidean) topology τ_d.

Let a, b, c ∈ℝ where a < b < c.

Let A be the union of the two open intervals:
:A := ( a  . . )b ∪( b  . . )c


Then ( A, τ_d ) is the union of adjacent open intervals.",Union
"['Definitions/Units of Rings', 'Definitions/Ring Theory', 'Definitions/Factorization']",Definition:Unit,"Let $left( R, +, circ right)$ be a ring with unity whose unity is $1_R$.


=== Definition 1 ===
Let $left( R, +, circ right)$ be a ring with unity whose unity is $1_R$.


An element $x in R$ is a unit of $left( R, +, circ right)$  if and only if  $x$ is invertible under $circ$.


That is, a unit of $R$ is an element of $R$ which has an inverse.
:$exists y in R: x circ y = 1_R = y circ x$

=== Definition 2 ===
Let $left( R, +, circ right)$ be a ring with unity whose unity is $1_R$.


An element $x in R$ is a unit of $left( R, +, circ right)$  if and only if  $x$ is divisor of $1_R$.

=== Product Inverse ===
Let $left( R, +, circ right)$ be a ring with unity.

Let $U_R$ denotes the group of units of $R$.

The inverse of $x in U_R$ by $circ$ is called the (ring) product inverse of $x$.


The usual means of denoting the product inverse of an element $x$ is by $x^{-1}$.

Thus it is distinguished from the additive inverse of $x$, that is, the (ring) negative of $x$, which is usually denoted $-x$.

=== Group of Units ===
Let $left( R, +, circ right)$ be a ring with unity.


Then the set $U_R$ of units of $left( R, +, circ right)$ is called the group of units of $left( R, +, circ right)$.


This can be denoted explicitly as $left( U_R, circ right)$.",Definition:Unit of Ring,,false,"Let ( R, +, ∘) be a ring with unity whose unity is 1_R.


=== Definition 1 ===
Let ( R, +, ∘) be a ring with unity whose unity is 1_R.


An element x ∈ R is a unit of ( R, +, ∘)  if and only if  x is invertible under ∘.


That is, a unit of R is an element of R which has an inverse.
:∃ y ∈ R: x ∘ y = 1_R = y ∘ x

=== Definition 2 ===
Let ( R, +, ∘) be a ring with unity whose unity is 1_R.


An element x ∈ R is a unit of ( R, +, ∘)  if and only if  x is divisor of 1_R.

=== Product Inverse ===
Let ( R, +, ∘) be a ring with unity.

Let U_R denotes the group of units of R.

The inverse of x ∈ U_R by ∘ is called the (ring) product inverse of x.


The usual means of denoting the product inverse of an element x is by x^-1.

Thus it is distinguished from the additive inverse of x, that is, the (ring) negative of x, which is usually denoted -x.

=== Group of Units ===
Let ( R, +, ∘) be a ring with unity.


Then the set U_R of units of ( R, +, ∘) is called the group of units of ( R, +, ∘).


This can be denoted explicitly as ( U_R, ∘).",Unit
['Definitions/Set Systems'],Definition:Unit,"Let $mathcal S$ be a system of sets.

Let $U in mathcal S$ such that:
:$forall A in mathcal S: A cap U = A$


Then $U$ is the unit of $mathcal S$.


Note that, for a given system of sets, if $U$ exists then it is unique.

",Definition:Unit of System of Sets,,false,"Let 𝒮 be a system of sets.

Let U ∈𝒮 such that:
:∀ A ∈𝒮: A ∩ U = A


Then U is the unit of 𝒮.


Note that, for a given system of sets, if U exists then it is unique.

",Unit
['Definitions/Unital Algebras'],Definition:Unit,"Let $R$ be a commutative ring.

Let $left( A_R, oplus right)$ be a unital algebra over $R$.


The unit of $left( A_R, oplus right)$, denoted $1_A$, is the identity element of the operation $oplus$:
:$forall a in A_R: a oplus 1_A = 1_A oplus a = a$

It is usually denoted $1$ when there is no source of confusion with the identity elements of the underlying structures of the algebra.",Definition:Unit of Algebra,,false,"Let R be a commutative ring.

Let ( A_R, ⊕) be a unital algebra over R.


The unit of ( A_R, ⊕), denoted 1_A, is the identity element of the operation ⊕:
:∀ a ∈ A_R: a ⊕ 1_A = 1_A ⊕ a = a

It is usually denoted 1 when there is no source of confusion with the identity elements of the underlying structures of the algebra.",Unit
['Definitions/Algebras'],Definition:Unit,"Let $R$ be a commutative ring.

Let $left( A, * right)$ be an algebra over $R$. 


Then $left( A, * right)$ is a unital algebra  if and only if  the algebraic structure $left( A, oplus right)$ has an identity element.

That is:
:$exists 1_A in A: forall a in A: a * 1_A = 1_A * a = a$",Definition:Unital Algebra,,false,"Let R be a commutative ring.

Let ( A, * ) be an algebra over R. 


Then ( A, * ) is a unital algebra  if and only if  the algebraic structure ( A, ⊕) has an identity element.

That is:
:∃ 1_A ∈ A: ∀ a ∈ A: a * 1_A = 1_A * a = a",Unit
"['Definitions/Ring Theory', 'Definitions/Unity']",Definition:Unit,"Let $left( R, +, circ right)$ be a ring.

If the semigroup $left( R, circ right)$ has an identity, this identity is referred to as the unity of the ring $left( R, +, circ right)$.

It is (usually) denoted $1_R$, where the subscript denotes the particular ring to which $1_R$ belongs (or often $1$ if there is no danger of ambiguity).


The ring $R$ itself is then referred to as a ring with unity.",Definition:Unity (Abstract Algebra)/Ring,,false,"Let ( R, +, ∘) be a ring.

If the semigroup ( R, ∘) has an identity, this identity is referred to as the unity of the ring ( R, +, ∘).

It is (usually) denoted 1_R, where the subscript denotes the particular ring to which 1_R belongs (or often 1 if there is no danger of ambiguity).


The ring R itself is then referred to as a ring with unity.",Unit
"['Definitions/Physics', 'Definitions/Units of Measurement']",Definition:Unit,"A unit of measurement is a specified magnitude of a given physical quantity, defined by convention.

It is used as a standard for measurement of that physical quantity.

Any other value of the physical quantity can be expressed as a multiple of that unit of measurement.",Definition:Unit of Measurement,,false,"A unit of measurement is a specified magnitude of a given physical quantity, defined by convention.

It is used as a standard for measurement of that physical quantity.

Any other value of the physical quantity can be expressed as a multiple of that unit of measurement.",Unit
"['Definitions/Unity', 'Definitions/Identity Elements']",Definition:Unity,"=== Unity of Ring ===
Let $left( R, +, circ right)$ be a ring.

If the semigroup $left( R, circ right)$ has an identity, this identity is referred to as the unity of the ring $left( R, +, circ right)$.

It is (usually) denoted $1_R$, where the subscript denotes the particular ring to which $1_R$ belongs (or often $1$ if there is no danger of ambiguity).


The ring $R$ itself is then referred to as a ring with unity.

=== Unity of Field ===
Let $left( F, +, times right)$ be a field.

The identity element of the multiplicative group $left( F^*, times right)$ of $F$ is called the multiplicative identity of $F$.

It is often denoted $e_F$ or $1_F$, or, if there is no danger of ambiguity, $e$ or $1$.

Category:Definitions/Unity
Category:Definitions/Identity Elements",Definition:Unity (Abstract Algebra),,false,"=== Unity of Ring ===
Let ( R, +, ∘) be a ring.

If the semigroup ( R, ∘) has an identity, this identity is referred to as the unity of the ring ( R, +, ∘).

It is (usually) denoted 1_R, where the subscript denotes the particular ring to which 1_R belongs (or often 1 if there is no danger of ambiguity).


The ring R itself is then referred to as a ring with unity.

=== Unity of Field ===
Let ( F, +, ×) be a field.

The identity element of the multiplicative group ( F^*, ×) of F is called the multiplicative identity of F.

It is often denoted e_F or 1_F, or, if there is no danger of ambiguity, e or 1.

Category:Definitions/Unity
Category:Definitions/Identity Elements",Unity
"['Definitions/Ring Theory', 'Definitions/Unity']",Definition:Unity,"Let $left( R, +, circ right)$ be a ring.

If the semigroup $left( R, circ right)$ has an identity, this identity is referred to as the unity of the ring $left( R, +, circ right)$.

It is (usually) denoted $1_R$, where the subscript denotes the particular ring to which $1_R$ belongs (or often $1$ if there is no danger of ambiguity).


The ring $R$ itself is then referred to as a ring with unity.",Definition:Unity (Abstract Algebra)/Ring,,false,"Let ( R, +, ∘) be a ring.

If the semigroup ( R, ∘) has an identity, this identity is referred to as the unity of the ring ( R, +, ∘).

It is (usually) denoted 1_R, where the subscript denotes the particular ring to which 1_R belongs (or often 1 if there is no danger of ambiguity).


The ring R itself is then referred to as a ring with unity.",Unity
"['Definitions/Field Theory', 'Definitions/Unity']",Definition:Unity,"Let $left( F, +, times right)$ be a field.

The identity element of the multiplicative group $left( F^*, times right)$ of $F$ is called the multiplicative identity of $F$.

It is often denoted $e_F$ or $1_F$, or, if there is no danger of ambiguity, $e$ or $1$.",Definition:Multiplicative Identity,,false,"Let ( F, +, ×) be a field.

The identity element of the multiplicative group ( F^*, ×) of F is called the multiplicative identity of F.

It is often denoted e_F or 1_F, or, if there is no danger of ambiguity, e or 1.",Unity
['Definitions/Set Theory'],Definition:Universal,"Sets are considered to be subsets of some large universal set, also called the universe.

Exactly what this universe is will vary depending on the subject and context.

When discussing particular sets, it should be made clear just what that universe is.

However, note that from There Exists No Universal Set, this universe cannot be everything that there is.


The traditional symbol used to signify the universe is $mathfrak A$.

However, this is old-fashioned and inconvenient, so some newer texts have taken to using $mathbb U$ or just $U$ instead.


With this notation, this definition can be put into symbols as:
:$forall S: S subseteq mathbb U$


The use of $mathbb U$ or a variant is not universal: some sources use $X$.",Definition:Universe (Set Theory),,false,"Sets are considered to be subsets of some large universal set, also called the universe.

Exactly what this universe is will vary depending on the subject and context.

When discussing particular sets, it should be made clear just what that universe is.

However, note that from There Exists No Universal Set, this universe cannot be everything that there is.


The traditional symbol used to signify the universe is 𝔄.

However, this is old-fashioned and inconvenient, so some newer texts have taken to using 𝕌 or just U instead.


With this notation, this definition can be put into symbols as:
:∀ S: S ⊆𝕌


The use of 𝕌 or a variant is not universal: some sources use X.",Universal
"['Definitions/Class Theory', 'Definitions/Universal Class']",Definition:Universal,"The universal class is the class of which all sets are elements.


The universal class is defined most commonly in literature as:

:$V = leftlbrace x: x = x rightrbrace$

where $x$ ranges over all sets.


It can be briefly defined as the class of all sets.",Definition:Universal Class,,false,"The universal class is the class of which all sets are elements.


The universal class is defined most commonly in literature as:

:V = { x: x = x }

where x ranges over all sets.


It can be briefly defined as the class of all sets.",Universal
['Definitions/Category Theory'],Definition:Universal,"Let $C$ be a category.


A universal object of $C$ is an object that is initial or terminal.",Definition:Universal Object,,false,"Let C be a category.


A universal object of C is an object that is initial or terminal.",Universal
['Definitions/Topology'],Definition:Universal,"A universal cover is a covering space which is simply connected.

 

Category:Definitions/Topology",Definition:Universal Cover,,false,"A universal cover is a covering space which is simply connected.

 

Category:Definitions/Topology",Universal
"['Definitions/Universe of Discourse', 'Definitions/Logic']",Definition:Universe,"The universe of discourse is the term used to mean everything we are talking about.


When introducing the symbols:
:$forall$ (the universal quantifier)
or:
:$exists$ (the existential quantifier)
it is understood that the objects referred to are those in the specified universe of discourse.

It is usual to define that universe.",Definition:Universe of Discourse,,false,"The universe of discourse is the term used to mean everything we are talking about.


When introducing the symbols:
:∀ (the universal quantifier)
or:
:∃ (the existential quantifier)
it is understood that the objects referred to are those in the specified universe of discourse.

It is usual to define that universe.",Universe
"['Definitions/Tarski-Grothendieck Set Theory', 'Definitions/Category Theory']",Definition:Universe,"A Grothendieck universe is a set (not a class) which has the properties expected of the universe $mathbb U$ of sets in the sense of the Zermelo-Fraenkel axioms with the following properties:

:$(1): quad mathbb U$ is a transitive set: If $u in mathbb U$ and $x in u$ then $x in mathbb U$

:$(2): quad$ If $ u, v in mathbb U$ then $leftlbrace u, v rightrbrace in mathbb U$

:$(3): quad$ If $u in mathbb U$ then the power set $mathcal P left( u right) in mathbb U$

:$(4): quad$ If $A in mathbb U$, and $leftlbrace u_alpha: alpha in A rightrbrace$ is a family of elements $u_alpha in mathbb U$ indexed by $A$, then $ds bigcup_{alpha mathop in A} u_alpha in mathbb U$


 ",Definition:Grothendieck Universe,,false,"A Grothendieck universe is a set (not a class) which has the properties expected of the universe 𝕌 of sets in the sense of the Zermelo-Fraenkel axioms with the following properties:

:(1):   𝕌 is a transitive set: If u ∈𝕌 and x ∈ u then x ∈𝕌

:(2): If u, v ∈𝕌 then { u, v }∈𝕌

:(3): If u ∈𝕌 then the power set 𝒫( u ) ∈𝕌

:(4): If A ∈𝕌, and { u_α: α∈ A } is a family of elements u_α∈𝕌 indexed by A, then ⋃_α∈ A u_α∈𝕌


 ",Universe
"['Definitions/Statistics', 'Definitions/Applied Mathematics']",Definition:Universe,"The population of a statistical study is everything in the universe of discourse that is relevant to the study.

It includes all objects, measures, and observations under discussion.


=== Finite Population ===
A finite population is a population which is finite.

=== Infinite Population ===
An infinite population is a population which is infinite.",Definition:Population,,false,"The population of a statistical study is everything in the universe of discourse that is relevant to the study.

It includes all objects, measures, and observations under discussion.


=== Finite Population ===
A finite population is a population which is finite.

=== Infinite Population ===
An infinite population is a population which is infinite.",Universe
"['Definitions/Applied Mathematics', 'Definitions/Physics']",Definition:Universe,"The physical universe, or usually just universe, is commonly defined as, and understood to be,  .


=== Real-World ===
Used to describe a phenomenon or object that has a genuine existence in the physical universe.",Definition:Physical Universe,,false,"The physical universe, or usually just universe, is commonly defined as, and understood to be,  .


=== Real-World ===
Used to describe a phenomenon or object that has a genuine existence in the physical universe.",Universe
['Definitions/Boundedness'],Definition:Upper Bound,"Let $left( S, preceq right)$ be an ordered set.

Let $T$ be a subset of $S$.


An upper bound for $T$ (in $S$) is an element $M in S$ such that:
:$forall t in T: t preceq M$

That is, $M$ succeeds every element of $T$.


=== Subset of Real Numbers ===

The concept is usually encountered where $left( S, preceq right)$ is the set of real numbers under the usual ordering $left( mathbb R, le right)$:

Let $mathbb R$ be the set of real numbers.

Let $T$ be a subset of $mathbb R$.


An upper bound for $T$ (in $mathbb R$) is an element $M in mathbb R$ such that:
:$forall t in T: t le M$

That is, $M$ is greater than or equal to every element of $T$.


=== Upper Bound of Number ===
When considering the upper bound of a set of numbers, it is commonplace to ignore the set and instead refer just to the number itself.

Thus the construction:

:The set of numbers which fulfil the propositional function $P left(   right)n$ is bounded above with the upper bound $N$

would be reported as:

:The number $n$ such that $P left(   right)n$ has the upper bound $N$.


This construct obscures the details of what is actually being stated. Its use on   is considered an abuse of notation and so discouraged.


This also applies in the case where it is the upper bound of a mapping which is under discussion.


Category:Definitions/Numbers
Category:Definitions/Boundedness",Definition:Upper Bound of Set,,false,"Let ( S, ≼) be an ordered set.

Let T be a subset of S.


An upper bound for T (in S) is an element M ∈ S such that:
:∀ t ∈ T: t ≼ M

That is, M succeeds every element of T.


=== Subset of Real Numbers ===

The concept is usually encountered where ( S, ≼) is the set of real numbers under the usual ordering ( ℝ, ≤):

Let ℝ be the set of real numbers.

Let T be a subset of ℝ.


An upper bound for T (in ℝ) is an element M ∈ℝ such that:
:∀ t ∈ T: t ≤ M

That is, M is greater than or equal to every element of T.


=== Upper Bound of Number ===
When considering the upper bound of a set of numbers, it is commonplace to ignore the set and instead refer just to the number itself.

Thus the construction:

:The set of numbers which fulfil the propositional function P (   )n is bounded above with the upper bound N

would be reported as:

:The number n such that P (   )n has the upper bound N.


This construct obscures the details of what is actually being stated. Its use on   is considered an abuse of notation and so discouraged.


This also applies in the case where it is the upper bound of a mapping which is under discussion.


Category:Definitions/Numbers
Category:Definitions/Boundedness",Upper Bound
['Definitions/Boundedness'],Definition:Upper Bound,"Let $mathbb R$ be the set of real numbers.

Let $T$ be a subset of $mathbb R$.


An upper bound for $T$ (in $mathbb R$) is an element $M in mathbb R$ such that:
:$forall t in T: t le M$

That is, $M$ is greater than or equal to every element of $T$.


=== Upper Bound of Number ===
When considering the upper bound of a set of numbers, it is commonplace to ignore the set and instead refer just to the number itself.

Thus the construction:

:The set of numbers which fulfil the propositional function $P left(   right)n$ is bounded above with the upper bound $N$

would be reported as:

:The number $n$ such that $P left(   right)n$ has the upper bound $N$.


This construct obscures the details of what is actually being stated. Its use on   is considered an abuse of notation and so discouraged.


This also applies in the case where it is the upper bound of a mapping which is under discussion.


Category:Definitions/Numbers
Category:Definitions/Boundedness",Definition:Upper Bound of Set/Real Numbers,,false,"Let ℝ be the set of real numbers.

Let T be a subset of ℝ.


An upper bound for T (in ℝ) is an element M ∈ℝ such that:
:∀ t ∈ T: t ≤ M

That is, M is greater than or equal to every element of T.


=== Upper Bound of Number ===
When considering the upper bound of a set of numbers, it is commonplace to ignore the set and instead refer just to the number itself.

Thus the construction:

:The set of numbers which fulfil the propositional function P (   )n is bounded above with the upper bound N

would be reported as:

:The number n such that P (   )n has the upper bound N.


This construct obscures the details of what is actually being stated. Its use on   is considered an abuse of notation and so discouraged.


This also applies in the case where it is the upper bound of a mapping which is under discussion.


Category:Definitions/Numbers
Category:Definitions/Boundedness",Upper Bound
['Definitions/Boundedness'],Definition:Upper Bound,"Let $f: S to T$ be a mapping whose codomain is an ordered set $left( T, preceq right)$.


Let $f$ be bounded above in $T$ by $H in T$.


Then $H$ is an upper bound of $f$.


=== Real-Valued Function ===

The concept is usually encountered where $left( T, preceq right)$ is the set of real numbers under the usual ordering $left( mathbb R, le right)$:

Let $f: S to mathbb R$ be a real-valued function.


Let $f$ be bounded above in $mathbb R$ by $H in mathbb R$.


Then $H$ is an upper bound of $f$.


=== Upper Bound of Number ===
When considering the upper bound of a set of numbers, it is commonplace to ignore the set and instead refer just to the number itself.

Thus the construction:

:The set of numbers which fulfil the propositional function $P left(   right)n$ is bounded above with the upper bound $N$

would be reported as:

:The number $n$ such that $P left(   right)n$ has the upper bound $N$.


This construct obscures the details of what is actually being stated. Its use on   is considered an abuse of notation and so discouraged.


This also applies in the case where it is the upper bound of a mapping which is under discussion.


Category:Definitions/Numbers
Category:Definitions/Boundedness",Definition:Upper Bound of Mapping,,false,"Let f: S → T be a mapping whose codomain is an ordered set ( T, ≼).


Let f be bounded above in T by H ∈ T.


Then H is an upper bound of f.


=== Real-Valued Function ===

The concept is usually encountered where ( T, ≼) is the set of real numbers under the usual ordering ( ℝ, ≤):

Let f: S →ℝ be a real-valued function.


Let f be bounded above in ℝ by H ∈ℝ.


Then H is an upper bound of f.


=== Upper Bound of Number ===
When considering the upper bound of a set of numbers, it is commonplace to ignore the set and instead refer just to the number itself.

Thus the construction:

:The set of numbers which fulfil the propositional function P (   )n is bounded above with the upper bound N

would be reported as:

:The number n such that P (   )n has the upper bound N.


This construct obscures the details of what is actually being stated. Its use on   is considered an abuse of notation and so discouraged.


This also applies in the case where it is the upper bound of a mapping which is under discussion.


Category:Definitions/Numbers
Category:Definitions/Boundedness",Upper Bound
['Definitions/Boundedness'],Definition:Upper Bound,"Let $f: S to mathbb R$ be a real-valued function.


Let $f$ be bounded above in $mathbb R$ by $H in mathbb R$.


Then $H$ is an upper bound of $f$.


=== Upper Bound of Number ===
When considering the upper bound of a set of numbers, it is commonplace to ignore the set and instead refer just to the number itself.

Thus the construction:

:The set of numbers which fulfil the propositional function $P left(   right)n$ is bounded above with the upper bound $N$

would be reported as:

:The number $n$ such that $P left(   right)n$ has the upper bound $N$.


This construct obscures the details of what is actually being stated. Its use on   is considered an abuse of notation and so discouraged.


This also applies in the case where it is the upper bound of a mapping which is under discussion.


Category:Definitions/Numbers
Category:Definitions/Boundedness",Definition:Upper Bound of Mapping/Real-Valued,,false,"Let f: S →ℝ be a real-valued function.


Let f be bounded above in ℝ by H ∈ℝ.


Then H is an upper bound of f.


=== Upper Bound of Number ===
When considering the upper bound of a set of numbers, it is commonplace to ignore the set and instead refer just to the number itself.

Thus the construction:

:The set of numbers which fulfil the propositional function P (   )n is bounded above with the upper bound N

would be reported as:

:The number n such that P (   )n has the upper bound N.


This construct obscures the details of what is actually being stated. Its use on   is considered an abuse of notation and so discouraged.


This also applies in the case where it is the upper bound of a mapping which is under discussion.


Category:Definitions/Numbers
Category:Definitions/Boundedness",Upper Bound
['Definitions/Boundedness'],Definition:Upper Bound,"A special case of an upper bound of a mapping is an upper bound of a sequence, where the domain of the mapping is $mathbb N$.

Let $left( T, preceq right)$ be an ordered set.

Let $leftlangle x_n rightrangle$ be a sequence in $T$.


Let $leftlangle x_n rightrangle$ be bounded above in $T$ by $H in T$.


Then $H$ is an upper bound of $leftlangle x_n rightrangle$.


=== Real Sequence ===

The concept is usually encountered where $left( T, preceq right)$ is the set of real numbers under the usual ordering $left( mathbb R, le right)$:

Let $leftlangle x_n rightrangle$ be a real sequence.


Let $leftlangle x_n rightrangle$ be bounded above by $H in mathbb R$.


Then $H$ is an upper bound of $leftlangle x_n rightrangle$.


=== Upper Bound of Number ===
When considering the upper bound of a set of numbers, it is commonplace to ignore the set and instead refer just to the number itself.

Thus the construction:

:The set of numbers which fulfil the propositional function $P left(   right)n$ is bounded above with the upper bound $N$

would be reported as:

:The number $n$ such that $P left(   right)n$ has the upper bound $N$.


This construct obscures the details of what is actually being stated. Its use on   is considered an abuse of notation and so discouraged.


This also applies in the case where it is the upper bound of a mapping which is under discussion.


Category:Definitions/Numbers
Category:Definitions/Boundedness",Definition:Upper Bound of Sequence,,false,"A special case of an upper bound of a mapping is an upper bound of a sequence, where the domain of the mapping is ℕ.

Let ( T, ≼) be an ordered set.

Let ⟨ x_n ⟩ be a sequence in T.


Let ⟨ x_n ⟩ be bounded above in T by H ∈ T.


Then H is an upper bound of ⟨ x_n ⟩.


=== Real Sequence ===

The concept is usually encountered where ( T, ≼) is the set of real numbers under the usual ordering ( ℝ, ≤):

Let ⟨ x_n ⟩ be a real sequence.


Let ⟨ x_n ⟩ be bounded above by H ∈ℝ.


Then H is an upper bound of ⟨ x_n ⟩.


=== Upper Bound of Number ===
When considering the upper bound of a set of numbers, it is commonplace to ignore the set and instead refer just to the number itself.

Thus the construction:

:The set of numbers which fulfil the propositional function P (   )n is bounded above with the upper bound N

would be reported as:

:The number n such that P (   )n has the upper bound N.


This construct obscures the details of what is actually being stated. Its use on   is considered an abuse of notation and so discouraged.


This also applies in the case where it is the upper bound of a mapping which is under discussion.


Category:Definitions/Numbers
Category:Definitions/Boundedness",Upper Bound
"['Definitions/Boundedness', 'Definitions/Sequences']",Definition:Upper Bound,"Let $leftlangle x_n rightrangle$ be a real sequence.


Let $leftlangle x_n rightrangle$ be bounded above by $H in mathbb R$.


Then $H$ is an upper bound of $leftlangle x_n rightrangle$.


=== Upper Bound of Number ===
When considering the upper bound of a set of numbers, it is commonplace to ignore the set and instead refer just to the number itself.

Thus the construction:

:The set of numbers which fulfil the propositional function $P left(   right)n$ is bounded above with the upper bound $N$

would be reported as:

:The number $n$ such that $P left(   right)n$ has the upper bound $N$.


This construct obscures the details of what is actually being stated. Its use on   is considered an abuse of notation and so discouraged.


This also applies in the case where it is the upper bound of a mapping which is under discussion.


Category:Definitions/Numbers
Category:Definitions/Boundedness",Definition:Upper Bound of Sequence/Real,,false,"Let ⟨ x_n ⟩ be a real sequence.


Let ⟨ x_n ⟩ be bounded above by H ∈ℝ.


Then H is an upper bound of ⟨ x_n ⟩.


=== Upper Bound of Number ===
When considering the upper bound of a set of numbers, it is commonplace to ignore the set and instead refer just to the number itself.

Thus the construction:

:The set of numbers which fulfil the propositional function P (   )n is bounded above with the upper bound N

would be reported as:

:The number n such that P (   )n has the upper bound N.


This construct obscures the details of what is actually being stated. Its use on   is considered an abuse of notation and so discouraged.


This also applies in the case where it is the upper bound of a mapping which is under discussion.


Category:Definitions/Numbers
Category:Definitions/Boundedness",Upper Bound
['Definitions/Logic'],Definition:Vacuous,"Let $P implies Q$ be a conditional statement.

Suppose that $P$ is false.

Then the statement $P implies Q$ is a vacuous truth, or is vacuously true.


It is frequently encountered in the form:
:$forall x: P left(   right)x implies Q left(   right)x$
when the propositional function $P left(   right)x$ is false for all $x$.

Such a statement is also a vacuous truth.


For example, the statement:
:All cats who are expert chess-players are also fluent in ancient Sanskrit
is (vacuously) true, because (as far as the author knows) there are no cats who are expert chess-players.",Definition:Vacuous Truth,,false,"Let P  Q be a conditional statement.

Suppose that P is false.

Then the statement P  Q is a vacuous truth, or is vacuously true.


It is frequently encountered in the form:
:∀ x: P (   )x  Q (   )x
when the propositional function P (   )x is false for all x.

Such a statement is also a vacuous truth.


For example, the statement:
:All cats who are expert chess-players are also fluent in ancient Sanskrit
is (vacuously) true, because (as far as the author knows) there are no cats who are expert chess-players.",Vacuous
"['Definitions/Empty Set', 'Definitions/Set Theory']",Definition:Vacuous,"The empty set is a set which has no elements.

That is, $x in varnothing$ is false, whatever $x$ is.


It is usually denoted by some variant of a zero with a line through it, for example $varnothing$ or $emptyset$, and can always be represented as $leftlbrace  rightrbrace$.",Definition:Empty Set,,false,"The empty set is a set which has no elements.

That is, x ∈∅ is false, whatever x is.


It is usually denoted by some variant of a zero with a line through it, for example ∅ or ∅, and can always be represented as {}.",Vacuous
['Definitions/Summations'],Definition:Vacuous,"Take the summation:
:$ds sum_{Phi left(   right)j} a_j$
where $Phi left(   right)j$ is a propositional function of $j$.

Suppose that there are no values of $j$ for which $Phi left(   right)j$ is true.

Then $ds sum_{Phi left(   right)j} a_j$ is defined as being $0$.

This summation is called a vacuous summation.


This is because:
:$forall a: a + 0 = a$
where $a$ is a number.

Hence for all $j$ for which $Phi left(   right)j$ is false, the sum is unaffected.


This is most frequently seen in the form:
:$ds sum_{j mathop = m}^n a_j = 0$
where $m > n$.

In this case, $j$ can not at the same time be both greater than or equal to $m$ and less than or equal to $n$.


Some sources consider such a treatment as abuse of notation.",Definition:Summation/Vacuous Summation,,false,"Take the summation:
:∑_Φ(   )j a_j
where Φ(   )j is a propositional function of j.

Suppose that there are no values of j for which Φ(   )j is true.

Then ∑_Φ(   )j a_j is defined as being 0.

This summation is called a vacuous summation.


This is because:
:∀ a: a + 0 = a
where a is a number.

Hence for all j for which Φ(   )j is false, the sum is unaffected.


This is most frequently seen in the form:
:∑_j  = m^n a_j = 0
where m > n.

In this case, j can not at the same time be both greater than or equal to m and less than or equal to n.


Some sources consider such a treatment as abuse of notation.",Vacuous
['Definitions/Continued Products'],Definition:Vacuous,"Take the composite expressed as a continued product:
:$ds prod_{R left(   right)j} a_j$
where $R left(   right)j$ is a propositional function of $j$.

Suppose that there are no values of $j$ for which $R left(   right)j$ is true.

Then $ds prod_{R left(   right)j} a_j$ is defined to be $1$.

Beware: not zero.

This composite is called a vacuous product.


This is because:
:$forall a: a times 1 = a$
where $a$ is a number.

Hence for all $j$ for which $R left(   right)j$ is false, the value of the product is unaffected.


This is most frequently seen in the form:
:$ds prod_{j mathop = m}^n a_j = 1$
where $m > n$.

In this case, $j$ can not at the same time be both greater than or equal to $m$ and less than or equal to $n$.",Definition:Continued Product/Vacuous Product,,false,"Take the composite expressed as a continued product:
:∏_R (   )j a_j
where R (   )j is a propositional function of j.

Suppose that there are no values of j for which R (   )j is true.

Then ∏_R (   )j a_j is defined to be 1.

Beware: not zero.

This composite is called a vacuous product.


This is because:
:∀ a: a × 1 = a
where a is a number.

Hence for all j for which R (   )j is false, the value of the product is unaffected.


This is most frequently seen in the form:
:∏_j  = m^n a_j = 1
where m > n.

In this case, j can not at the same time be both greater than or equal to m and less than or equal to n.",Vacuous
"['Definitions/Valid Arguments', 'Definitions/Logical Arguments']",Definition:Valid,"A valid argument is a logical argument in which the premises provide conclusive reasons for the conclusion.


When a proof is valid, we may say one of the following:
* The conclusion follows from the premises;
* The premises entail the conclusion;
* The conclusion is true on the strength of the premises;
* The conclusion is drawn from the premises;
* The conclusion is deduced from the premises;
* The conclusion is derived from the premises.


=== Proof ===

If all the premises of a valid argument are true, then the conclusion must also therefore be true.

It is not possible for the premises of a valid argument to be true, but for the conclusion to be false.

A proof is a valid argument whose premises are all true.


Hence a valid argument that has one or more false premises is not a proof.


Suppose $P$ is a proposition whose truth or falsehood is to be determined.

Constructing a valid argument upon a set of premises, all of which have previously been established as being true, is called proving $P$.


=== Formal Proof ===

Let $mathscr P$ be a proof system for a formal language $mathcal L$.

Let $mathscr P$ be a proof system for a formal language $mathcal L$.


Let $phi$ be a WFF of $mathcal L$.

A formal proof of $phi$ in $mathscr P$ is a collection of axioms and rules of inference of $mathscr P$ that leads to the conclusion that $phi$ is a theorem of $mathscr P$.


The term formal proof is also used to refer to specific presentations of such collections.

For example, the term applies to tableau proofs in natural deduction.",Definition:Valid Argument,,false,"A valid argument is a logical argument in which the premises provide conclusive reasons for the conclusion.


When a proof is valid, we may say one of the following:
* The conclusion follows from the premises;
* The premises entail the conclusion;
* The conclusion is true on the strength of the premises;
* The conclusion is drawn from the premises;
* The conclusion is deduced from the premises;
* The conclusion is derived from the premises.


=== Proof ===

If all the premises of a valid argument are true, then the conclusion must also therefore be true.

It is not possible for the premises of a valid argument to be true, but for the conclusion to be false.

A proof is a valid argument whose premises are all true.


Hence a valid argument that has one or more false premises is not a proof.


Suppose P is a proposition whose truth or falsehood is to be determined.

Constructing a valid argument upon a set of premises, all of which have previously been established as being true, is called proving P.


=== Formal Proof ===

Let 𝒫 be a proof system for a formal language ℒ.

Let 𝒫 be a proof system for a formal language ℒ.


Let ϕ be a WFF of ℒ.

A formal proof of ϕ in 𝒫 is a collection of axioms and rules of inference of 𝒫 that leads to the conclusion that ϕ is a theorem of 𝒫.


The term formal proof is also used to refer to specific presentations of such collections.

For example, the term applies to tableau proofs in natural deduction.",Valid
['Definitions/Formal Semantics'],Definition:Valid,"Let $mathcal L$ be a formal language.

Part of specifying a formal semantics $mathscr M$ for $mathcal L$ is to define a notion of validity.


Concretely, a precise meaning needs to be assigned to the phrase:

:""The $mathcal L$-WFF $phi$ is valid in the $mathscr M$-structure $mathcal M$.""

It can be expressed symbolically as:

:$mathcal M models_{mathscr M} phi$",Definition:Formal Semantics/Valid,,false,"Let ℒ be a formal language.

Part of specifying a formal semantics ℳ for ℒ is to define a notion of validity.


Concretely, a precise meaning needs to be assigned to the phrase:

:""The ℒ-WFF ϕ is valid in the ℳ-structure ℳ.""

It can be expressed symbolically as:

:ℳ_ℳϕ",Valid
"['Definitions/Predicate Logic', 'Definitions/Algebra', 'Definitions/Variables']",Definition:Value,"A variable $x$ may be (temporarily, conceptually) identified with a particular object.

If so, then that object is called the value of $x$.",Definition:Variable/Value,,false,"A variable x may be (temporarily, conceptually) identified with a particular object.

If so, then that object is called the value of x.",Value
"['Definitions/Absolute Value Function', 'Definitions/Field Theory', 'Definitions/Algebra', 'Definitions/Real Analysis']",Definition:Value,"=== Definition 1 ===
Let $x in mathbb R$ be a real number.


The absolute value of $x$ is denoted $leftlvert x rightrvert$, and is defined using the usual ordering on the real numbers as follows:
:$leftlvert x rightrvert = begin{cases} x & : x > 0 \ 0 & : x = 0 \ -x & : x < 0 end{cases}$

=== Definition 2 ===
Let $x in mathbb R$ be a real number.

The absolute value of $x$ is denoted $leftlvert x rightrvert$, and is defined as:

:$leftlvert x rightrvert = +sqrt {x^2}$

where $+sqrt {x^2}$ is the positive square root of $x^2$.",Definition:Absolute Value,,false,"=== Definition 1 ===
Let x ∈ℝ be a real number.


The absolute value of x is denoted | x |, and is defined using the usual ordering on the real numbers as follows:
:| x | =  x     : x > 0 
 0     : x = 0 
 -x     : x < 0

=== Definition 2 ===
Let x ∈ℝ be a real number.

The absolute value of x is denoted | x |, and is defined as:

:| x | = +√(x^2)

where +√(x^2) is the positive square root of x^2.",Value
['Definitions/Continued Fractions'],Definition:Value,"Let $F$ be a field, such as the field of real numbers $mathbb R$.


=== Finite Continued Fraction ===
Let $F$ be a field, such as the field of real numbers $mathbb R$.

Let $n ge 0$ be a natural number.

Let $leftlangle a_k rightrangle_{0 mathop le k mathop le n}$ be a finite continued fraction in $F$.

Let $overline F = F cup leftlbrace infty rightrbrace$ be extended by infinity.


=== Definition 1 ===

The value $left[ a_0, a_1, ldots, a_n right] in F cup leftlbrace infty rightrbrace$ is the right iteration of the binary operation:
:$left[ cdot, cdot right]: F times overline F to overline F$:
:$left[ a, b right] = a + dfrac 1 b$.
That is, it is recursively defined as:
:$left[ a_0, ldots, a_n right] = begin{cases} 
  a_0 & : n = 0 \
  a_0 + dfrac 1 {left[ a_1, ldots, a_n right] } & : n > 0 \
end{cases}$
or as:
:$left[ a_0, ldots, a_n right] = begin{cases}
  a_0 & : n = 0 \
  left[ a_0, ldots, a_{n - 2}, a_{n - 1} + dfrac 1 {a_n}  right] & : n > 0 \
end{cases}$


=== Definition 2 ===

Let the matrix product:
:$begin{pmatrix} a_0 & 1 \ 1 & 0 end{pmatrix} cdots begin{pmatrix} a_n & 1 \ 1 & 0 end{pmatrix} = begin{pmatrix} x_{11} & x_{12} \ x_{21} & x_{22} end{pmatrix}$


The value of the finite continued fraction is $dfrac{x_{11} }{x_{21} }$

=== Infinite Continued Fraction ===
Let $left( F, leftlVert ,cdot, rightrVert  right)$ be a valued field.

Let $C = leftlangle a_n rightrangle_{n mathop ge 0}$ be a infinite continued fraction in $F$.


Let $C$ converge to $x in F$:

Then $x$ is the value of $C$.",Definition:Value of Continued Fraction,,false,"Let F be a field, such as the field of real numbers ℝ.


=== Finite Continued Fraction ===
Let F be a field, such as the field of real numbers ℝ.

Let n ≥ 0 be a natural number.

Let ⟨ a_k ⟩_0 ≤ k ≤ n be a finite continued fraction in F.

Let F = F ∪{∞} be extended by infinity.


=== Definition 1 ===

The value [ a_0, a_1, …, a_n ] ∈ F ∪{∞} is the right iteration of the binary operation:
:[ ·, ·]: F ×F→F:
:[ a, b ] = a +  1 b.
That is, it is recursively defined as:
:[ a_0, …, a_n ] =  
  a_0     : n = 0 

  a_0 +  1 [ a_1, …, a_n ]     : n > 0
or as:
:[ a_0, …, a_n ] = 
  a_0     : n = 0 
[ a_0, …, a_n - 2, a_n - 1 +  1 a_n]     : n > 0


=== Definition 2 ===

Let the matrix product:
:[ a_0   1;   1   0 ]⋯[ a_n   1;   1   0 ] = [ x_11 x_12; x_21 x_22 ]


The value of the finite continued fraction is x_11x_21

=== Infinite Continued Fraction ===
Let ( F, ‖ · ‖) be a valued field.

Let C = ⟨ a_n ⟩_n ≥ 0 be a infinite continued fraction in F.


Let C converge to x ∈ F:

Then x is the value of C.",Value
['Definitions/Images'],Definition:Value,"Let $f: S to T$ be a mapping.

Let $s in S$.

The image of $s$ (under $f$) is defined as:

:$mathrm {Img} left( s right) = f left(   right)s = ds bigcup leftlbrace t in T: left( s, t right) in f rightrbrace$

That is, $f left(   right)s$ is the element of the codomain of $f$ related to $s$ by $f$.


By the nature of a mapping, $f left(   right)s$ is guaranteed to exist and to be unique for any given $s$ in the domain of $f$.",Definition:Image (Relation Theory)/Mapping/Element,,false,"Let f: S → T be a mapping.

Let s ∈ S.

The image of s (under f) is defined as:

:Img( s ) = f (   )s = ⋃{ t ∈ T: ( s, t ) ∈ f }

That is, f (   )s is the element of the codomain of f related to s by f.


By the nature of a mapping, f (   )s is guaranteed to exist and to be unique for any given s in the domain of f.",Value
"['Definitions/Values of Games', 'Definitions/Game Theory']",Definition:Value,"Let $G$ be a game.


The value of $G$ is the payoff resulting from a solution of $G$.",Definition:Value of Game,,false,"Let G be a game.


The value of G is the payoff resulting from a solution of G.",Value
"['Definitions/Physics', 'Definitions/Physics']",Definition:Value,"Let $K$ be a physical constant.

The value of $K$ is defined as the number of units of the specific physical quantity that go to make up $K$.


Category:Definitions/Physics",Definition:Value (Physics),,false,"Let K be a physical constant.

The value of K is defined as the number of units of the specific physical quantity that go to make up K.


Category:Definitions/Physics",Value
['Definitions/Commutative Algebra'],Definition:Vanishing Ideal,"Let $A$ be a commutative ring with unity.

Let $V subseteq mathrm {Spec} left( A right)$ be a set of prime ideals of $A$.


Its vanishing ideal is its intersection, the set of elements of $A$ that are in each $mathfrak p in V$:
:$I left(   right)V = bigcap V$",Definition:Vanishing Ideal of Set of Prime Ideals,,false,"Let A be a commutative ring with unity.

Let V ⊆Spec( A ) be a set of prime ideals of A.


Its vanishing ideal is its intersection, the set of elements of A that are in each 𝔭∈ V:
:I (   )V = ⋂ V",Vanishing Ideal
['Definitions/Algebraic Geometry'],Definition:Vanishing Ideal,"Let $k$ be a field.

Let $n ge 0$ be a natural number.

Let $k left[ X_1, ldots, X_n right]$ be the polynomial ring in $n$ variables over $k$.

Let $S subseteq mathbb A^n_k$ be a subset of the standard affine space over $k$.


Its vanishing ideal is the ideal:
:$I left(   right)S = leftlbrace f in k left[ X_1, ldots, X_n right] : forall x in S : f left(   right)x = 0 rightrbrace$",Definition:Vanishing Ideal of Subset of Affine Space,,false,"Let k be a field.

Let n ≥ 0 be a natural number.

Let k [ X_1, …, X_n ] be the polynomial ring in n variables over k.

Let S ⊆𝔸^n_k be a subset of the standard affine space over k.


Its vanishing ideal is the ideal:
:I (   )S = { f ∈ k [ X_1, …, X_n ] : ∀ x ∈ S : f (   )x = 0 }",Vanishing Ideal
['Definitions/Vertices of Graphs'],Definition:Vertex," 

Let $G = left( V, E right)$ be a graph.

The vertices (singular: vertex) are the elements of $V$.

Informally, the vertices are the points that are connected by the edges.


In the above, the vertices are the points $A, B, C, D, E, F, G$ which are marked as dots.",Definition:Graph (Graph Theory)/Vertex,,false," 

Let G = ( V, E ) be a graph.

The vertices (singular: vertex) are the elements of V.

Informally, the vertices are the points that are connected by the edges.


In the above, the vertices are the points A, B, C, D, E, F, G which are marked as dots.",Vertex
['Definitions/Geometry'],Definition:Vertex,"A vertex is a point on the boundary of a geometric figure at which $2$ or more line segments meet.

=== Vertex of Polygon ===

:

:

A corner of a polygon is known as a vertex.

Thus, in the polygon above, the vertices are $A, B, C, D$ and $E$.

=== Vertex of Polyhedron ===
The vertices of a polyhedron are the vertices of the polygons which constitute its faces.",Definition:Vertex (Geometry),point,true,"A vertex is a point on the boundary of a geometric figure at which 2 or more line segments meet.

=== Vertex of Polygon ===

:

:

A corner of a polygon is known as a vertex.

Thus, in the polygon above, the vertices are A, B, C, D and E.

=== Vertex of Polyhedron ===
The vertices of a polyhedron are the vertices of the polygons which constitute its faces.",Vertex
"['Definitions/Polygons', 'Definitions/Vertices (Geometry)']",Definition:Vertex,":

A corner of a polygon is known as a vertex.

Thus, in the polygon above, the vertices are $A, B, C, D$ and $E$.",Definition:Polygon/Vertex,,false,":

A corner of a polygon is known as a vertex.

Thus, in the polygon above, the vertices are A, B, C, D and E.",Vertex
"['Definitions/Polyhedra', 'Definitions/Vertices (Geometry)']",Definition:Vertex,The vertices of a polyhedron are the vertices of the polygons which constitute its faces.,Definition:Polyhedron/Vertex,,false,The vertices of a polyhedron are the vertices of the polygons which constitute its faces.,Vertex
['Definitions/Angles'],Definition:Vertex,The point at which the arms of an angle meet is known as the vertex of that angle.,Definition:Angle/Vertex,,false,The point at which the arms of an angle meet is known as the vertex of that angle.,Vertex
['Definitions/Solid Angles'],Definition:Vertex,The common vertex of the angles containing a solid angle is known as the vertex of that solid angle.,Definition:Solid Angle/Vertex,,false,The common vertex of the angles containing a solid angle is known as the vertex of that solid angle.,Vertex
['Definitions/Cones'],Definition:Vertex,"Consider a cone consisting of the set of all straight lines joining the boundary of a plane figure $PQR$ to a point $A$ not in the same plane of $PQR$:


:


In the above diagram, the point $A$ is known as the apex of the cone.",Definition:Cone (Geometry)/Apex,,false,"Consider a cone consisting of the set of all straight lines joining the boundary of a plane figure PQR to a point A not in the same plane of PQR:


:


In the above diagram, the point A is known as the apex of the cone.",Vertex
"['Definitions/Vertices of Conic Sections', 'Definitions/Conic Sections']",Definition:Vertex,"=== Vertex of Ellipse ===
:


Let $K$ be an ellipse.

A vertex of $K$ is either of the two endpoints of the major axis of $K$.


In the above diagram, $V_1$ and $V_2$ are the vertices of $K$.

=== Vertex of Parabola ===
:


Let $P$ be a parabola.

The vertex of $P$ is the point where the axis intersects $P$.


In the above diagram, $V$ is the vertex of $P$.

=== Vertex of Hyperbola ===
",Definition:Vertex of Conic Section,,false,"=== Vertex of Ellipse ===
:


Let K be an ellipse.

A vertex of K is either of the two endpoints of the major axis of K.


In the above diagram, V_1 and V_2 are the vertices of K.

=== Vertex of Parabola ===
:


Let P be a parabola.

The vertex of P is the point where the axis intersects P.


In the above diagram, V is the vertex of P.

=== Vertex of Hyperbola ===
",Vertex
"['Definitions/Vertices of Ellipses', 'Definitions/Vertices of Conic Sections', 'Definitions/Ellipses']",Definition:Vertex,":


Let $K$ be an ellipse.

A vertex of $K$ is either of the two endpoints of the major axis of $K$.


In the above diagram, $V_1$ and $V_2$ are the vertices of $K$.",Definition:Ellipse/Vertex,,false,":


Let K be an ellipse.

A vertex of K is either of the two endpoints of the major axis of K.


In the above diagram, V_1 and V_2 are the vertices of K.",Vertex
"['Definitions/Vertices of Conic Sections', 'Definitions/Parabolas']",Definition:Vertex,":


Let $P$ be a parabola.

The vertex of $P$ is the point where the axis intersects $P$.


In the above diagram, $V$ is the vertex of $P$.",Definition:Parabola/Vertex,,false,":


Let P be a parabola.

The vertex of P is the point where the axis intersects P.


In the above diagram, V is the vertex of P.",Vertex
"['Definitions/Graph Theory', 'Definitions/Walks']",Definition:Walk,"Let $G = left( V, E right)$ be a graph.

A walk $W$ on $G$ is:
:an alternating sequence of vertices $v_1, v_2, ldots$ and edges $e_1, e_2, ldots$ of $G$
:beginning and ending with a vertex
:in which edge $e_j$ of $W$ is incident with the vertex $v_j$ and the vertex $v_{j + 1}$.


A walk between two vertices $u$ and $v$ is called a $u$-$v$ walk.


To describe a walk on a simple graph it is sufficient to list just the vertices in order, as the edges (being unique between vertices) are unambiguous.


=== Closed ===
A closed walk is a walk whose first vertex is the same as the last.

That is, it is a walk which ends where it starts.
=== Open ===
An open walk is a walk whose first vertex and last vertex are distinct.

That is, it is a walk which ends on a different vertex from the one where it starts.",Definition:Walk (Graph Theory),,false,"Let G = ( V, E ) be a graph.

A walk W on G is:
:an alternating sequence of vertices v_1, v_2, … and edges e_1, e_2, … of G
:beginning and ending with a vertex
:in which edge e_j of W is incident with the vertex v_j and the vertex v_j + 1.


A walk between two vertices u and v is called a u-v walk.


To describe a walk on a simple graph it is sufficient to list just the vertices in order, as the edges (being unique between vertices) are unambiguous.


=== Closed ===
A closed walk is a walk whose first vertex is the same as the last.

That is, it is a walk which ends where it starts.
=== Open ===
An open walk is a walk whose first vertex and last vertex are distinct.

That is, it is a walk which ends on a different vertex from the one where it starts.",Walk
['Definitions/Walks'],Definition:Walk,"A closed walk is a walk whose first vertex is the same as the last.

That is, it is a walk which ends where it starts.",Definition:Walk (Graph Theory)/Closed,,false,"A closed walk is a walk whose first vertex is the same as the last.

That is, it is a walk which ends where it starts.",Walk
['Definitions/Walks'],Definition:Walk,"An open walk is a walk whose first vertex and last vertex are distinct.

That is, it is a walk which ends on a different vertex from the one where it starts.",Definition:Walk (Graph Theory)/Open,,false,"An open walk is a walk whose first vertex and last vertex are distinct.

That is, it is a walk which ends on a different vertex from the one where it starts.",Walk
"['Definitions/Digraphs', 'Definitions/Walks']",Definition:Walk,"Let $G = left( V, A right)$ be a digraph.


A directed walk in $G$ is a finite or infinite sequence $leftlangle x_k rightrangle$ such that:

:$forall k in mathbb N: k + 1 in mathrm {Dom} left( leftlangle x_k rightrangle  right): left( x_k, x_{k + 1}  right) in A$",Definition:Directed Walk,,false,"Let G = ( V, A ) be a digraph.


A directed walk in G is a finite or infinite sequence ⟨ x_k ⟩ such that:

:∀ k ∈ℕ: k + 1 ∈Dom( ⟨ x_k ⟩): ( x_k, x_k + 1) ∈ A",Walk
"['Definitions/Stochastic Processes', 'Definitions/Markov Chains', 'Definitions/Random Walks']",Definition:Walk,"=== One-Dimensional Random Walk ===
Let $leftlangle X_n rightrangle_{n mathop ge 0}$ be a Markov chain whose state space is the set of integers $mathbb Z$.

Let $leftlangle X_n rightrangle$ be such that $X_{n + 1}$ is an element of the set $leftlbrace X_n + 1, X_n, X_n - 1 rightrbrace$.

Then $leftlangle X_n rightrangle$ is a one-dimensional random walk.

Category:Definitions/Stochastic Processes
Category:Definitions/Markov Chains
Category:Definitions/Random Walks",Definition:Random Walk,,false,"=== One-Dimensional Random Walk ===
Let ⟨ X_n ⟩_n ≥ 0 be a Markov chain whose state space is the set of integers ℤ.

Let ⟨ X_n ⟩ be such that X_n + 1 is an element of the set { X_n + 1, X_n, X_n - 1 }.

Then ⟨ X_n ⟩ is a one-dimensional random walk.

Category:Definitions/Stochastic Processes
Category:Definitions/Markov Chains
Category:Definitions/Random Walks",Walk
"['Definitions/Periodic Waves', 'Definitions/Wavelength']",Definition:Wavelength,"Let $phi$ be a periodic wave expressed as:
:$forall x, t in mathbb R: phi left(   right){x, t} = f left(   right){x - c t}$


The wavelength $lambda$ of $phi$ is the period of the wave profile of $phi$.",Definition:Periodic Wave/Wavelength,,false,"Let ϕ be a periodic wave expressed as:
:∀ x, t ∈ℝ: ϕ(   )x, t = f (   )x - c t


The wavelength λ of ϕ is the period of the wave profile of ϕ.",Wavelength
['Definitions/Physics'],Definition:Wavelength,"The wavelength of a wave is is the distance over which the wave's shape repeats.


 ",Definition:Wavelength (Physics),,false,"The wavelength of a wave is is the distance over which the wave's shape repeats.


 ",Wavelength
['Definitions/Conditional'],Definition:Weak,"In a conditional $p implies q$, the statement $q$ is weaker than $p$.",Definition:Conditional/Language of Conditional/Weak,,false,"In a conditional p  q, the statement q is weaker than p.",Weak
"['Definitions/Examples of Topologies', 'Definitions/Initial Topology']",Definition:Weak,"Let $X$ be a set.

Let $I$ be an indexing set.


Let $leftlangle left( Y_i, tau_i right)  rightrangle_{i mathop in I}$ be an indexed family of topological spaces indexed by $I$.

Let $leftlangle f_i: X to Y_i rightrangle_{i mathop in I}$ be an indexed family of mappings indexed by $I$.


=== Definition 1 ===
Let $X$ be a set.

Let $I$ be an indexing set.


Let $leftlangle left( Y_i, tau_i right)  rightrangle_{i mathop in I}$ be an indexed family of topological spaces indexed by $I$.

Let $leftlangle f_i: X to Y_i rightrangle_{i mathop in I}$ be an indexed family of mappings indexed by $I$.


Let:
:$mathcal S = leftlbrace f_i^{-1} left[ U right]: i in I, U in tau_i rightrbrace$
where $f_i^{-1} left[ U right]$ denotes the preimage of $U$ under $f_i$.

The topology $tau$ on $X$ generated by $mathcal S$ is called the initial topology on $X$ with respect to $leftlangle f_i rightrangle_{i mathop in I}$.

=== Definition 2 ===
Let $X$ be a set.

Let $I$ be an indexing set.


Let $leftlangle left( Y_i, tau_i right)  rightrangle_{i mathop in I}$ be an indexed family of topological spaces indexed by $I$.

Let $leftlangle f_i: X to Y_i rightrangle_{i mathop in I}$ be an indexed family of mappings indexed by $I$.


Let $tau$ be the coarsest topology on $X$ such that each $f_i: X to Y_i$ is $left( tau, tau_i right)$-continuous.

Then $tau$ is known as the initial topology on $X$ with respect to $leftlangle f_i rightrangle_{i mathop in I}$.",Definition:Initial Topology,,false,"Let X be a set.

Let I be an indexing set.


Let ⟨( Y_i, τ_i )  ⟩_i ∈ I be an indexed family of topological spaces indexed by I.

Let ⟨ f_i: X → Y_i ⟩_i ∈ I be an indexed family of mappings indexed by I.


=== Definition 1 ===
Let X be a set.

Let I be an indexing set.


Let ⟨( Y_i, τ_i )  ⟩_i ∈ I be an indexed family of topological spaces indexed by I.

Let ⟨ f_i: X → Y_i ⟩_i ∈ I be an indexed family of mappings indexed by I.


Let:
:𝒮 = { f_i^-1[ U ]: i ∈ I, U ∈τ_i }
where f_i^-1[ U ] denotes the preimage of U under f_i.

The topology τ on X generated by 𝒮 is called the initial topology on X with respect to ⟨ f_i ⟩_i ∈ I.

=== Definition 2 ===
Let X be a set.

Let I be an indexing set.


Let ⟨( Y_i, τ_i )  ⟩_i ∈ I be an indexed family of topological spaces indexed by I.

Let ⟨ f_i: X → Y_i ⟩_i ∈ I be an indexed family of mappings indexed by I.


Let τ be the coarsest topology on X such that each f_i: X → Y_i is ( τ, τ_i )-continuous.

Then τ is known as the initial topology on X with respect to ⟨ f_i ⟩_i ∈ I.",Weak
['Definitions/Compact Spaces'],Definition:Weak,"Let $T = left( S, tau right)$ be a topological space.


Then $T$ is weakly locally compact  if and only if  every point of $S$ has a compact neighborhood.",Definition:Weakly Locally Compact Space,,false,"Let T = ( S, τ) be a topological space.


Then T is weakly locally compact  if and only if  every point of S has a compact neighborhood.",Weak
"['Definitions/Weight (Physics)', 'Definitions/Physics', 'Definitions/Dimensions of Measurement']",Definition:Weight,"The weight of a body is the magnitude of the force exerted on it by the influence of a gravitational field.


The context is that the gravitational field in question is usually that of the Earth.


=== Weigh ===
To weigh a body is to determine its weight, and thence its mass.

Similarly we can say that:
:body $B$ weigh $x$
to mean:
:the weight of the body $B$ is $x$
and both mean the same thing.

=== Dimension of Weight ===
The dimension of weight is $mathsf M mathsf L mathsf T^{-2}$: mass times acceleration, that is, a force.

=== Units of Weight ===
The SI unit of weight is $mathrm N$ (newton).

The CGS unit of weight is $mathrm {dyn}$ (dyne).",Definition:Weight (Physics),magnitude,true,"The weight of a body is the magnitude of the force exerted on it by the influence of a gravitational field.


The context is that the gravitational field in question is usually that of the Earth.


=== Weigh ===
To weigh a body is to determine its weight, and thence its mass.

Similarly we can say that:
:body B weigh x
to mean:
:the weight of the body B is x
and both mean the same thing.

=== Dimension of Weight ===
The dimension of weight is 𝖬𝖫𝖳^-2: mass times acceleration, that is, a force.

=== Units of Weight ===
The SI unit of weight is N (newton).

The CGS unit of weight is dyn (dyne).",Weight
['Definitions/Network Theory'],Definition:Weight,"Let $N = left( V, E, w right)$ be a network with weight function $w: E to mathbb R$.


The values of the elements of $E$ under $w$ are known as the weights of the edges of $N$.


The weights of a network $N$ can be depicted by writing the appropriate numbers next to the edges of the underlying graph of $N$.",Definition:Network/Weight,,false,"Let N = ( V, E, w ) be a network with weight function w: E →ℝ.


The values of the elements of E under w are known as the weights of the edges of N.


The weights of a network N can be depicted by writing the appropriate numbers next to the edges of the underlying graph of N.",Weight
['Definitions/Network Theory'],Definition:Weight,"Let $N = left({V, E, w}right)$ be a network.

The mapping $w: E to mathbb R$ is known as the weight function of $N$.",Definition:Network/Weight Function,,false,"Let N = (V, E, w) be a network.

The mapping w: E →ℝ is known as the weight function of N.",Weight
"['Definitions/Statistics', 'Definitions/Discrete Mathematics', 'Definitions/Analysis']",Definition:Weight,"A weight function on a set $S$ is a mapping from $S$ to the real numbers:
:$w: S to mathbb R$


It is common for the requirements of a specific application under discussion for the codomain of $w$ to be restricted to the positive reals:
:$w: S to mathbb R_{ge 0}$


The thing that determines whether a given mapping is a weight function depends more on how it is used.",Definition:Weight Function,,false,"A weight function on a set S is a mapping from S to the real numbers:
:w: S →ℝ


It is common for the requirements of a specific application under discussion for the codomain of w to be restricted to the positive reals:
:w: S →ℝ_≥ 0


The thing that determines whether a given mapping is a weight function depends more on how it is used.",Weight
['Definitions/Linear Codes'],Definition:Weight,"Let $C$ be a codeword of a linear code.

The weight of $C$ is the number of non-zero terms of $C$.",Definition:Weight of Linear Codeword,,false,"Let C be a codeword of a linear code.

The weight of C is the number of non-zero terms of C.",Weight
['Definitions/Weight (Physics)'],Definition:Weight,"A set of weights consists of a number of solid pieces of matter whose mass is measured and known.

They are used as reference masses for the purpose of determining to a certain limit of accuracy the weight of a body whose mass is unknown.


 ",Definition:Weights,,false,"A set of weights consists of a number of solid pieces of matter whose mass is measured and known.

They are used as reference masses for the purpose of determining to a certain limit of accuracy the weight of a body whose mass is unknown.


 ",Weight
"['Definitions/Words (Abstract Algebra)', 'Definitions/Group Theory', 'Definitions/Abstract Algebra']",Definition:Word,"Let $left( M, circ right)$ be a magma.

Let $S subseteq M$ be a subset.

A word in $S$ is the product of a finite number of elements of $S$.


The set of words in $S$ is denoted $W left(   right)S$:
:$W left(   right)S := leftlbrace s_1 circ s_2 circ cdots circ s_n: n in mathbb N_{>0}: s_i in S, 1 le i le n rightrbrace$


Note that there is nothing in this definition preventing any of the elements of $S$ being repeated, neither is anything said about the order of these elements.


=== Monoid ===
Let $left( M, circ right)$ be a monoid whose identity element is $e$.

Let $S subseteq M$ be a subset of $M$.

The set of words in $S$ is denoted and defined:
:$W left(   right)S := leftlbrace ds sum_{i mathop = 1}^r n_i cdot s_i : r in mathbb N, n_i in mathbb N, s_i in S rightrbrace$
where:
:$n_i cdot s_i$ denotes the power of $s_i$:
::$n cdot a = begin {cases}
e & : n = 0 \
left( left( n - 1 right) cdot a right) circ a & : n > 0
end {cases}$
:$ds sum_{i mathop = 1}^r n_i cdot s_i := left( n_1 cdot s_1 right) circ left( n_2 cdot s_2 right) circ cdots circ left( n_r cdot s_r right)$",Definition:Word (Abstract Algebra),,false,"Let ( M, ∘) be a magma.

Let S ⊆ M be a subset.

A word in S is the product of a finite number of elements of S.


The set of words in S is denoted W (   )S:
:W (   )S := { s_1 ∘ s_2 ∘⋯∘ s_n: n ∈ℕ_>0: s_i ∈ S, 1 ≤ i ≤ n }


Note that there is nothing in this definition preventing any of the elements of S being repeated, neither is anything said about the order of these elements.


=== Monoid ===
Let ( M, ∘) be a monoid whose identity element is e.

Let S ⊆ M be a subset of M.

The set of words in S is denoted and defined:
:W (   )S := {∑_i  = 1^r n_i · s_i : r ∈ℕ, n_i ∈ℕ, s_i ∈ S }
where:
:n_i · s_i denotes the power of s_i:
::n · a = 
e     : n = 0 
( ( n - 1 ) · a ) ∘ a     : n > 0
:∑_i  = 1^r n_i · s_i := ( n_1 · s_1 ) ∘( n_2 · s_2 ) ∘⋯∘( n_r · s_r )",Word
"['Definitions/Set Theory', 'Definitions/Mapping Theory', 'Definitions/Cartesian Product', 'Definitions/Sequences', 'Definitions/Ordered Tuples']",Definition:Word,"Let $n in mathbb N$ be a natural number.

Let $mathbb N^*_n$ be the first $n$ non-zero natural numbers:
:$mathbb N^*_n := leftlbrace 1, 2, ldots, n rightrbrace$


=== Definition 1 ===
Let $n in mathbb N$ be a natural number.

Let $mathbb N^*_n$ be the first $n$ non-zero natural numbers:
:$mathbb N^*_n := leftlbrace 1, 2, ldots, n rightrbrace$


An ordered tuple (of length $n$) is a finite sequence whose domain is $mathbb N^*_n$.

=== Definition 2 ===
Let $n in mathbb N$ be a natural number.

Let $mathbb N_n$ denote the first $n$ non-zero natural numbers:
:$mathbb N_n := leftlbrace 1, 2, ldots, n rightrbrace$


Let $leftlangle S_i rightrangle_{i mathop in mathbb N_n}$ be a family of sets indexed by $mathbb N_n$.

Let $ds prod_{i mathop in mathbb N_n} S_i$ be the Cartesian product of $leftlangle S_i rightrangle_{i mathop in mathbb N_n}$.

An ordered tuple of length $n$ of $leftlangle S_i rightrangle$ is an element of $ds prod_{i mathop in mathbb N_n} S_i$.

=== Ordered Tuple on Set ===
Let $S$ be a set.

Let $s: mathbb N^*_n to S$ be an ordered tuple.


Then $s$ is called an ordered tuple on $S$, its codomain.


Category:Definitions/Ordered Tuples

=== Empty Ordered Tuple ===
Let $S$ be a set.

The empty ordered tuple on $S$ is the empty mapping:

:$varnothing to S$

from the empty set $varnothing$ to $S$.

It is justified to call this an ordered tuple because the ""first $0$ non-zero natural numbers"" form the empty set:

:$mathbb N^*_0 = varnothing$

=== Ordered Tuple Defined by Sequence ===
Let $leftlangle a_k rightrangle_{k mathop in A}$ be a finite sequence of $n$ terms.

Let $sigma$ be a permutation of $A$.


Then the ordered $n$-tuple defined by the sequence $leftlangle a_{sigma left(   right)k}  rightrangle_{k mathop in A}$ is the ordered $n$-tuple:
:$leftlangle a_{sigma left(   right){tau left(   right)j} }  rightrangle_{1 mathop le j mathop le n}$
where $tau$ is the unique isomorphism from the totally ordered set $left[ 1 ,.,.,   right]n$ onto the totally ordered set $A$.


 ",Definition:Ordered Tuple,,false,"Let n ∈ℕ be a natural number.

Let ℕ^*_n be the first n non-zero natural numbers:
:ℕ^*_n := { 1, 2, …, n }


=== Definition 1 ===
Let n ∈ℕ be a natural number.

Let ℕ^*_n be the first n non-zero natural numbers:
:ℕ^*_n := { 1, 2, …, n }


An ordered tuple (of length n) is a finite sequence whose domain is ℕ^*_n.

=== Definition 2 ===
Let n ∈ℕ be a natural number.

Let ℕ_n denote the first n non-zero natural numbers:
:ℕ_n := { 1, 2, …, n }


Let ⟨ S_i ⟩_i ∈ℕ_n be a family of sets indexed by ℕ_n.

Let ∏_i ∈ℕ_n S_i be the Cartesian product of ⟨ S_i ⟩_i ∈ℕ_n.

An ordered tuple of length n of ⟨ S_i ⟩ is an element of ∏_i ∈ℕ_n S_i.

=== Ordered Tuple on Set ===
Let S be a set.

Let s: ℕ^*_n → S be an ordered tuple.


Then s is called an ordered tuple on S, its codomain.


Category:Definitions/Ordered Tuples

=== Empty Ordered Tuple ===
Let S be a set.

The empty ordered tuple on S is the empty mapping:

:∅→ S

from the empty set ∅ to S.

It is justified to call this an ordered tuple because the ""first 0 non-zero natural numbers"" form the empty set:

:ℕ^*_0 = ∅

=== Ordered Tuple Defined by Sequence ===
Let ⟨ a_k ⟩_k ∈ A be a finite sequence of n terms.

Let σ be a permutation of A.


Then the ordered n-tuple defined by the sequence ⟨ a_σ(   )k⟩_k ∈ A is the ordered n-tuple:
:⟨ a_σ(   )τ(   )j⟩_1 ≤ j ≤ n
where τ is the unique isomorphism from the totally ordered set [ 1  . . ]n onto the totally ordered set A.


 ",Word
['Definitions/Group Words'],Definition:Word,"Let $S$ be a set.


A group word on $S$ is an ordered tuple on the set of literals $S^pm$ of $S$.",Definition:Group Word on Set,,false,"Let S be a set.


A group word on S is an ordered tuple on the set of literals S^± of S.",Word
['Definitions/Collations'],Definition:Word,"Let $mathcal A$ be an alphabet.


Then a word in $mathcal A$ is a juxtaposition of finitely many (primitive) symbols of $mathcal A$.

Words are the most ubiquitous of collations used for formal languages.",Definition:Word (Formal Systems),,false,"Let 𝒜 be an alphabet.


Then a word in 𝒜 is a juxtaposition of finitely many (primitive) symbols of 𝒜.

Words are the most ubiquitous of collations used for formal languages.",Word
['Definitions/Language Definitions'],Definition:Word,"A word in natural language is intuitively understood as a sequence of sounds which expresses a concept.

When written down, it appears as a sequence of letters, each one of which either is, or contributes to, a phoneme.",Definition:Word (Natural Language),understood,true,"A word in natural language is intuitively understood as a sequence of sounds which expresses a concept.

When written down, it appears as a sequence of letters, each one of which either is, or contributes to, a phoneme.",Word
"['Definitions/Group Theory', 'Definitions/Examples of Metric Spaces']",Definition:Word,"Let $left( G, circ right)$ be a group.

Let $S$ be a generating set for $G$ which is closed under inverses (that is, $x^{-1} in S iff x in S$).


The word metric on $G$ with respect to $S$ is the metric $d_S$ defined as follows:

:For any $g, h in G$, let $d_S left(   right){g, h}$ be the minimum length among the finite sequences $left( x_1, dots, x_n right)$ with each $x_i in S$ such that $g circ x_1 circ cdots circ x_n = h$.


Informally, $d_S left(   right){g, h}$ is the smallest number of elements from $S$ that one needs to multiply by to get from $g$ to $h$.",Definition:Word Metric,,false,"Let ( G, ∘) be a group.

Let S be a generating set for G which is closed under inverses (that is, x^-1∈ S  x ∈ S).


The word metric on G with respect to S is the metric d_S defined as follows:

:For any g, h ∈ G, let d_S (   )g, h be the minimum length among the finite sequences ( x_1, …, x_n ) with each x_i ∈ S such that g ∘ x_1 ∘⋯∘ x_n = h.


Informally, d_S (   )g, h is the smallest number of elements from S that one needs to multiply by to get from g to h.",Word
['Definitions/Ordinals'],Definition:Zero,"The zero ordinal, denoted $0$, is the empty set $varnothing$.",Definition:Zero (Ordinal),,false,"The zero ordinal, denoted 0, is the empty set ∅.",Zero
['Definitions/Cardinals'],Definition:Zero,"The cardinal associated with the empty set $varnothing$ is called zero, and is denoted $0$.


More informally, this means that zero is defined as being the number of elements in the empty set.",Definition:Zero (Cardinal),,false,"The cardinal associated with the empty set ∅ is called zero, and is denoted 0.


More informally, this means that zero is defined as being the number of elements in the empty set.",Zero
"['Definitions/Abstract Algebra', 'Definitions/Zero']",Definition:Zero,"The number zero is defined as being the cardinal of the empty set.


=== Naturally Ordered Semigroup ===
Let $left( S, circ, preceq right)$ be a naturally ordered semigroup.

Then from  , $left( S, circ, preceq right)$ has a smallest element.


This smallest element of $left( S, circ, preceq right)$ is called zero and has the symbol $0$.

That is:
:$forall n in S: 0 preceq n$

=== Natural Numbers ===

=== Integers ===

=== Rational Numbers ===

=== Real Numbers ===

=== Complex Numbers ===
Let $mathbb C$ denote the set of complex numbers.

The zero of $mathbb C$ is the complex number:
:$0 + 0 i$

 ",Definition:Zero (Number),,false,"The number zero is defined as being the cardinal of the empty set.


=== Naturally Ordered Semigroup ===
Let ( S, ∘, ≼) be a naturally ordered semigroup.

Then from  , ( S, ∘, ≼) has a smallest element.


This smallest element of ( S, ∘, ≼) is called zero and has the symbol 0.

That is:
:∀ n ∈ S: 0 ≼ n

=== Natural Numbers ===

=== Integers ===

=== Rational Numbers ===

=== Real Numbers ===

=== Complex Numbers ===
Let ℂ denote the set of complex numbers.

The zero of ℂ is the complex number:
:0 + 0 i

 ",Zero
"['Definitions/Complex Numbers', 'Definitions/Zero']",Definition:Zero,"Let $mathbb C$ denote the set of complex numbers.

The zero of $mathbb C$ is the complex number:
:$0 + 0 i$",Definition:Zero (Number)/Complex,,false,"Let ℂ denote the set of complex numbers.

The zero of ℂ is the complex number:
:0 + 0 i",Zero
"['Definitions/Zero Digit', 'Definitions/Zero', 'Definitions/Digits', 'Definitions/Numbers']",Definition:Zero,"Let $x in mathbb R$ be a number.

Let $b in mathbb Z$ such that $b > 1$ be a number base in which $x$ is represented.

By the Basis Representation Theorem, $x$ can be expressed uniquely in the form:

:$ds x = sum_{j mathop in mathbb Z}^m r_j b^j$


Any instance of $r_j$ being equal to $0$ is known as a zero (digit) of $n$.",Definition:Zero Digit,,false,"Let x ∈ℝ be a number.

Let b ∈ℤ such that b > 1 be a number base in which x is represented.

By the Basis Representation Theorem, x can be expressed uniquely in the form:

:x = ∑_j ∈ℤ^m r_j b^j


Any instance of r_j being equal to 0 is known as a zero (digit) of n.",Zero
"['Definitions/Naturally Ordered Semigroup', 'Definitions/Zero']",Definition:Zero,"Let $left( S, circ, preceq right)$ be a naturally ordered semigroup.

Then from  , $left( S, circ, preceq right)$ has a smallest element.


This smallest element of $left( S, circ, preceq right)$ is called zero and has the symbol $0$.

That is:
:$forall n in S: 0 preceq n$",Definition:Zero (Number)/Naturally Ordered Semigroup,,false,"Let ( S, ∘, ≼) be a naturally ordered semigroup.

Then from  , ( S, ∘, ≼) has a smallest element.


This smallest element of ( S, ∘, ≼) is called zero and has the symbol 0.

That is:
:∀ n ∈ S: 0 ≼ n",Zero
"['Definitions/Abstract Algebra', 'Definitions/Zero Elements']",Definition:Zero,"Let $left( S, circ right)$ be an algebraic structure.


=== Left Zero ===
Let $left( S, circ right)$ be an algebraic structure.

An element $z_L in S$ is called a left zero element (or just left zero)  if and only if :
:$forall x in S: z_L circ x = z_L$

=== Right Zero ===
Let $left( S, circ right)$ be an algebraic structure.

An element $z_R in S$ is called a right zero element (or just right zero)  if and only if :
:$forall x in S: x circ z_R = z_R$

=== Zero ===

An element $z in S$ is called a two-sided zero element (or simply zero element or zero)  if and only if  it is both a left zero and a right zero:
:$forall x in S: x circ z = z = z circ x$",Definition:Zero Element,,false,"Let ( S, ∘) be an algebraic structure.


=== Left Zero ===
Let ( S, ∘) be an algebraic structure.

An element z_L ∈ S is called a left zero element (or just left zero)  if and only if :
:∀ x ∈ S: z_L ∘ x = z_L

=== Right Zero ===
Let ( S, ∘) be an algebraic structure.

An element z_R ∈ S is called a right zero element (or just right zero)  if and only if :
:∀ x ∈ S: x ∘ z_R = z_R

=== Zero ===

An element z ∈ S is called a two-sided zero element (or simply zero element or zero)  if and only if  it is both a left zero and a right zero:
:∀ x ∈ S: x ∘ z = z = z ∘ x",Zero
['Definitions/Ring Theory'],Definition:Zero,"Let $left( R, +, circ right)$ be a ring.

The identity for ring addition is called the ring zero (of $left( R, +, circ right)$).


It is denoted $0_R$ (or just $0$ if there is no danger of ambiguity).",Definition:Ring Zero,,false,"Let ( R, +, ∘) be a ring.

The identity for ring addition is called the ring zero (of ( R, +, ∘)).


It is denoted 0_R (or just 0 if there is no danger of ambiguity).",Zero
['Definitions/Field Theory'],Definition:Zero,"Let $left( F, +, times right)$ be a field.

The identity for field addition is called the field zero (of $left( F, +, times right)$).


It is denoted $0_F$ (or just $0$ if there is no danger of ambiguity).",Definition:Field Zero,,false,"Let ( F, +, ×) be a field.

The identity for field addition is called the field zero (of ( F, +, ×)).


It is denoted 0_F (or just 0 if there is no danger of ambiguity).",Zero
['Definitions/Mapping Theory'],Definition:Zero,"Let $mathbb A$ be one of the standard number systems $mathbb N,mathbb Z,mathbb Q,mathbb R,mathbb C$.

Let $S$ be a set.


Let $f_0: S to mathbb A$ denote the constant mapping:

:$forall x in S: f_0 left(   right)x = 0$

Then $f_0$ is referred to as the zero mapping.


=== Vector Space ===
Let $Y$ be a vector space.

Let $S$ be a set.

Let $mathbf 0_Y$ be the identity element of $Y$.

Suppose $mathbf 0 : S to Y$ is a mapping such that:

:$forall x in S: mathbf 0 left(   right)x = mathbf 0_Y$


Then $mathbf 0$ is referred to as the zero mapping.

=== Distribution ===
Let $mathcal D left(   right)mathbb R$ be the test function space.

Let $mathbf 0 in mathcal D' left(   right)mathbb R$ be a distribution.

Suppose:

:$forall phi in mathcal D left(   right)mathbb R : mathbf 0 left(   right)phi = 0$


Then $mathbf 0$ is referred to as the zero distribution.


Category:Definitions/Distributions",Definition:Zero Mapping,,false,"Let 𝔸 be one of the standard number systems ℕ,ℤ,ℚ,ℝ,ℂ.

Let S be a set.


Let f_0: S →𝔸 denote the constant mapping:

:∀ x ∈ S: f_0 (   )x = 0

Then f_0 is referred to as the zero mapping.


=== Vector Space ===
Let Y be a vector space.

Let S be a set.

Let 0_Y be the identity element of Y.

Suppose 0 : S → Y is a mapping such that:

:∀ x ∈ S: 0(   )x = 0_Y


Then 0 is referred to as the zero mapping.

=== Distribution ===
Let 𝒟(   )ℝ be the test function space.

Let 0∈𝒟' (   )ℝ be a distribution.

Suppose:

:∀ϕ∈𝒟(   )ℝ : 0(   )ϕ = 0


Then 0 is referred to as the zero distribution.


Category:Definitions/Distributions",Zero
"['Definitions/Roots of Mappings', 'Definitions/Ring Theory', 'Definitions/Field Theory', 'Definitions/Real Analysis', 'Definitions/Complex Analysis']",Definition:Zero,"Let $f: R to R$ be a mapping on a ring $R$.

Let $x in R$.


Then the values of $x$ for which $f left(   right)x = 0_R$ are known as the roots of the mapping $f$.",Definition:Root of Mapping,,false,"Let f: R → R be a mapping on a ring R.

Let x ∈ R.


Then the values of x for whic",