2025-02-28 09:31:58,411 : 1196508602.py : eval_pred : INFO : processing 0 lines 
2025-02-28 09:36:06,427 : 1196508602.py : eval_pred : INFO : processing 100 lines 
2025-02-28 09:38:24,226 : 1196508602.py : eval_pred : INFO : processing 200 lines 
2025-02-28 09:40:53,644 : 1196508602.py : eval_pred : INFO : processing 300 lines 
2025-02-28 09:43:35,485 : 1196508602.py : eval_pred : INFO : processing 400 lines 
2025-02-28 12:05:15,968 : 1196508602.py : eval_pred : INFO : processing 0 lines 
2025-02-28 12:06:47,416 : 1196508602.py : eval_pred : INFO : processing 100 lines 
2025-02-28 12:08:05,397 : 1196508602.py : eval_pred : INFO : processing 200 lines 
2025-02-28 12:09:26,252 : 1196508602.py : eval_pred : INFO : processing 300 lines 
2025-02-28 12:38:14,257 : 1196508602.py : eval_pred : INFO : processing 0 lines 
2025-02-28 12:39:18,314 : 1196508602.py : eval_pred : INFO : processing 100 lines 
2025-02-28 12:40:25,313 : 1196508602.py : eval_pred : INFO : processing 200 lines 
2025-02-28 12:41:44,071 : 1196508602.py : eval_pred : INFO : processing 300 lines 
2025-02-28 12:43:08,590 : 1196508602.py : eval_pred : INFO : processing 400 lines 
2025-02-28 12:44:18,852 : 1196508602.py : eval_pred : INFO : processing 500 lines 
2025-02-28 12:45:28,745 : 1196508602.py : eval_pred : INFO : processing 600 lines 
2025-02-28 12:46:42,610 : 1196508602.py : eval_pred : INFO : processing 700 lines 
2025-02-28 12:48:01,376 : 1196508602.py : eval_pred : INFO : processing 800 lines 
2025-02-28 12:49:25,208 : 1196508602.py : eval_pred : INFO : processing 900 lines 
2025-02-28 12:50:37,437 : 1196508602.py : eval_pred : INFO : processing 1000 lines 
2025-02-28 12:51:56,351 : 1196508602.py : eval_pred : INFO : processing 1100 lines 
2025-02-28 15:05:23,358 : 1196508602.py : eval_pred : INFO : processing 0 lines 
2025-02-28 15:12:03,717 : 1196508602.py : eval_pred : INFO : processing 100 lines 
2025-02-28 15:17:14,911 : 1196508602.py : eval_pred : INFO : processing 200 lines 
2025-02-28 15:22:32,267 : 1196508602.py : eval_pred : INFO : processing 300 lines 
2025-02-28 15:27:55,350 : 1196508602.py : eval_pred : INFO : processing 400 lines 
2025-02-28 16:24:09,805 : 1196508602.py : eval_pred : INFO : processing 0 lines 
2025-02-28 16:27:39,870 : 1196508602.py : eval_pred : INFO : processing 100 lines 
2025-02-28 16:31:22,960 : 1196508602.py : eval_pred : INFO : processing 200 lines 
2025-02-28 16:35:15,181 : 1196508602.py : eval_pred : INFO : processing 300 lines 
2025-02-28 16:50:40,214 : 1196508602.py : eval_pred : INFO : processing 0 lines 
2025-02-28 16:51:28,232 : 1196508602.py : eval_pred : INFO : processing 100 lines 
2025-02-28 16:51:53,999 : 1196508602.py : eval_pred : INFO : processing 200 lines 
2025-02-28 16:52:15,965 : 1196508602.py : eval_pred : INFO : processing 300 lines 
2025-02-28 16:53:52,419 : 1196508602.py : eval_pred : INFO : processing 0 lines 
2025-02-28 16:56:19,834 : 1196508602.py : eval_pred : INFO : processing 100 lines 
2025-02-28 16:57:50,604 : 1196508602.py : eval_pred : INFO : processing 200 lines 
2025-02-28 16:58:17,349 : 1196508602.py : eval_pred : INFO : processing 300 lines 
2025-02-28 16:58:29,113 : 1196508602.py : eval_pred : INFO : processing 400 lines 
2025-02-28 17:19:26,775 : 1196508602.py : eval_pred : INFO : processing 0 lines 
2025-02-28 17:19:27,001 : 1196508602.py : eval_pred : INFO : processing 100 lines 
2025-02-28 17:19:27,230 : 1196508602.py : eval_pred : INFO : processing 200 lines 
2025-02-28 17:19:27,462 : 1196508602.py : eval_pred : INFO : processing 300 lines 
2025-02-28 17:19:27,782 : 1196508602.py : eval_pred : INFO : processing 0 lines 
2025-02-28 17:19:28,835 : 1196508602.py : eval_pred : INFO : processing 100 lines 
2025-02-28 17:19:29,299 : 1196508602.py : eval_pred : INFO : processing 200 lines 
2025-02-28 17:19:30,056 : 1196508602.py : eval_pred : INFO : processing 300 lines 
2025-02-28 17:19:30,561 : 1196508602.py : eval_pred : INFO : processing 400 lines 
2025-02-28 18:57:31,546 : 1196508602.py : eval_pred : INFO : processing 0 lines 
2025-02-28 18:57:31,756 : 1196508602.py : eval_pred : INFO : processing 100 lines 
2025-02-28 18:57:31,946 : 1196508602.py : eval_pred : INFO : processing 200 lines 
2025-02-28 18:57:32,155 : 1196508602.py : eval_pred : INFO : processing 300 lines 
2025-02-28 19:05:35,260 : 1196508602.py : eval_pred : INFO : processing 0 lines 
2025-02-28 19:05:37,521 : 1196508602.py : eval_pred : INFO : processing 100 lines 
2025-02-28 19:05:39,839 : 1196508602.py : eval_pred : INFO : processing 200 lines 
2025-02-28 19:05:41,960 : 1196508602.py : eval_pred : INFO : processing 300 lines 
2025-02-28 19:05:43,673 : 1196508602.py : eval_pred : INFO : processing 400 lines 
2025-02-28 19:05:44,094 : 1196508602.py : eval_pred : INFO : processing 0 lines 
2025-02-28 19:05:44,173 : 1196508602.py : eval_pred : INFO : processing 100 lines 
2025-02-28 19:05:44,246 : 1196508602.py : eval_pred : INFO : processing 200 lines 
2025-02-28 19:05:44,327 : 1196508602.py : eval_pred : INFO : processing 300 lines 
2025-02-28 19:05:44,403 : 1196508602.py : eval_pred : INFO : processing 400 lines 
2025-02-28 19:05:44,482 : 1196508602.py : eval_pred : INFO : processing 500 lines 
2025-02-28 19:05:44,548 : 1196508602.py : eval_pred : INFO : processing 600 lines 
2025-02-28 19:05:44,651 : 1196508602.py : eval_pred : INFO : processing 700 lines 
2025-02-28 19:05:44,730 : 1196508602.py : eval_pred : INFO : processing 800 lines 
2025-02-28 19:05:44,813 : 1196508602.py : eval_pred : INFO : processing 900 lines 
2025-02-28 19:05:44,889 : 1196508602.py : eval_pred : INFO : processing 1000 lines 
2025-02-28 19:05:44,980 : 1196508602.py : eval_pred : INFO : processing 1100 lines 
2025-02-28 19:06:35,120 : 1196508602.py : eval_pred : INFO : processing 0 lines 
2025-02-28 19:06:35,256 : 1196508602.py : eval_pred : INFO : processing 100 lines 
2025-02-28 19:06:35,384 : 1196508602.py : eval_pred : INFO : processing 200 lines 
2025-02-28 19:06:35,517 : 1196508602.py : eval_pred : INFO : processing 300 lines 
2025-02-28 19:06:35,693 : 1196508602.py : eval_pred : INFO : processing 0 lines 
2025-02-28 19:06:36,088 : 1196508602.py : eval_pred : INFO : processing 100 lines 
2025-02-28 19:06:36,370 : 1196508602.py : eval_pred : INFO : processing 200 lines 
2025-02-28 19:06:36,837 : 1196508602.py : eval_pred : INFO : processing 300 lines 
2025-02-28 19:06:37,169 : 1196508602.py : eval_pred : INFO : processing 400 lines 
2025-02-28 19:20:36,646 : 1196508602.py : eval_pred : INFO : processing 0 lines 
2025-02-28 19:20:36,795 : 1196508602.py : eval_pred : INFO : processing 100 lines 
2025-02-28 19:20:36,940 : 1196508602.py : eval_pred : INFO : processing 200 lines 
2025-02-28 19:20:37,083 : 1196508602.py : eval_pred : INFO : processing 300 lines 
2025-02-28 19:20:37,290 : 1196508602.py : eval_pred : INFO : processing 0 lines 
2025-02-28 19:20:37,683 : 1196508602.py : eval_pred : INFO : processing 100 lines 
2025-02-28 19:20:37,854 : 1196508602.py : eval_pred : INFO : processing 200 lines 
2025-02-28 19:20:38,141 : 1196508602.py : eval_pred : INFO : processing 300 lines 
2025-02-28 19:20:38,337 : 1196508602.py : eval_pred : INFO : processing 400 lines 
2025-02-28 19:20:38,557 : 1196508602.py : eval_pred : INFO : processing 0 lines 
2025-02-28 19:20:39,200 : 1196508602.py : eval_pred : INFO : processing 100 lines 
2025-02-28 19:20:39,494 : 1196508602.py : eval_pred : INFO : processing 200 lines 
2025-02-28 19:20:39,954 : 1196508602.py : eval_pred : INFO : processing 300 lines 
2025-02-28 19:20:40,299 : 1196508602.py : eval_pred : INFO : processing 400 lines 
2025-02-28 19:33:16,342 : 1196508602.py : eval_pred : INFO : processing 0 lines 
2025-02-28 19:33:16,489 : 1196508602.py : eval_pred : INFO : processing 100 lines 
2025-02-28 19:33:16,630 : 1196508602.py : eval_pred : INFO : processing 200 lines 
2025-02-28 19:33:16,791 : 1196508602.py : eval_pred : INFO : processing 300 lines 
2025-02-28 19:33:16,984 : 1196508602.py : eval_pred : INFO : processing 0 lines 
2025-02-28 19:33:17,610 : 1196508602.py : eval_pred : INFO : processing 100 lines 
2025-02-28 19:33:17,915 : 1196508602.py : eval_pred : INFO : processing 200 lines 
2025-02-28 19:33:18,364 : 1196508602.py : eval_pred : INFO : processing 300 lines 
2025-02-28 19:33:18,681 : 1196508602.py : eval_pred : INFO : processing 400 lines 
2025-02-28 19:45:26,034 : 1196508602.py : eval_pred : INFO : processing 0 lines 
2025-02-28 19:45:26,180 : 1196508602.py : eval_pred : INFO : processing 100 lines 
2025-02-28 19:45:26,319 : 1196508602.py : eval_pred : INFO : processing 200 lines 
2025-02-28 19:45:26,461 : 1196508602.py : eval_pred : INFO : processing 300 lines 
2025-02-28 19:45:26,670 : 1196508602.py : eval_pred : INFO : processing 0 lines 
2025-02-28 19:45:27,384 : 1196508602.py : eval_pred : INFO : processing 100 lines 
2025-02-28 19:45:27,665 : 1196508602.py : eval_pred : INFO : processing 200 lines 
2025-02-28 19:45:28,126 : 1196508602.py : eval_pred : INFO : processing 300 lines 
2025-02-28 19:45:28,465 : 1196508602.py : eval_pred : INFO : processing 400 lines 
2025-02-28 19:50:47,631 : 1196508602.py : eval_pred : INFO : processing 0 lines 
2025-02-28 19:50:49,459 : 1196508602.py : eval_pred : INFO : processing 100 lines 
2025-02-28 19:50:52,131 : 1196508602.py : eval_pred : INFO : processing 200 lines 
2025-02-28 19:50:53,832 : 1196508602.py : eval_pred : INFO : processing 300 lines 
2025-02-28 19:50:55,816 : 1196508602.py : eval_pred : INFO : processing 400 lines 
2025-02-28 19:50:56,127 : 1196508602.py : eval_pred : INFO : processing 0 lines 
2025-02-28 19:50:56,205 : 1196508602.py : eval_pred : INFO : processing 100 lines 
2025-02-28 19:50:56,277 : 1196508602.py : eval_pred : INFO : processing 200 lines 
2025-02-28 19:50:56,357 : 1196508602.py : eval_pred : INFO : processing 300 lines 
2025-02-28 19:50:56,431 : 1196508602.py : eval_pred : INFO : processing 400 lines 
2025-02-28 19:50:56,509 : 1196508602.py : eval_pred : INFO : processing 500 lines 
2025-02-28 19:50:56,573 : 1196508602.py : eval_pred : INFO : processing 600 lines 
2025-02-28 19:50:56,673 : 1196508602.py : eval_pred : INFO : processing 700 lines 
2025-02-28 19:50:56,752 : 1196508602.py : eval_pred : INFO : processing 800 lines 
2025-02-28 19:50:56,833 : 1196508602.py : eval_pred : INFO : processing 900 lines 
2025-02-28 19:50:56,907 : 1196508602.py : eval_pred : INFO : processing 1000 lines 
2025-02-28 19:50:56,995 : 1196508602.py : eval_pred : INFO : processing 1100 lines 
2025-02-28 20:03:00,814 : 1196508602.py : eval_pred : INFO : processing 0 lines 
2025-02-28 20:03:05,078 : 1196508602.py : eval_pred : INFO : processing 100 lines 
2025-02-28 20:03:09,729 : 1196508602.py : eval_pred : INFO : processing 200 lines 
2025-02-28 20:03:15,566 : 1196508602.py : eval_pred : INFO : processing 300 lines 
2025-02-28 20:03:19,202 : 1196508602.py : eval_pred : INFO : processing 400 lines 
2025-02-28 20:03:19,880 : 1196508602.py : eval_pred : INFO : processing 0 lines 
2025-02-28 20:03:23,689 : 1196508602.py : eval_pred : INFO : processing 100 lines 
2025-02-28 20:03:28,157 : 1196508602.py : eval_pred : INFO : processing 200 lines 
2025-02-28 20:03:33,174 : 1196508602.py : eval_pred : INFO : processing 300 lines 
2025-02-28 20:03:36,801 : 1196508602.py : eval_pred : INFO : processing 400 lines 
2025-02-28 20:16:17,444 : 1196508602.py : eval_pred : INFO : processing 0 lines 
2025-02-28 20:16:17,591 : 1196508602.py : eval_pred : INFO : processing 100 lines 
2025-02-28 20:16:17,732 : 1196508602.py : eval_pred : INFO : processing 200 lines 
2025-02-28 20:16:17,875 : 1196508602.py : eval_pred : INFO : processing 300 lines 
2025-02-28 20:16:18,065 : 1196508602.py : eval_pred : INFO : processing 0 lines 
2025-02-28 20:16:18,703 : 1196508602.py : eval_pred : INFO : processing 100 lines 
2025-02-28 20:16:18,980 : 1196508602.py : eval_pred : INFO : processing 200 lines 
2025-02-28 20:16:19,439 : 1196508602.py : eval_pred : INFO : processing 300 lines 
2025-02-28 20:16:19,766 : 1196508602.py : eval_pred : INFO : processing 400 lines 
2025-02-28 20:16:20,682 : 1196508602.py : eval_pred : INFO : processing 0 lines 
2025-02-28 20:16:20,770 : 1196508602.py : eval_pred : INFO : processing 100 lines 
2025-02-28 20:16:20,855 : 1196508602.py : eval_pred : INFO : processing 200 lines 
2025-02-28 20:16:20,942 : 1196508602.py : eval_pred : INFO : processing 300 lines 
2025-02-28 20:16:21,063 : 1196508602.py : eval_pred : INFO : processing 0 lines 
2025-02-28 20:16:21,443 : 1196508602.py : eval_pred : INFO : processing 100 lines 
2025-02-28 20:16:21,613 : 1196508602.py : eval_pred : INFO : processing 200 lines 
2025-02-28 20:16:21,901 : 1196508602.py : eval_pred : INFO : processing 300 lines 
2025-02-28 20:16:22,101 : 1196508602.py : eval_pred : INFO : processing 400 lines 
2025-02-28 20:16:23,094 : 1196508602.py : eval_pred : INFO : processing 0 lines 
2025-02-28 20:16:23,240 : 1196508602.py : eval_pred : INFO : processing 100 lines 
2025-02-28 20:16:23,385 : 1196508602.py : eval_pred : INFO : processing 200 lines 
2025-02-28 20:16:23,529 : 1196508602.py : eval_pred : INFO : processing 300 lines 
2025-02-28 20:16:23,723 : 1196508602.py : eval_pred : INFO : processing 0 lines 
2025-02-28 20:16:24,373 : 1196508602.py : eval_pred : INFO : processing 100 lines 
2025-02-28 20:16:24,657 : 1196508602.py : eval_pred : INFO : processing 200 lines 
2025-02-28 20:16:25,125 : 1196508602.py : eval_pred : INFO : processing 300 lines 
2025-02-28 20:16:25,472 : 1196508602.py : eval_pred : INFO : processing 400 lines 
2025-02-28 20:21:51,104 : 1196508602.py : eval_pred : INFO : processing 0 lines 
2025-02-28 20:21:51,201 : 1196508602.py : eval_pred : INFO : processing 100 lines 
2025-02-28 20:21:51,288 : 1196508602.py : eval_pred : INFO : processing 200 lines 
2025-02-28 20:21:51,375 : 1196508602.py : eval_pred : INFO : processing 300 lines 
2025-02-28 20:21:51,498 : 1196508602.py : eval_pred : INFO : processing 0 lines 
2025-02-28 20:21:51,879 : 1196508602.py : eval_pred : INFO : processing 100 lines 
2025-02-28 20:21:52,049 : 1196508602.py : eval_pred : INFO : processing 200 lines 
2025-02-28 20:21:52,324 : 1196508602.py : eval_pred : INFO : processing 300 lines 
2025-02-28 20:21:52,518 : 1196508602.py : eval_pred : INFO : processing 400 lines 
2025-03-01 13:35:53,794 : 916236790.py : __call__ : INFO : loaded dataset, sample = 1158
2025-03-01 13:35:53,796 : 1341934384.py : train_scp : INFO : 109483778
2025-03-01 13:35:54,669 : 1341934384.py : train_scp : INFO : sample = 16, loss = 0.0686359778046608
2025-03-01 13:35:54,840 : 1341934384.py : train_scp : INFO : sample = 32, loss = 0.08270967751741409
2025-03-01 13:35:55,168 : 1341934384.py : train_scp : INFO : sample = 48, loss = 0.06628353024522464
2025-03-01 13:35:55,559 : 1341934384.py : train_scp : INFO : sample = 64, loss = 0.056424719747155905
2025-03-01 13:35:55,854 : 1341934384.py : train_scp : INFO : sample = 80, loss = 0.0718107383698225
2025-03-01 13:35:56,044 : 1341934384.py : train_scp : INFO : sample = 96, loss = 0.06128863524645567
2025-03-01 13:35:56,188 : 1341934384.py : train_scp : INFO : sample = 112, loss = 0.05253311592553343
2025-03-01 13:35:56,316 : 1341934384.py : train_scp : INFO : sample = 128, loss = 0.049126381520181894
2025-03-01 13:35:56,488 : 1341934384.py : train_scp : INFO : sample = 144, loss = 0.04366789468460613
2025-03-01 13:35:56,665 : 1341934384.py : train_scp : INFO : sample = 160, loss = 0.04168568458408117
2025-03-01 13:35:56,780 : 1341934384.py : train_scp : INFO : sample = 176, loss = 0.048462169244885445
2025-03-01 13:35:56,937 : 1341934384.py : train_scp : INFO : sample = 192, loss = 0.04945263685658574
2025-03-01 13:35:57,101 : 1341934384.py : train_scp : INFO : sample = 208, loss = 0.049370890769820944
2025-03-01 13:35:57,295 : 1341934384.py : train_scp : INFO : sample = 224, loss = 0.05008231436035463
2025-03-01 13:35:57,518 : 1341934384.py : train_scp : INFO : sample = 240, loss = 0.04911137310167154
2025-03-01 13:35:57,756 : 1341934384.py : train_scp : INFO : sample = 256, loss = 0.046825920697301626
2025-03-01 13:35:58,256 : 1341934384.py : train_scp : INFO : sample = 272, loss = 0.045511108658769554
2025-03-01 13:35:58,749 : 1341934384.py : train_scp : INFO : sample = 288, loss = 0.04606165664477481
2025-03-01 13:35:58,958 : 1341934384.py : train_scp : INFO : sample = 304, loss = 0.04744279629697925
2025-03-01 13:35:59,449 : 1341934384.py : train_scp : INFO : sample = 320, loss = 0.04980232762172818
2025-03-01 13:35:59,570 : 1341934384.py : train_scp : INFO : sample = 336, loss = 0.047430788211169694
2025-03-01 13:35:59,801 : 1341934384.py : train_scp : INFO : sample = 352, loss = 0.047150335274636745
2025-03-01 13:35:59,979 : 1341934384.py : train_scp : INFO : sample = 368, loss = 0.04510032069747862
2025-03-01 13:36:00,119 : 1341934384.py : train_scp : INFO : sample = 384, loss = 0.04782855611604949
2025-03-01 13:36:00,307 : 1341934384.py : train_scp : INFO : sample = 400, loss = 0.04649105831980705
2025-03-01 13:36:00,447 : 1341934384.py : train_scp : INFO : sample = 416, loss = 0.0454721782499781
2025-03-01 13:36:00,627 : 1341934384.py : train_scp : INFO : sample = 432, loss = 0.044291892261416825
2025-03-01 13:36:00,856 : 1341934384.py : train_scp : INFO : sample = 448, loss = 0.0434713441479419
2025-03-01 13:36:01,032 : 1341934384.py : train_scp : INFO : sample = 464, loss = 0.04427947161783432
2025-03-01 13:36:01,188 : 1341934384.py : train_scp : INFO : sample = 480, loss = 0.044350206044813
2025-03-01 13:36:01,348 : 1341934384.py : train_scp : INFO : sample = 496, loss = 0.04544090137125984
2025-03-01 13:36:01,675 : 1341934384.py : train_scp : INFO : sample = 512, loss = 0.045437079679686576
2025-03-01 13:36:01,799 : 1341934384.py : train_scp : INFO : sample = 528, loss = 0.04406019847727183
2025-03-01 13:36:02,245 : 1341934384.py : train_scp : INFO : sample = 544, loss = 0.042764310286763835
2025-03-01 13:36:02,713 : 1341934384.py : train_scp : INFO : sample = 560, loss = 0.04243383545960699
2025-03-01 13:36:02,890 : 1341934384.py : train_scp : INFO : sample = 576, loss = 0.043868561275303364
2025-03-01 13:36:03,132 : 1341934384.py : train_scp : INFO : sample = 592, loss = 0.0448869922877969
2025-03-01 13:36:03,617 : 1341934384.py : train_scp : INFO : sample = 608, loss = 0.04565572473955782
2025-03-01 13:36:03,918 : 1341934384.py : train_scp : INFO : sample = 624, loss = 0.045258299996837593
2025-03-01 13:36:04,020 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.04552162452600896
2025-03-01 13:36:04,198 : 1341934384.py : train_scp : INFO : sample = 656, loss = 0.04555570238792315
2025-03-01 13:36:04,332 : 1341934384.py : train_scp : INFO : sample = 672, loss = 0.04622503418830179
2025-03-01 13:36:04,715 : 1341934384.py : train_scp : INFO : sample = 688, loss = 0.04515003339322501
2025-03-01 13:36:04,817 : 1341934384.py : train_scp : INFO : sample = 704, loss = 0.04412389627065171
2025-03-01 13:36:04,943 : 1341934384.py : train_scp : INFO : sample = 720, loss = 0.044396966819961865
2025-03-01 13:36:05,115 : 1341934384.py : train_scp : INFO : sample = 736, loss = 0.045778953632258854
2025-03-01 13:36:05,568 : 1341934384.py : train_scp : INFO : sample = 752, loss = 0.0457506860269511
2025-03-01 13:36:05,779 : 1341934384.py : train_scp : INFO : sample = 768, loss = 0.045937477184149124
2025-03-01 13:36:06,125 : 1341934384.py : train_scp : INFO : sample = 784, loss = 0.04569383866476769
2025-03-01 13:36:06,610 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04704653579741716
2025-03-01 13:36:06,774 : 1341934384.py : train_scp : INFO : sample = 816, loss = 0.046608627098155955
2025-03-01 13:36:07,102 : 1341934384.py : train_scp : INFO : sample = 832, loss = 0.04687235629759156
2025-03-01 13:36:07,326 : 1341934384.py : train_scp : INFO : sample = 848, loss = 0.04598797221650493
2025-03-01 13:36:07,819 : 1341934384.py : train_scp : INFO : sample = 864, loss = 0.04608349112310895
2025-03-01 13:36:07,954 : 1341934384.py : train_scp : INFO : sample = 880, loss = 0.04594535282389684
2025-03-01 13:36:08,168 : 1341934384.py : train_scp : INFO : sample = 896, loss = 0.04549601940172059
2025-03-01 13:36:08,296 : 1341934384.py : train_scp : INFO : sample = 912, loss = 0.04536627082709681
2025-03-01 13:36:08,778 : 1341934384.py : train_scp : INFO : sample = 928, loss = 0.044723510710072925
2025-03-01 13:36:09,189 : 1341934384.py : train_scp : INFO : sample = 944, loss = 0.04396548510481745
2025-03-01 13:36:09,332 : 1341934384.py : train_scp : INFO : sample = 960, loss = 0.04370537338157495
2025-03-01 13:36:09,611 : 1341934384.py : train_scp : INFO : sample = 976, loss = 0.04348850460936789
2025-03-01 13:36:09,762 : 1341934384.py : train_scp : INFO : sample = 992, loss = 0.0438218429504383
2025-03-01 13:36:10,151 : 1341934384.py : train_scp : INFO : sample = 1008, loss = 0.043405923519342665
2025-03-01 13:36:10,290 : 1341934384.py : train_scp : INFO : sample = 1024, loss = 0.04311076839803718
2025-03-01 13:36:10,622 : 1341934384.py : train_scp : INFO : sample = 1040, loss = 0.04263724352304752
2025-03-01 13:36:11,112 : 1341934384.py : train_scp : INFO : sample = 1056, loss = 0.04338405518369241
2025-03-01 13:36:11,260 : 1341934384.py : train_scp : INFO : sample = 1072, loss = 0.04367511274654474
2025-03-01 13:36:11,374 : 1341934384.py : train_scp : INFO : sample = 1088, loss = 0.04371374703067191
2025-03-01 13:36:11,804 : 1341934384.py : train_scp : INFO : sample = 1104, loss = 0.04374781056590702
2025-03-01 13:36:11,947 : 1341934384.py : train_scp : INFO : sample = 1120, loss = 0.04312284184353692
2025-03-01 13:36:12,442 : 1341934384.py : train_scp : INFO : sample = 1136, loss = 0.04326729035713303
2025-03-01 13:36:12,824 : 1341934384.py : train_scp : INFO : sample = 1152, loss = 0.04285225864603288
2025-03-01 13:36:12,945 : 1341934384.py : train_scp : INFO : sample = 1168, loss = 0.04276044067148477
2025-03-01 13:36:13,097 : 1341934384.py : train_scp : INFO : sample = 1184, loss = 0.042270922831989625
2025-03-01 13:36:13,255 : 1341934384.py : train_scp : INFO : sample = 1200, loss = 0.04292497163017591
2025-03-01 13:36:13,749 : 1341934384.py : train_scp : INFO : sample = 1216, loss = 0.04280284708855968
2025-03-01 13:36:13,864 : 1341934384.py : train_scp : INFO : sample = 1232, loss = 0.04282509119479687
2025-03-01 13:36:14,313 : 1341934384.py : train_scp : INFO : sample = 1248, loss = 0.04355401698595438
2025-03-01 13:36:14,652 : 1341934384.py : train_scp : INFO : sample = 1264, loss = 0.043656251051380664
2025-03-01 13:36:14,803 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.0441099617164582
2025-03-01 13:36:15,195 : 1341934384.py : train_scp : INFO : sample = 1296, loss = 0.044157725011492954
2025-03-01 13:36:15,588 : 1341934384.py : train_scp : INFO : sample = 1312, loss = 0.04406537565334541
2025-03-01 13:36:16,029 : 1341934384.py : train_scp : INFO : sample = 1328, loss = 0.04353446751294366
2025-03-01 13:36:16,501 : 1341934384.py : train_scp : INFO : sample = 1344, loss = 0.04450243378856352
2025-03-01 13:36:16,986 : 1341934384.py : train_scp : INFO : sample = 1360, loss = 0.04436733323861571
2025-03-01 13:36:17,103 : 1341934384.py : train_scp : INFO : sample = 1376, loss = 0.044332393466733226
2025-03-01 13:36:17,234 : 1341934384.py : train_scp : INFO : sample = 1392, loss = 0.044036009124126925
2025-03-01 13:36:17,409 : 1341934384.py : train_scp : INFO : sample = 1408, loss = 0.04381987880068747
2025-03-01 13:36:17,562 : 1341934384.py : train_scp : INFO : sample = 1424, loss = 0.043660977980896326
2025-03-01 13:36:17,786 : 1341934384.py : train_scp : INFO : sample = 1440, loss = 0.04317585600333081
2025-03-01 13:36:17,919 : 1341934384.py : train_scp : INFO : sample = 1456, loss = 0.04320063112446895
2025-03-01 13:36:18,098 : 1341934384.py : train_scp : INFO : sample = 1472, loss = 0.04299844380306161
2025-03-01 13:36:18,592 : 1341934384.py : train_scp : INFO : sample = 1488, loss = 0.04333313370263705
2025-03-01 13:36:18,709 : 1341934384.py : train_scp : INFO : sample = 1504, loss = 0.04383983304525944
2025-03-01 13:36:19,213 : 1341934384.py : train_scp : INFO : sample = 1520, loss = 0.04391031159382117
2025-03-01 13:36:19,717 : 1341934384.py : train_scp : INFO : sample = 1536, loss = 0.04400852171238512
2025-03-01 13:36:19,937 : 1341934384.py : train_scp : INFO : sample = 1552, loss = 0.04383655958184876
2025-03-01 13:36:20,169 : 1341934384.py : train_scp : INFO : sample = 1568, loss = 0.043496707131211856
2025-03-01 13:36:20,306 : 1341934384.py : train_scp : INFO : sample = 1584, loss = 0.043310762919259796
2025-03-01 13:36:20,591 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04360099207609892
2025-03-01 13:36:20,763 : 1341934384.py : train_scp : INFO : sample = 1616, loss = 0.04316929908524646
2025-03-01 13:36:21,257 : 1341934384.py : train_scp : INFO : sample = 1632, loss = 0.04299594235478663
2025-03-01 13:36:21,759 : 1341934384.py : train_scp : INFO : sample = 1648, loss = 0.04336862077990782
2025-03-01 13:36:22,095 : 1341934384.py : train_scp : INFO : sample = 1664, loss = 0.04331514339607496
2025-03-01 13:36:22,367 : 1341934384.py : train_scp : INFO : sample = 1680, loss = 0.04346612138407571
2025-03-01 13:36:22,586 : 1341934384.py : train_scp : INFO : sample = 1696, loss = 0.04305606363516933
2025-03-01 13:36:23,054 : 1341934384.py : train_scp : INFO : sample = 1712, loss = 0.04298526713642004
2025-03-01 13:36:23,234 : 1341934384.py : train_scp : INFO : sample = 1728, loss = 0.04308397519505686
2025-03-01 13:36:23,457 : 1341934384.py : train_scp : INFO : sample = 1744, loss = 0.04332865012484953
2025-03-01 13:36:23,954 : 1341934384.py : train_scp : INFO : sample = 1760, loss = 0.04336762289431962
2025-03-01 13:36:24,136 : 1341934384.py : train_scp : INFO : sample = 1776, loss = 0.04331330394556931
2025-03-01 13:36:24,399 : 1341934384.py : train_scp : INFO : sample = 1792, loss = 0.043211665569937656
2025-03-01 13:36:24,616 : 1341934384.py : train_scp : INFO : sample = 1808, loss = 0.0434868814077525
2025-03-01 13:36:24,769 : 1341934384.py : train_scp : INFO : sample = 1824, loss = 0.04353098460195357
2025-03-01 13:36:24,905 : 1341934384.py : train_scp : INFO : sample = 1840, loss = 0.04367015685724176
2025-03-01 13:36:25,121 : 1341934384.py : train_scp : INFO : sample = 1856, loss = 0.04385369900485565
2025-03-01 13:36:25,554 : 1341934384.py : train_scp : INFO : sample = 1872, loss = 0.044032486903871224
2025-03-01 13:36:25,865 : 1341934384.py : train_scp : INFO : sample = 1888, loss = 0.04365933023519435
2025-03-01 13:36:26,000 : 1341934384.py : train_scp : INFO : sample = 1904, loss = 0.04387776296930153
2025-03-01 13:36:26,153 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.04351211494455735
2025-03-01 13:36:26,293 : 1341934384.py : train_scp : INFO : sample = 1936, loss = 0.04334662092680281
2025-03-01 13:36:26,439 : 1341934384.py : train_scp : INFO : sample = 1952, loss = 0.04307089614697167
2025-03-01 13:36:26,664 : 1341934384.py : train_scp : INFO : sample = 1968, loss = 0.044081129345709714
2025-03-01 13:36:26,969 : 1341934384.py : train_scp : INFO : sample = 1984, loss = 0.04389205200958156
2025-03-01 13:36:27,146 : 1341934384.py : train_scp : INFO : sample = 2000, loss = 0.043540915593504904
2025-03-01 13:36:27,327 : 1341934384.py : train_scp : INFO : sample = 2016, loss = 0.04349577454997906
2025-03-01 13:36:27,551 : 1341934384.py : train_scp : INFO : sample = 2032, loss = 0.04397605833282152
2025-03-01 13:36:27,730 : 1341934384.py : train_scp : INFO : sample = 2048, loss = 0.04435714035935234
2025-03-01 13:36:27,892 : 1341934384.py : train_scp : INFO : sample = 2064, loss = 0.04417560018541277
2025-03-01 13:36:28,229 : 1341934384.py : train_scp : INFO : sample = 2080, loss = 0.044057937209995894
2025-03-01 13:36:28,719 : 1341934384.py : train_scp : INFO : sample = 2096, loss = 0.04372161707862188
2025-03-01 13:36:28,953 : 1341934384.py : train_scp : INFO : sample = 2112, loss = 0.043828142369448236
2025-03-01 13:36:29,301 : 1341934384.py : train_scp : INFO : sample = 2128, loss = 0.04368622502998302
2025-03-01 13:36:29,477 : 1341934384.py : train_scp : INFO : sample = 2144, loss = 0.043360208425281654
2025-03-01 13:36:29,609 : 1341934384.py : train_scp : INFO : sample = 2160, loss = 0.04367808845859987
2025-03-01 13:36:30,003 : 1341934384.py : train_scp : INFO : sample = 2176, loss = 0.04367289664771627
2025-03-01 13:36:30,166 : 1341934384.py : train_scp : INFO : sample = 2192, loss = 0.043826887533612496
2025-03-01 13:36:30,343 : 1341934384.py : train_scp : INFO : sample = 2208, loss = 0.0436175642805039
2025-03-01 13:36:30,483 : 1341934384.py : train_scp : INFO : sample = 2224, loss = 0.04406668826026453
2025-03-01 13:36:30,959 : 1341934384.py : train_scp : INFO : sample = 2240, loss = 0.04401669019301023
2025-03-01 13:36:31,124 : 1341934384.py : train_scp : INFO : sample = 2256, loss = 0.04404713468401567
2025-03-01 13:36:31,294 : 1341934384.py : train_scp : INFO : sample = 2272, loss = 0.04398588412506899
2025-03-01 13:36:31,395 : 1341934384.py : train_scp : INFO : sample = 2288, loss = 0.044071057160104905
2025-03-01 13:36:31,519 : 1341934384.py : train_scp : INFO : sample = 2304, loss = 0.04430125629167176
2025-03-01 13:36:31,649 : 1341934384.py : train_scp : INFO : sample = 2320, loss = 0.04400811228772689
2025-03-01 13:36:31,890 : 1341934384.py : train_scp : INFO : sample = 2336, loss = 0.04467412791125579
2025-03-01 13:36:32,009 : 1341934384.py : train_scp : INFO : sample = 2352, loss = 0.044633686745247876
2025-03-01 13:36:32,155 : 1341934384.py : train_scp : INFO : sample = 2368, loss = 0.04461049706347891
2025-03-01 13:36:32,545 : 1341934384.py : train_scp : INFO : sample = 2384, loss = 0.04434464978141673
2025-03-01 13:36:33,023 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.044253264889121055
2025-03-01 13:36:33,508 : 1341934384.py : train_scp : INFO : sample = 2416, loss = 0.0439601969097229
2025-03-01 13:36:33,700 : 1341934384.py : train_scp : INFO : sample = 2432, loss = 0.0439200393218351
2025-03-01 13:36:34,181 : 1341934384.py : train_scp : INFO : sample = 2448, loss = 0.043725381234015516
2025-03-01 13:36:34,374 : 1341934384.py : train_scp : INFO : sample = 2464, loss = 0.04407655741632372
2025-03-01 13:36:34,515 : 1341934384.py : train_scp : INFO : sample = 2480, loss = 0.04395050031523551
2025-03-01 13:36:34,778 : 1341934384.py : train_scp : INFO : sample = 2496, loss = 0.04413866843932714
2025-03-01 13:36:35,072 : 1341934384.py : train_scp : INFO : sample = 2512, loss = 0.04430002823566935
2025-03-01 13:36:35,363 : 1341934384.py : train_scp : INFO : sample = 2528, loss = 0.04401964831012714
2025-03-01 13:36:35,543 : 1341934384.py : train_scp : INFO : sample = 2544, loss = 0.04435522231103489
2025-03-01 13:36:36,023 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.044544246466830376
2025-03-01 13:36:36,464 : 1341934384.py : train_scp : INFO : sample = 2576, loss = 0.044795249791248985
2025-03-01 13:36:36,748 : 1341934384.py : train_scp : INFO : sample = 2592, loss = 0.04492202070024279
2025-03-01 13:36:37,004 : 1341934384.py : train_scp : INFO : sample = 2608, loss = 0.04504473685669753
2025-03-01 13:36:37,299 : 1341934384.py : train_scp : INFO : sample = 2624, loss = 0.045074136291698715
2025-03-01 13:36:37,471 : 1341934384.py : train_scp : INFO : sample = 2640, loss = 0.04480095970811266
2025-03-01 13:36:37,691 : 1341934384.py : train_scp : INFO : sample = 2656, loss = 0.04491470170667373
2025-03-01 13:36:37,882 : 1341934384.py : train_scp : INFO : sample = 2672, loss = 0.04464575139705292
2025-03-01 13:36:38,096 : 1341934384.py : train_scp : INFO : sample = 2688, loss = 0.044690304302743504
2025-03-01 13:36:38,594 : 1341934384.py : train_scp : INFO : sample = 2704, loss = 0.04462566469195326
2025-03-01 13:36:38,763 : 1341934384.py : train_scp : INFO : sample = 2720, loss = 0.044363160782000595
2025-03-01 13:36:39,002 : 1341934384.py : train_scp : INFO : sample = 2736, loss = 0.04442377580188171
2025-03-01 13:36:39,147 : 1341934384.py : train_scp : INFO : sample = 2752, loss = 0.04416690066153573
2025-03-01 13:36:39,445 : 1341934384.py : train_scp : INFO : sample = 2768, loss = 0.04421999754459527
2025-03-01 13:36:39,807 : 1341934384.py : train_scp : INFO : sample = 2784, loss = 0.0447732578085243
2025-03-01 13:36:39,964 : 1341934384.py : train_scp : INFO : sample = 2800, loss = 0.04451741062104702
2025-03-01 13:36:40,120 : 1341934384.py : train_scp : INFO : sample = 2816, loss = 0.044443108956329525
2025-03-01 13:36:40,455 : 1341934384.py : train_scp : INFO : sample = 2832, loss = 0.044192017945276815
2025-03-01 13:36:40,749 : 1341934384.py : train_scp : INFO : sample = 2848, loss = 0.0442182571219092
2025-03-01 13:36:40,864 : 1341934384.py : train_scp : INFO : sample = 2864, loss = 0.04397122775251306
2025-03-01 13:36:41,036 : 1341934384.py : train_scp : INFO : sample = 2880, loss = 0.044033995074116525
2025-03-01 13:36:41,214 : 1341934384.py : train_scp : INFO : sample = 2896, loss = 0.044005029490063206
2025-03-01 13:36:41,611 : 1341934384.py : train_scp : INFO : sample = 2912, loss = 0.04393156513489865
2025-03-01 13:36:41,973 : 1341934384.py : train_scp : INFO : sample = 2928, loss = 0.04369150193744018
2025-03-01 13:36:42,088 : 1341934384.py : train_scp : INFO : sample = 2944, loss = 0.043993930555070226
2025-03-01 13:36:42,582 : 1341934384.py : train_scp : INFO : sample = 2960, loss = 0.044083189742790685
2025-03-01 13:36:42,763 : 1341934384.py : train_scp : INFO : sample = 2976, loss = 0.04407296305702579
2025-03-01 13:36:43,101 : 1341934384.py : train_scp : INFO : sample = 2992, loss = 0.04385709772533911
2025-03-01 13:36:43,247 : 1341934384.py : train_scp : INFO : sample = 3008, loss = 0.04378916988981531
2025-03-01 13:36:43,583 : 1341934384.py : train_scp : INFO : sample = 3024, loss = 0.04355748116023957
2025-03-01 13:36:43,809 : 1341934384.py : train_scp : INFO : sample = 3040, loss = 0.04372072074758379
2025-03-01 13:36:43,939 : 1341934384.py : train_scp : INFO : sample = 3056, loss = 0.04413733494843488
2025-03-01 13:36:44,147 : 1341934384.py : train_scp : INFO : sample = 3072, loss = 0.04405996639009876
2025-03-01 13:36:44,345 : 1341934384.py : train_scp : INFO : sample = 3088, loss = 0.04413100367730455
2025-03-01 13:36:44,689 : 1341934384.py : train_scp : INFO : sample = 3104, loss = 0.043985238039539645
2025-03-01 13:36:45,190 : 1341934384.py : train_scp : INFO : sample = 3120, loss = 0.043759672716259954
2025-03-01 13:36:45,426 : 1341934384.py : train_scp : INFO : sample = 3136, loss = 0.04367620585372253
2025-03-01 13:36:45,616 : 1341934384.py : train_scp : INFO : sample = 3152, loss = 0.04354991178562496
2025-03-01 13:36:45,715 : 1341934384.py : train_scp : INFO : sample = 3168, loss = 0.04332996273620261
2025-03-01 13:36:45,988 : 1341934384.py : train_scp : INFO : sample = 3184, loss = 0.043521691478751415
2025-03-01 13:36:46,130 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.043556004194542766
2025-03-01 13:36:46,407 : 1341934384.py : train_scp : INFO : sample = 3216, loss = 0.043441005681284624
2025-03-01 13:36:46,914 : 1341934384.py : train_scp : INFO : sample = 3232, loss = 0.043225951197713906
2025-03-01 13:36:47,151 : 1341934384.py : train_scp : INFO : sample = 3248, loss = 0.04347572772960945
2025-03-01 13:36:47,379 : 1341934384.py : train_scp : INFO : sample = 3264, loss = 0.04353190428924327
2025-03-01 13:36:47,529 : 1341934384.py : train_scp : INFO : sample = 3280, loss = 0.04331955353661281
2025-03-01 13:36:47,636 : 1341934384.py : train_scp : INFO : sample = 3296, loss = 0.04316724700292627
2025-03-01 13:36:47,864 : 1341934384.py : train_scp : INFO : sample = 3312, loss = 0.04349803291981059
2025-03-01 13:36:48,248 : 1341934384.py : train_scp : INFO : sample = 3328, loss = 0.043529709111540936
2025-03-01 13:36:48,344 : 1341934384.py : train_scp : INFO : sample = 3344, loss = 0.043478158093168975
2025-03-01 13:36:48,493 : 1341934384.py : train_scp : INFO : sample = 3360, loss = 0.04351545629934186
2025-03-01 13:36:48,774 : 1341934384.py : train_scp : INFO : sample = 3376, loss = 0.04354527662397843
2025-03-01 13:36:48,884 : 1341934384.py : train_scp : INFO : sample = 3392, loss = 0.0435067765010556
2025-03-01 13:36:48,995 : 1341934384.py : train_scp : INFO : sample = 3408, loss = 0.0435588487419584
2025-03-01 13:36:49,486 : 1341934384.py : train_scp : INFO : sample = 3424, loss = 0.04370233847402802
2025-03-01 13:36:49,756 : 1341934384.py : train_scp : INFO : sample = 3440, loss = 0.04384696077295514
2025-03-01 13:36:49,906 : 1341934384.py : train_scp : INFO : sample = 3456, loss = 0.044131982752501416
2025-03-01 13:36:50,012 : 1341934384.py : train_scp : INFO : sample = 3472, loss = 0.044204060923874654
2025-03-01 13:36:50,447 : 1341934384.py : train_scp : INFO : sample = 3488, loss = 0.04413921997771351
2025-03-01 13:36:50,612 : 1341934384.py : train_scp : INFO : sample = 3504, loss = 0.04427181806874602
2025-03-01 13:36:51,112 : 1341934384.py : train_scp : INFO : sample = 3520, loss = 0.044370048120617865
2025-03-01 13:36:51,238 : 1341934384.py : train_scp : INFO : sample = 3536, loss = 0.044169278672108285
2025-03-01 13:36:51,345 : 1341934384.py : train_scp : INFO : sample = 3552, loss = 0.043970317957369054
2025-03-01 13:36:51,649 : 1341934384.py : train_scp : INFO : sample = 3568, loss = 0.04426248338190429
2025-03-01 13:36:51,775 : 1341934384.py : train_scp : INFO : sample = 3584, loss = 0.04406488300966365
2025-03-01 13:36:52,125 : 1341934384.py : train_scp : INFO : sample = 3600, loss = 0.0441851935784022
2025-03-01 13:36:52,397 : 1341934384.py : train_scp : INFO : sample = 3616, loss = 0.044202673191254115
2025-03-01 13:36:52,522 : 1341934384.py : train_scp : INFO : sample = 3632, loss = 0.04450768957710476
2025-03-01 13:36:52,719 : 1341934384.py : train_scp : INFO : sample = 3648, loss = 0.044382496508197825
2025-03-01 13:36:52,910 : 1341934384.py : train_scp : INFO : sample = 3664, loss = 0.04418868647977775
2025-03-01 13:36:53,047 : 1341934384.py : train_scp : INFO : sample = 3680, loss = 0.044523908538014995
2025-03-01 13:36:53,271 : 1341934384.py : train_scp : INFO : sample = 3696, loss = 0.04435194837679336
2025-03-01 13:36:53,744 : 1341934384.py : train_scp : INFO : sample = 3712, loss = 0.04449361529811446
2025-03-01 13:36:54,123 : 1341934384.py : train_scp : INFO : sample = 3728, loss = 0.04448743350803852
2025-03-01 13:36:54,244 : 1341934384.py : train_scp : INFO : sample = 3744, loss = 0.04429731627082468
2025-03-01 13:36:54,538 : 1341934384.py : train_scp : INFO : sample = 3760, loss = 0.044298730116892365
2025-03-01 13:36:54,826 : 1341934384.py : train_scp : INFO : sample = 3776, loss = 0.04423212396549219
2025-03-01 13:36:54,976 : 1341934384.py : train_scp : INFO : sample = 3792, loss = 0.04433928698081256
2025-03-01 13:36:55,092 : 1341934384.py : train_scp : INFO : sample = 3808, loss = 0.04453665391393319
2025-03-01 13:36:55,586 : 1341934384.py : train_scp : INFO : sample = 3824, loss = 0.04445345302318928
2025-03-01 13:36:55,798 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.04458488283368448
2025-03-01 13:36:55,980 : 1341934384.py : train_scp : INFO : sample = 3856, loss = 0.04444973945957744
2025-03-01 13:36:56,092 : 1341934384.py : train_scp : INFO : sample = 3872, loss = 0.044283575663142956
2025-03-01 13:36:56,400 : 1341934384.py : train_scp : INFO : sample = 3888, loss = 0.0442114743814179
2025-03-01 13:36:56,538 : 1341934384.py : train_scp : INFO : sample = 3904, loss = 0.04429044926416923
2025-03-01 13:36:56,938 : 1341934384.py : train_scp : INFO : sample = 3920, loss = 0.044424455787758436
2025-03-01 13:36:57,051 : 1341934384.py : train_scp : INFO : sample = 3936, loss = 0.044632844028737004
2025-03-01 13:36:57,202 : 1341934384.py : train_scp : INFO : sample = 3952, loss = 0.04452323795179365
2025-03-01 13:36:57,380 : 1341934384.py : train_scp : INFO : sample = 3968, loss = 0.04442840995597503
2025-03-01 13:36:57,499 : 1341934384.py : train_scp : INFO : sample = 3984, loss = 0.04443362281175263
2025-03-01 13:36:57,801 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04476712576299906
2025-03-01 13:36:57,983 : 1341934384.py : train_scp : INFO : sample = 4016, loss = 0.04488289155125381
2025-03-01 13:36:58,478 : 1341934384.py : train_scp : INFO : sample = 4032, loss = 0.044820899757305306
2025-03-01 13:36:58,633 : 1341934384.py : train_scp : INFO : sample = 4048, loss = 0.04473360398187939
2025-03-01 13:36:59,067 : 1341934384.py : train_scp : INFO : sample = 4064, loss = 0.044591513777694367
2025-03-01 13:36:59,361 : 1341934384.py : train_scp : INFO : sample = 4080, loss = 0.0447738659002033
2025-03-01 13:36:59,859 : 1341934384.py : train_scp : INFO : sample = 4096, loss = 0.04487706326472107
2025-03-01 13:36:59,997 : 1341934384.py : train_scp : INFO : sample = 4112, loss = 0.04492689298052732
2025-03-01 13:37:00,498 : 1341934384.py : train_scp : INFO : sample = 4128, loss = 0.04508012279059536
2025-03-01 13:37:00,964 : 1341934384.py : train_scp : INFO : sample = 4144, loss = 0.04490606826244634
2025-03-01 13:37:01,263 : 1341934384.py : train_scp : INFO : sample = 4160, loss = 0.04496215358376503
2025-03-01 13:37:01,445 : 1341934384.py : train_scp : INFO : sample = 4176, loss = 0.044855831319402004
2025-03-01 13:37:01,896 : 1341934384.py : train_scp : INFO : sample = 4192, loss = 0.04483732553412213
2025-03-01 13:37:02,236 : 1341934384.py : train_scp : INFO : sample = 4208, loss = 0.044849720567521484
2025-03-01 13:37:02,424 : 1341934384.py : train_scp : INFO : sample = 4224, loss = 0.04493147419116488
2025-03-01 13:37:02,914 : 1341934384.py : train_scp : INFO : sample = 4240, loss = 0.044860812178197894
2025-03-01 13:37:03,047 : 1341934384.py : train_scp : INFO : sample = 4256, loss = 0.04475038275262691
2025-03-01 13:37:03,180 : 1341934384.py : train_scp : INFO : sample = 4272, loss = 0.044848936725031124
2025-03-01 13:37:03,277 : 1341934384.py : train_scp : INFO : sample = 4288, loss = 0.04468751897507194
2025-03-01 13:37:03,417 : 1341934384.py : train_scp : INFO : sample = 4304, loss = 0.04456366406802129
2025-03-01 13:37:03,636 : 1341934384.py : train_scp : INFO : sample = 4320, loss = 0.04459417485114601
2025-03-01 13:37:03,935 : 1341934384.py : train_scp : INFO : sample = 4336, loss = 0.04451469718942563
2025-03-01 13:37:04,216 : 1341934384.py : train_scp : INFO : sample = 4352, loss = 0.044499311507131684
2025-03-01 13:37:04,715 : 1341934384.py : train_scp : INFO : sample = 4368, loss = 0.04466324222475399
2025-03-01 13:37:05,231 : 1341934384.py : train_scp : INFO : sample = 4384, loss = 0.04475746240575601
2025-03-01 13:37:05,386 : 1341934384.py : train_scp : INFO : sample = 4400, loss = 0.044720938185399225
2025-03-01 13:37:05,681 : 1341934384.py : train_scp : INFO : sample = 4416, loss = 0.045081691985167024
2025-03-01 13:37:06,172 : 1341934384.py : train_scp : INFO : sample = 4432, loss = 0.04502129489522333
2025-03-01 13:37:06,657 : 1341934384.py : train_scp : INFO : sample = 4448, loss = 0.04485934779128368
2025-03-01 13:37:06,869 : 1341934384.py : train_scp : INFO : sample = 4464, loss = 0.04481704483029022
2025-03-01 13:37:07,016 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.04494484671657639
2025-03-01 13:37:07,520 : 1341934384.py : train_scp : INFO : sample = 4496, loss = 0.04517583268631608
2025-03-01 13:37:07,699 : 1341934384.py : train_scp : INFO : sample = 4512, loss = 0.04501563469806673
2025-03-01 13:37:08,055 : 1341934384.py : train_scp : INFO : sample = 4528, loss = 0.045108117722633026
2025-03-01 13:37:08,541 : 1341934384.py : train_scp : INFO : sample = 4544, loss = 0.04509724412297069
2025-03-01 13:37:08,763 : 1341934384.py : train_scp : INFO : sample = 4560, loss = 0.045214104475943664
2025-03-01 13:37:08,923 : 1341934384.py : train_scp : INFO : sample = 4576, loss = 0.04517752086496228
2025-03-01 13:37:09,416 : 1341934384.py : train_scp : INFO : sample = 4592, loss = 0.04504308957308011
2025-03-01 13:37:09,909 : 1341934384.py : train_scp : INFO : sample = 4608, loss = 0.045257840499592326
2025-03-01 13:37:10,188 : 1341934384.py : train_scp : INFO : sample = 4624, loss = 0.045337998936642414
2025-03-01 13:37:10,316 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models/_bert-base-uncased_101_epoch.pt
2025-03-01 13:37:11,378 : 1341934384.py : train_scp : INFO : sample = 16, loss = 0.06434599310159683
2025-03-01 13:37:11,506 : 1341934384.py : train_scp : INFO : sample = 32, loss = 0.05766240321099758
2025-03-01 13:37:11,670 : 1341934384.py : train_scp : INFO : sample = 48, loss = 0.0632905401289463
2025-03-01 13:37:11,906 : 1341934384.py : train_scp : INFO : sample = 64, loss = 0.06804010551422834
2025-03-01 13:37:12,101 : 1341934384.py : train_scp : INFO : sample = 80, loss = 0.06637860611081123
2025-03-01 13:37:12,440 : 1341934384.py : train_scp : INFO : sample = 96, loss = 0.06150638498365879
2025-03-01 13:37:12,938 : 1341934384.py : train_scp : INFO : sample = 112, loss = 0.061030427792242596
2025-03-01 13:37:13,399 : 1341934384.py : train_scp : INFO : sample = 128, loss = 0.058290150947868824
2025-03-01 13:37:13,894 : 1341934384.py : train_scp : INFO : sample = 144, loss = 0.05944175604316923
2025-03-01 13:37:15,093 : 1341934384.py : train_scp : INFO : sample = 160, loss = 0.060353200882673264
2025-03-01 13:37:15,281 : 1341934384.py : train_scp : INFO : sample = 176, loss = 0.05486654625697569
2025-03-01 13:37:15,584 : 1341934384.py : train_scp : INFO : sample = 192, loss = 0.05709367680052916
2025-03-01 13:37:15,775 : 1341934384.py : train_scp : INFO : sample = 208, loss = 0.05519809058079353
2025-03-01 13:37:16,069 : 1341934384.py : train_scp : INFO : sample = 224, loss = 0.05524107015558651
2025-03-01 13:37:16,246 : 1341934384.py : train_scp : INFO : sample = 240, loss = 0.05439468026161194
2025-03-01 13:37:16,385 : 1341934384.py : train_scp : INFO : sample = 256, loss = 0.052101779845543206
2025-03-01 13:37:16,678 : 1341934384.py : train_scp : INFO : sample = 272, loss = 0.050330192508066404
2025-03-01 13:37:16,823 : 1341934384.py : train_scp : INFO : sample = 288, loss = 0.05066878741814031
2025-03-01 13:37:17,122 : 1341934384.py : train_scp : INFO : sample = 304, loss = 0.04997189186121288
2025-03-01 13:37:17,346 : 1341934384.py : train_scp : INFO : sample = 320, loss = 0.04816017569974065
2025-03-01 13:37:17,538 : 1341934384.py : train_scp : INFO : sample = 336, loss = 0.04925757548993542
2025-03-01 13:37:17,690 : 1341934384.py : train_scp : INFO : sample = 352, loss = 0.049219401299276135
2025-03-01 13:37:18,182 : 1341934384.py : train_scp : INFO : sample = 368, loss = 0.04826465799756672
2025-03-01 13:37:18,366 : 1341934384.py : train_scp : INFO : sample = 384, loss = 0.04859871401761969
2025-03-01 13:37:18,513 : 1341934384.py : train_scp : INFO : sample = 400, loss = 0.048220428973436355
2025-03-01 13:37:19,008 : 1341934384.py : train_scp : INFO : sample = 416, loss = 0.04836731404066086
2025-03-01 13:37:19,121 : 1341934384.py : train_scp : INFO : sample = 432, loss = 0.0465759320391549
2025-03-01 13:37:19,410 : 1341934384.py : train_scp : INFO : sample = 448, loss = 0.04671733892921891
2025-03-01 13:37:19,607 : 1341934384.py : train_scp : INFO : sample = 464, loss = 0.047302472822625063
2025-03-01 13:37:19,761 : 1341934384.py : train_scp : INFO : sample = 480, loss = 0.049350390955805776
2025-03-01 13:37:19,906 : 1341934384.py : train_scp : INFO : sample = 496, loss = 0.04853119406728975
2025-03-01 13:37:20,101 : 1341934384.py : train_scp : INFO : sample = 512, loss = 0.04701459425268695
2025-03-01 13:37:20,604 : 1341934384.py : train_scp : INFO : sample = 528, loss = 0.04687989102394292
2025-03-01 13:37:20,720 : 1341934384.py : train_scp : INFO : sample = 544, loss = 0.0455010706997093
2025-03-01 13:37:20,915 : 1341934384.py : train_scp : INFO : sample = 560, loss = 0.04429110512137413
2025-03-01 13:37:21,114 : 1341934384.py : train_scp : INFO : sample = 576, loss = 0.04476845874968502
2025-03-01 13:37:21,296 : 1341934384.py : train_scp : INFO : sample = 592, loss = 0.045813708994034176
2025-03-01 13:37:21,765 : 1341934384.py : train_scp : INFO : sample = 608, loss = 0.045634828802002106
2025-03-01 13:37:22,268 : 1341934384.py : train_scp : INFO : sample = 624, loss = 0.047345833805127024
2025-03-01 13:37:22,530 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.046162187959998845
2025-03-01 13:37:22,725 : 1341934384.py : train_scp : INFO : sample = 656, loss = 0.04621905942515629
2025-03-01 13:37:22,880 : 1341934384.py : train_scp : INFO : sample = 672, loss = 0.045849074476531575
2025-03-01 13:37:23,040 : 1341934384.py : train_scp : INFO : sample = 688, loss = 0.044782816930565725
2025-03-01 13:37:23,210 : 1341934384.py : train_scp : INFO : sample = 704, loss = 0.04454634194685654
2025-03-01 13:37:23,527 : 1341934384.py : train_scp : INFO : sample = 720, loss = 0.04390584093828996
2025-03-01 13:37:24,011 : 1341934384.py : train_scp : INFO : sample = 736, loss = 0.04334629444486421
2025-03-01 13:37:24,510 : 1341934384.py : train_scp : INFO : sample = 752, loss = 0.04321722333577085
2025-03-01 13:37:24,672 : 1341934384.py : train_scp : INFO : sample = 768, loss = 0.044055229790198304
2025-03-01 13:37:24,900 : 1341934384.py : train_scp : INFO : sample = 784, loss = 0.043662311167133094
2025-03-01 13:37:25,240 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04326495010405779
2025-03-01 13:37:25,457 : 1341934384.py : train_scp : INFO : sample = 816, loss = 0.04290572634222461
2025-03-01 13:37:25,687 : 1341934384.py : train_scp : INFO : sample = 832, loss = 0.043759690812573984
2025-03-01 13:37:26,046 : 1341934384.py : train_scp : INFO : sample = 848, loss = 0.043367777195460394
2025-03-01 13:37:26,235 : 1341934384.py : train_scp : INFO : sample = 864, loss = 0.04256467021035927
2025-03-01 13:37:26,422 : 1341934384.py : train_scp : INFO : sample = 880, loss = 0.04351941668851809
2025-03-01 13:37:26,572 : 1341934384.py : train_scp : INFO : sample = 896, loss = 0.04280770383775234
2025-03-01 13:37:26,847 : 1341934384.py : train_scp : INFO : sample = 912, loss = 0.042282699525617716
2025-03-01 13:37:27,036 : 1341934384.py : train_scp : INFO : sample = 928, loss = 0.04155368746483121
2025-03-01 13:37:27,154 : 1341934384.py : train_scp : INFO : sample = 944, loss = 0.04370234758293225
2025-03-01 13:37:27,658 : 1341934384.py : train_scp : INFO : sample = 960, loss = 0.04384774674351017
2025-03-01 13:37:28,049 : 1341934384.py : train_scp : INFO : sample = 976, loss = 0.04360922242774338
2025-03-01 13:37:28,328 : 1341934384.py : train_scp : INFO : sample = 992, loss = 0.043868536189679175
2025-03-01 13:37:28,512 : 1341934384.py : train_scp : INFO : sample = 1008, loss = 0.043968194179118625
2025-03-01 13:37:29,009 : 1341934384.py : train_scp : INFO : sample = 1024, loss = 0.04495039291214198
2025-03-01 13:37:29,151 : 1341934384.py : train_scp : INFO : sample = 1040, loss = 0.045453803126628584
2025-03-01 13:37:29,269 : 1341934384.py : train_scp : INFO : sample = 1056, loss = 0.04575586375413519
2025-03-01 13:37:29,635 : 1341934384.py : train_scp : INFO : sample = 1072, loss = 0.04550790111186789
2025-03-01 13:37:30,129 : 1341934384.py : train_scp : INFO : sample = 1088, loss = 0.04517826585866073
2025-03-01 13:37:30,353 : 1341934384.py : train_scp : INFO : sample = 1104, loss = 0.04452350838244825
2025-03-01 13:37:30,495 : 1341934384.py : train_scp : INFO : sample = 1120, loss = 0.045255101099610326
2025-03-01 13:37:30,680 : 1341934384.py : train_scp : INFO : sample = 1136, loss = 0.044929945211805086
2025-03-01 13:37:31,159 : 1341934384.py : train_scp : INFO : sample = 1152, loss = 0.044347794416050114
2025-03-01 13:37:31,269 : 1341934384.py : train_scp : INFO : sample = 1168, loss = 0.04515849451904427
2025-03-01 13:37:31,566 : 1341934384.py : train_scp : INFO : sample = 1184, loss = 0.04550191166030394
2025-03-01 13:37:31,679 : 1341934384.py : train_scp : INFO : sample = 1200, loss = 0.044895219504833224
2025-03-01 13:37:32,125 : 1341934384.py : train_scp : INFO : sample = 1216, loss = 0.04591988066309377
2025-03-01 13:37:32,610 : 1341934384.py : train_scp : INFO : sample = 1232, loss = 0.045644842983259784
2025-03-01 13:37:32,855 : 1341934384.py : train_scp : INFO : sample = 1248, loss = 0.04598945572685737
2025-03-01 13:37:33,050 : 1341934384.py : train_scp : INFO : sample = 1264, loss = 0.04540731071765664
2025-03-01 13:37:33,163 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.045190263586118815
2025-03-01 13:37:33,404 : 1341934384.py : train_scp : INFO : sample = 1296, loss = 0.04492919549437953
2025-03-01 13:37:33,511 : 1341934384.py : train_scp : INFO : sample = 1312, loss = 0.04486622232035166
2025-03-01 13:37:33,627 : 1341934384.py : train_scp : INFO : sample = 1328, loss = 0.044605542647551344
2025-03-01 13:37:33,869 : 1341934384.py : train_scp : INFO : sample = 1344, loss = 0.04407452428269954
2025-03-01 13:37:33,985 : 1341934384.py : train_scp : INFO : sample = 1360, loss = 0.0441432266989175
2025-03-01 13:37:34,484 : 1341934384.py : train_scp : INFO : sample = 1376, loss = 0.04452978312795938
2025-03-01 13:37:34,600 : 1341934384.py : train_scp : INFO : sample = 1392, loss = 0.04401794654028169
2025-03-01 13:37:34,779 : 1341934384.py : train_scp : INFO : sample = 1408, loss = 0.044319464190101084
2025-03-01 13:37:34,954 : 1341934384.py : train_scp : INFO : sample = 1424, loss = 0.043821492682347134
2025-03-01 13:37:35,444 : 1341934384.py : train_scp : INFO : sample = 1440, loss = 0.043593671069377
2025-03-01 13:37:35,943 : 1341934384.py : train_scp : INFO : sample = 1456, loss = 0.043560858304192734
2025-03-01 13:37:36,127 : 1341934384.py : train_scp : INFO : sample = 1472, loss = 0.04362243735838844
2025-03-01 13:37:36,431 : 1341934384.py : train_scp : INFO : sample = 1488, loss = 0.043153378892169206
2025-03-01 13:37:36,740 : 1341934384.py : train_scp : INFO : sample = 1504, loss = 0.042774941217392046
2025-03-01 13:37:37,072 : 1341934384.py : train_scp : INFO : sample = 1520, loss = 0.042497200734521214
2025-03-01 13:37:37,568 : 1341934384.py : train_scp : INFO : sample = 1536, loss = 0.04235109677150225
2025-03-01 13:37:38,056 : 1341934384.py : train_scp : INFO : sample = 1552, loss = 0.0422090942558554
2025-03-01 13:37:38,276 : 1341934384.py : train_scp : INFO : sample = 1568, loss = 0.04251361904399736
2025-03-01 13:37:38,769 : 1341934384.py : train_scp : INFO : sample = 1584, loss = 0.042168656495785474
2025-03-01 13:37:38,971 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04325922440737486
2025-03-01 13:37:39,246 : 1341934384.py : train_scp : INFO : sample = 1616, loss = 0.04347422656298864
2025-03-01 13:37:39,579 : 1341934384.py : train_scp : INFO : sample = 1632, loss = 0.043537533969855775
2025-03-01 13:37:39,694 : 1341934384.py : train_scp : INFO : sample = 1648, loss = 0.04365539424193716
2025-03-01 13:37:39,832 : 1341934384.py : train_scp : INFO : sample = 1664, loss = 0.043572475524762504
2025-03-01 13:37:39,991 : 1341934384.py : train_scp : INFO : sample = 1680, loss = 0.043721915107397806
2025-03-01 13:37:40,103 : 1341934384.py : train_scp : INFO : sample = 1696, loss = 0.04396253258411614
2025-03-01 13:37:40,210 : 1341934384.py : train_scp : INFO : sample = 1712, loss = 0.04389395742355106
2025-03-01 13:37:40,333 : 1341934384.py : train_scp : INFO : sample = 1728, loss = 0.04433848367383083
2025-03-01 13:37:40,451 : 1341934384.py : train_scp : INFO : sample = 1744, loss = 0.0448650991492862
2025-03-01 13:37:40,607 : 1341934384.py : train_scp : INFO : sample = 1760, loss = 0.04479825994507833
2025-03-01 13:37:40,746 : 1341934384.py : train_scp : INFO : sample = 1776, loss = 0.04487540203708786
2025-03-01 13:37:40,945 : 1341934384.py : train_scp : INFO : sample = 1792, loss = 0.04447472880461386
2025-03-01 13:37:41,132 : 1341934384.py : train_scp : INFO : sample = 1808, loss = 0.04485499885229938
2025-03-01 13:37:41,287 : 1341934384.py : train_scp : INFO : sample = 1824, loss = 0.04472010904563624
2025-03-01 13:37:41,785 : 1341934384.py : train_scp : INFO : sample = 1840, loss = 0.04515369691602562
2025-03-01 13:37:41,925 : 1341934384.py : train_scp : INFO : sample = 1856, loss = 0.04501763875756798
2025-03-01 13:37:54,942 : 916236790.py : __call__ : INFO : loaded dataset, sample = 1158
2025-03-01 13:37:54,944 : 1341934384.py : train_scp : INFO : 109483778
2025-03-01 13:37:57,671 : 1341934384.py : train_scp : INFO : sample = 160, loss = 0.04222539179027081
2025-03-01 13:37:59,681 : 1341934384.py : train_scp : INFO : sample = 320, loss = 0.03861848646774888
2025-03-01 13:38:02,955 : 1341934384.py : train_scp : INFO : sample = 480, loss = 0.041599612372616925
2025-03-01 13:38:05,320 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.039654718013480306
2025-03-01 13:38:07,355 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04337444826960564
2025-03-01 13:38:09,869 : 1341934384.py : train_scp : INFO : sample = 960, loss = 0.04056592145934701
2025-03-01 13:38:12,828 : 1341934384.py : train_scp : INFO : sample = 1120, loss = 0.03890906928905419
2025-03-01 13:38:15,229 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.03861385183408857
2025-03-01 13:38:18,139 : 1341934384.py : train_scp : INFO : sample = 1440, loss = 0.03988590838594569
2025-03-01 13:38:20,406 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.0419427040219307
2025-03-01 13:38:23,103 : 1341934384.py : train_scp : INFO : sample = 1760, loss = 0.04026273890313777
2025-03-01 13:38:26,630 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.04115406501417359
2025-03-01 13:38:29,831 : 1341934384.py : train_scp : INFO : sample = 2080, loss = 0.04009904550531736
2025-03-01 13:38:32,473 : 1341934384.py : train_scp : INFO : sample = 2240, loss = 0.03942152088774102
2025-03-01 13:38:35,028 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.039809679177900154
2025-03-01 13:38:38,304 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.041297792794648555
2025-03-01 13:38:41,136 : 1341934384.py : train_scp : INFO : sample = 2720, loss = 0.04162075658712317
2025-03-01 13:38:43,997 : 1341934384.py : train_scp : INFO : sample = 2880, loss = 0.04056001872652107
2025-03-01 13:38:46,322 : 1341934384.py : train_scp : INFO : sample = 3040, loss = 0.040310059370178924
2025-03-01 13:38:48,014 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04152067200280726
2025-03-01 13:38:50,373 : 1341934384.py : train_scp : INFO : sample = 3360, loss = 0.04187487310596875
2025-03-01 13:38:52,517 : 1341934384.py : train_scp : INFO : sample = 3520, loss = 0.0420142543502152
2025-03-01 13:38:55,039 : 1341934384.py : train_scp : INFO : sample = 3680, loss = 0.04196158972285364
2025-03-01 13:38:58,002 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.042343842688327034
2025-03-01 13:39:01,207 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.0422800577506423
2025-03-01 13:39:03,492 : 1341934384.py : train_scp : INFO : sample = 4160, loss = 0.042614020973157425
2025-03-01 13:39:05,707 : 1341934384.py : train_scp : INFO : sample = 4320, loss = 0.04321628733089677
2025-03-01 13:39:07,366 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.04297990739079458
2025-03-01 13:39:10,207 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models/_bert-base-uncased_101_epoch.pt
2025-03-01 13:39:13,301 : 1341934384.py : train_scp : INFO : sample = 160, loss = 0.05164366364479065
2025-03-01 13:39:15,846 : 1341934384.py : train_scp : INFO : sample = 320, loss = 0.04400898870080709
2025-03-01 13:39:17,842 : 1341934384.py : train_scp : INFO : sample = 480, loss = 0.04168586122492949
2025-03-01 13:39:20,617 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.04674805048853159
2025-03-01 13:39:23,277 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04445542279630899
2025-03-01 13:39:25,936 : 1341934384.py : train_scp : INFO : sample = 960, loss = 0.04223686490828792
2025-03-01 13:39:27,912 : 1341934384.py : train_scp : INFO : sample = 1120, loss = 0.04077765156647989
2025-03-01 13:39:29,843 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.04116598553955555
2025-03-01 13:39:32,994 : 1341934384.py : train_scp : INFO : sample = 1440, loss = 0.04228121464451154
2025-03-01 13:39:35,448 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04254441656172275
2025-03-01 13:39:38,122 : 1341934384.py : train_scp : INFO : sample = 1760, loss = 0.04253988316790624
2025-03-01 13:39:41,596 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.04313336436947187
2025-03-01 13:39:43,619 : 1341934384.py : train_scp : INFO : sample = 2080, loss = 0.04402739510226708
2025-03-01 13:39:46,458 : 1341934384.py : train_scp : INFO : sample = 2240, loss = 0.04395963437855244
2025-03-01 13:39:49,004 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04505563165992498
2025-03-01 13:39:51,267 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.04462799783796072
2025-03-01 13:39:54,338 : 1341934384.py : train_scp : INFO : sample = 2720, loss = 0.04448515487944379
2025-03-01 13:39:57,472 : 1341934384.py : train_scp : INFO : sample = 2880, loss = 0.0449246379857262
2025-03-01 13:39:59,657 : 1341934384.py : train_scp : INFO : sample = 3040, loss = 0.04494077878955163
2025-03-01 13:40:02,560 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04466781036928296
2025-03-01 13:40:04,968 : 1341934384.py : train_scp : INFO : sample = 3360, loss = 0.04458179956390744
2025-03-01 13:40:07,392 : 1341934384.py : train_scp : INFO : sample = 3520, loss = 0.04363148754293268
2025-03-01 13:40:10,012 : 1341934384.py : train_scp : INFO : sample = 3680, loss = 0.043426716254781124
2025-03-01 13:40:12,392 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.043653910808886094
2025-03-01 13:40:15,274 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04381870019435882
2025-03-01 13:40:18,189 : 1341934384.py : train_scp : INFO : sample = 4160, loss = 0.044536557077215266
2025-03-01 13:40:21,425 : 1341934384.py : train_scp : INFO : sample = 4320, loss = 0.04435863407949606
2025-03-01 13:40:23,749 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.04447096342753087
2025-03-01 13:40:26,047 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models/_bert-base-uncased_102_epoch.pt
2025-03-01 13:40:29,192 : 1341934384.py : train_scp : INFO : sample = 160, loss = 0.033046933077275756
2025-03-01 13:40:31,842 : 1341934384.py : train_scp : INFO : sample = 320, loss = 0.048514494113624095
2025-03-01 13:40:34,135 : 1341934384.py : train_scp : INFO : sample = 480, loss = 0.05093991030007601
2025-03-01 13:40:36,798 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.04479429088532925
2025-03-01 13:40:40,640 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.044447793290019036
2025-03-01 13:40:44,133 : 1341934384.py : train_scp : INFO : sample = 960, loss = 0.04944617052872976
2025-03-01 13:40:47,149 : 1341934384.py : train_scp : INFO : sample = 1120, loss = 0.049871761405042236
2025-03-01 13:40:49,441 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.048914043023250996
2025-03-01 13:40:52,060 : 1341934384.py : train_scp : INFO : sample = 1440, loss = 0.04880418688472774
2025-03-01 13:40:54,842 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.048537349812686446
2025-03-01 13:40:56,624 : 1341934384.py : train_scp : INFO : sample = 1760, loss = 0.046656909077004954
2025-03-01 13:40:59,273 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.04553725693064432
2025-03-01 13:41:01,541 : 1341934384.py : train_scp : INFO : sample = 2080, loss = 0.043542500184132506
2025-03-01 13:41:03,923 : 1341934384.py : train_scp : INFO : sample = 2240, loss = 0.04472224267997912
2025-03-01 13:41:06,718 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04537444082399209
2025-03-01 13:41:09,973 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.04659379260847345
2025-03-01 13:41:12,429 : 1341934384.py : train_scp : INFO : sample = 2720, loss = 0.045998441362205676
2025-03-01 13:41:15,071 : 1341934384.py : train_scp : INFO : sample = 2880, loss = 0.04583689272403717
2025-03-01 13:41:17,372 : 1341934384.py : train_scp : INFO : sample = 3040, loss = 0.04644816307056891
2025-03-01 13:41:19,841 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04617599598132074
2025-03-01 13:41:22,945 : 1341934384.py : train_scp : INFO : sample = 3360, loss = 0.046389917435035816
2025-03-01 13:41:25,235 : 1341934384.py : train_scp : INFO : sample = 3520, loss = 0.047421562409197746
2025-03-01 13:41:27,506 : 1341934384.py : train_scp : INFO : sample = 3680, loss = 0.04705249038727387
2025-03-01 13:41:30,786 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.04752713021201392
2025-03-01 13:41:33,568 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04741209378093481
2025-03-01 13:41:35,870 : 1341934384.py : train_scp : INFO : sample = 4160, loss = 0.04755765297808326
2025-03-01 13:41:39,109 : 1341934384.py : train_scp : INFO : sample = 4320, loss = 0.04741805386212137
2025-03-01 13:41:40,686 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.04734257758328957
2025-03-01 13:41:44,153 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models/_bert-base-uncased_103_epoch.pt
2025-03-01 13:41:47,670 : 1341934384.py : train_scp : INFO : sample = 160, loss = 0.05313739739358425
2025-03-01 13:41:50,374 : 1341934384.py : train_scp : INFO : sample = 320, loss = 0.04627811182290316
2025-03-01 13:41:52,740 : 1341934384.py : train_scp : INFO : sample = 480, loss = 0.03946693309893211
2025-03-01 13:41:55,418 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.042109706858173016
2025-03-01 13:41:57,887 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04459207203239202
2025-03-01 13:42:00,838 : 1341934384.py : train_scp : INFO : sample = 960, loss = 0.043119892260680595
2025-03-01 13:42:03,083 : 1341934384.py : train_scp : INFO : sample = 1120, loss = 0.040687133770968234
2025-03-01 13:42:05,341 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.03878640397451818
2025-03-01 13:42:08,278 : 1341934384.py : train_scp : INFO : sample = 1440, loss = 0.03581085569328732
2025-03-01 13:42:11,397 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.03577996872365475
2025-03-01 13:42:14,542 : 1341934384.py : train_scp : INFO : sample = 1760, loss = 0.037042455053464936
2025-03-01 13:42:17,072 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.037325796422859034
2025-03-01 13:42:20,281 : 1341934384.py : train_scp : INFO : sample = 2080, loss = 0.03857919844583823
2025-03-01 13:42:22,852 : 1341934384.py : train_scp : INFO : sample = 2240, loss = 0.03811286368540355
2025-03-01 13:42:25,293 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.03881518700470527
2025-03-01 13:42:28,693 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.039717079501133415
2025-03-01 13:42:31,740 : 1341934384.py : train_scp : INFO : sample = 2720, loss = 0.04026289575678461
2025-03-01 13:42:35,036 : 1341934384.py : train_scp : INFO : sample = 2880, loss = 0.04019530989850561
2025-03-01 13:42:37,547 : 1341934384.py : train_scp : INFO : sample = 3040, loss = 0.039960141283901116
2025-03-01 13:42:40,112 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04097747242078185
2025-03-01 13:42:42,891 : 1341934384.py : train_scp : INFO : sample = 3360, loss = 0.04050778047669502
2025-03-01 13:42:45,057 : 1341934384.py : train_scp : INFO : sample = 3520, loss = 0.04027236205610362
2025-03-01 13:42:47,945 : 1341934384.py : train_scp : INFO : sample = 3680, loss = 0.04118714614406876
2025-03-01 13:42:50,312 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.04198493026973059
2025-03-01 13:42:53,033 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04227281954884529
2025-03-01 13:42:55,486 : 1341934384.py : train_scp : INFO : sample = 4160, loss = 0.04219934467512828
2025-03-01 13:42:57,747 : 1341934384.py : train_scp : INFO : sample = 4320, loss = 0.042489779388739005
2025-03-01 13:42:59,704 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.043480885022186805
2025-03-01 13:43:01,828 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models/_bert-base-uncased_104_epoch.pt
2025-03-01 13:43:04,704 : 1341934384.py : train_scp : INFO : sample = 160, loss = 0.037069044448435305
2025-03-01 13:43:06,809 : 1341934384.py : train_scp : INFO : sample = 320, loss = 0.04424793664366007
2025-03-01 13:43:09,698 : 1341934384.py : train_scp : INFO : sample = 480, loss = 0.044462895455459756
2025-03-01 13:43:12,020 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.04527010708115995
2025-03-01 13:43:15,103 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.043646359741687776
2025-03-01 13:43:18,400 : 1341934384.py : train_scp : INFO : sample = 960, loss = 0.04487434507658084
2025-03-01 13:43:20,793 : 1341934384.py : train_scp : INFO : sample = 1120, loss = 0.04583159259387425
2025-03-01 13:43:23,475 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.04601491696666926
2025-03-01 13:43:26,308 : 1341934384.py : train_scp : INFO : sample = 1440, loss = 0.0494562574972709
2025-03-01 13:43:28,773 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.048129695765674115
2025-03-01 13:43:31,338 : 1341934384.py : train_scp : INFO : sample = 1760, loss = 0.04870167538862337
2025-03-01 13:43:33,677 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.04800552615585427
2025-03-01 13:43:36,494 : 1341934384.py : train_scp : INFO : sample = 2080, loss = 0.049365134508563925
2025-03-01 13:43:38,260 : 1341934384.py : train_scp : INFO : sample = 2240, loss = 0.0487166054546833
2025-03-01 13:43:40,566 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04857248368362586
2025-03-01 13:43:43,286 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.04857554086484015
2025-03-01 13:43:45,692 : 1341934384.py : train_scp : INFO : sample = 2720, loss = 0.048144791636835126
2025-03-01 13:43:48,912 : 1341934384.py : train_scp : INFO : sample = 2880, loss = 0.048616847416592966
2025-03-01 13:43:51,790 : 1341934384.py : train_scp : INFO : sample = 3040, loss = 0.0491389094900928
2025-03-01 13:43:54,234 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04866193427704275
2025-03-01 13:43:56,835 : 1341934384.py : train_scp : INFO : sample = 3360, loss = 0.04826203160697506
2025-03-01 13:43:59,850 : 1341934384.py : train_scp : INFO : sample = 3520, loss = 0.048231058212166486
2025-03-01 13:44:02,099 : 1341934384.py : train_scp : INFO : sample = 3680, loss = 0.04839657844732637
2025-03-01 13:44:05,594 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.04803317959109942
2025-03-01 13:44:07,878 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04756819559633732
2025-03-01 13:44:10,033 : 1341934384.py : train_scp : INFO : sample = 4160, loss = 0.04698890631015484
2025-03-01 13:44:12,702 : 1341934384.py : train_scp : INFO : sample = 4320, loss = 0.04651136728072608
2025-03-01 13:44:14,893 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.046513009550315995
2025-03-01 13:44:18,247 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models/_bert-base-uncased_105_epoch.pt
2025-03-01 13:44:21,252 : 1341934384.py : train_scp : INFO : sample = 160, loss = 0.038564670272171496
2025-03-01 13:44:24,808 : 1341934384.py : train_scp : INFO : sample = 320, loss = 0.04694237494841218
2025-03-01 13:44:27,501 : 1341934384.py : train_scp : INFO : sample = 480, loss = 0.052883120564123
2025-03-01 13:44:29,790 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.04915239992551505
2025-03-01 13:44:32,203 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.047167686708271504
2025-03-01 13:44:35,017 : 1341934384.py : train_scp : INFO : sample = 960, loss = 0.04618834160889188
2025-03-01 13:44:37,487 : 1341934384.py : train_scp : INFO : sample = 1120, loss = 0.048107075345303334
2025-03-01 13:44:39,534 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.0468123208032921
2025-03-01 13:44:41,772 : 1341934384.py : train_scp : INFO : sample = 1440, loss = 0.048995424931248026
2025-03-01 13:44:44,117 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.05030644169077277
2025-03-01 13:44:46,609 : 1341934384.py : train_scp : INFO : sample = 1760, loss = 0.0510849996723912
2025-03-01 13:44:49,669 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.051323293863485255
2025-03-01 13:44:52,761 : 1341934384.py : train_scp : INFO : sample = 2080, loss = 0.05077434438638962
2025-03-01 13:44:55,638 : 1341934384.py : train_scp : INFO : sample = 2240, loss = 0.0521087900868484
2025-03-01 13:44:58,726 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.05166172794997692
2025-03-01 13:45:01,706 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.05317234959220514
2025-03-01 13:45:04,631 : 1341934384.py : train_scp : INFO : sample = 2720, loss = 0.05245258155114511
2025-03-01 13:45:06,758 : 1341934384.py : train_scp : INFO : sample = 2880, loss = 0.05156618298755752
2025-03-01 13:45:09,485 : 1341934384.py : train_scp : INFO : sample = 3040, loss = 0.05107268747922621
2025-03-01 13:45:12,084 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.05134461400099099
2025-03-01 13:45:15,007 : 1341934384.py : train_scp : INFO : sample = 3360, loss = 0.05045621215055386
2025-03-01 13:45:16,917 : 1341934384.py : train_scp : INFO : sample = 3520, loss = 0.04993898278440941
2025-03-01 13:45:19,758 : 1341934384.py : train_scp : INFO : sample = 3680, loss = 0.049671934391169445
2025-03-01 13:45:22,941 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.049814661211955054
2025-03-01 13:45:25,279 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.05035691378265619
2025-03-01 13:45:28,391 : 1341934384.py : train_scp : INFO : sample = 4160, loss = 0.050423293911780305
2025-03-01 13:45:31,020 : 1341934384.py : train_scp : INFO : sample = 4320, loss = 0.050072967323164144
2025-03-01 13:45:33,852 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.050124013623488804
2025-03-01 13:45:36,079 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models/_bert-base-uncased_106_epoch.pt
2025-03-01 13:45:39,420 : 1341934384.py : train_scp : INFO : sample = 160, loss = 0.06442835144698619
2025-03-01 13:45:41,823 : 1341934384.py : train_scp : INFO : sample = 320, loss = 0.053569108713418245
2025-03-01 13:45:45,201 : 1341934384.py : train_scp : INFO : sample = 480, loss = 0.05636135277648767
2025-03-01 13:45:48,677 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.04822257123887539
2025-03-01 13:45:51,573 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04625828500837088
2025-03-01 13:45:54,460 : 1341934384.py : train_scp : INFO : sample = 960, loss = 0.047711463272571565
2025-03-01 13:45:56,796 : 1341934384.py : train_scp : INFO : sample = 1120, loss = 0.047088476297046454
2025-03-01 13:45:59,552 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.04571769346948713
2025-03-01 13:46:01,788 : 1341934384.py : train_scp : INFO : sample = 1440, loss = 0.044801348199447
2025-03-01 13:46:04,446 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.042798608150333164
2025-03-01 13:46:06,658 : 1341934384.py : train_scp : INFO : sample = 1760, loss = 0.04136567877774889
2025-03-01 13:46:09,603 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.04104181096578638
2025-03-01 13:46:12,342 : 1341934384.py : train_scp : INFO : sample = 2080, loss = 0.040310781434751475
2025-03-01 13:46:14,594 : 1341934384.py : train_scp : INFO : sample = 2240, loss = 0.040687641088983845
2025-03-01 13:46:16,918 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04019379857927561
2025-03-01 13:46:19,390 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.03984541118843481
2025-03-01 13:46:21,936 : 1341934384.py : train_scp : INFO : sample = 2720, loss = 0.040602831717799694
2025-03-01 13:46:24,662 : 1341934384.py : train_scp : INFO : sample = 2880, loss = 0.04143125971572267
2025-03-01 13:46:27,279 : 1341934384.py : train_scp : INFO : sample = 3040, loss = 0.0414039929838557
2025-03-01 13:46:29,613 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04141654229722917
2025-03-01 13:46:31,936 : 1341934384.py : train_scp : INFO : sample = 3360, loss = 0.04166937069523902
2025-03-01 13:46:34,877 : 1341934384.py : train_scp : INFO : sample = 3520, loss = 0.04160948129032146
2025-03-01 13:46:37,955 : 1341934384.py : train_scp : INFO : sample = 3680, loss = 0.04155200701206922
2025-03-01 13:46:40,430 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.04152938158561786
2025-03-01 13:46:43,406 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.041313108623027804
2025-03-01 13:46:46,905 : 1341934384.py : train_scp : INFO : sample = 4160, loss = 0.04119064176025299
2025-03-01 13:46:49,110 : 1341934384.py : train_scp : INFO : sample = 4320, loss = 0.04198437105450365
2025-03-01 13:46:51,913 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.0416945971082896
2025-03-01 13:46:54,471 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models/_bert-base-uncased_107_epoch.pt
2025-03-01 13:46:58,486 : 1341934384.py : train_scp : INFO : sample = 160, loss = 0.0697862109169364
2025-03-01 13:47:01,660 : 1341934384.py : train_scp : INFO : sample = 320, loss = 0.05419884882867336
2025-03-01 13:47:04,177 : 1341934384.py : train_scp : INFO : sample = 480, loss = 0.055168907965222995
2025-03-01 13:47:07,068 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.04992188587784767
2025-03-01 13:47:09,284 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.05007538210600614
2025-03-01 13:47:12,536 : 1341934384.py : train_scp : INFO : sample = 960, loss = 0.04804636814321081
2025-03-01 13:47:15,725 : 1341934384.py : train_scp : INFO : sample = 1120, loss = 0.046441047079861165
2025-03-01 13:47:18,317 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.04612432590220124
2025-03-01 13:47:21,324 : 1341934384.py : train_scp : INFO : sample = 1440, loss = 0.044570839570628275
2025-03-01 13:47:23,891 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04652831157669425
2025-03-01 13:47:26,433 : 1341934384.py : train_scp : INFO : sample = 1760, loss = 0.04645819804546508
2025-03-01 13:47:28,973 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.046910857083275914
2025-03-01 13:47:31,664 : 1341934384.py : train_scp : INFO : sample = 2080, loss = 0.04786756954227502
2025-03-01 13:47:34,368 : 1341934384.py : train_scp : INFO : sample = 2240, loss = 0.047663577659321686
2025-03-01 13:47:37,006 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04710379631568988
2025-03-01 13:47:39,870 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.04778084582649171
2025-03-01 13:47:42,304 : 1341934384.py : train_scp : INFO : sample = 2720, loss = 0.046755045586649106
2025-03-01 13:47:45,224 : 1341934384.py : train_scp : INFO : sample = 2880, loss = 0.046754834883742866
2025-03-01 13:47:48,104 : 1341934384.py : train_scp : INFO : sample = 3040, loss = 0.046342971215122625
2025-03-01 13:47:51,056 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04734457105398178
2025-03-01 13:47:53,733 : 1341934384.py : train_scp : INFO : sample = 3360, loss = 0.04673239636634077
2025-03-01 13:47:56,088 : 1341934384.py : train_scp : INFO : sample = 3520, loss = 0.046537491399794816
2025-03-01 13:47:57,580 : 1341934384.py : train_scp : INFO : sample = 3680, loss = 0.045879290965588196
2025-03-01 13:47:59,727 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.04579939284982781
2025-03-01 13:48:02,408 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04672948722541332
2025-03-01 13:48:05,085 : 1341934384.py : train_scp : INFO : sample = 4160, loss = 0.04688091475086716
2025-03-01 13:48:07,536 : 1341934384.py : train_scp : INFO : sample = 4320, loss = 0.045970258544440624
2025-03-01 13:48:09,891 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.04553297805999006
2025-03-01 13:48:11,953 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models/_bert-base-uncased_108_epoch.pt
2025-03-01 13:48:14,521 : 1341934384.py : train_scp : INFO : sample = 160, loss = 0.04331570528447628
2025-03-01 13:48:17,283 : 1341934384.py : train_scp : INFO : sample = 320, loss = 0.045989579427987336
2025-03-01 13:48:19,780 : 1341934384.py : train_scp : INFO : sample = 480, loss = 0.05063945061216752
2025-03-01 13:48:23,007 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.048484754376113416
2025-03-01 13:48:25,143 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04897965218871832
2025-03-01 13:48:27,104 : 1341934384.py : train_scp : INFO : sample = 960, loss = 0.04932607160881162
2025-03-01 13:48:29,655 : 1341934384.py : train_scp : INFO : sample = 1120, loss = 0.0469893408407058
2025-03-01 13:48:33,026 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.04777778882998973
2025-03-01 13:48:36,247 : 1341934384.py : train_scp : INFO : sample = 1440, loss = 0.04828506836460696
2025-03-01 13:48:39,168 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04816825145855546
2025-03-01 13:48:41,387 : 1341934384.py : train_scp : INFO : sample = 1760, loss = 0.04800370733507655
2025-03-01 13:48:44,401 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.048768071938926974
2025-03-01 13:48:47,598 : 1341934384.py : train_scp : INFO : sample = 2080, loss = 0.04828983092537293
2025-03-01 13:48:50,156 : 1341934384.py : train_scp : INFO : sample = 2240, loss = 0.04901567603062306
2025-03-01 13:48:52,760 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.049459372436006864
2025-03-01 13:48:54,984 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.04924728110199794
2025-03-01 13:48:57,454 : 1341934384.py : train_scp : INFO : sample = 2720, loss = 0.048936323624323395
2025-03-01 13:48:59,575 : 1341934384.py : train_scp : INFO : sample = 2880, loss = 0.0493777830981546
2025-03-01 13:49:01,812 : 1341934384.py : train_scp : INFO : sample = 3040, loss = 0.049812192244357185
2025-03-01 13:49:04,270 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04963511037640274
2025-03-01 13:49:06,778 : 1341934384.py : train_scp : INFO : sample = 3360, loss = 0.0502006694940584
2025-03-01 13:49:09,973 : 1341934384.py : train_scp : INFO : sample = 3520, loss = 0.0503003124570982
2025-03-01 13:49:12,627 : 1341934384.py : train_scp : INFO : sample = 3680, loss = 0.05015279738961355
2025-03-01 13:49:15,760 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.050129814182097714
2025-03-01 13:49:17,698 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.050936334490776065
2025-03-01 13:49:19,895 : 1341934384.py : train_scp : INFO : sample = 4160, loss = 0.05160491128380482
2025-03-01 13:49:22,454 : 1341934384.py : train_scp : INFO : sample = 4320, loss = 0.05182183170346198
2025-03-01 13:49:24,785 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.051221781011138647
2025-03-01 13:49:27,395 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models/_bert-base-uncased_109_epoch.pt
2025-03-01 13:49:30,565 : 1341934384.py : train_scp : INFO : sample = 160, loss = 0.049463724717497826
2025-03-01 13:49:33,073 : 1341934384.py : train_scp : INFO : sample = 320, loss = 0.04111521048471332
2025-03-01 13:49:35,459 : 1341934384.py : train_scp : INFO : sample = 480, loss = 0.04444769142816464
2025-03-01 13:49:38,217 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.04670422803610563
2025-03-01 13:49:40,882 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.049238469190895556
2025-03-01 13:49:42,540 : 1341934384.py : train_scp : INFO : sample = 960, loss = 0.04600180018072327
2025-03-01 13:49:44,759 : 1341934384.py : train_scp : INFO : sample = 1120, loss = 0.04692608767322132
2025-03-01 13:49:46,845 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.04767466438934207
2025-03-01 13:49:50,329 : 1341934384.py : train_scp : INFO : sample = 1440, loss = 0.04999895416614082
2025-03-01 13:49:52,612 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.050213227458298204
2025-03-01 13:49:55,675 : 1341934384.py : train_scp : INFO : sample = 1760, loss = 0.04887034478174015
2025-03-01 13:49:58,827 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.04824380079905192
2025-03-01 13:50:01,436 : 1341934384.py : train_scp : INFO : sample = 2080, loss = 0.04754880480468273
2025-03-01 13:50:04,994 : 1341934384.py : train_scp : INFO : sample = 2240, loss = 0.04784264572496925
2025-03-01 13:50:07,746 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04702029137561719
2025-03-01 13:50:09,942 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.04742217964958399
2025-03-01 13:50:12,489 : 1341934384.py : train_scp : INFO : sample = 2720, loss = 0.04707212542348048
2025-03-01 13:50:15,217 : 1341934384.py : train_scp : INFO : sample = 2880, loss = 0.04654878604536255
2025-03-01 13:50:18,278 : 1341934384.py : train_scp : INFO : sample = 3040, loss = 0.045431829694854584
2025-03-01 13:50:20,107 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04505283856764436
2025-03-01 13:50:23,119 : 1341934384.py : train_scp : INFO : sample = 3360, loss = 0.04512717278585548
2025-03-01 13:50:25,437 : 1341934384.py : train_scp : INFO : sample = 3520, loss = 0.04543511644005775
2025-03-01 13:50:27,840 : 1341934384.py : train_scp : INFO : sample = 3680, loss = 0.04514437078133873
2025-03-01 13:50:30,021 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.04616868641848366
2025-03-01 13:50:33,263 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04619488273561001
2025-03-01 13:50:36,147 : 1341934384.py : train_scp : INFO : sample = 4160, loss = 0.04616713386315566
2025-03-01 13:50:38,062 : 1341934384.py : train_scp : INFO : sample = 4320, loss = 0.04653054608928937
2025-03-01 13:50:40,987 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.046608068095520136
2025-03-01 13:50:43,652 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models/_bert-base-uncased_110_epoch.pt
2025-03-01 13:50:47,213 : 1341934384.py : train_scp : INFO : sample = 160, loss = 0.059526252746582034
2025-03-01 13:50:49,414 : 1341934384.py : train_scp : INFO : sample = 320, loss = 0.05416468009352684
2025-03-01 13:50:51,743 : 1341934384.py : train_scp : INFO : sample = 480, loss = 0.047624854867657024
2025-03-01 13:50:53,675 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.04802906038239598
2025-03-01 13:50:56,670 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04759101640433073
2025-03-01 13:50:59,528 : 1341934384.py : train_scp : INFO : sample = 960, loss = 0.045123556070029734
2025-03-01 13:51:02,410 : 1341934384.py : train_scp : INFO : sample = 1120, loss = 0.044104289662625105
2025-03-01 13:51:05,104 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.04468635858502239
2025-03-01 13:51:08,310 : 1341934384.py : train_scp : INFO : sample = 1440, loss = 0.044675198963118924
2025-03-01 13:51:11,823 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04427364528179169
2025-03-01 13:51:14,401 : 1341934384.py : train_scp : INFO : sample = 1760, loss = 0.04337620880793441
2025-03-01 13:51:17,379 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.0431640420264254
2025-03-01 13:51:19,917 : 1341934384.py : train_scp : INFO : sample = 2080, loss = 0.0446618407821426
2025-03-01 13:51:22,965 : 1341934384.py : train_scp : INFO : sample = 2240, loss = 0.04565949028890048
2025-03-01 13:51:25,970 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04520043004304171
2025-03-01 13:51:28,180 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.04590584299294278
2025-03-01 13:51:31,330 : 1341934384.py : train_scp : INFO : sample = 2720, loss = 0.04517834072603899
2025-03-01 13:51:34,488 : 1341934384.py : train_scp : INFO : sample = 2880, loss = 0.04474513089905183
2025-03-01 13:51:37,208 : 1341934384.py : train_scp : INFO : sample = 3040, loss = 0.044722193342290426
2025-03-01 13:51:39,364 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04369368014857173
2025-03-01 13:51:41,738 : 1341934384.py : train_scp : INFO : sample = 3360, loss = 0.04390732625588065
2025-03-01 13:51:43,813 : 1341934384.py : train_scp : INFO : sample = 3520, loss = 0.04345434268259189
2025-03-01 13:51:46,628 : 1341934384.py : train_scp : INFO : sample = 3680, loss = 0.04341155338384535
2025-03-01 13:51:49,043 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.04367201426066458
2025-03-01 13:51:51,430 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.043321503452956676
2025-03-01 13:51:54,292 : 1341934384.py : train_scp : INFO : sample = 4160, loss = 0.043755341586298666
2025-03-01 13:51:56,740 : 1341934384.py : train_scp : INFO : sample = 4320, loss = 0.04393654834755041
2025-03-01 13:51:59,197 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.044084529764950274
2025-03-01 13:52:01,686 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models/_bert-base-uncased_111_epoch.pt
2025-03-01 13:52:05,788 : 1341934384.py : train_scp : INFO : sample = 160, loss = 0.055973381362855436
2025-03-01 13:52:08,864 : 1341934384.py : train_scp : INFO : sample = 320, loss = 0.041638084407895806
2025-03-01 13:52:11,564 : 1341934384.py : train_scp : INFO : sample = 480, loss = 0.044510851117471856
2025-03-01 13:52:14,698 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.04436643905937672
2025-03-01 13:52:17,378 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.0440882408618927
2025-03-01 13:52:19,518 : 1341934384.py : train_scp : INFO : sample = 960, loss = 0.04800183723370234
2025-03-01 13:52:22,629 : 1341934384.py : train_scp : INFO : sample = 1120, loss = 0.04662519256983485
2025-03-01 13:52:25,442 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.04664396671578288
2025-03-01 13:52:28,041 : 1341934384.py : train_scp : INFO : sample = 1440, loss = 0.04749959038777484
2025-03-01 13:52:30,645 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.046289879269897936
2025-03-01 13:52:33,244 : 1341934384.py : train_scp : INFO : sample = 1760, loss = 0.04491878424517133
2025-03-01 13:52:35,093 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.04409955573889116
2025-03-01 13:52:37,258 : 1341934384.py : train_scp : INFO : sample = 2080, loss = 0.04548906312825588
2025-03-01 13:52:40,307 : 1341934384.py : train_scp : INFO : sample = 2240, loss = 0.04592098331611071
2025-03-01 13:52:42,296 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04688773587346077
2025-03-01 13:52:45,225 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.046896440093405545
2025-03-01 13:52:48,558 : 1341934384.py : train_scp : INFO : sample = 2720, loss = 0.04788261631174999
2025-03-01 13:52:50,525 : 1341934384.py : train_scp : INFO : sample = 2880, loss = 0.046874194147272244
2025-03-01 13:52:53,297 : 1341934384.py : train_scp : INFO : sample = 3040, loss = 0.046161952555963864
2025-03-01 13:52:55,984 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04609033339656889
2025-03-01 13:52:59,207 : 1341934384.py : train_scp : INFO : sample = 3360, loss = 0.04579972263780378
2025-03-01 13:53:01,797 : 1341934384.py : train_scp : INFO : sample = 3520, loss = 0.04506177016618577
2025-03-01 13:53:04,216 : 1341934384.py : train_scp : INFO : sample = 3680, loss = 0.044561925635713595
2025-03-01 13:53:07,050 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.04451614764208595
2025-03-01 13:53:09,686 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04412249653041363
2025-03-01 13:53:11,849 : 1341934384.py : train_scp : INFO : sample = 4160, loss = 0.045771333030783214
2025-03-01 13:53:14,035 : 1341934384.py : train_scp : INFO : sample = 4320, loss = 0.04548359598826479
2025-03-01 13:53:16,457 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.04600239328907004
2025-03-01 13:53:19,402 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models/_bert-base-uncased_112_epoch.pt
2025-03-01 13:53:22,478 : 1341934384.py : train_scp : INFO : sample = 160, loss = 0.0340649027377367
2025-03-01 13:53:24,614 : 1341934384.py : train_scp : INFO : sample = 320, loss = 0.032789841014891866
2025-03-01 13:53:27,223 : 1341934384.py : train_scp : INFO : sample = 480, loss = 0.04660460837185383
2025-03-01 13:53:30,219 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.04115399825386703
2025-03-01 13:53:32,540 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04369066700339317
2025-03-01 13:53:36,128 : 1341934384.py : train_scp : INFO : sample = 960, loss = 0.04632170389716824
2025-03-01 13:53:38,215 : 1341934384.py : train_scp : INFO : sample = 1120, loss = 0.04695787730493716
2025-03-01 13:53:41,468 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.046956906211562455
2025-03-01 13:53:44,079 : 1341934384.py : train_scp : INFO : sample = 1440, loss = 0.044977260153326724
2025-03-01 13:53:46,435 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04437453484162688
2025-03-01 13:53:49,357 : 1341934384.py : train_scp : INFO : sample = 1760, loss = 0.045545591583306136
2025-03-01 13:53:51,897 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.045243323392545186
2025-03-01 13:53:54,457 : 1341934384.py : train_scp : INFO : sample = 2080, loss = 0.04486756650014566
2025-03-01 13:53:56,946 : 1341934384.py : train_scp : INFO : sample = 2240, loss = 0.04497612286359072
2025-03-01 13:53:59,858 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04530233547091484
2025-03-01 13:54:02,753 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.044755579519551246
2025-03-01 13:54:05,849 : 1341934384.py : train_scp : INFO : sample = 2720, loss = 0.045593166800544545
2025-03-01 13:54:08,042 : 1341934384.py : train_scp : INFO : sample = 2880, loss = 0.04554934472673469
2025-03-01 13:54:11,110 : 1341934384.py : train_scp : INFO : sample = 3040, loss = 0.04527936417021249
2025-03-01 13:54:13,760 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.045374854980036614
2025-03-01 13:54:15,758 : 1341934384.py : train_scp : INFO : sample = 3360, loss = 0.045595183241225426
2025-03-01 13:54:18,431 : 1341934384.py : train_scp : INFO : sample = 3520, loss = 0.04551676254380833
2025-03-01 13:54:20,421 : 1341934384.py : train_scp : INFO : sample = 3680, loss = 0.04561582800486813
2025-03-01 13:54:22,969 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.045989603448348744
2025-03-01 13:54:25,413 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04573146434128284
2025-03-01 13:54:28,396 : 1341934384.py : train_scp : INFO : sample = 4160, loss = 0.045537603173691495
2025-03-01 13:54:31,199 : 1341934384.py : train_scp : INFO : sample = 4320, loss = 0.0453979259011922
2025-03-01 13:54:33,651 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.04589761697820255
2025-03-01 13:54:36,010 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models/_bert-base-uncased_113_epoch.pt
2025-03-01 13:54:40,124 : 1341934384.py : train_scp : INFO : sample = 160, loss = 0.046790119260549545
2025-03-01 13:54:42,852 : 1341934384.py : train_scp : INFO : sample = 320, loss = 0.04284877888858318
2025-03-01 13:54:45,227 : 1341934384.py : train_scp : INFO : sample = 480, loss = 0.0392844986791412
2025-03-01 13:54:47,771 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.042781790252774954
2025-03-01 13:54:50,068 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.042546330131590364
2025-03-01 13:54:52,276 : 1341934384.py : train_scp : INFO : sample = 960, loss = 0.04148242026567459
2025-03-01 13:54:55,548 : 1341934384.py : train_scp : INFO : sample = 1120, loss = 0.03856618029198476
2025-03-01 13:54:57,619 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.04052845239639282
2025-03-01 13:55:00,187 : 1341934384.py : train_scp : INFO : sample = 1440, loss = 0.043521588171521824
2025-03-01 13:55:03,222 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04771868173032999
2025-03-01 13:55:05,753 : 1341934384.py : train_scp : INFO : sample = 1760, loss = 0.04798698914660649
2025-03-01 13:55:07,661 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.04680531754468878
2025-03-01 13:55:10,271 : 1341934384.py : train_scp : INFO : sample = 2080, loss = 0.04663818862575751
2025-03-01 13:55:13,249 : 1341934384.py : train_scp : INFO : sample = 2240, loss = 0.0458120636242841
2025-03-01 13:55:17,129 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04562993853042523
2025-03-01 13:55:19,870 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.04550502973143011
2025-03-01 13:55:23,123 : 1341934384.py : train_scp : INFO : sample = 2720, loss = 0.044939500334508276
2025-03-01 13:55:25,986 : 1341934384.py : train_scp : INFO : sample = 2880, loss = 0.04414362365172969
2025-03-01 13:55:29,427 : 1341934384.py : train_scp : INFO : sample = 3040, loss = 0.043454050704052574
2025-03-01 13:55:32,212 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04341015859507024
2025-03-01 13:55:34,605 : 1341934384.py : train_scp : INFO : sample = 3360, loss = 0.043660842050753886
2025-03-01 13:55:36,844 : 1341934384.py : train_scp : INFO : sample = 3520, loss = 0.04324416887353767
2025-03-01 13:55:39,381 : 1341934384.py : train_scp : INFO : sample = 3680, loss = 0.04288344477181849
2025-03-01 13:55:42,159 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.04319032616137217
2025-03-01 13:55:44,951 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.043783068746328355
2025-03-01 13:55:47,990 : 1341934384.py : train_scp : INFO : sample = 4160, loss = 0.04379500882126964
2025-03-01 13:55:51,447 : 1341934384.py : train_scp : INFO : sample = 4320, loss = 0.04390916291072413
2025-03-01 13:55:53,887 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.04457783655795668
2025-03-01 13:55:55,738 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models/_bert-base-uncased_114_epoch.pt
2025-03-01 13:55:58,910 : 1341934384.py : train_scp : INFO : sample = 160, loss = 0.03427744302898646
2025-03-01 13:56:01,968 : 1341934384.py : train_scp : INFO : sample = 320, loss = 0.037264774832874534
2025-03-01 13:56:04,298 : 1341934384.py : train_scp : INFO : sample = 480, loss = 0.04572680195172628
2025-03-01 13:56:07,855 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.04658455974422395
2025-03-01 13:56:10,826 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04677738886326552
2025-03-01 13:56:13,020 : 1341934384.py : train_scp : INFO : sample = 960, loss = 0.04775218324114879
2025-03-01 13:56:15,446 : 1341934384.py : train_scp : INFO : sample = 1120, loss = 0.04958100318908691
2025-03-01 13:56:18,708 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.047686536936089395
2025-03-01 13:56:21,229 : 1341934384.py : train_scp : INFO : sample = 1440, loss = 0.045308924093842504
2025-03-01 13:56:24,442 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04716470556333661
2025-03-01 13:56:27,221 : 1341934384.py : train_scp : INFO : sample = 1760, loss = 0.048002126457338985
2025-03-01 13:56:29,814 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.04732029129130145
2025-03-01 13:56:32,008 : 1341934384.py : train_scp : INFO : sample = 2080, loss = 0.0467358974310068
2025-03-01 13:56:35,066 : 1341934384.py : train_scp : INFO : sample = 2240, loss = 0.046938937450093886
2025-03-01 13:56:37,528 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04743073542912801
2025-03-01 13:56:39,330 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.04797357841162011
2025-03-01 13:56:41,509 : 1341934384.py : train_scp : INFO : sample = 2720, loss = 0.04804043551797376
2025-03-01 13:56:44,469 : 1341934384.py : train_scp : INFO : sample = 2880, loss = 0.047622611632363664
2025-03-01 13:56:46,745 : 1341934384.py : train_scp : INFO : sample = 3040, loss = 0.047923301130925355
2025-03-01 13:56:49,407 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.047783989384770395
2025-03-01 13:56:52,227 : 1341934384.py : train_scp : INFO : sample = 3360, loss = 0.048873742084418026
2025-03-01 13:56:54,966 : 1341934384.py : train_scp : INFO : sample = 3520, loss = 0.04901525368914008
2025-03-01 13:56:57,558 : 1341934384.py : train_scp : INFO : sample = 3680, loss = 0.048357725062448044
2025-03-01 13:57:00,635 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.04954888161737472
2025-03-01 13:57:03,526 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.049991539545357226
2025-03-01 13:57:06,070 : 1341934384.py : train_scp : INFO : sample = 4160, loss = 0.04954280573874712
2025-03-01 13:57:08,303 : 1341934384.py : train_scp : INFO : sample = 4320, loss = 0.04902716727444419
2025-03-01 13:57:10,432 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.048570829230759824
2025-03-01 13:57:12,816 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models/_bert-base-uncased_115_epoch.pt
2025-03-01 13:57:16,209 : 1341934384.py : train_scp : INFO : sample = 160, loss = 0.03646880984306335
2025-03-01 13:57:18,434 : 1341934384.py : train_scp : INFO : sample = 320, loss = 0.03806143775582314
2025-03-01 13:57:20,859 : 1341934384.py : train_scp : INFO : sample = 480, loss = 0.04054269064217806
2025-03-01 13:57:23,254 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.04236833308823407
2025-03-01 13:57:25,993 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.045409991852939126
2025-03-01 13:57:29,389 : 1341934384.py : train_scp : INFO : sample = 960, loss = 0.04656298405801256
2025-03-01 13:57:32,101 : 1341934384.py : train_scp : INFO : sample = 1120, loss = 0.04753089861146041
2025-03-01 13:57:35,100 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.04702442248817533
2025-03-01 13:57:37,737 : 1341934384.py : train_scp : INFO : sample = 1440, loss = 0.045641862580345735
2025-03-01 13:57:40,666 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.045169676188379526
2025-03-01 13:57:43,554 : 1341934384.py : train_scp : INFO : sample = 1760, loss = 0.04671957994049246
2025-03-01 13:57:45,786 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.04675204874947667
2025-03-01 13:57:48,905 : 1341934384.py : train_scp : INFO : sample = 2080, loss = 0.04614481188070316
2025-03-01 13:57:51,477 : 1341934384.py : train_scp : INFO : sample = 2240, loss = 0.046051584383738894
2025-03-01 13:57:54,395 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.045797663231690725
2025-03-01 13:57:56,551 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.044960392580833286
2025-03-01 13:57:58,994 : 1341934384.py : train_scp : INFO : sample = 2720, loss = 0.04472342375665903
2025-03-01 13:58:01,760 : 1341934384.py : train_scp : INFO : sample = 2880, loss = 0.04442735350587302
2025-03-01 13:58:04,734 : 1341934384.py : train_scp : INFO : sample = 3040, loss = 0.04392336318759542
2025-03-01 13:58:06,708 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.044340523211285475
2025-03-01 13:58:09,274 : 1341934384.py : train_scp : INFO : sample = 3360, loss = 0.044359509309842476
2025-03-01 13:58:12,125 : 1341934384.py : train_scp : INFO : sample = 3520, loss = 0.04422640030018308
2025-03-01 13:58:14,404 : 1341934384.py : train_scp : INFO : sample = 3680, loss = 0.04437997733121333
2025-03-01 13:58:16,393 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.044308605091646316
2025-03-01 13:58:18,362 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04464582051336765
2025-03-01 13:58:21,557 : 1341934384.py : train_scp : INFO : sample = 4160, loss = 0.04418898260650726
2025-03-01 13:58:23,425 : 1341934384.py : train_scp : INFO : sample = 4320, loss = 0.0440074280180313
2025-03-01 13:58:26,801 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.04443800914367395
2025-03-01 13:58:29,980 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models/_bert-base-uncased_116_epoch.pt
2025-03-01 13:58:33,084 : 1341934384.py : train_scp : INFO : sample = 160, loss = 0.061381605267524716
2025-03-01 13:58:35,220 : 1341934384.py : train_scp : INFO : sample = 320, loss = 0.05268324315547943
2025-03-01 13:58:37,610 : 1341934384.py : train_scp : INFO : sample = 480, loss = 0.05211486853659153
2025-03-01 13:58:39,725 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.053668382484465836
2025-03-01 13:58:42,679 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.05130523681640625
2025-03-01 13:58:45,096 : 1341934384.py : train_scp : INFO : sample = 960, loss = 0.04942513930921753
2025-03-01 13:58:47,621 : 1341934384.py : train_scp : INFO : sample = 1120, loss = 0.04880772318158831
2025-03-01 13:58:50,881 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.05049784246366471
2025-03-01 13:58:53,265 : 1341934384.py : train_scp : INFO : sample = 1440, loss = 0.04969348178969489
2025-03-01 13:58:55,916 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04787270655855536
2025-03-01 13:58:58,329 : 1341934384.py : train_scp : INFO : sample = 1760, loss = 0.0477492476051504
2025-03-01 13:59:01,357 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.04709803297494849
2025-03-01 13:59:03,898 : 1341934384.py : train_scp : INFO : sample = 2080, loss = 0.04599793725288831
2025-03-01 13:59:06,553 : 1341934384.py : train_scp : INFO : sample = 2240, loss = 0.04726045039881553
2025-03-01 13:59:08,992 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04698248522977034
2025-03-01 13:59:11,583 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.046456861589103934
2025-03-01 13:59:13,677 : 1341934384.py : train_scp : INFO : sample = 2720, loss = 0.046566736577626534
2025-03-01 13:59:16,799 : 1341934384.py : train_scp : INFO : sample = 2880, loss = 0.04573744475427601
2025-03-01 13:59:19,924 : 1341934384.py : train_scp : INFO : sample = 3040, loss = 0.04529600533607759
2025-03-01 13:59:22,524 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.044922062596306205
2025-03-01 13:59:25,428 : 1341934384.py : train_scp : INFO : sample = 3360, loss = 0.04491046997053283
2025-03-01 13:59:27,950 : 1341934384.py : train_scp : INFO : sample = 3520, loss = 0.04513965062797069
2025-03-01 13:59:30,838 : 1341934384.py : train_scp : INFO : sample = 3680, loss = 0.04543911914300659
2025-03-01 13:59:34,261 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.04513220381146918
2025-03-01 14:06:14,990 : 916236790.py : __call__ : INFO : loaded dataset, sample = 1158
2025-03-01 14:06:14,992 : 1341934384.py : train_scp : INFO : 109483778
2025-03-01 14:06:27,254 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.20233834385871888
2025-03-01 14:06:41,337 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.20116894505918026
2025-03-01 14:06:56,288 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.20077989275256794
2025-03-01 14:07:08,430 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.20058372169733046
2025-03-01 14:07:20,331 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.2004669001698494
2025-03-01 14:07:33,385 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.20038912954429786
2025-03-01 14:07:46,422 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.20033415449517114
2025-03-01 14:07:48,680 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=32/_bert-base-uncased_1_epoch.pt
2025-03-01 14:08:03,049 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.19999443143606185
2025-03-01 14:08:16,108 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.19999655671417713
2025-03-01 14:08:26,673 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.19999890178442
2025-03-01 14:08:40,463 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.19999866019934415
2025-03-01 14:08:53,531 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.19999925464391707
2025-03-01 14:09:07,162 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.20000029901663463
2025-03-01 14:09:20,467 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.20000050121120044
2025-03-01 14:09:23,885 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=32/_bert-base-uncased_2_epoch.pt
2025-03-01 14:09:38,085 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.19999753385782243
2025-03-01 14:09:49,219 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.19999915324151515
2025-03-01 14:10:01,962 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.19999947076042493
2025-03-01 14:10:14,166 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.19999972824007273
2025-03-01 14:10:26,947 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.19999993115663528
2025-03-01 14:10:39,540 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.19999991791943708
2025-03-01 14:10:54,090 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.19999971315264703
2025-03-01 14:10:57,518 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=32/_bert-base-uncased_3_epoch.pt
2025-03-01 14:11:11,600 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.20000037401914597
2025-03-01 14:11:26,121 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.19999970830976962
2025-03-01 14:11:38,914 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.1999986946582794
2025-03-01 14:11:52,101 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.19999907333403827
2025-03-01 14:12:05,690 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.1999989554286003
2025-03-01 14:12:17,628 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.19999652703603107
2025-03-01 14:12:30,663 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.20000032537749834
2025-03-01 14:12:34,120 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=32/_bert-base-uncased_4_epoch.pt
2025-03-01 14:12:46,351 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.199907186627388
2025-03-01 14:13:00,096 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.20020098127424718
2025-03-01 14:13:11,037 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.20013999715447425
2025-03-01 14:13:24,368 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.20010514352470637
2025-03-01 14:13:38,754 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.20008497759699823
2025-03-01 14:13:50,316 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.2000688888132572
2025-03-01 14:14:01,988 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.20005812336291587
2025-03-01 14:14:05,378 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=32/_bert-base-uncased_5_epoch.pt
2025-03-01 14:14:18,039 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.19998752549290658
2025-03-01 14:14:30,928 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.2000033061951399
2025-03-01 14:14:43,031 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.19995731537540753
2025-03-01 14:14:56,471 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.19996419847011565
2025-03-01 14:15:11,246 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.199894907027483
2025-03-01 14:15:23,301 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.19984106806417307
2025-03-01 14:15:35,102 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.19983739544238363
2025-03-01 14:15:38,573 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=32/_bert-base-uncased_6_epoch.pt
2025-03-01 14:15:52,823 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.19993987530469895
2025-03-01 14:16:04,813 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.1996406391263008
2025-03-01 14:16:17,477 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.1997744175295035
2025-03-01 14:16:32,128 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.19983843062072992
2025-03-01 14:16:42,335 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.19986506402492524
2025-03-01 14:16:56,306 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.19988617151975632
2025-03-01 14:17:07,321 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.19990272170731
2025-03-01 14:17:10,162 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=32/_bert-base-uncased_7_epoch.pt
2025-03-01 14:17:22,879 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.20000932291150092
2025-03-01 14:17:35,843 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.20000868141651154
2025-03-01 14:17:49,278 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.20001223360498746
2025-03-01 14:18:02,850 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.20000430047512055
2025-03-01 14:18:16,713 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.200000529140234
2025-03-01 14:18:28,512 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.20000077436367672
2025-03-01 14:18:42,041 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.20000073036977223
2025-03-01 14:18:45,248 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=32/_bert-base-uncased_8_epoch.pt
2025-03-01 14:18:58,363 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.19996752366423606
2025-03-01 14:19:13,500 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.19996506161987782
2025-03-01 14:19:26,904 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.19997140963872273
2025-03-01 14:19:40,776 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.1999704184010625
2025-03-01 14:19:54,241 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.19994624555110932
2025-03-01 14:20:05,759 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.19999221563339234
2025-03-01 14:20:16,032 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.2000001416674682
2025-03-01 14:20:19,144 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=32/_bert-base-uncased_9_epoch.pt
2025-03-01 14:20:32,449 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.1998692885041237
2025-03-01 14:20:45,156 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.1999190106987953
2025-03-01 14:20:58,702 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.200006149460872
2025-03-01 14:21:12,085 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.19993477538228036
2025-03-01 14:21:25,040 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.1998451507091522
2025-03-01 14:21:39,042 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.1997969093422095
2025-03-01 14:21:52,002 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.19979164642947062
2025-03-01 14:21:55,342 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=32/_bert-base-uncased_10_epoch.pt
2025-03-01 14:22:09,289 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.20011971071362494
2025-03-01 14:22:24,186 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.2000340547412634
2025-03-01 14:22:37,465 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.1999925807118416
2025-03-01 14:22:49,506 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.199940930493176
2025-03-01 14:23:03,183 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.19964573293924331
2025-03-01 14:23:14,694 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.19834265038371085
2025-03-01 14:23:26,128 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.1916084713701691
2025-03-01 14:23:29,666 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=32/_bert-base-uncased_11_epoch.pt
2025-03-01 14:23:41,172 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.09719137363135814
2025-03-01 14:23:54,924 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.09589155949652195
2025-03-01 14:24:07,265 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.08974808901548385
2025-03-01 14:24:18,142 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.08981368015520275
2025-03-01 14:24:31,450 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.08728203732520341
2025-03-01 14:24:45,100 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.08565511374423901
2025-03-01 14:24:57,803 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.08510962837774838
2025-03-01 14:25:01,688 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=32/_bert-base-uncased_12_epoch.pt
2025-03-01 14:25:13,784 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.06912975343875587
2025-03-01 14:25:26,674 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.07062291481997818
2025-03-01 14:25:38,640 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.07087378464639187
2025-03-01 14:25:51,147 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.07050596973858773
2025-03-01 14:26:04,058 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.06884644197300077
2025-03-01 14:26:15,266 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.06757862347488602
2025-03-01 14:26:27,846 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.06980750781616994
2025-03-01 14:26:31,289 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=32/_bert-base-uncased_13_epoch.pt
2025-03-01 14:26:44,789 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.05660563143901527
2025-03-01 14:26:55,999 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.060505041293799876
2025-03-01 14:27:08,005 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.05894325117891033
2025-03-01 14:27:20,158 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.06301850404124706
2025-03-01 14:27:32,894 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.06508989794179797
2025-03-01 14:27:44,963 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.06364405809435994
2025-03-01 14:27:58,076 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.06153796870660569
2025-03-01 14:28:00,513 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=32/_bert-base-uncased_14_epoch.pt
2025-03-01 14:28:12,433 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.05326125891879201
2025-03-01 14:28:24,664 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.05419988879002631
2025-03-01 14:28:38,628 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.05263635919739803
2025-03-01 14:28:52,146 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.05298502191435546
2025-03-01 14:29:04,128 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.05465237961150706
2025-03-01 14:29:16,213 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.05511584742926061
2025-03-01 14:29:28,215 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.05571303205298526
2025-03-01 14:29:30,836 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=32/_bert-base-uncased_15_epoch.pt
2025-03-01 14:29:43,640 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.06158953942358494
2025-03-01 14:29:55,688 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.05998807307332754
2025-03-01 14:30:08,070 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.060514705783377094
2025-03-01 14:30:21,851 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.05886175954947248
2025-03-01 14:30:33,791 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.058533668452873826
2025-03-01 14:30:47,977 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.057428293970103066
2025-03-01 14:31:00,376 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.05699740855156311
2025-03-01 14:31:02,657 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=32/_bert-base-uncased_16_epoch.pt
2025-03-01 14:31:13,572 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.06743126669898629
2025-03-01 14:31:27,876 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.06540492004714907
2025-03-01 14:31:40,240 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.06564493250722686
2025-03-01 14:31:52,294 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.06403163261711597
2025-03-01 14:32:03,958 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.06334656715393067
2025-03-01 14:32:16,929 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.06150162375997752
2025-03-01 14:32:27,406 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.058966760290786624
2025-03-01 14:32:30,514 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=32/_bert-base-uncased_17_epoch.pt
2025-03-01 14:32:43,381 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.05172937768511474
2025-03-01 14:32:54,957 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.04938493759837002
2025-03-01 14:33:08,052 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.04982441915199161
2025-03-01 14:33:20,299 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.05418138378299773
2025-03-01 14:33:33,338 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.058997023683041334
2025-03-01 14:33:45,437 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.059296544765432675
2025-03-01 14:33:58,079 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.05798249206771808
2025-03-01 14:34:01,334 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=32/_bert-base-uncased_18_epoch.pt
2025-03-01 14:34:14,086 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.06046518459916115
2025-03-01 14:34:27,043 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.06128151337616146
2025-03-01 14:34:38,225 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.06142567675560713
2025-03-01 14:34:50,709 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.06027794888941571
2025-03-01 14:35:03,276 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.06114711436443031
2025-03-01 14:35:17,375 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.06115868728763114
2025-03-01 14:35:31,042 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.060133583890274164
2025-03-01 14:35:33,969 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=32/_bert-base-uncased_19_epoch.pt
2025-03-01 14:35:45,975 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.05941490479744971
2025-03-01 14:35:58,650 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.05562272178940475
2025-03-01 14:36:12,242 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.05376758687198162
2025-03-01 14:36:24,556 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.051986622472759335
2025-03-01 14:36:36,480 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.0516991055291146
2025-03-01 14:36:49,483 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.05362821538777401
2025-03-01 14:37:02,808 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.05447654144040176
2025-03-01 14:37:06,264 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=32/_bert-base-uncased_20_epoch.pt
2025-03-01 14:37:19,697 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.05928832464851439
2025-03-01 14:37:33,893 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.052966902893967924
2025-03-01 14:37:47,110 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.053406891770039995
2025-03-01 14:38:00,378 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.052833522495348006
2025-03-01 14:38:13,036 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.050734153902158144
2025-03-01 14:38:24,134 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.05000360099753986
2025-03-01 14:38:35,936 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.05017080742067524
2025-03-01 14:38:38,242 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=32/_bert-base-uncased_21_epoch.pt
2025-03-01 14:38:51,277 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.042951022274792194
2025-03-01 14:39:04,442 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.04901497177779675
2025-03-01 14:39:15,208 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.04759073778986931
2025-03-01 14:39:28,425 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.051440693181939424
2025-03-01 14:39:41,933 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.05111457445658743
2025-03-01 14:39:54,363 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.05230473168970396
2025-03-01 14:40:06,155 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.0525115321722946
2025-03-01 14:40:08,626 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=32/_bert-base-uncased_22_epoch.pt
2025-03-01 14:40:20,914 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.047516222763806584
2025-03-01 14:40:32,558 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.051015817327424884
2025-03-01 14:40:44,775 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.05188590774002175
2025-03-01 14:40:57,091 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.051384111831430344
2025-03-01 14:41:10,588 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.05177208886481822
2025-03-01 14:41:23,193 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.052567626000382006
2025-03-01 14:41:37,577 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.05262487393005618
2025-03-01 14:41:40,736 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=32/_bert-base-uncased_23_epoch.pt
2025-03-01 14:41:52,970 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.0548783996142447
2025-03-01 14:42:05,111 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.0553406385704875
2025-03-01 14:42:16,780 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.056008009845390914
2025-03-01 14:42:29,734 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.05518983408110216
2025-03-01 14:42:42,039 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.05555954157374799
2025-03-01 14:42:54,685 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.05580667070268343
2025-03-01 14:43:08,106 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.05596843557432294
2025-03-01 14:43:10,947 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=32/_bert-base-uncased_24_epoch.pt
2025-03-01 14:43:24,216 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.05232700342312455
2025-03-01 14:43:37,875 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.05282543362118304
2025-03-01 14:43:50,743 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.05651305013646682
2025-03-01 14:44:03,599 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.05344610793981701
2025-03-01 14:44:15,414 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.05564039200544357
2025-03-01 14:44:27,656 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.055735166960706316
2025-03-01 14:44:39,152 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.055663524182247265
2025-03-01 14:44:41,908 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=32/_bert-base-uncased_25_epoch.pt
2025-03-01 14:44:54,622 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.05629542483948171
2025-03-01 14:45:08,565 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.055457899416796866
2025-03-01 14:45:20,602 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.05867436296927432
2025-03-01 14:45:34,721 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.0583686369122006
2025-03-01 14:45:48,368 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.056490741204470396
2025-03-01 14:45:59,647 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.05492070751885573
2025-03-01 14:46:12,393 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.0560067222719746
2025-03-01 14:46:15,572 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=32/_bert-base-uncased_26_epoch.pt
2025-03-01 14:46:29,862 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.050735700549557804
2025-03-01 14:46:42,875 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.0524250460555777
2025-03-01 14:46:55,241 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.052172774681821465
2025-03-01 14:47:07,573 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.050955895811785015
2025-03-01 14:47:20,052 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.05105238944292068
2025-03-01 14:47:32,249 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.05329085863971462
2025-03-01 14:47:45,315 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.05371913778196488
2025-03-01 14:47:48,560 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=32/_bert-base-uncased_27_epoch.pt
2025-03-01 14:48:02,427 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.04436932555399835
2025-03-01 14:48:14,399 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.046149826631881295
2025-03-01 14:48:28,752 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.049632651700327796
2025-03-01 14:48:39,886 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.04778419630602002
2025-03-01 14:48:51,621 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.05040148654021323
2025-03-01 14:49:03,357 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.05082488663028926
2025-03-01 14:49:14,280 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.050819133055795516
2025-03-01 14:49:17,268 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=32/_bert-base-uncased_28_epoch.pt
2025-03-01 14:49:30,477 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.044195419270545246
2025-03-01 14:49:42,548 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.04876677463762462
2025-03-01 14:49:54,518 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.050123708710695304
2025-03-01 14:50:07,228 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.05141556072048843
2025-03-01 14:50:19,918 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.05395207153633237
2025-03-01 14:50:31,252 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.05352747974296411
2025-03-01 14:50:44,347 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.051964729871334776
2025-03-01 14:50:47,752 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=32/_bert-base-uncased_29_epoch.pt
2025-03-01 14:50:59,558 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.04242906500585377
2025-03-01 14:51:10,136 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.03866574058774859
2025-03-01 14:51:21,355 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.03938809752774735
2025-03-01 14:51:34,360 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.040242900501471016
2025-03-01 14:51:47,088 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.042905602045357226
2025-03-01 14:52:00,763 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.04550225097530832
2025-03-01 14:52:12,534 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.04561920409489955
2025-03-01 14:52:15,237 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=32/_bert-base-uncased_30_epoch.pt
2025-03-01 14:52:28,373 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.03989300234243274
2025-03-01 14:52:40,683 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.04812764266971499
2025-03-01 14:52:53,463 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.048225989239290355
2025-03-01 14:53:05,402 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.047385498031508176
2025-03-01 14:53:16,897 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.048135152542963626
2025-03-01 14:53:27,381 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.049351040506735444
2025-03-01 14:53:40,188 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.05050643827500088
2025-03-01 14:53:44,147 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=32/_bert-base-uncased_31_epoch.pt
2025-03-01 14:53:58,383 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.054959011171013115
2025-03-01 14:54:11,832 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.051148574450053275
2025-03-01 14:54:23,108 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.04995908224955201
2025-03-01 14:54:34,538 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.050508944934699686
2025-03-01 14:54:47,804 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04893470632843673
2025-03-01 14:54:59,927 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.05049670941662043
2025-03-01 14:55:13,051 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.05120016016465213
2025-03-01 14:55:15,518 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=32/_bert-base-uncased_32_epoch.pt
2025-03-01 14:55:28,122 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.041016230499371885
2025-03-01 14:55:39,861 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.05068239534739405
2025-03-01 14:55:51,900 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.05129870677677294
2025-03-01 14:56:05,681 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.05625317277153954
2025-03-01 14:56:17,803 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.05600710153579712
2025-03-01 14:56:28,534 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.055800268814588584
2025-03-01 14:56:40,960 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.053817556179793814
2025-03-01 14:56:43,810 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=32/_bert-base-uncased_33_epoch.pt
2025-03-01 14:56:57,155 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.053022258635610345
2025-03-01 14:57:11,150 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.04605468714144081
2025-03-01 14:57:22,275 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.04700018133347233
2025-03-01 14:57:34,884 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.049867729819379744
2025-03-01 14:57:44,820 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.05063280179165304
2025-03-01 14:57:56,693 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.05068509744014591
2025-03-01 14:58:10,915 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.04982295990921557
2025-03-01 14:58:13,270 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=32/_bert-base-uncased_34_epoch.pt
2025-03-01 14:58:25,595 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.06263976572081446
2025-03-01 14:58:37,910 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.05360096129588783
2025-03-01 14:58:52,455 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.054141107170532145
2025-03-01 14:59:05,480 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.05294455743860453
2025-03-01 14:59:18,330 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.05127241074107587
2025-03-01 14:59:30,973 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.04905157385704418
2025-03-01 14:59:41,972 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.05001804633066058
2025-03-01 14:59:44,794 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=32/_bert-base-uncased_35_epoch.pt
2025-03-01 14:59:57,389 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.05385772120207548
2025-03-01 15:00:10,702 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.05055831621866673
2025-03-01 15:00:24,418 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.053224547455708184
2025-03-01 15:00:36,456 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.04908574325963855
2025-03-01 15:00:48,331 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.05084112795069814
2025-03-01 15:01:00,974 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.05082064373418689
2025-03-01 15:01:12,415 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.05132721461621778
2025-03-01 15:01:15,258 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=32/_bert-base-uncased_36_epoch.pt
2025-03-01 15:01:27,992 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.053323785122483966
2025-03-01 15:01:40,454 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.0549827532377094
2025-03-01 15:01:53,123 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.053229692842190465
2025-03-01 15:02:05,662 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.05532193622784689
2025-03-01 15:02:18,896 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.05234255703166127
2025-03-01 15:02:31,800 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.052121108296948174
2025-03-01 15:02:43,713 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.052551560975345114
2025-03-01 15:02:45,798 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=32/_bert-base-uncased_37_epoch.pt
2025-03-01 15:02:57,220 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.052841095719486475
2025-03-01 15:03:10,823 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.050775847490876914
2025-03-01 15:03:22,129 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.05133153755838672
2025-03-01 15:03:35,462 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.053463587258011105
2025-03-01 15:03:48,907 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.05291598726063967
2025-03-01 15:04:01,210 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.05347849962611993
2025-03-01 15:04:13,347 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.05237465451604554
2025-03-01 15:04:17,045 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=32/_bert-base-uncased_38_epoch.pt
2025-03-01 15:04:30,652 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.040736954938620326
2025-03-01 15:04:41,482 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.04380540365818888
2025-03-01 15:04:53,409 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.04422256125447651
2025-03-01 15:05:03,637 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.04941593260737136
2025-03-01 15:05:17,542 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04935842771083117
2025-03-01 15:05:29,443 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.0488598253345117
2025-03-01 15:05:41,501 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.048558053986302446
2025-03-01 15:05:45,041 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=32/_bert-base-uncased_39_epoch.pt
2025-03-01 15:05:55,478 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.04955136366188526
2025-03-01 15:06:08,515 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.048399954987689855
2025-03-01 15:06:20,867 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.049602584540843965
2025-03-01 15:06:32,494 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.04878617495996877
2025-03-01 15:06:45,087 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04960579846054316
2025-03-01 15:06:58,119 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.05064014395854125
2025-03-01 15:07:09,977 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.049763242068833535
2025-03-01 15:07:13,008 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=32/_bert-base-uncased_40_epoch.pt
2025-03-01 15:07:25,205 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.05023576053790748
2025-03-01 15:07:35,790 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.0516733051976189
2025-03-01 15:07:48,763 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.051120688874895376
2025-03-01 15:08:02,152 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.05040947081288323
2025-03-01 15:08:13,542 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.051032705279067156
2025-03-01 15:08:27,055 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.051286500451775886
2025-03-01 15:08:40,192 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.051464778411069084
2025-03-01 15:08:42,548 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=32/_bert-base-uncased_41_epoch.pt
2025-03-01 15:08:55,719 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.05292693194933236
2025-03-01 15:09:06,907 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.0491895746672526
2025-03-01 15:09:22,639 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.0460113900868843
2025-03-01 15:09:33,142 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.0476593847037293
2025-03-01 15:09:45,096 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.050080690467730164
2025-03-01 15:09:56,850 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.04972763805029293
2025-03-01 15:10:10,281 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.050588832456352456
2025-03-01 15:10:13,890 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=32/_bert-base-uncased_42_epoch.pt
2025-03-01 15:10:26,050 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.04198459628969431
2025-03-01 15:10:37,409 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.050212943274527785
2025-03-01 15:10:49,712 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.04768253412718574
2025-03-01 15:11:02,855 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.04896201556548476
2025-03-01 15:11:14,899 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.0490165269933641
2025-03-01 15:11:26,657 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.04841652423298607
2025-03-01 15:11:40,076 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.04856503591207521
2025-03-01 15:11:42,579 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=32/_bert-base-uncased_43_epoch.pt
2025-03-01 15:11:53,131 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.059166777227073905
2025-03-01 15:12:04,816 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.053655818966217336
2025-03-01 15:12:15,571 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.05344203957356512
2025-03-01 15:12:27,738 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.05240613973001018
2025-03-01 15:12:41,917 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.05251404172740877
2025-03-01 15:12:54,705 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.05210016766407837
2025-03-01 15:13:05,112 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.05084793574309775
2025-03-01 15:13:08,455 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=32/_bert-base-uncased_44_epoch.pt
2025-03-01 15:13:23,598 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.04676552223972976
2025-03-01 15:13:34,555 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.05177240748889744
2025-03-01 15:13:45,331 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.05102606884514292
2025-03-01 15:13:57,726 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.05070823003770784
2025-03-01 15:14:08,382 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.05057551917620003
2025-03-01 15:14:20,311 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.049554032972082496
2025-03-01 15:14:33,369 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.04972042740056557
2025-03-01 15:14:36,697 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=32/_bert-base-uncased_45_epoch.pt
2025-03-01 15:14:47,727 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.04197372039780021
2025-03-01 15:15:00,750 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.04836643827147782
2025-03-01 15:15:14,566 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.04813099939686557
2025-03-01 15:15:28,479 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.0500343092600815
2025-03-01 15:15:39,723 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.05032244349829853
2025-03-01 15:15:50,472 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.0486104355348895
2025-03-01 15:16:04,294 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.04874453735537827
2025-03-01 15:16:07,103 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=32/_bert-base-uncased_46_epoch.pt
2025-03-01 15:16:18,595 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.045093773817643526
2025-03-01 15:16:29,966 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.04997937818989158
2025-03-01 15:16:41,827 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.04969589126606782
2025-03-01 15:16:55,637 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.04919590160716325
2025-03-01 15:17:08,802 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.0488937356043607
2025-03-01 15:17:18,911 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.04867382140364498
2025-03-01 15:17:31,585 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.047631859998883946
2025-03-01 15:17:34,952 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=32/_bert-base-uncased_47_epoch.pt
2025-03-01 15:17:48,461 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.04591072136536241
2025-03-01 15:17:59,750 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.05255672847852111
2025-03-01 15:18:11,227 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.05292747045556704
2025-03-01 15:18:22,241 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.05010556059423834
2025-03-01 15:18:34,795 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.047377978954464196
2025-03-01 15:18:48,544 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.04619424556537221
2025-03-01 15:18:58,874 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.04668151473493448
2025-03-01 15:19:01,290 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=32/_bert-base-uncased_48_epoch.pt
2025-03-01 15:19:11,732 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.052237815037369725
2025-03-01 15:19:23,225 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.054195912275463345
2025-03-01 15:19:36,174 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.05567317766447862
2025-03-01 15:19:46,532 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.05319041988113895
2025-03-01 15:19:57,369 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.05222444284707308
2025-03-01 15:20:11,240 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.051582884447028236
2025-03-01 15:20:25,356 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.05123281057790986
2025-03-01 15:20:28,442 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=32/_bert-base-uncased_49_epoch.pt
2025-03-01 15:20:39,754 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.04779899204149842
2025-03-01 15:20:50,195 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.05044641918502748
2025-03-01 15:21:02,555 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.05091765499673784
2025-03-01 15:21:15,160 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.05069421795196831
2025-03-01 15:21:28,555 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.05021779433824122
2025-03-01 15:21:40,558 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.04913015476583193
2025-03-01 15:21:54,400 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.049555835114525895
2025-03-01 15:21:56,962 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=32/_bert-base-uncased_50_epoch.pt
2025-03-01 15:22:09,958 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.047422136180102825
2025-03-01 15:22:22,153 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.0451990774134174
2025-03-01 15:22:34,950 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.0447058165135483
2025-03-01 15:22:47,481 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.043838667322415856
2025-03-01 15:22:58,992 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04676351690664887
2025-03-01 15:23:09,357 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.04726157618376116
2025-03-01 15:23:21,578 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.047503486069451484
2025-03-01 15:23:24,836 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=32/_bert-base-uncased_51_epoch.pt
2025-03-01 15:23:37,056 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.04771140767261386
2025-03-01 15:23:48,218 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.047824799781665206
2025-03-01 15:23:59,804 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.045507267319286865
2025-03-01 15:24:11,871 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.04494800650281831
2025-03-01 15:24:23,408 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04234729674644768
2025-03-01 15:24:37,576 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.04293946451507509
2025-03-01 15:24:49,844 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.04305512978296195
2025-03-01 15:24:52,872 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=32/_bert-base-uncased_52_epoch.pt
2025-03-01 15:25:05,195 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.0450373541098088
2025-03-01 15:25:17,126 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.04521612832322717
2025-03-01 15:25:30,004 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.04329211227595806
2025-03-01 15:25:41,425 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.04239665507338941
2025-03-01 15:25:54,756 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04333432455547154
2025-03-01 15:26:06,766 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.04284742746967822
2025-03-01 15:26:19,724 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.04307489248790911
2025-03-01 15:26:22,618 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=32/_bert-base-uncased_53_epoch.pt
2025-03-01 15:26:36,027 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.046972753433510664
2025-03-01 15:26:47,004 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.04324188798200339
2025-03-01 15:26:59,695 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.04313180590979755
2025-03-01 15:27:10,790 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.04524762702640146
2025-03-01 15:27:23,119 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.044324837308377024
2025-03-01 15:27:35,415 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.04682849628540377
2025-03-01 15:27:47,548 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.04778455558366009
2025-03-01 15:27:50,417 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=32/_bert-base-uncased_54_epoch.pt
2025-03-01 15:28:03,416 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.05542416521348059
2025-03-01 15:28:14,389 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.052701874542981385
2025-03-01 15:28:26,234 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.04893920097189645
2025-03-01 15:28:37,231 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.047284109645988795
2025-03-01 15:28:49,063 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04649043692275882
2025-03-01 15:29:02,336 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.046480812675630055
2025-03-01 15:29:14,105 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.04664337813322033
2025-03-01 15:29:17,809 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=32/_bert-base-uncased_55_epoch.pt
2025-03-01 15:29:30,716 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.04642126704566181
2025-03-01 15:29:42,552 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.04840039983391762
2025-03-01 15:29:55,362 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.0503990332596004
2025-03-01 15:30:07,013 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.04672377163078636
2025-03-01 15:30:17,831 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04727298357523978
2025-03-01 15:30:30,108 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.0483585386381795
2025-03-01 15:30:42,755 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.047825277929327316
2025-03-01 15:30:45,802 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=32/_bert-base-uncased_56_epoch.pt
2025-03-01 15:30:58,770 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.04776011924259364
2025-03-01 15:31:09,962 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.04807861086446792
2025-03-01 15:31:21,962 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.05018968611645202
2025-03-01 15:31:34,231 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.049434321490116416
2025-03-01 15:31:46,445 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04850280522368848
2025-03-01 15:31:57,913 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.048781220451928675
2025-03-01 15:32:10,100 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.04832647933757731
2025-03-01 15:32:11,820 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=32/_bert-base-uncased_57_epoch.pt
2025-03-01 15:32:24,977 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.04045392125844956
2025-03-01 15:32:37,185 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.03947852198034525
2025-03-01 15:32:48,433 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.04198198914527893
2025-03-01 15:33:00,786 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.04316281226929277
2025-03-01 15:33:14,123 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04538499465212226
2025-03-01 15:33:26,680 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.04351590177199493
2025-03-01 15:33:40,246 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.04346137343506728
2025-03-01 15:33:43,214 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=32/_bert-base-uncased_58_epoch.pt
2025-03-01 15:33:57,537 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.05067460983991623
2025-03-01 15:34:09,782 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.049269352271221575
2025-03-01 15:34:21,433 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.05188210505681733
2025-03-01 15:34:33,830 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.049880027491599324
2025-03-01 15:34:45,809 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04775666216388345
2025-03-01 15:34:58,197 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.04782520150765777
2025-03-01 15:35:10,648 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.04746835284334208
2025-03-01 15:35:13,023 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=32/_bert-base-uncased_59_epoch.pt
2025-03-01 15:35:24,834 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.040767420455813407
2025-03-01 15:35:37,338 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.04667856157757342
2025-03-01 15:35:48,251 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.05013333045256634
2025-03-01 15:35:59,332 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.05086052728584036
2025-03-01 15:36:10,910 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04916935596615076
2025-03-01 15:36:23,686 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.04824065922293812
2025-03-01 15:36:37,446 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.049087601002039655
2025-03-01 15:36:40,601 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=32/_bert-base-uncased_60_epoch.pt
2025-03-01 15:36:51,875 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.04722288008779287
2025-03-01 15:37:05,272 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.04589450489729643
2025-03-01 15:37:17,623 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.04477556454949081
2025-03-01 15:37:28,690 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.044328160968143494
2025-03-01 15:37:42,553 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04559959689155221
2025-03-01 15:37:54,557 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.046427609577464564
2025-03-01 15:38:07,591 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.04591438872739673
2025-03-01 15:38:10,220 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=32/_bert-base-uncased_61_epoch.pt
2025-03-01 15:38:22,066 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.04823936074972153
2025-03-01 15:38:33,572 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.043093025614507494
2025-03-01 15:38:44,276 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.04531802440372606
2025-03-01 15:38:57,071 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.047337661276105794
2025-03-01 15:39:10,701 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04765240118838847
2025-03-01 15:39:22,521 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.04738515967813631
2025-03-01 15:39:33,828 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.047724988790495054
2025-03-01 15:39:37,626 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=32/_bert-base-uncased_62_epoch.pt
2025-03-01 15:39:50,342 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.04565675747580826
2025-03-01 15:40:00,750 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.04494806609582156
2025-03-01 15:40:14,613 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.044526624493300915
2025-03-01 15:40:27,455 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.04401152309728786
2025-03-01 15:40:41,099 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04518008250743151
2025-03-01 15:40:51,522 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.044855500532624624
2025-03-01 15:41:03,803 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.04602687427374933
2025-03-01 15:41:06,813 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=32/_bert-base-uncased_63_epoch.pt
2025-03-01 15:41:19,274 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.05689805462025106
2025-03-01 15:41:32,818 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.050278338976204394
2025-03-01 15:41:45,596 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.04810247573380669
2025-03-01 15:41:58,398 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.0476919335895218
2025-03-01 15:42:11,339 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04884807833470404
2025-03-01 15:42:22,353 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.04813043505419046
2025-03-01 15:42:32,578 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.04771202994244439
2025-03-01 15:42:35,661 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=32/_bert-base-uncased_64_epoch.pt
2025-03-01 15:42:47,762 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.0520048881880939
2025-03-01 15:43:00,974 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.05211824120488018
2025-03-01 15:43:13,765 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.048301676443467535
2025-03-01 15:43:25,494 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.049079816474113615
2025-03-01 15:43:37,075 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04899709549732506
2025-03-01 15:43:49,168 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.048532932554371654
2025-03-01 15:44:01,096 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.048303247070206064
2025-03-01 15:44:04,438 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=32/_bert-base-uncased_65_epoch.pt
2025-03-01 15:44:16,406 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.0483804221265018
2025-03-01 15:44:28,067 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.047300479677505794
2025-03-01 15:44:41,746 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.044345635718976456
2025-03-01 15:44:54,220 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.04783606873825193
2025-03-01 15:45:05,418 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.046725891856476666
2025-03-01 15:45:16,859 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.04650430098020782
2025-03-01 15:45:30,133 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.04692519106902182
2025-03-01 15:45:33,196 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=32/_bert-base-uncased_66_epoch.pt
2025-03-01 15:45:45,922 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.04283974273130298
2025-03-01 15:45:57,347 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.048971361154690383
2025-03-01 15:46:09,424 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.05301294640327493
2025-03-01 15:46:20,535 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.04900950283044949
2025-03-01 15:46:33,741 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.047575527662411334
2025-03-01 15:46:48,089 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.046437720341297485
2025-03-01 15:46:59,825 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.047318690150443994
2025-03-01 15:47:02,743 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=32/_bert-base-uncased_67_epoch.pt
2025-03-01 15:47:16,027 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.04595395415090024
2025-03-01 15:47:28,572 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.050169784831814467
2025-03-01 15:47:40,506 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.04951035760653515
2025-03-01 15:47:52,973 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.0491983056650497
2025-03-01 15:48:03,527 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.047053174031898376
2025-03-01 15:48:15,106 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.04733388000167906
2025-03-01 15:48:26,954 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.04827948643027672
2025-03-01 15:48:29,405 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=32/_bert-base-uncased_68_epoch.pt
2025-03-01 15:48:43,400 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.04697232283651829
2025-03-01 15:48:56,415 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.05184338251128793
2025-03-01 15:49:06,514 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.04903775850931803
2025-03-01 15:49:19,514 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.04986858330667019
2025-03-01 15:49:32,750 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.047964455168694256
2025-03-01 15:49:45,020 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.048717614356428386
2025-03-01 15:49:55,385 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.04888327643275261
2025-03-01 15:49:58,846 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=32/_bert-base-uncased_69_epoch.pt
2025-03-01 15:50:13,447 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.044120280025526884
2025-03-01 15:50:24,862 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.047353177424520256
2025-03-01 15:50:37,736 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.044643291660274066
2025-03-01 15:50:51,274 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.04738189888885245
2025-03-01 15:51:02,251 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.048578337412327526
2025-03-01 15:51:13,657 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.048373011476360264
2025-03-01 15:51:25,143 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.04830156374456627
2025-03-01 15:51:28,246 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=32/_bert-base-uncased_70_epoch.pt
2025-03-01 15:51:40,701 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.05031014089472592
2025-03-01 15:51:50,633 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.05087813020218164
2025-03-01 15:52:03,334 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.047654247377067804
2025-03-01 15:52:16,360 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.04829829388763755
2025-03-01 15:52:28,191 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.047661214265972376
2025-03-01 15:52:40,246 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.04804159423802048
2025-03-01 15:52:50,778 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.0471713884267956
2025-03-01 15:52:55,021 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=32/_bert-base-uncased_71_epoch.pt
2025-03-01 15:53:07,480 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.044701676908880474
2025-03-01 15:53:19,456 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.04348741350695491
2025-03-01 15:53:32,152 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.047006566155080996
2025-03-01 15:53:44,454 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.04612819165922701
2025-03-01 15:53:56,398 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04645065209828317
2025-03-01 15:54:08,796 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.04621638919537266
2025-03-01 15:54:22,304 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.04571052112483553
2025-03-01 15:54:24,909 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=32/_bert-base-uncased_72_epoch.pt
2025-03-01 15:54:36,408 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.044198115402832625
2025-03-01 15:54:48,206 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.050034003355540334
2025-03-01 15:55:00,868 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.04733273672560851
2025-03-01 15:55:13,946 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.04712043127510697
2025-03-01 15:55:25,977 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04716363251209259
2025-03-01 15:55:38,617 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.04626566383522004
2025-03-01 15:55:50,585 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.04828992778036211
2025-03-01 15:55:54,210 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=32/_bert-base-uncased_73_epoch.pt
2025-03-01 15:56:05,512 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.05145760108716786
2025-03-01 15:56:17,669 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.04903888492844999
2025-03-01 15:56:29,171 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.04657659865915775
2025-03-01 15:56:42,755 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.04670326802879572
2025-03-01 15:56:54,705 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04749625517055392
2025-03-01 15:57:09,332 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.04699811776323865
2025-03-01 15:57:22,675 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.04713581115273493
2025-03-01 15:57:25,191 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=32/_bert-base-uncased_74_epoch.pt
2025-03-01 15:57:39,545 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.04553772006183863
2025-03-01 15:57:50,199 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.04438454802148044
2025-03-01 15:58:02,197 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.04489806578494608
2025-03-01 15:58:15,159 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.044398634671233596
2025-03-01 15:58:28,033 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04650818800553679
2025-03-01 15:58:38,025 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.04576746761643638
2025-03-01 15:58:50,404 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.045992129861510224
2025-03-01 15:58:53,427 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=32/_bert-base-uncased_75_epoch.pt
2025-03-01 15:59:05,965 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.048409445025026795
2025-03-01 15:59:17,965 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.053307889960706234
2025-03-01 15:59:29,803 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.05122801042161882
2025-03-01 15:59:42,167 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.04998877519974485
2025-03-01 15:59:55,471 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04998395124450326
2025-03-01 16:00:07,329 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.05090018290405472
2025-03-01 16:00:18,801 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.05060316253719585
2025-03-01 16:00:22,021 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=32/_bert-base-uncased_76_epoch.pt
2025-03-01 16:00:33,645 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.04824080895632506
2025-03-01 16:00:44,539 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.04185322152916342
2025-03-01 16:00:58,040 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.043731374635050696
2025-03-01 16:01:10,731 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.04550525667145848
2025-03-01 16:01:23,775 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04399298133328557
2025-03-01 16:01:36,443 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.04391704044149568
2025-03-01 16:01:47,146 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.04521936905969467
2025-03-01 16:01:50,026 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=32/_bert-base-uncased_77_epoch.pt
2025-03-01 16:02:04,572 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.043071484519168736
2025-03-01 16:02:17,719 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.04569307144265622
2025-03-01 16:02:30,002 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.04459767901959519
2025-03-01 16:02:42,437 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.04521032248158008
2025-03-01 16:02:55,500 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.0449072899017483
2025-03-01 16:03:06,280 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.04521474338447054
2025-03-01 16:03:18,814 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.046393050426351175
2025-03-01 16:03:21,880 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=32/_bert-base-uncased_78_epoch.pt
2025-03-01 16:03:34,760 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.044592638267204165
2025-03-01 16:03:45,948 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.05251986072398722
2025-03-01 16:03:59,084 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.05245547069547077
2025-03-01 16:04:11,486 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.05274438423803076
2025-03-01 16:04:23,816 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.05171008630655706
2025-03-01 16:04:35,848 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.05253924195809911
2025-03-01 16:04:46,502 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.05220575565472245
2025-03-01 16:04:49,715 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=32/_bert-base-uncased_79_epoch.pt
2025-03-01 16:05:01,679 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.04303863411769271
2025-03-01 16:05:13,941 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.043638288881629704
2025-03-01 16:05:27,432 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.044258511640752354
2025-03-01 16:05:38,114 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.045170895417686555
2025-03-01 16:05:49,941 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.045145184453576806
2025-03-01 16:06:02,452 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.04396477152282993
2025-03-01 16:06:14,927 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.04500824882249747
2025-03-01 16:06:17,262 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=32/_bert-base-uncased_80_epoch.pt
2025-03-01 16:06:28,376 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.041149424575269225
2025-03-01 16:06:43,195 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.04490709104575217
2025-03-01 16:06:55,419 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.042059368500486013
2025-03-01 16:07:07,227 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.04308808990754187
2025-03-01 16:07:18,966 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.0428816487826407
2025-03-01 16:07:32,217 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.04346078292777141
2025-03-01 16:07:44,096 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.04219194061921111
2025-03-01 16:07:46,744 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=32/_bert-base-uncased_81_epoch.pt
2025-03-01 16:07:59,201 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.0479832089971751
2025-03-01 16:08:10,379 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.044479417218826714
2025-03-01 16:08:24,055 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.041618401821081834
2025-03-01 16:08:36,537 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.04166262975195423
2025-03-01 16:08:49,588 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.043552391482517126
2025-03-01 16:09:01,828 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.04437882686809947
2025-03-01 16:09:13,120 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.04422287763362484
2025-03-01 16:09:16,009 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=32/_bert-base-uncased_82_epoch.pt
2025-03-01 16:09:28,380 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.05264291539788246
2025-03-01 16:09:41,160 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.04726367613766343
2025-03-01 16:09:53,958 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.045063502807170155
2025-03-01 16:10:06,504 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.04578573330072686
2025-03-01 16:10:18,562 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04724083944223821
2025-03-01 16:10:30,010 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.04669467309334626
2025-03-01 16:10:41,592 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.045867598608934454
2025-03-01 16:10:44,638 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=32/_bert-base-uncased_83_epoch.pt
2025-03-01 16:10:58,198 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.050983051350340244
2025-03-01 16:11:09,451 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.04948255005292594
2025-03-01 16:11:22,016 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.05081404065713287
2025-03-01 16:11:33,014 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.050364206719677895
2025-03-01 16:11:43,629 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.050724540036171675
2025-03-01 16:11:56,427 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.05117105993752678
2025-03-01 16:12:09,747 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.05083441805493619
2025-03-01 16:12:13,015 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=32/_bert-base-uncased_84_epoch.pt
2025-03-01 16:12:26,036 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.04554460956715047
2025-03-01 16:12:37,994 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.04354918226599693
2025-03-01 16:12:49,932 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.04603463138143222
2025-03-01 16:13:00,283 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.04654589423444122
2025-03-01 16:13:12,381 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.045292290998622774
2025-03-01 16:13:25,207 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.04517719737874965
2025-03-01 16:13:37,305 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.04493902380844312
2025-03-01 16:13:39,986 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=32/_bert-base-uncased_85_epoch.pt
2025-03-01 16:13:51,876 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.04228241592645645
2025-03-01 16:14:04,530 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.044979259744286536
2025-03-01 16:14:17,282 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.04757380912390848
2025-03-01 16:14:31,249 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.04807702815160155
2025-03-01 16:14:42,391 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.047476298687979576
2025-03-01 16:14:53,441 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.04619912328198552
2025-03-01 16:15:05,786 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.04534343246902738
2025-03-01 16:15:08,814 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=32/_bert-base-uncased_86_epoch.pt
2025-03-01 16:15:22,461 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.04453690517693758
2025-03-01 16:15:33,649 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.04603129616007209
2025-03-01 16:15:46,180 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.04550895628829797
2025-03-01 16:15:59,062 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.043610613932833074
2025-03-01 16:16:12,112 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04411313458345831
2025-03-01 16:16:25,248 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.04559558601273845
2025-03-01 16:16:38,603 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.04550679146445223
2025-03-01 16:16:41,707 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=32/_bert-base-uncased_87_epoch.pt
2025-03-01 16:16:53,038 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.05717298574745655
2025-03-01 16:17:05,108 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.05509876413270831
2025-03-01 16:17:19,272 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.051965416440119344
2025-03-01 16:17:30,772 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.05002377174096182
2025-03-01 16:17:42,978 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04868518388830125
2025-03-01 16:17:56,474 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.04870424543041736
2025-03-01 16:18:07,684 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.047818548438538395
2025-03-01 16:18:09,568 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=32/_bert-base-uncased_88_epoch.pt
2025-03-01 16:18:22,356 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.042923401948064564
2025-03-01 16:18:33,889 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.04458403768949211
2025-03-01 16:18:45,240 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.042693118238821624
2025-03-01 16:18:55,786 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.04515405454440043
2025-03-01 16:19:05,565 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04662255713716149
2025-03-01 16:19:17,361 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.04809386912578096
2025-03-01 16:19:31,722 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.04759780198468694
2025-03-01 16:19:35,119 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=32/_bert-base-uncased_89_epoch.pt
2025-03-01 16:19:47,853 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.053092001797631384
2025-03-01 16:20:01,545 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.0463548147585243
2025-03-01 16:20:12,686 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.0468436121630172
2025-03-01 16:20:26,372 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.04655305091291666
2025-03-01 16:20:37,140 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.0466524472553283
2025-03-01 16:20:49,939 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.04708744343370199
2025-03-01 16:21:01,401 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.04631870879259493
2025-03-01 16:21:04,976 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=32/_bert-base-uncased_90_epoch.pt
2025-03-01 16:21:18,013 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.05218466524966061
2025-03-01 16:21:30,171 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.04998449038248509
2025-03-01 16:21:44,445 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.048457880457863214
2025-03-01 16:21:56,269 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.047517760074697435
2025-03-01 16:22:06,302 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.047254390493035314
2025-03-01 16:22:19,331 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.04932783572003245
2025-03-01 16:22:31,386 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.04873666769292738
2025-03-01 16:22:35,029 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=32/_bert-base-uncased_91_epoch.pt
2025-03-01 16:22:48,173 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.04127555624581873
2025-03-01 16:22:58,981 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.04452871200628579
2025-03-01 16:23:11,746 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.046544410117591424
2025-03-01 16:23:23,764 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.04375268903095275
2025-03-01 16:23:35,850 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.044357868228107694
2025-03-01 16:23:49,628 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.04509710747903834
2025-03-01 16:24:01,390 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.045918215119412964
2025-03-01 16:24:04,566 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=32/_bert-base-uncased_92_epoch.pt
2025-03-01 16:24:18,099 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.05186436269432306
2025-03-01 16:24:31,314 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.0489296551560983
2025-03-01 16:24:44,579 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.04666599206005533
2025-03-01 16:24:56,292 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.04690972829703242
2025-03-01 16:25:08,926 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04678777627646923
2025-03-01 16:25:20,795 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.04747995991880695
2025-03-01 16:25:31,846 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.047606630683211346
2025-03-01 16:25:34,819 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=32/_bert-base-uncased_93_epoch.pt
2025-03-01 16:25:47,689 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.04421647624112666
2025-03-01 16:26:01,687 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.03965106168761849
2025-03-01 16:26:13,956 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.0428692625525097
2025-03-01 16:26:26,631 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.041553870623465626
2025-03-01 16:26:38,212 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.043814600072801115
2025-03-01 16:26:50,691 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.04505779248817513
2025-03-01 16:27:02,441 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.04703891952522099
2025-03-01 16:27:04,988 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=32/_bert-base-uncased_94_epoch.pt
2025-03-01 16:27:17,429 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.045088731404393914
2025-03-01 16:27:30,950 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.0442949621938169
2025-03-01 16:27:44,269 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.04212094185252984
2025-03-01 16:27:53,205 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.045251070079393683
2025-03-01 16:28:04,818 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04512589208781719
2025-03-01 16:28:16,079 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.04444900921856364
2025-03-01 16:28:29,760 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.04564619611150452
2025-03-01 16:28:32,688 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=32/_bert-base-uncased_95_epoch.pt
2025-03-01 16:28:44,270 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.04551021857187152
2025-03-01 16:28:56,890 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.05464140146505088
2025-03-01 16:29:09,597 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.04977207034826279
2025-03-01 16:29:20,956 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.05066060610115528
2025-03-01 16:29:34,693 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.05014375325292349
2025-03-01 16:29:45,889 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.049602974314863484
2025-03-01 16:29:58,602 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.04962707638208355
2025-03-01 16:30:01,355 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=32/_bert-base-uncased_96_epoch.pt
2025-03-01 16:30:11,987 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.039813916618004444
2025-03-01 16:30:24,956 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.042476972681470215
2025-03-01 16:30:36,725 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.04518421239530047
2025-03-01 16:30:48,549 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.04542769615072757
2025-03-01 16:31:00,412 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.045771353589370845
2025-03-01 16:31:10,258 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.046297154459171
2025-03-01 16:31:25,090 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.04589743639475533
2025-03-01 16:31:28,707 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=32/_bert-base-uncased_97_epoch.pt
2025-03-01 16:31:41,567 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.040518816513940695
2025-03-01 16:31:54,516 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.04380192595999688
2025-03-01 16:32:04,990 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.04602941015424828
2025-03-01 16:32:18,619 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.04668226332869381
2025-03-01 16:32:29,454 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.045469876863062385
2025-03-01 16:32:40,351 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.045330531111297506
2025-03-01 16:32:53,629 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.045634801313281056
2025-03-01 16:32:56,109 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=32/_bert-base-uncased_98_epoch.pt
2025-03-01 16:33:07,222 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.048628730792552234
2025-03-01 16:33:18,053 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.046360975387506186
2025-03-01 16:33:31,071 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.04689611135981977
2025-03-01 16:33:44,789 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.0476718598860316
2025-03-01 16:33:56,708 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04768739546649158
2025-03-01 16:34:09,424 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.046201524992162984
2025-03-01 16:34:20,815 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.046621101722121236
2025-03-01 16:34:24,031 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=32/_bert-base-uncased_99_epoch.pt
2025-03-01 16:34:37,648 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.05182184465229511
2025-03-01 16:34:48,761 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.04890332065988332
2025-03-01 16:35:00,673 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.04834200151575108
2025-03-01 16:35:13,575 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.04568475829437375
2025-03-01 16:35:26,554 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04390355738811195
2025-03-01 16:35:37,616 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.04425754384913792
2025-03-01 16:35:49,044 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.04260623026639223
2025-03-01 16:35:51,495 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=32/_bert-base-uncased_100_epoch.pt
2025-03-01 16:36:05,445 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.034519475884735584
2025-03-01 16:36:18,203 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.041957067255862054
2025-03-01 16:36:29,665 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.04454962161059181
2025-03-01 16:36:41,721 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.04646781836636364
2025-03-01 16:36:52,728 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04670335713773966
2025-03-01 16:37:05,613 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.04676196021027863
2025-03-01 16:37:18,149 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.04715122881877635
2025-03-01 16:37:20,234 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=32/_bert-base-uncased_101_epoch.pt
2025-03-01 16:37:33,622 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.032780233724042776
2025-03-01 16:37:44,389 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.03912658921908587
2025-03-01 16:37:57,674 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.04228589811051885
2025-03-01 16:38:08,518 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.044207657885272054
2025-03-01 16:38:23,153 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04441542521119118
2025-03-01 16:38:35,571 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.043662623649773497
2025-03-01 16:38:46,611 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.04396406313670533
2025-03-01 16:38:49,149 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=32/_bert-base-uncased_102_epoch.pt
2025-03-01 16:39:01,595 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.0486471644602716
2025-03-01 16:39:13,150 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.04499966469593346
2025-03-01 16:39:25,522 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.04708122469795247
2025-03-01 16:39:37,447 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.047895460098516196
2025-03-01 16:39:49,249 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.047705274634063245
2025-03-01 16:40:00,998 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.046418554599707326
2025-03-01 16:40:13,688 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.04791025268579168
2025-03-01 16:40:17,756 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=32/_bert-base-uncased_103_epoch.pt
2025-03-01 16:40:30,657 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.04250291739590466
2025-03-01 16:40:42,958 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.045738178049214186
2025-03-01 16:40:55,721 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.04429224341486891
2025-03-01 16:41:06,918 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.04472497175447643
2025-03-01 16:41:19,191 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.044374774824827906
2025-03-01 16:41:31,736 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.0422079926977555
2025-03-01 16:41:43,849 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.04311986470461956
2025-03-01 16:41:47,025 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=32/_bert-base-uncased_104_epoch.pt
2025-03-01 16:42:00,800 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.0453197633381933
2025-03-01 16:42:13,089 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.04567704973742366
2025-03-01 16:42:25,093 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.043455721468975146
2025-03-01 16:42:36,712 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.045272875658702104
2025-03-01 16:42:47,475 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.043269668808206915
2025-03-01 16:42:57,498 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.0426470499796172
2025-03-01 16:43:10,042 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.043165638956374354
2025-03-01 16:43:12,610 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=32/_bert-base-uncased_105_epoch.pt
2025-03-01 16:43:25,391 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.059936681063845754
2025-03-01 16:43:38,775 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.051749533996917306
2025-03-01 16:43:51,412 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.050442484735200806
2025-03-01 16:44:01,641 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.04665300569031387
2025-03-01 16:44:14,675 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04614251955412328
2025-03-01 16:44:26,379 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.04776503896185507
2025-03-01 16:44:37,203 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.04662361916686807
2025-03-01 16:44:39,996 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=32/_bert-base-uncased_106_epoch.pt
2025-03-01 16:44:52,791 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.05107428878545761
2025-03-01 16:45:04,094 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.047617131192237136
2025-03-01 16:45:16,432 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.04599813874810934
2025-03-01 16:45:27,728 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.04498057203600183
2025-03-01 16:45:38,942 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04477026961743832
2025-03-01 16:45:51,630 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.04488696013577283
2025-03-01 16:46:04,396 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.04477590449553515
2025-03-01 16:46:07,453 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=32/_bert-base-uncased_107_epoch.pt
2025-03-01 16:46:21,744 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.05229901475831866
2025-03-01 16:46:34,922 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.046974070672877136
2025-03-01 16:46:47,089 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.04649822213687003
2025-03-01 16:46:59,081 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.04752857645507902
2025-03-01 16:47:10,150 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04677135203033686
2025-03-01 16:47:22,339 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.04600380894262344
2025-03-01 16:47:35,224 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.04508064169702786
2025-03-01 16:47:38,200 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=32/_bert-base-uncased_108_epoch.pt
2025-03-01 16:47:50,317 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.0446321552619338
2025-03-01 16:48:03,052 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.04436346499715001
2025-03-01 16:48:14,627 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.044993386541803675
2025-03-01 16:48:24,206 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.04566664405865595
2025-03-01 16:48:37,328 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04745414302684367
2025-03-01 16:48:49,911 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.04587092340613405
2025-03-01 16:49:01,687 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.04584326311014593
2025-03-01 16:49:04,800 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=32/_bert-base-uncased_109_epoch.pt
2025-03-01 16:49:17,973 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.04827269101515412
2025-03-01 16:49:28,680 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.048034566245041786
2025-03-01 16:49:39,446 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.04782227117878695
2025-03-01 16:49:50,450 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.04856934086419642
2025-03-01 16:50:03,048 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04853748618625105
2025-03-01 16:50:15,569 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.0479278471864139
2025-03-01 16:50:28,775 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.04743198599400265
2025-03-01 16:50:32,285 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=32/_bert-base-uncased_110_epoch.pt
2025-03-01 16:50:45,179 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.050045176036655904
2025-03-01 16:50:55,617 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.04803643610794097
2025-03-01 16:51:07,635 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.04819408540303508
2025-03-01 16:51:20,111 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.04578193633351475
2025-03-01 16:51:33,629 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04636140340939164
2025-03-01 16:51:47,319 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.04512656888303657
2025-03-01 16:51:59,757 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.044701075939727684
2025-03-01 16:52:02,173 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=32/_bert-base-uncased_111_epoch.pt
2025-03-01 16:52:14,031 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.042618549149483445
2025-03-01 16:52:26,046 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.04380162940360606
2025-03-01 16:52:41,345 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.04440008915650348
2025-03-01 16:52:54,737 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.04432343151420355
2025-03-01 16:53:07,782 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.045984582789242265
2025-03-01 16:53:19,054 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.046167264715768395
2025-03-01 16:53:30,124 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.04633679605488266
2025-03-01 16:53:33,126 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=32/_bert-base-uncased_112_epoch.pt
2025-03-01 16:53:44,911 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.04927619444206357
2025-03-01 16:53:57,415 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.04698206502944231
2025-03-01 16:54:09,749 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.0458118282413731
2025-03-01 16:54:23,296 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.045816074206959455
2025-03-01 16:54:35,520 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04460311012342572
2025-03-01 16:54:46,559 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.04435431998378287
2025-03-01 16:54:58,248 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.0445421680342406
2025-03-01 16:55:00,818 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=32/_bert-base-uncased_113_epoch.pt
2025-03-01 16:55:13,727 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.04229772472754121
2025-03-01 16:55:24,211 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.04803754375316203
2025-03-01 16:55:34,123 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.04629213886025051
2025-03-01 16:55:46,817 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.04490116400411352
2025-03-01 16:55:59,188 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04489553972147405
2025-03-01 16:56:11,280 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.04426115390378982
2025-03-01 16:56:24,996 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.04414135197044483
2025-03-01 16:56:28,436 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=32/_bert-base-uncased_114_epoch.pt
2025-03-01 16:56:42,940 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.03949118978343904
2025-03-01 16:56:58,107 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.04251754214055836
2025-03-01 16:57:09,634 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.04161309185437858
2025-03-01 16:57:22,639 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.0415840148110874
2025-03-01 16:57:34,980 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.0403659625723958
2025-03-01 16:57:46,506 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.04132244719658047
2025-03-01 16:57:58,744 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.042558358782636266
2025-03-01 16:58:00,910 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=32/_bert-base-uncased_115_epoch.pt
2025-03-01 16:58:14,488 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.044838270731270315
2025-03-01 16:58:26,586 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.04703681105747819
2025-03-01 16:58:38,917 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.04442282092447082
2025-03-01 16:58:50,245 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.043999814335256814
2025-03-01 16:59:01,453 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.0447572666592896
2025-03-01 16:59:12,941 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.04514586437338342
2025-03-01 16:59:26,656 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.04689592019255672
2025-03-01 16:59:29,660 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=32/_bert-base-uncased_116_epoch.pt
2025-03-01 16:59:43,638 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.045396523410454394
2025-03-01 16:59:54,338 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.04341815104708076
2025-03-01 17:00:05,032 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.042392616237824164
2025-03-01 17:00:16,698 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.042258214997127654
2025-03-01 17:00:29,311 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04308755217120051
2025-03-01 17:00:40,726 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.043822150638637446
2025-03-01 17:00:54,405 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.04407188408741994
2025-03-01 17:00:57,485 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=32/_bert-base-uncased_117_epoch.pt
2025-03-01 17:01:09,700 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.04573238966986537
2025-03-01 17:01:23,106 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.041842113924212755
2025-03-01 17:01:36,004 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.04387623444199562
2025-03-01 17:01:47,900 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.04680856035556644
2025-03-01 17:01:59,913 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.045953789344057444
2025-03-01 17:02:09,941 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.045100341411307455
2025-03-01 17:02:23,311 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.044741243117355875
2025-03-01 17:02:26,224 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=32/_bert-base-uncased_118_epoch.pt
2025-03-01 17:02:38,303 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.04209047518670559
2025-03-01 17:02:49,065 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.040464849257841704
2025-03-01 17:03:00,876 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.042949488510688145
2025-03-01 17:03:12,282 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.04371601227903739
2025-03-01 17:03:25,381 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04458793670870364
2025-03-01 17:03:38,165 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.04530622350672881
2025-03-01 17:03:50,432 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.046552870249641796
2025-03-01 17:03:53,689 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=32/_bert-base-uncased_119_epoch.pt
2025-03-01 17:04:05,632 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.04883526712656021
2025-03-01 17:04:17,818 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.04501674238126725
2025-03-01 17:04:30,000 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.043383964166666074
2025-03-01 17:04:40,773 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.044546192465350035
2025-03-01 17:04:54,502 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04419521448202431
2025-03-01 17:05:06,966 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.045489697996526954
2025-03-01 17:05:19,373 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.04627840211614966
2025-03-01 17:05:22,243 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=32/_bert-base-uncased_120_epoch.pt
2025-03-01 17:05:34,699 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.04533128552138806
2025-03-01 17:05:46,026 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.0459560077637434
2025-03-01 17:05:58,640 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.04427334444286923
2025-03-01 17:06:10,930 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.046292371861636636
2025-03-01 17:06:22,679 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04751993535086513
2025-03-01 17:06:36,562 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.04761692952209463
2025-03-01 17:06:48,423 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.04636764418599861
2025-03-01 17:06:50,275 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=32/_bert-base-uncased_121_epoch.pt
2025-03-01 17:07:01,975 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.04947328488342464
2025-03-01 17:07:13,488 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.047377104545012115
2025-03-01 17:07:25,292 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.045649649171779556
2025-03-01 17:07:36,532 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.046816281788051126
2025-03-01 17:07:48,857 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04780299680307507
2025-03-01 17:08:03,002 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.04628171044557045
2025-03-01 17:08:15,208 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.04688217403101069
2025-03-01 17:08:17,529 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=32/_bert-base-uncased_122_epoch.pt
2025-03-01 17:08:29,866 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.04345886190421879
2025-03-01 17:08:42,888 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.04616934573277831
2025-03-01 17:08:53,715 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.04476636396721005
2025-03-01 17:09:06,974 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.043935529619921
2025-03-01 17:09:18,554 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04529302392154932
2025-03-01 17:09:31,156 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.046972305475113295
2025-03-01 17:09:43,661 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.04587004525986101
2025-03-01 17:09:46,190 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=32/_bert-base-uncased_123_epoch.pt
2025-03-01 17:09:58,964 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.03776071490719914
2025-03-01 17:10:10,852 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.04797980282455683
2025-03-01 17:10:23,717 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.045948631642386314
2025-03-01 17:10:36,114 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.04579916005022824
2025-03-01 17:10:47,773 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.043496359176933765
2025-03-01 17:10:59,519 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.04407748041364054
2025-03-01 17:11:13,686 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.04494041208443897
2025-03-01 17:11:17,444 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=32/_bert-base-uncased_124_epoch.pt
2025-03-01 17:11:28,968 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.04757582750171423
2025-03-01 17:11:41,193 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.042996479640714826
2025-03-01 17:11:53,464 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.04335697049585482
2025-03-01 17:12:06,778 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.043603034096304324
2025-03-01 17:12:18,856 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04128142627887428
2025-03-01 17:12:32,065 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.043680763733573255
2025-03-01 17:12:44,407 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.044139620235988074
2025-03-01 17:12:47,311 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=32/_bert-base-uncased_125_epoch.pt
2025-03-01 17:12:59,538 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.048313901340588924
2025-03-01 17:13:11,337 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.048000677255913615
2025-03-01 17:13:24,471 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.048334621746713914
2025-03-01 17:13:36,373 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.04613039398100227
2025-03-01 17:13:48,489 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04706991270184517
2025-03-01 17:14:02,899 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.04695122595876455
2025-03-01 17:14:15,416 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.04837536473891565
2025-03-01 17:14:19,092 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=32/_bert-base-uncased_126_epoch.pt
2025-03-01 17:14:32,593 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.04083536262623966
2025-03-01 17:14:46,545 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.04243612545542419
2025-03-01 17:14:57,063 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.04239692445844412
2025-03-01 17:15:09,085 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.044477208983153105
2025-03-01 17:15:22,484 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04512415092438459
2025-03-01 17:15:35,272 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.04617749131284654
2025-03-01 17:15:46,996 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.04557740440193032
2025-03-01 17:15:49,416 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=32/_bert-base-uncased_127_epoch.pt
2025-03-01 17:16:04,114 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.05231964052654803
2025-03-01 17:16:16,317 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.04417495690286159
2025-03-01 17:16:27,832 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.04565455258513491
2025-03-01 17:16:39,847 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.045238090760540216
2025-03-01 17:16:51,645 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04596071762032807
2025-03-01 17:17:02,760 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.04608854419396569
2025-03-01 17:17:17,117 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.04730734718032181
2025-03-01 17:17:19,712 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=32/_bert-base-uncased_128_epoch.pt
2025-03-01 17:17:33,603 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.04456308810040355
2025-03-01 17:17:45,860 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.045107228611595926
2025-03-01 17:17:57,545 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.04437947190987567
2025-03-01 17:18:09,596 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.04631626415066421
2025-03-01 17:18:21,331 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04723951429128647
2025-03-01 17:18:35,017 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.04693158421820651
2025-03-01 17:18:48,845 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.04722132777928242
2025-03-01 17:18:51,492 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=32/_bert-base-uncased_129_epoch.pt
2025-03-01 17:19:03,735 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.053195701958611605
2025-03-01 17:19:16,160 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.052357085049152374
2025-03-01 17:19:29,078 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.051431566632042326
2025-03-01 17:19:42,233 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.05061967051587999
2025-03-01 17:19:53,794 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.047731390157714486
2025-03-01 17:20:06,211 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.04749727494393786
2025-03-01 17:20:19,566 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.047095995848732336
2025-03-01 17:20:21,708 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=32/_bert-base-uncased_130_epoch.pt
2025-03-01 17:20:34,770 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.059131323080509904
2025-03-01 17:20:46,615 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.05133662330918014
2025-03-01 17:20:58,640 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.046651297140245636
2025-03-01 17:21:10,300 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.046256360539700836
2025-03-01 17:21:22,797 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04562445391900837
2025-03-01 17:21:35,488 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.04583011552070578
2025-03-01 17:21:48,996 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.044546237634494903
2025-03-01 17:21:52,154 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=32/_bert-base-uncased_131_epoch.pt
2025-03-01 17:22:05,989 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.04279708103276789
2025-03-01 17:22:18,006 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.04237357247620821
2025-03-01 17:22:29,878 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.04017733902049561
2025-03-01 17:22:42,508 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.041651786281727256
2025-03-01 17:22:56,661 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04428089397959411
2025-03-01 17:23:10,254 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.04362912057743718
2025-03-01 17:23:21,909 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.046583331595840195
2025-03-01 17:23:24,368 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=32/_bert-base-uncased_132_epoch.pt
2025-03-01 17:23:34,844 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.04031197922304273
2025-03-01 17:23:46,509 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.04323216981720179
2025-03-01 17:23:59,208 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.0443917213473469
2025-03-01 17:24:11,712 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.044658854696899654
2025-03-01 17:24:22,820 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04582921726629138
2025-03-01 17:24:35,708 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.046599807171151045
2025-03-01 17:24:48,514 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.04669742245626237
2025-03-01 17:24:50,935 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=32/_bert-base-uncased_133_epoch.pt
2025-03-01 17:25:02,599 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.04685508473776281
2025-03-01 17:25:15,814 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.04930006433278322
2025-03-01 17:25:26,698 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.0499762451586624
2025-03-01 17:25:39,830 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.050023284822236745
2025-03-01 17:25:50,994 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.050638801278546454
2025-03-01 17:26:03,611 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.050356608202370506
2025-03-01 17:26:16,111 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.04901633752243859
2025-03-01 17:26:18,627 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=32/_bert-base-uncased_134_epoch.pt
2025-03-01 17:26:31,081 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.04817740335129202
2025-03-01 17:26:42,494 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.047110974229872225
2025-03-01 17:26:54,873 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.048153806120778124
2025-03-01 17:27:08,397 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.04796578960958868
2025-03-01 17:27:22,157 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.049626701371744276
2025-03-01 17:27:35,232 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.050416183184521894
2025-03-01 17:27:47,523 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.0508311841437327
2025-03-01 17:27:48,989 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=32/_bert-base-uncased_135_epoch.pt
2025-03-01 17:28:00,864 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.03805196522735059
2025-03-01 17:28:12,211 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.04564826083369553
2025-03-01 17:28:25,573 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.05026708164562781
2025-03-01 17:28:39,136 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.047878864814993
2025-03-01 17:28:50,910 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.0495208761934191
2025-03-01 17:29:03,142 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.04958948392110566
2025-03-01 17:29:15,445 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.05085115737414786
2025-03-01 17:29:17,334 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=32/_bert-base-uncased_136_epoch.pt
2025-03-01 17:29:31,205 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.04463106095790863
2025-03-01 17:29:42,387 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.042764154449105266
2025-03-01 17:29:55,964 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.04430193821899593
2025-03-01 17:30:06,475 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.046221448329742995
2025-03-01 17:30:18,947 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.047178039867430924
2025-03-01 17:30:30,570 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.046606285582917434
2025-03-01 17:30:42,124 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.04707660553976893
2025-03-01 17:30:45,372 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=32/_bert-base-uncased_137_epoch.pt
2025-03-01 17:30:59,429 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.03297966215759516
2025-03-01 17:31:11,920 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.04256311310455203
2025-03-01 17:31:23,437 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.041720077659313876
2025-03-01 17:31:34,155 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.045364124432671814
2025-03-01 17:31:46,250 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04529802707023919
2025-03-01 17:31:58,626 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.047111903076681
2025-03-01 17:32:11,039 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.04658167081485901
2025-03-01 17:32:14,205 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=32/_bert-base-uncased_138_epoch.pt
2025-03-01 17:32:27,093 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.05374966580420733
2025-03-01 17:32:37,971 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.05076758018694818
2025-03-01 17:32:51,375 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.051947569691886505
2025-03-01 17:33:03,966 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.05121846015099436
2025-03-01 17:33:15,700 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.05031581031158566
2025-03-01 17:33:28,111 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.049014701740816236
2025-03-01 17:33:40,064 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.04852834168289389
2025-03-01 17:33:43,119 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=32/_bert-base-uncased_139_epoch.pt
2025-03-01 17:33:55,510 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.048496427666395905
2025-03-01 17:34:06,528 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.04793218944687396
2025-03-01 17:34:19,316 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.045636577423041065
2025-03-01 17:34:33,488 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.04564056234667078
2025-03-01 17:34:44,879 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.0432808216381818
2025-03-01 17:34:57,750 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.045595158458066484
2025-03-01 17:35:09,889 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.046358689765578935
2025-03-01 17:35:13,524 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=32/_bert-base-uncased_140_epoch.pt
2025-03-01 17:35:26,887 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.04773201597854495
2025-03-01 17:35:38,973 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.04553653441835195
2025-03-01 17:35:51,480 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.04592602619280418
2025-03-01 17:36:04,066 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.04636992537416518
2025-03-01 17:36:15,569 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.046992695108056065
2025-03-01 17:36:28,162 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.046343092884247504
2025-03-01 17:36:40,680 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.04781906649337283
2025-03-01 17:36:42,966 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=32/_bert-base-uncased_141_epoch.pt
2025-03-01 17:36:56,136 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.044285756442695855
2025-03-01 17:37:08,172 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.04657786430325359
2025-03-01 17:37:19,497 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.04651053656513492
2025-03-01 17:37:32,130 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.04456823962973431
2025-03-01 17:37:44,132 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04436104005202651
2025-03-01 17:37:56,879 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.0448838253505528
2025-03-01 17:38:10,360 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.044469185965135695
2025-03-01 17:38:12,845 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=32/_bert-base-uncased_142_epoch.pt
2025-03-01 17:38:27,652 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.035255025839433074
2025-03-01 17:38:38,570 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.038157994765788315
2025-03-01 17:38:50,746 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.038569999672472474
2025-03-01 17:39:04,617 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.041146099613979456
2025-03-01 17:39:16,894 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04131693631410599
2025-03-01 17:39:28,341 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.041829721978865565
2025-03-01 17:39:40,502 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.04186864515899547
2025-03-01 17:39:42,397 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=32/_bert-base-uncased_143_epoch.pt
2025-03-01 17:39:56,239 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.041355530917644504
2025-03-01 17:40:08,425 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.04815124233718961
2025-03-01 17:40:20,027 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.047712856360400714
2025-03-01 17:40:32,931 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.04572931546717882
2025-03-01 17:40:45,914 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.044560587853193286
2025-03-01 17:40:57,856 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.04552514924822996
2025-03-01 17:41:09,306 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.04572780361798193
2025-03-01 17:41:12,841 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=32/_bert-base-uncased_144_epoch.pt
2025-03-01 17:41:25,988 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.040328398579731584
2025-03-01 17:41:36,985 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.043493194482289256
2025-03-01 17:41:48,938 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.04395006075501442
2025-03-01 17:42:01,487 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.047005816840101036
2025-03-01 17:42:14,024 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.0460959619935602
2025-03-01 17:42:26,492 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.04588036174730708
2025-03-01 17:42:40,064 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.04521218361998243
2025-03-01 17:42:42,222 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=32/_bert-base-uncased_145_epoch.pt
2025-03-01 17:42:55,574 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.05248149293474853
2025-03-01 17:43:08,294 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.050284999306313694
2025-03-01 17:43:20,813 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.04953873809427023
2025-03-01 17:43:33,926 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.046693863929249346
2025-03-01 17:43:46,219 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04675526554696262
2025-03-01 17:43:56,889 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.04528865267833074
2025-03-01 17:44:10,005 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.04543316260512386
2025-03-01 17:44:12,807 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=32/_bert-base-uncased_146_epoch.pt
2025-03-01 17:44:25,630 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.04200608255341649
2025-03-01 17:44:38,722 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.042635749303735794
2025-03-01 17:44:50,374 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.04627805799245834
2025-03-01 17:45:01,713 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.04528137043816969
2025-03-01 17:45:13,739 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04626050865277648
2025-03-01 17:49:39,648 : 916236790.py : __call__ : INFO : loaded dataset, sample = 1158
2025-03-01 17:49:39,649 : 1341934384.py : train_scp : INFO : 109483778
2025-03-01 17:49:42,180 : 1341934384.py : train_scp : INFO : sample = 160, loss = 0.23174543529748917
2025-03-01 17:49:44,433 : 1341934384.py : train_scp : INFO : sample = 320, loss = 0.21859089136123658
2025-03-01 17:49:46,444 : 1341934384.py : train_scp : INFO : sample = 480, loss = 0.2123940519988537
2025-03-01 17:49:49,053 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.2092980680987239
2025-03-01 17:49:51,353 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.20743851661682128
2025-03-01 17:49:53,948 : 1341934384.py : train_scp : INFO : sample = 960, loss = 0.20619866823156674
2025-03-01 17:49:56,014 : 1341934384.py : train_scp : INFO : sample = 1120, loss = 0.20531319230794906
2025-03-01 17:49:58,070 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.20464895954355597
2025-03-01 17:50:00,277 : 1341934384.py : train_scp : INFO : sample = 1440, loss = 0.20413243803713058
2025-03-01 17:50:02,838 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.20371875166893005
2025-03-01 17:50:05,191 : 1341934384.py : train_scp : INFO : sample = 1760, loss = 0.2033804095604203
2025-03-01 17:50:07,682 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.20309882989774147
2025-03-01 17:50:09,772 : 1341934384.py : train_scp : INFO : sample = 2080, loss = 0.2028603826004725
2025-03-01 17:50:12,132 : 1341934384.py : train_scp : INFO : sample = 2240, loss = 0.2026560207030603
2025-03-01 17:50:14,803 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.20247633417447408
2025-03-01 17:50:16,749 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.20231993845663965
2025-03-01 17:50:18,962 : 1341934384.py : train_scp : INFO : sample = 2720, loss = 0.20218323328915763
2025-03-01 17:50:21,032 : 1341934384.py : train_scp : INFO : sample = 2880, loss = 0.20206227153539658
2025-03-01 17:50:23,329 : 1341934384.py : train_scp : INFO : sample = 3040, loss = 0.20195340557317984
2025-03-01 17:50:25,520 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.20185531858354808
2025-03-01 17:50:27,998 : 1341934384.py : train_scp : INFO : sample = 3360, loss = 0.20176698604509943
2025-03-01 17:50:30,551 : 1341934384.py : train_scp : INFO : sample = 3520, loss = 0.20168834094974128
2025-03-01 17:50:32,646 : 1341934384.py : train_scp : INFO : sample = 3680, loss = 0.2016153059899807
2025-03-01 17:50:34,927 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.20154811094204586
2025-03-01 17:50:37,550 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.20148628655076026
2025-03-01 17:50:40,003 : 1341934384.py : train_scp : INFO : sample = 4160, loss = 0.2014289652212308
2025-03-01 17:50:42,557 : 1341934384.py : train_scp : INFO : sample = 4320, loss = 0.20137599060932795
2025-03-01 17:50:45,042 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.20132688635161944
2025-03-01 17:50:46,839 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_1_epoch.pt
2025-03-01 17:50:49,606 : 1341934384.py : train_scp : INFO : sample = 160, loss = 0.19999948292970657
2025-03-01 17:50:51,726 : 1341934384.py : train_scp : INFO : sample = 320, loss = 0.20000007823109628
2025-03-01 17:50:53,936 : 1341934384.py : train_scp : INFO : sample = 480, loss = 0.2000005230307579
2025-03-01 17:50:56,320 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.20000195372849702
2025-03-01 17:50:58,740 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.20000153422355652
2025-03-01 17:51:01,027 : 1341934384.py : train_scp : INFO : sample = 960, loss = 0.19999868062635262
2025-03-01 17:51:03,364 : 1341934384.py : train_scp : INFO : sample = 1120, loss = 0.19999940331493105
2025-03-01 17:51:05,665 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.19999945322051643
2025-03-01 17:51:07,818 : 1341934384.py : train_scp : INFO : sample = 1440, loss = 0.19999945726659563
2025-03-01 17:51:10,451 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.19999943874776363
2025-03-01 17:51:12,688 : 1341934384.py : train_scp : INFO : sample = 1760, loss = 0.19999953698028217
2025-03-01 17:51:15,536 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.1999996963267525
2025-03-01 17:51:17,850 : 1341934384.py : train_scp : INFO : sample = 2080, loss = 0.19999976937587444
2025-03-01 17:51:20,068 : 1341934384.py : train_scp : INFO : sample = 2240, loss = 0.199999858971153
2025-03-01 17:51:22,130 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.19999990622202554
2025-03-01 17:51:24,475 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.19999991692602634
2025-03-01 17:51:26,906 : 1341934384.py : train_scp : INFO : sample = 2720, loss = 0.19999988298205768
2025-03-01 17:51:29,382 : 1341934384.py : train_scp : INFO : sample = 2880, loss = 0.2000002634194162
2025-03-01 17:51:31,076 : 1341934384.py : train_scp : INFO : sample = 3040, loss = 0.19999993635635627
2025-03-01 17:51:33,062 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.2000010572373867
2025-03-01 17:51:35,421 : 1341934384.py : train_scp : INFO : sample = 3360, loss = 0.2000010558891864
2025-03-01 17:51:37,451 : 1341934384.py : train_scp : INFO : sample = 3520, loss = 0.20000056692145088
2025-03-01 17:51:39,733 : 1341934384.py : train_scp : INFO : sample = 3680, loss = 0.1999981881159803
2025-03-01 17:51:42,239 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.19999728668481112
2025-03-01 17:51:45,012 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.1999986641407013
2025-03-01 17:51:47,317 : 1341934384.py : train_scp : INFO : sample = 4160, loss = 0.1999990534897034
2025-03-01 17:51:49,626 : 1341934384.py : train_scp : INFO : sample = 4320, loss = 0.19999825982032
2025-03-01 17:51:51,840 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.19999847279063293
2025-03-01 17:51:53,854 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_2_epoch.pt
2025-03-01 17:51:56,836 : 1341934384.py : train_scp : INFO : sample = 160, loss = 0.19998912438750266
2025-03-01 17:51:58,838 : 1341934384.py : train_scp : INFO : sample = 320, loss = 0.19999400861561298
2025-03-01 17:52:00,637 : 1341934384.py : train_scp : INFO : sample = 480, loss = 0.1999935659269492
2025-03-01 17:52:02,932 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.19998850021511316
2025-03-01 17:52:05,320 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.20006998524069786
2025-03-01 17:52:07,979 : 1341934384.py : train_scp : INFO : sample = 960, loss = 0.20005835766593616
2025-03-01 17:52:10,093 : 1341934384.py : train_scp : INFO : sample = 1120, loss = 0.2000500224530697
2025-03-01 17:52:12,453 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.20004373593255878
2025-03-01 17:52:14,804 : 1341934384.py : train_scp : INFO : sample = 1440, loss = 0.2000389922824171
2025-03-01 17:52:17,324 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.20003508366644382
2025-03-01 17:52:19,464 : 1341934384.py : train_scp : INFO : sample = 1760, loss = 0.20003187520937485
2025-03-01 17:52:21,768 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.2000292188798388
2025-03-01 17:52:23,929 : 1341934384.py : train_scp : INFO : sample = 2080, loss = 0.2000269602697629
2025-03-01 17:52:26,593 : 1341934384.py : train_scp : INFO : sample = 2240, loss = 0.2000249951013497
2025-03-01 17:52:29,366 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.20002323428789776
2025-03-01 17:52:31,782 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.20002178926952183
2025-03-01 17:52:34,047 : 1341934384.py : train_scp : INFO : sample = 2720, loss = 0.20002048098865677
2025-03-01 17:52:35,658 : 1341934384.py : train_scp : INFO : sample = 2880, loss = 0.20001936327252123
2025-03-01 17:52:37,934 : 1341934384.py : train_scp : INFO : sample = 3040, loss = 0.200018360936328
2025-03-01 17:52:40,206 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.20001743629574775
2025-03-01 17:52:42,420 : 1341934384.py : train_scp : INFO : sample = 3360, loss = 0.2000166244804859
2025-03-01 17:52:44,517 : 1341934384.py : train_scp : INFO : sample = 3520, loss = 0.2000158360397274
2025-03-01 17:52:46,833 : 1341934384.py : train_scp : INFO : sample = 3680, loss = 0.2000151462852955
2025-03-01 17:52:49,545 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.20001452872529626
2025-03-01 17:52:52,199 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.20001395025849342
2025-03-01 17:52:54,679 : 1341934384.py : train_scp : INFO : sample = 4160, loss = 0.2000134309610495
2025-03-01 17:52:57,415 : 1341934384.py : train_scp : INFO : sample = 4320, loss = 0.20001291312553265
2025-03-01 17:52:59,640 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.20001245763685022
2025-03-01 17:53:01,582 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_3_epoch.pt
2025-03-01 17:53:03,888 : 1341934384.py : train_scp : INFO : sample = 160, loss = 0.19999757036566734
2025-03-01 17:53:05,975 : 1341934384.py : train_scp : INFO : sample = 320, loss = 0.19999863095581533
2025-03-01 17:53:08,082 : 1341934384.py : train_scp : INFO : sample = 480, loss = 0.19999917944272358
2025-03-01 17:53:10,555 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.19999965634196998
2025-03-01 17:53:12,972 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.19999970108270645
2025-03-01 17:53:15,097 : 1341934384.py : train_scp : INFO : sample = 960, loss = 0.1999998336037
2025-03-01 17:53:17,511 : 1341934384.py : train_scp : INFO : sample = 1120, loss = 0.1999998626964433
2025-03-01 17:53:20,203 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.19999986672773956
2025-03-01 17:53:22,310 : 1341934384.py : train_scp : INFO : sample = 1440, loss = 0.19999994552797742
2025-03-01 17:53:24,958 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.199999972358346
2025-03-01 17:53:27,164 : 1341934384.py : train_scp : INFO : sample = 1760, loss = 0.19999998076395556
2025-03-01 17:53:29,382 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.1999999708806475
2025-03-01 17:53:31,545 : 1341934384.py : train_scp : INFO : sample = 2080, loss = 0.19999987666423505
2025-03-01 17:53:33,874 : 1341934384.py : train_scp : INFO : sample = 2240, loss = 0.19999985375574658
2025-03-01 17:53:36,034 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.1999998913705349
2025-03-01 17:53:38,345 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.1999999113380909
2025-03-01 17:53:40,489 : 1341934384.py : train_scp : INFO : sample = 2720, loss = 0.1999998790814596
2025-03-01 17:53:43,362 : 1341934384.py : train_scp : INFO : sample = 2880, loss = 0.19999986038439804
2025-03-01 17:53:45,540 : 1341934384.py : train_scp : INFO : sample = 3040, loss = 0.19999987514395462
2025-03-01 17:53:47,535 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.19999978557229042
2025-03-01 17:53:49,679 : 1341934384.py : train_scp : INFO : sample = 3360, loss = 0.1999997945413703
2025-03-01 17:53:52,486 : 1341934384.py : train_scp : INFO : sample = 3520, loss = 0.19999982331964103
2025-03-01 17:53:54,890 : 1341934384.py : train_scp : INFO : sample = 3680, loss = 0.19999982977043027
2025-03-01 17:53:57,481 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.19999984636281928
2025-03-01 17:54:00,204 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.19999985083937644
2025-03-01 17:54:03,049 : 1341934384.py : train_scp : INFO : sample = 4160, loss = 0.19999985992908478
2025-03-01 17:54:05,421 : 1341934384.py : train_scp : INFO : sample = 4320, loss = 0.2000007764056877
2025-03-01 17:54:07,552 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.20000075636697667
2025-03-01 17:54:09,406 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_4_epoch.pt
2025-03-01 17:54:11,659 : 1341934384.py : train_scp : INFO : sample = 160, loss = 0.20000005811452864
2025-03-01 17:54:13,347 : 1341934384.py : train_scp : INFO : sample = 320, loss = 0.1999994158744812
2025-03-01 17:54:15,332 : 1341934384.py : train_scp : INFO : sample = 480, loss = 0.19999927282333374
2025-03-01 17:54:18,090 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.19999944549053908
2025-03-01 17:54:20,470 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.1999996057152748
2025-03-01 17:54:22,707 : 1341934384.py : train_scp : INFO : sample = 960, loss = 0.19999981845418613
2025-03-01 17:54:25,207 : 1341934384.py : train_scp : INFO : sample = 1120, loss = 0.1999992816575936
2025-03-01 17:54:27,409 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.19999935608357192
2025-03-01 17:54:29,783 : 1341934384.py : train_scp : INFO : sample = 1440, loss = 0.19999935461415186
2025-03-01 17:54:31,930 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.19999936394393444
2025-03-01 17:54:34,084 : 1341934384.py : train_scp : INFO : sample = 1760, loss = 0.19999929876490072
2025-03-01 17:54:36,689 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.19999935819456974
2025-03-01 17:54:38,836 : 1341934384.py : train_scp : INFO : sample = 2080, loss = 0.1999993797678214
2025-03-01 17:54:41,151 : 1341934384.py : train_scp : INFO : sample = 2240, loss = 0.1999994338623115
2025-03-01 17:54:43,235 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.19999955475330353
2025-03-01 17:54:45,207 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.199999580392614
2025-03-01 17:54:47,434 : 1341934384.py : train_scp : INFO : sample = 2720, loss = 0.19999957128482707
2025-03-01 17:54:49,786 : 1341934384.py : train_scp : INFO : sample = 2880, loss = 0.19999960797528427
2025-03-01 17:54:52,812 : 1341934384.py : train_scp : INFO : sample = 3040, loss = 0.1999996308825518
2025-03-01 17:54:55,196 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.19999965608119966
2025-03-01 17:54:57,144 : 1341934384.py : train_scp : INFO : sample = 3360, loss = 0.19999968054748718
2025-03-01 17:54:59,298 : 1341934384.py : train_scp : INFO : sample = 3520, loss = 0.19999968914145774
2025-03-01 17:55:01,649 : 1341934384.py : train_scp : INFO : sample = 3680, loss = 0.19999973981276803
2025-03-01 17:55:04,315 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.19999974103023607
2025-03-01 17:55:06,249 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.1999996781051159
2025-03-01 17:55:08,367 : 1341934384.py : train_scp : INFO : sample = 4160, loss = 0.19999969790761288
2025-03-01 17:55:11,360 : 1341934384.py : train_scp : INFO : sample = 4320, loss = 0.19999970926178826
2025-03-01 17:55:13,735 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.19999974393951042
2025-03-01 17:55:16,278 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_5_epoch.pt
2025-03-01 17:55:19,073 : 1341934384.py : train_scp : INFO : sample = 160, loss = 0.20000026226043702
2025-03-01 17:55:21,312 : 1341934384.py : train_scp : INFO : sample = 320, loss = 0.1999999914318323
2025-03-01 17:55:23,596 : 1341934384.py : train_scp : INFO : sample = 480, loss = 0.19999994138876598
2025-03-01 17:55:26,144 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.19999978616833686
2025-03-01 17:55:28,302 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.19999973133206367
2025-03-01 17:55:30,843 : 1341934384.py : train_scp : INFO : sample = 960, loss = 0.19999984055757522
2025-03-01 17:55:33,214 : 1341934384.py : train_scp : INFO : sample = 1120, loss = 0.1999998790877206
2025-03-01 17:55:35,737 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.1999997790902853
2025-03-01 17:55:37,806 : 1341934384.py : train_scp : INFO : sample = 1440, loss = 0.19999980843729442
2025-03-01 17:55:40,182 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.19999979496002196
2025-03-01 17:55:42,411 : 1341934384.py : train_scp : INFO : sample = 1760, loss = 0.19999980682676488
2025-03-01 17:55:44,710 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.19999971830596527
2025-03-01 17:55:46,846 : 1341934384.py : train_scp : INFO : sample = 2080, loss = 0.1999996683345391
2025-03-01 17:55:48,660 : 1341934384.py : train_scp : INFO : sample = 2240, loss = 0.199999659295593
2025-03-01 17:55:50,523 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.1999997713168462
2025-03-01 17:55:52,704 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.19999971273355185
2025-03-01 17:55:55,371 : 1341934384.py : train_scp : INFO : sample = 2720, loss = 0.19999971959520788
2025-03-01 17:55:57,500 : 1341934384.py : train_scp : INFO : sample = 2880, loss = 0.19999971658819252
2025-03-01 17:55:59,992 : 1341934384.py : train_scp : INFO : sample = 3040, loss = 0.19999974546463867
2025-03-01 17:56:02,211 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.19999958969652654
2025-03-01 17:56:05,154 : 1341934384.py : train_scp : INFO : sample = 3360, loss = 0.1999995849671818
2025-03-01 17:56:07,874 : 1341934384.py : train_scp : INFO : sample = 3520, loss = 0.19999960664321076
2025-03-01 17:56:10,244 : 1341934384.py : train_scp : INFO : sample = 3680, loss = 0.19999964357070302
2025-03-01 17:56:12,326 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.1999994393438101
2025-03-01 17:56:14,626 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.19999944391846658
2025-03-01 17:56:16,703 : 1341934384.py : train_scp : INFO : sample = 4160, loss = 0.19999947055028036
2025-03-01 17:56:19,347 : 1341934384.py : train_scp : INFO : sample = 4320, loss = 0.19999949330532993
2025-03-01 17:56:21,643 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.19999953601509332
2025-03-01 17:56:23,947 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_6_epoch.pt
2025-03-01 17:56:26,526 : 1341934384.py : train_scp : INFO : sample = 160, loss = 0.20000073090195655
2025-03-01 17:56:29,039 : 1341934384.py : train_scp : INFO : sample = 320, loss = 0.20000033378601073
2025-03-01 17:56:30,901 : 1341934384.py : train_scp : INFO : sample = 480, loss = 0.1999996083478133
2025-03-01 17:56:33,552 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.19999975338578224
2025-03-01 17:56:35,633 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.19999990746378898
2025-03-01 17:56:38,132 : 1341934384.py : train_scp : INFO : sample = 960, loss = 0.19999995330969492
2025-03-01 17:56:40,765 : 1341934384.py : train_scp : INFO : sample = 1120, loss = 0.19999992442982537
2025-03-01 17:56:42,545 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.19999990724027156
2025-03-01 17:56:44,821 : 1341934384.py : train_scp : INFO : sample = 1440, loss = 0.19999991638792886
2025-03-01 17:56:47,042 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.19999991960823535
2025-03-01 17:56:49,257 : 1341934384.py : train_scp : INFO : sample = 1760, loss = 0.20000172115185044
2025-03-01 17:56:51,607 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.20000154698888462
2025-03-01 17:56:54,044 : 1341934384.py : train_scp : INFO : sample = 2080, loss = 0.2000014375608701
2025-03-01 17:56:56,639 : 1341934384.py : train_scp : INFO : sample = 2240, loss = 0.2000013026275805
2025-03-01 17:56:59,433 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.20000123659769695
2025-03-01 17:57:01,670 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.20000117286108435
2025-03-01 17:57:03,951 : 1341934384.py : train_scp : INFO : sample = 2720, loss = 0.20000102533137096
2025-03-01 17:57:06,346 : 1341934384.py : train_scp : INFO : sample = 2880, loss = 0.20000097184545465
2025-03-01 17:57:08,513 : 1341934384.py : train_scp : INFO : sample = 3040, loss = 0.20000089489315687
2025-03-01 17:57:10,749 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.20000085469335319
2025-03-01 17:57:13,074 : 1341934384.py : train_scp : INFO : sample = 3360, loss = 0.20000075492120925
2025-03-01 17:57:15,086 : 1341934384.py : train_scp : INFO : sample = 3520, loss = 0.2000006442381577
2025-03-01 17:57:17,340 : 1341934384.py : train_scp : INFO : sample = 3680, loss = 0.20000061613062153
2025-03-01 17:57:19,393 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.20000057192519308
2025-03-01 17:57:21,861 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.20000056952238082
2025-03-01 17:57:24,427 : 1341934384.py : train_scp : INFO : sample = 4160, loss = 0.2000005427461404
2025-03-01 17:57:26,660 : 1341934384.py : train_scp : INFO : sample = 4320, loss = 0.20000049838865244
2025-03-01 17:57:29,015 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.20000049413314888
2025-03-01 17:57:31,460 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_7_epoch.pt
2025-03-01 17:57:34,144 : 1341934384.py : train_scp : INFO : sample = 160, loss = 0.19999980032444
2025-03-01 17:57:36,672 : 1341934384.py : train_scp : INFO : sample = 320, loss = 0.20000002682209014
2025-03-01 17:57:39,417 : 1341934384.py : train_scp : INFO : sample = 480, loss = 0.20000010480483374
2025-03-01 17:57:41,775 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.2000001534819603
2025-03-01 17:57:44,132 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.20000014811754227
2025-03-01 17:57:46,412 : 1341934384.py : train_scp : INFO : sample = 960, loss = 0.20000011684993904
2025-03-01 17:57:48,399 : 1341934384.py : train_scp : INFO : sample = 1120, loss = 0.20000015231115478
2025-03-01 17:57:50,779 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.20000015590339898
2025-03-01 17:57:52,506 : 1341934384.py : train_scp : INFO : sample = 1440, loss = 0.20000015613105562
2025-03-01 17:57:54,944 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.20000007539987563
2025-03-01 17:57:57,196 : 1341934384.py : train_scp : INFO : sample = 1760, loss = 0.20000001049854538
2025-03-01 17:57:59,820 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.1999999284123381
2025-03-01 17:58:02,100 : 1341934384.py : train_scp : INFO : sample = 2080, loss = 0.19999996836368855
2025-03-01 17:58:04,124 : 1341934384.py : train_scp : INFO : sample = 2240, loss = 0.1999999824911356
2025-03-01 17:58:06,394 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.19999998480081557
2025-03-01 17:58:08,686 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.19999995664693415
2025-03-01 17:58:10,917 : 1341934384.py : train_scp : INFO : sample = 2720, loss = 0.19999998286366463
2025-03-01 17:58:12,854 : 1341934384.py : train_scp : INFO : sample = 2880, loss = 0.19999999275637997
2025-03-01 17:58:15,576 : 1341934384.py : train_scp : INFO : sample = 3040, loss = 0.1999999769816273
2025-03-01 17:58:17,899 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.19999998934566976
2025-03-01 17:58:20,132 : 1341934384.py : train_scp : INFO : sample = 3360, loss = 0.19999975700463568
2025-03-01 17:58:22,514 : 1341934384.py : train_scp : INFO : sample = 3520, loss = 0.19999978935176677
2025-03-01 17:58:25,034 : 1341934384.py : train_scp : INFO : sample = 3680, loss = 0.19999980016247085
2025-03-01 17:58:27,531 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.19999979604035617
2025-03-01 17:58:29,790 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.19999976548552514
2025-03-01 17:58:32,098 : 1341934384.py : train_scp : INFO : sample = 4160, loss = 0.1999997704361494
2025-03-01 17:58:34,413 : 1341934384.py : train_scp : INFO : sample = 4320, loss = 0.19999978147723058
2025-03-01 17:58:36,458 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.19999976631786143
2025-03-01 17:58:38,444 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_8_epoch.pt
2025-03-01 17:58:41,262 : 1341934384.py : train_scp : INFO : sample = 160, loss = 0.20000063702464105
2025-03-01 17:58:43,851 : 1341934384.py : train_scp : INFO : sample = 320, loss = 0.2000001110136509
2025-03-01 17:58:46,983 : 1341934384.py : train_scp : INFO : sample = 480, loss = 0.20000016192595163
2025-03-01 17:58:49,152 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.20000026226043702
2025-03-01 17:58:51,645 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.20000021412968635
2025-03-01 17:58:53,932 : 1341934384.py : train_scp : INFO : sample = 960, loss = 0.2000001261631648
2025-03-01 17:58:56,467 : 1341934384.py : train_scp : INFO : sample = 1120, loss = 0.20000103222472326
2025-03-01 17:58:58,810 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.20000087367370725
2025-03-01 17:59:01,500 : 1341934384.py : train_scp : INFO : sample = 1440, loss = 0.20000085632006329
2025-03-01 17:59:04,018 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.20000076480209827
2025-03-01 17:59:06,223 : 1341934384.py : train_scp : INFO : sample = 1760, loss = 0.2000007276507941
2025-03-01 17:59:08,567 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.20000066297749677
2025-03-01 17:59:10,561 : 1341934384.py : train_scp : INFO : sample = 2080, loss = 0.2000005688231725
2025-03-01 17:59:12,718 : 1341934384.py : train_scp : INFO : sample = 2240, loss = 0.20000049322843552
2025-03-01 17:59:14,466 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.20000041296084722
2025-03-01 17:59:16,880 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.20000032032839954
2025-03-01 17:59:18,499 : 1341934384.py : train_scp : INFO : sample = 2720, loss = 0.20000028101836934
2025-03-01 17:59:21,235 : 1341934384.py : train_scp : INFO : sample = 2880, loss = 0.20000020985802014
2025-03-01 17:59:23,698 : 1341934384.py : train_scp : INFO : sample = 3040, loss = 0.20000003082187554
2025-03-01 17:59:25,647 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.20000005528330803
2025-03-01 17:59:27,882 : 1341934384.py : train_scp : INFO : sample = 3360, loss = 0.20000004328432539
2025-03-01 17:59:29,755 : 1341934384.py : train_scp : INFO : sample = 3520, loss = 0.2000000583177263
2025-03-01 17:59:32,246 : 1341934384.py : train_scp : INFO : sample = 3680, loss = 0.20000004327815513
2025-03-01 17:59:35,015 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.19999997140839695
2025-03-01 17:59:37,089 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.199999902844429
2025-03-01 17:59:39,277 : 1341934384.py : train_scp : INFO : sample = 4160, loss = 0.19999985766525452
2025-03-01 17:59:41,545 : 1341934384.py : train_scp : INFO : sample = 4320, loss = 0.1999992230700122
2025-03-01 17:59:43,723 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.19999921566673687
2025-03-01 17:59:46,065 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_9_epoch.pt
2025-03-01 17:59:48,400 : 1341934384.py : train_scp : INFO : sample = 160, loss = 0.20000608637928963
2025-03-01 17:59:50,833 : 1341934384.py : train_scp : INFO : sample = 320, loss = 0.200004506111145
2025-03-01 17:59:53,766 : 1341934384.py : train_scp : INFO : sample = 480, loss = 0.200003295391798
2025-03-01 17:59:56,097 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.20000239834189415
2025-03-01 17:59:58,416 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.20000207662582398
2025-03-01 18:00:01,593 : 1341934384.py : train_scp : INFO : sample = 960, loss = 0.20000172245005768
2025-03-01 18:00:03,460 : 1341934384.py : train_scp : INFO : sample = 1120, loss = 0.20000159346631596
2025-03-01 18:00:05,977 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.20000111050903796
2025-03-01 18:00:07,895 : 1341934384.py : train_scp : INFO : sample = 1440, loss = 0.2000008059044679
2025-03-01 18:00:10,275 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.20000081062316893
2025-03-01 18:00:12,820 : 1341934384.py : train_scp : INFO : sample = 1760, loss = 0.20000195225531406
2025-03-01 18:00:14,916 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.2000016334777077
2025-03-01 18:00:17,182 : 1341934384.py : train_scp : INFO : sample = 2080, loss = 0.2000015177978919
2025-03-01 18:00:19,731 : 1341934384.py : train_scp : INFO : sample = 2240, loss = 0.2000013829874141
2025-03-01 18:00:22,676 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.20000123406449954
2025-03-01 18:00:24,534 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.2000011218711734
2025-03-01 18:00:26,472 : 1341934384.py : train_scp : INFO : sample = 2720, loss = 0.20000114594312274
2025-03-01 18:00:28,797 : 1341934384.py : train_scp : INFO : sample = 2880, loss = 0.2000010732975271
2025-03-01 18:00:31,234 : 1341934384.py : train_scp : INFO : sample = 3040, loss = 0.20000101802380463
2025-03-01 18:00:33,342 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.20000096503645182
2025-03-01 18:00:35,928 : 1341934384.py : train_scp : INFO : sample = 3360, loss = 0.2000009131928285
2025-03-01 18:00:38,745 : 1341934384.py : train_scp : INFO : sample = 3520, loss = 0.20000085387040267
2025-03-01 18:00:40,618 : 1341934384.py : train_scp : INFO : sample = 3680, loss = 0.20000077550825865
2025-03-01 18:00:42,455 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.20000072807694474
2025-03-01 18:00:44,479 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.20000059285759925
2025-03-01 18:00:46,522 : 1341934384.py : train_scp : INFO : sample = 4160, loss = 0.20000059598913558
2025-03-01 18:00:48,640 : 1341934384.py : train_scp : INFO : sample = 4320, loss = 0.20000061080963524
2025-03-01 18:00:50,860 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.20000053950186286
2025-03-01 18:00:52,800 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_10_epoch.pt
2025-03-01 18:00:55,031 : 1341934384.py : train_scp : INFO : sample = 160, loss = 0.19999931752681732
2025-03-01 18:00:57,291 : 1341934384.py : train_scp : INFO : sample = 320, loss = 0.19999893195927143
2025-03-01 18:00:59,860 : 1341934384.py : train_scp : INFO : sample = 480, loss = 0.1999983586370945
2025-03-01 18:01:02,099 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.19999876506626607
2025-03-01 18:01:04,247 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.1999988752603531
2025-03-01 18:01:06,702 : 1341934384.py : train_scp : INFO : sample = 960, loss = 0.1999990033606688
2025-03-01 18:01:09,022 : 1341934384.py : train_scp : INFO : sample = 1120, loss = 0.1999991650027888
2025-03-01 18:01:11,178 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.19999857880175115
2025-03-01 18:01:13,478 : 1341934384.py : train_scp : INFO : sample = 1440, loss = 0.1999985296693113
2025-03-01 18:01:15,737 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.19999810174107552
2025-03-01 18:01:18,172 : 1341934384.py : train_scp : INFO : sample = 1760, loss = 0.19999811432578346
2025-03-01 18:01:20,094 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.2000009979431828
2025-03-01 18:01:22,476 : 1341934384.py : train_scp : INFO : sample = 2080, loss = 0.20000087997088065
2025-03-01 18:01:24,628 : 1341934384.py : train_scp : INFO : sample = 2240, loss = 0.2000008001923561
2025-03-01 18:01:27,206 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.20000056167443594
2025-03-01 18:01:29,986 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.20000050850212575
2025-03-01 18:01:32,396 : 1341934384.py : train_scp : INFO : sample = 2720, loss = 0.20000039702829192
2025-03-01 18:01:34,845 : 1341934384.py : train_scp : INFO : sample = 2880, loss = 0.20000024686257045
2025-03-01 18:01:37,252 : 1341934384.py : train_scp : INFO : sample = 3040, loss = 0.2000002189686424
2025-03-01 18:01:39,820 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.20000014949589967
2025-03-01 18:01:41,532 : 1341934384.py : train_scp : INFO : sample = 3360, loss = 0.20000011452606747
2025-03-01 18:01:43,781 : 1341934384.py : train_scp : INFO : sample = 3520, loss = 0.2000000921162692
2025-03-01 18:01:45,975 : 1341934384.py : train_scp : INFO : sample = 3680, loss = 0.20000001895038977
2025-03-01 18:01:48,254 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.1999998366770645
2025-03-01 18:01:50,587 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.19999403241276742
2025-03-01 18:01:52,765 : 1341934384.py : train_scp : INFO : sample = 4160, loss = 0.200611075386405
2025-03-01 18:01:55,044 : 1341934384.py : train_scp : INFO : sample = 4320, loss = 0.20058839387363858
2025-03-01 18:01:57,613 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.20056511483022144
2025-03-01 18:01:59,628 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_11_epoch.pt
2025-03-01 18:02:02,262 : 1341934384.py : train_scp : INFO : sample = 160, loss = 0.23246304392814637
2025-03-01 18:02:04,994 : 1341934384.py : train_scp : INFO : sample = 320, loss = 0.2164376687258482
2025-03-01 18:02:07,163 : 1341934384.py : train_scp : INFO : sample = 480, loss = 0.21095612918337187
2025-03-01 18:02:09,165 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.20821713469922543
2025-03-01 18:02:11,437 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.2065737120807171
2025-03-01 18:02:13,429 : 1341934384.py : train_scp : INFO : sample = 960, loss = 0.20547805838286876
2025-03-01 18:02:15,501 : 1341934384.py : train_scp : INFO : sample = 1120, loss = 0.20469545402697154
2025-03-01 18:02:17,890 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.20410868329927326
2025-03-01 18:02:20,113 : 1341934384.py : train_scp : INFO : sample = 1440, loss = 0.20365216806530953
2025-03-01 18:02:22,514 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.20328694492578506
2025-03-01 18:02:24,604 : 1341934384.py : train_scp : INFO : sample = 1760, loss = 0.2029881353405389
2025-03-01 18:02:26,562 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.20273908966531357
2025-03-01 18:02:29,029 : 1341934384.py : train_scp : INFO : sample = 2080, loss = 0.20252841607882427
2025-03-01 18:02:31,176 : 1341934384.py : train_scp : INFO : sample = 2240, loss = 0.20234780593642165
2025-03-01 18:02:33,220 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.20219130098819732
2025-03-01 18:02:35,259 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.20205434570088981
2025-03-01 18:02:37,803 : 1341934384.py : train_scp : INFO : sample = 2720, loss = 0.20193348728558597
2025-03-01 18:02:40,547 : 1341934384.py : train_scp : INFO : sample = 2880, loss = 0.20182555028133922
2025-03-01 18:02:43,122 : 1341934384.py : train_scp : INFO : sample = 3040, loss = 0.20172946264869288
2025-03-01 18:02:45,256 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.20164298865944147
2025-03-01 18:02:47,634 : 1341934384.py : train_scp : INFO : sample = 3360, loss = 0.2015647355644476
2025-03-01 18:02:50,033 : 1341934384.py : train_scp : INFO : sample = 3520, loss = 0.20149360864677213
2025-03-01 18:02:52,241 : 1341934384.py : train_scp : INFO : sample = 3680, loss = 0.20142866557706957
2025-03-01 18:02:54,707 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.2013691329707702
2025-03-01 18:02:57,259 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.20131437355279921
2025-03-01 18:02:59,407 : 1341934384.py : train_scp : INFO : sample = 4160, loss = 0.20126384490957627
2025-03-01 18:03:02,297 : 1341934384.py : train_scp : INFO : sample = 4320, loss = 0.20121703967452048
2025-03-01 18:03:04,767 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.20117357335984706
2025-03-01 18:03:07,219 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_12_epoch.pt
2025-03-01 18:03:10,137 : 1341934384.py : train_scp : INFO : sample = 160, loss = 0.20000040754675866
2025-03-01 18:03:12,489 : 1341934384.py : train_scp : INFO : sample = 320, loss = 0.20000022053718566
2025-03-01 18:03:14,936 : 1341934384.py : train_scp : INFO : sample = 480, loss = 0.20000045721729595
2025-03-01 18:03:17,317 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.20000037252902986
2025-03-01 18:03:19,993 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.20000034168362618
2025-03-01 18:03:22,476 : 1341934384.py : train_scp : INFO : sample = 960, loss = 0.20000025356809298
2025-03-01 18:03:24,704 : 1341934384.py : train_scp : INFO : sample = 1120, loss = 0.20000022245304924
2025-03-01 18:03:27,001 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.2000001510605216
2025-03-01 18:03:29,238 : 1341934384.py : train_scp : INFO : sample = 1440, loss = 0.20000012376242213
2025-03-01 18:03:31,152 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.20000009141862393
2025-03-01 18:03:33,506 : 1341934384.py : train_scp : INFO : sample = 1760, loss = 0.2000000921162692
2025-03-01 18:03:35,852 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.20000007649262747
2025-03-01 18:03:38,085 : 1341934384.py : train_scp : INFO : sample = 2080, loss = 0.20000008155520146
2025-03-01 18:03:40,474 : 1341934384.py : train_scp : INFO : sample = 2240, loss = 0.2000000808920179
2025-03-01 18:03:42,605 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.2000000736117363
2025-03-01 18:03:45,076 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.20000006784684957
2025-03-01 18:03:47,775 : 1341934384.py : train_scp : INFO : sample = 2720, loss = 0.2000000676688026
2025-03-01 18:03:50,040 : 1341934384.py : train_scp : INFO : sample = 2880, loss = 0.20000006614459886
2025-03-01 18:03:52,269 : 1341934384.py : train_scp : INFO : sample = 3040, loss = 0.20000003846852402
2025-03-01 18:03:54,504 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.200000008828938
2025-03-01 18:03:56,489 : 1341934384.py : train_scp : INFO : sample = 3360, loss = 0.1999999970552467
2025-03-01 18:03:59,145 : 1341934384.py : train_scp : INFO : sample = 3520, loss = 0.19999999468299476
2025-03-01 18:04:01,604 : 1341934384.py : train_scp : INFO : sample = 3680, loss = 0.199999975607447
2025-03-01 18:04:03,665 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.19999994232008855
2025-03-01 18:04:05,468 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.1999999543428421
2025-03-01 18:04:07,822 : 1341934384.py : train_scp : INFO : sample = 4160, loss = 0.19999995096944845
2025-03-01 18:04:10,242 : 1341934384.py : train_scp : INFO : sample = 4320, loss = 0.19999993575943842
2025-03-01 18:04:12,256 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.19999993105552027
2025-03-01 18:04:14,657 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_13_epoch.pt
2025-03-01 18:04:17,874 : 1341934384.py : train_scp : INFO : sample = 160, loss = 0.20000044032931327
2025-03-01 18:04:20,128 : 1341934384.py : train_scp : INFO : sample = 320, loss = 0.20000013783574105
2025-03-01 18:04:22,279 : 1341934384.py : train_scp : INFO : sample = 480, loss = 0.20000029504299163
2025-03-01 18:04:24,062 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.20000021029263734
2025-03-01 18:04:26,024 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.20000020384788514
2025-03-01 18:04:28,269 : 1341934384.py : train_scp : INFO : sample = 960, loss = 0.20000009002784888
2025-03-01 18:04:30,712 : 1341934384.py : train_scp : INFO : sample = 1120, loss = 0.20000006290418762
2025-03-01 18:04:33,217 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.20000010216608644
2025-03-01 18:04:35,383 : 1341934384.py : train_scp : INFO : sample = 1440, loss = 0.20000011291768816
2025-03-01 18:04:37,917 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.19999987185001372
2025-03-01 18:04:40,133 : 1341934384.py : train_scp : INFO : sample = 1760, loss = 0.19999989305030216
2025-03-01 18:04:42,519 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.19999991102765005
2025-03-01 18:04:44,768 : 1341934384.py : train_scp : INFO : sample = 2080, loss = 0.19999993661275275
2025-03-01 18:04:47,051 : 1341934384.py : train_scp : INFO : sample = 2240, loss = 0.19999995503042425
2025-03-01 18:04:49,329 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.1999999542037646
2025-03-01 18:04:51,427 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.19999995939433574
2025-03-01 18:04:53,542 : 1341934384.py : train_scp : INFO : sample = 2720, loss = 0.19999996844459983
2025-03-01 18:04:56,172 : 1341934384.py : train_scp : INFO : sample = 2880, loss = 0.19999997011489337
2025-03-01 18:04:58,116 : 1341934384.py : train_scp : INFO : sample = 3040, loss = 0.19999996525676628
2025-03-01 18:05:00,688 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.19999998301267624
2025-03-01 18:05:02,501 : 1341934384.py : train_scp : INFO : sample = 3360, loss = 0.19999998215408551
2025-03-01 18:05:05,011 : 1341934384.py : train_scp : INFO : sample = 3520, loss = 0.19999997781758957
2025-03-01 18:05:07,674 : 1341934384.py : train_scp : INFO : sample = 3680, loss = 0.19999997457084448
2025-03-01 18:05:10,282 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.1999999677762389
2025-03-01 18:05:13,193 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.19999996042251586
2025-03-01 18:05:15,742 : 1341934384.py : train_scp : INFO : sample = 4160, loss = 0.19999994455048672
2025-03-01 18:05:17,808 : 1341934384.py : train_scp : INFO : sample = 4320, loss = 0.19999994908769925
2025-03-01 18:05:20,420 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.19999994779271738
2025-03-01 18:05:22,304 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_14_epoch.pt
2025-03-01 18:05:25,184 : 1341934384.py : train_scp : INFO : sample = 160, loss = 0.20000006034970283
2025-03-01 18:05:27,623 : 1341934384.py : train_scp : INFO : sample = 320, loss = 0.2000000335276127
2025-03-01 18:05:29,833 : 1341934384.py : train_scp : INFO : sample = 480, loss = 0.20000006804863613
2025-03-01 18:05:32,134 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.19999985117465258
2025-03-01 18:05:34,593 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.1999998524785042
2025-03-01 18:05:36,343 : 1341934384.py : train_scp : INFO : sample = 960, loss = 0.19999990736444792
2025-03-01 18:05:38,751 : 1341934384.py : train_scp : INFO : sample = 1120, loss = 0.19999992549419404
2025-03-01 18:05:40,911 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.1999999424442649
2025-03-01 18:05:43,268 : 1341934384.py : train_scp : INFO : sample = 1440, loss = 0.19999994114041328
2025-03-01 18:05:45,378 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.19999990612268448
2025-03-01 18:05:47,512 : 1341934384.py : train_scp : INFO : sample = 1760, loss = 0.1999998923052441
2025-03-01 18:05:49,611 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.1999999559794863
2025-03-01 18:05:52,034 : 1341934384.py : train_scp : INFO : sample = 2080, loss = 0.19999993718587436
2025-03-01 18:05:54,243 : 1341934384.py : train_scp : INFO : sample = 2240, loss = 0.1999999601393938
2025-03-01 18:05:56,194 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.19999997382362683
2025-03-01 18:05:58,743 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.19999997816048562
2025-03-01 18:06:01,159 : 1341934384.py : train_scp : INFO : sample = 2720, loss = 0.19999995165888002
2025-03-01 18:06:03,456 : 1341934384.py : train_scp : INFO : sample = 2880, loss = 0.20000004238552518
2025-03-01 18:06:05,459 : 1341934384.py : train_scp : INFO : sample = 3040, loss = 0.20000002243016896
2025-03-01 18:06:07,678 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.20000002428889274
2025-03-01 18:06:10,258 : 1341934384.py : train_scp : INFO : sample = 3360, loss = 0.2000000325696809
2025-03-01 18:06:12,581 : 1341934384.py : train_scp : INFO : sample = 3520, loss = 0.20000002871860156
2025-03-01 18:06:14,907 : 1341934384.py : train_scp : INFO : sample = 3680, loss = 0.20000002695166547
2025-03-01 18:06:17,193 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.2000000512537857
2025-03-01 18:06:19,923 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.20000004860758783
2025-03-01 18:06:22,406 : 1341934384.py : train_scp : INFO : sample = 4160, loss = 0.200000049202488
2025-03-01 18:06:24,828 : 1341934384.py : train_scp : INFO : sample = 4320, loss = 0.20000007732046976
2025-03-01 18:06:27,476 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.20000007389379398
2025-03-01 18:06:29,554 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_15_epoch.pt
2025-03-01 18:06:32,486 : 1341934384.py : train_scp : INFO : sample = 160, loss = 0.1999998226761818
2025-03-01 18:06:34,336 : 1341934384.py : train_scp : INFO : sample = 320, loss = 0.19999961592257023
2025-03-01 18:06:36,816 : 1341934384.py : train_scp : INFO : sample = 480, loss = 0.19999950031439465
2025-03-01 18:06:39,160 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.1999995592981577
2025-03-01 18:06:41,335 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.1999997079372406
2025-03-01 18:06:43,351 : 1341934384.py : train_scp : INFO : sample = 960, loss = 0.1999997107932965
2025-03-01 18:06:45,664 : 1341934384.py : train_scp : INFO : sample = 1120, loss = 0.19999979585409164
2025-03-01 18:06:48,026 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.19999975590035318
2025-03-01 18:06:50,371 : 1341934384.py : train_scp : INFO : sample = 1440, loss = 0.19999983567330573
2025-03-01 18:06:52,805 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.1999998287856579
2025-03-01 18:06:55,374 : 1341934384.py : train_scp : INFO : sample = 1760, loss = 0.19999987862326882
2025-03-01 18:06:57,567 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.19999987507859865
2025-03-01 18:06:59,769 : 1341934384.py : train_scp : INFO : sample = 2080, loss = 0.19999989099227466
2025-03-01 18:07:02,333 : 1341934384.py : train_scp : INFO : sample = 2240, loss = 0.19999991346682822
2025-03-01 18:07:04,628 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.1999999171992143
2025-03-01 18:07:07,035 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.19999993513338268
2025-03-01 18:07:09,604 : 1341934384.py : train_scp : INFO : sample = 2720, loss = 0.19999995165888002
2025-03-01 18:07:11,928 : 1341934384.py : train_scp : INFO : sample = 2880, loss = 0.19999995624853506
2025-03-01 18:07:14,119 : 1341934384.py : train_scp : INFO : sample = 3040, loss = 0.19999995451224478
2025-03-01 18:07:16,533 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.19999996297061443
2025-03-01 18:07:18,462 : 1341934384.py : train_scp : INFO : sample = 3360, loss = 0.19999992130767733
2025-03-01 18:07:20,783 : 1341934384.py : train_scp : INFO : sample = 3520, loss = 0.19999993125146084
2025-03-01 18:07:23,055 : 1341934384.py : train_scp : INFO : sample = 3680, loss = 0.19999995105292487
2025-03-01 18:07:25,734 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.19999995656932393
2025-03-01 18:07:27,812 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.19999994072318078
2025-03-01 18:07:30,279 : 1341934384.py : train_scp : INFO : sample = 4160, loss = 0.1999999449516718
2025-03-01 18:07:32,292 : 1341934384.py : train_scp : INFO : sample = 4320, loss = 0.19999996255393382
2025-03-01 18:07:34,645 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.19999997088951724
2025-03-01 18:07:36,504 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_16_epoch.pt
2025-03-01 18:07:39,861 : 1341934384.py : train_scp : INFO : sample = 160, loss = 0.2000003933906555
2025-03-01 18:07:41,760 : 1341934384.py : train_scp : INFO : sample = 320, loss = 0.2000001322478056
2025-03-01 18:07:44,196 : 1341934384.py : train_scp : INFO : sample = 480, loss = 0.20000007798274358
2025-03-01 18:07:47,015 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.20000005885958672
2025-03-01 18:07:49,139 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.19999997764825822
2025-03-01 18:07:51,340 : 1341934384.py : train_scp : INFO : sample = 960, loss = 0.19999998820324738
2025-03-01 18:07:53,372 : 1341934384.py : train_scp : INFO : sample = 1120, loss = 0.1999999191079821
2025-03-01 18:07:55,542 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.19999984987080097
2025-03-01 18:07:58,468 : 1341934384.py : train_scp : INFO : sample = 1440, loss = 0.19999990430143144
2025-03-01 18:08:00,988 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.19999997682869433
2025-03-01 18:08:03,101 : 1341934384.py : train_scp : INFO : sample = 1760, loss = 0.19999995441599325
2025-03-01 18:08:05,216 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.1999999586492777
2025-03-01 18:08:07,479 : 1341934384.py : train_scp : INFO : sample = 2080, loss = 0.19999996882218582
2025-03-01 18:08:09,893 : 1341934384.py : train_scp : INFO : sample = 2240, loss = 0.19999999372022492
2025-03-01 18:08:12,408 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.20000002175569534
2025-03-01 18:08:14,886 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.19999999650754036
2025-03-01 18:08:17,228 : 1341934384.py : train_scp : INFO : sample = 2720, loss = 0.20000001064994755
2025-03-01 18:08:18,959 : 1341934384.py : train_scp : INFO : sample = 2880, loss = 0.2000000037252903
2025-03-01 18:08:21,046 : 1341934384.py : train_scp : INFO : sample = 3040, loss = 0.19999708082330855
2025-03-01 18:08:23,457 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.19999725833535195
2025-03-01 18:08:26,199 : 1341934384.py : train_scp : INFO : sample = 3360, loss = 0.19999740890094211
2025-03-01 18:08:28,561 : 1341934384.py : train_scp : INFO : sample = 3520, loss = 0.1999975975941528
2025-03-01 18:08:30,361 : 1341934384.py : train_scp : INFO : sample = 3680, loss = 0.19999771701252977
2025-03-01 18:08:32,894 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.1999978572440644
2025-03-01 18:08:35,208 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.19999792796373367
2025-03-01 18:08:37,864 : 1341934384.py : train_scp : INFO : sample = 4160, loss = 0.19999755618090814
2025-03-01 18:08:40,074 : 1341934384.py : train_scp : INFO : sample = 4320, loss = 0.20047372874838335
2025-03-01 18:08:42,123 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.20045681055635214
2025-03-01 18:08:44,435 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_17_epoch.pt
2025-03-01 18:08:47,235 : 1341934384.py : train_scp : INFO : sample = 160, loss = 0.20000002011656762
2025-03-01 18:08:50,112 : 1341934384.py : train_scp : INFO : sample = 320, loss = 0.2000000137835741
2025-03-01 18:08:52,156 : 1341934384.py : train_scp : INFO : sample = 480, loss = 0.1999999942878882
2025-03-01 18:08:54,424 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.20000000074505805
2025-03-01 18:08:56,903 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.19999999463558196
2025-03-01 18:08:59,321 : 1341934384.py : train_scp : INFO : sample = 960, loss = 0.1999999980131785
2025-03-01 18:09:01,438 : 1341934384.py : train_scp : INFO : sample = 1120, loss = 0.20000002192599434
2025-03-01 18:09:03,536 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.20000000894069672
2025-03-01 18:09:05,534 : 1341934384.py : train_scp : INFO : sample = 1440, loss = 0.20000001498394543
2025-03-01 18:09:07,606 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.20000000976026058
2025-03-01 18:09:10,024 : 1341934384.py : train_scp : INFO : sample = 1760, loss = 0.20000000562180173
2025-03-01 18:09:12,014 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.2000000011175871
2025-03-01 18:09:14,266 : 1341934384.py : train_scp : INFO : sample = 2080, loss = 0.2000000056739037
2025-03-01 18:09:16,322 : 1341934384.py : train_scp : INFO : sample = 2240, loss = 0.19999999904206822
2025-03-01 18:09:18,601 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.19999999637405078
2025-03-01 18:09:20,843 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.19999998519197107
2025-03-01 18:09:23,598 : 1341934384.py : train_scp : INFO : sample = 2720, loss = 0.19999997256433263
2025-03-01 18:09:25,843 : 1341934384.py : train_scp : INFO : sample = 2880, loss = 0.1999999802559614
2025-03-01 18:09:27,578 : 1341934384.py : train_scp : INFO : sample = 3040, loss = 0.1999999745895988
2025-03-01 18:09:30,146 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.19999997463077307
2025-03-01 18:09:32,502 : 1341934384.py : train_scp : INFO : sample = 3360, loss = 0.19999997690320015
2025-03-01 18:09:34,483 : 1341934384.py : train_scp : INFO : sample = 3520, loss = 0.19999997680160134
2025-03-01 18:09:37,141 : 1341934384.py : train_scp : INFO : sample = 3680, loss = 0.19999998104961023
2025-03-01 18:09:39,552 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.19999998261531193
2025-03-01 18:09:42,352 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.19999997925758362
2025-03-01 18:09:44,733 : 1341934384.py : train_scp : INFO : sample = 4160, loss = 0.19999998077177084
2025-03-01 18:09:46,943 : 1341934384.py : train_scp : INFO : sample = 4320, loss = 0.1999999808492484
2025-03-01 18:09:49,769 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.19999997839331626
2025-03-01 18:09:52,138 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_18_epoch.pt
2025-03-01 18:09:54,980 : 1341934384.py : train_scp : INFO : sample = 160, loss = 0.1999999277293682
2025-03-01 18:09:57,274 : 1341934384.py : train_scp : INFO : sample = 320, loss = 0.19999986849725246
2025-03-01 18:09:59,780 : 1341934384.py : train_scp : INFO : sample = 480, loss = 0.1999999165534973
2025-03-01 18:10:02,022 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.19999993741512298
2025-03-01 18:10:03,819 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.19999996289610864
2025-03-01 18:10:06,592 : 1341934384.py : train_scp : INFO : sample = 960, loss = 0.19999994399646917
2025-03-01 18:10:08,550 : 1341934384.py : train_scp : INFO : sample = 1120, loss = 0.19999994731375148
2025-03-01 18:10:11,115 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.1999999421648681
2025-03-01 18:10:14,009 : 1341934384.py : train_scp : INFO : sample = 1440, loss = 0.19999996423721314
2025-03-01 18:10:16,222 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.19999996468424797
2025-03-01 18:10:18,744 : 1341934384.py : train_scp : INFO : sample = 1760, loss = 0.19999996281482957
2025-03-01 18:10:21,048 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.19999996467183034
2025-03-01 18:10:23,083 : 1341934384.py : train_scp : INFO : sample = 2080, loss = 0.19999996933799524
2025-03-01 18:10:25,333 : 1341934384.py : train_scp : INFO : sample = 2240, loss = 0.1999999677496297
2025-03-01 18:10:27,132 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.19999997362494468
2025-03-01 18:10:29,699 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.19999997573904693
2025-03-01 18:10:31,921 : 1341934384.py : train_scp : INFO : sample = 2720, loss = 0.19999996997854289
2025-03-01 18:10:33,835 : 1341934384.py : train_scp : INFO : sample = 2880, loss = 0.1999999738815758
2025-03-01 18:10:35,850 : 1341934384.py : train_scp : INFO : sample = 3040, loss = 0.19999997051138627
2025-03-01 18:10:38,306 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.199999972358346
2025-03-01 18:10:40,455 : 1341934384.py : train_scp : INFO : sample = 3360, loss = 0.19999997371009418
2025-03-01 18:10:42,866 : 1341934384.py : train_scp : INFO : sample = 3520, loss = 0.19999997822398488
2025-03-01 18:10:45,125 : 1341934384.py : train_scp : INFO : sample = 3680, loss = 0.1999999821510004
2025-03-01 18:10:47,088 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.19999998857577642
2025-03-01 18:10:49,386 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.19999999541044236
2025-03-01 18:10:51,612 : 1341934384.py : train_scp : INFO : sample = 4160, loss = 0.19999999713439207
2025-03-01 18:10:54,117 : 1341934384.py : train_scp : INFO : sample = 4320, loss = 0.19999998396745436
2025-03-01 18:10:57,211 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.199999983156366
2025-03-01 18:10:59,733 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_19_epoch.pt
2025-03-01 18:11:02,078 : 1341934384.py : train_scp : INFO : sample = 160, loss = 0.20000000447034835
2025-03-01 18:11:04,051 : 1341934384.py : train_scp : INFO : sample = 320, loss = 0.1999999452382326
2025-03-01 18:11:06,269 : 1341934384.py : train_scp : INFO : sample = 480, loss = 0.1999999371667703
2025-03-01 18:11:08,592 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.19999994281679392
2025-03-01 18:11:11,041 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.19999992206692696
2025-03-01 18:11:13,044 : 1341934384.py : train_scp : INFO : sample = 960, loss = 0.19999994039535524
2025-03-01 18:11:15,021 : 1341934384.py : train_scp : INFO : sample = 1120, loss = 0.19999994188547135
2025-03-01 18:11:17,992 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.19999994980171323
2025-03-01 18:11:20,304 : 1341934384.py : train_scp : INFO : sample = 1440, loss = 0.19999995165401036
2025-03-01 18:11:22,381 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.19999994821846484
2025-03-01 18:11:24,924 : 1341934384.py : train_scp : INFO : sample = 1760, loss = 0.19999995251948183
2025-03-01 18:11:27,269 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.1999999590838949
2025-03-01 18:11:29,637 : 1341934384.py : train_scp : INFO : sample = 2080, loss = 0.19999996171547815
2025-03-01 18:11:31,842 : 1341934384.py : train_scp : INFO : sample = 2240, loss = 0.19999996881399837
2025-03-01 18:11:33,872 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.1999999708433946
2025-03-01 18:11:35,916 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.1999999641440809
2025-03-01 18:11:39,156 : 1341934384.py : train_scp : INFO : sample = 2720, loss = 0.19999996476313647
2025-03-01 18:11:41,777 : 1341934384.py : train_scp : INFO : sample = 2880, loss = 0.19999997065299088
2025-03-01 18:11:44,725 : 1341934384.py : train_scp : INFO : sample = 3040, loss = 0.19999996921733806
2025-03-01 18:11:46,964 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.19999996822327376
2025-03-01 18:11:48,993 : 1341934384.py : train_scp : INFO : sample = 3360, loss = 0.19999997374557313
2025-03-01 18:11:51,306 : 1341934384.py : train_scp : INFO : sample = 3520, loss = 0.19999997280538082
2025-03-01 18:11:53,175 : 1341934384.py : train_scp : INFO : sample = 3680, loss = 0.1999999734694543
2025-03-01 18:11:56,046 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.199999971780926
2025-03-01 18:11:58,543 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.19999997770786285
2025-03-01 18:12:00,668 : 1341934384.py : train_scp : INFO : sample = 4160, loss = 0.1999999823478552
2025-03-01 18:12:02,842 : 1341934384.py : train_scp : INFO : sample = 4320, loss = 0.19999998107000633
2025-03-01 18:12:04,784 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.19999998608337982
2025-03-01 18:12:07,277 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_20_epoch.pt
2025-03-01 18:12:10,415 : 1341934384.py : train_scp : INFO : sample = 160, loss = 0.20000002011656762
2025-03-01 18:12:12,635 : 1341934384.py : train_scp : INFO : sample = 320, loss = 0.2000000227242708
2025-03-01 18:12:15,313 : 1341934384.py : train_scp : INFO : sample = 480, loss = 0.20000001167257628
2025-03-01 18:12:17,804 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.20000002402812242
2025-03-01 18:12:19,977 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.20000000938773155
2025-03-01 18:12:22,125 : 1341934384.py : train_scp : INFO : sample = 960, loss = 0.20000001999239128
2025-03-01 18:12:24,567 : 1341934384.py : train_scp : INFO : sample = 1120, loss = 0.20000001788139343
2025-03-01 18:12:27,215 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.20000003715977072
2025-03-01 18:12:29,394 : 1341934384.py : train_scp : INFO : sample = 1440, loss = 0.20000003394153382
2025-03-01 18:12:31,706 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.20000002026557923
2025-03-01 18:12:33,986 : 1341934384.py : train_scp : INFO : sample = 1760, loss = 0.2000000281090086
2025-03-01 18:12:36,197 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.20000002880891163
2025-03-01 18:12:38,220 : 1341934384.py : train_scp : INFO : sample = 2080, loss = 0.20000002521734972
2025-03-01 18:12:39,993 : 1341934384.py : train_scp : INFO : sample = 2240, loss = 0.20000002315001827
2025-03-01 18:12:41,923 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.20000002319614094
2025-03-01 18:12:44,935 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.20000001108273863
2025-03-01 18:12:47,066 : 1341934384.py : train_scp : INFO : sample = 2720, loss = 0.20000001266598702
2025-03-01 18:12:49,298 : 1341934384.py : train_scp : INFO : sample = 2880, loss = 0.20000001096891032
2025-03-01 18:12:51,219 : 1341934384.py : train_scp : INFO : sample = 3040, loss = 0.2000000181951021
2025-03-01 18:12:53,705 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.2000000136345625
2025-03-01 18:12:55,616 : 1341934384.py : train_scp : INFO : sample = 3360, loss = 0.20000001135326567
2025-03-01 18:12:58,031 : 1341934384.py : train_scp : INFO : sample = 3520, loss = 0.20000001178546387
2025-03-01 18:13:01,106 : 1341934384.py : train_scp : INFO : sample = 3680, loss = 0.2000000120828981
2025-03-01 18:13:03,501 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.20000001657754182
2025-03-01 18:13:06,046 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.20000001260638237
2025-03-01 18:13:07,836 : 1341934384.py : train_scp : INFO : sample = 4160, loss = 0.20000001051678107
2025-03-01 18:13:10,079 : 1341934384.py : train_scp : INFO : sample = 4320, loss = 0.2000000074505806
2025-03-01 18:13:12,564 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.20000001080334187
2025-03-01 18:13:14,719 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_21_epoch.pt
2025-03-01 18:13:17,776 : 1341934384.py : train_scp : INFO : sample = 160, loss = 0.20000007301568984
2025-03-01 18:13:19,665 : 1341934384.py : train_scp : INFO : sample = 320, loss = 0.20000004917383193
2025-03-01 18:13:22,274 : 1341934384.py : train_scp : INFO : sample = 480, loss = 0.20000005662441253
2025-03-01 18:13:24,853 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.20000006556510924
2025-03-01 18:13:27,230 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.20000004276633263
2025-03-01 18:13:29,464 : 1341934384.py : train_scp : INFO : sample = 960, loss = 0.20000002992649873
2025-03-01 18:13:31,919 : 1341934384.py : train_scp : INFO : sample = 1120, loss = 0.2000000334211758
2025-03-01 18:13:33,965 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.20000004153698683
2025-03-01 18:13:36,286 : 1341934384.py : train_scp : INFO : sample = 1440, loss = 0.20000003327926
2025-03-01 18:13:38,371 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.20000002458691596
2025-03-01 18:13:40,670 : 1341934384.py : train_scp : INFO : sample = 1760, loss = 0.20000001463023098
2025-03-01 18:13:42,709 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.2000000252077977
2025-03-01 18:13:45,188 : 1341934384.py : train_scp : INFO : sample = 2080, loss = 0.20000002447229165
2025-03-01 18:13:47,122 : 1341934384.py : train_scp : INFO : sample = 2240, loss = 0.20000001942472798
2025-03-01 18:13:49,541 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.20000001991788546
2025-03-01 18:13:52,443 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.20000001545995474
2025-03-01 18:13:54,655 : 1341934384.py : train_scp : INFO : sample = 2720, loss = 0.200000011088217
2025-03-01 18:13:56,638 : 1341934384.py : train_scp : INFO : sample = 2880, loss = 0.20000000124176343
2025-03-01 18:13:59,275 : 1341934384.py : train_scp : INFO : sample = 3040, loss = 0.20000000270573717
2025-03-01 18:14:01,330 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.2000000002235174
2025-03-01 18:14:03,807 : 1341934384.py : train_scp : INFO : sample = 3360, loss = 0.19999999964521045
2025-03-01 18:14:06,562 : 1341934384.py : train_scp : INFO : sample = 3520, loss = 0.2000000090761618
2025-03-01 18:14:08,666 : 1341934384.py : train_scp : INFO : sample = 3680, loss = 0.20000000579849533
2025-03-01 18:14:11,034 : 1341934384.py : train_scp : INFO : sample = 3840, loss = 0.20000000465661288
2025-03-01 18:14:13,625 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.20000000450015068
2025-03-01 18:14:15,692 : 1341934384.py : train_scp : INFO : sample = 4160, loss = 0.2000000147578808
2025-03-01 18:14:17,931 : 1341934384.py : train_scp : INFO : sample = 4320, loss = 0.2000000144596453
2025-03-01 18:14:20,139 : 1341934384.py : train_scp : INFO : sample = 4480, loss = 0.20000001128230777
2025-03-01 18:14:22,518 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_22_epoch.pt
2025-03-01 18:14:25,161 : 1341934384.py : train_scp : INFO : sample = 160, loss = 0.2000008910894394
2025-03-01 18:14:26,999 : 1341934384.py : train_scp : INFO : sample = 320, loss = 0.20000047720968722
2025-03-01 18:14:29,505 : 1341934384.py : train_scp : INFO : sample = 480, loss = 0.2000003233551979
2025-03-01 18:14:31,670 : 1341934384.py : train_scp : INFO : sample = 640, loss = 0.2000002508983016
2025-03-01 18:14:34,377 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.20000019997358323
2025-03-01 18:14:36,886 : 1341934384.py : train_scp : INFO : sample = 960, loss = 0.2000001663963
2025-03-01 18:14:39,284 : 1341934384.py : train_scp : INFO : sample = 1120, loss = 0.20000012336032733
2025-03-01 18:14:41,555 : 1341934384.py : train_scp : INFO : sample = 1280, loss = 0.20000009909272193
2025-03-01 18:14:43,488 : 1341934384.py : train_scp : INFO : sample = 1440, loss = 0.2000000803834862
2025-03-01 18:14:45,737 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.20000007808208464
2025-03-01 18:14:47,993 : 1341934384.py : train_scp : INFO : sample = 1760, loss = 0.20000008351423523
2025-03-01 18:14:50,111 : 1341934384.py : train_scp : INFO : sample = 1920, loss = 0.20000008704761665
2025-03-01 18:14:52,656 : 1341934384.py : train_scp : INFO : sample = 2080, loss = 0.2000000838476878
2025-03-01 18:14:55,329 : 1341934384.py : train_scp : INFO : sample = 2240, loss = 0.20000008381903173
2025-03-01 18:14:57,404 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.20000007808208464
2025-03-01 18:14:59,915 : 1341934384.py : train_scp : INFO : sample = 2560, loss = 0.2000000740867108
2025-03-01 18:15:01,891 : 1341934384.py : train_scp : INFO : sample = 2720, loss = 0.20000006578424398
2025-03-01 18:15:04,468 : 1341934384.py : train_scp : INFO : sample = 2880, loss = 0.20000005749364694
2025-03-01 18:15:06,915 : 1341934384.py : train_scp : INFO : sample = 3040, loss = 0.20000005391867537
2025-03-01 18:15:09,536 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.2000000538304448
2025-03-01 18:15:25,326 : 916236790.py : __call__ : INFO : loaded dataset, sample = 1158
2025-03-01 18:15:25,327 : 1341934384.py : train_scp : INFO : 109483778
2025-03-01 18:15:35,761 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.20027510717511177
2025-03-01 18:15:47,802 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.20013710163533688
2025-03-01 18:15:58,825 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.2000912706553936
2025-03-01 18:16:11,434 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.20006833281368017
2025-03-01 18:16:23,186 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.20005428257584573
2025-03-01 18:16:32,332 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_1_epoch.pt
2025-03-01 18:16:43,758 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.20000398993492127
2025-03-01 18:16:55,833 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.20000166431069374
2025-03-01 18:17:07,422 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.20000063995520273
2025-03-01 18:17:20,114 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.20000013671815395
2025-03-01 18:17:31,442 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.1999990745484829
2025-03-01 18:17:39,842 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_2_epoch.pt
2025-03-01 18:17:51,776 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.1999895016849041
2025-03-01 18:18:02,515 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.19999426521360875
2025-03-01 18:18:14,428 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.19999711453914643
2025-03-01 18:18:27,093 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.19999765679240228
2025-03-01 18:18:38,563 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.1999980615079403
2025-03-01 18:18:47,898 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_3_epoch.pt
2025-03-01 18:18:59,155 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.19999906420707703
2025-03-01 18:19:11,066 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.1999989554286003
2025-03-01 18:19:23,542 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.19999791319171586
2025-03-01 18:19:34,977 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.1999994992092252
2025-03-01 18:19:46,559 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.19999926882982255
2025-03-01 18:19:55,981 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_4_epoch.pt
2025-03-01 18:20:09,172 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.19999949097633363
2025-03-01 18:20:20,387 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.1999963802099228
2025-03-01 18:20:32,352 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.19999883179863295
2025-03-01 18:20:43,831 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.1999990539252758
2025-03-01 18:20:54,438 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.19999924829602242
2025-03-01 18:21:03,417 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_5_epoch.pt
2025-03-01 18:21:16,117 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.19999730616807937
2025-03-01 18:21:28,214 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.20003350466489792
2025-03-01 18:21:39,217 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.20005818093816438
2025-03-01 18:21:49,736 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.20004411835223437
2025-03-01 18:22:00,908 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.20003458043932915
2025-03-01 18:22:10,305 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_6_epoch.pt
2025-03-01 18:22:23,062 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.20000679925084114
2025-03-01 18:22:34,132 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.2000042274594307
2025-03-01 18:22:45,129 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.2000025852024555
2025-03-01 18:22:56,514 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.2000015578046441
2025-03-01 18:23:08,481 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.2000013127028942
2025-03-01 18:23:17,736 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_7_epoch.pt
2025-03-01 18:23:29,708 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.2000020144879818
2025-03-01 18:23:41,059 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.20000149600207806
2025-03-01 18:23:53,314 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.20000164399544398
2025-03-01 18:24:03,990 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.2000010961294174
2025-03-01 18:24:15,787 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.2000013516843319
2025-03-01 18:24:25,253 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_8_epoch.pt
2025-03-01 18:24:37,311 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.19999824106693267
2025-03-01 18:24:48,848 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.19999986052513122
2025-03-01 18:25:00,130 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.20000017245610555
2025-03-01 18:25:12,408 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.2000000574439764
2025-03-01 18:25:24,129 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.19999982368946076
2025-03-01 18:25:32,882 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_9_epoch.pt
2025-03-01 18:25:44,688 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.19999822467565537
2025-03-01 18:25:56,802 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.19999782145023345
2025-03-01 18:26:08,637 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.199998779942592
2025-03-01 18:26:19,420 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.1999994356185198
2025-03-01 18:26:31,053 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.1999999009370804
2025-03-01 18:26:40,272 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_10_epoch.pt
2025-03-01 18:26:53,683 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.19999734327197075
2025-03-01 18:27:05,149 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.2000013817101717
2025-03-01 18:27:16,970 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.19999894777933758
2025-03-01 18:27:27,943 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.20000069487839936
2025-03-01 18:27:38,765 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.20000058352947236
2025-03-01 18:27:47,661 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_11_epoch.pt
2025-03-01 18:27:59,653 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.2000007975101471
2025-03-01 18:28:10,805 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.20000021889805794
2025-03-01 18:28:22,792 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.2000001006325086
2025-03-01 18:28:34,325 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.20000004462897777
2025-03-01 18:28:45,759 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.19999995642900467
2025-03-01 18:28:54,632 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_12_epoch.pt
2025-03-01 18:29:06,271 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.1999995920062065
2025-03-01 18:29:19,168 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.19999966517090798
2025-03-01 18:29:30,176 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.19999919811884562
2025-03-01 18:29:42,076 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.19999420411884786
2025-03-01 18:29:52,975 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.20002126410603524
2025-03-01 18:30:01,544 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_13_epoch.pt
2025-03-01 18:30:14,353 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.2002497798204422
2025-03-01 18:30:25,342 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.20012726843357087
2025-03-01 18:30:37,541 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.20008502215147017
2025-03-01 18:30:49,320 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.20006339363753795
2025-03-01 18:31:00,860 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.20004917708039283
2025-03-01 18:31:09,032 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_14_epoch.pt
2025-03-01 18:31:21,649 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.1999965177476406
2025-03-01 18:31:32,591 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.20001026079058648
2025-03-01 18:31:44,926 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.2000069790085157
2025-03-01 18:31:57,518 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.2000052025914192
2025-03-01 18:32:07,672 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.2000040830373764
2025-03-01 18:32:16,449 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_15_epoch.pt
2025-03-01 18:32:27,871 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.199999862909317
2025-03-01 18:32:39,223 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.2000000861287117
2025-03-01 18:32:50,106 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.20000015666087467
2025-03-01 18:33:01,994 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.200000254586339
2025-03-01 18:33:13,897 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.20000033873319625
2025-03-01 18:33:23,552 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_16_epoch.pt
2025-03-01 18:33:36,424 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.20000017285346985
2025-03-01 18:33:48,114 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.20000014588236809
2025-03-01 18:33:59,903 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.20000031391779582
2025-03-01 18:34:11,826 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.2000002582371235
2025-03-01 18:34:23,250 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.20000021845102312
2025-03-01 18:34:31,356 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_17_epoch.pt
2025-03-01 18:34:43,106 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.20000029921531678
2025-03-01 18:34:55,906 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.2000000908970833
2025-03-01 18:35:06,970 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.20000013530254365
2025-03-01 18:35:18,848 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.2000000700354576
2025-03-01 18:35:30,699 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.2000001779794693
2025-03-01 18:35:38,823 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_18_epoch.pt
2025-03-01 18:35:51,403 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.1999996981024742
2025-03-01 18:36:03,156 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.19999965772032738
2025-03-01 18:36:14,210 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.19999968846638996
2025-03-01 18:36:26,094 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.1999996481463313
2025-03-01 18:36:37,621 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.19999927607178689
2025-03-01 18:36:46,481 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_19_epoch.pt
2025-03-01 18:36:58,619 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.20000120207667352
2025-03-01 18:37:09,718 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.20000119872391223
2025-03-01 18:37:21,520 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.20000048413872717
2025-03-01 18:37:33,047 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.2000004042312503
2025-03-01 18:37:44,308 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.20000043258070946
2025-03-01 18:37:53,949 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_20_epoch.pt
2025-03-01 18:38:06,746 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.20000294253230094
2025-03-01 18:38:17,192 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.20000344082713128
2025-03-01 18:38:29,735 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.19999269381165505
2025-03-01 18:38:41,875 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.20004161272197962
2025-03-01 18:38:52,198 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.1956773805320263
2025-03-01 18:39:01,805 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_21_epoch.pt
2025-03-01 18:39:14,771 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.19453132316470145
2025-03-01 18:39:25,518 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.19693263061344624
2025-03-01 18:39:36,617 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.19679406836628913
2025-03-01 18:39:47,947 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.19789573535323143
2025-03-01 18:39:58,944 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.19828759729862214
2025-03-01 18:40:09,203 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_22_epoch.pt
2025-03-01 18:40:20,506 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.19959831342101098
2025-03-01 18:40:32,805 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.1997929610311985
2025-03-01 18:40:44,040 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.19829689025878905
2025-03-01 18:40:54,969 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.19872712083160876
2025-03-01 18:41:06,734 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.198983071655035
2025-03-01 18:41:16,606 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_23_epoch.pt
2025-03-01 18:41:29,523 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.19999714136123659
2025-03-01 18:41:41,348 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.1999971853941679
2025-03-01 18:41:52,366 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.19999632452925045
2025-03-01 18:42:03,808 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.19999806381762028
2025-03-01 18:42:15,256 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.19999604815244676
2025-03-01 18:42:24,421 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_24_epoch.pt
2025-03-01 18:42:35,513 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.19997118413448334
2025-03-01 18:42:47,402 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.19795816421508788
2025-03-01 18:42:58,506 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.19757659748196602
2025-03-01 18:43:09,890 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.19818411249667406
2025-03-01 18:43:22,399 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.19837602430582046
2025-03-01 18:43:32,104 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_25_epoch.pt
2025-03-01 18:43:43,951 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.19399612098932267
2025-03-01 18:43:55,265 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.1817849887907505
2025-03-01 18:44:06,906 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.18870107620954513
2025-03-01 18:44:18,053 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.1902766451984644
2025-03-01 18:44:29,784 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.18606165804713964
2025-03-01 18:44:38,954 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_26_epoch.pt
2025-03-01 18:44:51,503 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.18039898782968522
2025-03-01 18:45:02,968 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.17801135055720807
2025-03-01 18:45:14,160 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.1764859023441871
2025-03-01 18:45:26,724 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.17695486244745554
2025-03-01 18:45:37,304 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.17752420572191477
2025-03-01 18:45:46,434 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_27_epoch.pt
2025-03-01 18:45:58,649 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.20054940670728683
2025-03-01 18:46:10,309 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.1994605067372322
2025-03-01 18:46:22,065 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.19977277060349782
2025-03-01 18:46:33,137 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.19870471563190223
2025-03-01 18:46:45,177 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.19911183315515518
2025-03-01 18:46:53,856 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_28_epoch.pt
2025-03-01 18:47:05,516 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.2000723710656166
2025-03-01 18:47:16,653 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.1999094510823488
2025-03-01 18:47:28,434 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.19993164663513502
2025-03-01 18:47:39,687 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.19998441636562347
2025-03-01 18:47:52,091 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.19997823590040206
2025-03-01 18:48:01,349 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_29_epoch.pt
2025-03-01 18:48:14,649 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.20006732597947122
2025-03-01 18:48:26,531 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.20001636534929276
2025-03-01 18:48:37,399 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.20001840546727181
2025-03-01 18:48:48,357 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.2000363903492689
2025-03-01 18:48:59,407 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.20002647072076798
2025-03-01 18:49:08,287 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_30_epoch.pt
2025-03-01 18:49:20,366 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.19996020302176476
2025-03-01 18:49:32,533 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.19996523924171924
2025-03-01 18:49:44,037 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.19999101584156354
2025-03-01 18:49:55,860 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.20000593908131123
2025-03-01 18:50:07,810 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.20000759935379028
2025-03-01 18:50:16,312 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_31_epoch.pt
2025-03-01 18:50:26,810 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.2000781325995922
2025-03-01 18:50:38,515 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.19992388725280763
2025-03-01 18:50:50,375 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.20007284556825955
2025-03-01 18:51:02,938 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.20021464239805936
2025-03-01 18:51:13,425 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.20050548359751702
2025-03-01 18:51:23,366 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_32_epoch.pt
2025-03-01 18:51:35,680 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.19700490847229957
2025-03-01 18:51:46,996 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.19927389085292815
2025-03-01 18:51:59,054 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.20074615210294725
2025-03-01 18:52:11,053 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.2014732587710023
2025-03-01 18:52:21,495 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.20084652253985405
2025-03-01 18:52:30,699 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_33_epoch.pt
2025-03-01 18:52:41,803 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.1984658159315586
2025-03-01 18:52:53,075 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.1999339558184147
2025-03-01 18:53:04,089 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.2006091739734014
2025-03-01 18:53:15,702 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.19994646318256856
2025-03-01 18:53:27,487 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.1974124509692192
2025-03-01 18:53:37,207 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_34_epoch.pt
2025-03-01 18:53:48,572 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.17406801246106623
2025-03-01 18:53:59,838 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.1755132519453764
2025-03-01 18:54:10,855 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.1745119286576907
2025-03-01 18:54:23,368 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.17416901587508618
2025-03-01 18:54:35,012 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.1742979745566845
2025-03-01 18:54:43,757 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_35_epoch.pt
2025-03-01 18:54:56,064 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.1720029339939356
2025-03-01 18:55:07,760 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.1740491434559226
2025-03-01 18:55:19,086 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.17410025941828886
2025-03-01 18:55:30,288 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.17456212881952524
2025-03-01 18:55:41,724 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.17433431062102317
2025-03-01 18:55:50,770 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_36_epoch.pt
2025-03-01 18:56:03,472 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.1739271053671837
2025-03-01 18:56:15,107 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.17319892602041365
2025-03-01 18:56:26,658 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.1734834303582708
2025-03-01 18:56:37,593 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.1732612364832312
2025-03-01 18:56:48,899 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.17374759397655726
2025-03-01 18:56:57,849 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_37_epoch.pt
2025-03-01 18:57:10,427 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.17826624527573587
2025-03-01 18:57:21,785 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.19051965981721877
2025-03-01 18:57:33,735 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.19380408143003783
2025-03-01 18:57:44,984 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.1930340114608407
2025-03-01 18:57:56,724 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.19006328619271518
2025-03-01 18:58:05,229 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_38_epoch.pt
2025-03-01 18:58:17,789 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.20007660791277884
2025-03-01 18:58:28,944 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.20001657351851462
2025-03-01 18:58:40,236 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.20002514272928237
2025-03-01 18:58:51,001 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.20002042058855296
2025-03-01 18:59:03,585 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.2000169118642807
2025-03-01 18:59:13,180 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_39_epoch.pt
2025-03-01 18:59:25,044 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.20001521363854408
2025-03-01 18:59:37,347 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.20000600695610046
2025-03-01 18:59:48,223 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.20000844513376553
2025-03-01 18:59:59,394 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.20000795047730208
2025-03-01 19:00:10,539 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.20000964060425758
2025-03-01 19:00:20,528 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_40_epoch.pt
2025-03-01 19:00:32,907 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.20001575037837027
2025-03-01 19:00:44,311 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.20000565983355045
2025-03-01 19:00:55,626 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.19923624912897744
2025-03-01 19:01:06,559 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.1975101319141686
2025-03-01 19:01:17,745 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.1979900679141283
2025-03-01 19:01:27,110 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_41_epoch.pt
2025-03-01 19:01:38,637 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.20002704948186875
2025-03-01 19:01:50,297 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.199970980361104
2025-03-01 19:02:02,122 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.19996449291706087
2025-03-01 19:02:14,175 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.19997118290513755
2025-03-01 19:02:25,959 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.19709566554427146
2025-03-01 19:02:34,480 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_42_epoch.pt
2025-03-01 19:02:47,179 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.18851378157734872
2025-03-01 19:02:59,266 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.1844117919355631
2025-03-01 19:03:10,863 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.1887248523036639
2025-03-01 19:03:21,087 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.1915272642672062
2025-03-01 19:03:33,289 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.19136149240285158
2025-03-01 19:03:42,300 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_43_epoch.pt
2025-03-01 19:03:54,891 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.2002200023829937
2025-03-01 19:04:05,933 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.19987340651452543
2025-03-01 19:04:18,139 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.19957037568092345
2025-03-01 19:04:29,491 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.19911781225353478
2025-03-01 19:04:40,658 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.19881965324282647
2025-03-01 19:04:49,973 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_44_epoch.pt
2025-03-01 19:05:00,570 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.2005572597682476
2025-03-01 19:05:12,642 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.20020130179822446
2025-03-01 19:05:23,979 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.2001010482509931
2025-03-01 19:05:36,375 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.20006543066352606
2025-03-01 19:05:48,249 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.1998321813941002
2025-03-01 19:05:57,135 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_45_epoch.pt
2025-03-01 19:06:08,554 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.2000410781800747
2025-03-01 19:06:21,298 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.1985405722260475
2025-03-01 19:06:31,966 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.19902013669411342
2025-03-01 19:06:43,342 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.19929071109741925
2025-03-01 19:06:54,673 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.1994333238005638
2025-03-01 19:07:04,310 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_46_epoch.pt
2025-03-01 19:07:17,983 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.19985846027731896
2025-03-01 19:07:28,807 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.20009693145751953
2025-03-01 19:07:39,611 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.20001518949866295
2025-03-01 19:07:51,791 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.1999023039266467
2025-03-01 19:08:02,716 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.1999643723964691
2025-03-01 19:08:12,313 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_47_epoch.pt
2025-03-01 19:08:25,360 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.19999575152993201
2025-03-01 19:08:37,469 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.19841460809111594
2025-03-01 19:08:48,718 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.1983442165950934
2025-03-01 19:08:59,524 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.1969235922768712
2025-03-01 19:09:10,430 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.19330824168026448
2025-03-01 19:09:19,357 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_48_epoch.pt
2025-03-01 19:09:32,352 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.182707981467247
2025-03-01 19:09:43,197 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.18290052242577076
2025-03-01 19:09:54,401 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.18680169433355331
2025-03-01 19:10:05,882 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.19008964743465184
2025-03-01 19:10:16,899 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.1920749924480915
2025-03-01 19:10:26,734 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_49_epoch.pt
2025-03-01 19:10:38,422 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.19999210566282272
2025-03-01 19:10:50,241 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.19767697125673295
2025-03-01 19:11:02,470 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.19192526572694382
2025-03-01 19:11:13,327 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.1920219395775348
2025-03-01 19:11:24,255 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.19371433319896458
2025-03-01 19:11:33,976 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_50_epoch.pt
2025-03-01 19:11:46,914 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.200004004240036
2025-03-01 19:11:58,798 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.2000484160333872
2025-03-01 19:12:10,336 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.20005814835429192
2025-03-01 19:12:22,270 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.20007582664489745
2025-03-01 19:12:32,886 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.2001014697253704
2025-03-01 19:12:41,888 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_51_epoch.pt
2025-03-01 19:12:53,926 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.20008161544799805
2025-03-01 19:13:05,564 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.20006713770329954
2025-03-01 19:13:16,998 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.20001523569226265
2025-03-01 19:13:28,879 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.1999803214147687
2025-03-01 19:13:39,840 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.19997518035769463
2025-03-01 19:13:49,753 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_52_epoch.pt
2025-03-01 19:14:02,082 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.1998371036350727
2025-03-01 19:14:12,553 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.20001719154417516
2025-03-01 19:14:23,701 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.200015915731589
2025-03-01 19:14:35,505 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.20003945305943488
2025-03-01 19:14:47,307 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.20007156291604042
2025-03-01 19:14:56,713 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_53_epoch.pt
2025-03-01 19:15:09,809 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.20000405430793763
2025-03-01 19:15:21,022 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.19984065525233746
2025-03-01 19:15:32,386 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.19983322868744532
2025-03-01 19:15:43,022 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.19991716749966146
2025-03-01 19:15:55,295 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.19998168101906777
2025-03-01 19:16:04,576 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_54_epoch.pt
2025-03-01 19:16:16,981 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.19996999368071555
2025-03-01 19:16:28,466 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.2000473640114069
2025-03-01 19:16:39,239 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.19990352054437002
2025-03-01 19:16:51,092 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.19981539528816938
2025-03-01 19:17:02,643 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.19915086072683336
2025-03-01 19:17:12,253 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_55_epoch.pt
2025-03-01 19:17:24,088 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.20094599351286888
2025-03-01 19:17:35,514 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.1997158271074295
2025-03-01 19:17:47,314 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.19995450764894485
2025-03-01 19:17:58,837 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.2000154410675168
2025-03-01 19:18:10,355 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.20001914551854133
2025-03-01 19:18:19,720 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_56_epoch.pt
2025-03-01 19:18:31,864 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.20214143082499503
2025-03-01 19:18:43,434 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.20102884694933892
2025-03-01 19:18:54,569 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.20070593843857448
2025-03-01 19:19:05,661 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.20053471859544517
2025-03-01 19:19:17,523 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.20042349389195444
2025-03-01 19:19:27,290 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_57_epoch.pt
2025-03-01 19:19:39,136 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.1999206717312336
2025-03-01 19:19:50,637 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.19991950117051602
2025-03-01 19:20:02,545 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.19997820019721985
2025-03-01 19:20:13,662 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.1999827391654253
2025-03-01 19:20:25,103 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.19997353145480157
2025-03-01 19:20:34,641 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_58_epoch.pt
2025-03-01 19:20:46,104 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.1966654872149229
2025-03-01 19:20:57,727 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.19832110855728388
2025-03-01 19:21:10,262 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.19886858952542147
2025-03-01 19:21:21,499 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.19918390324339272
2025-03-01 19:21:32,865 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.19936561490595342
2025-03-01 19:21:41,687 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_59_epoch.pt
2025-03-01 19:21:53,481 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.2000863379240036
2025-03-01 19:22:05,408 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.20003189712762834
2025-03-01 19:22:17,079 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.20005042508244514
2025-03-01 19:22:28,299 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.2000464014336467
2025-03-01 19:22:39,717 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.20004051586985588
2025-03-01 19:22:49,808 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_60_epoch.pt
2025-03-01 19:23:02,022 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.20001150235533716
2025-03-01 19:23:12,795 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.20000649064779283
2025-03-01 19:23:24,587 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.20000189860661824
2025-03-01 19:23:35,396 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.19999816037714482
2025-03-01 19:23:47,894 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.1999949997663498
2025-03-01 19:23:57,089 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_61_epoch.pt
2025-03-01 19:24:08,586 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.2000040052831173
2025-03-01 19:24:20,066 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.20002386800944805
2025-03-01 19:24:31,237 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.20001860653360684
2025-03-01 19:24:42,733 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.2000139456614852
2025-03-01 19:24:54,083 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.2000142464339733
2025-03-01 19:25:03,923 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_62_epoch.pt
2025-03-01 19:25:16,055 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.20000154718756677
2025-03-01 19:25:26,578 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.20000010453164577
2025-03-01 19:25:39,949 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.1999964113533497
2025-03-01 19:25:51,506 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.19999361351132394
2025-03-01 19:26:02,377 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.1999868375957012
2025-03-01 19:26:11,769 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_63_epoch.pt
2025-03-01 19:26:24,192 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.19996546864509582
2025-03-01 19:26:36,873 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.19999077901244164
2025-03-01 19:26:48,324 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.20000915244221687
2025-03-01 19:26:58,945 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.20000867996364832
2025-03-01 19:27:10,179 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.20001159423589707
2025-03-01 19:27:19,101 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_64_epoch.pt
2025-03-01 19:27:31,569 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.2000042912364006
2025-03-01 19:27:43,074 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.19999570168554784
2025-03-01 19:27:54,594 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.1999898081521193
2025-03-01 19:28:05,921 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.19998329140245916
2025-03-01 19:28:17,386 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.19999439164996147
2025-03-01 19:28:25,896 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_65_epoch.pt
2025-03-01 19:28:38,229 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.19988669097423553
2025-03-01 19:28:50,204 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.2004442746937275
2025-03-01 19:29:01,885 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.2002918211122354
2025-03-01 19:29:13,138 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.20000461969524622
2025-03-01 19:29:24,807 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.19991359716653823
2025-03-01 19:29:33,864 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_66_epoch.pt
2025-03-01 19:29:46,836 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.1797996439039707
2025-03-01 19:29:57,880 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.17384947711601853
2025-03-01 19:30:09,027 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.17544908902297418
2025-03-01 19:30:20,842 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.17422215087339282
2025-03-01 19:30:31,375 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.17470849911868572
2025-03-01 19:30:41,051 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_67_epoch.pt
2025-03-01 19:30:52,669 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.17670423850417138
2025-03-01 19:31:03,894 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.1705707595869899
2025-03-01 19:31:15,757 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.17158966206014156
2025-03-01 19:31:27,136 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.17258886272087692
2025-03-01 19:31:38,586 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.1732488031089306
2025-03-01 19:31:47,717 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_68_epoch.pt
2025-03-01 19:31:59,466 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.17497815422713756
2025-03-01 19:32:10,176 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.1747366432286799
2025-03-01 19:32:22,258 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.17369199234992266
2025-03-01 19:32:34,469 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.1735786818806082
2025-03-01 19:32:45,910 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.17474347124248743
2025-03-01 19:32:54,935 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_69_epoch.pt
2025-03-01 19:33:07,523 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.17850068986415862
2025-03-01 19:33:18,390 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.17820325685665012
2025-03-01 19:33:29,107 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.18144015981505315
2025-03-01 19:33:40,674 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.18533674349077045
2025-03-01 19:33:52,379 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.18830906478315593
2025-03-01 19:34:02,461 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_70_epoch.pt
2025-03-01 19:34:13,407 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.19917318865656852
2025-03-01 19:34:24,683 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.1933729027211666
2025-03-01 19:34:36,179 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.18706699329117935
2025-03-01 19:34:48,493 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.18872110133990647
2025-03-01 19:35:00,748 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.18790111362934112
2025-03-01 19:35:10,536 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_71_epoch.pt
2025-03-01 19:35:23,010 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.197345160394907
2025-03-01 19:35:33,616 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.19495545096695424
2025-03-01 19:35:46,106 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.1967765724658966
2025-03-01 19:35:57,695 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.19747245747596026
2025-03-01 19:36:09,320 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.19793077141046525
2025-03-01 19:36:17,610 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_72_epoch.pt
2025-03-01 19:36:29,443 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.20145629465579987
2025-03-01 19:36:40,952 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.1972155350446701
2025-03-01 19:36:52,495 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.1916851366062959
2025-03-01 19:37:04,151 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.18645891023799777
2025-03-01 19:37:15,898 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.18408848790824414
2025-03-01 19:37:24,852 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_73_epoch.pt
2025-03-01 19:37:37,322 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.16899356048554182
2025-03-01 19:37:48,316 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.1748769750446081
2025-03-01 19:37:59,585 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.17326090695957344
2025-03-01 19:38:11,679 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.17135769098997117
2025-03-01 19:38:23,015 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.1712023111730814
2025-03-01 19:38:32,079 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_74_epoch.pt
2025-03-01 19:38:43,544 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.17075723946094512
2025-03-01 19:38:55,518 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.1711101358756423
2025-03-01 19:39:07,336 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.17084251080950102
2025-03-01 19:39:19,000 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.1709370704460889
2025-03-01 19:39:30,100 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.17027243695408106
2025-03-01 19:39:39,365 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_75_epoch.pt
2025-03-01 19:39:51,117 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.1704449490457773
2025-03-01 19:40:02,389 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.16844521900638937
2025-03-01 19:40:14,199 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.16934900833914676
2025-03-01 19:40:26,357 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.1706131494883448
2025-03-01 19:40:37,674 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.17046370638161898
2025-03-01 19:40:46,037 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_76_epoch.pt
2025-03-01 19:40:58,329 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.16437960617244243
2025-03-01 19:41:10,827 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.16441506136208772
2025-03-01 19:41:21,528 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.1658923061688741
2025-03-01 19:41:32,890 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.16688804550096392
2025-03-01 19:41:44,894 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.16908776523172855
2025-03-01 19:41:53,683 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_77_epoch.pt
2025-03-01 19:42:06,299 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.17230441324412824
2025-03-01 19:42:17,403 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.1699392506852746
2025-03-01 19:42:29,130 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.1706072475388646
2025-03-01 19:42:40,464 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.16937316081486642
2025-03-01 19:42:52,067 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.16918516396731137
2025-03-01 19:43:00,316 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_78_epoch.pt
2025-03-01 19:43:12,299 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.16316245272755622
2025-03-01 19:43:24,558 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.16465255837887527
2025-03-01 19:43:35,412 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.1650567231575648
2025-03-01 19:43:46,655 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.16647296380251647
2025-03-01 19:43:58,191 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.16749632972478867
2025-03-01 19:44:07,703 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_79_epoch.pt
2025-03-01 19:44:19,960 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.16460791155695914
2025-03-01 19:44:31,716 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.16549867432564497
2025-03-01 19:44:43,342 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.16657300256192684
2025-03-01 19:44:54,522 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.16593506082892417
2025-03-01 19:45:06,274 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.16751681685447692
2025-03-01 19:45:14,520 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_80_epoch.pt
2025-03-01 19:45:26,415 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.1628303048759699
2025-03-01 19:45:37,463 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.162918936945498
2025-03-01 19:45:49,459 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.16483118424812954
2025-03-01 19:46:01,504 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.16316394409164786
2025-03-01 19:46:13,450 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.16357296002656221
2025-03-01 19:46:21,935 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_81_epoch.pt
2025-03-01 19:46:33,861 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.1586641103029251
2025-03-01 19:46:45,595 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.15971905052661894
2025-03-01 19:46:57,461 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.1612120100359122
2025-03-01 19:47:08,790 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.16281570613384247
2025-03-01 19:47:20,274 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.16418971771746874
2025-03-01 19:47:29,397 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_82_epoch.pt
2025-03-01 19:47:42,019 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.16200264945626258
2025-03-01 19:47:53,073 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.16252843772992492
2025-03-01 19:48:04,887 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.16213490231583516
2025-03-01 19:48:16,427 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.1615876297932118
2025-03-01 19:48:27,337 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.16178553084284067
2025-03-01 19:48:37,039 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_83_epoch.pt
2025-03-01 19:48:47,946 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.17282365385442972
2025-03-01 19:48:59,539 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.1780870353244245
2025-03-01 19:49:11,211 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.1759892706697186
2025-03-01 19:49:22,835 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.17455224984325468
2025-03-01 19:49:34,652 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.17320020322501659
2025-03-01 19:49:44,520 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_84_epoch.pt
2025-03-01 19:49:56,889 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.16156940206885337
2025-03-01 19:50:09,059 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.16368762934580444
2025-03-01 19:50:20,315 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.16601656251897415
2025-03-01 19:50:30,901 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.16492015809752048
2025-03-01 19:50:41,845 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.16270718685537577
2025-03-01 19:50:51,567 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_85_epoch.pt
2025-03-01 19:51:03,332 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.1636353497952223
2025-03-01 19:51:15,134 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.16086555894464255
2025-03-01 19:51:26,418 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.15909712366759776
2025-03-01 19:51:37,335 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.1595607392117381
2025-03-01 19:51:50,187 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.15980102251470088
2025-03-01 19:51:58,712 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_86_epoch.pt
2025-03-01 19:52:10,023 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.15702959440648556
2025-03-01 19:52:21,985 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.15862242095172405
2025-03-01 19:52:33,346 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.15860318516691527
2025-03-01 19:52:44,780 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.15855139282532035
2025-03-01 19:52:56,916 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.16168531595915556
2025-03-01 19:53:05,930 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_87_epoch.pt
2025-03-01 19:53:17,808 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.16803306847810745
2025-03-01 19:53:29,136 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.16500868663191795
2025-03-01 19:53:40,939 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.16329436793923377
2025-03-01 19:53:52,032 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.16229090001434088
2025-03-01 19:54:02,747 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.1614102251678705
2025-03-01 19:54:12,411 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_88_epoch.pt
2025-03-01 19:54:24,303 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.16416747830808162
2025-03-01 19:54:35,656 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.16004450228065253
2025-03-01 19:54:47,496 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.1604890041301648
2025-03-01 19:54:58,700 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.16218699271790682
2025-03-01 19:55:10,948 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.16136483480781316
2025-03-01 19:55:20,047 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_89_epoch.pt
2025-03-01 19:55:31,814 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.1711150925606489
2025-03-01 19:55:43,553 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.1661697737313807
2025-03-01 19:55:54,490 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.1623031570141514
2025-03-01 19:56:07,046 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.16142827746458352
2025-03-01 19:56:18,741 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.16011296973377467
2025-03-01 19:56:27,752 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_90_epoch.pt
2025-03-01 19:56:39,636 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.15644144766032697
2025-03-01 19:56:51,457 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.15272625965997577
2025-03-01 19:57:03,140 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.15502229696760575
2025-03-01 19:57:14,476 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.1564338593557477
2025-03-01 19:57:26,079 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.1571361380815506
2025-03-01 19:57:35,168 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_91_epoch.pt
2025-03-01 19:57:47,262 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.15607306592166423
2025-03-01 19:57:58,444 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.1539211617782712
2025-03-01 19:58:09,939 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.15666178750495116
2025-03-01 19:58:21,706 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.1557616580836475
2025-03-01 19:58:33,461 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.15496716333925725
2025-03-01 19:58:42,229 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_92_epoch.pt
2025-03-01 19:58:54,513 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.15457008831202984
2025-03-01 19:59:05,722 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.15758482053875922
2025-03-01 19:59:17,568 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.15573823682963847
2025-03-01 19:59:29,744 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.15591095886193215
2025-03-01 19:59:40,506 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.15631436919420957
2025-03-01 19:59:49,944 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_93_epoch.pt
2025-03-01 20:00:02,046 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.16583930082619192
2025-03-01 20:00:14,448 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.15702343598008156
2025-03-01 20:00:25,420 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.1555638363957405
2025-03-01 20:00:36,619 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.15643558684736492
2025-03-01 20:00:47,713 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.15612630808353425
2025-03-01 20:00:56,985 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_94_epoch.pt
2025-03-01 20:01:09,036 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.15353427343070508
2025-03-01 20:01:20,915 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.1538952559232712
2025-03-01 20:01:32,616 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.1537863312413295
2025-03-01 20:01:42,987 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.15180706986226142
2025-03-01 20:01:54,384 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.1518390026912093
2025-03-01 20:02:03,555 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_95_epoch.pt
2025-03-01 20:02:15,033 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.1579307010024786
2025-03-01 20:02:27,758 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.15570174496620892
2025-03-01 20:02:38,607 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.15608392671992383
2025-03-01 20:02:50,242 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.1548782190401107
2025-03-01 20:03:01,513 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.15443046412616968
2025-03-01 20:03:10,692 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_96_epoch.pt
2025-03-01 20:03:22,956 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.16141691170632838
2025-03-01 20:03:34,376 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.15445088494569062
2025-03-01 20:03:45,488 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.15569394210974374
2025-03-01 20:03:56,225 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.15450874607078732
2025-03-01 20:04:09,040 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.15360313522070645
2025-03-01 20:04:18,103 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_97_epoch.pt
2025-03-01 20:04:30,317 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.1531808727234602
2025-03-01 20:04:41,150 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.1519601207599044
2025-03-01 20:04:51,989 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.15254722774028778
2025-03-01 20:05:04,043 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.15291071020066738
2025-03-01 20:05:16,037 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.15247361977398396
2025-03-01 20:05:24,849 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_98_epoch.pt
2025-03-01 20:05:36,714 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.14898830994963647
2025-03-01 20:05:48,494 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.14769301872700452
2025-03-01 20:06:00,610 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.14817514665424825
2025-03-01 20:06:11,872 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.14836643229238689
2025-03-01 20:06:23,828 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.14885451938956976
2025-03-01 20:06:32,412 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_99_epoch.pt
2025-03-01 20:06:45,208 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.14308712016791106
2025-03-01 20:06:55,963 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.1482539059780538
2025-03-01 20:07:07,445 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.14877277214080095
2025-03-01 20:07:19,012 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.14968864261172712
2025-03-01 20:07:30,857 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.14835409270972014
2025-03-01 20:07:53,040 : 916236790.py : __call__ : INFO : loaded dataset, sample = 1158
2025-03-01 20:07:53,041 : 1341934384.py : train_scp : INFO : 109483778
2025-03-01 20:08:05,425 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.20002094373106957
2025-03-01 20:08:17,116 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.2000104420632124
2025-03-01 20:08:29,166 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.2000069603820642
2025-03-01 20:08:39,792 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.20000520177185535
2025-03-01 20:08:51,124 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.20000416028499604
2025-03-01 20:09:00,566 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_1_epoch.pt
2025-03-01 20:09:12,867 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.2019717700779438
2025-03-01 20:09:24,343 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.20098057076334952
2025-03-01 20:09:36,575 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.20065275554855663
2025-03-01 20:09:48,191 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.2004868644848466
2025-03-01 20:09:59,296 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.20037327700853347
2025-03-01 20:10:08,392 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_2_epoch.pt
2025-03-01 20:10:19,869 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.1999935945868492
2025-03-01 20:10:31,669 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.20000186413526536
2025-03-01 20:10:43,029 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.1999992665151755
2025-03-01 20:10:55,398 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.20000026162713766
2025-03-01 20:11:05,961 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.19999804091453552
2025-03-01 20:11:15,991 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_3_epoch.pt
2025-03-01 20:11:28,918 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.2000022952258587
2025-03-01 20:11:39,867 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.19998535841703416
2025-03-01 20:11:51,122 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.20002179438869158
2025-03-01 20:12:02,863 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.20001492682844402
2025-03-01 20:12:14,477 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.2000135512948036
2025-03-01 20:12:23,760 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_4_epoch.pt
2025-03-01 20:13:16,247 : 916236790.py : __call__ : INFO : loaded dataset, sample = 1158
2025-03-01 20:13:16,248 : 1341934384.py : train_scp : INFO : 109483778
2025-03-01 20:13:27,667 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.20004116147756576
2025-03-01 20:13:39,828 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.19976021744310857
2025-03-01 20:13:51,191 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.20034535134832065
2025-03-01 20:14:02,597 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.20087261151522398
2025-03-01 20:14:14,015 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.20060124069452287
2025-03-01 20:14:23,771 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_1_epoch.pt
2025-03-01 20:14:36,485 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.2000070632994175
2025-03-01 20:14:47,937 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.19999553754925728
2025-03-01 20:14:59,877 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.19999152600765227
2025-03-01 20:15:11,153 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.19999196119606494
2025-03-01 20:15:23,058 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.19988991102576256
2025-03-01 20:15:32,172 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_2_epoch.pt
2025-03-01 20:15:44,679 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.19994185075163842
2025-03-01 20:15:55,392 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.1999816209822893
2025-03-01 20:16:06,039 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.199977821658055
2025-03-01 20:16:18,230 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.2000240010023117
2025-03-01 20:16:29,483 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.20001318964362144
2025-03-01 20:16:39,261 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_3_epoch.pt
2025-03-01 20:16:51,638 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.19999198406934737
2025-03-01 20:17:03,208 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.19998633198440074
2025-03-01 20:17:14,921 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.19998617192109425
2025-03-01 20:17:26,686 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.1999866185337305
2025-03-01 20:17:37,552 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.19998414278030396
2025-03-01 20:17:47,638 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_4_epoch.pt
2025-03-01 20:18:00,466 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.19999734401702882
2025-03-01 20:18:12,290 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.19998578116297722
2025-03-01 20:18:23,644 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.1992418979605039
2025-03-01 20:18:34,532 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.19864509107545017
2025-03-01 20:18:46,413 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.1989031540006399
2025-03-01 20:18:55,372 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_5_epoch.pt
2025-03-01 20:19:08,254 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.1999979403614998
2025-03-01 20:19:19,280 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.1997161689400673
2025-03-01 20:19:30,593 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.1998033501704534
2025-03-01 20:19:41,846 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.19984026830643414
2025-03-01 20:19:53,421 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.19988272076845168
2025-03-01 20:20:02,819 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_6_epoch.pt
2025-03-01 20:20:15,095 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.19968019872903825
2025-03-01 20:20:27,291 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.19930068373680115
2025-03-01 20:20:39,129 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.19991816038886706
2025-03-01 20:20:51,091 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.19993687938898802
2025-03-01 20:21:01,780 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.19994760328531266
2025-03-01 20:21:11,036 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_7_epoch.pt
2025-03-01 20:21:23,478 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.20244558468461038
2025-03-01 20:21:35,717 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.20096707165241243
2025-03-01 20:21:45,779 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.20047105277578037
2025-03-01 20:21:57,817 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.2003412739187479
2025-03-01 20:22:10,511 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.20027481964230537
2025-03-01 20:22:19,014 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_8_epoch.pt
2025-03-01 20:22:31,205 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.19996751189231873
2025-03-01 20:22:41,936 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.19995846800506115
2025-03-01 20:22:53,525 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.1996282128492991
2025-03-01 20:23:04,707 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.19971340090036394
2025-03-01 20:23:16,492 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.19965599447488785
2025-03-01 20:23:26,436 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_9_epoch.pt
2025-03-01 20:23:38,208 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.2000224043428898
2025-03-01 20:23:48,632 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.20000318013131618
2025-03-01 20:24:00,631 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.2000081330537796
2025-03-01 20:24:12,936 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.199621812235564
2025-03-01 20:24:25,252 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.19949753610789775
2025-03-01 20:24:34,352 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_10_epoch.pt
2025-03-01 20:24:47,046 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.19999009743332863
2025-03-01 20:24:57,982 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.1999781334400177
2025-03-01 20:25:09,558 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.19998362690210342
2025-03-01 20:25:21,251 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.19969504147768022
2025-03-01 20:25:32,511 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.19956590715050698
2025-03-01 20:25:41,935 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_11_epoch.pt
2025-03-01 20:25:54,064 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.20066262200474738
2025-03-01 20:26:06,115 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.20010177448391914
2025-03-01 20:26:18,344 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.2000504078467687
2025-03-01 20:26:29,302 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.20001399785280227
2025-03-01 20:26:40,078 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.2000226655602455
2025-03-01 20:26:49,292 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_12_epoch.pt
2025-03-01 20:27:01,399 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.19929661571979523
2025-03-01 20:27:12,971 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.19965317837893962
2025-03-01 20:27:26,107 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.19975894555449486
2025-03-01 20:27:37,238 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.19978982161730527
2025-03-01 20:27:48,164 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.19981649687886238
2025-03-01 20:27:56,916 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_13_epoch.pt
2025-03-01 20:28:09,063 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.19994617938995363
2025-03-01 20:28:21,179 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.2007021814584732
2025-03-01 20:28:32,245 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.19968985818326473
2025-03-01 20:28:43,173 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.1993371981009841
2025-03-01 20:28:54,393 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.1994962512999773
2025-03-01 20:29:04,440 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_14_epoch.pt
2025-03-01 20:29:17,320 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.20172909826040267
2025-03-01 20:29:28,485 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.19832302566617727
2025-03-01 20:29:40,330 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.19671157409747442
2025-03-01 20:29:52,080 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.19030514509417118
2025-03-01 20:30:03,284 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.18356944920867682
2025-03-01 20:30:12,233 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_15_epoch.pt
2025-03-01 20:30:24,212 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.15487426720559597
2025-03-01 20:30:35,560 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.149414677657187
2025-03-01 20:30:47,327 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.14727906780938307
2025-03-01 20:30:59,682 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.14334530589170755
2025-03-01 20:31:10,438 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.1375601918771863
2025-03-01 20:31:19,981 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_16_epoch.pt
2025-03-01 20:31:32,318 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.08933987699449063
2025-03-01 20:31:43,605 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.08503026582300663
2025-03-01 20:31:55,190 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.0839344421774149
2025-03-01 20:32:06,520 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.08374745775945484
2025-03-01 20:32:17,608 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.08045654577016831
2025-03-01 20:32:26,783 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_17_epoch.pt
2025-03-01 20:32:38,852 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.06533121343702078
2025-03-01 20:32:50,111 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.07382197665050626
2025-03-01 20:33:02,030 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.07796330956121286
2025-03-01 20:33:13,318 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.07808548053726554
2025-03-01 20:33:24,148 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.07722588804364204
2025-03-01 20:33:33,308 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_18_epoch.pt
2025-03-01 20:33:45,070 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.0693739590421319
2025-03-01 20:33:56,414 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.0671174370124936
2025-03-01 20:34:08,766 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.066016254487137
2025-03-01 20:34:20,152 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.06739243327639997
2025-03-01 20:34:31,023 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.06665487131476402
2025-03-01 20:34:40,170 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_19_epoch.pt
2025-03-01 20:34:52,402 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.066738849170506
2025-03-01 20:35:03,562 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.06336650084704161
2025-03-01 20:35:15,118 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.06535297457128764
2025-03-01 20:35:27,024 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.06166763555258512
2025-03-01 20:35:38,303 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.06340646017342806
2025-03-01 20:35:47,330 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_20_epoch.pt
2025-03-01 20:35:58,836 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.06257278122007846
2025-03-01 20:36:10,909 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.06547337427735328
2025-03-01 20:36:22,261 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.06774832806239525
2025-03-01 20:36:33,966 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.06477304242551327
2025-03-01 20:36:44,533 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.06547539118677377
2025-03-01 20:36:54,294 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_21_epoch.pt
2025-03-01 20:37:06,314 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.0635438534244895
2025-03-01 20:37:18,113 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.05695351053029299
2025-03-01 20:37:29,697 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.05743987111995617
2025-03-01 20:37:40,184 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.05826532242819667
2025-03-01 20:37:51,992 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.05714326490461826
2025-03-01 20:38:01,487 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_22_epoch.pt
2025-03-01 20:38:13,722 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.05793734483420849
2025-03-01 20:38:25,049 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.0615941197425127
2025-03-01 20:38:36,270 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.0598410706842939
2025-03-01 20:38:48,226 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.06116768744774163
2025-03-01 20:38:59,187 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.06164014682173729
2025-03-01 20:39:08,277 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_23_epoch.pt
2025-03-01 20:39:20,185 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.059474803544580936
2025-03-01 20:39:31,728 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.05759288551285863
2025-03-01 20:39:42,367 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.05817429353793462
2025-03-01 20:39:53,392 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.05833698897622526
2025-03-01 20:40:04,992 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.05758749960362911
2025-03-01 20:40:14,670 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_24_epoch.pt
2025-03-01 20:40:25,895 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.05670077115297317
2025-03-01 20:40:38,231 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.06352942334488035
2025-03-01 20:40:49,204 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.06017475588868062
2025-03-01 20:41:00,076 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.056481593316420914
2025-03-01 20:41:11,992 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.05682734629511833
2025-03-01 20:41:21,453 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_25_epoch.pt
2025-03-01 20:41:34,213 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.07160459887236356
2025-03-01 20:41:46,005 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.059966905899345876
2025-03-01 20:41:57,531 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.055666592344641684
2025-03-01 20:42:08,278 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.05600305972620845
2025-03-01 20:42:19,195 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.05446470890194178
2025-03-01 20:42:28,084 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_26_epoch.pt
2025-03-01 20:42:39,702 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.05783761564642191
2025-03-01 20:42:51,282 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.05784515008330345
2025-03-01 20:43:02,794 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.05638217102736234
2025-03-01 20:43:13,962 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.055121528692543505
2025-03-01 20:43:25,568 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.0561066829264164
2025-03-01 20:43:35,380 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_27_epoch.pt
2025-03-01 20:43:48,468 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.050783265754580496
2025-03-01 20:44:00,043 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04952578356489539
2025-03-01 20:44:11,884 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04880477697898944
2025-03-01 20:44:22,651 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.0499886610545218
2025-03-01 20:44:33,961 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.05094909183681011
2025-03-01 20:44:42,679 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_28_epoch.pt
2025-03-01 20:44:53,801 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.06801983766257763
2025-03-01 20:45:05,634 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.06082713637501001
2025-03-01 20:45:16,636 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.05605595864355564
2025-03-01 20:45:28,884 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.05436350702308118
2025-03-01 20:45:39,787 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.05470372354239225
2025-03-01 20:45:49,611 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_29_epoch.pt
2025-03-01 20:46:01,445 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.05502383291721344
2025-03-01 20:46:12,324 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.05327983222901821
2025-03-01 20:46:24,363 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.05471539938201507
2025-03-01 20:46:35,885 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.05613770510070026
2025-03-01 20:46:47,360 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.05356658936291933
2025-03-01 20:46:56,209 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_30_epoch.pt
2025-03-01 20:47:08,346 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.049708932414650915
2025-03-01 20:47:19,263 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.05293000148609281
2025-03-01 20:47:31,124 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.05113689996302128
2025-03-01 20:47:41,749 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.05196251911111176
2025-03-01 20:47:53,170 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.05404675451666117
2025-03-01 20:48:03,054 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_31_epoch.pt
2025-03-01 20:48:15,490 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.049628385491669175
2025-03-01 20:48:26,351 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.05002270985394716
2025-03-01 20:48:38,457 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.0529697339485089
2025-03-01 20:48:49,983 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.054007975179702045
2025-03-01 20:49:00,882 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.05317180975526571
2025-03-01 20:49:09,717 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_32_epoch.pt
2025-03-01 20:49:21,766 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.0384218917414546
2025-03-01 20:49:33,363 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04583260245621204
2025-03-01 20:49:44,547 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.048902361566821734
2025-03-01 20:49:55,697 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04826239800080657
2025-03-01 20:50:06,945 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04779242265969515
2025-03-01 20:50:16,558 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_33_epoch.pt
2025-03-01 20:50:28,324 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04855383422225714
2025-03-01 20:50:39,643 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04884025717154145
2025-03-01 20:50:51,318 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.049751192567249136
2025-03-01 20:51:02,653 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.05128346433863044
2025-03-01 20:51:14,268 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.052600873969495296
2025-03-01 20:51:23,516 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_34_epoch.pt
2025-03-01 20:51:34,696 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04254665099084377
2025-03-01 20:51:46,058 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04330223409458995
2025-03-01 20:51:58,096 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04341155992199977
2025-03-01 20:52:09,554 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.0462563746701926
2025-03-01 20:52:20,489 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04804927416145802
2025-03-01 20:52:29,571 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_35_epoch.pt
2025-03-01 20:52:41,030 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.05635196801275015
2025-03-01 20:52:53,274 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.058538419250398877
2025-03-01 20:53:05,156 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.05569413252174854
2025-03-01 20:53:16,706 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.05526819796301424
2025-03-01 20:53:28,039 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.053430981487035754
2025-03-01 20:53:36,758 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_36_epoch.pt
2025-03-01 20:53:48,774 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.053333162032067775
2025-03-01 20:54:00,654 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04963187702000141
2025-03-01 20:54:11,651 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.047886942562957606
2025-03-01 20:54:22,783 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04959414327517152
2025-03-01 20:54:33,996 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04842736238241196
2025-03-01 20:54:42,774 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_37_epoch.pt
2025-03-01 20:54:55,013 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04351550955325365
2025-03-01 20:55:06,998 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04302356522530317
2025-03-01 20:55:18,841 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.047810892077783745
2025-03-01 20:55:29,229 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.05089123534038663
2025-03-01 20:55:41,139 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.05007725886255503
2025-03-01 20:55:49,619 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_38_epoch.pt
2025-03-01 20:56:01,158 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.05037433430552483
2025-03-01 20:56:13,006 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.047981834523379806
2025-03-01 20:56:24,963 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04804479864736398
2025-03-01 20:56:36,138 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.05035400745458901
2025-03-01 20:56:47,087 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.049719852432608604
2025-03-01 20:56:56,128 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_39_epoch.pt
2025-03-01 20:57:07,823 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.040137652307748795
2025-03-01 20:57:19,447 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.042670436389744285
2025-03-01 20:57:31,620 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04651682529598475
2025-03-01 20:57:42,510 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.045811695605516435
2025-03-01 20:57:53,792 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04797175439447165
2025-03-01 20:58:03,136 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_40_epoch.pt
2025-03-01 20:58:15,722 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.046681554056704044
2025-03-01 20:58:26,861 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04871601464226842
2025-03-01 20:58:37,841 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.049515581044058
2025-03-01 20:58:49,841 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.049399668388068675
2025-03-01 20:59:01,012 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.050602048337459564
2025-03-01 20:59:09,594 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_41_epoch.pt
2025-03-01 20:59:20,786 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03999008506536484
2025-03-01 20:59:31,969 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.0444428195618093
2025-03-01 20:59:43,488 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.045541484169662
2025-03-01 20:59:55,076 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04484963310882449
2025-03-01 21:00:07,064 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.0459085303992033
2025-03-01 21:00:16,573 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_42_epoch.pt
2025-03-01 21:00:28,255 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04602298248559236
2025-03-01 21:00:38,026 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04476242063567042
2025-03-01 21:00:50,011 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04732518697778384
2025-03-01 21:01:01,780 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04815329981036484
2025-03-01 21:01:14,034 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.050699472025036815
2025-03-01 21:01:23,106 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_43_epoch.pt
2025-03-01 21:01:34,937 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04913207672536373
2025-03-01 21:01:46,838 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.05166301209479571
2025-03-01 21:01:57,079 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.050317504753669105
2025-03-01 21:02:09,005 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.052051539039239286
2025-03-01 21:02:20,911 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.05136838412284851
2025-03-01 21:02:29,945 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_44_epoch.pt
2025-03-01 21:02:42,172 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04215246506035328
2025-03-01 21:02:53,496 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04139816293492913
2025-03-01 21:03:05,395 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04473182234913111
2025-03-01 21:03:17,201 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04396832681261003
2025-03-01 21:03:27,750 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04444624681025743
2025-03-01 21:03:36,429 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_45_epoch.pt
2025-03-01 21:03:48,288 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.051111855283379554
2025-03-01 21:03:59,532 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.05326104113832116
2025-03-01 21:04:10,934 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.05266599496205648
2025-03-01 21:04:22,739 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.052796414345502855
2025-03-01 21:04:33,135 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.05012686525285244
2025-03-01 21:04:43,211 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_46_epoch.pt
2025-03-01 21:04:55,025 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.051026574112474916
2025-03-01 21:05:06,373 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.05184554476290941
2025-03-01 21:05:17,707 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04749245231350263
2025-03-01 21:05:28,934 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04743692901916802
2025-03-01 21:05:40,235 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04767449352145195
2025-03-01 21:05:49,528 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_47_epoch.pt
2025-03-01 21:06:01,824 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04547633957117796
2025-03-01 21:06:12,878 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04418847898021341
2025-03-01 21:06:24,536 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04510356597602368
2025-03-01 21:06:35,978 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04551550985313952
2025-03-01 21:06:46,920 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04514653830975294
2025-03-01 21:06:56,814 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_48_epoch.pt
2025-03-01 21:07:09,966 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.045099496245384216
2025-03-01 21:07:20,868 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.047271276991814375
2025-03-01 21:07:32,420 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04775381773710251
2025-03-01 21:07:43,558 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04753004181198776
2025-03-01 21:07:54,859 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04715425231307745
2025-03-01 21:08:03,445 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_49_epoch.pt
2025-03-01 21:08:15,747 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04821736339479685
2025-03-01 21:08:26,635 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04729792200028896
2025-03-01 21:08:37,846 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.047965347295006115
2025-03-01 21:08:49,681 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.048341395892202856
2025-03-01 21:09:01,058 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.047402644738554954
2025-03-01 21:09:10,115 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_50_epoch.pt
2025-03-01 21:09:20,920 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.0487503544613719
2025-03-01 21:09:33,293 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.047696770075708626
2025-03-01 21:09:44,842 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04680947454025348
2025-03-01 21:09:56,634 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04442506681196392
2025-03-01 21:10:07,543 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.043790796890854834
2025-03-01 21:10:16,615 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_51_epoch.pt
2025-03-01 21:10:29,162 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04381627704948187
2025-03-01 21:10:40,695 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.0459055614285171
2025-03-01 21:10:52,236 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04528524314363797
2025-03-01 21:11:02,850 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04357241475954652
2025-03-01 21:11:14,541 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04481501869112253
2025-03-01 21:11:23,443 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_52_epoch.pt
2025-03-01 21:11:35,182 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.041340965628623966
2025-03-01 21:11:47,034 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.043071474302560094
2025-03-01 21:11:57,727 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.047290443082650506
2025-03-01 21:12:08,916 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.047328490512445566
2025-03-01 21:12:20,259 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04791734530776739
2025-03-01 21:12:29,833 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_53_epoch.pt
2025-03-01 21:12:41,845 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.041568950228393076
2025-03-01 21:12:52,809 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04603210618719458
2025-03-01 21:13:03,781 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04633118027200302
2025-03-01 21:13:15,720 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04463335824199021
2025-03-01 21:13:26,576 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.044985302969813346
2025-03-01 21:13:37,131 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_54_epoch.pt
2025-03-01 21:13:50,276 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03854189474135637
2025-03-01 21:14:00,990 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04342087859287858
2025-03-01 21:14:12,330 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.046120420383910336
2025-03-01 21:14:23,641 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.045086986804381016
2025-03-01 21:14:34,680 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.045481529913842675
2025-03-01 21:14:44,037 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_55_epoch.pt
2025-03-01 21:14:56,750 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.050979372449219226
2025-03-01 21:15:08,201 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.0467217904701829
2025-03-01 21:15:19,667 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04337502226233483
2025-03-01 21:15:30,925 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.042754699168726804
2025-03-01 21:15:41,408 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04339354187250137
2025-03-01 21:15:50,951 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_56_epoch.pt
2025-03-01 21:16:03,030 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.0464548647031188
2025-03-01 21:16:13,458 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.047829434555023906
2025-03-01 21:16:24,758 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04291686793168386
2025-03-01 21:16:36,261 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04313068580813706
2025-03-01 21:16:48,539 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04464773627370596
2025-03-01 21:16:57,389 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_57_epoch.pt
2025-03-01 21:17:10,048 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.0458170048892498
2025-03-01 21:17:21,622 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04264335434883833
2025-03-01 21:17:33,855 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04158372857918342
2025-03-01 21:17:44,560 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04448976931162178
2025-03-01 21:17:55,778 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04497168733179569
2025-03-01 21:18:04,546 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_58_epoch.pt
2025-03-01 21:18:17,790 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.06031220406293869
2025-03-01 21:18:28,330 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.0501693245023489
2025-03-01 21:18:39,556 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04769576696058114
2025-03-01 21:18:51,523 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04633562869392335
2025-03-01 21:19:03,163 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.044146024234592914
2025-03-01 21:19:12,297 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_59_epoch.pt
2025-03-01 21:19:25,873 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04841405384242534
2025-03-01 21:19:37,170 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04581629009917378
2025-03-01 21:19:48,688 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04461683212469021
2025-03-01 21:19:59,870 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04317139963619411
2025-03-01 21:20:11,887 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04366827119886875
2025-03-01 21:20:20,579 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_60_epoch.pt
2025-03-01 21:20:33,606 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.045941256284713745
2025-03-01 21:20:44,749 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.0438750827498734
2025-03-01 21:20:55,863 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.043594395567973454
2025-03-01 21:21:07,044 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.043619382418692114
2025-03-01 21:21:18,919 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.0444210028424859
2025-03-01 21:21:28,373 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_61_epoch.pt
2025-03-01 21:21:41,944 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04936284240335226
2025-03-01 21:21:53,162 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04763017052784562
2025-03-01 21:22:04,228 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.045758360673983894
2025-03-01 21:22:15,790 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04538040585815906
2025-03-01 21:22:27,814 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04686747251451016
2025-03-01 21:22:36,737 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_62_epoch.pt
2025-03-01 21:22:48,713 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04242813300341368
2025-03-01 21:22:59,865 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.041048889122903345
2025-03-01 21:23:11,519 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04412658140063286
2025-03-01 21:23:22,714 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.046616995902732015
2025-03-01 21:23:34,379 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.0463623221218586
2025-03-01 21:23:43,937 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_63_epoch.pt
2025-03-01 21:23:56,264 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04722514867782593
2025-03-01 21:24:07,661 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04417810654267669
2025-03-01 21:24:18,636 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04623985710243384
2025-03-01 21:24:30,871 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04655623779632151
2025-03-01 21:24:42,841 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04640927311033011
2025-03-01 21:24:51,002 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_64_epoch.pt
2025-03-01 21:25:02,582 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04519277263432741
2025-03-01 21:25:14,112 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.042362974938005206
2025-03-01 21:25:24,795 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.0446069490040342
2025-03-01 21:25:36,666 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.0470660447422415
2025-03-01 21:25:48,499 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.0458846455141902
2025-03-01 21:25:57,723 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_65_epoch.pt
2025-03-01 21:26:09,521 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.0566946941241622
2025-03-01 21:26:19,836 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.049693208429962395
2025-03-01 21:26:31,791 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04945684586962064
2025-03-01 21:26:43,538 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04682144750840962
2025-03-01 21:26:55,165 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04670448190718889
2025-03-01 21:27:04,406 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_66_epoch.pt
2025-03-01 21:27:17,575 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04262783862650395
2025-03-01 21:27:28,658 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04295955024659634
2025-03-01 21:27:39,849 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.044230112843215465
2025-03-01 21:27:51,779 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04491817188449204
2025-03-01 21:28:03,047 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04549462766200304
2025-03-01 21:28:12,112 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_67_epoch.pt
2025-03-01 21:28:24,212 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.054649016596376894
2025-03-01 21:28:35,674 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04536604437977076
2025-03-01 21:28:47,187 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04425928788880507
2025-03-01 21:28:58,555 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04240811295807362
2025-03-01 21:29:09,273 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.042761735379695895
2025-03-01 21:29:18,348 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_68_epoch.pt
2025-03-01 21:29:29,674 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.0389691224694252
2025-03-01 21:29:40,496 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04095348834991455
2025-03-01 21:29:52,189 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04193751388539871
2025-03-01 21:30:03,797 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04217815459705889
2025-03-01 21:30:15,162 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.042497861586511136
2025-03-01 21:30:24,794 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_69_epoch.pt
2025-03-01 21:30:37,414 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.049758212193846706
2025-03-01 21:30:48,342 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04794561615213752
2025-03-01 21:31:00,430 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04869951542466879
2025-03-01 21:31:11,388 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.0466435283049941
2025-03-01 21:31:22,188 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04630678009986877
2025-03-01 21:31:31,431 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_70_epoch.pt
2025-03-01 21:31:44,102 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.046033805795013906
2025-03-01 21:31:55,216 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04212536597624421
2025-03-01 21:32:06,039 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04171128161251545
2025-03-01 21:32:16,984 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04478662167675793
2025-03-01 21:32:28,567 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.043729882813990116
2025-03-01 21:32:38,522 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_71_epoch.pt
2025-03-01 21:32:50,603 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04398476306349039
2025-03-01 21:33:01,875 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04893915578722954
2025-03-01 21:33:13,563 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.05002344037095706
2025-03-01 21:33:24,573 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04776802551932633
2025-03-01 21:33:35,611 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04775263047963381
2025-03-01 21:33:44,833 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_72_epoch.pt
2025-03-01 21:33:57,255 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.0438917950168252
2025-03-01 21:34:08,718 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04525036571547389
2025-03-01 21:34:20,293 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04783448015650114
2025-03-01 21:34:32,072 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04780944345518947
2025-03-01 21:34:43,550 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.046954727314412595
2025-03-01 21:34:51,413 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_73_epoch.pt
2025-03-01 21:35:03,464 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03579630978405476
2025-03-01 21:35:15,583 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.036557721123099324
2025-03-01 21:35:25,802 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.03946574122955402
2025-03-01 21:35:37,174 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04325795412063599
2025-03-01 21:35:49,500 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04509933371841907
2025-03-01 21:35:58,306 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_74_epoch.pt
2025-03-01 21:36:10,220 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.041238906346261504
2025-03-01 21:36:21,613 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.045086297951638696
2025-03-01 21:36:34,498 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.044077498018741605
2025-03-01 21:36:45,100 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.044482389306649564
2025-03-01 21:36:55,590 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.045583582304418085
2025-03-01 21:37:05,000 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_75_epoch.pt
2025-03-01 21:37:17,048 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.041562941521406174
2025-03-01 21:37:28,405 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.042364791557192805
2025-03-01 21:37:39,731 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04488845800360044
2025-03-01 21:37:50,442 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.044361673071980474
2025-03-01 21:38:02,384 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04498120366036892
2025-03-01 21:38:12,008 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_76_epoch.pt
2025-03-01 21:38:23,535 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04334544472396374
2025-03-01 21:38:34,564 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04167777854949236
2025-03-01 21:38:46,106 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04221772633492946
2025-03-01 21:38:57,973 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04457285697571933
2025-03-01 21:39:10,099 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.044012483201920985
2025-03-01 21:39:18,623 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_77_epoch.pt
2025-03-01 21:39:30,620 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.040567360967397686
2025-03-01 21:39:42,743 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04461913626641035
2025-03-01 21:39:55,089 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04393029144654671
2025-03-01 21:40:06,253 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.044975420059636236
2025-03-01 21:40:16,481 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04456634259968996
2025-03-01 21:40:25,522 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_78_epoch.pt
2025-03-01 21:40:38,824 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.05093480158597231
2025-03-01 21:40:50,538 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04723546238616109
2025-03-01 21:41:01,641 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04339168823013703
2025-03-01 21:41:12,690 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.0436584197729826
2025-03-01 21:41:23,949 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04336642383038997
2025-03-01 21:41:32,124 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_79_epoch.pt
2025-03-01 21:41:44,784 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04444714937359095
2025-03-01 21:41:56,463 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.044345946088433265
2025-03-01 21:42:07,363 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04426346909254789
2025-03-01 21:42:19,828 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04397246696986258
2025-03-01 21:42:30,454 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04471629814058542
2025-03-01 21:42:39,022 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_80_epoch.pt
2025-03-01 21:42:50,366 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04785444088280201
2025-03-01 21:43:02,371 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.044037052039057015
2025-03-01 21:43:13,930 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04416753228753805
2025-03-01 21:43:25,103 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04372820666991174
2025-03-01 21:43:37,186 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04169339340180159
2025-03-01 21:43:46,083 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_81_epoch.pt
2025-03-01 21:43:58,964 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04288677111268044
2025-03-01 21:44:11,499 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04681650349870324
2025-03-01 21:44:22,345 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.045973053053021434
2025-03-01 21:44:32,689 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.046139408620074394
2025-03-01 21:44:44,272 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.046879888221621514
2025-03-01 21:44:53,412 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_82_epoch.pt
2025-03-01 21:45:05,730 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04705443687736988
2025-03-01 21:45:16,767 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04984239896759391
2025-03-01 21:45:28,908 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.05027196692923705
2025-03-01 21:45:40,443 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04793701435439288
2025-03-01 21:45:51,776 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.046823011115193366
2025-03-01 21:45:59,692 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_83_epoch.pt
2025-03-01 21:46:10,997 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04812107801437378
2025-03-01 21:46:21,059 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04954038606956601
2025-03-01 21:46:32,355 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04777653655658166
2025-03-01 21:46:45,255 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04554807941429317
2025-03-01 21:46:57,090 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04454212763160467
2025-03-01 21:47:05,866 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_84_epoch.pt
2025-03-01 21:47:18,229 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03715104386210442
2025-03-01 21:47:29,878 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04041626805439592
2025-03-01 21:47:40,600 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04284542358169953
2025-03-01 21:47:51,854 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04588215759024024
2025-03-01 21:48:03,269 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.045613717012107376
2025-03-01 21:48:12,308 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_85_epoch.pt
2025-03-01 21:48:24,011 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.050477882251143455
2025-03-01 21:48:35,537 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04528242483735084
2025-03-01 21:48:47,049 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04636059490342935
2025-03-01 21:48:59,545 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04495571063831449
2025-03-01 21:49:10,175 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04476130471378565
2025-03-01 21:49:18,765 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_86_epoch.pt
2025-03-01 21:49:30,431 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04064190175384283
2025-03-01 21:49:42,522 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.03973109973594546
2025-03-01 21:49:54,199 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04189931395153205
2025-03-01 21:50:04,977 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.043019993491470815
2025-03-01 21:50:16,617 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04331943006813526
2025-03-01 21:50:25,427 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_87_epoch.pt
2025-03-01 21:50:37,852 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04558011617511511
2025-03-01 21:50:49,033 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04435894714668393
2025-03-01 21:51:01,320 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.041905396717290086
2025-03-01 21:51:12,589 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.044091101894155146
2025-03-01 21:51:23,761 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04383451777696609
2025-03-01 21:51:32,318 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_88_epoch.pt
2025-03-01 21:51:43,312 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.037881293520331386
2025-03-01 21:51:55,768 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.0431581674516201
2025-03-01 21:52:07,039 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.040910397755603
2025-03-01 21:52:18,158 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.041574060982093215
2025-03-01 21:52:29,617 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04103751704841852
2025-03-01 21:52:38,877 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_89_epoch.pt
2025-03-01 21:52:50,312 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.044975546412169935
2025-03-01 21:53:01,540 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.043842401430010794
2025-03-01 21:53:12,857 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.040374362965424855
2025-03-01 21:53:24,286 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.0433643364533782
2025-03-01 21:53:36,054 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04371600507199764
2025-03-01 21:53:45,419 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_90_epoch.pt
2025-03-01 21:53:57,066 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.0398328660055995
2025-03-01 21:54:08,447 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.03959434736520052
2025-03-01 21:54:20,718 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04204861870656411
2025-03-01 21:54:31,650 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04228303042240441
2025-03-01 21:54:42,968 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04142850968241692
2025-03-01 21:54:51,702 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_91_epoch.pt
2025-03-01 21:55:03,597 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04236866619437933
2025-03-01 21:55:15,144 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.043390410616993905
2025-03-01 21:55:26,041 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.044254231651624044
2025-03-01 21:55:37,237 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04142361254431307
2025-03-01 21:55:49,384 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04254414100944996
2025-03-01 21:55:58,390 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_92_epoch.pt
2025-03-01 21:56:10,700 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04570324849337339
2025-03-01 21:56:23,083 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04359730737283826
2025-03-01 21:56:34,753 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.042404842178026834
2025-03-01 21:56:46,320 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.041281008385121824
2025-03-01 21:56:56,754 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04207098536193371
2025-03-01 21:57:05,461 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_93_epoch.pt
2025-03-01 21:57:17,294 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.043972026631236075
2025-03-01 21:57:28,114 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04780748328194022
2025-03-01 21:57:39,935 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04453896276652813
2025-03-01 21:57:51,435 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04515513923019171
2025-03-01 21:58:02,633 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04500233449041843
2025-03-01 21:58:12,701 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_94_epoch.pt
2025-03-01 21:58:24,261 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03572786659002304
2025-03-01 21:58:35,194 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04491757156327367
2025-03-01 21:58:47,302 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04548909474164248
2025-03-01 21:58:59,132 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04425245290622115
2025-03-01 21:59:11,028 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04336541693657637
2025-03-01 21:59:19,753 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_95_epoch.pt
2025-03-01 21:59:31,938 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.045939112678170205
2025-03-01 21:59:42,648 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.044361874125897885
2025-03-01 21:59:53,284 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04390229302148024
2025-03-01 22:00:04,886 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.041693294197320936
2025-03-01 22:00:15,954 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04290498772263527
2025-03-01 22:00:26,266 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_96_epoch.pt
2025-03-01 22:00:38,340 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03484893526881933
2025-03-01 22:00:49,971 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.0397909914702177
2025-03-01 22:01:01,314 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04050184649725755
2025-03-01 22:01:12,598 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.0413041096739471
2025-03-01 22:01:24,043 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04210067101567984
2025-03-01 22:01:32,475 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_97_epoch.pt
2025-03-01 22:01:45,262 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04210420787334442
2025-03-01 22:01:56,594 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.041754499711096286
2025-03-01 22:02:06,988 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04439721625298262
2025-03-01 22:02:18,668 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04607331985607743
2025-03-01 22:02:29,895 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04498492209613323
2025-03-01 22:02:39,196 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_98_epoch.pt
2025-03-01 22:02:50,431 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.047034810110926625
2025-03-01 22:03:02,421 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04699059627950192
2025-03-01 22:03:14,130 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.045510471301774186
2025-03-01 22:03:26,355 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04394158494658768
2025-03-01 22:03:37,267 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04571440230309963
2025-03-01 22:03:45,583 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_99_epoch.pt
2025-03-01 22:03:57,008 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03845541913062334
2025-03-01 22:04:08,612 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.0399295068718493
2025-03-01 22:04:19,512 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.0403502473110954
2025-03-01 22:04:30,717 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04236420210450888
2025-03-01 22:04:43,220 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04249916449189186
2025-03-01 22:04:52,542 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_100_epoch.pt
2025-03-01 22:05:03,915 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04961647816002369
2025-03-01 22:05:15,182 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.044318588841706516
2025-03-01 22:05:26,922 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.045090707329412304
2025-03-01 22:05:38,605 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04428005236200988
2025-03-01 22:05:49,762 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04347517676651478
2025-03-01 22:05:58,873 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_101_epoch.pt
2025-03-01 22:06:10,775 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03652358386665583
2025-03-01 22:06:21,413 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.03896598517894745
2025-03-01 22:06:32,331 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.038899486015240355
2025-03-01 22:06:44,086 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.041650924943387506
2025-03-01 22:06:56,694 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.0417338422909379
2025-03-01 22:07:05,782 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_102_epoch.pt
2025-03-01 22:07:17,850 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.05166958570480347
2025-03-01 22:07:28,714 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.049788086004555226
2025-03-01 22:07:40,245 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04835507950435082
2025-03-01 22:07:51,849 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04761396690271795
2025-03-01 22:08:03,354 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04625599846243858
2025-03-01 22:08:12,106 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_103_epoch.pt
2025-03-01 22:08:24,921 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.045096481926739214
2025-03-01 22:08:36,117 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.043179368767887354
2025-03-01 22:08:47,345 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.042519592940807344
2025-03-01 22:08:58,656 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04236448686569929
2025-03-01 22:09:10,559 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.041847852423787114
2025-03-01 22:09:19,065 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_104_epoch.pt
2025-03-01 22:09:31,486 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.038094470016658305
2025-03-01 22:09:43,028 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04178268413990736
2025-03-01 22:09:54,809 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04010252748926481
2025-03-01 22:10:05,164 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04009370624087751
2025-03-01 22:10:16,527 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.041437743566930293
2025-03-01 22:10:24,916 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_105_epoch.pt
2025-03-01 22:10:36,335 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.041396153010427955
2025-03-01 22:10:47,617 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.038078166246414184
2025-03-01 22:10:58,717 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.043272682428359986
2025-03-01 22:11:10,509 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.0435485496558249
2025-03-01 22:11:21,342 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04560376639664173
2025-03-01 22:11:30,791 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_106_epoch.pt
2025-03-01 22:11:42,316 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04324278384447098
2025-03-01 22:11:54,356 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04271712878718972
2025-03-01 22:12:06,582 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04291849552343289
2025-03-01 22:12:18,437 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04281069130636752
2025-03-01 22:12:28,630 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04391971258074045
2025-03-01 22:12:38,163 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_107_epoch.pt
2025-03-01 22:12:50,651 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.045634242929518225
2025-03-01 22:13:01,699 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04490449257194996
2025-03-01 22:13:12,374 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04445399959882101
2025-03-01 22:13:24,232 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04348593445494771
2025-03-01 22:13:35,144 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.042240558698773384
2025-03-01 22:13:44,442 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_108_epoch.pt
2025-03-01 22:13:56,751 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04474280335009098
2025-03-01 22:14:07,934 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04518133021891117
2025-03-01 22:14:20,317 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04625126010427872
2025-03-01 22:14:31,018 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04914894632995129
2025-03-01 22:14:41,526 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04928324245661497
2025-03-01 22:14:50,839 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_109_epoch.pt
2025-03-01 22:15:02,842 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04004868146032095
2025-03-01 22:15:15,062 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04303977033123374
2025-03-01 22:15:26,892 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.044221912461022535
2025-03-01 22:15:37,220 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04279636802151799
2025-03-01 22:15:49,441 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04186164623498917
2025-03-01 22:15:57,796 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_110_epoch.pt
2025-03-01 22:16:10,572 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.036351099237799646
2025-03-01 22:16:22,279 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.03786924796178937
2025-03-01 22:16:33,247 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.03922958397616943
2025-03-01 22:16:44,745 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.03936323645524681
2025-03-01 22:16:56,105 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.03789167757332325
2025-03-01 22:17:04,809 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_111_epoch.pt
2025-03-01 22:17:17,321 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04797894302755594
2025-03-01 22:17:27,875 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04267761034891009
2025-03-01 22:17:40,905 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.043987080144385494
2025-03-01 22:17:51,635 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.043978963196277615
2025-03-01 22:18:03,303 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04382226381450891
2025-03-01 22:18:11,712 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_112_epoch.pt
2025-03-01 22:18:24,599 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04935128536075353
2025-03-01 22:18:35,934 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04459509771317244
2025-03-01 22:18:46,698 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.0448215032244722
2025-03-01 22:18:57,304 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04467941590584815
2025-03-01 22:19:08,800 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.042866776846349236
2025-03-01 22:19:18,132 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_113_epoch.pt
2025-03-01 22:19:30,193 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04010360602289438
2025-03-01 22:19:41,552 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.044270688146352766
2025-03-01 22:19:52,750 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04712872282912334
2025-03-01 22:20:04,212 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04651320781558752
2025-03-01 22:20:16,024 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04753611686080694
2025-03-01 22:20:25,324 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_114_epoch.pt
2025-03-01 22:20:37,411 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04567209430038929
2025-03-01 22:20:49,046 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.042819958850741385
2025-03-01 22:20:59,759 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04269780733933051
2025-03-01 22:21:10,474 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04219139308668673
2025-03-01 22:21:22,592 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04233327649533749
2025-03-01 22:21:31,636 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_115_epoch.pt
2025-03-01 22:21:42,789 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.046227719187736514
2025-03-01 22:21:53,983 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.044192474782466885
2025-03-01 22:22:06,572 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04595169149339199
2025-03-01 22:22:17,770 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04501463357359171
2025-03-01 22:22:29,303 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04592860285937786
2025-03-01 22:22:37,887 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_116_epoch.pt
2025-03-01 22:22:49,419 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.044391854144632814
2025-03-01 22:23:00,824 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.043208549842238424
2025-03-01 22:23:11,956 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04385096258173386
2025-03-01 22:23:23,693 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04365961480885744
2025-03-01 22:23:35,719 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04357171194255352
2025-03-01 22:23:44,764 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_117_epoch.pt
2025-03-01 22:23:55,811 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.042329966127872466
2025-03-01 22:24:07,966 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.0437429934181273
2025-03-01 22:24:19,186 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04398530670752128
2025-03-01 22:24:31,221 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04699414785951376
2025-03-01 22:24:43,122 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04646272686123848
2025-03-01 22:24:51,380 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_118_epoch.pt
2025-03-01 22:25:03,554 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03996174816042185
2025-03-01 22:25:15,715 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.039535934031009676
2025-03-01 22:25:26,934 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04157468227048715
2025-03-01 22:25:38,389 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04326615447178483
2025-03-01 22:25:50,022 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04386091720312834
2025-03-01 22:25:58,615 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_119_epoch.pt
2025-03-01 22:26:10,514 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04239326540380716
2025-03-01 22:26:21,464 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.03999445218592882
2025-03-01 22:26:33,454 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04234640715022882
2025-03-01 22:26:44,972 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04036677765659988
2025-03-01 22:26:55,808 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04116287561506033
2025-03-01 22:27:05,537 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_120_epoch.pt
2025-03-01 22:27:17,621 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.042081227265298365
2025-03-01 22:27:29,142 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.044635379742830994
2025-03-01 22:27:42,015 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.041330570640663304
2025-03-01 22:27:53,262 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04051001434214413
2025-03-01 22:28:03,386 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.042217493906617165
2025-03-01 22:28:11,682 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_121_epoch.pt
2025-03-01 22:28:23,817 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04922093246132135
2025-03-01 22:28:34,854 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.043592280559241775
2025-03-01 22:28:46,412 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04570461691667636
2025-03-01 22:28:57,410 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04709343058988452
2025-03-01 22:29:09,265 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.0479938989803195
2025-03-01 22:29:18,374 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_122_epoch.pt
2025-03-01 22:29:30,259 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.033234787844121455
2025-03-01 22:29:41,782 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.040312309209257366
2025-03-01 22:29:53,310 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.042724650477369626
2025-03-01 22:30:04,567 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04266048738732934
2025-03-01 22:30:16,304 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04336197226494551
2025-03-01 22:30:24,358 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_123_epoch.pt
2025-03-01 22:30:36,588 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04607795588672161
2025-03-01 22:30:47,416 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04334665155038238
2025-03-01 22:30:58,938 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04188701082020998
2025-03-01 22:31:11,047 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04419722161255777
2025-03-01 22:31:22,168 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04453094856441021
2025-03-01 22:31:30,633 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_124_epoch.pt
2025-03-01 22:31:41,427 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.044648531973361966
2025-03-01 22:31:52,295 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.0445646608248353
2025-03-01 22:32:04,189 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04484078612178564
2025-03-01 22:32:16,487 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04567121473141014
2025-03-01 22:32:28,335 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04406522314995527
2025-03-01 22:32:37,638 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_125_epoch.pt
2025-03-01 22:32:49,494 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04714911412447691
2025-03-01 22:33:01,127 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04372737683355808
2025-03-01 22:33:12,702 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.041868049191931885
2025-03-01 22:33:23,762 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.040763222230598334
2025-03-01 22:33:35,206 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04163503523916006
2025-03-01 22:33:44,537 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_126_epoch.pt
2025-03-01 22:33:56,618 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.036238793134689334
2025-03-01 22:34:08,515 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04206222169101238
2025-03-01 22:34:19,701 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04250210019449393
2025-03-01 22:34:30,513 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.043797648586332796
2025-03-01 22:34:42,392 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.0415212035998702
2025-03-01 22:34:51,748 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_127_epoch.pt
2025-03-01 22:35:03,753 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.036654925011098384
2025-03-01 22:35:14,568 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04165054094046354
2025-03-01 22:35:25,968 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04090745225548744
2025-03-01 22:35:37,547 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.042556720450520516
2025-03-01 22:35:49,860 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04281797014921904
2025-03-01 22:35:58,952 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_128_epoch.pt
2025-03-01 22:36:10,453 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.044111314825713635
2025-03-01 22:36:21,821 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.03953800020739436
2025-03-01 22:36:32,509 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.03882152090469996
2025-03-01 22:36:44,033 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.03986517047509551
2025-03-01 22:36:55,622 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04158048381656408
2025-03-01 22:37:05,453 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_129_epoch.pt
2025-03-01 22:37:17,261 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03951157044619322
2025-03-01 22:37:28,592 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.041772924978286025
2025-03-01 22:37:39,103 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04053218375891447
2025-03-01 22:37:51,055 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.03898797705769539
2025-03-01 22:38:03,103 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.040158177062869074
2025-03-01 22:38:12,088 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_130_epoch.pt
2025-03-01 22:38:22,925 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.044160937927663325
2025-03-01 22:38:33,930 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.042721608635038134
2025-03-01 22:38:47,125 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04527878838280837
2025-03-01 22:38:58,671 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04440818487666547
2025-03-01 22:39:10,118 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04519135295599699
2025-03-01 22:39:19,032 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_131_epoch.pt
2025-03-01 22:39:29,889 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.034997505880892274
2025-03-01 22:39:41,368 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04340194618329406
2025-03-01 22:39:52,773 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04380604269603888
2025-03-01 22:40:05,586 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04490211066789925
2025-03-01 22:40:17,275 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.044362830877304076
2025-03-01 22:40:25,466 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_132_epoch.pt
2025-03-01 22:40:37,257 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.041985436007380486
2025-03-01 22:40:48,250 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04122805114835501
2025-03-01 22:41:00,289 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04364790052175522
2025-03-01 22:41:11,667 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04521357570774853
2025-03-01 22:41:23,391 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.045647944912314414
2025-03-01 22:41:32,027 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_133_epoch.pt
2025-03-01 22:41:43,657 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04473138131201267
2025-03-01 22:41:55,337 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.045454473793506624
2025-03-01 22:42:06,853 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.043496695533394816
2025-03-01 22:42:18,671 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04267620014958084
2025-03-01 22:42:30,121 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04402620003372431
2025-03-01 22:42:38,759 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_134_epoch.pt
2025-03-01 22:42:50,325 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.030613365955650805
2025-03-01 22:43:02,118 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04227632958441973
2025-03-01 22:43:13,208 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.03994139391928911
2025-03-01 22:43:25,074 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.03774121843278408
2025-03-01 22:43:36,866 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.03897639359533787
2025-03-01 22:43:45,747 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_135_epoch.pt
2025-03-01 22:43:58,076 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04319568544626236
2025-03-01 22:44:08,856 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04221688574180007
2025-03-01 22:44:20,980 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.0399405920257171
2025-03-01 22:44:32,226 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.039569659465923904
2025-03-01 22:44:43,476 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.03882448821514845
2025-03-01 22:44:52,198 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_136_epoch.pt
2025-03-01 22:45:04,189 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04445025447756052
2025-03-01 22:45:15,603 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.042217424903064966
2025-03-01 22:45:27,464 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04106156221280495
2025-03-01 22:45:38,745 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04060337089002133
2025-03-01 22:45:50,297 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.041270686633884904
2025-03-01 22:45:59,761 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_137_epoch.pt
2025-03-01 22:46:12,624 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.039365829564630986
2025-03-01 22:46:23,185 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.03660214528441429
2025-03-01 22:46:34,645 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04005218703299761
2025-03-01 22:46:45,932 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04280622866936028
2025-03-01 22:46:57,049 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04314734411239624
2025-03-01 22:47:06,075 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_138_epoch.pt
2025-03-01 22:47:17,720 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04455182153731584
2025-03-01 22:47:29,832 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04163181057199836
2025-03-01 22:47:40,847 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04311661707858245
2025-03-01 22:47:52,276 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.043408052595332265
2025-03-01 22:48:03,453 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04470018933713436
2025-03-01 22:48:12,881 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_139_epoch.pt
2025-03-01 22:48:25,058 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.05035777110606432
2025-03-01 22:48:38,152 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.05323814058676362
2025-03-01 22:48:48,246 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.05029848558207353
2025-03-01 22:48:59,557 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04894702396355569
2025-03-01 22:49:10,906 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04707285553961992
2025-03-01 22:49:19,924 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_140_epoch.pt
2025-03-01 22:49:31,705 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04716570734977722
2025-03-01 22:49:42,980 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.045357207730412484
2025-03-01 22:49:54,373 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04422479271888733
2025-03-01 22:50:06,230 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.0434184764418751
2025-03-01 22:50:16,676 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.045670296117663385
2025-03-01 22:50:26,774 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_141_epoch.pt
2025-03-01 22:50:39,300 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04191404782235622
2025-03-01 22:50:51,082 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04398283537477255
2025-03-01 22:51:02,280 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04293039819846551
2025-03-01 22:51:14,426 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04180680078454316
2025-03-01 22:51:25,406 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04100590841472149
2025-03-01 22:51:34,442 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_142_epoch.pt
2025-03-01 22:51:46,171 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.043693210892379285
2025-03-01 22:51:57,169 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.047055592313408855
2025-03-01 22:52:08,452 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04530909776687622
2025-03-01 22:52:20,565 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.044071947867050765
2025-03-01 22:52:32,392 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04592304120957851
2025-03-01 22:52:41,529 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_143_epoch.pt
2025-03-01 22:52:53,199 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03689409643411636
2025-03-01 22:53:03,982 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.036797917876392604
2025-03-01 22:53:15,659 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.037774343887964884
2025-03-01 22:53:26,341 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.03989841022528708
2025-03-01 22:53:37,528 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04135143763571977
2025-03-01 22:53:47,569 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_144_epoch.pt
2025-03-01 22:53:59,802 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.046838622614741324
2025-03-01 22:54:10,335 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04172204446047544
2025-03-01 22:54:21,576 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04307191393027703
2025-03-01 22:54:33,285 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04258889145217836
2025-03-01 22:54:45,303 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04381569495052099
2025-03-01 22:54:54,248 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_145_epoch.pt
2025-03-01 22:55:05,595 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04451221790164709
2025-03-01 22:55:17,125 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04666209241375327
2025-03-01 22:55:27,978 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.045374203013877075
2025-03-01 22:55:40,276 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04554434563964605
2025-03-01 22:55:51,806 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04456639542430639
2025-03-01 22:56:00,554 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_146_epoch.pt
2025-03-01 22:56:12,720 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04737279791384935
2025-03-01 22:56:24,769 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.03953498911112547
2025-03-01 22:56:35,534 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.039944087949891884
2025-03-01 22:56:47,008 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04110930692404509
2025-03-01 22:56:58,843 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04098270933330059
2025-03-01 22:57:07,254 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_147_epoch.pt
2025-03-01 22:57:18,694 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.040739530511200425
2025-03-01 22:57:30,872 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04174506470561028
2025-03-01 22:57:42,074 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04466705011824767
2025-03-01 22:57:52,395 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04309218839742243
2025-03-01 22:58:04,148 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04280705685913563
2025-03-01 22:58:13,409 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_148_epoch.pt
2025-03-01 22:58:25,299 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.044861442372202874
2025-03-01 22:58:36,359 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04793055979534984
2025-03-01 22:58:47,495 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04536036574592193
2025-03-01 22:58:59,336 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04557876084931195
2025-03-01 22:59:10,914 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04397086249291897
2025-03-01 22:59:19,752 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_149_epoch.pt
2025-03-01 22:59:31,201 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.047526419423520566
2025-03-01 22:59:43,110 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04937934618443251
2025-03-01 22:59:54,813 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.047017169284323854
2025-03-01 23:00:05,858 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04459257996641099
2025-03-01 23:00:17,455 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.044140393294394016
2025-03-01 23:00:26,300 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_150_epoch.pt
2025-03-01 23:00:38,867 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.0401748039573431
2025-03-01 23:00:50,477 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04043410636484623
2025-03-01 23:01:00,933 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.042409204989671705
2025-03-01 23:01:11,980 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.044159862184897064
2025-03-01 23:01:23,770 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.044603692419826986
2025-03-01 23:01:32,738 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_151_epoch.pt
2025-03-01 23:01:44,659 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04376444816589355
2025-03-01 23:01:56,259 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.044874684158712624
2025-03-01 23:02:08,177 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04338145814836025
2025-03-01 23:02:19,195 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.0439366068970412
2025-03-01 23:02:30,942 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.043647191934287545
2025-03-01 23:02:39,875 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_152_epoch.pt
2025-03-01 23:02:51,201 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03549557615071535
2025-03-01 23:03:03,735 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.03920570775866508
2025-03-01 23:03:14,983 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04055796602120002
2025-03-01 23:03:26,855 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.041825276277959345
2025-03-01 23:03:38,117 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.041707263968884946
2025-03-01 23:03:46,788 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_153_epoch.pt
2025-03-01 23:03:58,452 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03861156202852726
2025-03-01 23:04:11,041 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.045582004580646755
2025-03-01 23:04:23,021 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.043992785761753715
2025-03-01 23:04:33,597 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.0417109268065542
2025-03-01 23:04:45,141 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04029430953413248
2025-03-01 23:04:54,054 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_154_epoch.pt
2025-03-01 23:05:06,008 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04637310460209847
2025-03-01 23:05:17,619 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04163976149633527
2025-03-01 23:05:28,392 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04053838860243559
2025-03-01 23:05:40,459 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.041355163902044294
2025-03-01 23:05:51,387 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04021342923492193
2025-03-01 23:05:59,885 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_155_epoch.pt
2025-03-01 23:06:11,952 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03737373124808073
2025-03-01 23:06:23,741 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.041264465134590866
2025-03-01 23:06:35,158 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04272076101352771
2025-03-01 23:06:46,621 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04224941523745656
2025-03-01 23:06:57,545 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04232469761371613
2025-03-01 23:07:06,891 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_156_epoch.pt
2025-03-01 23:07:19,401 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.030271930657327174
2025-03-01 23:07:31,356 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.040416880771517755
2025-03-01 23:07:41,797 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.040600484907627105
2025-03-01 23:07:53,810 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04124066180549562
2025-03-01 23:08:04,552 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.038968063302338124
2025-03-01 23:08:13,113 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_157_epoch.pt
2025-03-01 23:08:25,755 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04632240884006023
2025-03-01 23:08:36,796 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.042468760013580326
2025-03-01 23:08:48,104 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04136866026868423
2025-03-01 23:08:59,268 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04035402948968112
2025-03-01 23:09:10,975 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04152357875555754
2025-03-01 23:09:19,710 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_158_epoch.pt
2025-03-01 23:09:31,986 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03987795162945986
2025-03-01 23:09:42,634 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04355118326842785
2025-03-01 23:09:54,356 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04561536869655053
2025-03-01 23:10:05,406 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04409940058365464
2025-03-01 23:10:17,181 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.043594538435339926
2025-03-01 23:10:26,049 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_159_epoch.pt
2025-03-01 23:10:37,398 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04129462439566851
2025-03-01 23:10:48,461 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04403841586783528
2025-03-01 23:11:00,289 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04308269721766313
2025-03-01 23:11:12,015 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.042223664904013275
2025-03-01 23:11:23,319 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04345446138828993
2025-03-01 23:11:32,690 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_160_epoch.pt
2025-03-01 23:11:43,834 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04900818776339293
2025-03-01 23:11:55,847 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04526206644251943
2025-03-01 23:12:07,549 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.043388005991776786
2025-03-01 23:12:18,804 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04451811904087663
2025-03-01 23:12:29,591 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04329368762671947
2025-03-01 23:12:39,529 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_161_epoch.pt
2025-03-01 23:12:52,104 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.038105983287096024
2025-03-01 23:13:03,412 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.0393439981341362
2025-03-01 23:13:15,391 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04061423340191444
2025-03-01 23:13:26,199 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.039229450672864916
2025-03-01 23:13:38,409 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.041366992607712746
2025-03-01 23:13:46,768 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_162_epoch.pt
2025-03-01 23:13:58,523 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04103011894971132
2025-03-01 23:14:10,536 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04268092941492796
2025-03-01 23:14:21,395 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.03995762935529153
2025-03-01 23:14:33,548 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04154684929177165
2025-03-01 23:14:44,543 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04245979758352041
2025-03-01 23:14:53,347 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_163_epoch.pt
2025-03-01 23:15:05,902 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03839679107069969
2025-03-01 23:15:17,466 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.036879200302064416
2025-03-01 23:15:28,625 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04175670667241017
2025-03-01 23:15:39,838 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04155364517122507
2025-03-01 23:15:51,541 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.040853152692317964
2025-03-01 23:16:00,158 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_164_epoch.pt
2025-03-01 23:16:12,353 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.047493553459644317
2025-03-01 23:16:24,274 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04204248769208789
2025-03-01 23:16:34,945 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.042913851775228974
2025-03-01 23:16:47,004 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04061051064170897
2025-03-01 23:16:58,186 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04047732762247324
2025-03-01 23:17:06,613 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_165_epoch.pt
2025-03-01 23:17:19,380 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.038843313977122304
2025-03-01 23:17:30,123 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04345328886061907
2025-03-01 23:17:40,803 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.046402182665963965
2025-03-01 23:17:51,800 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.046272885324433445
2025-03-01 23:18:03,004 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.047636031493544576
2025-03-01 23:18:12,957 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_166_epoch.pt
2025-03-01 23:18:24,366 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04688443131744861
2025-03-01 23:18:36,671 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04557648507878184
2025-03-01 23:18:48,596 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.040667894432942074
2025-03-01 23:18:59,096 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04191234800964594
2025-03-01 23:19:10,866 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.040217958219349384
2025-03-01 23:19:19,812 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_167_epoch.pt
2025-03-01 23:19:32,530 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04790486954152584
2025-03-01 23:19:43,955 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04053557505831122
2025-03-01 23:19:55,250 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.044040000525613625
2025-03-01 23:20:06,269 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04343299889005721
2025-03-01 23:20:17,318 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.042385380282998084
2025-03-01 23:20:26,995 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_168_epoch.pt
2025-03-01 23:20:38,746 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.043809514828026294
2025-03-01 23:20:51,406 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.047276295572519306
2025-03-01 23:21:02,171 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04536832888921102
2025-03-01 23:21:13,354 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.046299218591302635
2025-03-01 23:21:24,797 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04543214258551598
2025-03-01 23:21:33,061 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_169_epoch.pt
2025-03-01 23:21:46,140 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04376632582396269
2025-03-01 23:21:58,301 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.044838878866285084
2025-03-01 23:22:08,742 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.045644775157173474
2025-03-01 23:22:19,429 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.045883932495489714
2025-03-01 23:22:30,636 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04589537119120359
2025-03-01 23:22:39,326 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_170_epoch.pt
2025-03-01 23:22:50,913 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.039981191381812095
2025-03-01 23:23:02,661 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.040291332565248014
2025-03-01 23:23:14,143 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.043932707024117314
2025-03-01 23:23:25,678 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.044916015164926645
2025-03-01 23:23:36,947 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.046453742675483224
2025-03-01 23:23:45,615 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_171_epoch.pt
2025-03-01 23:23:57,425 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.0342645076289773
2025-03-01 23:24:09,002 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04394351225346327
2025-03-01 23:24:20,353 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04066164808968703
2025-03-01 23:24:31,627 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04006246470846236
2025-03-01 23:24:43,563 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04084345219284296
2025-03-01 23:24:52,423 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_172_epoch.pt
2025-03-01 23:25:05,157 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04570793606340885
2025-03-01 23:25:16,134 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04090893365442753
2025-03-01 23:25:27,325 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.0387990694741408
2025-03-01 23:25:38,562 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.039625621307641266
2025-03-01 23:25:50,041 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04030953423678875
2025-03-01 23:25:59,182 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_173_epoch.pt
2025-03-01 23:26:11,487 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04028887890279293
2025-03-01 23:26:24,293 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04321152972057462
2025-03-01 23:26:34,730 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04114928668985764
2025-03-01 23:26:45,838 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04339772341772914
2025-03-01 23:26:57,407 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04301432440429926
2025-03-01 23:27:06,051 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_174_epoch.pt
2025-03-01 23:27:18,375 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.040848172754049304
2025-03-01 23:27:29,615 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04034798320382833
2025-03-01 23:27:40,490 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.037795076929032806
2025-03-01 23:27:51,440 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.03851338881067932
2025-03-01 23:28:03,126 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.03858014856278896
2025-03-01 23:28:12,318 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_175_epoch.pt
2025-03-01 23:28:24,936 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.039820495434105395
2025-03-01 23:28:37,363 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.03941668214276433
2025-03-01 23:28:47,574 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04125389475375414
2025-03-01 23:28:58,921 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04355799929238856
2025-03-01 23:29:10,848 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04401613005250692
2025-03-01 23:29:18,701 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_176_epoch.pt
2025-03-01 23:29:30,446 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04225594509392977
2025-03-01 23:29:41,487 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.0383594335988164
2025-03-01 23:29:52,806 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.03961484491825104
2025-03-01 23:30:05,858 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.040788142466917636
2025-03-01 23:30:16,566 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.041338374935090544
2025-03-01 23:30:25,157 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_177_epoch.pt
2025-03-01 23:30:37,200 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04909118957817554
2025-03-01 23:30:48,539 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04744359197095036
2025-03-01 23:31:00,012 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04618992604315281
2025-03-01 23:31:11,221 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.043540066508576274
2025-03-01 23:31:23,064 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.042305287182331085
2025-03-01 23:31:31,760 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_178_epoch.pt
2025-03-01 23:31:43,881 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04188372850418091
2025-03-01 23:31:54,852 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.041694998778402804
2025-03-01 23:32:06,748 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.0445604229097565
2025-03-01 23:32:17,986 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04194060157984495
2025-03-01 23:32:29,566 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04262540913373232
2025-03-01 23:32:38,606 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_179_epoch.pt
2025-03-01 23:32:50,362 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03838630668818951
2025-03-01 23:33:00,659 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.03986305503174663
2025-03-01 23:33:12,630 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04136392875264088
2025-03-01 23:33:24,324 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.041062524411827325
2025-03-01 23:33:36,027 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04143066460639239
2025-03-01 23:33:45,145 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_180_epoch.pt
2025-03-01 23:33:57,065 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.0475853481143713
2025-03-01 23:34:07,983 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.044872670508921145
2025-03-01 23:34:19,649 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.043035717060168587
2025-03-01 23:34:31,309 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.043064630897715686
2025-03-01 23:34:42,891 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.042870905868709085
2025-03-01 23:34:51,656 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_181_epoch.pt
2025-03-01 23:35:03,418 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.039316880255937575
2025-03-01 23:35:14,797 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.039502222072333094
2025-03-01 23:35:27,196 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.039620856468876205
2025-03-01 23:35:38,273 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.040051451111212374
2025-03-01 23:35:49,518 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.041001613333821296
2025-03-01 23:35:57,843 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_182_epoch.pt
2025-03-01 23:36:09,595 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.035567532479763034
2025-03-01 23:36:20,775 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04123750377446413
2025-03-01 23:36:32,424 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04139616588751475
2025-03-01 23:36:43,679 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.03990261481143534
2025-03-01 23:36:54,791 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04045559515804052
2025-03-01 23:37:03,995 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_183_epoch.pt
2025-03-01 23:37:15,560 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.0437835343927145
2025-03-01 23:37:26,673 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04674662102013826
2025-03-01 23:37:37,841 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.044350889312724275
2025-03-01 23:37:49,699 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04429111413657665
2025-03-01 23:38:01,193 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04345635379105806
2025-03-01 23:38:10,453 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_184_epoch.pt
2025-03-01 23:38:22,301 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.041024513244628906
2025-03-01 23:38:33,681 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04265327300876379
2025-03-01 23:38:45,752 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.040892898067831994
2025-03-01 23:38:56,447 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.040119189321994785
2025-03-01 23:39:08,213 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04048450984060764
2025-03-01 23:39:16,562 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_185_epoch.pt
2025-03-01 23:39:29,220 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.0457580341771245
2025-03-01 23:39:40,326 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.044155162163078784
2025-03-01 23:39:51,566 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04224500097334385
2025-03-01 23:40:03,346 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.044292543325573204
2025-03-01 23:40:14,520 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04448907759785652
2025-03-01 23:40:23,232 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_186_epoch.pt
2025-03-01 23:40:36,125 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04339942138642073
2025-03-01 23:40:48,206 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04431946152821183
2025-03-01 23:40:59,370 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04302712275336186
2025-03-01 23:41:11,066 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04256339771673083
2025-03-01 23:41:21,541 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.0423622991964221
2025-03-01 23:41:30,575 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_187_epoch.pt
2025-03-01 23:41:42,229 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.036680927462875844
2025-03-01 23:41:53,974 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.037644325979053976
2025-03-01 23:42:05,268 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04256033228089412
2025-03-01 23:42:17,098 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04315461098216474
2025-03-01 23:42:28,579 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04310978855192661
2025-03-01 23:42:37,198 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_188_epoch.pt
2025-03-01 23:42:49,282 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.042293506190180775
2025-03-01 23:43:00,601 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04214910801500082
2025-03-01 23:43:11,640 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04128184873610735
2025-03-01 23:43:22,673 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04033333882689476
2025-03-01 23:43:35,653 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04110084742307663
2025-03-01 23:43:44,561 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_189_epoch.pt
2025-03-01 23:43:57,034 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.045927487313747406
2025-03-01 23:44:08,970 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.05048673171550035
2025-03-01 23:44:19,839 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04978868417441845
2025-03-01 23:44:31,073 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04906205392442644
2025-03-01 23:44:42,018 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04728652355819941
2025-03-01 23:44:50,676 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_190_epoch.pt
2025-03-01 23:45:01,951 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.045751366205513476
2025-03-01 23:45:13,370 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04710099497810006
2025-03-01 23:45:24,367 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04388345229128997
2025-03-01 23:45:36,719 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04253955953754485
2025-03-01 23:45:48,420 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04145273223519325
2025-03-01 23:45:56,984 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_191_epoch.pt
2025-03-01 23:46:08,119 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04745357010513544
2025-03-01 23:46:20,422 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.045514870584011075
2025-03-01 23:46:32,877 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.044946786252160864
2025-03-01 23:46:44,253 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.044181037098169326
2025-03-01 23:46:55,117 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04442271137237549
2025-03-01 23:47:03,984 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_192_epoch.pt
2025-03-01 23:47:16,158 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.039783022962510586
2025-03-01 23:47:27,454 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.040633522495627405
2025-03-01 23:47:38,624 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04206274803727865
2025-03-01 23:47:50,290 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04234579199925065
2025-03-01 23:48:02,066 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04294752328842878
2025-03-01 23:48:11,290 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_193_epoch.pt
2025-03-01 23:48:22,880 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.047281364053487776
2025-03-01 23:48:34,605 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04412296894937753
2025-03-01 23:48:46,623 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.0435742391521732
2025-03-01 23:48:57,419 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.043583111586049196
2025-03-01 23:49:08,956 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04388334570080042
2025-03-01 23:49:18,221 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_194_epoch.pt
2025-03-01 23:49:29,960 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.03691936820745468
2025-03-01 23:49:42,787 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.03840710837393999
2025-03-01 23:49:53,312 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.039156393483281135
2025-03-01 23:50:04,561 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.03785356349311769
2025-03-01 23:50:15,328 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.03902239315956831
2025-03-01 23:50:24,930 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_195_epoch.pt
2025-03-01 23:50:36,382 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.046531363017857076
2025-03-01 23:50:47,604 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04576074158772826
2025-03-01 23:50:59,379 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.042370096147060395
2025-03-01 23:51:11,241 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04257233681157231
2025-03-01 23:51:22,445 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.041566412180662156
2025-03-01 23:51:31,922 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_196_epoch.pt
2025-03-01 23:51:42,885 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04848071504384279
2025-03-01 23:51:53,995 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.03939148550853133
2025-03-01 23:52:05,721 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.0417161692182223
2025-03-01 23:52:16,839 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.0404045763798058
2025-03-01 23:52:28,655 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04151582206040621
2025-03-01 23:52:38,158 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_197_epoch.pt
2025-03-01 23:52:50,185 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04138441625982523
2025-03-01 23:53:02,117 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.041573931947350505
2025-03-01 23:53:13,639 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04188643595824639
2025-03-01 23:53:24,463 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.0418415863905102
2025-03-01 23:53:35,860 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.04266736152023077
2025-03-01 23:53:44,901 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_198_epoch.pt
2025-03-01 23:53:56,473 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04778064504265785
2025-03-01 23:54:07,099 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04497530564665794
2025-03-01 23:54:19,117 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.0423484289025267
2025-03-01 23:54:31,171 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.041340414956212046
2025-03-01 23:54:42,608 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.0404342580884695
2025-03-01 23:54:51,697 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_199_epoch.pt
2025-03-01 23:55:03,050 : 1341934384.py : train_scp : INFO : sample = 800, loss = 0.04560804039239883
2025-03-01 23:55:14,242 : 1341934384.py : train_scp : INFO : sample = 1600, loss = 0.04314253138378263
2025-03-01 23:55:26,202 : 1341934384.py : train_scp : INFO : sample = 2400, loss = 0.04413032369067271
2025-03-01 23:55:37,846 : 1341934384.py : train_scp : INFO : sample = 3200, loss = 0.04249795588664711
2025-03-01 23:55:49,290 : 1341934384.py : train_scp : INFO : sample = 4000, loss = 0.0439677957072854
2025-03-01 23:55:58,688 : 1341934384.py : train_scp : INFO : the pre-training finished, saving model, path = trained_models_with_batch=8_lr5e-6/_bert-base-uncased_200_epoch.pt
2025-03-02 00:16:56,638 : 1196508602.py : eval_pred : INFO : processing 0 lines 
2025-03-02 00:18:07,032 : 1196508602.py : eval_pred : INFO : processing 100 lines 
2025-03-02 00:18:59,025 : 1196508602.py : eval_pred : INFO : processing 200 lines 
2025-03-02 00:19:57,606 : 1196508602.py : eval_pred : INFO : processing 300 lines 
2025-03-02 00:20:58,739 : 1683485883.py : run_eval : INFO : Epoch 134: F1: 0.9213221601489759, ACC: 0.9395604395604396
2025-03-02 00:20:59,549 : 1196508602.py : eval_pred : INFO : processing 0 lines 
2025-03-02 00:22:01,661 : 1196508602.py : eval_pred : INFO : processing 100 lines 
2025-03-02 00:22:51,950 : 1196508602.py : eval_pred : INFO : processing 200 lines 
2025-03-02 00:23:48,091 : 1196508602.py : eval_pred : INFO : processing 300 lines 
2025-03-02 00:24:46,784 : 1683485883.py : run_eval : INFO : Epoch 135: F1: 0.9175977653631286, ACC: 0.9368131868131868
2025-03-02 00:24:47,611 : 1196508602.py : eval_pred : INFO : processing 0 lines 
2025-03-02 00:25:49,758 : 1196508602.py : eval_pred : INFO : processing 100 lines 
2025-03-02 00:26:39,351 : 1196508602.py : eval_pred : INFO : processing 200 lines 
2025-03-02 00:27:34,417 : 1196508602.py : eval_pred : INFO : processing 300 lines 
2025-03-02 00:28:32,403 : 1683485883.py : run_eval : INFO : Epoch 136: F1: 0.9203910614525143, ACC: 0.9395604395604396
2025-03-02 00:28:33,202 : 1196508602.py : eval_pred : INFO : processing 0 lines 
2025-03-02 00:29:34,546 : 1196508602.py : eval_pred : INFO : processing 100 lines 
2025-03-02 00:30:23,263 : 1196508602.py : eval_pred : INFO : processing 200 lines 
2025-03-02 00:31:18,554 : 1196508602.py : eval_pred : INFO : processing 300 lines 
2025-03-02 00:32:17,239 : 1683485883.py : run_eval : INFO : Epoch 137: F1: 0.9199255121042832, ACC: 0.9395604395604396
2025-03-02 00:32:18,037 : 1196508602.py : eval_pred : INFO : processing 0 lines 
2025-03-02 00:35:26,606 : 1196508602.py : eval_pred : INFO : processing 100 lines 
2025-03-02 00:37:06,385 : 1196508602.py : eval_pred : INFO : processing 200 lines 
2025-03-02 00:39:06,845 : 1196508602.py : eval_pred : INFO : processing 300 lines 
2025-03-02 00:40:47,820 : 1196508602.py : eval_pred : INFO : processing 400 lines 
2025-03-02 00:41:59,595 : 1683485883.py : run_eval : INFO : Epoch 134: F1: 0.9141280353200885, ACC: 0.9264069264069265
2025-03-02 00:42:00,444 : 1196508602.py : eval_pred : INFO : processing 0 lines 
2025-03-02 00:45:10,829 : 1196508602.py : eval_pred : INFO : processing 100 lines 
2025-03-02 00:46:50,953 : 1196508602.py : eval_pred : INFO : processing 200 lines 
2025-03-02 00:48:51,158 : 1196508602.py : eval_pred : INFO : processing 300 lines 
2025-03-02 00:50:30,962 : 1196508602.py : eval_pred : INFO : processing 400 lines 
2025-03-02 00:51:43,416 : 1683485883.py : run_eval : INFO : Epoch 135: F1: 0.9148638704930095, ACC: 0.9264069264069265
2025-03-02 00:51:44,304 : 1196508602.py : eval_pred : INFO : processing 0 lines 
2025-03-02 00:54:54,740 : 1196508602.py : eval_pred : INFO : processing 100 lines 
2025-03-02 00:56:35,196 : 1196508602.py : eval_pred : INFO : processing 200 lines 
2025-03-02 00:58:34,959 : 1196508602.py : eval_pred : INFO : processing 300 lines 
2025-03-02 01:00:15,705 : 1196508602.py : eval_pred : INFO : processing 400 lines 
2025-03-02 01:01:26,987 : 1683485883.py : run_eval : INFO : Epoch 136: F1: 0.9163355408388522, ACC: 0.9285714285714286
2025-03-02 01:01:27,846 : 1196508602.py : eval_pred : INFO : processing 0 lines 
2025-03-02 01:04:39,894 : 1196508602.py : eval_pred : INFO : processing 100 lines 
2025-03-02 01:06:20,849 : 1196508602.py : eval_pred : INFO : processing 200 lines 
2025-03-02 01:08:21,591 : 1196508602.py : eval_pred : INFO : processing 300 lines 
2025-03-02 01:10:02,478 : 1196508602.py : eval_pred : INFO : processing 400 lines 
2025-03-02 01:11:13,842 : 1683485883.py : run_eval : INFO : Epoch 137: F1: 0.915599705665931, ACC: 0.9285714285714286
2025-03-02 01:11:14,600 : 1196508602.py : eval_pred : INFO : processing 0 lines 
2025-03-02 01:12:18,588 : 1196508602.py : eval_pred : INFO : processing 100 lines 
2025-03-02 01:13:09,698 : 1196508602.py : eval_pred : INFO : processing 200 lines 
2025-03-02 01:14:06,745 : 1196508602.py : eval_pred : INFO : processing 300 lines 
2025-03-02 01:15:05,028 : 1683485883.py : run_eval : INFO : Epoch 110: F1: 0.9250465549348231, ACC: 0.9423076923076923
2025-03-02 01:15:05,846 : 1196508602.py : eval_pred : INFO : processing 0 lines 
2025-03-02 01:16:09,538 : 1196508602.py : eval_pred : INFO : processing 100 lines 
2025-03-02 01:16:59,412 : 1196508602.py : eval_pred : INFO : processing 200 lines 
2025-03-02 01:17:56,662 : 1196508602.py : eval_pred : INFO : processing 300 lines 
2025-03-02 01:18:55,623 : 1683485883.py : run_eval : INFO : Epoch 111: F1: 0.921322160148976, ACC: 0.9395604395604396
2025-03-02 01:18:56,348 : 1196508602.py : eval_pred : INFO : processing 0 lines 
2025-03-02 01:22:07,405 : 1196508602.py : eval_pred : INFO : processing 100 lines 
2025-03-02 01:23:49,223 : 1196508602.py : eval_pred : INFO : processing 200 lines 
2025-03-02 01:25:50,597 : 1196508602.py : eval_pred : INFO : processing 300 lines 
2025-03-02 01:27:30,364 : 1196508602.py : eval_pred : INFO : processing 400 lines 
2025-03-02 01:28:41,511 : 1683485883.py : run_eval : INFO : Epoch 110: F1: 0.911552612214864, ACC: 0.9264069264069265
2025-03-02 01:28:42,332 : 1196508602.py : eval_pred : INFO : processing 0 lines 
2025-03-02 01:31:53,152 : 1196508602.py : eval_pred : INFO : processing 100 lines 
2025-03-02 01:33:33,749 : 1196508602.py : eval_pred : INFO : processing 200 lines 
2025-03-02 01:35:34,644 : 1196508602.py : eval_pred : INFO : processing 300 lines 
2025-03-02 01:37:14,382 : 1196508602.py : eval_pred : INFO : processing 400 lines 
2025-03-02 01:38:26,613 : 1683485883.py : run_eval : INFO : Epoch 111: F1: 0.9075791022810892, ACC: 0.922077922077922
2025-03-02 01:38:27,467 : 1196508602.py : eval_pred : INFO : processing 0 lines 
2025-03-02 01:39:30,351 : 1196508602.py : eval_pred : INFO : processing 100 lines 
2025-03-02 01:40:20,414 : 1196508602.py : eval_pred : INFO : processing 200 lines 
2025-03-02 01:41:16,467 : 1196508602.py : eval_pred : INFO : processing 300 lines 
2025-03-02 01:42:16,859 : 1683485883.py : run_eval : INFO : Epoch 154: F1: 0.9201117318435756, ACC: 0.9368131868131868
2025-03-02 01:42:17,633 : 1196508602.py : eval_pred : INFO : processing 0 lines 
2025-03-02 01:43:20,819 : 1196508602.py : eval_pred : INFO : processing 100 lines 
2025-03-02 01:44:11,068 : 1196508602.py : eval_pred : INFO : processing 200 lines 
2025-03-02 01:45:07,829 : 1196508602.py : eval_pred : INFO : processing 300 lines 
2025-03-02 01:46:07,075 : 1683485883.py : run_eval : INFO : Epoch 155: F1: 0.9201117318435756, ACC: 0.9368131868131868
2025-03-02 01:46:08,464 : 1196508602.py : eval_pred : INFO : processing 0 lines 
2025-03-02 01:47:11,615 : 1196508602.py : eval_pred : INFO : processing 100 lines 
2025-03-02 01:48:02,045 : 1196508602.py : eval_pred : INFO : processing 200 lines 
2025-03-02 01:48:58,328 : 1196508602.py : eval_pred : INFO : processing 300 lines 
2025-03-02 01:49:57,608 : 1683485883.py : run_eval : INFO : Epoch 156: F1: 0.9213221601489759, ACC: 0.9395604395604396
2025-03-02 01:49:58,366 : 1196508602.py : eval_pred : INFO : processing 0 lines 
2025-03-02 01:51:01,461 : 1196508602.py : eval_pred : INFO : processing 100 lines 
2025-03-02 01:51:51,642 : 1196508602.py : eval_pred : INFO : processing 200 lines 
2025-03-02 01:52:47,953 : 1196508602.py : eval_pred : INFO : processing 300 lines 
2025-03-02 01:53:46,672 : 1683485883.py : run_eval : INFO : Epoch 157: F1: 0.9208566108007451, ACC: 0.9395604395604396
2025-03-02 01:53:47,491 : 1196508602.py : eval_pred : INFO : processing 0 lines 
2025-03-02 01:56:58,933 : 1196508602.py : eval_pred : INFO : processing 100 lines 
2025-03-02 01:58:39,587 : 1196508602.py : eval_pred : INFO : processing 200 lines 
2025-03-02 02:00:40,261 : 1196508602.py : eval_pred : INFO : processing 300 lines 
2025-03-02 02:02:20,175 : 1196508602.py : eval_pred : INFO : processing 400 lines 
2025-03-02 02:03:32,963 : 1683485883.py : run_eval : INFO : Epoch 154: F1: 0.9163355408388522, ACC: 0.9285714285714286
2025-03-02 02:03:33,741 : 1196508602.py : eval_pred : INFO : processing 0 lines 
2025-03-02 02:06:44,352 : 1196508602.py : eval_pred : INFO : processing 100 lines 
2025-03-02 02:08:24,654 : 1196508602.py : eval_pred : INFO : processing 200 lines 
2025-03-02 02:10:24,217 : 1196508602.py : eval_pred : INFO : processing 300 lines 
2025-03-02 02:12:03,815 : 1196508602.py : eval_pred : INFO : processing 400 lines 
2025-03-02 02:13:15,389 : 1683485883.py : run_eval : INFO : Epoch 155: F1: 0.9163355408388522, ACC: 0.9285714285714286
2025-03-02 02:13:16,218 : 1196508602.py : eval_pred : INFO : processing 0 lines 
2025-03-02 02:16:24,961 : 1196508602.py : eval_pred : INFO : processing 100 lines 
2025-03-02 02:18:05,117 : 1196508602.py : eval_pred : INFO : processing 200 lines 
2025-03-02 02:20:04,798 : 1196508602.py : eval_pred : INFO : processing 300 lines 
2025-03-02 02:21:45,429 : 1196508602.py : eval_pred : INFO : processing 400 lines 
2025-03-02 02:22:56,492 : 1683485883.py : run_eval : INFO : Epoch 156: F1: 0.9163355408388522, ACC: 0.9285714285714286
2025-03-02 02:22:57,337 : 1196508602.py : eval_pred : INFO : processing 0 lines 
2025-03-02 02:26:07,338 : 1196508602.py : eval_pred : INFO : processing 100 lines 
2025-03-02 02:27:48,104 : 1196508602.py : eval_pred : INFO : processing 200 lines 
2025-03-02 02:29:48,544 : 1196508602.py : eval_pred : INFO : processing 300 lines 
2025-03-02 02:31:28,248 : 1196508602.py : eval_pred : INFO : processing 400 lines 
2025-03-02 02:32:39,868 : 1683485883.py : run_eval : INFO : Epoch 157: F1: 0.9192788815305372, ACC: 0.9307359307359307
2025-03-02 02:32:40,642 : 1196508602.py : eval_pred : INFO : processing 0 lines 
2025-03-02 02:33:42,885 : 1196508602.py : eval_pred : INFO : processing 100 lines 
2025-03-02 02:34:32,368 : 1196508602.py : eval_pred : INFO : processing 200 lines 
2025-03-02 02:35:28,449 : 1196508602.py : eval_pred : INFO : processing 300 lines 
2025-03-02 02:36:26,228 : 1683485883.py : run_eval : INFO : Epoch 174: F1: 0.9208566108007451, ACC: 0.9395604395604396
2025-03-02 02:36:26,963 : 1196508602.py : eval_pred : INFO : processing 0 lines 
2025-03-02 02:37:29,984 : 1196508602.py : eval_pred : INFO : processing 100 lines 
2025-03-02 02:38:19,961 : 1196508602.py : eval_pred : INFO : processing 200 lines 
2025-03-02 02:39:16,439 : 1196508602.py : eval_pred : INFO : processing 300 lines 
2025-03-02 02:40:14,338 : 1683485883.py : run_eval : INFO : Epoch 175: F1: 0.9208566108007451, ACC: 0.9395604395604396
2025-03-02 02:40:15,098 : 1196508602.py : eval_pred : INFO : processing 0 lines 
2025-03-02 02:43:21,771 : 1196508602.py : eval_pred : INFO : processing 100 lines 
2025-03-02 02:45:00,541 : 1196508602.py : eval_pred : INFO : processing 200 lines 
2025-03-02 02:46:59,988 : 1196508602.py : eval_pred : INFO : processing 300 lines 
2025-03-02 02:48:38,719 : 1196508602.py : eval_pred : INFO : processing 400 lines 
2025-03-02 02:49:48,411 : 1683485883.py : run_eval : INFO : Epoch 174: F1: 0.9211920529801326, ACC: 0.9329004329004329
2025-03-02 02:49:49,207 : 1196508602.py : eval_pred : INFO : processing 0 lines 
2025-03-02 02:52:58,134 : 1196508602.py : eval_pred : INFO : processing 100 lines 
2025-03-02 02:54:38,958 : 1196508602.py : eval_pred : INFO : processing 200 lines 
2025-03-02 02:56:39,609 : 1196508602.py : eval_pred : INFO : processing 300 lines 
2025-03-02 02:58:18,110 : 1196508602.py : eval_pred : INFO : processing 400 lines 
2025-03-02 02:59:30,481 : 1683485883.py : run_eval : INFO : Epoch 175: F1: 0.9219278881530538, ACC: 0.9329004329004329
