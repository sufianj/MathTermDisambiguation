{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2191d23f-648a-4c6f-9bf4-0a6e6d2aa831",
   "metadata": {},
   "source": [
    "# Sentence Pair Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72e4533f-fd7b-470c-a6e9-303091bb7088",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import torch\n",
    "import numpy as np\n",
    "import argparse\n",
    "from math import exp\n",
    "\n",
    "from torch import nn\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "from random import random as rand\n",
    "\n",
    "\n",
    "from transformers import BertTokenizer, BertForNextSentencePrediction\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM,AutoModel\n",
    "#import sentence_pair_utils\n",
    "#import sentence_pair_evaluation\n",
    "import transformers\n",
    "import pandas as pd\n",
    "\n",
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85880ba5-1260-4d7b-9cfe-5bc9b80005db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "transformers.logging.set_verbosity_error()\n",
    "\n",
    "LOG_FORMAT = '%(asctime)s : %(filename)s : %(funcName)s : %(levelname)s : %(message)s'\n",
    "logging.basicConfig(filename='SentencePairCLS.log', level=logging.INFO, format=LOG_FORMAT)\n",
    "logger = logging.getLogger(\"SentencePairCLS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "296682a0-ba60-4e5c-a039-3f725f498044",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "def pkl_vars(varname, filename):\n",
    "    with open(filename, 'wb') as file:\n",
    "        pickle.dump(varname, file)\n",
    "        \n",
    "def reload_vars(filename):\n",
    "    this_var = None\n",
    "    with open(filename, 'rb') as file:\n",
    "        this_var = pickle.load(file)      \n",
    "    return this_var"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5756a2a8-ae30-4038-a134-28721e89b255",
   "metadata": {},
   "source": [
    "## Dataset prepation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0a0b92ab-2088-4379-abc5-306cad73b5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_flattened_dd = pd.read_csv(\"parsed_disambiguation_list_without===.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "99c5fe0c-48d7-4f7f-8250-4fd7fb18f68d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dd_page_title</th>\n",
       "      <th>term</th>\n",
       "      <th>definition</th>\n",
       "      <th>def_page_title</th>\n",
       "      <th>categories</th>\n",
       "      <th>plain_text_definition</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1976</th>\n",
       "      <td>Definition:Zero</td>\n",
       "      <td>Zero</td>\n",
       "      <td>Let $\\mathbb A$ be one of the standard number ...</td>\n",
       "      <td>Definition:Zero Mapping</td>\n",
       "      <td>['Definitions/Mapping Theory']</td>\n",
       "      <td>Let ùî∏ be one of the standard number systems ‚Ñï,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1977</th>\n",
       "      <td>Definition:Zero</td>\n",
       "      <td>Zero</td>\n",
       "      <td>Let $f: R \\to R$ be a mapping on a ring $R$.\\n...</td>\n",
       "      <td>Definition:Root of Mapping</td>\n",
       "      <td>['Definitions/Roots of Mappings', 'Definitions...</td>\n",
       "      <td>Let f: R ‚Üí R be a mapping on a ring R.\\n\\nLet ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1978</th>\n",
       "      <td>Definition:Zero</td>\n",
       "      <td>Zero</td>\n",
       "      <td>Let $\\left( R, +_R, \\times_R \\right)$ be a rin...</td>\n",
       "      <td>Definition:Zero Vector</td>\n",
       "      <td>['Definitions/Module Theory', 'Definitions/Vec...</td>\n",
       "      <td>Let ( R, +_R, √ó_R ) be a ring.\\n\\nLet ( G, +_G...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1979</th>\n",
       "      <td>Definition:Zero Locus</td>\n",
       "      <td>Zero Locus</td>\n",
       "      <td>Let $k$ be a field.\\n\\nLet $n\\geq1$ be a natur...</td>\n",
       "      <td>Definition:Zero Locus of Set of Polynomials</td>\n",
       "      <td>['Definitions/Algebraic Geometry']</td>\n",
       "      <td>Let k be a field.\\n\\nLet n‚â•1 be a natural numb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1980</th>\n",
       "      <td>Definition:Zero Locus</td>\n",
       "      <td>Zero Locus</td>\n",
       "      <td>Let $A$ be a commutative ring with unity.\\n\\nL...</td>\n",
       "      <td>Definition:Vanishing Set of Subset of Ring</td>\n",
       "      <td>['Definitions/Zariski Topology']</td>\n",
       "      <td>Let A be a commutative ring with unity.\\n\\nLet...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              dd_page_title        term   \n",
       "1976        Definition:Zero        Zero  \\\n",
       "1977        Definition:Zero        Zero   \n",
       "1978        Definition:Zero        Zero   \n",
       "1979  Definition:Zero Locus  Zero Locus   \n",
       "1980  Definition:Zero Locus  Zero Locus   \n",
       "\n",
       "                                             definition   \n",
       "1976  Let $\\mathbb A$ be one of the standard number ...  \\\n",
       "1977  Let $f: R \\to R$ be a mapping on a ring $R$.\\n...   \n",
       "1978  Let $\\left( R, +_R, \\times_R \\right)$ be a rin...   \n",
       "1979  Let $k$ be a field.\\n\\nLet $n\\geq1$ be a natur...   \n",
       "1980  Let $A$ be a commutative ring with unity.\\n\\nL...   \n",
       "\n",
       "                                   def_page_title   \n",
       "1976                      Definition:Zero Mapping  \\\n",
       "1977                   Definition:Root of Mapping   \n",
       "1978                       Definition:Zero Vector   \n",
       "1979  Definition:Zero Locus of Set of Polynomials   \n",
       "1980   Definition:Vanishing Set of Subset of Ring   \n",
       "\n",
       "                                             categories   \n",
       "1976                     ['Definitions/Mapping Theory']  \\\n",
       "1977  ['Definitions/Roots of Mappings', 'Definitions...   \n",
       "1978  ['Definitions/Module Theory', 'Definitions/Vec...   \n",
       "1979                 ['Definitions/Algebraic Geometry']   \n",
       "1980                   ['Definitions/Zariski Topology']   \n",
       "\n",
       "                                  plain_text_definition  \n",
       "1976  Let ùî∏ be one of the standard number systems ‚Ñï,...  \n",
       "1977  Let f: R ‚Üí R be a mapping on a ring R.\\n\\nLet ...  \n",
       "1978  Let ( R, +_R, √ó_R ) be a ring.\\n\\nLet ( G, +_G...  \n",
       "1979  Let k be a field.\\n\\nLet n‚â•1 be a natural numb...  \n",
       "1980  Let A be a commutative ring with unity.\\n\\nLet...  "
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_flattened_dd.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "c28c0a72-63db-4c2c-bba9-61dbb0d79a6e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def_set = set(df_flattened_dd[\"plain_text_definition\"])\n",
    "pkl_vars(def_set, \"data/vars/def_set.pkl\")\n",
    "\n",
    "title_set = set(df_flattened_dd[\"def_page_title\"])\n",
    "pkl_vars(title_set, \"data/vars/title_set.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b2dcea42-878c-4b22-b7d7-a955c6c88908",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "parsed_disamb_list = []\n",
    "for key, group in df_flattened_dd.groupby(\"term\"):\n",
    "    items = group[[\"plain_text_definition\", \"def_page_title\"]].to_dict(orient=\"records\")\n",
    "    parsed_disamb_list.append({\"term\": key, \"def_item_list\": items})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4bbbec8e-b5fe-4724-8963-0445730d9d08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pkl_vars(parsed_disamb_list,\"data/vars/parsed_disamb_list_without===.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8b9ff774-1c8e-4882-be45-0b2369d3c283",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "parsed_disamb_list = reload_vars(\"data/vars/parsed_disamb_list_without===.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d6ec2a9e-6897-4685-889e-069ffddc3478",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "344"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(parsed_disamb_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9414c660-e4da-49af-8fa1-7d31f204130c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "276"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "random.shuffle(parsed_disamb_list)\n",
    "test_disamb_list = parsed_disamb_list[:68] # int(0.2 * len(parsed_disamb_list)) = 68\n",
    "train_disamb_list = parsed_disamb_list[68:]\n",
    "len(train_disamb_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b0ffd6de-7973-465f-aed1-ee13accfa172",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pkl_vars(train_disamb_list, 'data/vars/train_disamb_list.pkl')\n",
    "pkl_vars(test_disamb_list, 'data/vars/test_disamb_list.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "261b348e-206a-4f31-a6a3-31370e0dfe77",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'term': 'Negation',\n",
       " 'def_item_list': [{'plain_text_definition': 'The logical not or (logical) negation operator is a unary connective whose action is to reverse the truth value of the statement on which it operates.\\n\\n:p is defined as:\\n:p is not true\\n:It is not the case that p is true\\n:It is false that p\\n:p is false.\\n\\n\\nThus the statement p is called the negation of p.\\n\\n\\np is voiced not p.\\n\\n\\n=== Truth Function ===\\nThe logical not connective defines the truth function f^ as follows:\\n\\n \\n \\n \\n \\n\\n=== Truth Table ===\\nThe characteristic truth table of the negation operator p is as follows:\\n\\n:[ p p; F T; T F;   ]\\n\\n=== Boolean Interpretation ===\\nLet ùêÄ be a propositional formula.\\n\\nLet  denote the negation operator.\\n\\n\\nThe truth value of ùêÄ under a boolean interpretation v is given by:\\n\\n:v (   )ùêÄ = T    : v (   )ùêÄ = F\\nF    : v (   )ùêÄ = T',\n",
       "   'def_page_title': 'Definition:Logical Not'},\n",
       "  {'plain_text_definition': 'The negation function is the function defined on the various standard number systems as follows:\\n\\n\\n=== Integer Negation Function ===\\nThe negation function h: ‚Ñ§‚Üí‚Ñ§ is defined on the set of integers as:\\n:‚àÄ n ‚àà‚Ñ§: h (   )n = -n\\n\\n\\nCategory:Definitions/Negation Functions\\nCategory:Definitions/Integers\\n\\n=== Rational Negation Function ===\\nThe negation function h: ‚Ñö‚Üí‚Ñö is defined on the set of rational numbers as:\\n:‚àÄ x ‚àà‚Ñö: h (   )x = -x\\n\\n\\nCategory:Definitions/Negation Functions\\nCategory:Definitions/Rational Numbers\\n\\n=== Real Negation Function ===\\nThe negation function h: ‚Ñù‚Üí‚Ñù is defined on the set of real numbers as:\\n:‚àÄ x ‚àà‚Ñù: h (   )x = -x\\n\\n\\nCategory:Definitions/Negation Functions\\nCategory:Definitions/Real Numbers\\n\\n=== Complex Negation Function ===\\nThe negation function h: ‚Ñù‚Üí‚Ñù is defined on the set of complex numbers as:\\n:‚àÄ z = x + i y ‚àà‚ÑÇ: h (   )z = -x - i y\\n\\n\\nCategory:Definitions/Negation Functions\\nCategory:Definitions/Complex Numbers\\n\\nCategory:Definitions/Numbers\\nCategory:Definitions/Negation Functions',\n",
       "   'def_page_title': 'Definition:Negation Function'}]}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_disamb_list[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b8c70b-d4fd-4949-b28e-22c2dbb78720",
   "metadata": {
    "tags": []
   },
   "source": [
    "### make  dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c649ee5c-8271-4f7f-99d5-e20ef9fb146f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "344"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the kb is a dictionary of notion and definition page titles, each title contains the term and its domain\n",
    "def make_def_kb(def_disamb_list):\n",
    "    def_kb = dict()\n",
    "    for x in def_disamb_list:\n",
    "        term = x['term']\n",
    "        titles = [def_item['def_page_title'] for def_item in x['def_item_list']]\n",
    "        def_kb[term] = titles\n",
    "    return def_kb\n",
    "\n",
    "def_disamb_kb = make_def_kb(parsed_disamb_list)\n",
    "len(def_disamb_kb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "10ca8d87-4c71-4c3b-80f2-7e8cc81cbb7e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pkl_vars(def_disamb_kb, 'data/vars/def_disamb_kb.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "4101a17d-1469-4a6c-b6be-6e1ffda45aed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def flatten_def_disamb_list(disamb_list):\n",
    "    flattened_dd_list = []\n",
    "    for li in disamb_list:\n",
    "        for def_item in li['def_item_list']:\n",
    "            data = {\n",
    "                'title': def_item['def_page_title'],\n",
    "                'term': li['term'],\n",
    "                'definition': def_item['plain_text_definition'],\n",
    "                #'def_page_title': def_item['title'],\n",
    "                #'categories': def_item['categories']\n",
    "            }\n",
    "            flattened_dd_list.append(data)\n",
    "    return flattened_dd_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "01020a1b-e172-427b-ad77-a7b9fc35ab63",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "376"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flattened_test_disam_list = flatten_def_disamb_list(test_disamb_list)\n",
    "len(flattened_test_disam_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ee2345a9-d713-4ad0-bb59-41109b6ad7d4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1605"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flattened_train_disamb_list = flatten_def_disamb_list(train_disamb_list)\n",
    "len(flattened_train_disamb_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b35d897a-1249-478e-91e8-ad4e9d7d3b1a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pkl_vars(flattened_test_disam_list, 'data/vars/flattened_test_disam_list.pkl'), pkl_vars(flattened_train_disamb_list, 'data/vars/flattened_train_disamb_list.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b09cd075-1931-4051-90d3-d6177c78fee3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "flattened_test_disam_list = reload_vars('data/vars/flattened_test_disam_list.pkl')\n",
    "df_flattened_test_disam_list = pd.DataFrame(flattened_test_disam_list)\n",
    "df_flattened_test_disam_list.to_csv(\"data/SP_CLS/df_flattened_test_disam_list.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "72e6e7dd-a2b9-421b-8d7d-161345c8ac88",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "flattened_train_disamb_list = reload_vars('data/vars/flattened_train_disamb_list.pkl')\n",
    "df_flattened_train_disam_list = pd.DataFrame(flattened_train_disamb_list)\n",
    "df_flattened_train_disam_list.to_csv(\"data/SP_CLS/df_flattened_train_disam_list.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1640513-2792-4950-a0a7-55c907cbbe65",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### GLADIS data samples"
   ]
  },
  {
   "cell_type": "raw",
   "id": "911837b6-bc6a-4fa0-b561-85d32ca924fa",
   "metadata": {},
   "source": [
    "# eval/test_set/bio_umls_test.json\n",
    "{\"index\": 28, \"short_term\": \"PEP\", \"long_term\": \"pre-ejection period\", \"type\": \"Biomedical Concept\", \"tokens\": [\"With\", \"regard\", \"to\", \"ANS\", \"activity\", \",\", \"positive\", \"direct\", \"effects\", \"on\", \"externalizing\", \"behaviour\", \"problems\", \"were\", \"present\", \"for\", \"HR\", \"(\", \"standardized\", \"\\u03b2\", \"=\", \"0.306\", \",\", \"p\", \"=\", \"0.020\", \")\", \"and\", \"PEP\", \"(\", \"standardized\", \"\\u03b2\", \"=\", \"-0.323\", \",\", \"p\", \"=\", \"0.031\", \")\", \",\", \"though\", \"not\", \"for\", \"RSA\", \".\"]},\n",
    "{}"
   ]
  },
  {
   "cell_type": "raw",
   "id": "55493662-e891-4718-9cbe-b49c3b672dd1",
   "metadata": {},
   "source": [
    "# eval/dict/bio_umls_dict.json\n",
    "\n",
    "{\n",
    "    \"PEP\": [ /*short term*/\n",
    "        [\n",
    "            \"pepleomycin\",  /*long term*/\n",
    "            1\n",
    "        ],\n",
    "        \n",
    "        ...\n",
    "        \n",
    "        [\n",
    "            \"pre-ejection period\", /*long term*/\n",
    "            1\n",
    "        ]\n",
    "    ],\n",
    "    ...\n",
    "    \n",
    "    \"TLR2\": [ /*short term*/\n",
    "        [\n",
    "            \"Toll-like receptor 2\",\n",
    "            1\n",
    "        ]\n",
    "    ],\n",
    "}"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3e0ce64e-48b6-4fc2-a530-77e92a265d14",
   "metadata": {},
   "source": [
    "#input/dataset/biomedical/test.json\n",
    "\n",
    "{\"index\": 28, \"short_term\": \"PEP\", \"long_term\": \"pre-ejection period\", \"type\": \"Biomedical Concept\", \"tokens\": [\"With\", \"regard\", \"to\", \"ANS\", \"activity\", \",\", \"positive\", \"direct\", \"effects\", \"on\", \"externalizing\", \"behaviour\", \"problems\", \"were\", \"present\", \"for\", \"HR\", \"(\", \"standardized\", \"\\u03b2\", \"=\", \"0.306\", \",\", \"p\", \"=\", \"0.020\", \")\", \"and\", \"PEP\", \"(\", \"standardized\", \"\\u03b2\", \"=\", \"-0.323\", \",\", \"p\", \"=\", \"0.031\", \")\", \",\", \"though\", \"not\", \"for\", \"RSA\", \".\"]}\n",
    "{\"index\": 17, \"short_term\": \"CBF\", \"long_term\": \"cerebral blood flow\", \"type\": \"Biomedical Concept\", \"tokens\": [\"The\", \"reconstructed\", \"volume\", \"was\", \"then\", \"compared\", \"with\", \"corresponding\", \"magnetic\", \"resonance\", \"images\", \"demonstrating\", \"that\", \"the\", \"volume\", \"of\", \"reduced\", \"CBF\", \"agrees\", \"with\", \"the\", \"infarct\", \"zone\", \"at\", \"twenty-four\", \"hours\", \".\"]}\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f6d50d53-93dd-4406-933f-5060df24ab4f",
   "metadata": {},
   "source": [
    "#input/pre_train_sample.txt\n",
    "\n",
    "PEP\tprospective early pregnancy\tMethods In a PEP cohort (2016-2017), 203 pregnant women were recruited from 4 to 8 weeks' gestation.\n",
    "PEP\tPoverty and Economic Policy\tAbout 1000 children under the age of 5 die everyREPUBLIC OF KENYA MINISTRY OF STATE FOR PLANNING, NATIONAL DEVELOPMENT AND VISION 2030 8 TH PEP RESEARCH NETWORK CONFERENCE IN DAKAR, SENEGAL PAPER ON THE ACTUAL USES OF CBMSWorld Food Programme capacity building support to Disaster Preparedness and Response in the Philippines Strengthening Food Security through Disaster Risk Reduction January 2013 Disaster Preparedness andDepartment of Public Service Vote Number: 093 Controlling Officer: 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aada8348-8361-4a4a-8795-7e7becd6e3b9",
   "metadata": {},
   "source": [
    "# defining DefDisambiguationBERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c93ffce-6875-4339-8e79-98266bb1638d",
   "metadata": {},
   "source": [
    "## model for next sentence prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "272b769e-13fe-4d70-b199-caba60ac6e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Copied from https://github.com/tigerchen52/GLADIS/blob/master/source/acrobert.py\n",
    "def_disamb_kb = reload_vars('data/vars/def_disamb_kb.pkl')\n",
    "\n",
    "class DefDisambiguationBERT(nn.Module):\n",
    "    def __init__(self, model_name=\"bert-base-uncased\", from_tf=False, device='cpu'):\n",
    "        super().__init__()\n",
    "        self.model_name=model_name\n",
    "        self.device = device\n",
    "        self.model = BertForNextSentencePrediction.from_pretrained(model_name, from_tf=from_tf)\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    def forward(self, pos_x, masked_pos_x=None, neg_x=None, train=True):\n",
    "        loss, scores = 0.0, 0.0\n",
    "        if train:\n",
    "            pos_samples = self.tokenizer(pos_x, padding=True, return_tensors='pt', truncation=True)[\"input_ids\"]\n",
    "            neg_x = self.tokenizer(neg_x, padding=True, return_tensors='pt', truncation=True)[\"input_ids\"]\n",
    "\n",
    "            pos_samples = pos_samples.to(self.device)\n",
    "            neg_x = neg_x.to(self.device)\n",
    "\n",
    "            pos_outputs = self.model(pos_samples).logits\n",
    "            neg_outputs = self.model(neg_x).logits\n",
    "            pos_scores = 1 - nn.Softmax(dim=1)(pos_outputs)[:, 0]\n",
    "            neg_scores = 1 - nn.Softmax(dim=1)(neg_outputs)[:, 0]\n",
    "            loss = triplet_loss(pos_scores, neg_scores, args.margin)\n",
    "\n",
    "        else:\n",
    "            samples = self.tokenizer(pos_x, padding=True, return_tensors='pt', truncation=True)[\"input_ids\"]\n",
    "            samples = samples.to(self.device)\n",
    "            outputs = self.model(samples).logits\n",
    "            scores = nn.Softmax(dim=1)(outputs)[:, 0]\n",
    "\n",
    "\n",
    "        return loss if train else scores\n",
    "    \n",
    "        \n",
    "def triplet_loss(pos_score, neg_score, margin=0.2):\n",
    "    losses = torch.relu(pos_score - neg_score + margin)\n",
    "    return losses.mean()\n",
    "\n",
    "\n",
    "def softmax(elements):\n",
    "    total = sum([exp(e) for e in elements])\n",
    "    return exp(elements[0]) / total\n",
    "\n",
    "def cal_nsp_score(model, tokenizer, titles, definition, batch_size):\n",
    "    ps = list()\n",
    "    for index in range(0, len(titles), batch_size):\n",
    "        batch_tt = titles[index:index + batch_size]\n",
    "        batch_df = [definition] * len(batch_tt)\n",
    "        encoding = tokenizer(batch_tt, batch_df, return_tensors=\"pt\", padding=True, truncation=True, max_length=400)#.to(device)\n",
    "        outputs = model(**encoding)\n",
    "        logits = outputs.logits#.cpu().detach().numpy()\n",
    "        p = [softmax(lg) for lg in logits]\n",
    "        ps.extend(p)\n",
    "    return ps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72468da-0876-4ee4-862e-aad688a7f3fb",
   "metadata": {},
   "source": [
    "## model for embedding similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "bfbc297b-b6ae-4c28-ad54-6220c6c254f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SentenceEncoderModel(nn.Module):\n",
    "    def __init__(self, model_name=\"bert-base-uncased\", from_tf=False, device='cpu'):\n",
    "        super().__init__()\n",
    "        self.model_name=model_name\n",
    "        self.device = device\n",
    "        self.model = AutoModel.from_pretrained(model_name, from_tf=from_tf)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        \n",
    "        def_emb_dict_path = f\"data/vars/{model_name.split('/')[-1]}_def_emb_dict.pkl\"\n",
    "        if not os.path.isfile(def_emb_dict_path):\n",
    "            defs = list(reload_vars(\"data/vars/def_set.pkl\"))\n",
    "            def_embs = get_embeddings(self.model, self.tokenizer, defs)\n",
    "            def_emb_dict = dict()\n",
    "            for index, definition in enumerate(defs):\n",
    "                def_emb_dict[definition] = def_embs[index]\n",
    "            pkl_vars(def_emb_dict, def_emb_dict_path)\n",
    "        else:\n",
    "            def_emb_dict = reload_vars(def_emb_dict_path)           \n",
    "        self.def_emb_dict = def_emb_dict\n",
    "\n",
    "        title_emb_dict_path = f\"data/vars/{model_name.split('/')[-1]}_title_emb_dict_path.pkl\"\n",
    "        if not os.path.isfile(title_emb_dict_path):\n",
    "            titles = list(reload_vars(\"data/vars/title_set.pkl\"))\n",
    "            tt_embs = get_embeddings(self.model, self.tokenizer, titles)\n",
    "            title_emb_dict = dict()\n",
    "            for index, tt in enumerate(titles):\n",
    "                title_emb_dict[tt] = tt_embs[index]\n",
    "            pkl_vars(title_emb_dict, title_emb_dict_path)\n",
    "        else:\n",
    "            title_emb_dict = reload_vars(title_emb_dict_path)\n",
    "        self.title_emb_dict = title_emb_dict\n",
    "        \n",
    "        \n",
    "#Mean Pooling - Take attention mask into account for correct averaging\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "\n",
    "def get_embeddings(model, tokenizer, sentences):\n",
    "    batch_tokenizer_output = tokenizer(\n",
    "        sentences,\n",
    "        max_length=400,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        batch_model_output = model(**batch_tokenizer_output, output_hidden_states=True)\n",
    "    token_embeddings = batch_model_output.hidden_states[-1]\n",
    "    return mean_pooling(batch_model_output, batch_tokenizer_output.attention_mask).tolist() # convert .tolist() to save storage spaces\n",
    "        \n",
    "\n",
    "def cal_sim_score(sem_model, titles, definition):\n",
    "    if definition in sem_model.def_emb_dict:\n",
    "        def_emb = sem_model.def_emb_dict[definition]\n",
    "    else:\n",
    "        def_emb = get_embeddings(sem_model.model, sem_model.tokenizer, [definition])[0]\n",
    "\n",
    "    tt_embs = []\n",
    "    for tt in titles: \n",
    "        if tt in sem_model.title_emb_dict:\n",
    "            tt_emb = sem_model.title_emb_dict[tt]\n",
    "        else:\n",
    "            tt_emb = get_embeddings(sem_model.model, sem_model.tokenizer, [tt])[0]\n",
    "        tt_embs.append(tt_emb)\n",
    "    \n",
    "    ps = [F.cosine_similarity(torch.Tensor(def_emb), torch.Tensor(tt_emb), dim = -1) for tt_emb in tt_embs]\n",
    "    return ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "3f0821a7-8f34-4f88-b8df-9b6fcf0ce52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(task_model, term, definition, batch_size, dd_kb=def_disamb_kb, cal=\"nsp\"):\n",
    "    titles = get_candidate(dd_kb, term)\n",
    "    #titles = [str.lower(can) for can in titles]\n",
    "    if \"nsp\" == cal:\n",
    "        scores = cal_nsp_score(task_model.model, task_model.tokenizer, titles, definition, batch_size)\n",
    "        max_index = np.argmax(scores)\n",
    "    else:\n",
    "        scores = cal_sim_score(task_model, titles, definition)\n",
    "        max_index = np.argmax(scores)\n",
    "    return titles[max_index]\n",
    "\n",
    "# evaluate prediction based on nsp and kb\n",
    "def eval_pred(task_model, flattened_dd_list_path, batch_size, dd_kb=def_disamb_kb, cal=\"nsp\",train=True):\n",
    "    data = reload_vars(flattened_dd_list_path)\n",
    "    true_labels, pred_labels = list(), list()\n",
    "    for index, sample in enumerate(data):\n",
    "        if index % 100 == 0:\n",
    "            logger.info('processing {a} lines '.format(a=index))\n",
    "        if train and index > 200: break\n",
    "        term = sample['term']\n",
    "        title = sample['title']\n",
    "        definition = sample['definition']\n",
    "        pred = predict(task_model, term, definition, batch_size, dd_kb=def_disamb_kb, cal=cal)\n",
    "        true_labels.append(title)\n",
    "        pred_labels.append(pred)\n",
    "    df_res = pd.DataFrame(data)\n",
    "    df_res['pred'] = pred_labels\n",
    "    df_res.to_csv(f\"data/res/{task_model.model_name.split('/')[-1]}_{cal}.csv\")\n",
    "    macro_f1_score = macro_f1(true_labels, pred_labels)\n",
    "    acc = accuracy(true_labels, pred_labels)\n",
    "    return macro_f1_score, acc\n",
    "\n",
    "\n",
    "# √† refaire\n",
    "def train(device):\n",
    "    model = DefDisambiguationBERT(device=device)\n",
    "    model.to(device)\n",
    "    loader = utils.AcroBERTLoader(batch_size=args.batch_size, tokenizer=model.tokenizer, kb=def_disamb_kb, shuffle=args.shuffle, hard_num=args.hard_neg_numbers)\n",
    "\n",
    "    train_loader = loader(data_path=args.pre_train_path)\n",
    "    trainable_num = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    logger.info(trainable_num)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=args.learning_rate)\n",
    "    scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=0.99)\n",
    "\n",
    "    max_f1, max_epoch = 0.0, 0\n",
    "    for e in range(args.epoch):\n",
    "        epoch_loss = 0\n",
    "        batch_num = 0\n",
    "\n",
    "        for pos_samples, masked_pos_samples, neg_samples in train_loader:\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            if batch_num % args.loss_check_step == 0 and batch_num != 0:\n",
    "                logger.info('sample = {b}, loss = {a}'.format(a=epoch_loss / batch_num, b=batch_num * args.batch_size))\n",
    "\n",
    "            if batch_num % args.check_step == 0 and batch_num != 0:\n",
    "                for g in optimizer.param_groups:\n",
    "                    g['lr'] *= args.lr_decay\n",
    "            if batch_num % args.check_step == 0 and batch_num != 0:\n",
    "                temp_path = args.model_path.format(a=str(e + 1) + '_' + str(batch_num))\n",
    "                torch.save(model.state_dict(), temp_path)\n",
    "            loss = model(pos_samples, masked_pos_samples, neg_samples)\n",
    "\n",
    "            # backward\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "            batch_num += 1\n",
    "\n",
    "        scheduler.step()\n",
    "        temp_path = args.model_path.format(a=e + 1)\n",
    "        logger.info('the pre-training finished, saving model, path = {a}'.format(a=temp_path))\n",
    "        torch.save(model.state_dict(), temp_path)\n",
    "    return max_f1, max_epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e92fef4-4406-44e1-a93b-c0c6507a8270",
   "metadata": {
    "tags": []
   },
   "source": [
    "## utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "3ecbe34c-031a-46c8-ac16-0899cd0e43e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# adpated from https://github.com/tigerchen52/GLADIS/blob/master/source/utils.py \n",
    "\n",
    "#def_disamb_kb = reload_vars('data/vars/def_disamb_kb.pkl')\n",
    "def get_candidate(def_disamb_kb, term):\n",
    "    return def_disamb_kb[term]\n",
    "\n",
    "class TextData(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.all_term = data['term']\n",
    "        self.all_title = data['title']\n",
    "        self.all_definition = data['definition']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all_term)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.all_term[idx], self.all_title[idx], self.all_definition[idx]\n",
    "    \n",
    "def load_pretrain(train_data_path):\n",
    "    all_term, all_title, all_definition = list(), list(), list()\n",
    "    cnt = 0\n",
    "    for line in open(data_path, encoding='utf8'):\n",
    "        cnt += 1\n",
    "        row = line.strip().split('\\t')\n",
    "        if len(row) != 3:continue\n",
    "        # if cnt>200:continue\n",
    "        # obj = json.loads(line)\n",
    "        # short_term, long_term, context = obj['short_term'], obj['long_term'], ' '.join(obj['tokens'])\n",
    "        term, long_term, context = row[0], row[1], row[2]\n",
    "        all_term.append(term)\n",
    "        all_title.append(title)\n",
    "        all_definition.append(definition)\n",
    "\n",
    "    return {'term': all_term, 'title': all_title, 'definition': all_definition}\n",
    "\n",
    "class AcroBERTLoader():\n",
    "    def __init__(self, batch_size, tokenizer, kb, shuffle=True, masked_prob=0.15, hard_num=2):\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.tokenizer = tokenizer\n",
    "        self.masked_prob = masked_prob\n",
    "        self.hard_num = hard_num\n",
    "        self.kb = kb\n",
    "        self.all_long_terms = list()\n",
    "        for vs in self.kb.values():\n",
    "            self.all_long_terms.extend(list(vs))\n",
    "\n",
    "    def select_negative(self, target):\n",
    "        selected, flag, max_time = None, True, 10\n",
    "        if target in self.kb:\n",
    "            long_term_candidates = self.kb[target]\n",
    "            if len(long_term_candidates) == 1:\n",
    "                long_term_candidates = self.all_long_terms\n",
    "        else:\n",
    "            long_term_candidates = self.all_long_terms\n",
    "        attempt = 0\n",
    "        while flag and attempt < max_time:\n",
    "            attempt += 1\n",
    "            selected = random.choice(long_term_candidates)\n",
    "            if selected != target:\n",
    "                flag = False\n",
    "        if attempt == max_time:\n",
    "            selected = random.choice(self.all_long_terms)\n",
    "        return selected\n",
    "\n",
    "    def collate_fn(self, batch_data):\n",
    "        batch_short_term, batch_long_term, batch_context = list(zip(*batch_data))\n",
    "        pos_samples, neg_samples, masked_pos_samples = list(),  list(), list()\n",
    "        for _ in range(self.hard_num):\n",
    "            temp_pos_samples = [batch_long_term[index] + ' [SEP] ' + batch_context[index] for index in range(len(batch_long_term))]\n",
    "            neg_long_terms = [self.select_negative(st) for st in batch_short_term]\n",
    "            temp_neg_samples = [neg_long_terms[index] + ' [SEP] ' + batch_context[index] for index in range(len(batch_long_term))]\n",
    "            temp_masked_pos_samples = [batch_long_term[index] + ' [SEP] ' + batch_context[index] for index in range(len(batch_long_term))]\n",
    "\n",
    "            pos_samples.extend(temp_pos_samples)\n",
    "            neg_samples.extend(temp_neg_samples)\n",
    "            masked_pos_samples.extend(temp_masked_pos_samples)\n",
    "        return pos_samples,  masked_pos_samples,  neg_samples\n",
    "\n",
    "    def __call__(self, data_path):\n",
    "        dataset = load_pretrain(data_path=data_path)\n",
    "        logger.info('loaded dataset, sample = {a}'.format(a=len(dataset['short_term'])))\n",
    "        dataset = TextData(dataset)\n",
    "        train_iterator = DataLoader(dataset=dataset, batch_size=self.batch_size // (2 * self.hard_num), shuffle=self.shuffle,\n",
    "                                    collate_fn=self.collate_fn)\n",
    "        return train_iterator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4844877-3dda-49e9-8d5e-c201ec28b325",
   "metadata": {
    "tags": []
   },
   "source": [
    "## evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "933d3143-0d3c-4d2e-ae86-c3f705c4b488",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# adpated from https://github.com/tigerchen52/GLADIS/blob/master/source/evaluation.py\n",
    "from sklearn.metrics import f1_score\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "def transform_to_index(trues, preds):\n",
    "\n",
    "    true_map = dict([(title, index) for index, title in enumerate(list(OrderedDict.fromkeys(trues)))])\n",
    "    tag_cnt = len(true_map)\n",
    "    true_index = [true_map[t] for t in trues]\n",
    "\n",
    "    for pred in preds:\n",
    "        if pred not in true_map:\n",
    "            true_map[pred] = len(true_map)\n",
    "    pred_index = [true_map[pred] for pred in preds]\n",
    "    return true_index, pred_index, tag_cnt\n",
    "\n",
    "\n",
    "def macro_f1(trues, preds):\n",
    "    true_index, pred_index, tag_cnt = transform_to_index(trues, preds)\n",
    "    f1_scores = f1_score(true_index, pred_index, average=None)[:tag_cnt]\n",
    "    macro_f1_score = sum(f1_scores) / len(f1_scores)\n",
    "    return macro_f1_score\n",
    "\n",
    "\n",
    "def accuracy(trues, preds):\n",
    "    acc_cnt = 0\n",
    "    for index, true in enumerate(trues):\n",
    "        pred = preds[index]\n",
    "        if pred == true:\n",
    "            acc_cnt += 1\n",
    "    acc = acc_cnt * 1.0 / len(trues)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00567ac4-e8c9-4a91-956a-38914d922b62",
   "metadata": {},
   "source": [
    "# Evaluate NSP with OOB LMs\n",
    "Note: To cache the pairwised NSP calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "a383f87a-a425-4860-b8c1-27378e05f532",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2h 58min 16s, sys: 8min 32s, total: 3h 6min 49s\n",
      "Wall time: 4min 7s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.08303624480095069, 0.13031914893617022)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "cc_model = DefDisambiguationBERT(\"InriaValda/cc_math_bert_ep10\", from_tf=True, device= device)\n",
    "macro_f1_score, acc = eval_pred(cc_model, \"data/vars/flattened_test_disam_list.pkl\",\n",
    "                                batch_size = 32, dd_kb=def_disamb_kb, train=False) \n",
    "macro_f1_score, acc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "18f2dfae-fe36-449e-ba45-2bb4b4718c3a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3h 8min, sys: 10min 44s, total: 3h 18min 44s\n",
      "Wall time: 4min 21s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.7405525846702316, 0.7872340425531915)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time \n",
    "BERT_OOB = DefDisambiguationBERT(device= device)\n",
    "macro_f1_score, acc = eval_pred(BERT_OOB, \"data/vars/flattened_test_disam_list.pkl\",\n",
    "                                batch_size = 4, dd_kb=def_disamb_kb, train=False) \n",
    "macro_f1_score, acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20bd796d-7a43-4a42-8000-2543189e9067",
   "metadata": {},
   "source": [
    "# Evaluate embedding similarity with OOB LMs\n",
    "Notes: without the pickled sentence-embedding dictionaries, each 768-dimension sentence bert model takes ~ 11 minutes to calculate all the embeddings once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "71252c27-57a4-4286-8aec-575647af3ab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3h 53min 50s, sys: 1h 46min 47s, total: 5h 40min 37s\n",
      "Wall time: 10min 35s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cc_sem_model = SentenceEncoderModel(\"InriaValda/cc_math_bert_ep10\", from_tf=True, device= device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "a328e315-de76-4ab1-b71e-68b711d277ef",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 104 ms, sys: 20.2 ms, total: 124 ms\n",
      "Wall time: 123 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.3265739803243325, 0.42819148936170215)"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "macro_f1_score, acc = eval_pred(cc_sem_model, \"data/vars/flattened_test_disam_list.pkl\",\n",
    "                                batch_size = 4, dd_kb=def_disamb_kb, train=False,\n",
    "                               cal=\"sim\") \n",
    "macro_f1_score, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "237f82eb-2069-4bdc-ae08-a1f1dbde4f65",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 677 ms, sys: 49 ms, total: 726 ms\n",
      "Wall time: 725 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.31244211633218444, 0.4093457943925234)"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "macro_f1_score, acc = eval_pred(cc_sem_model, \"data/vars/flattened_train_disamb_list.pkl\",\n",
    "                                batch_size = 4, dd_kb=def_disamb_kb, train=False,\n",
    "                               cal=\"sim\") \n",
    "macro_f1_score, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "48fce2f2-f52d-4822-b61e-0d8169df273b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 35min 38s, sys: 42min 59s, total: 1h 18min 38s\n",
      "Wall time: 2min 32s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8358798064680415, 0.8696808510638298)"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "mini_sem_model = SentenceEncoderModel(\"sentence-transformers/all-MiniLM-L6-v2\", device= device)\n",
    "\n",
    "macro_f1_score, acc = eval_pred(mini_sem_model, \"data/vars/flattened_test_disam_list.pkl\",\n",
    "                                batch_size = 4, dd_kb=def_disamb_kb, train=False,\n",
    "                               cal=\"sim\") \n",
    "macro_f1_score, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "bfe59298-08d5-4b89-bb88-54df1e382b3b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8222014051522248, 0.8616822429906542)"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "macro_f1_score, acc = eval_pred(mini_sem_model, \"data/vars/flattened_train_disamb_list.pkl\",\n",
    "                                batch_size = 4, dd_kb=def_disamb_kb, train=False,\n",
    "                               cal=\"sim\") \n",
    "macro_f1_score, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "9ce985b9-b346-4ecb-9f47-2975eeb046e8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4h 12min 53s, sys: 1h 47min 41s, total: 6h 35s\n",
      "Wall time: 10min 25s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.2493523113843971, 0.3484042553191489)"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "bert_sem_model = SentenceEncoderModel(\"bert-base-uncased\", device= device)\n",
    "\n",
    "macro_f1_score, acc = eval_pred(bert_sem_model, \"data/vars/flattened_test_disam_list.pkl\",\n",
    "                                batch_size = 4, dd_kb=def_disamb_kb, train=False,\n",
    "                               cal=\"sim\") \n",
    "macro_f1_score, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "f62d46a7-a95f-4cb1-aeb3-1554d39b48f8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.65 s, sys: 78.1 ms, total: 3.73 s\n",
      "Wall time: 3.73 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.2755867156522898, 0.3719626168224299)"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "macro_f1_score, acc = eval_pred(bert_sem_model, \"data/vars/flattened_train_disamb_list.pkl\",\n",
    "                                batch_size = 4, dd_kb=def_disamb_kb, train=False,\n",
    "                               cal=\"sim\") \n",
    "macro_f1_score, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "84a06b83-fd4d-4c65-b90b-817d57df4bb3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4h 10min 15s, sys: 1h 48min 7s, total: 5h 58min 23s\n",
      "Wall time: 10min 32s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.3274004367587258, 0.4308510638297872)"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "MLM_arXiv_sem_model = SentenceEncoderModel(\"math-similarity/Bert-MLM_arXiv\", device= device)\n",
    "\n",
    "macro_f1_score, acc = eval_pred(MLM_arXiv_sem_model, \"data/vars/flattened_test_disam_list.pkl\",\n",
    "                                batch_size = 4, dd_kb=def_disamb_kb, train=False,\n",
    "                               cal=\"sim\") \n",
    "macro_f1_score, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "7a2538bc-4fbb-41dc-b7a8-dab476ab6981",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.28560215301681124, 0.3850467289719626)"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "macro_f1_score, acc = eval_pred(MLM_arXiv_sem_model, \"data/vars/flattened_train_disamb_list.pkl\",\n",
    "                                batch_size = 4, dd_kb=def_disamb_kb, train=False,\n",
    "                               cal=\"sim\") \n",
    "macro_f1_score, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "38485f21-716f-4dee-84d6-8ae64d6871b3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a6cc030a37549f8b5d1709273168117",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/676 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76d23770d5a3488c9273591c9ec9a568",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a127396278154cae9a48056588a58051",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/366 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49037758b6bc4414984daeb37ed63600",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c93ca37df4a84a11961c87a33d5ea98d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/712k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e58054279b394677af8c349ca20c8fcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4h 21min 22s, sys: 1h 44min 14s, total: 6h 5min 36s\n",
      "Wall time: 10min 51s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.5223028605381546, 0.6063829787234043)"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "zbmath_sem_model = SentenceEncoderModel(\"math-similarity/Bert-MLM_arXiv-MP-class_zbMath\", device= device)\n",
    "\n",
    "macro_f1_score, acc = eval_pred(zbmath_sem_model, \"data/vars/flattened_test_disam_list.pkl\",\n",
    "                                batch_size = 4, dd_kb=def_disamb_kb, train=False,\n",
    "                               cal=\"sim\") \n",
    "macro_f1_score, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "4f33e79b-fa56-49de-8f9c-d8355e4a85f1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.4723080926359607, 0.5526479750778817)"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "macro_f1_score, acc = eval_pred(zbmath_sem_model, \"data/vars/flattened_train_disamb_list.pkl\",\n",
    "                                batch_size = 4, dd_kb=def_disamb_kb, train=False,\n",
    "                               cal=\"sim\") \n",
    "macro_f1_score, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "67a5471b-dcbf-41e6-bb58-aba259ee452e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cc_sem_model.title_emb_dict['Definition:Field Zero'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "06abe521-c11f-4e88-827f-3d2641cd8ae2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(0.6233), tensor(0.5268)]"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cal_sim_score(cc_sem_model, [title0, 'blabla'], def0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "cfbb9a3b-8eea-40da-8100-c73534b2e8be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "title02 = 'Definition:Field Zero2'\n",
    "def02 = 'A2 dynamical system is a non-linear system in which a function describes the time dependence of a point in a geometrical space.\\n\\n\\n=== Flow ===\\nIn a dynamical system, a set of time-dependent equations is known as flow.\\n\\n '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "51b8be7a-2e40-40d2-ac5b-e13f0e8d8bbf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "embs = get_embeddings(cc_sem_model.model, cc_sem_model.tokenizer, [title0, 'blabla'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c758967-05d6-4bfc-bdd5-0274ea9fc031",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
